From jholtman at gmail.com  Tue Aug  1 00:09:47 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 31 Jul 2006 18:09:47 -0400
Subject: [R] if function and apply
In-Reply-To: <20060731215428.15996.qmail@web33811.mail.mud.yahoo.com>
References: <20060731215428.15996.qmail@web33811.mail.mud.yahoo.com>
Message-ID: <644e1f320607311509h25de6d9bm9d17d7d536b34fd7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/26afbbc3/attachment.pl 

From ggrothendieck at gmail.com  Tue Aug  1 00:26:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 31 Jul 2006 18:26:25 -0400
Subject: [R] How does biplot.princomp scale its axes?
In-Reply-To: <20060731214735.GL12850@ihug.co.nz>
References: <20060731214735.GL12850@ihug.co.nz>
Message-ID: <971536df0607311526r42916902naae4d36322a1f76f@mail.gmail.com>

Its easiest to just check the source.  biplot is a generic which calls
biplot.princomp which calls biplot.default which in turn calls plot so
try this and examine the source:

stats:::biplot.default


On 7/31/06, Patrick Connolly <p_connolly at ihug.co.nz> wrote:
> I'm attempting to modify how biplot draws its red vectors (among other
> things).  This is how I've started:
>
>
> Biplot <- function(xx, comps = c(1, 2), cex = c(.6, .4))
> {
>  ## Purpose: Makes a biplot with princomp() object to not show arrows
>  ## ----------------------------------------------------------------------
>  ## Arguments: xx is an object made using princomp()
>  ## ----------------------------------------------------------------------
> scores <- xx$scores[, paste("Comp", comps, sep = ".")]
> loadings <- xx$loadings[, paste("Comp", comps, sep = ".")]
> plot(range(scores), range(scores), xlab = "", ylab = "", xaxt = "n",
>           yaxt = "n", pch = " ")
> text(scores[,1], scores[,2], rownames(scores), cex = cex[1])
> axis(2)
> axis(1)
> }
>
> I can make part of a biplot using that function with the USArrests data:
> Biplot(princomp(USArrests, cor = TRUE), c(1,2), cex = c(.6, .4))
>
> Compare that with what we get using biplot.princomp:
> biplot(princomp(USArrests, cor = TRUE), c(1,2), cex = c(.6, .4))
>
> It seems to me that the y-values are the same in both plots, but some
> sort of scaling on the x-axis is happening.  Something similar seems
> to happen with the loadings as well.
>
> I notice in the documentation for biplot, mention is made of "... many
> variations on biplots".  Would I be doing something inexcusable if I
> ignored the differences I've noticed here?
>
> TIA
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>   ___    Patrick Connolly
>  {~._.~}                         Great minds discuss ideas
>  _( Y )_                        Middle minds discuss events
> (:_~*~_:)                        Small minds discuss people
>  (_)-(_)                                   ..... Anon
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Tue Aug  1 00:28:53 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Jul 2006 17:28:53 -0500
Subject: [R] Question about data used to fit the mixed model
In-Reply-To: <BAY106-F1324849CBE25BB139CBA06A25F0@phx.gbl>
References: <BAY106-F1324849CBE25BB139CBA06A25F0@phx.gbl>
Message-ID: <40e66e0b0607311528i7932c313o681d4aec518a7879@mail.gmail.com>

On 7/29/06, Nantachai Kantanantha <kantanantha at hotmail.com> wrote:
> Hi everyone,
>
> I would like to ask a question regarding to the data used to fit the mixed
> model.
>
> I wonder that, for the response variable data used to fit the mixed model
> (either via "spm" or "lme"), we must have several observations per subject
> (i.e. Yij,  i = 1,..,M,  j = 1,.., ni) or it can be just one observation per
> subject (i.e. Yi,  i = 1,...,M). Since we have to specify the groups for
> random effect components, if we have only one observation per subject, then
> each group will have only one observation.

As Harold Doran mentioned in his earlier reply, if you only have one
observation in each group you can't estimate the parameters in a mixed
model because the random effect for a group is completely confounded
with the per-observation noise term for the observation.  The model
would be of the form

X\beta + Z b + \epsilon

for which you would estimate the variance of the components of b and
the variance of the components of \epsilon.  However, with only one
observation per group the number of components in b and in \epsilon
would be the same and, by suitably reordering the observations, the
matrix Z could be made to be an identity matrix.  Thus the model
reduces to

 X\beta + (b + \epsilon)

and the elements of b are confounded with those of \epsilon.

A different version of this question is to ask whether some of the
groups can have only a single observation while others have more that
one observation.  The answer to that is a qualified "yes".

An example of data with different numbers of observations per group is
the star data that Harold mentioned.  The "student" identifier in this
data set is named "id".  If we table the number of observations per
student then table that result we get a table of the number of
students with 1, 2, 3 or 4 observations.

> data("star", package = 'mlmRev')
> table(table(star$id))

   1    2    3    4
4314 2455 1744 3085
> length(unique(star$id))
[1] 11598
> 4314/11598
[1] 0.3719607

This shows that more than a third of the students have data from only
a single year.

It is possible to include such students in a mixed model with a random
effect for student.  It is even possible to include such students in a
mixed model with a random intercept and a random slope with respect to
time for student.  However, such students contribute very little
information to the model fit and the "estimates" (actually
"predictors") of the random effects for such students are artificially
small because they are confounded with the per-observation noise term.

So while it can be attractive when designing an experimental or
planning a observational study to have many groups and few
observations per group, such experiments or studies provide very
sparse information.  Using a mixed model on such data doesn't
magically add information to the data.  Mixed models are statistical
models, not magic.


From bates at stat.wisc.edu  Tue Aug  1 00:37:37 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Jul 2006 17:37:37 -0500
Subject: [R] Fred Mosteller and the "star" data from package "mlmRev"
Message-ID: <40e66e0b0607311537t4f3a9d3tcc097ea116be6ba2@mail.gmail.com>

In writing about the star data from package "mlmRev" I was reminded of
a comment in the New York Times obituary,  "C. Frederick Mosteller, a
Pioneer of Statistics, Dies at 89", that appeared on July 27. In part
it stated

"In the 1980's, he was instrumental in persuading Tennessee to conduct
a controlled study on the effect of classroom size. The study showed
convincingly that smaller classes significantly helped children from
poorer minority families."

I believe this is referring to the study that generated the star data
in package "mlmRev".


From dylan.beaudette at gmail.com  Tue Aug  1 01:43:10 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Mon, 31 Jul 2006 16:43:10 -0700
Subject: [R] questions regarding spline functions
Message-ID: <200607311643.10178.dylan.beaudette@gmail.com>

Greetings,

A couple general questions regarding the use of splines to interpolate depth 
profile data.

Here is an example of a set of depths, with associated attributes for a given 
soil profile, along  with a function for calculating midpoints from a set of 
soil horizon boundaries:

#calculate midpoints:
mid <- function(x) {
for( i in 1:length(x)) {
 if( i > 1) {
   a[i] = (x[i] - x[i-1]) / 2 + x[i-1] 
  } 
 } 
#reurn the results
a[which(!is.na(a))]
}

#horizon depth bounds
z <- c(0,2,18,24,68,160,170,192,200)

#horizon midpoints, associated with horizon attribute
x <- mid(z)

#clay pct
y <- c(0,1,2,2,4,7,6,1)

#plot them
plot(y ~ x, xlab="Depth", ylab="Percent Clay", type="s")
points(y ~ x, cex=0.5, pch=16)

These point pairs usually represent a trend with depth, which I would like to 
model with splines - or some similar approach, as they have been found to 
work better than other methods such as a fitted polynomial.

Using the B Spline function from the 'splines' package, it is possible to fit 
a model of some property with depth based on the bs() function:

#natual, B-Splines
library(splines)

#fit a b-spline model:
fm <- lm(y ~ bs(x, df=5) )

I am able to predict a soil property with depth, at unsampled locations with 
this model with:

new_x <-  seq(range(x)[1], range(x)[2], len = 200)

#predict attribute at unsampled depths:
new_y <- predict(fm, data.frame(x=new_x) )

#plot the predicted attribute at the unsampled depths
lines(new_x, new_y, col='red')

This tends to work fairly well (see attached), but I am wondering if I can use 
the 'knots' parameter in the bs() function for incorporation of horizon 
boundary information into the creation of the spline. Moreover, it would be 
nice if the spline-based model 'fm' would predict a set of values with 
similar mean and range as the original data points: i.e

#summary of clay values from original data:
summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   1.000   2.000   2.875   4.500   7.00

#see above
summary(new_y)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.05786  2.09500  3.13200  3.62800  5.17100  7.08700


This is based on an article I read :
http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V67-3WWRDYY-3&_user=4421&_handle=V-WA-A-W-AU-MsSAYWW-UUA-U-AACZEDZWBC-AACVCCDUBC-BZAYUEWB-AU-U&_fmt=summary&_coverDate=08%2F31%2F1999&_rdoc=3&_orig=browse&_srch=%23toc%235807%231999%23999089998%23108393!&_cdi=5807&view=c&_acct=C000059598&_version=1&_urlVersion=0&_userid=4421&md5=488f1e114d8d64265ff65506e9587e71

where the author talks about a so-called 'equal-area quadratic smoothing 
spline' approach to describing a soil property depth function. Unfortunately 
the author did not provide sample code....

Any thoughts / input would be greatly appreciated!

Cheers,

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341
-------------- next part --------------
A non-text attachment was scrubbed...
Name: spline1.png
Type: image/png
Size: 3953 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060731/5a5b9df6/attachment.png 

From jz7 at duke.edu  Tue Aug  1 01:29:08 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Mon, 31 Jul 2006 19:29:08 -0400 (EDT)
Subject: [R] question about prediction etc. in Ridge regression (MASS
	library)
In-Reply-To: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
Message-ID: <Pine.GSO.4.58.0607311919200.20820@godzilla.acpub.duke.edu>

Dear all,

I am trying to apply Ridge regression to my dataset, and then I would like
to predict the Y responses using the Ridge model (of certain lambda) for
new data point. The only Ridge regression functions I found is in "MASS"
library. However, there are very few functions available: lm.ridge(),
plot(), and select(). I didn't see any option to "predict" the Y response.

Does anyone know what else functions I could use to make prediction (using
Ridge model) or how I should write my own code to do the prediction? Also,
is there any way to calculate R^2 (or q^2) or the LOO-CV for Ridge model?

Really appreciate your kind help!

Sincerely,
Jeny


From mscabral at fc.ul.pt  Tue Aug  1 01:43:03 2006
From: mscabral at fc.ul.pt (=?iso-8859-1?Q?Maria_Salom=E9_Esteves_Cabral?=)
Date: Tue, 1 Aug 2006 00:43:03 +0100
Subject: [R] standard dev in glmmPQL
Message-ID: <3AA0B59C9640784C8956888BF8AFC5DD1FF247@fc-mailserver01.ul.pt>

Hi!
 
Can anyone let me know how can I get the stdDev of the random intercept from the output  of glmmPQL?
 
 
Thanks
 
Salom?


From ccarey at fhcrc.org  Tue Aug  1 02:33:32 2006
From: ccarey at fhcrc.org (ccarey at fhcrc.org)
Date: Mon, 31 Jul 2006 17:33:32 -0700
Subject: [R] rgb and col2rgb color conversion/modification/shading
Message-ID: <1154392412.44cea15c0c1fa@webmail.fhcrc.org>

I want to get a lighter shade of a color...I have a lot of colored objects and
want each one printed as a foreground against a slightly lighter background.

I thought I could try something like changing the alpha channel by first
converting it to rgb. 

But prior to trying that, I'm stuck with how to get the color after converting
using col2rgb() to be interpreted again as a color, rather than a simple
vector?

Anyone have any help/ or alternative suggestion...

Thanks, -c
----------------------
TRYING WITH A SINGLE COLOR:

mycol<-"red"

> col2rgb(mycol)
      [,1]
red    255
green    0
blue     0

> rgb(col2rgb(mycol),maxColorValue=255)

Error in rgb(col2rgb("red")) : argument "green" is missing, with no default


From maj at stats.waikato.ac.nz  Tue Aug  1 06:08:39 2006
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 01 Aug 2006 16:08:39 +1200
Subject: [R] Fitting models in a loop
Message-ID: <44CED3C7.5020600@stats.waikato.ac.nz>

If I want to display a few polynomial regression fits I can do something 
like

for (i in 1:6) {
	mod <- lm(y ~ poly(x,i))
	print(summary(mod))
	}

Suppose that I don't want to over-write the fitted model objects, 
though. How do I create a list of blank fitted model objects for later 
use in a loop?

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From CNI-COPYRIGHT-digest at cni.org  Tue Aug  1 06:30:41 2006
From: CNI-COPYRIGHT-digest at cni.org (CNI-COPYRIGHT administration)
Date: Tue, 01 Aug 2006 00:30:41 -0400
Subject: [R] Confirmation Request (3355406281)
Message-ID: <listadmin-14656424@cni.org>

This is an automated message from the
  <CNI-COPYRIGHT at cni.org> mailing list manager

Somebody (probably you) have requested the subscribe(digest) operation
  for your <r-help at stat.math.ethz.ch> address

If you want to confirm this operation,
  use the Reply command in your mailer.

Check that the Subject of the reply message contains
  the confirmation ID: 3355406281,
  the reply is directed to <CNI-COPYRIGHT-digest at cni.org>,
  and the 'From' address of your reply is <r-help at stat.math.ethz.ch>.

If you do not want to confirm the requested operation, simply do nothing

All requests about this mailing list
  should be sent to <CNI-COPYRIGHT-request at cni.org>


From Bill.Venables at csiro.au  Tue Aug  1 07:16:05 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 1 Aug 2006 15:16:05 +1000
Subject: [R] Fitting models in a loop
Message-ID: <B998A44C8986644EA8029CFE6396A9245475BD@exqld2-bne.qld.csiro.au>

Murray,

Here is a general paradigm I tend to use for such problems.  It extends
to fairly general model sequences, including different responses, &c

First a couple of tiny, tricky but useful functions:

subst <- function(Command, ...) do.call("substitute", list(Command,
list(...)))

abut <- function(...)  ## jam things tightly together
  do.call("paste", c(lapply(list(...), as.character), sep = "")) 

Name <- function(...) as.name(do.call("abut", list(...)))

Now the gist.

fitCommand <- quote({
	MODELi <- lm(y ~ poly(x, degree = i), theData)
	print(summary(MODELi))
})
for(i in 1:6) {
	thisCommand <- subst(fitCommand, MODELi = Name("model_", i), i =
i)
	print(thisCommand)  ## only as a check
	eval(thisCommand)
}

At this point you should have the results and

objects(pat = "^model_")

should list the fitted model objects, all of which can be updated,
summarised, plotted, &c, because the information on their construction
is all embedded in the call.

Bill.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray Jorgensen
Sent: Tuesday, 1 August 2006 2:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Fitting models in a loop

If I want to display a few polynomial regression fits I can do something

like

for (i in 1:6) {
	mod <- lm(y ~ poly(x,i))
	print(summary(mod))
	}

Suppose that I don't want to over-write the fitted model objects, 
though. How do I create a list of blank fitted model objects for later 
use in a loop?

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From andza at osi.lv  Tue Aug  1 09:15:50 2006
From: andza at osi.lv (Andris Jankevics)
Date: Tue, 1 Aug 2006 10:15:50 +0300
Subject: [R] question about dataframe ("sensory") in PLS package
In-Reply-To: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
Message-ID: <200608011015.50608.andza@osi.lv>

Hello, I do this in such way:

DATAX <- matrix(seq(1,6,1),2,3)
DATAY <- matrix (seq(1,4,1),2,2)
rownames(DATAX) <- c("s1","s2")
rownames(DATAY) <- c("s1","s2")
colnames (DATAX) <- c("v1","v2","v3")
colnames (DATAY) <- c("respone_1","response_2")
KAL <- data.frame (N = rownames(DATAX))
KAL$Y <- DATAY
KAL$X <- DATAX
KAL$X
KAL$Y	

DATAX is a matrix of testing data, but DATAY is a matrix of responses.


Andris Jankevics

On Pirmdiena, 31. J?lijs 2006 05:45, jz7 at duke.edu wrote:
> Dear all,
>
> I am trying to my dataframe for the PLS analysis using the PLS package.
> However I have some trouble generating the correct dataframe. The main
> problem is how to use one name to represent several columns in the
> dataframe.
>
> The example dataframe in PLS package is called "sensory". I cannot
> directly read the data file since it's a binary file. If I use
> "names(sensory)" command, I will get two names: "Quality" and "Panel". But
> if I use "summary(sensory)" command, I will get information of five
> columns for "Quality" and 6 columns for "Panel" (such as "Quality.Acidity"
> "Quality.Peroxide"...). So when I use PLS regression, the function is
> simply "Panel ~ Quality" (but it's actually multiple regression).
>
> Does anyone know how to build such dataframe? Please share some
> experience. Really appreciate the help!
>
> Sincerely,
> Jeny
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From Jan.Wijffels at ucs.kuleuven.be  Tue Aug  1 09:43:50 2006
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Tue, 1 Aug 2006 09:43:50 +0200
Subject: [R] R crashes using pdf() windows() or postscript()
Message-ID: <005701c6b53e$3acd7120$2c70210a@UCSPC32>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/9fd6363d/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Aug  1 10:10:18 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Aug 2006 10:10:18 +0200
Subject: [R] rgb and col2rgb color conversion/modification/shading
In-Reply-To: <1154392412.44cea15c0c1fa@webmail.fhcrc.org>
References: <1154392412.44cea15c0c1fa@webmail.fhcrc.org>
Message-ID: <44CF0C6A.7040303@statistik.uni-dortmund.de>

ccarey at fhcrc.org wrote:
> I want to get a lighter shade of a color...I have a lot of colored objects and
> want each one printed as a foreground against a slightly lighter background.
> 
> I thought I could try something like changing the alpha channel by first
> converting it to rgb. 
> 
> But prior to trying that, I'm stuck with how to get the color after converting
> using col2rgb() to be interpreted again as a color, rather than a simple
> vector?
> 
> Anyone have any help/ or alternative suggestion...
> 
> Thanks, -c
> ----------------------
> TRYING WITH A SINGLE COLOR:
> 
> mycol<-"red"
> 
>> col2rgb(mycol)
>       [,1]
> red    255
> green    0
> blue     0
> 
>> rgb(col2rgb(mycol),maxColorValue=255)


rgb() required separate arguments for red green and blue.

Hence saying

   mycol2 <- col2rgb(mycol)
   rgb(mycol2[1,], mycol2[2,], mycol2[3,], maxColorValue=255)

does not look like beautiful code, but is probably easier than the other 
"cool" tricks.

Uwe Ligges



> Error in rgb(col2rgb("red")) : argument "green" is missing, with no default
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Tue Aug  1 10:17:40 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Aug 2006 10:17:40 +0200
Subject: [R] resampling mean distances
In-Reply-To: <01421F01-E8EB-45C7-BBB9-EAA68BCE8693@cornell.edu>
References: <000601c6b4c7$c2fd4e70$6500a8c0@mynewbox>	<44CE4379.5080204@pdf.com>	<Pine.LNX.4.64.0607312033580.28171@gannet.stats.ox.ac.uk>
	<01421F01-E8EB-45C7-BBB9-EAA68BCE8693@cornell.edu>
Message-ID: <44CF0E24.2040900@statistik.uni-dortmund.de>

Jose Andres wrote:
>   Hi all,
> 
> I am trying to generate a distribution for the mean euclidean  
> distance between a group of n elements in a given surface (the  
> elements are randomly picked).  Fo doing so I've written the  
> following code:
> 
> sampling<- function(x,size) {
> 
> x<- x[sample(1:nrow(x),size),]
> 
> mat<- matrix(c(x$V6,x$V7,x$V8), ncol=3)
> 
> mean.dist<- mean(dist(mat,"euclidean"))

# insert some return value such as:
return(mean.dist)

> }



Now replicate() the stuff, e.g.:

    replicate(10000, sampling(x, 10))

Uwe Ligges



> x is the file where the data are stored
> size is the size of the group
> mat generates a simple matrix. V6, V7, and V8 are the 3D (x,y,z)  
> coordinates of the group elements .
> mean.dist  calculates the mean pairwise distance between the objects  
> of the group.
> 
> Everything works fine but I want  to repeat this many times (e.g.  
> 10000) and  store the mean.dist values in a new variable so I can   
> generate the distribution of mean pairwise distances of a group of  
> size n in my surface.
> 
> Is there any easy way to do this? I'd really appreciate all your  
> comments.
> 
> Thanks in advance,
> 
> /Jose
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> On Jul 31, 2006, at 15:35, Prof Brian Ripley wrote:
> 
>> On Mon, 31 Jul 2006, Sundar Dorai-Raj wrote:
>>
>>>
>>> LL wrote:
>>>> Hi.. I am running R version 2.3.1 on a Windows XP machine with  
>>>> the latest Miktex 2.5 installed. I get no errors from R when  
>>>> running the Sweave example,
>>>>
>>>> testfile <- system.file("Sweave", "Sweave-test-1.Rnw", package =  
>>>> "utils")
>>>>
>>>> However, when I tex the resulting .tex file (after installing  
>>>> a4.sty) I get the error below.
>>>>
>>>> This is pdfeTeX, Version 3.141592-1.30.6-2.2 (MiKTeX 2.5 RC 1)
>>>> entering extended mode
>>>> (Sweave-test-1.tex
>>>> LaTeX2e <2005/12/01>
>>>> Babel <v3.8g> and hyphenation patterns for english, dumylang,  
>>>> nohyphenation, ge
>>>> rman, ngerman, french, loaded.
>>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\base\article.cls"
>>>> Document Class: article 2005/09/16 v1.4f Standard LaTeX document  
>>>> class
>>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\base\size10.clo"))
>>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\ltxmisc\a4wide.sty"
>>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\ntgclass\a4.sty"))
>>>> ! Missing \endcsname inserted.
>>>> <to be read again>
>>>>                    \protect
>>>> l.11 \begin
>>>>            {document}
>>>> ?
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> This works for me. However, when I ran this, MiKTeX prompted me to
>>> install the ntgclass package, which I did. Everything ran smoothly  
>>> after
>>> that. I'm using R-2.3.1 with MiKTeX 2.4 in WinXP Pro.
>> But he is using MiKTeX 2.5: looks like a problem with MiKTeX, as  
>> the latex
>> error is in the initial processing.
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Tue Aug  1 10:19:00 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Aug 2006 10:19:00 +0200
Subject: [R] Fw:  na.rm problem
In-Reply-To: <20060731203645.M66654@deepfoo.com>
References: <20060731202426.M69111@deepfoo.com>
	<20060731203645.M66654@deepfoo.com>
Message-ID: <44CF0E74.5010108@statistik.uni-dortmund.de>

sonal at deepfoo.com wrote:
> hi,
> 
> i am a new member.
> 
> i am using R in finding correlation between two variables of unequal length.
> 
> when i use
> 
> cor(x,y,na.rm=T,use="complete")
> 
> where x has observations from 1928 to 2006 & y has observations from 1950 to
> 2006. I used na.rm=T to use the "complete observations".  So missing values
> should be handled by casewise deletion. But it gives me error
> 
> Error in cor(close, close1, na.rm = T, use = "complete") : 
>         unused argument(s) (na.rm ...)
> 
> Please help me with this as I am new to R.


See ?cor. It does not have an na.rm argument. Just use:

   cor(x, y, use = "complete")

Uwe Ligges


> Thanks,
> Sonal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rdiaz at cnio.es  Tue Aug  1 10:32:18 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Tue, 1 Aug 2006 10:32:18 +0200
Subject: [R] memory problems when combining randomForests
In-Reply-To: <1154364337.44ce33b17dced@webmail.cryst.bbk.ac.uk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>
	<cdf817830607310838lce85b1dy6107884b7f25561e@mail.gmail.com>
	<1154364337.44ce33b17dced@webmail.cryst.bbk.ac.uk>
Message-ID: <200608011032.18364.rdiaz@cnio.es>

Dear Eleni,

>
> But if every time you remove a variable you pass some test data (ie data
> not used to train the model) and base the performance of the new, reduced
> model on the error rate on the confusion matrix for the test data, then
> this "overfitting" should not be an issue, right?  (unless of course you
> were referring to unsupervised learning).
>


Yes and no. The problem there could arise if you do this iteratively and use 
the minimum value you obtain with your procedure to return an estimate of the 
error rate. In such a case, you should, instead, do a double cross-validation 
or bootstrap (i.e., estimate, via cross-validation ---or the bootstrap--- the 
error rate of your complete procedure).

Both Andy and collaborators on the one hand and myself on the other have done 
some further work on these issues.

Svetnik V, Liaw A, Tong C, Wang T: Application of Breiman's random forest to 
modeling structure-activity relationships of pharmaceutical molecules.
Multiple Classier Systems, Fifth International Workshop, MCS 2004, 
Proceedings, 9???11 June 2004, Cagliari, Italy. Lecture Notes in Computer 
Science, Springer 2004, 3077:334-343.

Gene selection and classification of microarray data using random forest
Ram??n D??az-Uriarte and Sara Alvarez de Andr??s. BMC Bioinformatics 2006, 7:3. 
http://www.biomedcentral.com/1471-2105/7/3


Best,

R.



On Monday 31 July 2006 18:45, Eleni Rapsomaniki wrote:
> Hi Andy,
>
> > > I get different order of importance for my variables depending on their
>
> order in the training data.
>
> Perhaps answering my own question, the change in importance rankings could
> be attributed to the fact that before passing my data to randomForest I
> impute the missing values randomly (using the combined distributions of
> pos+neg), so the data seen by RF is slightly different. Then combining this
> with the fact that RF chooses data randomly it makes sense to see different
> rankings.
>
> In a previous thread regarding simplifying variables:
> http://thread.gmane.org/gmane.comp.lang.r.general/6989/focus=6993
>
> you say:
> "The basic problem is that when you select important variables by RF and
> then re-run RF with those variables, the OOB error rate become biased
> downward. As you iterate more times, the "overfitting" becomes more and
> more severe (in the sense that, the OOB error rate will keep decreasing
> while error rate on an independent test set will be flat or increases)"
>
> But if every time you remove a variable you pass some test data (ie data
> not used to train the model) and base the performance of the new, reduced
> model on the error rate on the confusion matrix for the test data, then
> this "overfitting" should not be an issue, right?  (unless of course you
> were referring to unsupervised learning).
>
> Best regards
> Eleni Rapsomaniki
> Birkbeck College, UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Ram??n D??az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From Gregor.Gorjanc at bfro.uni-lj.si  Tue Aug  1 11:33:48 2006
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 1 Aug 2006 11:33:48 +0200
Subject: [R] Global setting for na.rm=TRUE
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31C03@pollux.bfro.uni-lj.si>

Hello!

Is it possible to set na.rm=TRUE in a global way? I'am constantly
forgeting on this when performing analyses. I agree that one should
be carefull with this when developing some code, but not necesarilly
so in data analysis.

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From Markus.Gesmann at lloyds.com  Tue Aug  1 11:58:42 2006
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Tue, 1 Aug 2006 10:58:42 +0100
Subject: [R] Fitting models in a loop
Message-ID: <C3E3A3D81F4E0F438118DAA9722F12A9011963C2@LNVCNTEXCH01.corp.lloydsnet>

Murray,

How about creating an empty list and filling it during your loop:

 mod <- list()
 for (i in 1:6) {
	mod[[i]] <- lm(y ~ poly(x,i))
	print(summary(mod[[i]]))
	}

All your models are than stored in one object and you can use lapply to
do something on them, like:
 lapply(mod, summary) or lapply(mod, coef)


Kind Regards

Markus Gesmann
FPMA
Lloyd's Market Analysis
Lloyd's * One Lime Street * London * EC3M 7HA
Telephone +44 (0)20 7327 6472
Facsimile +44 (0)20 7327 5718
http://www.lloyds.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Bill.Venables at csiro.au
Sent: 01 August 2006 06:16
To: maj at waikato.ac.nz; r-help at stat.math.ethz.ch
Subject: Re: [R] Fitting models in a loop


Murray,

Here is a general paradigm I tend to use for such problems.  It extends
to fairly general model sequences, including different responses, &c

First a couple of tiny, tricky but useful functions:

subst <- function(Command, ...) do.call("substitute", list(Command,
list(...)))

abut <- function(...)  ## jam things tightly together
  do.call("paste", c(lapply(list(...), as.character), sep = "")) 

Name <- function(...) as.name(do.call("abut", list(...)))

Now the gist.

fitCommand <- quote({
	MODELi <- lm(y ~ poly(x, degree = i), theData)
	print(summary(MODELi))
})
for(i in 1:6) {
	thisCommand <- subst(fitCommand, MODELi = Name("model_", i), i =
i)
	print(thisCommand)  ## only as a check
	eval(thisCommand)
}

At this point you should have the results and

objects(pat = "^model_")

should list the fitted model objects, all of which can be updated,
summarised, plotted, &c, because the information on their construction
is all embedded in the call.

Bill.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray Jorgensen
Sent: Tuesday, 1 August 2006 2:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Fitting models in a loop

If I want to display a few polynomial regression fits I can do something

like

for (i in 1:6) {
	mod <- lm(y ~ poly(x,i))
	print(summary(mod))
	}

Suppose that I don't want to over-write the fitted model objects, 
though. How do I create a list of blank fitted model objects for later 
use in a loop?

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
**********************************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}


From p.dalgaard at biostat.ku.dk  Tue Aug  1 12:07:11 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Aug 2006 12:07:11 +0200
Subject: [R] Fitting models in a loop
In-Reply-To: <C3E3A3D81F4E0F438118DAA9722F12A9011963C2@LNVCNTEXCH01.corp.lloydsnet>
References: <C3E3A3D81F4E0F438118DAA9722F12A9011963C2@LNVCNTEXCH01.corp.lloydsnet>
Message-ID: <x2irlc93cg.fsf@viggo.kubism.ku.dk>

"Gesmann, Markus" <Markus.Gesmann at lloyds.com> writes:

> Murray,
> 
> How about creating an empty list and filling it during your loop:
> 
>  mod <- list()
>  for (i in 1:6) {
> 	mod[[i]] <- lm(y ~ poly(x,i))
> 	print(summary(mod[[i]]))
> 	}
> 
> All your models are than stored in one object and you can use lapply to
> do something on them, like:
>  lapply(mod, summary) or lapply(mod, coef)

Ouch. Make that 

mod <- vector("list",6)

Otherwise you'll be extending the vector on every pass through the
loop. 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gregor.gorjanc at bfro.uni-lj.si  Tue Aug  1 12:20:02 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 1 Aug 2006 10:20:02 +0000 (UTC)
Subject: [R] if function and  apply
References: <20060731215428.15996.qmail@web33811.mail.mud.yahoo.com>
Message-ID: <loom.20060801T121746-139@post.gmane.org>

John Kane <jrkrideau <at> yahoo.ca> writes:
> I have a dataset just imported from SPSS.  It has any
> number of 99's as missing data and it looks like the
> next dataset will have custom missing codes. I have
> abouat 120 variables and an N of 2000.
> 
...
> 
> #  define function
> fn <- function (x a) {

you need comma between x and a i.e. (x, a)
                                      ^

> if (x==a)return  (b) else x
> }

How can you return b, if b was not defined before

Gregor


From uhkeller at web.de  Tue Aug  1 13:02:16 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Tue, 01 Aug 2006 13:02:16 +0200
Subject: [R] read.spss  'error reading system-file header'
In-Reply-To: <44CE57B3.9090901@fs-analyse.dk>
References: <44CE57B3.9090901@fs-analyse.dk>
Message-ID: <44CF34B8.7000000@web.de>

Question 2: Try saving the data as an SPSS portable file. I never had 
trouble reading these in R.

Finn Sand? wrote:
> When I try to import an spss sav file with read.spss() I am getting the 
> following error
> 'Error in read.spss("X:\\xxxx.sav") : error reading system-file header' 
> and the import process is aborted.
> I have tried in v. 2.3.0 and 2.3.1
> The sav-file loads without problems in spss v14 I have tried saving in 
> older spss v7 but are getting the same result.
> The read.spss() has other errors (the 'Unrecognized record type 7, 
> subtype 7 encountered in system file') but it does not seem to have any 
> impact.
> This leads me to thinking that the spss.read() slowly is growing out of 
> date which would be sad.
> So question 1:
> Does anyone know if these problems are going to be solved? I know the 
> read.spss() function is build on the PSPP project so maybe it takes 
> someone with c-knowledge to do something about it.
> If someone is going to work on the problem I will be happy to help by 
> testing and providing problematic test-files.
> Question 2
> Is there some way to import spss-sav files in this case other than save 
> in a non-spss format?
> Regards
> FS
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From uhkeller at web.de  Tue Aug  1 13:38:38 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Tue, 01 Aug 2006 13:38:38 +0200
Subject: [R] na.rm problem
In-Reply-To: <20060731202426.M69111@deepfoo.com>
References: <20060731202426.M69111@deepfoo.com>
Message-ID: <44CF3D3E.6060309@web.de>

Like the error message tells you, cor does not have an argument "na.rm". 
use="complete" already does what you want, namely casewise deletion of 
missing values.

However, this will not work with vectors of unequal length (how is R to 
determine which observations in x correspond to those in y?). What you 
could do is create a copy of y that has the same length as x but is 
"padded" with NAs (missing values) using something like:

y.NA<-c(rep(NA, length(x) - length(y)), y)

Then you can compute the correlation:

cor(x, y.NA, use="complete")

sonal at deepfoo.com wrote:
> hi,
>
> i am a new member.
>
> i am using R in finding correlation between two variables of unequal length.
>
> when i use 
>
> cor(x,y,na.rm=T,use="complete")
>
> where x has observations from 1928 to 2006 & y has observations from 1950 to
> 2006. I used na.rm=T to use the "complete observations".  So missing values
> should be handled by casewise deletion. But it gives me error
>
> Error in cov(close, close1, na.rm = T, use = "complete") : 
>         unused argument(s) (na.rm ...)
>
>
> Please help me with this as I am new to R.
>
> Thanks,
> Sonal
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From petr.pikal at precheza.cz  Tue Aug  1 13:51:00 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 01 Aug 2006 13:51:00 +0200
Subject: [R] Algebraic operation on the missing values
In-Reply-To: <20060731123733.31125.qmail@web50213.mail.yahoo.com>
References: <44CE04B6.19223.1295639@localhost>
Message-ID: <44CF5C44.18102.1495786@localhost>

Hi

try to post some reproducible example and maybe somebody can help you 
more. You need to handle missing values one way or another. Either 
you can supply a value for it or you can get rid of them. If you used 
it in computation, the result will be NA. At least I think so.

HTH
Petr




On 31 Jul 2006 at 14:37, Joanna Procelewska wrote:

Date sent:      	Mon, 31 Jul 2006 14:37:33 +0200 (CEST)
From:           	Joanna Procelewska <jprocelewska at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Algebraic operation on the missing values

> Thanks for the answer.
> 
> The problem is I have to perform a forward selection on the set and in
> every step construct an orthonormal base for the subspace spanned on
> the selected vectors. This means that I can use only the "full"
> vectors for the constructing a base, or? 
> 
> Joanna
> 
> --- Petr Pikal <petr.pikal at precheza.cz> schrieb:
> 
> > Hi
> > 
> > see
> > ?complete.cases and/or ?is.na for evaluating non missing entries. 
> > 
> > However in any operation in which you use NA value, result shall be
> > NA as you do not know what actually is NA.
> > 
> > HTH
> > Petr
> > 
> > > Hi all,
> > > 
> > > I have a large set of descriptors, which are stored as the
> > > vectors, each one containing about 450 elements. Now I have to
> > > perform some algebraical operations on this set to eliminate the
> > > redundant ones. The problem is, that not all vales in the vectors
> > > are known. Are there any norm defined how should I process such
> > > vectors? Simple example: having two vectors:
> > > 
> > > a      b
> > > 3      4
> > > 2      null
> > > 3      6
> > > 
> > > I can imagine that a+b is [7 null 9]', but what about scalar
> > > product? Is it null or have it a value? I don't want to replace
> > > missing values with the concrete ones, but they significantly
> > > complicate my computations. 
> > > 
> > > Does anyone know whether there are any ways to solve this problem?
> > > Please share some experience. Really appreciate the help!
> > > 
> > > Sincerely,
> > > 
> > > Joanna
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From lanre.okusanya at gmail.com  Tue Aug  1 14:09:38 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Tue, 1 Aug 2006 08:09:38 -0400
Subject: [R] Overlay Boxplot with scatter plot
Message-ID: <6e25bb420608010509k361d0ed0wa0b4f745baa3e362@mail.gmail.com>

I am trying to make a box plot and overlay it with a scatter plot from
another data.frame. I was able to successfully create the boxplot, but
when i tried using points(x~y...) the dots did not show up.

example code

aa<-(300,300,300,300,600,600,600,600,900,900,900,900)
bb<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
cc<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
nn<-data.frame(aa,bb)
mm<-data.frame(aa,cc)
boxplot(bb~aa, data=nn)
lines(cc~aa, data=cc)

Any help with example code is appreciated.

Thank you.

Lanre


From petr.pikal at precheza.cz  Tue Aug  1 14:33:42 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 01 Aug 2006 14:33:42 +0200
Subject: [R] Overlay Boxplot with scatter plot
In-Reply-To: <6e25bb420608010509k361d0ed0wa0b4f745baa3e362@mail.gmail.com>
Message-ID: <44CF6646.13251.1706FA4@localhost>

Hi

your example code is not reproducible.

On 1 Aug 2006 at 8:09, Lanre Okusanya wrote:

Date sent:      	Tue, 1 Aug 2006 08:09:38 -0400
From:           	"Lanre Okusanya" <lanre.okusanya at gmail.com>
To:             	R-help at stat.math.ethz.ch
Subject:        	[R] Overlay Boxplot with scatter plot

> I am trying to make a box plot and overlay it with a scatter plot from
> another data.frame. I was able to successfully create the boxplot, but
> when i tried using points(x~y...) the dots did not show up.
> 
> example code
> 
> aa<-(300,300,300,300,600,600,600,600,900,900,900,900)
         ^^^ missing c
> bb<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> cc<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> nn<-data.frame(aa,bb)
different length of aa and bb

> mm<-data.frame(aa,cc)
> boxplot(bb~aa, data=nn)
> lines(cc~aa, data=cc)
> 
> Any help with example code is appreciated.

x<-rnorm(20)
y<-rep(c(10,20),each=10) 
bbb<-boxplot(x~as.factor(y))

although it seems as boxes are drawn at 10, 20 they are not.
Actually the x position of each box is located at 

- from help page -
at 		numeric vector giving the locations where the boxplots should be 
drawn, particularly when add = TRUE; defaults to 1:n where n is the 
number of boxes. 

as you can clearly demonstrate by issuing

points(1,0, cex=10)
 
HTH
Petr

> 
> Thank you.
> 
> Lanre
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From lanre.okusanya at gmail.com  Tue Aug  1 14:51:50 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Tue, 1 Aug 2006 08:51:50 -0400
Subject: [R] Overlay Boxplot with scatter plot
In-Reply-To: <44CF6646.13251.1706FA4@localhost>
References: <6e25bb420608010509k361d0ed0wa0b4f745baa3e362@mail.gmail.com>
	<44CF6646.13251.1706FA4@localhost>
Message-ID: <6e25bb420608010551y10d36874k98252de5fa586723@mail.gmail.com>

my apologies about the initial bad sample code. Now assume I have

a<-rnorm(6)
b<-rep(c(10,20),each=3)
ab<-data.frame(a,b)

and I wanted to overlay this points on the graph?

On 8/1/06, Petr Pikal <petr.pikal at precheza.cz> wrote:
> Hi
>
> your example code is not reproducible.
>
> On 1 Aug 2006 at 8:09, Lanre Okusanya wrote:
>
> Date sent:              Tue, 1 Aug 2006 08:09:38 -0400
> From:                   "Lanre Okusanya" <lanre.okusanya at gmail.com>
> To:                     R-help at stat.math.ethz.ch
> Subject:                [R] Overlay Boxplot with scatter plot
>
> > I am trying to make a box plot and overlay it with a scatter plot from
> > another data.frame. I was able to successfully create the boxplot, but
> > when i tried using points(x~y...) the dots did not show up.
> >
> > example code
> >
> > aa<-(300,300,300,300,600,600,600,600,900,900,900,900)
>          ^^^ missing c
> > bb<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> > cc<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> > nn<-data.frame(aa,bb)
> different length of aa and bb
>
> > mm<-data.frame(aa,cc)
> > boxplot(bb~aa, data=nn)
> > lines(cc~aa, data=cc)
> >
> > Any help with example code is appreciated.
>
> x<-rnorm(20)
> y<-rep(c(10,20),each=10)
> bbb<-boxplot(x~as.factor(y))
>
> although it seems as boxes are drawn at 10, 20 they are not.
> Actually the x position of each box is located at
>
> - from help page -
> at              numeric vector giving the locations where the boxplots should be
> drawn, particularly when add = TRUE; defaults to 1:n where n is the
> number of boxes.
>
> as you can clearly demonstrate by issuing
>
> points(1,0, cex=10)
>
> HTH
> Petr
>
> >
> > Thank you.
> >
> > Lanre
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>


From joris.dewolf at cropdesign.com  Tue Aug  1 14:53:34 2006
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Tue, 01 Aug 2006 14:53:34 +0200
Subject: [R] Overlay Boxplot with scatter plot
In-Reply-To: <6e25bb420608010509k361d0ed0wa0b4f745baa3e362@mail.gmail.com>
References: <6e25bb420608010509k361d0ed0wa0b4f745baa3e362@mail.gmail.com>
Message-ID: <44CF4ECE.9050903@cropdesign.com>

boxplot does not use the actual values of aa as x-value, but considers 
aa as a factor having three levels.

Try

bxp <- boxplot(bb~aa, data=nn)
bxp

to see what the boxplot actually is.
For your lines you should also use three levels.

aa2<-c(1,1,1,1,2,2,2,2,3,3,3,3,3,3,3)
points(cc~aa2)

(after you have been correcting your example code...)
Joris


Lanre Okusanya wrote:
> I am trying to make a box plot and overlay it with a scatter plot from
> another data.frame. I was able to successfully create the boxplot, but
> when i tried using points(x~y...) the dots did not show up.
> 
> example code
> 
> aa<-(300,300,300,300,600,600,600,600,900,900,900,900)
> bb<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> cc<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> nn<-data.frame(aa,bb)
> mm<-data.frame(aa,cc)
> boxplot(bb~aa, data=nn)
> lines(cc~aa, data=cc)
> 
> Any help with example code is appreciated.
> 
> Thank you.
> 
> Lanre
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}


From chonghuitan at smu.edu.sg  Tue Aug  1 15:06:34 2006
From: chonghuitan at smu.edu.sg (TAN Chong Hui)
Date: Tue, 1 Aug 2006 21:06:34 +0800
Subject: [R] A problem with R CMD SHLIB
Message-ID: <FA3090E732DC6A4EB8E2D875EEB486A508CBD7@EX01.staff.smu.edu.sg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/eb9db95e/attachment.pl 

From ggrothendieck at gmail.com  Tue Aug  1 15:38:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Aug 2006 09:38:54 -0400
Subject: [R] Global setting for na.rm=TRUE
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31C03@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31C03@pollux.bfro.uni-lj.si>
Message-ID: <971536df0608010638k3cbd1e43t8b064befc4258b6d@mail.gmail.com>

There is a global option setting for na.action.  See ?na.action .
That does not completely address your question but might
help with lm, glm, etc.

You could define your own wrapper functions if you know ahead of time
which functions with na.rm= args you need. e.g.

my.max = function(..., na.rm = getOption("na.rm")) max(..., na.rm = na.rm)
getOption("na.rm") # NULL
my.max(1, 2, NA) # 2
options(na.rm = FALSE)
my.max(1,2,NA) # NA

On 8/1/06, Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si> wrote:
> Hello!
>
> Is it possible to set na.rm=TRUE in a global way? I'am constantly
> forgeting on this when performing analyses. I agree that one should
> be carefull with this when developing some code, but not necesarilly
> so in data analysis.
>
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Tue Aug  1 15:40:41 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 01 Aug 2006 09:40:41 -0400
Subject: [R] A problem with R CMD SHLIB
In-Reply-To: <FA3090E732DC6A4EB8E2D875EEB486A508CBD7@EX01.staff.smu.edu.sg>
References: <FA3090E732DC6A4EB8E2D875EEB486A508CBD7@EX01.staff.smu.edu.sg>
Message-ID: <44CF59D9.5040006@stats.uwo.ca>

On 8/1/2006 9:06 AM, TAN Chong Hui wrote:
> Hi,
> 
> I followed the example in "Writing R Extensions" to create a shared
> object in Windows, using the command
> 
> R CMD SHLIB X.cc X_main.cc
> 
> This was encountered: 
> 
> ../src/gnuwin32/MkRules:155: warning: overriding commands for target
> '.c.d'
> ../src/gnuwin32/MkRules:143: warning: ignoring old commands for target
> '.c.d'
> ../src/gnuwin32/MkRules:171: warning: overriding commands for target
> '.c.d'
> ../src/gnuwin32/MkRules:159: warning: ignoring old commands for target
> '.c.d'
> make: *** No rule to make target 'X_main.cc''.  Stop.
> 
> What might be the problem?
> Please advise, anyone?

I think you need to tell us more. Have you got a Makefile or Makevars in 
the same directory?  If so, they are probably conflicting with the 
default ones.

If  X.cc and X_main.cc are self-contained, then you won't need Makevars 
or Makefile.  R will use its own.

Duncan Murdoch

> 
> Rgds
> Chong Hui 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Tue Aug  1 15:55:55 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 1 Aug 2006 14:55:55 +0100
Subject: [R] Overlay Boxplot with scatter plot
In-Reply-To: <6e25bb420608010551y10d36874k98252de5fa586723@mail.gmail.com>
References: <6e25bb420608010509k361d0ed0wa0b4f745baa3e362@mail.gmail.com>
	<44CF6646.13251.1706FA4@localhost>
	<6e25bb420608010551y10d36874k98252de5fa586723@mail.gmail.com>
Message-ID: <f8e6ff050608010655r2c116e8ct7b183297ca40f120@mail.gmail.com>

This is very easy to do with ggplot:

# you need to get the development version from http://had.co.nz/ggplot:
install.packages("ggplot", dep=TRUE, repos="http://www.ggobi.org/r/")
library(ggplot)

qplot(a, factor(b), type=c("boxplot","point"))
qplot(factor(b), a, type=c("boxplot","point"))

Regards,

Hadley

On 8/1/06, Lanre Okusanya <lanre.okusanya at gmail.com> wrote:
> my apologies about the initial bad sample code. Now assume I have
>
> a<-rnorm(6)
> b<-rep(c(10,20),each=3)
> ab<-data.frame(a,b)
>
> and I wanted to overlay this points on the graph?
>
> On 8/1/06, Petr Pikal <petr.pikal at precheza.cz> wrote:
> > Hi
> >
> > your example code is not reproducible.
> >
> > On 1 Aug 2006 at 8:09, Lanre Okusanya wrote:
> >
> > Date sent:              Tue, 1 Aug 2006 08:09:38 -0400
> > From:                   "Lanre Okusanya" <lanre.okusanya at gmail.com>
> > To:                     R-help at stat.math.ethz.ch
> > Subject:                [R] Overlay Boxplot with scatter plot
> >
> > > I am trying to make a box plot and overlay it with a scatter plot from
> > > another data.frame. I was able to successfully create the boxplot, but
> > > when i tried using points(x~y...) the dots did not show up.
> > >
> > > example code
> > >
> > > aa<-(300,300,300,300,600,600,600,600,900,900,900,900)
> >          ^^^ missing c
> > > bb<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> > > cc<-(13,12,14,11,56,44,34,75,22.,34,22,98,59,55,56)
> > > nn<-data.frame(aa,bb)
> > different length of aa and bb
> >
> > > mm<-data.frame(aa,cc)
> > > boxplot(bb~aa, data=nn)
> > > lines(cc~aa, data=cc)
> > >
> > > Any help with example code is appreciated.
> >
> > x<-rnorm(20)
> > y<-rep(c(10,20),each=10)
> > bbb<-boxplot(x~as.factor(y))
> >
> > although it seems as boxes are drawn at 10, 20 they are not.
> > Actually the x position of each box is located at
> >
> > - from help page -
> > at              numeric vector giving the locations where the boxplots should be
> > drawn, particularly when add = TRUE; defaults to 1:n where n is the
> > number of boxes.
> >
> > as you can clearly demonstrate by issuing
> >
> > points(1,0, cex=10)
> >
> > HTH
> > Petr
> >
> > >
> > > Thank you.
> > >
> > > Lanre
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kartik.pappu at gmail.com  Tue Aug  1 16:03:52 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Tue, 1 Aug 2006 07:03:52 -0700
Subject: [R] Extracting a row number from a matrix
Message-ID: <97faa3210608010703g179ca981l3d78f74069760933@mail.gmail.com>

Hi all,

I have a matrix with each column containing a large number of integers
(0 and above). in each column beyond a certain row (say row 120 in
column 1, row 134 in column 2, 142 in column 3...)  there are only
0's.  I want to find, for each column the row number of the last row
which contains a positive integer beyond which there are 10 or more
0's.

so in the following example (single column, but my real data has
multiple columns) how do I  get the row number of the last row of x
beyond which there are 10 or more 0's (which in this case is row#100).

x <- as.matrix(c(rep(seq(1:20),5),rep(0,20)))

I am still new to R so I was wondering if anyone had a quick fix.

Thanks
Kartik


From JeeBee at troefpunt.nl  Tue Aug  1 16:13:37 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Tue, 01 Aug 2006 16:13:37 +0200
Subject: [R] Extracting a row number from a matrix
References: <97faa3210608010703g179ca981l3d78f74069760933@mail.gmail.com>
Message-ID: <pan.2006.08.01.14.13.37.77264@troefpunt.nl>

> so in the following example (single column, but my real data has
> multiple columns) how do I  get the row number of the last row of x
> beyond which there are 10 or more 0's (which in this case is row#100).
> 
> x <- as.matrix(c(rep(seq(1:20),5),rep(0,20)))
> 

max( which(x > 0) )

JeeBee.


From lothar.schmid at googlemail.com  Tue Aug  1 16:23:46 2006
From: lothar.schmid at googlemail.com (Lothar Schmid)
Date: Tue, 1 Aug 2006 16:23:46 +0200
Subject: [R] How to convert two-dimensional function to matrix?
Message-ID: <eaa9854d0608010723k5826a58bkbea9a77192ff8aa1@mail.gmail.com>

I'd like to convert a two-dimensional function f(x,y) to a matrix m,
so that m[x,y] = f [x,y]. How can I achieve this?

Thanks,

Lothar


From epistat at gmail.com  Tue Aug  1 16:33:55 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 1 Aug 2006 22:33:55 +0800
Subject: [R] help on fitting negative binomial distribution with MLE
Message-ID: <2fc17e30608010733m5693115awe383d10afd96df5b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/705e22f0/attachment.pl 

From mschwartz at mn.rr.com  Tue Aug  1 16:42:47 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 01 Aug 2006 09:42:47 -0500
Subject: [R] Extracting a row number from a matrix
In-Reply-To: <97faa3210608010703g179ca981l3d78f74069760933@mail.gmail.com>
References: <97faa3210608010703g179ca981l3d78f74069760933@mail.gmail.com>
Message-ID: <1154443367.4785.4.camel@localhost.localdomain>

On Tue, 2006-08-01 at 07:03 -0700, Kartik Pappu wrote:
> Hi all,
> 
> I have a matrix with each column containing a large number of integers
> (0 and above). in each column beyond a certain row (say row 120 in
> column 1, row 134 in column 2, 142 in column 3...)  there are only
> 0's.  I want to find, for each column the row number of the last row
> which contains a positive integer beyond which there are 10 or more
> 0's.
> 
> so in the following example (single column, but my real data has
> multiple columns) how do I  get the row number of the last row of x
> beyond which there are 10 or more 0's (which in this case is row#100).
> 
> x <- as.matrix(c(rep(seq(1:20),5),rep(0,20)))
> 
> I am still new to R so I was wondering if anyone had a quick fix.
> 
> Thanks
> Kartik

Not fully tested, but something like the following:

x <- as.matrix(c(rep(seq(1:20),5),rep(0,20)))

get.zeros <- function(x)
{
  runs <- rle(x == 0)
  pos <- max(which(runs$values & runs$lengths >= 10))
  sum(runs$lengths[1:(pos - 1)])
}

> apply(x, 2, get.zeros)
[1] 100

See ?rle for getting information about sequences of values in a vector.

HTH,

Marc Schwartz


From JeeBee at troefpunt.nl  Tue Aug  1 16:52:21 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Tue, 01 Aug 2006 16:52:21 +0200
Subject: [R] How to convert two-dimensional function to matrix?
References: <eaa9854d0608010723k5826a58bkbea9a77192ff8aa1@mail.gmail.com>
Message-ID: <pan.2006.08.01.14.52.20.761741@troefpunt.nl>

On Tue, 01 Aug 2006 16:23:46 +0200, Lothar Schmid wrote:

> I'd like to convert a two-dimensional function f(x,y) to a matrix m,
> so that m[x,y] = f [x,y]. How can I achieve this?

Maybe this helps.
I create an empty matrix first, then apply f to the indices.

f <- function(x, y) { 1000*x + y }

( m <- matrix( data = NA, nrow = 5, ncol = 4 ) )
( m <- f(row(m), col(m)) )

JeeBee.


From p.dalgaard at biostat.ku.dk  Tue Aug  1 17:06:55 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Aug 2006 17:06:55 +0200
Subject: [R] How to convert two-dimensional function to matrix?
In-Reply-To: <pan.2006.08.01.14.52.20.761741@troefpunt.nl>
References: <eaa9854d0608010723k5826a58bkbea9a77192ff8aa1@mail.gmail.com>
	<pan.2006.08.01.14.52.20.761741@troefpunt.nl>
Message-ID: <x2y7u8a41c.fsf@viggo.kubism.ku.dk>

JeeBee <JeeBee at troefpunt.nl> writes:

> On Tue, 01 Aug 2006 16:23:46 +0200, Lothar Schmid wrote:
> 
> > I'd like to convert a two-dimensional function f(x,y) to a matrix m,
> > so that m[x,y] = f [x,y]. How can I achieve this?
> 
> Maybe this helps.
> I create an empty matrix first, then apply f to the indices.
> 
> f <- function(x, y) { 1000*x + y }
> 
> ( m <- matrix( data = NA, nrow = 5, ncol = 4 ) )
> ( m <- f(row(m), col(m)) )

outer(1:5, 1:4, f)


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Tue Aug  1 17:20:20 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Aug 2006 16:20:20 +0100 (BST)
Subject: [R] help on fitting negative binomial distribution with MLE
In-Reply-To: <2fc17e30608010733m5693115awe383d10afd96df5b@mail.gmail.com>
References: <2fc17e30608010733m5693115awe383d10afd96df5b@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608011618220.3887@gannet.stats.ox.ac.uk>

This is a special case of

MASS::fitdistr
MASS::glm.nb

and the first will be easiest for you.

On Tue, 1 Aug 2006, zhijie zhang wrote:

> Dear friends,
>   Anybody knows how to  fit the negative binomial distribution with MLE
> using R or other software? I can't find the solution, any suggestions or
> help would be greatly appreciated.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From john_d_mchenry at yahoo.com  Tue Aug  1 17:34:30 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 1 Aug 2006 08:34:30 -0700 (PDT)
Subject: [R] Tcltk package
Message-ID: <20060801153430.5986.qmail@web35404.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/bbe1d700/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Aug  1 17:50:17 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Aug 2006 17:50:17 +0200
Subject: [R] Tcltk package
In-Reply-To: <20060801153430.5986.qmail@web35404.mail.mud.yahoo.com>
References: <20060801153430.5986.qmail@web35404.mail.mud.yahoo.com>
Message-ID: <x2psfka212.fsf@viggo.kubism.ku.dk>

John McHenry <john_d_mchenry at yahoo.com> writes:

> Hi WizaRds,
> 
> I ran into trouble trying to install the "debug" package, which requires TCL/TK support. It seems like the tcltk package is not installed on my system.
> From: http://tolstoy.newcastle.edu.au/R/help/05/07/7993.html it seems that the tcltk is bundled with the base R distribution.
> 
> I'm running R under linux:
> 
> > version
>                _
> platform       i686-pc-linux-gnu
> arch           i686
> os             linux-gnu
> system         i686, linux-gnu
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> 
>  tcl8.4 and tk8.4 are both installed.
>  
>  The messages I get when I try to install the debug package are:
> 
> > install.packages("debug")
> trying URL 'http://cran.us.r-project.org/src/contrib/debug_1.1.0.tar.gz'
> Content type 'application/x-tar' length 26492 bytes
> opened URL
> ==================================================
> downloaded 25Kb
> 
> * Installing *source* package 'debug' ...
> ** R
> ** inst
> ** save image
> Loading required package: mvbutils
> MVBUTILS: no "tasks" vector found in ROOT
> Loading required package: tcltk
> Error in firstlib(which.lib.loc, package) :
>         Tcl/Tk support is not available on this system
> Error: package 'tcltk' could not be loaded
> Execution halted
> ERROR: execution of package source for 'debug' failed
> ** Removing '/usr/local/lib/R/library/debug'
> 
> The downloaded packages are in
>         /tmp/RtmpEocXcC/downloaded_packages
> Warning message:
> installation of package 'debug' had non-zero exit status in: install.packages("debug")
> 
> Where am I going wrong?

Did you build R yourself, and can you do library(tcltk) on R's command
line? You may well be missing the -devel packages for tcl and tk.

And, BTW, which Linux distribution is this? "i686-pc-linux-gnu" is not
sufficient. 


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From sundar.dorai-raj at pdf.com  Tue Aug  1 17:57:03 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Aug 2006 10:57:03 -0500
Subject: [R] deleting a directory
Message-ID: <44CF79CF.50908@pdf.com>

Hi, all,

I'm looking a utility for removing a directory from within R. Currently, 
I'm using:

foo <- function(...) {
   mydir <- tempdir()
   dir.create(mydir, showWarnings = FALSE, recursive = TRUE)
   on.exit(system(sprintf("rm -rf %s", mydir)))
   ## do some stuff in "mydir"
   invisible()
}

However, this is assumes "rm" is available. I know of ?dir.create, but 
there is no opposite. And ?file.remove appears to work only on files and 
not directories.

Any advice? Or is my current approach the only solution?

 > R.version
                _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)


Thanks,

--sundar


From sundar.dorai-raj at pdf.com  Tue Aug  1 17:59:38 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Aug 2006 10:59:38 -0500
Subject: [R] deleting a directory
In-Reply-To: <44CF79CF.50908@pdf.com>
References: <44CF79CF.50908@pdf.com>
Message-ID: <44CF7A6A.3010408@pdf.com>

Please ignore. I forgot ?unlink had a recursive argument.

Thanks.

--sundar

Sundar Dorai-Raj wrote:
> Hi, all,
> 
> I'm looking a utility for removing a directory from within R. Currently, 
> I'm using:
> 
> foo <- function(...) {
>   mydir <- tempdir()
>   dir.create(mydir, showWarnings = FALSE, recursive = TRUE)
>   on.exit(system(sprintf("rm -rf %s", mydir)))
>   ## do some stuff in "mydir"
>   invisible()
> }
> 
> However, this is assumes "rm" is available. I know of ?dir.create, but 
> there is no opposite. And ?file.remove appears to work only on files and 
> not directories.
> 
> Any advice? Or is my current approach the only solution?
> 
>  > R.version
>                _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> Thanks,
> 
> --sundar
> 
>


From ana.pmartins at ine.pt  Tue Aug  1 18:04:29 2006
From: ana.pmartins at ine.pt (Ana Patricia Martins)
Date: Tue, 1 Aug 2006 17:04:29 +0100 
Subject: [R] boxplot
Message-ID: <E97312684A84D511BDD40002A50968D60822CA15@lxpobw01.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/004d8db8/attachment.pl 

From tplate at acm.org  Tue Aug  1 18:23:38 2006
From: tplate at acm.org (Tony Plate)
Date: Tue, 01 Aug 2006 10:23:38 -0600
Subject: [R] deleting a directory
In-Reply-To: <44CF79CF.50908@pdf.com>
References: <44CF79CF.50908@pdf.com>
Message-ID: <44CF800A.7090002@acm.org>

?unlink says that unlink() can remove directories (and has a 'recursive' 
argument).  'unlink' is in the "SEE ALSO" section in ?file.remove.

-- Tony Plate

Sundar Dorai-Raj wrote:
> Hi, all,
> 
> I'm looking a utility for removing a directory from within R. Currently, 
> I'm using:
> 
> foo <- function(...) {
>    mydir <- tempdir()
>    dir.create(mydir, showWarnings = FALSE, recursive = TRUE)
>    on.exit(system(sprintf("rm -rf %s", mydir)))
>    ## do some stuff in "mydir"
>    invisible()
> }
> 
> However, this is assumes "rm" is available. I know of ?dir.create, but 
> there is no opposite. And ?file.remove appears to work only on files and 
> not directories.
> 
> Any advice? Or is my current approach the only solution?
> 
>  > R.version
>                 _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> Thanks,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From john_d_mchenry at yahoo.com  Tue Aug  1 18:24:17 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 1 Aug 2006 09:24:17 -0700 (PDT)
Subject: [R] Tcltk package
In-Reply-To: <x2psfka212.fsf@viggo.kubism.ku.dk>
Message-ID: <20060801162417.29995.qmail@web35412.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/27c214ea/attachment.pl 

From ana.pmartins at ine.pt  Tue Aug  1 18:24:25 2006
From: ana.pmartins at ine.pt (Ana Patricia Martins)
Date: Tue, 1 Aug 2006 17:24:25 +0100 
Subject: [R]  boxplot
Message-ID: <E97312684A84D511BDD40002A50968D60822CA63@lxpobw01.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/1a104d9b/attachment.pl 

From hidorothy1979 at yahoo.com  Tue Aug  1 19:01:54 2006
From: hidorothy1979 at yahoo.com (qian li)
Date: Tue, 1 Aug 2006 10:01:54 -0700 (PDT)
Subject: [R] open DLL in R
Message-ID: <20060801170154.16504.qmail@web39109.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/dda42ce9/attachment.pl 

From emilymaxwellsmith at yahoo.com  Tue Aug  1 19:28:57 2006
From: emilymaxwellsmith at yahoo.com (Emily Smith Tonorezos)
Date: Tue, 1 Aug 2006 10:28:57 -0700 (PDT)
Subject: [R] natural spline function
Message-ID: <20060801172857.63009.qmail@web34203.mail.mud.yahoo.com>

Hello,
I am having difficulty with the ns function. I am
trying to use natural spline of a list of dates
in order to create a time series analysis where
date does not need to have a linear relationship
with the outcome.
Here is my code and the error message - I have
tried changing the class of variable but doesn't
seem to help.

> library(splines)
> date <- dataset$date
> mode(date)
[1] "numeric"
> class(date)
[1] "factor"
> try1 <- ns(date,6) ## with date as factor
Error in (1 - h) * qs[i] : non-numeric argument
to binary operator
> 

> date <- strptime(admissions$date,"%d%b%y")
> mode(date)
[1] "list"
> class(date)
[1] "POSIXt"  "POSIXlt"
> foo <- ns(date,6) ## with date as POSIXt

Error in as.POSIXct.default(X[[2]], ...) : 
        do not know how to convert 'X[[2]]' to
class "POSIXlt"

> try <- as.character(date)
> mode(try)
[1] "character"
> class(try)
[1] "character"
> foo <- ns(try,6)  
Error in qsort(x, FALSE) : argument is not a
numeric vector


From murdoch at stats.uwo.ca  Tue Aug  1 19:39:41 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 01 Aug 2006 13:39:41 -0400
Subject: [R] open DLL in R
In-Reply-To: <20060801170154.16504.qmail@web39109.mail.mud.yahoo.com>
References: <20060801170154.16504.qmail@web39109.mail.mud.yahoo.com>
Message-ID: <44CF91DD.1050500@stats.uwo.ca>

On 8/1/2006 1:01 PM, qian li wrote:
> I have downloaded a DLL file. I want to look at the contents in the DLL file. How can I do it in R?


As far as I know R has no tools to do that other than readBin, which 
will just show you a bunch of meaningless bytes.

If you're on Windows, the MinGW compiler used to build R and packages 
includes objdump and dlltool which can display DLL contents in a human 
readable format.

Duncan Murdoch


From jrkrideau at yahoo.ca  Tue Aug  1 19:43:01 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 1 Aug 2006 13:43:01 -0400 (EDT)
Subject: [R] What's a labelled data.frame? And how do I work with it?
Message-ID: <20060801174301.54024.qmail@web33801.mail.mud.yahoo.com>

I imported an SPSS file with its data labels using
spss.get (Library(Hmisc).
Class = data.frame

I then updated some of the spss labels and added a
label to the object itself.

label (staff.allocation) <- "raw data from the spss
file"

 I then save it as an R object. When I load the object
for further work it comes in as Class =  "labelled"
"data.frame"
 

 Then I try this

---------------------------------------------------------
 # Get the raw data that we have imported from SPSS.
load("H:/R.objects/staff.allocation.Rdata")

# Recode all 99's in the data base as NA except dates
and Subject since subject
# 99 is a valid ID number.

# Drop dates and ids
st1 <- staff.allocation[, -1:-4]

 st1[st1==99] <- NA

#replace date and id forget the entry date
st2 <-data.frame(staff.allocation$site,
staff.allocation$subject,
          staff.allocation$date, st1)
 -----------------------------------------------------
If I have applied a label to the data.frame I get an
error
Error in data.frame(staff.allocation$site,
staff.allocation$subject, staff.allocation$date,  :   
    arguments imply differing number of rows: 1865,
114
        
I orgininally was getting an error about
 " staff.allocation$site is a labelled
class and cannot be coerced into a data.frame" 

but I lost that one while trying to see what was
happeing
        
If I do not apply the label to staff.allocation then I
get a Class = "data.frame and I have no problem
creating the second data.frame.

Can anyone suggest what I am missing or what is
happening

Thanks


From dylan.beaudette at gmail.com  Tue Aug  1 20:06:16 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Tue, 1 Aug 2006 11:06:16 -0700
Subject: [R] questions regarding spline functions
In-Reply-To: <BE4711F3229E3941AE4A2D115F20A34859B744@UM-XMAIL05.um.umsystem.edu>
References: <BE4711F3229E3941AE4A2D115F20A34859B744@UM-XMAIL05.um.umsystem.edu>
Message-ID: <200608011106.17079.dylan.beaudette@gmail.com>

Posting reply to the list as well:

On Tuesday 01 August 2006 07:02, Myers, Brent wrote:
> Dylan,
>
> We are working on the same problem. I'm shocked that I hadn't seen the
> paper you posted and I am most grateful for that. I try to pay attention
> to McBratney's work too!
>

Hi Brent, glad to hear from a fellow soil scientist.
I originally found the McBratney paper in the book: Upscaling and Downscaling 
Methods for Environmental Research.

> The originator of the area-preserving spline is DeBoor, in A Practical
> Guide to Splines, Chapter 8. The DeBoor algorithm is available in the
> Matlab Spline Toolkit, which he wrote. Too pricey for me however. I have
> been unsuccessful implementing it thus far in S-Plus, the reference is
> not very clear in the construction of the coefficient matrix. I can send
> you my notes on that. I've paused on that for now, more below. Notice
> that these are quadratic splines (due to the number of constraints) and
> that they are not so 'flexible' in representing soils data.

Interesting. I will lookup the original author, and see if I can make any 
sense of it. This is not critical, merely a bit of initial exploration on 
horizon depth function characterizations for large data sets. 

> There are several other functions available from a CERN library
> available also in their Physics Analyst Workstation software (GPL). I
> haven't tried them. Realize also that some of the various density
> estimation algorithms in S/R are spline fits of this nature.

I will have to read up on those...

> I also considered the possibility of boundary data to define slope at
> some of the knots, but only about 1/3 of my data have good horizon
> descriptions. Linear stairstep with sloped lines across boundaries are
> trivial, but are a marked improvement over mid-horizon data.

I too have seen similar results. However, I cannot quite get the bs() function 
to use the "knots" that I have in mind: note that this may be an operator 
error... Does it make sense to use the horizon boundaries as knots?

> I am sure you are thinking about this...one big problem with area
> preserving splines is that they will not represent the true distribution
> of the soil property within the horizon if there are peaks. You
> undoubtedly realize that since the vector of pedogenesis is anisotropic,
> and that splines cannot represent this anisotropy. This is a real
> problem for me in that it is critical to identify the location of minima
> and maxima within the soil profile, more important than the property
> value in my case. Their location is grossly distorted by a spline fit to
> a mid-horizon data point. The mid-point data is better. I am taking a
> separate approach to solve that problem, and then fit the area
> preserving spline with knots defined at these locations. We need a
> general algorithm to handle this data!!!! It would be very useful for
> the soil science community!

Indeed. If you wouldn't mind, I would be interested in hearing how this 
progresses. While we are not in a position to develop such a technique, 
outside of something rather simple, we sure would be able to put it to use.

A quick summary in code and with an attached image of the various methods I 
have tried thus far, with one simple, and one complicated clay depth profile:

require(splines)
#example of two pedons on one plot:
z.1 <- c(0,2,18,24,68,160,170,192,200)
z.2 <- c(0,3,14,26,70,108,145,170,226,240)

x.1 <- mid(z.1)
x.2 <- mid(z.2)

#clay pct
y.1 <- c(0,1,2,2,4,7,6,1)
y.2 <- c(0,5,3,3,3,2,27,3,5)

fm.1 <- lm(y.1 ~ bs(x.1, df=5) )
fm.2 <- lm(y.2 ~ bs(x.2, df=5) )

isfm.1 <- interpSpline(y.1 ~ x.1)
isfm.2 <- interpSpline(y.2 ~ x.2)

new_x.1 <-  seq(range(x.1)[1], range(x.1)[2], len = 200)
new_x.2 <-  seq(range(x.2)[1], range(x.2)[2], len = 200)

new_y.1 <- predict(fm.1, data.frame(x.1=new_x.1) )
new_y.2 <- predict(fm.2, data.frame(x.2=new_x.2) )

#interpSpline() method
#note different predict() syntax
ispline_y.1 <- predict(isfm.1, new_x.1)
ispline_y.2 <- predict(isfm.2, new_x.2)

par(mfrow=c(2,1))

#simple pedon example
plot(y.1 ~ x.1, xlab="Depth", ylab="Percent Clay", type="b", pch=16, lwd=2, 
main="Simple Pedon")
#spline examples
lines(spline(y.1 ~ x.1), col="blue", lty=2)
lines(new_x.1, new_y.1, col='red', lty=2)
lines(ispline_y.1, col='green', lty=2)

legend(8,6, legend=c('soil data', 'splines()', 'interpSpline()', 'bs()'), 
col=c('black', 'blue', 'red', 'green'), lty=c(1,1,1,1), lwd=c(2,1,1,1), 
cex=0.7)

#complex pedon:
plot(y.2 ~ x.2, xlab="Depth", ylab="Percent Clay", type="b", pch=16, lwd=2, 
main="Complex Pedon")
#spline examples
lines(spline(y.2 ~ x.2), col="blue", lty=2)
lines(new_x.2, new_y.2, col='red', lty=2)
lines(ispline_y.2, col='green', lty=2)

legend(8.4,24, legend=c('soil data', 'splines()', 'interpSpline()', 'bs()'), 
col=c('black', 'blue', 'red', 'green'), lty=c(1,1,1,1), lwd=c(2,1,1,1), 
cex=0.7)


Cheers,

Dylan


> Brent
>
> D. Brenton Myers
> Graduate Fellow
> University of Missouri
> Soil Environmental and Atmospheric Sciences
> 269 Agriculture Engineering Building
> Columbia, MO 65211
> Work: (573) 882-1146
> Home: (573) 446-6856
> Cell: (573) 881-6855
> email: myersdb at missouri.edu
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dylan Beaudette
> Sent: Monday, July 31, 2006 6:43 PM
> To: RHELP
> Subject: [R] questions regarding spline functions
>
> Greetings,
>
> A couple general questions regarding the use of splines to interpolate
> depth
> profile data.
>
> Here is an example of a set of depths, with associated attributes for a
> given
> soil profile, along  with a function for calculating midpoints from a
> set of
> soil horizon boundaries:
>
> #calculate midpoints:
> mid <- function(x) {
> for( i in 1:length(x)) {
>  if( i > 1) {
>    a[i] = (x[i] - x[i-1]) / 2 + x[i-1]
>   }
>  }
> #reurn the results
> a[which(!is.na(a))]
> }
>
> #horizon depth bounds
> z <- c(0,2,18,24,68,160,170,192,200)
>
> #horizon midpoints, associated with horizon attribute
> x <- mid(z)
>
> #clay pct
> y <- c(0,1,2,2,4,7,6,1)
>
> #plot them
> plot(y ~ x, xlab="Depth", ylab="Percent Clay", type="s")
> points(y ~ x, cex=0.5, pch=16)
>
> These point pairs usually represent a trend with depth, which I would
> like to
> model with splines - or some similar approach, as they have been found
> to
> work better than other methods such as a fitted polynomial.
>
> Using the B Spline function from the 'splines' package, it is possible
> to fit
> a model of some property with depth based on the bs() function:
>
> #natual, B-Splines
> library(splines)
>
> #fit a b-spline model:
> fm <- lm(y ~ bs(x, df=5) )
>
> I am able to predict a soil property with depth, at unsampled locations
> with
> this model with:
>
> new_x <-  seq(range(x)[1], range(x)[2], len = 200)
>
> #predict attribute at unsampled depths:
> new_y <- predict(fm, data.frame(x=new_x) )
>
> #plot the predicted attribute at the unsampled depths
> lines(new_x, new_y, col='red')
>
> This tends to work fairly well (see attached), but I am wondering if I
> can use
> the 'knots' parameter in the bs() function for incorporation of horizon
> boundary information into the creation of the spline. Moreover, it would
> be
> nice if the spline-based model 'fm' would predict a set of values with
> similar mean and range as the original data points: i.e
>
> #summary of clay values from original data:
> summary(y)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   0.000   1.000   2.000   2.875   4.500   7.00
>
> #see above
> summary(new_y)
>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
> -0.05786  2.09500  3.13200  3.62800  5.17100  7.08700
>
>
> This is based on an article I read :
> http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V67-3WWRDYY-3
> &_user=4421&_handle=V-WA-A-W-AU-MsSAYWW-UUA-U-AACZEDZWBC-AACVCCDUBC-BZAY
> UEWB-AU-U&_fmt=summary&_coverDate=08%2F31%2F1999&_rdoc=3&_orig=browse&_s
> rch=%23toc%235807%231999%23999089998%23108393!&_cdi=5807&view=c&_acct=C0
> 00059598&_version=1&_urlVersion=0&_userid=4421&md5=488f1e114d8d64265ff65
> 506e9587e71
>
> where the author talks about a so-called 'equal-area quadratic smoothing
>
> spline' approach to describing a soil property depth function.
> Unfortunately
> the author did not provide sample code....
>
> Any thoughts / input would be greatly appreciated!
>
> Cheers,

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341
-------------- next part --------------
A non-text attachment was scrubbed...
Name: spline2.png
Type: image/png
Size: 33301 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060801/4f119153/attachment.png 

From ripley at stats.ox.ac.uk  Tue Aug  1 19:48:50 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Aug 2006 18:48:50 +0100 (BST)
Subject: [R] open DLL in R
In-Reply-To: <20060801170154.16504.qmail@web39109.mail.mud.yahoo.com>
References: <20060801170154.16504.qmail@web39109.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608011844120.21448@gannet.stats.ox.ac.uk>

On Tue, 1 Aug 2006, qian li wrote:

> I have downloaded a DLL file. I want to look at the contents in the DLL file. How can I do it in R?

You need a disassembler such as VC++'s DUMPBIN, but looking at compiled 
code you did not write is not an easy task.  (Or objdump from the MinGW 
toolset.)

If only you want to know what entry points it exports, use pedump -e for 
the pedump.exe in tools.zip (see the R-admin manual).

What has this to do with R?

>    
>   Thanks,
>    
>   QL
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From efg at stowers-institute.org  Tue Aug  1 21:21:42 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 1 Aug 2006 14:21:42 -0500
Subject: [R] rgb and col2rgb color conversion/modification/shading
References: <1154392412.44cea15c0c1fa@webmail.fhcrc.org>
Message-ID: <eao9ka$2lf$1@sea.gmane.org>

<ccarey at fhcrc.org> wrote in message 
news:1154392412.44cea15c0c1fa at webmail.fhcrc.org...
>I want to get a lighter shade of a color...I have a lot of colored objects 
>and
> want each one printed as a foreground against a slightly lighter 
> background.
>
> I thought I could try something like changing the alpha channel by first
> converting it to rgb.

I'm not sure what you want to do with the alpha channel - it's sometimes 
used for transparency, especially on Macs, but is not used much on PCs 
(AFAIK).



Let's say you want different shades of gold:

> colors()[142]

[1] "gold"



Instead of RGB color space perhaps you should consider HSV 
(Hue-Saturation-Value) color space.



Let's convert "gold" to rgb to hsv:



> col2rgb( colors()[142] )

      [,1]

red    255

green  215

blue     0



> rgb2hsv( col2rgb( colors()[142] ) )

       [,1]

h 0.1405229

s 1.0000000

v 1.0000000



The "hue" (h) is the color ranging from 0 to 1 around a color circle (with 
red= 0 or 1).  Find h = 0.140 ("gold") in this color circle:



hue <- seq(0.0, 1.0, by=1/40)



pie(rep(1,40),

    labels=formatC(hue, digits=3, format="f"), cex=0.75,

    col=hsv(hue, 1.0, 1.0),

    radius=1.0,

    main="HSV (S=1, V=1)" )




Hues range from 0.0 to 1.0.




A color is saturated (s=1) when it is "far" from a shade of gray (ranging 
from black to white).  Grays are unsaturated (no color) colors with s = 0. 
Saturation ranges from 0.0 to 1.0.



The value (v) is the brightness of the color.  Low values appear quite dark 
but still have color.  v=1 is as bright as possible.   Values range from 0.0 
to 1.0.



You can get different "shades" of the same color by varying changing the 
saturation and value for a given hue.  The hsv function returns the RGB 
color in hex form.



Consider:

> hsv(0.1405, 1, 1)

[1] "#FFD700"



Hex FF = decimal 255 = red

Hex D7 = decimal 215 = green

Hex 00 = decimal 0 = blue



Let's vary Saturation from 0.0 to 1.0 and Value from 0.0 to 1.0 in this 
plot:





MakeHSVRectangle <- function(saturation, value)

{

  GoldHue <- 0.140

  color <- hsv(GoldHue, saturation, value)

  rect(100*saturation, 100*value, 100*saturation+4, 100*value+4, col=color)

}





plot(0:110,0:110, type="n",

     xlab="Saturation[%]", ylab="Value[%]",

     main="Shades of Gold, H=0.140")

outer(seq(0.0, 1.0, 0.05), seq(0.0, 1.0, 0.05), MakeHSVRectangle)





With Value = 0, all colors are "black".  With Saturation=0, the only 
"colors" along the y axis are the shades of gray.  The original "gold" 
rectangle is at the upper right.



So, given a starting color, you have a number of "shades" (various 
saturations and values) with the same color hue.



I hope this helps.



efg

Earl F. Glynn

Scientific Programmer

Stowers Institute for Medical Research


From chonghuitan at smu.edu.sg  Tue Aug  1 21:37:47 2006
From: chonghuitan at smu.edu.sg (TAN Chong Hui)
Date: Wed, 2 Aug 2006 03:37:47 +0800
Subject: [R] libRmath
In-Reply-To: <FA3090E732DC6A4EB8E2D875EEB486A508CBD7@EX01.staff.smu.edu.sg>
Message-ID: <FA3090E732DC6A4EB8E2D875EEB486A508CBE1@EX01.staff.smu.edu.sg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/f3d44c15/attachment.pl 

From chonghuitan at smu.edu.sg  Tue Aug  1 21:44:12 2006
From: chonghuitan at smu.edu.sg (TAN Chong Hui)
Date: Wed, 2 Aug 2006 03:44:12 +0800
Subject: [R] Use R in Java
In-Reply-To: <FA3090E732DC6A4EB8E2D875EEB486A508CBE1@EX01.staff.smu.edu.sg>
Message-ID: <FA3090E732DC6A4EB8E2D875EEB486A508CBE2@EX01.staff.smu.edu.sg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/fe63de74/attachment.pl 

From jrkrideau at yahoo.ca  Tue Aug  1 21:48:27 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 1 Aug 2006 15:48:27 -0400 (EDT)
Subject: [R] What's a labelled data.frame? And how do I work with it?
In-Reply-To: <20060801174301.54024.qmail@web33801.mail.mud.yahoo.com>
Message-ID: <20060801194827.50593.qmail@web33813.mail.mud.yahoo.com>


--- John Kane <jrkrideau at yahoo.ca> wrote:

> I imported an SPSS file with its data labels using
> spss.get (Library(Hmisc).
> Class = data.frame
> 
> I then updated some of the spss labels and added a
> label to the object itself.
> 
> label (staff.allocation) <- "raw data from the spss
> file"
> 
>  I then save it as an R object. When I load the
> object
> for further work it comes in as Class =  "labelled"
> "data.frame"
>  
> 
>  Then I try this
> 
>
---------------------------------------------------------
>  # Get the raw data that we have imported from SPSS.
> load("H:/R.objects/staff.allocation.Rdata")
> 
> # Recode all 99's in the data base as NA except
> dates
> and Subject since subject
> # 99 is a valid ID number.
> 
> # Drop dates and ids
> st1 <- staff.allocation[, -1:-4]
> 
>  st1[st1==99] <- NA
> 
> #replace date and id forget the entry date
> st2 <-data.frame(staff.allocation$site,
> staff.allocation$subject,
>           staff.allocation$date, st1)
> 
>
-----------------------------------------------------
> If I have applied a label to the data.frame I get an
> error
> Error in data.frame(staff.allocation$site,
> staff.allocation$subject, staff.allocation$date,  : 
>  
>     arguments imply differing number of rows: 1865,
> 114
>         
> I orgininally was getting an error about
>  " staff.allocation$site is a labelled
> class and cannot be coerced into a data.frame" 
> 
> but I lost that one while trying to see what was
> happeing
>         
> If I do not apply the label to staff.allocation then
> I
> get a Class = "data.frame and I have no problem
> creating the second data.frame.
> 
> Can anyone suggest what I am missing or what is
> happening
> 
> Thanks

To follow up to my own post it appears that if I make
any changes in the imported labels I also get an error
message when I attempt to create that data.frame
The message is
Error in as.data.frame.default(x[[i]], optional =
TRUE) :  cannot coerce class "labelled" into a
data.frame


From kamil at ualberta.ca  Tue Aug  1 21:53:15 2006
From: kamil at ualberta.ca (kamil Marcinkowski)
Date: Tue, 1 Aug 2006 13:53:15 -0600
Subject: [R] R an AIX
In-Reply-To: <1d2321fe.88c213bd.819dd00@m4500-03.uchicago.edu>
References: <1d2321fe.88c213bd.819dd00@m4500-03.uchicago.edu>
Message-ID: <34F0FB91-EF06-415A-8584-1600AD6E8DA3@ualberta.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/bf7324fd/attachment.pl 

From swidanf at janelia.hhmi.org  Tue Aug  1 21:57:01 2006
From: swidanf at janelia.hhmi.org (Swidan, Firas)
Date: Tue, 01 Aug 2006 15:57:01 -0400
Subject: [R] Indexing issue
Message-ID: <C0F52A4D.101C%swidanf@janelia.hhmi.org>

Hi,

R is having the following weird behavior and I am not sure if that is a
feature or a bug:

I am working on the following "3D" array:

> bIm
, , 1

     [,1]
[1,] TRUE
[2,] TRUE
[3,] TRUE
[4,] TRUE
[5,] TRUE

> class(bIm)
[1] "array"
> dim(bIm)
[1] 5 1 1

When I try to get the first 2D subarray, the whole thing folds into a
vector:

> bIm[,,1]
[1] TRUE TRUE TRUE TRUE TRUE

This causes a lot of trouble in the R code as one would have expected to get
a 2D array but ends up with this "logical" vector.

Is this the way it was meant to be? Does not this behavior bother anyone
else besides me?

Thanks for the help,
Firas.


From f.harrell at vanderbilt.edu  Tue Aug  1 21:59:47 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 01 Aug 2006 14:59:47 -0500
Subject: [R] What's a labelled data.frame? And how do I work with it?
In-Reply-To: <20060801194827.50593.qmail@web33813.mail.mud.yahoo.com>
References: <20060801194827.50593.qmail@web33813.mail.mud.yahoo.com>
Message-ID: <44CFB2B3.3030605@vanderbilt.edu>

John Kane wrote:
> --- John Kane <jrkrideau at yahoo.ca> wrote:
> 
>> I imported an SPSS file with its data labels using
>> spss.get (Library(Hmisc).
>> Class = data.frame
>>
>> I then updated some of the spss labels and added a
>> label to the object itself.
>>
>> label (staff.allocation) <- "raw data from the spss
>> file"
>>
>>  I then save it as an R object. When I load the
>> object
>> for further work it comes in as Class =  "labelled"
>> "data.frame"
>>  
>>
>>  Then I try this
>>
>>
> ---------------------------------------------------------
>>  # Get the raw data that we have imported from SPSS.
>> load("H:/R.objects/staff.allocation.Rdata")
>>
>> # Recode all 99's in the data base as NA except
>> dates
>> and Subject since subject
>> # 99 is a valid ID number.
>>
>> # Drop dates and ids
>> st1 <- staff.allocation[, -1:-4]
>>
>>  st1[st1==99] <- NA
>>
>> #replace date and id forget the entry date
>> st2 <-data.frame(staff.allocation$site,
>> staff.allocation$subject,
>>           staff.allocation$date, st1)
>>
>>
> -----------------------------------------------------
>> If I have applied a label to the data.frame I get an
>> error
>> Error in data.frame(staff.allocation$site,
>> staff.allocation$subject, staff.allocation$date,  : 
>>  
>>     arguments imply differing number of rows: 1865,
>> 114
>>         
>> I orgininally was getting an error about
>>  " staff.allocation$site is a labelled
>> class and cannot be coerced into a data.frame" 
>>
>> but I lost that one while trying to see what was
>> happeing
>>         
>> If I do not apply the label to staff.allocation then
>> I
>> get a Class = "data.frame and I have no problem
>> creating the second data.frame.
>>
>> Can anyone suggest what I am missing or what is
>> happening
>>
>> Thanks
> 
> To follow up to my own post it appears that if I make
> any changes in the imported labels I also get an error
> message when I attempt to create that data.frame
> The message is
> Error in as.data.frame.default(x[[i]], optional =
> TRUE) :  cannot coerce class "labelled" into a
> data.frame

No, you can do things like

library(Hmisc)
label(mydata$x) <- 'my label'

or use the upData function in Hmisc

A labelled data frame just sets up so that subsetting with [ will keep 
labels for all the variables in the data frame.

Frank

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From rguha at indiana.edu  Tue Aug  1 22:00:01 2006
From: rguha at indiana.edu (Rajarshi Guha)
Date: Tue, 01 Aug 2006 16:00:01 -0400
Subject: [R] Use R in Java
In-Reply-To: <FA3090E732DC6A4EB8E2D875EEB486A508CBE2@EX01.staff.smu.edu.sg>
References: <FA3090E732DC6A4EB8E2D875EEB486A508CBE2@EX01.staff.smu.edu.sg>
Message-ID: <1154462401.5350.62.camel@localhost>

On Wed, 2006-08-02 at 03:44 +0800, TAN Chong Hui wrote:
> Hi
> 
> I'm trying to use R from Java.
> There doesn't seem to be much documentation on this. 
> I installed JGR that has JRI and Rjava coming with it.
> 
> According to the short description at www.rosuda.org/JRI/,
> there's supposed to be an "examples" directory in JRI.
> I don't seem to be able to find that.
> 
> I tried to install JRI on its own (not with JGR) from rosuda.
> But there seems to be some error with the zip-file.
> 
> Help, anybody?

I don't know how it'd work on Windows (though I suppose it should be
similar) but on Linux, you basically need to 

* Put JRI.jar in your CLASSPATH
* Put libjrio.so in your LD_LIBRARY_PATH (libR.so should also be in this
PATH)
* Set R_HOME to point to wherever you installed R

Also if you only want to use R from Java (and don't need Java from R)
then you should not need Rjava at all. The JRI source's should be
sufficient.

The CDK uses JRI to use R as a backend and you can take a look at the
code that we use to interact with R. On example can be seen at 

http://svn.sourceforge.net/viewvc/cdk/trunk/cdk/src/org/openscience/cdk/qsar/model/R2/RModel.java?view=markup

-------------------------------------------------------------------
Rajarshi Guha <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: What's purple and commutes?
A: An abelian grape.


From tlumley at u.washington.edu  Tue Aug  1 22:13:23 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 1 Aug 2006 13:13:23 -0700 (PDT)
Subject: [R] Indexing issue
In-Reply-To: <C0F52A4D.101C%swidanf@janelia.hhmi.org>
References: <C0F52A4D.101C%swidanf@janelia.hhmi.org>
Message-ID: <Pine.LNX.4.64.0608011310130.31895@homer22.u.washington.edu>

On Tue, 1 Aug 2006, Swidan, Firas wrote:
> Hi,
>
> R is having the following weird behavior and I am not sure if that is a
> feature or a bug:

It's a feature. And a very old FAQ (#7.5)


> I am working on the following "3D" array:
>
>> bIm
> , , 1
>
>     [,1]
> [1,] TRUE
> [2,] TRUE
> [3,] TRUE
> [4,] TRUE
> [5,] TRUE
>
>> class(bIm)
> [1] "array"
>> dim(bIm)
> [1] 5 1 1
>
> When I try to get the first 2D subarray, the whole thing folds into a
> vector:
>
>> bIm[,,1]
> [1] TRUE TRUE TRUE TRUE TRUE
>
> This causes a lot of trouble in the R code as one would have expected to get
> a 2D array but ends up with this "logical" vector.
>
> Is this the way it was meant to be? Does not this behavior bother anyone
> else besides me?

Yes, it is the way it was meant to be. It has bothered other people, but 
the alternatives are probably worse.

 	-thomas


From ripley at stats.ox.ac.uk  Tue Aug  1 22:20:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Aug 2006 21:20:27 +0100 (BST)
Subject: [R] libRmath
In-Reply-To: <FA3090E732DC6A4EB8E2D875EEB486A508CBE1@EX01.staff.smu.edu.sg>
References: <FA3090E732DC6A4EB8E2D875EEB486A508CBE1@EX01.staff.smu.edu.sg>
Message-ID: <Pine.LNX.4.64.0608012119590.26279@gannet.stats.ox.ac.uk>

On Wed, 2 Aug 2006, TAN Chong Hui wrote:

> Hi
> 
> I'm trying to use the standalone library libRmath.
> According to the documentation, I need to builld it in
> src/nmath/standalone
> 
> But this directory does not exist in the R I installed.
> 
> What's the problem here, anyone?

Did you install a *source* version of R: you are reading documentation 
about the sources of R?

> 
> Thanks!
> 
> Rgds
> Chong Hui
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Aug  1 22:22:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Aug 2006 21:22:36 +0100 (BST)
Subject: [R] R an AIX
In-Reply-To: <34F0FB91-EF06-415A-8584-1600AD6E8DA3@ualberta.ca>
References: <1d2321fe.88c213bd.819dd00@m4500-03.uchicago.edu>
	<34F0FB91-EF06-415A-8584-1600AD6E8DA3@ualberta.ca>
Message-ID: <Pine.LNX.4.64.0608012121060.26279@gannet.stats.ox.ac.uk>

Did you look in config.log for more information about the error?
It looks like you cannot even run your compiler.

(This is not really an R question, and off-topic for R-help: see the 
posting guide.)

On Tue, 1 Aug 2006, kamil Marcinkowski wrote:

> Hello All,
> 
> I am having trouble with installing the latest R on AIX 5.3, I can't  
> even configure.
> 
> > checking how to get verbose linking output from /usr/vac/bin/ 
> > xlf_r... configure: WARNING: compilation failed
> >
> > checking for Fortran libraries of /usr/vac/bin/xlf_r...
> > checking how to get verbose linking output from /usr/vac/bin/ 
> > xlc_r... -v
> > checking for C libraries of /usr/vac/bin/xlc_r...  -L/usr/vac/lib - 
> > lxlopt -L/usr/lib/threads -lpthreads
> > checking for dummy main to link with Fortran libraries... none
> > checking for Fortran name-mangling scheme... configure: error:  
> > cannot compile a simple Fortran program
> 
> 
> Has anyone installed R-2.3.1 on AIX 5.3 using the native complier 
> (xlc_r)?
> If so would you send which flags and options did you use?
> 
> Thanks,
> 
> Kamil
> 
> Kamil Marcinkowski                   Westgrid System Administrator
> kamil at ualberta.ca                     University of Alberta site
>   Tel.780 492-0354                     Research Computing Support
> Fax.780 492-1729                     Academic ICT
> Edmonton, Alberta, CANADA    University of Alberta
> 
> 
> "This communication is intended for the use of the recipient to which  
> it is
> addressed, and may contain confidential, personal, and/or privileged
> information.  Please contact us immediately if you are not the intended
> recipient of this communication.  If you are not the intended  
> recipient of
> this communication, do not copy, distribute, or take action on it. Any
> communication received in error, or subsequent reply, should be  
> deleted or
> destroyed."
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From neuro3000 at hotmail.com  Tue Aug  1 22:38:14 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 01 Aug 2006 16:38:14 -0400
Subject: [R] Indexing issue
In-Reply-To: <C0F52A4D.101C%swidanf@janelia.hhmi.org>
Message-ID: <BAY112-F290C03B370DC3267A9BD22AF5D0@phx.gbl>

Hi,

Try this:

bIm <-array(rep(TRUE,5),c(5,1,1))
bIm[,,1]
#[1] TRUE TRUE TRUE TRUE TRUE

bIm[,,1,drop=FALSE]
, , 1

     [,1]
[1,] TRUE
[2,] TRUE
[3,] TRUE
[4,] TRUE
[5,] TRUE

See ?'[' for details on drop

drop: For matrices and arrays.  If 'TRUE' the result is coerced to
          the lowest possible dimension


>From: "Swidan, Firas" <swidanf at janelia.hhmi.org>
>To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>Subject: [R] Indexing issue
>Date: Tue, 01 Aug 2006 15:57:01 -0400
>
>Hi,
>
>R is having the following weird behavior and I am not sure if that is a
>feature or a bug:
>
>I am working on the following "3D" array:
>
> > bIm
>, , 1
>
>      [,1]
>[1,] TRUE
>[2,] TRUE
>[3,] TRUE
>[4,] TRUE
>[5,] TRUE
>
> > class(bIm)
>[1] "array"
> > dim(bIm)
>[1] 5 1 1
>
>When I try to get the first 2D subarray, the whole thing folds into a
>vector:
>
> > bIm[,,1]
>[1] TRUE TRUE TRUE TRUE TRUE
>
>This causes a lot of trouble in the R code as one would have expected to 
>get
>a 2D array but ends up with this "logical" vector.
>
>Is this the way it was meant to be? Does not this behavior bother anyone
>else besides me?
>
>Thanks for the help,
>Firas.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From John.Kerpel at infores.com  Tue Aug  1 22:40:00 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Tue, 1 Aug 2006 15:40:00 -0500
Subject: [R] Replacing NA in fSeries
Message-ID: <44A8B25381923D4F93B74B2676A50F6D03059EEF@MAIL1.infores.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/875ccd85/attachment.pl 

From fredrik.jansson at dynasam.com  Tue Aug  1 23:26:17 2006
From: fredrik.jansson at dynasam.com (Fredrik Jansson)
Date: Tue, 01 Aug 2006 23:26:17 +0200
Subject: [R] rsurv in ipred
Message-ID: <op.tdmi53pg1vvm94@50-02503.wp.ams.se>

Hi,

I'm trying to find information about rsurv "Simulating Survival data" in  
the IPred package, without luck this far. In the description of this  
function we are asked to consult Hothorn et al. (2003) for the details.  
This paper is not in the reference list. Should it be same authors (2004)?  
In that case I will try my library, in any other case could someone please  
give me some directions to where I can find the paper.

Regards
Fredrik Jansson


From wowen at richmond.edu  Tue Aug  1 23:28:14 2006
From: wowen at richmond.edu (Owen, Jason)
Date: Tue, 1 Aug 2006 17:28:14 -0400
Subject: [R] plot() with TukeyHSD
Message-ID: <4619CB1B8D625544BF3764E6B346DA1F6411A4@helena.richmond.edu>

Hello,

When plotting the results of a TukeyHSD multiple comparisons
procedure with an ANOVA (lm) object, an extra line appears
in the confidence intervals that contain 0.  For example (this
is straight from the TukeyHSD helpfile):

> summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
> TukeyHSD(fm1, "tension", ordered = TRUE)
> plot(TukeyHSD(fm1, "tension"))

An extra line segment appears in the interval to the right of the 
dotted zero line.  Coincidentally, if all of the calculated 
intervals for a dataset don't contain zero, this isn't a problem 
-- so I guess the line segment "belongs" to the zero line.  I
checked R Search and didn't find this mentioned before... but
please let me know if I overlooked something.

Jason

R 2.2.0 on Windows 
-- BTW, the error didn't happen on my Mac OSX w/ R 1.9.1

--
Assistant Professor of Statistics
Mathematics and Computer Science Department
University of Richmond, Virginia 23173
(804) 289-8081   fax:(804) 287-6664
http://www.mathcs.richmond.edu/~wowen

"This is R. There is no if. Only how."
Simon (Yoda) Blomberg


From gunter.berton at gene.com  Tue Aug  1 23:47:04 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 1 Aug 2006 14:47:04 -0700
Subject: [R] R Reference Card and other help (especially useful for Newbies)
Message-ID: <005301c6b5b4$0736f220$711f210a@gne.windows.gene.com>


Hi all: 

  
Newbies (and others!) may find useful the R Reference Card made available by

Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through 
the "Contributed" link on CRAN (where some other reference cards are also 
linked). It categorizes and organizes a bunch of R's basic, most used 
functions so that they can be easily found. For example, paste() is under 
the "Strings" heading and expand.grid() is under "Data Creation." For 
newbies struggling to find the right R function as well as veterans who 
can't quite remember the function name, it's very handy. 

Also don't forget R's other Help facilties: 

help.search("keyword or phrase") to search the **installed** man pages 

RSiteSearch("keyword or phrase") to search the CRAN website via Jonathan
Baron's search engine. This can also be done directly from CRAN by following
the "search" link there.

And, occasionally, find()/apropos() to search the ** attached** packages for
functions using regexp's. 

Though R certainly can be intimidating, please **do** try these measures
first before posting questions to the list. And please **do** read the other
basic R reference materials. Better and faster answers can often be found
this way.

  
-- Bert Gunter 
Genentech Non-Clinical Statistics 
South San Francisco, CA 
  
"The business of the statistician is to catalyze the scientific learning 
process."  - George E. P. Box 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


From Bill.Venables at csiro.au  Wed Aug  2 01:28:12 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Wed, 2 Aug 2006 09:28:12 +1000
Subject: [R] Fitting models in a loop
Message-ID: <B998A44C8986644EA8029CFE6396A9245475CD@exqld2-bne.qld.csiro.au>

 
Markus Gesmann writes:

> Murray,
> 
> How about creating an empty list and filling it during your loop:
> 
>  mod <- list()
>  for (i in 1:6) {
> 	  mod[[i]] <- lm(y ~ poly(x,i))
> 	  print(summary(mod[[i]]))
> 	  }
> 
> All your models are than stored in one object and you can use lapply
to
> do something on them, like:
>  lapply(mod, summary) or lapply(mod, coef)

I think it is important to see why this deceptively simple 
solution does not achieve the result that Murray wanted.

Take any fitted model object, say mod[[4]].  For this object the 
formula component of the call will be, literally,  y ~ poly(x, i), 
and not y ~ poly(x, 4), as would be required to use the object,
e.g. for prediction.  In fact all objects have the same formula.

You could, of course, re-create i and some things would be OK, 
but getting pretty messy.

You would still have a problem if you wanted to plot the fit with 
termplot(), for example, as it would try to do a two-dimensional 
plot of the component if both arguments to poly were variables.

> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Bill.Venables at csiro.au
> Sent: 01 August 2006 06:16
> To: maj at waikato.ac.nz; r-help at stat.math.ethz.ch
> Subject: Re: [R] Fitting models in a loop
> 
> 
> Murray,
> 
> Here is a general paradigm I tend to use for such problems.  It
extends
> to fairly general model sequences, including different responses, &c
> 
> First a couple of tiny, tricky but useful functions:
> 
> subst <- function(Command, ...) do.call("substitute", list(Command,
> list(...)))
> 
> abut <- function(...)  ## jam things tightly together
>   do.call("paste", c(lapply(list(...), as.character), sep = "")) 
> 
> Name <- function(...) as.name(do.call("abut", list(...)))
> 
> Now the gist.
> 
> fitCommand <- quote({
> 	  MODELi <- lm(y ~ poly(x, degree = i), theData)
> 	  print(summary(MODELi))
> })
> for(i in 1:6) {
> 	  thisCommand <- subst(fitCommand, MODELi = Name("model_", i), i
=
> i)
> 	  print(thisCommand)  ## only as a check
> 	  eval(thisCommand)
> }
> 
> At this point you should have the results and
> 
> objects(pat = "^model_")
> 
> should list the fitted model objects, all of which can be updated,
> summarised, plotted, &c, because the information on their construction
> is all embedded in the call.
> 
> Bill.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray
Jorgensen
> Sent: Tuesday, 1 August 2006 2:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Fitting models in a loop
> 
> If I want to display a few polynomial regression fits I can do
something
> 
> like
> 
> for (i in 1:6) {
> 	  mod <- lm(y ~ poly(x,i))
> 	  print(summary(mod))
> 	  }
> 
> Suppose that I don't want to over-write the fitted model objects, 
> though. How do I create a list of blank fitted model objects for later

> use in a loop?
> 
> Murray Jorgensen
> --


From p.murrell at auckland.ac.nz  Wed Aug  2 03:59:26 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 02 Aug 2006 13:59:26 +1200
Subject: [R] R crashes using pdf() windows() or postscript()
In-Reply-To: <005701c6b53e$3acd7120$2c70210a@UCSPC32>
References: <005701c6b53e$3acd7120$2c70210a@UCSPC32>
Message-ID: <44D006FE.4010807@stat.auckland.ac.nz>

Hi

The problem is with the font information that R loads into memory.  R
tries to load each unique font only once per session, but there was a
bug (or two) which meant that each time you create a new PDF (or
PostScript) device, R loaded another copy of the font information into
memory.  So in your example below, we end up trying to load 10000 font
structures, where each one is not tiny.  Hence eventual slow-down,
crashes and/or low-memory messages. A fix has been made to the
development version of R, so you could try that out.  Otherwise, I'm
afraid I can only suggest that you produce the plots in multiple R
sessions (has anyone ever tried to use package 'snow' with Sweave?)

Paul


Jan Wijffels wrote:
> Dear HelpeRs,
> I have a script where I save several thousands of graphics. These are
> then used in Latex through Sweave. Unfortunately R crashes while making
> these plots and Windows pops up some message that I run low on virtual
> memory. I tried to save the plots using pdf(), windows() and
> postscript() and also tried to run it with R CMD BATCH myscript.R. But
> after a while R slows down and crashes eventually or stops computing.
> I'm using windows XP with R 2.3.1. I included a script that shows the
> problem if you let it run for a while.
>> R.version
>                 _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>  
>  
> draw.rectangle <- function(
>      label,
>      figurename="figure",
>      figurepath=getwd(),
>      box.gpar=gpar(fill=rgb(red=51, green=51, blue=204, maxColorValue =
> 255),
>                    col=rgb(red=128, green=128, blue=128, maxColorValue =
> 255),
>                    lwd=2),
>      text.gpar=gpar(col="white", fontface="bold", fontsize=14, cex=1,
> fontfamily="sans"),
>      type="pdf") {
>    setwd(figurepath)
>    if(type == "win"){
>      windows(width=1.4, height=0.5)
>    }
>    else if(type == "pdf"){
>      pdf(file = paste(figurename, ".pdf", sep=''), width = 1.4, height
> = 0.5, onefile = TRUE, family = "Helvetica", paper = "special")
>    }
>    else if(type == "ps"){
>      postscript(file = paste(figurename, ".ps", sep=''), width = 1.4,
> height = 0.5, onefile = TRUE, family = "Helvetica", paper = "special",
> fonts = "sans")
>    }
>    boxheight <- unit(1, "npc")
>    boxwidth  <- unit(1, "npc")
>    pushViewport(viewport(x=0.5, y=0.5, width = boxwidth, height =
> boxheight))
>      grid.roundRect(height=boxheight, width=boxwidth, r=unit(3, "mm"),
> gp = box.gpar)
>      grid.text(label, just = "centre",  gp = text.gpar)
>    popViewport()
>    if(type == "win"){
>      tmp <- savePlot(filename = figurename, type = "pdf", device =
> dev.cur(), restoreConsole = TRUE)
>    }
>    tmp <- dev.off()
> }
> require(RGraphics)
> setwd("C:\\")
> for(i in 1:10000){
>    draw.rectangle("blablabla", type="win")
> }
>  
> Anyone has suggestions on how to solve this?
>  
>  
> Jan 
>  
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ggrothendieck at gmail.com  Wed Aug  2 04:00:50 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Aug 2006 22:00:50 -0400
Subject: [R] Fitting models in a loop
In-Reply-To: <B998A44C8986644EA8029CFE6396A9245475CD@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9245475CD@exqld2-bne.qld.csiro.au>
Message-ID: <971536df0608011900k36c60f1ax8522fcb675eaca2f@mail.gmail.com>

A simple way around this is to pass it as a data frame.
In the code below the only change we made was to change
the formula from y ~ poly(x, i) to y ~ . and pass poly(x,i)
in a data frame as argument 2 of lm:

# test data
set.seed(1)
x <- 1:10
y <- x^3 + rnorm(10)

# run same code except change the lm call
mod <- list()
for (i in 1:3) {
        mod[[i]] <- lm(y ~., data.frame(poly(x, i)))
        print(summary(mod[[i]]))
}

After running the above we can test that it works:

> for(i in 1:3) print(formula(mod[[i]]))
y ~ X1
y ~ X1 + X2
y ~ X1 + X2 + X3

On 8/1/06, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
>
> Markus Gesmann writes:
>
> > Murray,
> >
> > How about creating an empty list and filling it during your loop:
> >
> >  mod <- list()
> >  for (i in 1:6) {
> >         mod[[i]] <- lm(y ~ poly(x,i))
> >         print(summary(mod[[i]]))
> >         }
> >
> > All your models are than stored in one object and you can use lapply
> to
> > do something on them, like:
> >  lapply(mod, summary) or lapply(mod, coef)
>
> I think it is important to see why this deceptively simple
> solution does not achieve the result that Murray wanted.
>
> Take any fitted model object, say mod[[4]].  For this object the
> formula component of the call will be, literally,  y ~ poly(x, i),
> and not y ~ poly(x, 4), as would be required to use the object,
> e.g. for prediction.  In fact all objects have the same formula.
>
> You could, of course, re-create i and some things would be OK,
> but getting pretty messy.
>
> You would still have a problem if you wanted to plot the fit with
> termplot(), for example, as it would try to do a two-dimensional
> plot of the component if both arguments to poly were variables.
>
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Bill.Venables at csiro.au
> > Sent: 01 August 2006 06:16
> > To: maj at waikato.ac.nz; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Fitting models in a loop
> >
> >
> > Murray,
> >
> > Here is a general paradigm I tend to use for such problems.  It
> extends
> > to fairly general model sequences, including different responses, &c
> >
> > First a couple of tiny, tricky but useful functions:
> >
> > subst <- function(Command, ...) do.call("substitute", list(Command,
> > list(...)))
> >
> > abut <- function(...)  ## jam things tightly together
> >   do.call("paste", c(lapply(list(...), as.character), sep = ""))
> >
> > Name <- function(...) as.name(do.call("abut", list(...)))
> >
> > Now the gist.
> >
> > fitCommand <- quote({
> >         MODELi <- lm(y ~ poly(x, degree = i), theData)
> >         print(summary(MODELi))
> > })
> > for(i in 1:6) {
> >         thisCommand <- subst(fitCommand, MODELi = Name("model_", i), i
> =
> > i)
> >         print(thisCommand)  ## only as a check
> >         eval(thisCommand)
> > }
> >
> > At this point you should have the results and
> >
> > objects(pat = "^model_")
> >
> > should list the fitted model objects, all of which can be updated,
> > summarised, plotted, &c, because the information on their construction
> > is all embedded in the call.
> >
> > Bill.
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray
> Jorgensen
> > Sent: Tuesday, 1 August 2006 2:09 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Fitting models in a loop
> >
> > If I want to display a few polynomial regression fits I can do
> something
> >
> > like
> >
> > for (i in 1:6) {
> >         mod <- lm(y ~ poly(x,i))
> >         print(summary(mod))
> >         }
> >
> > Suppose that I don't want to over-write the fitted model objects,
> > though. How do I create a list of blank fitted model objects for later
>
> > use in a loop?
> >
> > Murray Jorgensen
> > --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.murrell at auckland.ac.nz  Wed Aug  2 04:10:16 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 02 Aug 2006 14:10:16 +1200
Subject: [R] boxplot
In-Reply-To: <E97312684A84D511BDD40002A50968D60822CA15@lxpobw01.ine.pt>
References: <E97312684A84D511BDD40002A50968D60822CA15@lxpobw01.ine.pt>
Message-ID: <44D00988.9050106@stat.auckland.ac.nz>

Hi


Ana Patricia Martins wrote:
> Hello R-users and developers,
> 
>  
> 
> Once again, I'm asking for your help.
> 
> I've used "identify" to identify points in a scatter plot. However, I can't
> apple in the boxplot.....
> 
>  
> 
> I need to identify the outlier's id in the boxplot. Can anyone help me?


The box for the i'th group is at x-location 'i' so ...

boxplot(count ~ spray, data = InsectSprays, col = "lightgray")
with(InsectSprays, identify(unclass(spray), count))

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From p.murrell at auckland.ac.nz  Wed Aug  2 04:13:46 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 02 Aug 2006 14:13:46 +1200
Subject: [R] rgb and col2rgb color conversion/modification/shading
In-Reply-To: <eao9ka$2lf$1@sea.gmane.org>
References: <1154392412.44cea15c0c1fa@webmail.fhcrc.org>
	<eao9ka$2lf$1@sea.gmane.org>
Message-ID: <44D00A5A.5030906@stat.auckland.ac.nz>

Hi


Earl F. Glynn wrote:
> <ccarey at fhcrc.org> wrote in message 
> news:1154392412.44cea15c0c1fa at webmail.fhcrc.org...
>> I want to get a lighter shade of a color...I have a lot of colored objects 
>> and
>> want each one printed as a foreground against a slightly lighter 
>> background.
>>
>> I thought I could try something like changing the alpha channel by first
>> converting it to rgb.
> 
> I'm not sure what you want to do with the alpha channel - it's sometimes 
> used for transparency, especially on Macs, but is not used much on PCs 
> (AFAIK).
> 
> 
> 
> Let's say you want different shades of gold:
> 
>> colors()[142]
> 
> [1] "gold"
> 
> 
> 
> Instead of RGB color space perhaps you should consider HSV 
> (Hue-Saturation-Value) color space.


Or, use a perceptually-based colour space like HCL (Hue Chroma
Luminance);  see the hcl() function and the 'colorspace' package for one
way to convert from RGB.

Paul


> Let's convert "gold" to rgb to hsv:
> 
> 
> 
>> col2rgb( colors()[142] )
> 
>       [,1]
> 
> red    255
> 
> green  215
> 
> blue     0
> 
> 
> 
>> rgb2hsv( col2rgb( colors()[142] ) )
> 
>        [,1]
> 
> h 0.1405229
> 
> s 1.0000000
> 
> v 1.0000000
> 
> 
> 
> The "hue" (h) is the color ranging from 0 to 1 around a color circle (with 
> red= 0 or 1).  Find h = 0.140 ("gold") in this color circle:
> 
> 
> 
> hue <- seq(0.0, 1.0, by=1/40)
> 
> 
> 
> pie(rep(1,40),
> 
>     labels=formatC(hue, digits=3, format="f"), cex=0.75,
> 
>     col=hsv(hue, 1.0, 1.0),
> 
>     radius=1.0,
> 
>     main="HSV (S=1, V=1)" )
> 
> 
> 
> 
> Hues range from 0.0 to 1.0.
> 
> 
> 
> 
> A color is saturated (s=1) when it is "far" from a shade of gray (ranging 
> from black to white).  Grays are unsaturated (no color) colors with s = 0. 
> Saturation ranges from 0.0 to 1.0.
> 
> 
> 
> The value (v) is the brightness of the color.  Low values appear quite dark 
> but still have color.  v=1 is as bright as possible.   Values range from 0.0 
> to 1.0.
> 
> 
> 
> You can get different "shades" of the same color by varying changing the 
> saturation and value for a given hue.  The hsv function returns the RGB 
> color in hex form.
> 
> 
> 
> Consider:
> 
>> hsv(0.1405, 1, 1)
> 
> [1] "#FFD700"
> 
> 
> 
> Hex FF = decimal 255 = red
> 
> Hex D7 = decimal 215 = green
> 
> Hex 00 = decimal 0 = blue
> 
> 
> 
> Let's vary Saturation from 0.0 to 1.0 and Value from 0.0 to 1.0 in this 
> plot:
> 
> 
> 
> 
> 
> MakeHSVRectangle <- function(saturation, value)
> 
> {
> 
>   GoldHue <- 0.140
> 
>   color <- hsv(GoldHue, saturation, value)
> 
>   rect(100*saturation, 100*value, 100*saturation+4, 100*value+4, col=color)
> 
> }
> 
> 
> 
> 
> 
> plot(0:110,0:110, type="n",
> 
>      xlab="Saturation[%]", ylab="Value[%]",
> 
>      main="Shades of Gold, H=0.140")
> 
> outer(seq(0.0, 1.0, 0.05), seq(0.0, 1.0, 0.05), MakeHSVRectangle)
> 
> 
> 
> 
> 
> With Value = 0, all colors are "black".  With Saturation=0, the only 
> "colors" along the y axis are the shades of gray.  The original "gold" 
> rectangle is at the upper right.
> 
> 
> 
> So, given a starting color, you have a number of "shades" (various 
> saturations and values) with the same color hue.
> 
> 
> 
> I hope this helps.
> 
> 
> 
> efg
> 
> Earl F. Glynn
> 
> Scientific Programmer
> 
> Stowers Institute for Medical Research
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From p.murrell at auckland.ac.nz  Wed Aug  2 04:22:38 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 02 Aug 2006 14:22:38 +1200
Subject: [R] placing rectangle behind plot
In-Reply-To: <971536df0607301924u231c052eo8281f107db46dbb9@mail.gmail.com>
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>	
	<44CD3D37.4020106@stat.auckland.ac.nz>
	<971536df0607301924u231c052eo8281f107db46dbb9@mail.gmail.com>
Message-ID: <44D00C6E.6030903@stat.auckland.ac.nz>

Hi


Gabor Grothendieck wrote:
> Thanks.  That's helpful.
> 
> I would be interested in the case where
> 
> 1. one does not have a variable latticeplot, as per your example,
> but just has the output of
> 
>    xyplot(x ~ x | gl(2,1), layout = 1:2)
> 
> sitting on the screen, having been "printed" by a prior
> function.  We can assume that no other graphics have been
> issued since then. Can one still create a grey rectangle behind
> the lower panel?


xyplot(x ~ x | gl(2,1), layout = 1:2)

latticeplot <- grid.grab()
# Continue as for grid.grabExpr() example ...


> 2. In fact, ideally what I would like is to create a function,
> put.in.bg, say, that works something like this:
> 
>    xyplot(x ~ x | gl(2,1), layout = 1:2)
>    trellis.focus("panel", 1, 1)
>    put.in.bg(grid.rect(w = 0.5))
>    trellis.unfocus()
> 
> or maybe
> 
>    xyplot(x ~ x | gl(2,1), layout = 1:2)
>    trellis.focus.bg("panel", 1, 1)
>    grid.rect(w = 0.5)
>    trellis.unfocus()
> 
> That allows one to add objects to a lattice panel behind the objects
> that are already there. This would also be helpful for adding grid
> lines afterwards or other lines, rectangles, etc.


I could imagine something like ...

    xyplot(x ~ x | gl(2,1), layout = 1:2)
    put.in.bg("panel", 1, 1, rectGrob(w = 0.5))

... where you just wrap the approach I described (using grid.grab() to
capture the existing plot, then modifying the resulting grob), but such
a function would obviously not work well when called after something
other than just a trellis plot.

Paul


> On 7/30/06, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
>> Hi
>>
>>
>> Gabor Grothendieck wrote:
>>> I am trying to create a lattice plot and would like to later, i.e. after
>>> the plot is drawn, add a grey rectangle behind a portion of it.
>>> The following works except that the rectrangle is on top of and
>>> obscures a portion of the chart.  I also tried adding col = "transparent"
>>> to the gpar list but that did not help -- I am on windows and
>>> perhaps the windows device does not support transparency?
>>
>> Correct.
>>
>>
>>> At any rate, how can I place the rectangle behind the plotted
>>> points without drawing the rectangle first?
>>>
>>> library(lattice)
>>> library(grid)
>>> trellis.unfocus()
>>> x <- 1:10
>>> xyplot(x ~ x | gl(2,1), layout = 1:2)
>>> trellis.focus("panel", 1, 1)
>>> grid.rect(w = .5, gp = gpar(fill = "light grey"))
>>> trellis.unfocus()
>>
>> The user-interface is a little rough, but this can be done by accessing
>> the underlying grid objects.  Here's an example, with explanatory bits
>> interspersed ...
>>
>> # "grab" the lattice plot as a grid gTree
>> # There are warnings, but they are ignorable
>> latticeplot <- grid.grabExpr(print(xyplot(x ~ x | gl(2,1),
>>                                          layout = 1:2)))
>>
>> # Demonstrate that the gTree faithfully replicates the
>> # original lattice plot (not necessary, just to to what's going on)
>> grid.newpage()
>> grid.draw(latticeplot)
>>
>> # Explore the gTree (just to to show what's going on)
>> # Better user-interface would be nice here ...
>> childNames(latticeplot)
>> # Identify which children are which
>> # (appropriate grob names would be nice here)
>> lapply(latticeplot$children, class)
>> # Identify where each child is drawn
>> latticeplot$childrenvp
>> lapply(latticeplot$children, "[[", "vp")
>>
>> # Add a rect (starts off on top of everything else)
>> # NOTE that rect has to have correct vpPath
>> plotwithrect <- addGrob(latticeplot,
>>                        rectGrob(w = .5, gp = gpar(fill = "light grey"),
>>                                 vp=vpPath("plot1.toplevel.vp",
>>                                           "plot1.panel.1.1.vp")))
>>
>> # Check this draws what we expect (just to show what's going on)
>> grid.newpage()
>> grid.draw(plotwithrect)
>>
>> # Reorder children to put rect at back
>> # Appropriate user-interface would be nice here ...
>> nc <- length(plotwithrect$childrenOrder)
>> plotwithrect$childrenOrder <-
>>    plotwithrect$childrenOrder[c(nc, 1:(nc - 1))]
>>
>> # Final result
>> grid.newpage()
>> grid.draw(plotwithrect)
>>
>> Paul
>> --
>> Dr Paul Murrell
>> Department of Statistics
>> The University of Auckland
>> Private Bag 92019
>> Auckland
>> New Zealand
>> 64 9 3737599 x85392
>> paul at stat.auckland.ac.nz
>> http://www.stat.auckland.ac.nz/~paul/
>>
>>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ggrothendieck at gmail.com  Wed Aug  2 04:37:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Aug 2006 22:37:36 -0400
Subject: [R] Fitting models in a loop
In-Reply-To: <971536df0608011900k36c60f1ax8522fcb675eaca2f@mail.gmail.com>
References: <B998A44C8986644EA8029CFE6396A9245475CD@exqld2-bne.qld.csiro.au>
	<971536df0608011900k36c60f1ax8522fcb675eaca2f@mail.gmail.com>
Message-ID: <971536df0608011937y6de3878ak4c68cbd6d5f9fe73@mail.gmail.com>

Actually in thinking about this some more that still gets you
into a mess if you want to do prediction at anything other
than the original points.

On 8/1/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> A simple way around this is to pass it as a data frame.
> In the code below the only change we made was to change
> the formula from y ~ poly(x, i) to y ~ . and pass poly(x,i)
> in a data frame as argument 2 of lm:
>
> # test data
> set.seed(1)
> x <- 1:10
> y <- x^3 + rnorm(10)
>
> # run same code except change the lm call
> mod <- list()
> for (i in 1:3) {
>        mod[[i]] <- lm(y ~., data.frame(poly(x, i)))
>        print(summary(mod[[i]]))
> }
>
> After running the above we can test that it works:
>
> > for(i in 1:3) print(formula(mod[[i]]))
> y ~ X1
> y ~ X1 + X2
> y ~ X1 + X2 + X3
>
> On 8/1/06, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> >
> > Markus Gesmann writes:
> >
> > > Murray,
> > >
> > > How about creating an empty list and filling it during your loop:
> > >
> > >  mod <- list()
> > >  for (i in 1:6) {
> > >         mod[[i]] <- lm(y ~ poly(x,i))
> > >         print(summary(mod[[i]]))
> > >         }
> > >
> > > All your models are than stored in one object and you can use lapply
> > to
> > > do something on them, like:
> > >  lapply(mod, summary) or lapply(mod, coef)
> >
> > I think it is important to see why this deceptively simple
> > solution does not achieve the result that Murray wanted.
> >
> > Take any fitted model object, say mod[[4]].  For this object the
> > formula component of the call will be, literally,  y ~ poly(x, i),
> > and not y ~ poly(x, 4), as would be required to use the object,
> > e.g. for prediction.  In fact all objects have the same formula.
> >
> > You could, of course, re-create i and some things would be OK,
> > but getting pretty messy.
> >
> > You would still have a problem if you wanted to plot the fit with
> > termplot(), for example, as it would try to do a two-dimensional
> > plot of the component if both arguments to poly were variables.
> >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > Bill.Venables at csiro.au
> > > Sent: 01 August 2006 06:16
> > > To: maj at waikato.ac.nz; r-help at stat.math.ethz.ch
> > > Subject: Re: [R] Fitting models in a loop
> > >
> > >
> > > Murray,
> > >
> > > Here is a general paradigm I tend to use for such problems.  It
> > extends
> > > to fairly general model sequences, including different responses, &c
> > >
> > > First a couple of tiny, tricky but useful functions:
> > >
> > > subst <- function(Command, ...) do.call("substitute", list(Command,
> > > list(...)))
> > >
> > > abut <- function(...)  ## jam things tightly together
> > >   do.call("paste", c(lapply(list(...), as.character), sep = ""))
> > >
> > > Name <- function(...) as.name(do.call("abut", list(...)))
> > >
> > > Now the gist.
> > >
> > > fitCommand <- quote({
> > >         MODELi <- lm(y ~ poly(x, degree = i), theData)
> > >         print(summary(MODELi))
> > > })
> > > for(i in 1:6) {
> > >         thisCommand <- subst(fitCommand, MODELi = Name("model_", i), i
> > =
> > > i)
> > >         print(thisCommand)  ## only as a check
> > >         eval(thisCommand)
> > > }
> > >
> > > At this point you should have the results and
> > >
> > > objects(pat = "^model_")
> > >
> > > should list the fitted model objects, all of which can be updated,
> > > summarised, plotted, &c, because the information on their construction
> > > is all embedded in the call.
> > >
> > > Bill.
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray
> > Jorgensen
> > > Sent: Tuesday, 1 August 2006 2:09 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Fitting models in a loop
> > >
> > > If I want to display a few polynomial regression fits I can do
> > something
> > >
> > > like
> > >
> > > for (i in 1:6) {
> > >         mod <- lm(y ~ poly(x,i))
> > >         print(summary(mod))
> > >         }
> > >
> > > Suppose that I don't want to over-write the fitted model objects,
> > > though. How do I create a list of blank fitted model objects for later
> >
> > > use in a loop?
> > >
> > > Murray Jorgensen
> > > --
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From jennytimp at hotmail.com  Wed Aug  2 04:46:29 2006
From: jennytimp at hotmail.com (jenny tan)
Date: Wed, 02 Aug 2006 10:46:29 +0800
Subject: [R] Data transformation
Message-ID: <BAY118-F29FED50EEC6D9B9E06AE56B9520@phx.gbl>

Hi there,

Wonder if someone who is R-savvy can help me with the following task (see 
below) that I occasionally do work and gets quite tedious if I do it 
manually. Thanks in advance!

jenny.

-----------------

I have a column of data that looks like this:
NA18501
NA18502
NA18504
NA18505
NA18507
NA18508
NA18516
NA18517
NA18522
NA18523

And I want to duplicate the values and sort of "interweave" them to look 
like this:

NA18501
NA18501
NA18502
NA18502
NA18504
NA18504
NA18505
NA18505
NA18507
NA18507
NA18508
NA18508
NA18516
NA18516
NA18517
NA18517
NA18522
NA18522
NA18523
NA18523


From blomsp at ozemail.com.au  Wed Aug  2 05:01:01 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Wed, 02 Aug 2006 13:01:01 +1000
Subject: [R] Data transformation
In-Reply-To: <BAY118-F29FED50EEC6D9B9E06AE56B9520@phx.gbl>
References: <BAY118-F29FED50EEC6D9B9E06AE56B9520@phx.gbl>
Message-ID: <44D0156D.2070306@ozemail.com.au>

If x is your data, and is a vector, then

rep(x, each=2)

should do it for you.

Cheers,

Simon.

jenny tan wrote:
> Hi there,
>
> Wonder if someone who is R-savvy can help me with the following task (see 
> below) that I occasionally do work and gets quite tedious if I do it 
> manually. Thanks in advance!
>
> jenny.
>
> -----------------
>
> I have a column of data that looks like this:
> NA18501
> NA18502
> NA18504
> NA18505
> NA18507
> NA18508
> NA18516
> NA18517
> NA18522
> NA18523
>
> And I want to duplicate the values and sort of "interweave" them to look 
> like this:
>
> NA18501
> NA18501
> NA18502
> NA18502
> NA18504
> NA18504
> NA18505
> NA18505
> NA18507
> NA18507
> NA18508
> NA18508
> NA18516
> NA18516
> NA18517
> NA18517
> NA18522
> NA18522
> NA18523
> NA18523
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From binabina at bellsouth.net  Wed Aug  2 05:02:28 2006
From: binabina at bellsouth.net (zubin)
Date: Tue, 01 Aug 2006 23:02:28 -0400
Subject: [R] zero values in LHS and RHS
In-Reply-To: <44A70DB2.9080106@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
Message-ID: <44D015C4.6080207@bellsouth.net>

Hello!

I have a data set where i need to predict hotel stays in a 3 month 
period by customer.  I have stays for each customer in the 3 month 
period and the previous 3 month period + other variables.  these stays 
contain an integer that ranges from 0 to 10.  classic poisson distribution.

my question is around zeros.  usually we could take logs on the stays 
and run an OLS model i think an enhancement would be to fit a poisson 
GLM model.  i am confused about handling of zeros.  i will have zero 
values on the RHS and potentially on the LHS.  It seems like the poisson 
GLM model can handle zeros on the LHS and RHS.   i guess my question is 
fitting a poisson GLM, are zeros allowed?  If so, how is this estimated 
with a log link function?

-zubin


From jholtman at gmail.com  Wed Aug  2 05:08:18 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 1 Aug 2006 23:08:18 -0400
Subject: [R] Data transformation
In-Reply-To: <BAY118-F29FED50EEC6D9B9E06AE56B9520@phx.gbl>
References: <BAY118-F29FED50EEC6D9B9E06AE56B9520@phx.gbl>
Message-ID: <644e1f320608012008u7dc2ebb6hd0d3565d542d45f0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/adb396bf/attachment.pl 

From Bill.Venables at csiro.au  Wed Aug  2 05:21:51 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Wed, 2 Aug 2006 13:21:51 +1000
Subject: [R] Fitting models in a loop
Message-ID: <B998A44C8986644EA8029CFE6396A9245475D8@exqld2-bne.qld.csiro.au>

This (below) also runs into trouble if you try to predict with new data
since you have no rule for re-constructing the formula.  Also, you can't
plot the term as a single contributor to the linear predictor with
termplot().

I'm sure given enough ingenuity you can get round these two, but why
avoid the language manipulation solution, when it does the lot?

Bill.


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, 2 August 2006 12:01 PM
To: Venables, Bill (CMIS, Cleveland)
Cc: Markus.Gesmann at lloyds.com; maj at waikato.ac.nz;
r-help at stat.math.ethz.ch
Subject: Re: [R] Fitting models in a loop

A simple way around this is to pass it as a data frame.
In the code below the only change we made was to change
the formula from y ~ poly(x, i) to y ~ . and pass poly(x,i)
in a data frame as argument 2 of lm:

# test data
set.seed(1)
x <- 1:10
y <- x^3 + rnorm(10)

# run same code except change the lm call
mod <- list()
for (i in 1:3) {
        mod[[i]] <- lm(y ~., data.frame(poly(x, i)))
        print(summary(mod[[i]]))
}

After running the above we can test that it works:

> for(i in 1:3) print(formula(mod[[i]]))
y ~ X1
y ~ X1 + X2
y ~ X1 + X2 + X3

On 8/1/06, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
>
> Markus Gesmann writes:
>
> > Murray,
> >
> > How about creating an empty list and filling it during your loop:
> >
> >  mod <- list()
> >  for (i in 1:6) {
> >         mod[[i]] <- lm(y ~ poly(x,i))
> >         print(summary(mod[[i]]))
> >         }
> >
> > All your models are than stored in one object and you can use lapply
> to
> > do something on them, like:
> >  lapply(mod, summary) or lapply(mod, coef)
>
> I think it is important to see why this deceptively simple
> solution does not achieve the result that Murray wanted.
>
> Take any fitted model object, say mod[[4]].  For this object the
> formula component of the call will be, literally,  y ~ poly(x, i),
> and not y ~ poly(x, 4), as would be required to use the object,
> e.g. for prediction.  In fact all objects have the same formula.
>
> You could, of course, re-create i and some things would be OK,
> but getting pretty messy.
>
> You would still have a problem if you wanted to plot the fit with
> termplot(), for example, as it would try to do a two-dimensional
> plot of the component if both arguments to poly were variables.
>
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Bill.Venables at csiro.au
> > Sent: 01 August 2006 06:16
> > To: maj at waikato.ac.nz; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Fitting models in a loop
> >
> >
> > Murray,
> >
> > Here is a general paradigm I tend to use for such problems.  It
> extends
> > to fairly general model sequences, including different responses, &c
> >
> > First a couple of tiny, tricky but useful functions:
> >
> > subst <- function(Command, ...) do.call("substitute", list(Command,
> > list(...)))
> >
> > abut <- function(...)  ## jam things tightly together
> >   do.call("paste", c(lapply(list(...), as.character), sep = ""))
> >
> > Name <- function(...) as.name(do.call("abut", list(...)))
> >
> > Now the gist.
> >
> > fitCommand <- quote({
> >         MODELi <- lm(y ~ poly(x, degree = i), theData)
> >         print(summary(MODELi))
> > })
> > for(i in 1:6) {
> >         thisCommand <- subst(fitCommand, MODELi = Name("model_", i),
i
> =
> > i)
> >         print(thisCommand)  ## only as a check
> >         eval(thisCommand)
> > }
> >
> > At this point you should have the results and
> >
> > objects(pat = "^model_")
> >
> > should list the fitted model objects, all of which can be updated,
> > summarised, plotted, &c, because the information on their
construction
> > is all embedded in the call.
> >
> > Bill.
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray
> Jorgensen
> > Sent: Tuesday, 1 August 2006 2:09 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Fitting models in a loop
> >
> > If I want to display a few polynomial regression fits I can do
> something
> >
> > like
> >
> > for (i in 1:6) {
> >         mod <- lm(y ~ poly(x,i))
> >         print(summary(mod))
> >         }
> >
> > Suppose that I don't want to over-write the fitted model objects,
> > though. How do I create a list of blank fitted model objects for
later
>
> > use in a loop?
> >
> > Murray Jorgensen
> > --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Aug  2 05:57:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Aug 2006 23:57:37 -0400
Subject: [R] Fitting models in a loop
In-Reply-To: <971536df0608011937y6de3878ak4c68cbd6d5f9fe73@mail.gmail.com>
References: <B998A44C8986644EA8029CFE6396A9245475CD@exqld2-bne.qld.csiro.au>
	<971536df0608011900k36c60f1ax8522fcb675eaca2f@mail.gmail.com>
	<971536df0608011937y6de3878ak4c68cbd6d5f9fe73@mail.gmail.com>
Message-ID: <971536df0608012057q42b7209wb201e5241356b61c@mail.gmail.com>

Here is another attempt.  This one allows general prediction
yet its actually shorter and does not use any advanced
language constructs (although to understand why it works
one must understand that formulas have environments
and the environment of the formula corresponding to each
component of mod is the environment within the anonymous
function instance that created it):

# test data - as before
set.seed(1)
x <- 1:10
y <- x^3 + rnorm(10)

mod <- lapply(1:3, function(i) lm(y ~ poly(x,i)))
print(mod)

# test - each component of mod remembers its 'i'
# This returns 1, 2 and 3 as required.
for (j in 1:3) print(environment(formula(mod[[j]]))$i)

# following two lines give same answer
# showing prediction works
predict(mod[[2]], list(x = 1:10))
fitted(lm(y ~ poly(x,2)))

On 8/1/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Actually in thinking about this some more that still gets you
> into a mess if you want to do prediction at anything other
> than the original points.
>
> On 8/1/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > A simple way around this is to pass it as a data frame.
> > In the code below the only change we made was to change
> > the formula from y ~ poly(x, i) to y ~ . and pass poly(x,i)
> > in a data frame as argument 2 of lm:
> >
> > # test data
> > set.seed(1)
> > x <- 1:10
> > y <- x^3 + rnorm(10)
> >
> > # run same code except change the lm call
> > mod <- list()
> > for (i in 1:3) {
> >        mod[[i]] <- lm(y ~., data.frame(poly(x, i)))
> >        print(summary(mod[[i]]))
> > }
> >
> > After running the above we can test that it works:
> >
> > > for(i in 1:3) print(formula(mod[[i]]))
> > y ~ X1
> > y ~ X1 + X2
> > y ~ X1 + X2 + X3
> >
> > On 8/1/06, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> > >
> > > Markus Gesmann writes:
> > >
> > > > Murray,
> > > >
> > > > How about creating an empty list and filling it during your loop:
> > > >
> > > >  mod <- list()
> > > >  for (i in 1:6) {
> > > >         mod[[i]] <- lm(y ~ poly(x,i))
> > > >         print(summary(mod[[i]]))
> > > >         }
> > > >
> > > > All your models are than stored in one object and you can use lapply
> > > to
> > > > do something on them, like:
> > > >  lapply(mod, summary) or lapply(mod, coef)
> > >
> > > I think it is important to see why this deceptively simple
> > > solution does not achieve the result that Murray wanted.
> > >
> > > Take any fitted model object, say mod[[4]].  For this object the
> > > formula component of the call will be, literally,  y ~ poly(x, i),
> > > and not y ~ poly(x, 4), as would be required to use the object,
> > > e.g. for prediction.  In fact all objects have the same formula.
> > >
> > > You could, of course, re-create i and some things would be OK,
> > > but getting pretty messy.
> > >
> > > You would still have a problem if you wanted to plot the fit with
> > > termplot(), for example, as it would try to do a two-dimensional
> > > plot of the component if both arguments to poly were variables.
> > >
> > > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > Bill.Venables at csiro.au
> > > > Sent: 01 August 2006 06:16
> > > > To: maj at waikato.ac.nz; r-help at stat.math.ethz.ch
> > > > Subject: Re: [R] Fitting models in a loop
> > > >
> > > >
> > > > Murray,
> > > >
> > > > Here is a general paradigm I tend to use for such problems.  It
> > > extends
> > > > to fairly general model sequences, including different responses, &c
> > > >
> > > > First a couple of tiny, tricky but useful functions:
> > > >
> > > > subst <- function(Command, ...) do.call("substitute", list(Command,
> > > > list(...)))
> > > >
> > > > abut <- function(...)  ## jam things tightly together
> > > >   do.call("paste", c(lapply(list(...), as.character), sep = ""))
> > > >
> > > > Name <- function(...) as.name(do.call("abut", list(...)))
> > > >
> > > > Now the gist.
> > > >
> > > > fitCommand <- quote({
> > > >         MODELi <- lm(y ~ poly(x, degree = i), theData)
> > > >         print(summary(MODELi))
> > > > })
> > > > for(i in 1:6) {
> > > >         thisCommand <- subst(fitCommand, MODELi = Name("model_", i), i
> > > =
> > > > i)
> > > >         print(thisCommand)  ## only as a check
> > > >         eval(thisCommand)
> > > > }
> > > >
> > > > At this point you should have the results and
> > > >
> > > > objects(pat = "^model_")
> > > >
> > > > should list the fitted model objects, all of which can be updated,
> > > > summarised, plotted, &c, because the information on their construction
> > > > is all embedded in the call.
> > > >
> > > > Bill.
> > > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray
> > > Jorgensen
> > > > Sent: Tuesday, 1 August 2006 2:09 PM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] Fitting models in a loop
> > > >
> > > > If I want to display a few polynomial regression fits I can do
> > > something
> > > >
> > > > like
> > > >
> > > > for (i in 1:6) {
> > > >         mod <- lm(y ~ poly(x,i))
> > > >         print(summary(mod))
> > > >         }
> > > >
> > > > Suppose that I don't want to over-write the fitted model objects,
> > > > though. How do I create a list of blank fitted model objects for later
> > >
> > > > use in a loop?
> > > >
> > > > Murray Jorgensen
> > > > --
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From gregor.gorjanc at bfro.uni-lj.si  Wed Aug  2 06:05:13 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Wed, 2 Aug 2006 04:05:13 +0000 (UTC)
Subject: [R] zero values in LHS and RHS
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44D015C4.6080207@bellsouth.net>
Message-ID: <loom.20060802T060145-343@post.gmane.org>

zubin <binabina <at> bellsouth.net> writes:

> 
> Hello!
> 
> I have a data set where i need to predict hotel stays in a 3 month 
> period by customer.  I have stays for each customer in the 3 month 
> period and the previous 3 month period + other variables.  these stays 
> contain an integer that ranges from 0 to 10.  classic poisson distribution.
> 
> my question is around zeros.  usually we could take logs on the stays 
> and run an OLS model i think an enhancement would be to fit a poisson 
> GLM model.  i am confused about handling of zeros.  i will have zero 
> values on the RHS and potentially on the LHS.  It seems like the poisson 
> GLM model can handle zeros on the LHS and RHS.   i guess my question is 
> fitting a poisson GLM, are zeros allowed?  If so, how is this estimated 
> with a log link function?
> 

I am not sure about the structure of LHS and RHS matrices, but yes, 
Poisson model can handle zeroes. If it turns out that there are to many 
zeroes as expected, you might try ZIP model.

Log link is the default for Poisson and it links the expected value of 
Poisson distribution and not the values itself.

Gregor


From gyadav at ccilindia.co.in  Wed Aug  2 07:08:45 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 2 Aug 2006 10:38:45 +0530
Subject: [R] open DLL in R
In-Reply-To: <Pine.LNX.4.64.0608011844120.21448@gannet.stats.ox.ac.uk>
Message-ID: <OF1EBA4261.DD7E7C49-ON652571BE.001C0079-652571A4.001C6C1A@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/31981822/attachment.pl 

From ggrothendieck at gmail.com  Wed Aug  2 07:23:27 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Aug 2006 01:23:27 -0400
Subject: [R] open DLL in R
In-Reply-To: <OF1EBA4261.DD7E7C49-ON652571BE.001C0079-652571A4.001C6C1A@ccilindia.co.in>
References: <Pine.LNX.4.64.0608011844120.21448@gannet.stats.ox.ac.uk>
	<OF1EBA4261.DD7E7C49-ON652571BE.001C0079-652571A4.001C6C1A@ccilindia.co.in>
Message-ID: <971536df0608012223p1f0ac47dg3050d96314486758@mail.gmail.com>

This refers to the windows command pedump.exe found in the Rtools
collection at:

   http://www.murdoch-sutherland.com/Rtools/

On 8/2/06, gyadav at ccilindia.co.in <gyadav at ccilindia.co.in> wrote:
>
> Hello Sir,
>
> I am just wondering that pedump is a command of 'R' because in could not
> find in the 'R' help using help.search("pedump"). I am requesting you to
> narrate as i also have to look into .dll(s). Is there any way to know what
> are the exported functions and constants and imported functions and
> constants in a easy way.
>
> thanks
> -gaurav.
>
>
>
> Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Sent by: r-help-bounces at stat.math.ethz.ch
> 01-08-06 11:18 PM
>
> To
> qian li <hidorothy1979 at yahoo.com>
> cc
> r-help at stat.math.ethz.ch
> Subject
> Re: [R] open DLL in R
>
>
>
>
>
>
> On Tue, 1 Aug 2006, qian li wrote:
>
> > I have downloaded a DLL file. I want to look at the contents in the DLL
> file. How can I do it in R?
>
> You need a disassembler such as VC++'s DUMPBIN, but looking at compiled
> code you did not write is not an easy task.  (Or objdump from the MinGW
> toolset.)
>
> If only you want to know what entry points it exports, use pedump -e for
> the pedump.exe in tools.zip (see the R-admin manual).
>
> What has this to do with R?
>
> >
> >   Thanks,
> >
> >   QL
> >
> >
> > ---------------------------------
> >
> >
> >                [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Wed Aug  2 07:52:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Aug 2006 06:52:19 +0100 (BST)
Subject: [R] open DLL in R
In-Reply-To: <OF1EBA4261.DD7E7C49-ON652571BE.001C0079-652571A4.001C6C1A@ccilindia.co.in>
References: <OF1EBA4261.DD7E7C49-ON652571BE.001C0079-652571A4.001C6C1A@ccilindia.co.in>
Message-ID: <Pine.LNX.4.64.0608020651470.13686@gannet.stats.ox.ac.uk>

On Wed, 2 Aug 2006, gyadav at ccilindia.co.in wrote:

> 
> Hello Sir,
> 
> I am just wondering that pedump is a command of 'R' because in could not 

It is not, as I stated.

> find in the 'R' help using help.search("pedump"). I am requesting you to 
> narrate as i also have to look into .dll(s). Is there any way to know what 
> are the exported functions and constants and imported functions and 
> constants in a easy way.

Yes, see the R-admin manual, using non-R tools.

> thanks
> -gaurav.
> 
> 
> 
> Prof Brian Ripley <ripley at stats.ox.ac.uk> 
> Sent by: r-help-bounces at stat.math.ethz.ch
> 01-08-06 11:18 PM
> 
> To
> qian li <hidorothy1979 at yahoo.com>
> cc
> r-help at stat.math.ethz.ch
> Subject
> Re: [R] open DLL in R
> 
> 
> 
> 
> 
> 
> On Tue, 1 Aug 2006, qian li wrote:
> 
> > I have downloaded a DLL file. I want to look at the contents in the DLL 
> file. How can I do it in R?
> 
> You need a disassembler such as VC++'s DUMPBIN, but looking at compiled 
> code you did not write is not an easy task.  (Or objdump from the MinGW 
> toolset.)
> 
> If only you want to know what entry points it exports, use pedump -e for 
> the pedump.exe in tools.zip (see the R-admin manual).
> 
> What has this to do with R?
> 
> > 
> >   Thanks,
> > 
> >   QL
> > 
> > 
> > ---------------------------------
> > 
> > 
> >                [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Aug  2 08:05:37 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Aug 2006 07:05:37 +0100 (BST)
Subject: [R] plot() with TukeyHSD
In-Reply-To: <4619CB1B8D625544BF3764E6B346DA1F6411A4@helena.richmond.edu>
References: <4619CB1B8D625544BF3764E6B346DA1F6411A4@helena.richmond.edu>
Message-ID: <Pine.LNX.4.64.0608020659520.13843@gannet.stats.ox.ac.uk>

On Tue, 1 Aug 2006, Owen, Jason wrote:

> Hello,
> 
> When plotting the results of a TukeyHSD multiple comparisons
> procedure with an ANOVA (lm) object, an extra line appears
> in the confidence intervals that contain 0.  For example (this
> is straight from the TukeyHSD helpfile):
> 
> > summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
> > TukeyHSD(fm1, "tension", ordered = TRUE)
> > plot(TukeyHSD(fm1, "tension"))
> 
> An extra line segment appears in the interval to the right of the 
> dotted zero line.  Coincidentally, if all of the calculated 
> intervals for a dataset don't contain zero, this isn't a problem 
> -- so I guess the line segment "belongs" to the zero line.  I
> checked R Search and didn't find this mentioned before... but
> please let me know if I overlooked something.

You overlooked the following in the posting guide

  If you are using an old version of R and think it does not work 
  properly, upgrade to the latest version and try that, before posting. 

The NEWS for 2.2.1 says

    o	The plot() method for TukeyHSD() needed updating after adding
	adjusted p-values.  (PR#8229)

which is probably what you are referring to.

> 
> Jason
> 
> R 2.2.0 on Windows 

Far too old.

> -- BTW, the error didn't happen on my Mac OSX w/ R 1.9.1
> 
> --
> Assistant Professor of Statistics
> Mathematics and Computer Science Department
> University of Richmond, Virginia 23173
> (804) 289-8081   fax:(804) 287-6664
> http://www.mathcs.richmond.edu/~wowen
> 
> "This is R. There is no if. Only how."
> Simon (Yoda) Blomberg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From nmi13 at ext.canterbury.ac.nz  Wed Aug  2 08:43:01 2006
From: nmi13 at ext.canterbury.ac.nz (nmi13)
Date: Wed, 02 Aug 2006 18:43:01 +1200
Subject: [R] RE
Message-ID: <44D18E90@webmail>

Hi any,

Can some please detail me the createX command in bayesm package?

To make things easy for you to help me, let me put forward my problem

Suppose I have 3 covariates (say X matrix) and my Y has 3 categories say 
(1,2,3). Now from the CreateX I understand that the data matrix say 'Xa' must 
be of dimension n* (naxp), where 'na' is the number of variables and 'p' is 
the number of categories that Y has and 'n' is the number of observations. Now 
the Xa matrix will have 9 columns if I give na=3 and p=3, and 6 columns if I 
give na =2 and p=3. I understand this part. In order to create Xa with a 
dimension of n*9 or n*6 we have to create Xa as cbind(Xa,-Xa) and now when I 
get the design matrix say XD then I have 5 or 4 variables, which will be same 
as the beta matrix that  I intend to get, I get this, but my question is when 
I get the XD matrix as explained below the covariate matrix (X) which 
initially had  3 columns now changed to a  9 rows and one column and two 
additional variables X4 and X5 as explained belwo which I guess for the beta, 
what is the role of these variables in the 'rmnlIndepMetrop'analysis.
example
x1<-runif(3,-1,1)
x2<-runif(3,0,1)
x3<-runif(3,10,50)
X<-cbind(x1,x2,x3)
X
             x1        x2       x3
[1,] -0.9701396 0.4084203 41.31097
[2,]  0.3844539 0.4791997 36.85861
[3,]  0.2732056 0.5433642 13.14610

Xa<-cbind(X,-X)
XD<-createX(p=3,na=2,nd=NULL,Xa=Xa,Xd=NULL)
XD
      [,1] [,2]       [,3]        [,4]
 [1,]    1    0 -0.9701396   0.9701396
 [2,]    0    1  0.4084203  -0.4084203
 [3,]    0    0 41.3109655 -41.3109655
 [4,]    1    0  0.3844539  -0.3844539
 [5,]    0    1  0.4791997  -0.4791997
 [6,]    0    0 36.8586070 -36.8586070
 [7,]    1    0  0.2732056  -0.2732056
 [8,]    0    1  0.5433642  -0.5433642
 [9,]    0    0 13.1461040 -13.1461040
Xa<-cbind(X,-X,X^2) (is this a correct way)
XD<-createX(p=3,na=3,nd=NULL,Xa=Xa,Xd=NULL)
XD
      [,1] [,2]       [,3]        [,4]         [,5]
 [1,]    1    0 -0.9701396   0.9701396    0.9411709
 [2,]    0    1  0.4084203  -0.4084203    0.1668071
 [3,]    0    0 41.3109655 -41.3109655 1706.5958746
 [4,]    1    0  0.3844539  -0.3844539    0.1478048
 [5,]    0    1  0.4791997  -0.4791997    0.2296324
 [6,]    0    0 36.8586070 -36.8586070 1358.5569127
 [7,]    1    0  0.2732056  -0.2732056    0.0746413
 [8,]    0    1  0.5433642  -0.5433642    0.2952447
 [9,]    0    0 13.1461040 -13.1461040  172.8200512

In the above example my X matrix as you can see has 3 columns with 3 
observations, which now in XD are 9 observations in 3rd column, I don't know 
how col 4 and col 5 of XD play a role in computing the llmnl and 
rmnlIndepMetrop.

Thanks for all your help and time.

Regards,
Murthy.


From epistat at gmail.com  Wed Aug  2 09:57:40 2006
From: epistat at gmail.com (zhijie zhang)
Date: Wed, 2 Aug 2006 15:57:40 +0800
Subject: [R] questions on aggregate data
Message-ID: <2fc17e30608020057r5115d6c3q5196266531d476ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/8878474f/attachment.pl 

From garbuzov at resnet.sfu.ca  Wed Aug  2 10:03:45 2006
From: garbuzov at resnet.sfu.ca (Boris Garbuzov)
Date: Wed, 2 Aug 2006 01:03:45 -0700
Subject: [R] rpad, leaps, regsubsets
References: <005301c6b5b4$0736f220$711f210a@gne.windows.gene.com>
Message-ID: <006c01c6b60a$2d895160$521f6e41@home>

Thanks for the resources, Berton. but unfortunately, that rpad link fails, 
and I still do not know where to get leaps or regsubsets functions. 
Sincerely, Boris.
-------------- 
Hello, dear r team. Please help the newbie. My r is not finding leaps or 
regsubsets finctions. What should I do? Any name changes or library loading 
issues?
-------------------------
Boris Garbuzov
E-mail: bgarbuzo at sfu.ca
ICQ:  146995300
MSN: garbuzov at hotmail.com
Residence: 3007 Hamilton Hall, 8888 University Drive, Burnaby BC, V5A 1S6, 
Canada
Telephone: 604-339-9964 (cell), 1.888.9.555.777 (toll free).
----- Original Message ----- 
From: "Berton Gunter" <gunter.berton at gene.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, August 01, 2006 2:47 PM
Subject: [R] R Reference Card and other help (especially useful for Newbies)


>
> Hi all:
>
>
> Newbies (and others!) may find useful the R Reference Card made available 
> by
>
> Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or 
> through
> the "Contributed" link on CRAN (where some other reference cards are also
> linked). It categorizes and organizes a bunch of R's basic, most used
> functions so that they can be easily found. For example, paste() is under
> the "Strings" heading and expand.grid() is under "Data Creation." For
> newbies struggling to find the right R function as well as veterans who
> can't quite remember the function name, it's very handy.
>
> Also don't forget R's other Help facilties:
>
> help.search("keyword or phrase") to search the **installed** man pages
>


From jacques.veslot at good.ibl.fr  Wed Aug  2 10:11:11 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 02 Aug 2006 10:11:11 +0200
Subject: [R] questions on aggregate data
In-Reply-To: <2fc17e30608020057r5115d6c3q5196266531d476ef@mail.gmail.com>
References: <2fc17e30608020057r5115d6c3q5196266531d476ef@mail.gmail.com>
Message-ID: <44D05E1F.6010005@good.ibl.fr>

data.frame(x = with(df1, rep(x, freq)))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


zhijie zhang a ?crit :
> Dear friends,
>  my question is how to aggregate dataset  and the inverse manipulation.
> e.g.My dataset
> data structure1:
> x
>  1
> 1
> 2
> 3
> 3
> data structure2:
> x  freq
> 1  2
> 2  1
> 3  2
> Then how to generate dataset2 from dataset1 and generate dataset1 from
> dataset2?
> 
> e.g. dataset2 from dataset1 :
> x<-c(1,1,2,3,3)
> a<-tab(x)
> as.data.frame(a)
> 
> *But i can't do the inverse manipulation:generate dataset1 from dataset2*,
> anybody can help me on the two different manipulations?
> 
> Thanks a lot!
> 
> 
> 
> 
>


From spencer.graves at pdf.com  Wed Aug  2 10:25:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Aug 2006 01:25:01 -0700
Subject: [R] Correlation adjusted Bonferroni? (was: Multiple tests on
 repeated measurements)
In-Reply-To: <63E04C5ADEDACB4989972239CDABF05782BEEE@chlsne01.nestle.com>
References: <63E04C5ADEDACB4989972239CDABF05782BEEE@chlsne01.nestle.com>
Message-ID: <44D0615D.9090702@pdf.com>

	  I'm not familiar with the correlation adjustment to Bonferroni you 
mention below, though it sounds interesting.  However, I think there is 
something not right about it or about how you have interpreted it.  Your 
code produced the following for me:

	 p.value.raw p.value.bon p.value.adj
           = raw.p      = bon.p   =multcomp.p "bon.cor.p"
diff/v=0 0.028572509 0.057145019 0.054951102 0.034934913
diff/v=1 0.001727993 0.003455987 0.003415545 0.002119276

	  In the absence of other information, I'd be inclined to believe 
csimint(..)$p.value.adj or ..$p.value.bon over your "bon.cor.p".

	
	  Hope this helps.
	  Spencer Graves

Grathwohl, Dominik, LAUSANNE, NRC-BAS wrote:
> Dear R-helpers:
> 
> My question is how do I efficient and valid correct for multiple tests in a repeated measurement design: 
> Suppose we measure at two distinct visits with repeated subjects a treatment difference on the same variable. 
> The treatment differences are assessed with a mixed model and adjusted by two methods for multiple tests:
> 
> # 1. Method: Adjustment with library(multcomp)
> 
> library(nlme)
> library(multcomp)
> 
> n <- 30 # number of subjects
> sd1 <- 0.5 # Standard deviation of the random intercept
> sd2 <- 0.8 # Standard deviation of the residuals
> id <- rep(1:n,times=2); v <- rep(0:1, each=n); trt <- rep(sample(rep(0:1, each=n/2), n), times=2)
> df <- data.frame(id, v, trt, 
> y=2 + rep(rnorm(10,0,sd1), times=2) + 0.5*v + 0.7*trt + 0.2*v*trt + rnorm(2*n, 0, sd2))
> m1 <- lme(y ~ v + trt + v*trt, data=df, random= ~ 1|id)
> summary(m1)
> par4 <- m1$coef$fixed
> cov4 <- vcov(m1)
> cm4 <- matrix(c(0, 0, 1, 0, 0, 0, 1, 1), nrow = 2, ncol=4, byrow=TRUE, 
> 	dimnames = list(c("diff/v=0", "diff/v=1"), c("C.1", "C.2", "C.3", "C.4")))
> v4 <- csimint(estpar=par4, df=n-6, # I'm not sure whether I found 
>      # the correct degrees of freedom
> 	covm=cov4,
> 	cmatrix=cm4, conf.level=0.95)
> sv4 <- summary(v4)
> 
> # 2. Method: I found in Handbook of Statistics Vol 13, p.616,
> # same can be found in http://home.clara.net/sisa/bonhlp.htm
> # Bonferroni on correlated outcomes:
> 
> raw.p <- sv4$p.value.raw
> co4 <- cor(df$y[df$v==0],df$y[df$v==1])
> rho <- mean(c(1,co4,co4,1))
> pai <- 1-(1-raw.p)^2^(1-rho) 
> 
> # The results of two methods are presented in the following lines:
> out <- cbind(raw.p, sv4$p.value.bon, sv4$p.value.adj, pai)
> colnames(out) <- c("raw.p", "bon.p", "multcomp.p", "bon.cor.p")
> out
> 
> As you can see there are quite big differences 
> between the two ways adjusting for multiple tests on repeated measurements. 
> I guess that the multcomp library is not appropriate for this kind of hypotheses. 
> However I could not find an explanation in the help files. 
> May be one of the experts can point me in the right direction?
> 
> Kind regards,
> 
> Dominik
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.1            
> year     2005           
> month    12             
> day      20             
> svn rev  36812          
> language R    
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rs.gmcg at maths.strath.ac.uk  Wed Aug  2 10:20:31 2006
From: rs.gmcg at maths.strath.ac.uk (G.C. McGuinness)
Date: Wed,  2 Aug 2006 09:20:31 +0100
Subject: [R] Support vector in lcrabs example
Message-ID: <1154506830.44d0604f00866@gauss.maths.strath.ac.uk>

Can anyone explain the root of my problem?

When I type the following code into R, I receive 42 support
vectors insted of the 21 stated in the book 'Modern Applied
Statistics with S':

library(MASS);
library(e1071);
library(class);
lcrabs <- log(crabs[,4:8]);
(svm(crabs$sp ~ ., data = lcrabs, cost = 100, gamma = 1));

By changing the value of gamma I can obtain only 21 support vectors,
but I not sure where an explanation to my problem can be found. I
use R 2.3.2 and the most recent version of the package 'e1071'. My goal
is to minimise the number of SVs for a separate data set.

Many thanks, Graeme.


From ripley at stats.ox.ac.uk  Wed Aug  2 11:32:51 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Aug 2006 10:32:51 +0100 (BST)
Subject: [R] Support vector in lcrabs example
In-Reply-To: <1154506830.44d0604f00866@gauss.maths.strath.ac.uk>
References: <1154506830.44d0604f00866@gauss.maths.strath.ac.uk>
Message-ID: <Pine.LNX.4.64.0608021029140.23449@gannet.stats.ox.ac.uk>

The examples in the book were run in 2001, using S-PLUS (as it says).  
The R package e1071 has changed since then, and hence the results it gives 
have changed. However, the S-PLUS version (which has not been updated) 
still gives the results in the book.

The `problem' is your expectation that R in 2006 is identical to S-PLUS 
in 2001.

On Wed, 2 Aug 2006, G.C. McGuinness wrote:

> Can anyone explain the root of my problem?
> 
> When I type the following code into R, I receive 42 support
> vectors insted of the 21 stated in the book 'Modern Applied
> Statistics with S':
> 
> library(MASS);
> library(e1071);
> library(class);
> lcrabs <- log(crabs[,4:8]);
> (svm(crabs$sp ~ ., data = lcrabs, cost = 100, gamma = 1));
> 
> By changing the value of gamma I can obtain only 21 support vectors,
> but I not sure where an explanation to my problem can be found. I
> use R 2.3.2 and the most recent version of the package 'e1071'. My goal
> is to minimise the number of SVs for a separate data set.

There is no `R 2.3.2'.

> Many thanks, Graeme.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Wed Aug  2 12:16:23 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Aug 2006 03:16:23 -0700
Subject: [R] HELP with NLME
In-Reply-To: <Pine.GSO.4.64.0607251534400.3053@euclid.ucsd.edu>
References: <Pine.GSO.4.64.0607251534400.3053@euclid.ucsd.edu>
Message-ID: <44D07B77.2070301@pdf.com>

	  Have you tried fitting the model with "verbose=TRUE" and possibly 
also "control=nlmeControl(msVerbose=TRUE)"?

	  Also, have you consulted Pinheiro and Bates (2000) Mixed-Effects 
Models in S and S-PLUS (Springer)?  That book contains several examples 
of use of lme and nlme.  R script files named "ch01.R", "ch02.R", ..., 
"ch06.R", "ch08.R" can be found in "~R\library\nlme\scripts", where 
"~R\" is your R installation directory.  If you work through these files 
one line at a time until you think you understand it all, you might get 
an answer to your question.

	  If this fails to produce answer your question, I would try to find a 
much simpler and self contained example that still returns your error 
message.  I might do this by removing terms from the model, removing 
parameters from the model, etc., trying to write a very few lines of 
code that would produce a data set that will still generate the error 
message you see.

	  Hope this helps.
	  Spencer Graves
p.s.  Is it feasible for you to upgrade to R2.3.1?  R2.1 is by now 
rather old.

Loki Natarajan wrote:
> Hi,
> 
> I was very much hoping someone could help me with the following.
> I am trying to convert some SAS NLMIXED code to NLME in R (v.2.1),
> but I get an error message. Does anyone have any suggestions?
> I think my error is with the random effect "u" which seems to be
> parametrized differently in the SAS code. In case it's helpful, 
> what I am essentially trying to do is estimate parameters using ML in a 
> measurement error setting with some validation data (indicated by 
> vs.flag). Any help would be greatly appreciated.I apologize for the
> clumsiness of the R code.
> Many thanks in advance.
> 
> Sincerely,
> Loki
> #################################################################
> SAS Code:
> 
> proc nlmixed data=repdat
> parms b0 -3 b1 -.135 a0 3 a1 4 sigsq 0.25;
> 
> if vs.flag = 1 then do;
>     eta1 = b0 + b1*lnbldT;
>     llbin = anybc.cens.ind*eta1 - log(1+exp(eta1));
>     eta2 = a0 + a1*lnndsTs;
>     llnorm = -1/(2*sigsq)*(lnbldT - eta2)**2 - .5*log(sigsq);
>     ll = llbin + llnorm;
>     end;
> 
> else do;
>     eta2 = a0 + a1*lnndsTs;
>     eta1 = b0 + b1*eta2 + u;
>     llbin = anybc.cens.ind*eta1 - log(1+exp(eta1));
>     ll = llbin;
>     end;
> 
> sigma2 = sigmasq*b1**2 /*variance of random effect;
> model anybc.cens.indic ~ general(ll);
> random u ~ normal(0,sigma2) subject = CaseID; */;
> run;
> ####################################################################
> R Code with error message:
> 
> me.km.nlme <- nlme(model = anybc.cens.indic ~ 
> vs.flag*((anybc.cens.indic*(b0+b1*lnbldT) - log(1 + exp(b0+b1*lnbldT))) + 
> (((-1/(2*sigsq))*(lnbldT -a0 -a1*lnndsTs)^2) - 0.5*log(sigsq))) + 
> (1-vs.flag)*((anybc.cens.indic*(b0+b1*(a0 + a1*lnndsTs + u))) - log(1 + 
> exp(b0+b1*(a0 + a1*lnndsTs + u)))), fixed = 
> list(a0~1,a1~1,b0~1,b1~1,sigsq~1),na.action=na.omit, data=rc.df, 
> method="ML", random=u~1|CaseID,
> start = c(a0=0, a1=1, b0=-3, b1 = -0.135, sigsq = 0.25))
> 
> + Error: Singularity in backsolve at level 0, block 1
> In addition: There were 16 warnings (use warnings() to see them)
>> warnings()
> Warning messages:
> 1: Singular precision matrix in level -1, block 5
> 2: Singular precision matrix in level -1, block 5
> 3: Singular precision matrix in level -1, block 5
> 4: NA/Inf replaced by maximum positive value
> 5: Singular precision matrix in level -1, block 5
> 6: Singular precision matrix in level -1, block 5
> 7: Singular precision matrix in level -1, block 5
> 8: NA/Inf replaced by maximum positive value
> 9: NaNs produced in: log(x)
> 10: NaNs produced in: log(x)
> 11: NaNs produced in: log(x)
> 12: NaNs produced in: log(x)
> 13: NaNs produced in: log(x)
> 14: NaNs produced in: log(x)
> 15: NaNs produced in: log(x)
> 16: NaNs produced in: log(x)
> #####################################################################
> 
> 
> Loki Natarajan
> Associate Professor of Biostatistics
> Moores UCSD Cancer Center
> 3855 Health Sciences Drive #0901
> La Jolla, CA 92093-0901
> 
> 
> phone:     858 822 4763
> Fax:       858 822 6897
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phhs80 at gmail.com  Wed Aug  2 12:32:34 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 2 Aug 2006 11:32:34 +0100
Subject: [R] Syntax of Levene's test
Message-ID: <6ade6f6c0608020332tf81f3a5v531c789c67a5cf80@mail.gmail.com>

Dear All

I am trying to use Levene's test (of package car), but I do not
understand quite well how to use it. '?levene.test' does not
unfortunately provide any example. My data are in a data frame and
correspond to 4 factors plus response. Could someone please give me an
example about how to use the command

levene.test(y, group)

?

Thanks in advance,

Paul


From grazzi at sssup.it  Wed Aug  2 12:56:55 2006
From: grazzi at sssup.it (Marco Grazzi)
Date: Wed, 2 Aug 2006 12:56:55 +0200
Subject: [R] loop, pipe connection, quote/unquote
Message-ID: <20060802125655.54564b8d.grazzi@sssup.it>

Hi all,

I have the following problem.
Inside R, I am trying to run a loop on several files. 
The data are stored in these files in a peculiar way, thus, at the same time I load the data, I would like to invoke a utility. I do this with "pipe". (The utility I am using is gbget from the package gbutils. It works correctly from shell, and it is not the problem.)
The problem is that from within pipe I do not know how to have the loop running. I guess I should manage to unquote the quotation mark inside the pipe for the loop to run, but I do not how to do it.
In the following examples, for instance, I would like pipe to interpret the "i" as the file.names of the loop specified above.

# specify the files on which I want to have the loop running
file.names <- system("ls ???.gz", intern=TRUE) 
# Start loop
for(i in file.names){
        dati     <- read.table(pipe("gbget  'i[160](1)' '[37](1)' '[145](1)' |gbget '()D' "))
	# [...] some statistical analysis follows [...]
}

Thanks for your help (hoping I manged to be enough clear), 
marco


-- 


Marco Grazzi

-----

PhD Candidate in Economics and Management
LEM-Sant'Anna School of Advanced Studies
Piazza Martiri della Liberta', 33
56127 Pisa, Italy
Tel. +39-050-883365 Fax +39-050-883344
Web site: https://mail.sssup.it/~grazzi


From R.Knell at qmul.ac.uk  Wed Aug  2 13:04:45 2006
From: R.Knell at qmul.ac.uk (Rob Knell)
Date: Wed, 2 Aug 2006 12:04:45 +0100
Subject: [R] Trying to use segmented in a function
Message-ID: <453F37C6-7EF9-489F-B45B-B3B09757E41F@qmul.ac.uk>

Hi folks

I wonder if anyone can help me. I want to run some simulations to see  
how big a sample size might be necessary to distinguish a curved  
bivariate relationship (e.g. something that might be best described  
by a quadratic model) from a relationship that is two straight lines  
with a sudden change in slope (e.g. something best described by a  
breakpoint regression). I am using segmented to do the breakpoint  
regression: this package seems to be the one that most people use for  
this, as far as I can see.

Since I want to run some simulations, I'm trying to write functions  
that use segmented, and it's driving me mad. Here's a simple example:

simdata<-function 
(Ns=200,Xmean=20,Xsd=5,SdYerr=0.5,Yint=0,threshold=20,slopebelow=0.5,slo 
peabove=1)
{
	Xs<-rnorm(Ns,Xmean,Xsd)
	Yerr<-rnorm(Ns,0,SdYerr)
	D<-ifelse(Xs<=threshold,0,1)
	XminusX0<-Xs-threshold
	Ys<-Yint+slopebelow*Xs+slopeabove*XminusX0*D+Yerr

	plot(Xs,Ys)
	
	linmod<-lm(Ys~Xs)
	segment<-segmented(linmod,Z=Xs,psi=threshold)

	segment


}

This code should simply simulate some "breakpoint" data, with the  
change in slope at "threshold" and then fit a model with segmented.  
If I just use the code for simulating the data, and run that, and  
then run segmented as normal in R, then I occasionally get an error  
when it exceeds the maximum iterations, but 99% of the time it will  
fit a model happily. When I incorporate it into the function,  
however, it will sometimes fit a model (about 20% of the time) but  
most of the time I get this:


 > test<-simdata()
Error in segmented.lm(linmod, Z = Xs, psi = threshold) :
	(Some) estimated psi out of its range
 >

I emphasise that this is using exactly the same code to simulate the  
data that gives good results when used without segmented in the  
function. I'm even giving it the exact right value of the breakpoint  
to start with in its estimation.

If anyone could give me some advice on where I'm going wrong, I would  
be very pleased to hear it.


Thanks everyone

Rob Knell

School of Biological Sciences
Queen Mary, University of London

'Phone +44 (0)20 7882 7720
Skype Rob Knell
http://www.qmw.ac.uk/~ugbt794
http://www.mopane.org

"The truth is that they have no clue why the beetles had horns, it's  
the researchers who have sex on the brain and everything has to have  
a sexual explanation. And this is reasearch?!" Correspondent known as  
FairOpinion on Neo-Con American website discussing my research.


From ggrothendieck at gmail.com  Wed Aug  2 14:38:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Aug 2006 08:38:54 -0400
Subject: [R] loop, pipe connection, quote/unquote
In-Reply-To: <20060802125655.54564b8d.grazzi@sssup.it>
References: <20060802125655.54564b8d.grazzi@sssup.it>
Message-ID: <971536df0608020538v3428acc5t22cae025dfc3bedb@mail.gmail.com>

Try something along these lines assuming that the current
directory is \Program Files\R\R-2.3.1pat.  Note use
of paste to create the command line to pass to pipe:

# search for indicated string in each of the files and
# for each match output the file name
Files <- c("CHANGES", "COPYING", "NEWS", "NEWS")
for(f in Files) print(readLines(pipe(paste("findstr /m out-of-range", f))))


On 8/2/06, Marco Grazzi <grazzi at sssup.it> wrote:
> Hi all,
>
> I have the following problem.
> Inside R, I am trying to run a loop on several files.
> The data are stored in these files in a peculiar way, thus, at the same time I load the data, I would like to invoke a utility. I do this with "pipe". (The utility I am using is gbget from the package gbutils. It works correctly from shell, and it is not the problem.)
> The problem is that from within pipe I do not know how to have the loop running. I guess I should manage to unquote the quotation mark inside the pipe for the loop to run, but I do not how to do it.
> In the following examples, for instance, I would like pipe to interpret the "i" as the file.names of the loop specified above.
>
> # specify the files on which I want to have the loop running
> file.names <- system("ls ???.gz", intern=TRUE)
> # Start loop
> for(i in file.names){
>        dati     <- read.table(pipe("gbget  'i[160](1)' '[37](1)' '[145](1)' |gbget '()D' "))
>        # [...] some statistical analysis follows [...]
> }
>
> Thanks for your help (hoping I manged to be enough clear),
> marco
>
>
> --
>
>
> Marco Grazzi
>
> -----
>
> PhD Candidate in Economics and Management
> LEM-Sant'Anna School of Advanced Studies
> Piazza Martiri della Liberta', 33
> 56127 Pisa, Italy
> Tel. +39-050-883365 Fax +39-050-883344
> Web site: https://mail.sssup.it/~grazzi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Wed Aug  2 14:39:26 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 2 Aug 2006 08:39:26 -0400
Subject: [R] Syntax of Levene's test
In-Reply-To: <6ade6f6c0608020332tf81f3a5v531c789c67a5cf80@mail.gmail.com>
Message-ID: <20060802123925.SXBV1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Paul,

The argument y is the response variable and group is a factor defining
groups (as ?levene.test says). If you have more than one factor, then you
can use interaction() to create from them a factor with levels given by the
product set of the levels of the individual factors. Here's an example

> library(car)
> data(Moore)
> attach(Moore)
> levene.test(conformity, interaction(fcategory, partner.status))
Levene's Test for Homogeneity of Variance
      Df F value Pr(>F)
group  5  1.4694 0.2219
      39               
> levels(interaction(fcategory, partner.status))
[1] "high.high"   "low.high"    "medium.high" "high.low"    "low.low"    
[6] "medium.low" 
> levels(fcategory)
[1] "high"   "low"    "medium"
> levels(partner.status)
[1] "high" "low" 

I'll add a couple of examples to the help page.

I hope this helps,
 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Wednesday, August 02, 2006 5:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Syntax of Levene's test
> 
> Dear All
> 
> I am trying to use Levene's test (of package car), but I do 
> not understand quite well how to use it. '?levene.test' does 
> not unfortunately provide any example. My data are in a data 
> frame and correspond to 4 factors plus response. Could 
> someone please give me an example about how to use the command
> 
> levene.test(y, group)
> 
> ?
> 
> Thanks in advance,
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mcardeal at ufba.br  Wed Aug  2 14:55:41 2006
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Wed, 02 Aug 2006 09:55:41 -0300
Subject: [R] missing value
Message-ID: <44D0A0CD.402@ufba.br>

Hi all

# I have this data set and how can I assign NA?s in just one command ? 
And why the summary(dat) function preserves the value 9 as real. ?

x <- c(1,2,3,9,4)
y <- c(3,6,9,2,3)
z <- c(9,9,2,2,8)
w <- c(6,5,3,0,9)

dat <- cbind(x,y,z,w)
summary(dat)

x[x==9] <- NA
y[y==9] <- NA
z[z==9] <- NA
w[w==9] <- NA

summary(dat)
summary(x)
summary(y)
summary(z)
summary(w)

Thank you all,
Mauricio


From ligges at statistik.uni-dortmund.de  Wed Aug  2 15:09:32 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Aug 2006 15:09:32 +0200
Subject: [R] missing value
In-Reply-To: <44D0A0CD.402@ufba.br>
References: <44D0A0CD.402@ufba.br>
Message-ID: <44D0A40C.7090605@statistik.uni-dortmund.de>

Mauricio Cardeal wrote:
> Hi all
> 
> # I have this data set and how can I assign NA?s in just one command ? 

is.na(dat[dat==9]) <- TRUE

> And why the summary(dat) function preserves the value 9 as real. ?
>

Because you have not changed the contents of dat at all, only the 
contents of x,y,z, and w.

Uwe Ligges


> x <- c(1,2,3,9,4)
> y <- c(3,6,9,2,3)
> z <- c(9,9,2,2,8)
> w <- c(6,5,3,0,9)
> 
> dat <- cbind(x,y,z,w)
> summary(dat)
> 
> x[x==9] <- NA
> y[y==9] <- NA
> z[z==9] <- NA
> w[w==9] <- NA
> 
> summary(dat)
> summary(x)
> summary(y)
> summary(z)
> summary(w)
> 
> Thank you all,
> Mauricio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jacques.veslot at good.ibl.fr  Wed Aug  2 15:12:57 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 02 Aug 2006 15:12:57 +0200
Subject: [R] missing value
In-Reply-To: <44D0A0CD.402@ufba.br>
References: <44D0A0CD.402@ufba.br>
Message-ID: <44D0A4D9.8040005@good.ibl.fr>

dat[dat==9] <- NA
because the result of mean() is real and summary()'s output is a vector.
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Mauricio Cardeal a ?crit :
> Hi all
> 
> # I have this data set and how can I assign NA?s in just one command ? 
> And why the summary(dat) function preserves the value 9 as real. ?
> 
> x <- c(1,2,3,9,4)
> y <- c(3,6,9,2,3)
> z <- c(9,9,2,2,8)
> w <- c(6,5,3,0,9)
> 
> dat <- cbind(x,y,z,w)
> summary(dat)
> 
> x[x==9] <- NA
> y[y==9] <- NA
> z[z==9] <- NA
> w[w==9] <- NA
> 
> summary(dat)
> summary(x)
> summary(y)
> summary(z)
> summary(w)
> 
> Thank you all,
> Mauricio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From s.martino at tno.it  Wed Aug  2 15:18:28 2006
From: s.martino at tno.it (Sergio Martino)
Date: Wed, 2 Aug 2006 15:18:28 +0200
Subject: [R] How to share variables
Message-ID: <005d01c6b636$24f1e680$3c32428a@msi>

Hi,

I would like to realize in R a structure like the fortran common ie a way to
declare some variable that can only be accessed by all the functions which
need to.

Browsing the archive it seems that the simplest way is to declare the
variables and the functions in a big function which wraps all. But this is
impratical when the functions are big.

The environments seems to do the trick but I am not enough familiar with
them to make my ways out.

Is there any example or pointers to easy but complete environment usage?

Thanks in Advance

Sergio Martino


From r at fam-kuster.ch  Wed Aug  2 15:22:54 2006
From: r at fam-kuster.ch (Thomas Kuster)
Date: Wed, 2 Aug 2006 15:22:54 +0200
Subject: [R] read.spss and umlaut
Message-ID: <200608021522.54958.r@fam-kuster.ch>

Hello

When I read a SPSS *.por file with read.spss everything after a umlaut is 
missing:

> library("foreign")
> spssdaten <- read.spss("projets.por")
> attr(spssdaten$PROJETX, "value.labels")[1:20]
              Bg Stammzellenforschung                                  Bb
                                  863                                   862
Bb Neugestaltung des Finanzausgleichs
                                  861                                   854
                     EV Postdienste f                                   Bb
                                  853                                   852
                                  Bb                         Bg Steuerpaket
                                  851                                   843
     Bb Anhebung der Mehrwertsteuer s                      11. AHV-Revision
                                  842                                   841
Volkinitiative Lebenslange Verwahrung
                                  833                                   832
              Gegenentwurf zur Avanti             EV Lehrstellen-Initiative
                                  831                                   824
                   EV Moratorium Plus                    EV Strom ohne Atom
                                  823                                   822
               EV Ja zu fairen Mieten                   EV Gleiche Rechte f
                                  821                                   815
             EV Gesundheitsinitiative                EV Sonntags-Initiative
                                  814                                   813

The SPSS-File is okay:
> system("cat projets.por |grep Postdienste")
echtserwerb 3. GenerationSD/N/EV Postdienste f?r alleSE/16/?nderrung Bg  EOG 
Mut

How can I read the SPSS-File with the Umlaut?

Bye
Thomas Kuster

R: 2.1.0 (2005-04-18)
OS: Debian Sarge (Version 2.6.10-isgee-neptun-1)


From maj at stats.waikato.ac.nz  Tue Aug  1 23:37:58 2006
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Wed, 02 Aug 2006 09:37:58 +1200
Subject: [R] Fitting models in a loop
In-Reply-To: <x2irlc93cg.fsf@viggo.kubism.ku.dk>
References: <C3E3A3D81F4E0F438118DAA9722F12A9011963C2@LNVCNTEXCH01.corp.lloydsnet>
	<x2irlc93cg.fsf@viggo.kubism.ku.dk>
Message-ID: <44CFC9B6.6080705@stats.waikato.ac.nz>

Thanks to all for their help. I am busy today but tomorrow I will have 
time to digest all the feedback and follow up if necessary

Cheers,  Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From ricardosilva at serasa.com.br  Tue Aug  1 18:46:58 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Tue, 1 Aug 2006 13:46:58 -0300
Subject: [R] Pseudo R for Quant Reg
Message-ID: <OF7ECE18FE.9C3DD575-ON032571BD.005B179F-032571BD.005C3100@serasa.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060801/b414e0bd/attachment.pl 

From r at fam-kuster.ch  Wed Aug  2 15:31:56 2006
From: r at fam-kuster.ch (Thomas Kuster)
Date: Wed, 2 Aug 2006 15:31:56 +0200
Subject: [R] read.spss and umlaut
Message-ID: <200608021531.56340.r@fam-kuster.ch>

Hello

When I read a SPSS *.por file with read.spss everything after a umlaut is 
missing:

> library("foreign")
> spssdaten <- read.spss("projets.por")
> attr(spssdaten$PROJETX, "value.labels")[1:20]
              Bg Stammzellenforschung                                  Bb
                                  863                                   862
Bb Neugestaltung des Finanzausgleichs
                                  861                                   854
                     EV Postdienste f                                   Bb
                                  853                                   852
                                  Bb                         Bg Steuerpaket
                                  851                                   843
     Bb Anhebung der Mehrwertsteuer s                      11. AHV-Revision
                                  842                                   841
Volkinitiative Lebenslange Verwahrung
                                  833                                   832
              Gegenentwurf zur Avanti             EV Lehrstellen-Initiative
                                  831                                   824
                   EV Moratorium Plus                    EV Strom ohne Atom
                                  823                                   822
               EV Ja zu fairen Mieten                   EV Gleiche Rechte f
                                  821                                   815
             EV Gesundheitsinitiative                EV Sonntags-Initiative
                                  814                                   813

The SPSS-File is okay:
> system("cat projets.por |grep Postdienste")
echtserwerb 3. GenerationSD/N/EV Postdienste f?r alleSE/16/?nderrung Bg  EOG 
Mut

How can I read the SPSS-File with the Umlaut?

Bye
Thomas Kuster

R: 2.1.0 (2005-04-18)
OS: Debian Linux, 2.6.10-isgee-neptun-1


From ggrothendieck at gmail.com  Wed Aug  2 15:50:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Aug 2006 09:50:33 -0400
Subject: [R] How to share variables
In-Reply-To: <005d01c6b636$24f1e680$3c32428a@msi>
References: <005d01c6b636$24f1e680$3c32428a@msi>
Message-ID: <971536df0608020650vbc87b7n3daa28acfc792900@mail.gmail.com>

On 8/2/06, Sergio Martino <s.martino at tno.it> wrote:
> Hi,
>
> I would like to realize in R a structure like the fortran common ie a way to
> declare some variable that can only be accessed by all the functions which
> need to.
>
> Browsing the archive it seems that the simplest way is to declare the
> variables and the functions in a big function which wraps all. But this is
> impratical when the functions are big.

There is a demonstration of that found by issuing the command:

demo(scoping)

>
> The environments seems to do the trick but I am not enough familiar with
> them to make my ways out.

Yes place your data in an environment as shown and then for
each function that is to access the environment should have
its environment set accordingly:

e <- new.env()
e$dat <- 1:3
myfun <- function(x) sum(x + dat)
environment(myfun) <- e
myfun(10)  # fun can access dat

Realize that what you are trying to do is to create a sort of object
oriented structure with the data being the objects and the functions
being the methods.  The proto package provides some functionality
to implement that and also supports delegation (similar to
inheritance):

library(proto)
package?proto # all sources of info on proto

# example - create proto object p with some data dat and a method fun
p <- proto(dat = 1:3, fun = function(., x) sum(x + .$dat))

# invoke method
p$fun(10)  # runs fun.  fun has access to dat

# create a child q of p and run fun
# q overrides dat with its own dat while inheriting fun
q <- p$proto(dat = 4:6)
q$fun(10)

Another possibility would be to look at the R.oo package which is
another object oriented infrastructure based on environments.

>
> Is there any example or pointers to easy but complete environment usage?
>
> Thanks in Advance
>
> Sergio Martino


From gregd at stats.uct.ac.za  Wed Aug  2 15:55:53 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Wed, 2 Aug 2006 15:55:53 +0200
Subject: [R] Plotting a ranef object in NLME
Message-ID: <19ec01c6b63b$5f3e9360$6f179e89@UCTPCGREGD>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/f9dd9164/attachment.pl 

From rkoenker at uiuc.edu  Wed Aug  2 16:13:52 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 2 Aug 2006 09:13:52 -0500
Subject: [R] Pseudo R for Quant Reg
In-Reply-To: <OF7ECE18FE.9C3DD575-ON032571BD.005B179F-032571BD.005C3100@serasa.com.br>
References: <OF7ECE18FE.9C3DD575-ON032571BD.005B179F-032571BD.005C3100@serasa.com.br>
Message-ID: <2CCDCDED-E709-477A-82D6-51070C9A97AD@uiuc.edu>

This is getting to be a faq -- here is a prior answer:

> No, but the objective function can be computed for any fitted
> rq object, say f,  as
>
> 	rho <- function(u,tau=.5)u*(tau - (u < 0))
> 	V <- sum(rho(f$resid, f$tau))
>
> so it is easy to roll your own....

I don't much like R1, or R2 for that matter, so it isn't likely to
be automatically provided in quantreg any time soon.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Aug 1, 2006, at 11:46 AM, ricardosilva at serasa.com.br wrote:

> Dear R Users,
>
> Did someone implemented the R1 (Pseudo R-2) and likelihood ratio
> statistics for quantile regressions,  which are some of the inference
> procedures for quantile regression
> found in Koenker and Machado (1999)?
> I tried the Ox version, but my dataset is too large (> 50.000) and the
> algorith breaks.
> ________________________________________
> Ricardo Gon?alves Silva, M. Sc.
> Apoio aos Processos de Modelagem Matem?tica
> Econometria & Inadimpl?ncia
> Serasa S.A.
> (11) - 6847-8889
> ricardosilva at serasa.com.br
>
> ********************************************************************** 
> ************
> As informa??es contidas nesta mensagem e no(s) arquivo(s...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rjvbertin at gmail.com  Wed Aug  2 16:41:27 2006
From: rjvbertin at gmail.com (=?ISO-8859-1?Q?Ren=E9_J.V._Bertin?=)
Date: Wed, 2 Aug 2006 16:41:27 +0200
Subject: [R] best way to calculate per-parameter differences in
	across-subject means
Message-ID: <797664590608020741m241c9782v34386e01811ea58@mail.gmail.com>

Hello,

I have some data in a data.frame where for each of a number of
subjects, I have scores for all of a number of symptoms.

Subjects are subdivided in a number of groups, which have unequal sizes.

I'd like to plot between-group differences in the scores on the
various symptoms. Ideally, that would be in a form as would be
produced by

> bwplot( Score~Symptom )

but I'm not sure one can say anything about the distribution of
differences when the sample sizes differ as much as they do.

So I will start plotting the per-group differences in the per-symptom
mean scores.

Is there a better way (rather than using loops) to get a table of
those per-symptom means, something like

> with( subset(dat, group==1, drop=FALSE), Score~Symptom )

Thanks in advance,
Ren? Bertin


From z.dalton at lancaster.ac.uk  Wed Aug  2 17:09:10 2006
From: z.dalton at lancaster.ac.uk (z.dalton at lancaster.ac.uk)
Date: Wed, 2 Aug 2006 16:09:10 +0100 (BST)
Subject: [R] ordering columns (longitudinal data in wide format)
Message-ID: <E1G8ILO-00021d-00@wing1.lancs.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/e641004e/attachment.pl 

From tlumley at u.washington.edu  Wed Aug  2 17:11:13 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 2 Aug 2006 08:11:13 -0700 (PDT)
Subject: [R] read.spss and umlaut
In-Reply-To: <200608021531.56340.r@fam-kuster.ch>
References: <200608021531.56340.r@fam-kuster.ch>
Message-ID: <Pine.LNX.4.64.0608020752400.2107@homer23.u.washington.edu>

On Wed, 2 Aug 2006, Thomas Kuster wrote:

> Hello
>
> When I read a SPSS *.por file with read.spss everything after a umlaut is
> missing:

This sounds like a conflict between encodings -- eg if R is assuming UTF-8 
and the file is encoding in Latin-1 then the sequence
U+00FC : LATIN SMALL LETTER U WITH DIAERESIS
U+0072 : LATIN SMALL LETTER R
is coded as FC72 in the file, which is an illegal byte sequence in UTF-8.

The underlying C code (being written in the US quite a long time ago) 
doesn't know about encodings, and I don't know what the rules are in SPSS 
for valid characters (I suspect that in these old portable file formats it 
probably just reads and writes bytes, leaving it up to the OS to interpret 
them.

You could try running R in a non-UTF-8 locale to see if it helps.

If anyone has definitive information about how SPSS represents strings and 
decides on valid characters that might be useful too.

 	-thomas

>> library("foreign")
>> spssdaten <- read.spss("projets.por")
>> attr(spssdaten$PROJETX, "value.labels")[1:20]
>              Bg Stammzellenforschung                                  Bb
>                                  863                                   862
> Bb Neugestaltung des Finanzausgleichs
>                                  861                                   854
>                     EV Postdienste f                                   Bb
>                                  853                                   852
>                                  Bb                         Bg Steuerpaket
>                                  851                                   843
>     Bb Anhebung der Mehrwertsteuer s                      11. AHV-Revision
>                                  842                                   841
> Volkinitiative Lebenslange Verwahrung
>                                  833                                   832
>              Gegenentwurf zur Avanti             EV Lehrstellen-Initiative
>                                  831                                   824
>                   EV Moratorium Plus                    EV Strom ohne Atom
>                                  823                                   822
>               EV Ja zu fairen Mieten                   EV Gleiche Rechte f
>                                  821                                   815
>             EV Gesundheitsinitiative                EV Sonntags-Initiative
>                                  814                                   813
>
> The SPSS-File is okay:
>> system("cat projets.por |grep Postdienste")
> echtserwerb 3. GenerationSD/N/EV Postdienste f?r alleSE/16/?nderrung Bg  EOG
> Mut
>
> How can I read the SPSS-File with the Umlaut?
>
> Bye
> Thomas Kuster
>
> R: 2.1.0 (2005-04-18)
> OS: Debian Linux, 2.6.10-isgee-neptun-1
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From tillard at cirad.fr  Wed Aug  2 17:13:36 2006
From: tillard at cirad.fr (Emmanuel Tillard)
Date: Wed, 02 Aug 2006 17:13:36 +0200
Subject: [R] expected survival from a frailty cox model using survfit
Message-ID: <44D0C120.5060409@cirad.fr>

Hello R users

Would somebody know how to estimate survival from a frailty cox model, 
using the function survfit
and the argument newdata ? (or from any other way that could provide 
individual expected survival
with standard error); Is the problem related to how the random term is 
included in newdata ?

kfitm1 <- coxph(Surv(time,status) ~ age + sex + disease + frailty(id, 
dist='gauss'), kidney)
survfit(kfitm1) #ok for mean expected survival

Call: survfit.coxph(object = kfitm1)

      n  events  median 0.95LCL 0.95UCL
     76      58      63      39     132

survfit(kfitm1, newdata=kidney[1,]) #return an error message

Erreur dans x2 %*% coef : arguments inad?quats

Thanks in Advance

-- 
Emmanuel Tillard
Veterinaire
CIRAD-EMVT
Unite de recherche 18

UMR868 Elevage des Ruminants en Regions Chaudes (ERRC)
Campus ENSA-INRA
2 place Viala
34060 Montpellier cedex 1

tel:	0499612265 (fixe)
	0633850598 (gsm)
fax:	0467545694
e-mail: tillard at cirad.fr


From ggrothendieck at gmail.com  Wed Aug  2 17:26:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Aug 2006 11:26:00 -0400
Subject: [R] ordering columns (longitudinal data in wide format)
In-Reply-To: <E1G8ILO-00021d-00@wing1.lancs.ac.uk>
References: <E1G8ILO-00021d-00@wing1.lancs.ac.uk>
Message-ID: <971536df0608020826l2a732b43r9d0f4769a4fbf499@mail.gmail.com>

Assuming this data:

s <- structure(list(L.qol.0 = 83, L.qol.0.08 = 86, L.qol.0.17 = 89,
    L.qol.0.25 = 92, L.qol.0.5 = 91, L.qol.0.42 = 87, L.qol.0.34 = 90),
   .Names = c("L.qol.0", "L.qol.0.08", "L.qol.0.17", "L.qol.0.25",
   "L.qol.0.5", "L.qol.0.42", "L.qol.0.34"),
   class = "data.frame", row.names = "1")

# we can sort it by column names like this:
s[,sort(names(s))]

# also note that mixed sort in gtools can sort by numeric
# value in mixed character/numeric names which gives the
# same result here but may not in different examples
library(gtools)
s[,mixedsort(names(s))]


On 8/2/06, z.dalton at lancaster.ac.uk <z.dalton at lancaster.ac.uk> wrote:
> Hi,
>
> I am working on some longitudinal data in wide format and I am having a problem ordering the data columns.  To expand, a subset of what I am working on is as follows;
>
> >s
>   L.qol.0 L.qol.0.08 L.qol.0.17 L.qol.0.25 L.qol.0.5 L.qol.0.42 L.qol.0.34
> 1      83         86         89         92        91         87         90
>
> >names(s)
> [1] "L.qol.0"    "L.qol.0.08" "L.qol.0.17" "L.qol.0.25" "L.qol.0.5"
> [6] "L.qol.0.42" "L.qol.0.34"
>
> # in this object s (not a vector), 'L.qol' is measured at time points 0, 0.08, 0.17, 0.25, 0.34, 0.42 and 0.5.  As you can see, however, the time points are not in the correct order in object s.  Does anyone know how to order these column names along with their corresponding measurements?  Clearly s[order(s)] does not work since this just orders the corresponding measurements.
>
> I would be extremely grateful for any help on this matter, it may be really simple, but I have tried for ages.
>
> Thank you,
>
> Zoe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jz7 at duke.edu  Wed Aug  2 17:29:47 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Wed, 2 Aug 2006 11:29:47 -0400 (EDT)
Subject: [R] question about correlation coefficeint and root mean square
In-Reply-To: <Pine.GSO.4.58.0607311919200.20820@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607311919200.20820@godzilla.acpub.duke.edu>
Message-ID: <Pine.GSO.4.58.0608021043440.15922@godzilla.acpub.duke.edu>

Dear all,

I am using different multiple regression models (OLS and principal
component regression (PCR)) to make prediction of my test set. And those
models come from the same training set, except that the number of
variables or descriptors (columns of X) used in OLS is less than those
used in PCR.

And I use square correlation coefficient (r^2) and root mean square to see
the relationship between my prediction and the experimental measurements
of the test set. Here is the problem:

My r^2 from PCR prediction is higher than r^2 from OLS prediction (0.8 vs.
0.7). However, my RMS of PCR prediction is also higher than OLS (0.55 vs.
0.48). I would expect r^2 and RMS show consistant trend. But why am I
getting opposite results? Is it because PCR is a biased method? Which one
(r^2 or RMS) should be more reliable to evaluate the model?

Really appreciate your kind help!

Sincerely,
Jeny


From gunter.berton at gene.com  Wed Aug  2 17:33:03 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Aug 2006 08:33:03 -0700
Subject: [R] rpad, leaps, regsubsets
In-Reply-To: <006c01c6b60a$2d895160$521f6e41@home>
Message-ID: <000c01c6b648$f20d5b60$711f210a@gne.windows.gene.com>

Boris:

Thankyou for this. All the RPAD links now appear to be dead. However, the
Reference Card is still available in the CONTRIBUTED link on CRAN, as I
said.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA


> 
> Thanks for the resources, Berton. but unfortunately, that 
> rpad link fails, 
> and I still do not know where to get leaps or regsubsets functions. 
> Sincerely, Boris.
> -------------- 
> Hello, dear r team. Please help the newbie. My r is not 
> finding leaps or 
> regsubsets finctions. What should I do? Any name changes or 
> library loading 
> issues?
> -------------------------
> Boris Garbuzov
> E-mail: bgarbuzo at sfu.ca
> ICQ:  146995300
> MSN: garbuzov at hotmail.com
> Residence: 3007 Hamilton Hall, 8888 University Drive, Burnaby 
> BC, V5A 1S6, 
> Canada


From dieter.menne at menne-biomed.de  Wed Aug  2 18:04:52 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 2 Aug 2006 16:04:52 +0000 (UTC)
Subject: [R]
	=?utf-8?q?best_way_to_calculate_per-parameter_differences_in?=
	=?utf-8?q?=09across-subject_means?=
References: <797664590608020741m241c9782v34386e01811ea58@mail.gmail.com>
Message-ID: <loom.20060802T180307-211@post.gmane.org>

Ren? J.V. Bertin <rjvbertin <at> gmail.com> writes:

> I have some data in a data.frame where for each of a number of
> subjects, I have scores for all of a number of symptoms.
> 
> Subjects are subdivided in a number of groups, which have unequal sizes.
> 
> I'd like to plot between-group differences in the scores on the
> various symptoms. Ideally, that would be in a form as would be
> produced by

....

Maybe it's a bit more than you want, but possibly you are happy with it: see the
example under TukeyHSD.

summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
TukeyHSD(fm1, "tension", ordered = TRUE)
plot(TukeyHSD(fm1, "tension"))


Dieter


From Mat.Soukup at fda.hhs.gov  Wed Aug  2 18:43:56 2006
From: Mat.Soukup at fda.hhs.gov (Soukup, Mat)
Date: Wed, 2 Aug 2006 12:43:56 -0400
Subject: [R] [Off-Topic-but somewhat related] DIA/FDA Open Toolbox Initiative
Message-ID: <27CA3827C6B33E40874682C469E774DD02AC3167@FMD3CT001.fda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/e2d3d44e/attachment.pl 

From chrish at stats.ucl.ac.uk  Wed Aug  2 18:51:12 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 2 Aug 2006 17:51:12 +0100 (BST)
Subject: [R] Summary method needed?
Message-ID: <Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>

Hi list,

I'm updating my fpc package at the moment and will add some new functions. 
I learned that there should be print and summary methods for the key
functions.
The purpose of the summary methods seems to be to reduce the 
possibly incredibly complex information in the function's output and the 
print method (print.summary.foo) should print an overview of the result.

But in some cases the print method will make use of more or less all the 
output information of the function. Is there any reason to implement a 
summary method in these cases?

Best,
Christian

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From hodgess at gator.dt.uh.edu  Wed Aug  2 18:57:28 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 2 Aug 2006 11:57:28 -0500
Subject: [R]  listing of permutations
Message-ID: <200608021657.k72GvSBI026052@gator.dt.uh.edu>

Dear R People:

Suppose I have the 4 numbers: 1,2,3,4.

I would like to create a listing of the permutations
of 4 items taken 4 at a time.

Is there a built in function for that, please?

Thanks in advance!
R 2-3-1 for Windows or Linux
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From davidr at rhotrading.com  Wed Aug  2 19:14:12 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 2 Aug 2006 12:14:12 -0500
Subject: [R] listing of permutations
Message-ID: <F9F2A641C593D7408925574C05A7BE77076258@rhopost.rhotrading.com>

not very-well hidden:

permutations in (e1071)
permn in (combinat)

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
Sent: Wednesday, August 02, 2006 11:57 AM
To: r-help at stat.math.ethz.ch
Subject: [R] listing of permutations

Dear R People:

Suppose I have the 4 numbers: 1,2,3,4.

I would like to create a listing of the permutations
of 4 items taken 4 at a time.

Is there a built in function for that, please?

Thanks in advance!
R 2-3-1 for Windows or Linux
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jz7 at duke.edu  Wed Aug  2 19:12:25 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Wed, 2 Aug 2006 13:12:25 -0400 (EDT)
Subject: [R] question about correlation coefficeint and root mean square
 (with code used)
Message-ID: <Pine.GSO.4.58.0608021305370.15922@godzilla.acpub.duke.edu>

Dear all,

I am using different multiple regression models (OLS and principal
component regression (PCR)) to make prediction of my test set. And those
models come from the same training set, except that the number of
variables or descriptors (columns of X) used in OLS is less than those
used in PCR.

And I use square correlation coefficient (r^2) and root mean square to see
the relationship between my prediction and the experimental measurements
of the test set. Here is the problem:

My r^2 from PCR prediction is higher than r^2 from OLS prediction (0.8 vs.
0.7). However, my RMS of PCR prediction is also higher than OLS (0.55 vs.
0.48). I would expect r^2 and RMS show consistant trend (r^2 increase &
rms decrease, or the opposite). But why am I getting opposite results? Is
it because PCR is a biased method? Which one (r^2 or RMS) should be more
reliable to evaluate the model?

Here is the simple code I used for calculating r^2 and RMS in R (test set
size is 40):

r2=cor(test$p50, test.pred$fit)*cor(test$p50, test.pred$fit)

rms=sqrt((test.pred$fit-test$p50)%*%(test.pred$fit-test$p50)/40)

Really appreciate your kind help!

Sincerely,
Jeny


From gunter.berton at gene.com  Wed Aug  2 19:31:40 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Aug 2006 10:31:40 -0700
Subject: [R] listing of permutations
In-Reply-To: <200608021657.k72GvSBI026052@gator.dt.uh.edu>
Message-ID: <005201c6b659$83d5f6a0$711f210a@gne.windows.gene.com>

Erin:
You got 2 (so far) pre-packaged functions .Here's an obscenely inefficient
but short un-prepackaged way to do it:

k<-4
z<- do.call('expand.grid',as.data.frame(matrix(rep(1:k,k),nc=k)))
results<- z[apply(z,1,function(x)length(unique(x))==k),]

It is too inefficient to make public, though.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
> Sent: Wednesday, August 02, 2006 9:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] listing of permutations
> 
> Dear R People:
> 
> Suppose I have the 4 numbers: 1,2,3,4.
> 
> I would like to create a listing of the permutations
> of 4 items taken 4 at a time.
> 
> Is there a built in function for that, please?
> 
> Thanks in advance!
> R 2-3-1 for Windows or Linux
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Wed Aug  2 19:44:12 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 2 Aug 2006 10:44:12 -0700
Subject: [R] listing of permutations
In-Reply-To: <005201c6b659$83d5f6a0$711f210a@gne.windows.gene.com>
Message-ID: <006601c6b65b$44749cd0$711f210a@gne.windows.gene.com>

I seem to be on a roll of being dumb today. Sorry for posting my previous
silly solution to Erin's permutation problem. Please **do** ignore it.

-- Bert
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
> Sent: Wednesday, August 02, 2006 10:32 AM
> To: 'Erin Hodgess'; r-help at stat.math.ethz.ch
> Subject: Re: [R] listing of permutations
> 
> Erin:
> You got 2 (so far) pre-packaged functions .Here's an 
> obscenely inefficient
> but short un-prepackaged way to do it:
> 
> k<-4
> z<- do.call('expand.grid',as.data.frame(matrix(rep(1:k,k),nc=k)))
> results<- z[apply(z,1,function(x)length(unique(x))==k),]
> 
> It is too inefficient to make public, though.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
> > Sent: Wednesday, August 02, 2006 9:57 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] listing of permutations
> > 
> > Dear R People:
> > 
> > Suppose I have the 4 numbers: 1,2,3,4.
> > 
> > I would like to create a listing of the permutations
> > of 4 items taken 4 at a time.
> > 
> > Is there a built in function for that, please?
> > 
> > Thanks in advance!
> > R 2-3-1 for Windows or Linux
> > Sincerely,
> > Erin Hodgess
> > Associate Professor
> > Department of Computer and Mathematical Sciences
> > University of Houston - Downtown
> > mailto: hodgess at gator.uhd.edu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From heberto.ghezzo at mcgill.ca  Wed Aug  2 19:48:33 2006
From: heberto.ghezzo at mcgill.ca (R Heberto Ghezzo, Dr)
Date: Wed, 2 Aug 2006 13:48:33 -0400
Subject: [R] Correlation adjusted Bonferroni? (was: Multiple tests on
	repeated measurements)
References: <63E04C5ADEDACB4989972239CDABF05782BEEE@chlsne01.nestle.com>
	<44D0615D.9090702@pdf.com>
Message-ID: <05BE78B0CF1BBC4BBA4AA255568D8611029A9956@EXCHANGE2VS1.campus.mcgill.ca>

HI, just my 2 cents. Bonferroni et al, assume independent tests, thus p ~ p*k with k the number of tests, in repeated measures, each measure is correlated with the previous, so k is not 2 but 2-q. I do not know q but it should be a function of the correlation between measures, ie the Sigma in gee.
A long time ago, in my days as a student i remember, vagely, I saw a paper in Biometics on a correction to apply to Fisher method for pooling several tests to take into account the correlation between the variables.
Heberto Ghezzo
McGill University
Montreal - Canada


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of Spencer Graves
Sent: Wed 8/2/2006 4:25 AM
To: Grathwohl, Dominik, LAUSANNE, NRC-BAS
Cc: r-help at stat.math.ethz.ch; Torsten Hothorn
Subject: Re: [R] Correlation adjusted Bonferroni? (was: Multiple tests on repeated measurements)
 
	  I'm not familiar with the correlation adjustment to Bonferroni you 
mention below, though it sounds interesting.  However, I think there is 
something not right about it or about how you have interpreted it.  Your 
code produced the following for me:

	 p.value.raw p.value.bon p.value.adj
           = raw.p      = bon.p   =multcomp.p "bon.cor.p"
diff/v=0 0.028572509 0.057145019 0.054951102 0.034934913
diff/v=1 0.001727993 0.003455987 0.003415545 0.002119276

	  In the absence of other information, I'd be inclined to believe 
csimint(..)$p.value.adj or ..$p.value.bon over your "bon.cor.p".

	
	  Hope this helps.
	  Spencer Graves

Grathwohl, Dominik, LAUSANNE, NRC-BAS wrote:
> Dear R-helpers:
> 
> My question is how do I efficient and valid correct for multiple tests in a repeated measurement design: 
> Suppose we measure at two distinct visits with repeated subjects a treatment difference on the same variable. 
> The treatment differences are assessed with a mixed model and adjusted by two methods for multiple tests:
> 
> # 1. Method: Adjustment with library(multcomp)
> 
> library(nlme)
> library(multcomp)
> 
> n <- 30 # number of subjects
> sd1 <- 0.5 # Standard deviation of the random intercept
> sd2 <- 0.8 # Standard deviation of the residuals
> id <- rep(1:n,times=2); v <- rep(0:1, each=n); trt <- rep(sample(rep(0:1, each=n/2), n), times=2)
> df <- data.frame(id, v, trt, 
> y=2 + rep(rnorm(10,0,sd1), times=2) + 0.5*v + 0.7*trt + 0.2*v*trt + rnorm(2*n, 0, sd2))
> m1 <- lme(y ~ v + trt + v*trt, data=df, random= ~ 1|id)
> summary(m1)
> par4 <- m1$coef$fixed
> cov4 <- vcov(m1)
> cm4 <- matrix(c(0, 0, 1, 0, 0, 0, 1, 1), nrow = 2, ncol=4, byrow=TRUE, 
> 	dimnames = list(c("diff/v=0", "diff/v=1"), c("C.1", "C.2", "C.3", "C.4")))
> v4 <- csimint(estpar=par4, df=n-6, # I'm not sure whether I found 
>      # the correct degrees of freedom
> 	covm=cov4,
> 	cmatrix=cm4, conf.level=0.95)
> sv4 <- summary(v4)
> 
> # 2. Method: I found in Handbook of Statistics Vol 13, p.616,
> # same can be found in http://home.clara.net/sisa/bonhlp.htm
> # Bonferroni on correlated outcomes:
> 
> raw.p <- sv4$p.value.raw
> co4 <- cor(df$y[df$v==0],df$y[df$v==1])
> rho <- mean(c(1,co4,co4,1))
> pai <- 1-(1-raw.p)^2^(1-rho) 
> 
> # The results of two methods are presented in the following lines:
> out <- cbind(raw.p, sv4$p.value.bon, sv4$p.value.adj, pai)
> colnames(out) <- c("raw.p", "bon.p", "multcomp.p", "bon.cor.p")
> out
> 
> As you can see there are quite big differences 
> between the two ways adjusting for multiple tests on repeated measurements. 
> I guess that the multcomp library is not appropriate for this kind of hypotheses. 
> However I could not find an explanation in the help files. 
> May be one of the experts can point me in the right direction?
> 
> Kind regards,
> 
> Dominik
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.1            
> year     2005           
> month    12             
> day      20             
> svn rev  36812          
> language R    
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Aug  2 20:29:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Aug 2006 19:29:28 +0100 (BST)
Subject: [R] Summary method needed?
In-Reply-To: <Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0608021914430.27720@gannet.stats.ox.ac.uk>

On Wed, 2 Aug 2006, Christian Hennig wrote:

> Hi list,
> 
> I'm updating my fpc package at the moment and will add some new functions. 
> I learned that there should be print and summary methods for the key
> functions.

for 'classes', I think.

> The purpose of the summary methods seems to be to reduce the 
> possibly incredibly complex information in the function's output and the 
> print method (print.summary.foo) should print an overview of the result.

Normally, summary() gives more information than print() would give
for a non-data object, often by manipulations on the object.

Now, the White Book said that summary produces `a synopsis of an object', 
but that does not seem to be the practice for model-fitting classes even 
in the White Book (but it is for data objects).

> But in some cases the print method will make use of more or less all the 
> output information of the function. Is there any reason to implement a 
> summary method in these cases?

Would a more concise print() method be useful?  If so the existing print() 
could become summary().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From chrish at stats.ucl.ac.uk  Wed Aug  2 20:59:19 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 2 Aug 2006 19:59:19 +0100 (BST)
Subject: [R] Summary method needed?
In-Reply-To: <Pine.LNX.4.64.0608021914430.27720@gannet.stats.ox.ac.uk>
References: <Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608021914430.27720@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0608021953560.2736@egon.stats.ucl.ac.uk>

Thank you Brian!

>> I'm updating my fpc package at the moment and will add some new functions.
>> I learned that there should be print and summary methods for the key
>> functions.
>
> for 'classes', I think.

Yes.

>> But in some cases the print method will make use of more or less all the
>> output information of the function. Is there any reason to implement a
>> summary method in these cases?
>
> Would a more concise print() method be useful?  If so the existing print()
> could become summary().

:-)
What I initially did some years ago was to write summary methods to print 
out the required informations. Then M. Maechler told me that this is not 
the purpose of a summary method and I should write a print.summary method 
for this. Now I realise that I actually just want to print, and I don't 
really need the extra "synopsis" to be done by summary().

Now is there any recommendation on this? My intuition would be to write a 
print, but not a summary method.

Christian

>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From kwright68 at gmail.com  Wed Aug  2 21:25:20 2006
From: kwright68 at gmail.com (Kevin Wright)
Date: Wed, 2 Aug 2006 14:25:20 -0500
Subject: [R] lme4 and lmeSplines
Message-ID: <adf71a630608021225u73a2a547i9ba43fb930980a04@mail.gmail.com>

I'm trying to use the lmeSplines package together with lme4.

Below is (1) an example of lmeSplines together with nlme (2) an
attempt to use lmeSplines with lme4 (3) then a comparison of the
random effects from the two different methods.

(1)

require(lmeSplines)
data(smSplineEx1)
dat <- smSplineEx1
dat.lo <- loess(y~time, data=dat)
plot(dat.lo)
dat$all <- rep(1,nrow(dat))
times20 <- seq(1,100,length=20)
Zt20 <- smspline(times20)
dat$Zt20 <- approx.Z(Zt20, times20, dat$time)
fit1.20 <- lme(y~time, data=dat, random=list(all=pdIdent(~Zt20-1)))
# Loess model
dat.lo <- loess(y~time, data=dat)
plot(dat.lo)
# Spline model
with(dat, lines(fitted(fit1.20)~time, col="red"))
# Save random effects for later
ranef.nlme <- unlist(ranef(fit1.20))

(2) Now an attempt to use lme4:

library(lmeSplines)
detach(package:nlme)
library(lme4)
data(smSplineEx1)
# Use 20 spline in lme4
dat <- smSplineEx1
times20 <- seq(1,100,length=20)
Zt20 <- smspline(times20)
dat <- cbind(dat, approx.Z(Zt20, times20, dat$time))
names(dat)[4:21] <- paste("Zt",names(dat)[4:21],sep="")
dat$all <- rep(1, nrow(dat))
fit1.20 <- lmer(y~time
             +(-1+Zt1|all)+(-1+Zt2|all)+(-1+Zt3|all)+(-1+Zt4|all)+(-1+Zt5|all)+(-1+Zt6|all)
             +(-1+Zt7|all)+(-1+Zt8|all)+(-1+Zt9|all)+(-1+Zt10|all)+(-1+Zt11|all)+(-1+Zt12|all)
             +(-1+Zt13|all)+(-1+Zt14|all)+(-1+Zt15|all)+(-1+Zt16|all)+(-1+Zt17|all)+(-1+Zt18|all),
             data=dat)
#summary(fit1)
# Plot the data and loess fit
dat.lo <- loess(y~time, data=dat)
plot(dat.lo)
# Fitting with splines
with(dat, lines(fitted(fit1.20)~time, col="red"))
ranef.lme4 <- unlist(ranef(fit1.20))

(3) Compare nlme lme4 random effects

plot(ranef.nlme~ranef.lme4)

The plot of fitted values from lme4 is visually appealing, but the
random effects from lme4 are peculiar--three are non-zero and the rest
are essentially zero.

Any help in getting lme4 + lmeSplines working would be appreciated.
It is not unlikely that I have the lmer syntax wrong.

Kevin Wright


From s-walker at ti.com  Wed Aug  2 21:43:37 2006
From: s-walker at ti.com (Walker, Sam)
Date: Wed, 2 Aug 2006 14:43:37 -0500
Subject: [R] ggplot facet label font size
Message-ID: <1F5C95F1B887EF42B14F31AEF36AD22F05FEB78F@dlee04.ent.ti.com>

How do I change the font size in the facet labels along the edges of the
plot?

For example (from the ggplot help file):
     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
     gghistogram(p)

In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
Female", "sex: Male".  What command can I use to reduce the font size of
these labels?

In lattice terminology, cex is used to scale these strip labels.  But I
couldn't find the equivalent in ggplot.

The reason I'm asking is I have a 9x7 array of plots which I've been
plotting with lattice.  I wanted to use ggplot because I like having the
labels on the edge of the plots, but the label font size is too large
and exceeding the size of the label box.

Thanks in advance...
-Sam


From loesljrg at verizon.net  Wed Aug  2 21:50:04 2006
From: loesljrg at verizon.net (JRG)
Date: Wed, 02 Aug 2006 15:50:04 -0400
Subject: [R] Correlation adjusted Bonferroni? (was: Multiple tests on
 repeated measurements)
In-Reply-To: <05BE78B0CF1BBC4BBA4AA255568D8611029A9956@EXCHANGE2VS1.campus.mcgill.ca>
Message-ID: <44D0C9AC.14756.1A0D972@loesljrg.verizon.net>

On 2 Aug 2006 at 13:48, R Heberto Ghezzo, Dr wrote:

> HI, just my 2 cents. Bonferroni et al, assume independent tests, thus
> p ~ p*k with k the number of tests, in repeated measures, each measure
> is correlated with the previous, so k is not 2 but 2-q. I do not know
> q but it should be a function of the correlation between measures, ie
> the Sigma in gee. A long time ago, in my days as a student i remember,
> vagely, I saw a paper in Biometics on a correction to apply to Fisher
> method for pooling several tests to take into account the correlation
> between the variables. Heberto Ghezzo McGill University Montreal - Canada 
> 
> 

Most assuredly, the (usual) Bonferroni correction does not assume independent test statistics, as 
it is based on Bonferroni's Inequality which holds for absolutely any collection of valid tests.  
If the test statistics were independent 1-(1-p)^k would replace p*k.  But strong correlation among 
test statistics can certainly reduce the limit well below 1-(1-p)^k .

---JRG


John R. Gleason

Syracuse University
430 Huntington Hall                      Voice:   315-443-3107
Syracuse, NY 13244-2340  USA             FAX:     315-443-4085

PGP public key at keyservers


From ripley at stats.ox.ac.uk  Wed Aug  2 22:04:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Aug 2006 21:04:36 +0100 (BST)
Subject: [R] Summary method needed?
In-Reply-To: <Pine.LNX.4.64.0608021953560.2736@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608021914430.27720@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0608021953560.2736@egon.stats.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0608022103430.31307@gannet.stats.ox.ac.uk>

On Wed, 2 Aug 2006, Christian Hennig wrote:

> Thank you Brian!
> 
> > > I'm updating my fpc package at the moment and will add some new functions.
> > > I learned that there should be print and summary methods for the key
> > > functions.
> >
> > for 'classes', I think.
> 
> Yes.
> 
> > > But in some cases the print method will make use of more or less all the
> > > output information of the function. Is there any reason to implement a
> > > summary method in these cases?
> >
> > Would a more concise print() method be useful?  If so the existing print()
> > could become summary().
> 
> :-)
> What I initially did some years ago was to write summary methods to print out
> the required informations. Then M. Maechler told me that this is not the
> purpose of a summary method and I should write a print.summary method for
> this. Now I realise that I actually just want to print, and I don't really
> need the extra "synopsis" to be done by summary().
> 
> Now is there any recommendation on this? My intuition would be to write a
> print, but not a summary method.

That sounds fine for your purposes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From franco.mendolia at gmx.de  Wed Aug  2 22:06:17 2006
From: franco.mendolia at gmx.de (Franco Mendolia)
Date: Wed, 02 Aug 2006 22:06:17 +0200
Subject: [R] tcl/tk bind destroy event
Message-ID: <44D105B9.3000306@gmx.de>

Hello!

I want to create a messagebox whenever the user wants to destroy the window 
(e.g. <Alt-F4> or the 'x' in the right top corner) and ask if a modified file 
should be saved or not.

If 'cancel' is chosen then nothing should happen and the windows still should be 
existing. This doesn't work. When I press cancel the window will be destroyed 
although.

I also implemented a menu item 'Quit' where I show the same messagebox and there 
it works fine.

How can I make it work or is there another method to do this? I'm very new to R 
and tcl/tk.

Here is part of my code:


   exitProg <- function()
   {
     returnVal <- tkmessageBox(title="Question",
       message="Save modified file?",
       icon="question", type="yesnocancel", default="yes")

     returnVal <- as.character(returnVal)

     if( returnVal == "yes" )
     {
       # save file
       value <- saveFile()
       # destroy window when save was successfull
       if( value == 1 )
         tkdestroy(mw)
     }
     if( returnVal == "no" )
     {
       tkdestroy(mw)
     }
     if( returnVal == "cancel" )
     {
       # do nothing
       cat("Cancel was pressed.\n")
     }
   }

   # bind the destroy event in order to show a message box
   tkbind(mw,"<Destroy>",exitProg)

   # menu item which works fine
   tkadd(fileMenu, "command", label="Quit", command=exitProg)



Thank you.

Franco Mendolia


From elvis at xlsolutions-corp.com  Wed Aug  2 22:22:36 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Wed, 02 Aug 2006 13:22:36 -0700
Subject: [R] Course***Dr Frank Harrell's Regression Modeling Strategies in
	R/Splus course *** September 2006 near you (San Francisco,
	Washington DC, Atlanta)
Message-ID: <20060802132236.9f08cc34deb45d78e54b3b5664e21546.0f72940bc5.wbe@email.secureserver.net>


From jrkrideau at yahoo.ca  Wed Aug  2 23:01:53 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 2 Aug 2006 17:01:53 -0400 (EDT)
Subject: [R] Finding the position of a  variable in a data.frame
Message-ID: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>

Simple problem but I don't see the answer. I'm trying
to clean up some data
I have 120 columns in a data.frame.  I have one value
in a column named "blaw" that I want to change. How do
I find the coordinates. I can find the row by doing a
subset on the data.frame but how do I find out here
"blaw " is in columns without manually counting them
or converting names(Df) to a list and reading down the
list.

Simple example

cat <- c( 3,5,6,8,0)
dog <- c(3,5,3,6, 0)
rat <- c (5, 5, 4, 9, 0)
bat <- c( 12, 42, 45, 32, 54)

Df <- data.frame(cbind(cat, dog, rat, bat))
Df
subset(Df, bat >= 50)

----results
  cat dog rat bat
5   0   0   0  54


Thus I know that my target is in row 5 but how do I
figure out where 'bat' is?  

All I want to do is be able to say
Df[5,4] <- 100

Is there some way to have function(bat) return the
column number: some kind of a colnum() function?  I
had thought that I had found somthing  in
library(gdata) matchcols but no luck.


From jholtman at gmail.com  Wed Aug  2 23:10:50 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 2 Aug 2006 17:10:50 -0400
Subject: [R] Finding the position of a variable in a data.frame
In-Reply-To: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
References: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
Message-ID: <644e1f320608021410g53c0fc7bx5183867fd7c6ea55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/680c5e88/attachment.pl 

From rkoenker at uiuc.edu  Wed Aug  2 23:12:59 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 2 Aug 2006 16:12:59 -0500
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
References: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
Message-ID: <50637AD4-574B-4DB6-A626-8A0C9D026DCB@uiuc.edu>

it is the well-known wicked which problem:  if you had (grammatically  
incorrectly)
thought "... which I want to change" then you might have been led
to type (in another window):

	?which

and you would have seen the light.  Maybe that() should be an alias
for which()?

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Aug 2, 2006, at 4:01 PM, John Kane wrote:

> Simple problem but I don't see the answer. I'm trying
> to clean up some data
> I have 120 columns in a data.frame.  I have one value
> in a column named "blaw" that I want to change. How do
> I find the coordinates. I can find the row by doing a
> subset on the data.frame but how do I find out here
> "blaw " is in columns without manually counting them
> or converting names(Df) to a list and reading down the
> list.
>
> Simple example
>
> cat <- c( 3,5,6,8,0)
> dog <- c(3,5,3,6, 0)
> rat <- c (5, 5, 4, 9, 0)
> bat <- c( 12, 42, 45, 32, 54)
>
> Df <- data.frame(cbind(cat, dog, rat, bat))
> Df
> subset(Df, bat >= 50)
>
> ----results
>   cat dog rat bat
> 5   0   0   0  54
>
>
> Thus I know that my target is in row 5 but how do I
> figure out where 'bat' is?
>
> All I want to do is be able to say
> Df[5,4] <- 100
>
> Is there some way to have function(bat) return the
> column number: some kind of a colnum() function?  I
> had thought that I had found somthing  in
> library(gdata) matchcols but no luck.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ccleland at optonline.net  Wed Aug  2 23:12:53 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 02 Aug 2006 17:12:53 -0400
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
References: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
Message-ID: <44D11555.2010704@optonline.net>

John Kane wrote:
> Simple problem but I don't see the answer. I'm trying
> to clean up some data
> I have 120 columns in a data.frame.  I have one value
> in a column named "blaw" that I want to change. How do
> I find the coordinates. I can find the row by doing a
> subset on the data.frame but how do I find out here
> "blaw " is in columns without manually counting them
> or converting names(Df) to a list and reading down the
> list.
> 
> Simple example
> 
> cat <- c( 3,5,6,8,0)
> dog <- c(3,5,3,6, 0)
> rat <- c (5, 5, 4, 9, 0)
> bat <- c( 12, 42, 45, 32, 54)
> 
> Df <- data.frame(cbind(cat, dog, rat, bat))
> Df
> subset(Df, bat >= 50)
> 
> ----results
>   cat dog rat bat
> 5   0   0   0  54
> 
> 
> Thus I know that my target is in row 5 but how do I
> figure out where 'bat' is?  

grep("bat", names(Df))

> All I want to do is be able to say
> Df[5,4] <- 100

Why not do it this way?

Df$bat <- replace(Df$bat, Df$bat >=50, 100)

> Is there some way to have function(bat) return the
> column number: some kind of a colnum() function?  I
> had thought that I had found somthing  in
> library(gdata) matchcols but no luck.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jrkrideau at yahoo.ca  Wed Aug  2 23:21:45 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 2 Aug 2006 17:21:45 -0400 (EDT)
Subject: [R] Finding the position of a variable in a data.frame
In-Reply-To: <644e1f320608021410g53c0fc7bx5183867fd7c6ea55@mail.gmail.com>
Message-ID: <20060802212145.35114.qmail@web33811.mail.mud.yahoo.com>


--- jim holtman <jholtman at gmail.com> wrote:

> ?which
> 
> > which(Df >= 50, arr.ind=T)
>   row col
> 5   5   4



I knew it was going to be blinding obvious! I even
read 
?which somehow misunderstood arr.ind. 

Thanks again.
> 
> On 8/2/06, John Kane <jrkrideau at yahoo.ca> wrote:
> >
> > Simple problem but I don't see the answer. I'm
> trying
> > to clean up some data
> > I have 120 columns in a data.frame.  I have one
> value
> > in a column named "blaw" that I want to change.
> How do
> > I find the coordinates. I can find the row by
> doing a
> > subset on the data.frame but how do I find out
> here
> > "blaw " is in columns without manually counting
> them
> > or converting names(Df) to a list and reading down
> the
> > list.
> >
> > Simple example
> >
> > cat <- c( 3,5,6,8,0)
> > dog <- c(3,5,3,6, 0)
> > rat <- c (5, 5, 4, 9, 0)
> > bat <- c( 12, 42, 45, 32, 54)
> >
> > Df <- data.frame(cbind(cat, dog, rat, bat))
> > Df
> > subset(Df, bat >= 50)
> >
> > ----results
> > cat dog rat bat
> > 5   0   0   0  54
> >
> >
> > Thus I know that my target is in row 5 but how do
> I
> > figure out where 'bat' is?
> >
> > All I want to do is be able to say
> > Df[5,4] <- 100
> >
> > Is there some way to have function(bat) return the
> > column number: some kind of a colnum() function? 
> I
> > had thought that I had found somthing  in
> > library(gdata) matchcols but no luck.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
>


From jrkrideau at yahoo.ca  Wed Aug  2 23:24:50 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 2 Aug 2006 17:24:50 -0400 (EDT)
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <44D11555.2010704@optonline.net>
Message-ID: <20060802212450.82965.qmail@web33815.mail.mud.yahoo.com>


--- Chuck Cleland <ccleland at optonline.net> wrote:

> John Kane wrote:
> > Simple problem but I don't see the answer. I'm
> trying
> > to clean up some data
> > I have 120 columns in a data.frame.  I have one
> value
> > in a column named "blaw" that I want to change.
> How do
> > I find the coordinates. I can find the row by
> doing a
> > subset on the data.frame but how do I find out
> here
> > "blaw " is in columns without manually counting
> them
> > or converting names(Df) to a list and reading down
> the
> > list.
> > 
> > Simple example
> > 
> > cat <- c( 3,5,6,8,0)
> > dog <- c(3,5,3,6, 0)
> > rat <- c (5, 5, 4, 9, 0)
> > bat <- c( 12, 42, 45, 32, 54)
> > 
> > Df <- data.frame(cbind(cat, dog, rat, bat))
> > Df
> > subset(Df, bat >= 50)
> > 
> > ----results
> >   cat dog rat bat
> > 5   0   0   0  54
> > 
> > 
> > Thus I know that my target is in row 5 but how do
> I
> > figure out where 'bat' is?  
> 
> grep("bat", names(Df))

Thank you, I have never used grep.  More reading :(

> 
> > All I want to do is be able to say
> > Df[5,4] <- 100
> 
> Why not do it this way?
> 
> Df$bat <- replace(Df$bat, Df$bat >=50, 100)

Maybe because it is too blinding simple?  I'm still
thinking in SAS or Systat.

A great solution 

thanks very much

> 
> > Is there some way to have function(bat) return the
> > column number: some kind of a colnum() function? 
> I
> > had thought that I had found somthing  in
> > library(gdata) matchcols but no luck.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>


From jrkrideau at yahoo.ca  Wed Aug  2 23:26:44 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 2 Aug 2006 17:26:44 -0400 (EDT)
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <50637AD4-574B-4DB6-A626-8A0C9D026DCB@uiuc.edu>
Message-ID: <20060802212644.57365.qmail@web33806.mail.mud.yahoo.com>


--- roger koenker <rkoenker at uiuc.edu> wrote:

> it is the well-known wicked which problem:  if you
> had (grammatically  
> incorrectly)
> thought "... which I want to change" then you might
> have been led
> to type (in another window):
> 
> 	?which
> 
> and you would have seen the light.  Maybe that()
> should be an alias
> for which()?

One also has to understand which and I missed it.  I
don't think that which is really that grammatically
incorrect anymore.

Thanks
> 
> url:    www.econ.uiuc.edu/~roger            Roger
> Koenker
> email    rkoenker at uiuc.edu            Department of
> Economics
> vox:     217-333-4558                University of
> Illinois
> fax:       217-244-6678                Champaign, IL
> 61820
> 
> 
> On Aug 2, 2006, at 4:01 PM, John Kane wrote:
> 
> > Simple problem but I don't see the answer. I'm
> trying
> > to clean up some data
> > I have 120 columns in a data.frame.  I have one
> value
> > in a column named "blaw" that I want to change.
> How do
> > I find the coordinates. I can find the row by
> doing a
> > subset on the data.frame but how do I find out
> here
> > "blaw " is in columns without manually counting
> them
> > or converting names(Df) to a list and reading down
> the
> > list.
> >
> > Simple example
> >
> > cat <- c( 3,5,6,8,0)
> > dog <- c(3,5,3,6, 0)
> > rat <- c (5, 5, 4, 9, 0)
> > bat <- c( 12, 42, 45, 32, 54)
> >
> > Df <- data.frame(cbind(cat, dog, rat, bat))
> > Df
> > subset(Df, bat >= 50)
> >
> > ----results
> >   cat dog rat bat
> > 5   0   0   0  54
> >
> >
> > Thus I know that my target is in row 5 but how do
> I
> > figure out where 'bat' is?
> >
> > All I want to do is be able to say
> > Df[5,4] <- 100
> >
> > Is there some way to have function(bat) return the
> > column number: some kind of a colnum() function? 
> I
> > had thought that I had found somthing  in
> > library(gdata) matchcols but no luck.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting- 
> > guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
>


From jz7 at duke.edu  Wed Aug  2 23:29:47 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Wed, 2 Aug 2006 17:29:47 -0400 (EDT)
Subject: [R] question about stdize() in PLS package
Message-ID: <Pine.GSO.4.58.0608021718551.23378@godzilla.acpub.duke.edu>

Dear all,

I am using the PLS package for PLSR analysis. And I have a basic question
about the standardize procedure, which I feel the PLS manual does not
explain clearly. I am hoping that I could get some help from the list.

>From the example in the "Standardization of Data Matrices" section, I can
standardize X matrix and make prediction by using:

mod=plsr(y~stdize(X),ncomp=6,data=NIR[NIR$train,])
pred=predict(mod,newdata=NIR[!NIR$train,])

In the manual, it is commented that the prediction is "automatically
standardized". So I guess I won't need to standardize X matrix of the test
set for the prediction.

However, what if I do not want a standardize model from the beginning?
Then my code would be like:

mod=plsr(y~X,ncomp=6,data=NIR[NIR$train,])

But the R code for the prediction should still be the same (please
correct me if any code is wrong):

pred=predict(mod,newdata=NIR[!NIR$train,])

Would this time the X matrix of the newdata be automatical standardized or
not?

I am so confused about the "automatically standardization". Please share
some experience. Really appreciate your kind help!

Sincerely,
Jeny


From Kaushik.Katari at signaldemand.com  Wed Aug  2 23:56:19 2006
From: Kaushik.Katari at signaldemand.com (Kaushik Katari)
Date: Wed, 2 Aug 2006 17:56:19 -0400
Subject: [R] help with formatting legend in xyplot
Message-ID: <65531D426735784F854EE658938A5853052B1365@MI8NYCMAIL03.Mi8.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/8e2e1710/attachment.pl 

From phhs80 at gmail.com  Thu Aug  3 00:01:46 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 2 Aug 2006 23:01:46 +0100
Subject: [R] Syntax of Levene's test
In-Reply-To: <20060802123925.SXBV1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <6ade6f6c0608020332tf81f3a5v531c789c67a5cf80@mail.gmail.com>
	<20060802123925.SXBV1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <6ade6f6c0608021501m3ad73f54yf505dba93f75575e@mail.gmail.com>

On 8/2/06, John Fox <jfox at mcmaster.ca> wrote:
> The argument y is the response variable and group is a factor defining
> groups (as ?levene.test says). If you have more than one factor, then you
> can use interaction() to create from them a factor with levels given by the
> product set of the levels of the individual factors. Here's an example
>
> > library(car)
> > data(Moore)
> > attach(Moore)
> > levene.test(conformity, interaction(fcategory, partner.status))
> Levene's Test for Homogeneity of Variance
>       Df F value Pr(>F)
> group  5  1.4694 0.2219
>       39
> > levels(interaction(fcategory, partner.status))
> [1] "high.high"   "low.high"    "medium.high" "high.low"    "low.low"
> [6] "medium.low"
> > levels(fcategory)
> [1] "high"   "low"    "medium"
> > levels(partner.status)
> [1] "high" "low"
>
> I'll add a couple of examples to the help page.

Thanks, John. Now, I understand how to use levene.test. There is only
a question remaining: is the null hypothesis corresponding to
homogeneity of variances, i.e., should one conclude that

Levene's Test for Homogeneity of Variance
       Df F value    Pr(>F)
group  95  3.5919 < 2.2e-16 ***
      864

tell us that the hypothesis that the variances are equal is (highly)
significant?

Paul


From ggrothendieck at gmail.com  Thu Aug  3 00:16:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Aug 2006 18:16:52 -0400
Subject: [R] help with formatting legend in xyplot
In-Reply-To: <65531D426735784F854EE658938A5853052B1365@MI8NYCMAIL03.Mi8.com>
References: <65531D426735784F854EE658938A5853052B1365@MI8NYCMAIL03.Mi8.com>
Message-ID: <971536df0608021516i2bf9b0f1ybd60c84eedc083ef@mail.gmail.com>

If you set it through par.settings then it will affect both the
drawing and the legend:

xyplot(Sepal.Length ~ Petal.Length, iris, groups = Species, auto.key = TRUE,
   par.settings = list(superpose.symbol = list(pch = "*", cex = 1)))

On 8/2/06, Kaushik Katari <Kaushik.Katari at signaldemand.com> wrote:
> I am doing a  xyplot: (x~y, groups = z, pch=8, auto.key=T). This changes
> the symbol in the graph to an asterisk (*), but not in the legend, which
> is still an open circle. I have found out how to manipulate the position
> and the color of the letters in the legend, but cannot change the legend
> symbol to match the symbol in the graph. Could you help?
>
>
>
> Thanks,
>
> Kaushik
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From CodyH at BaylorHealth.edu  Thu Aug  3 00:38:55 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 2 Aug 2006 17:38:55 -0500
Subject: [R] Baseline levels summary.Design
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB68FE@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/4703996b/attachment.pl 

From gerald.jansen at newpage.ca  Thu Aug  3 00:53:02 2006
From: gerald.jansen at newpage.ca (Gerald Jansen)
Date: Wed, 2 Aug 2006 18:53:02 -0400
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <44D11555.2010704@optonline.net>
References: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
	<44D11555.2010704@optonline.net>
Message-ID: <20060802185302.07896e0c@hum>

On Wed, 02 Aug 2006 17:12:53 -0400, Chuck Cleland wrote:
> Why not do it this way?
> 
> Df$bat <- replace(Df$bat, Df$bat >=50, 100)

Is that any different, performancewise, than the following?

Df$bat[Df$bat >= 50] <- 100

Gerald Jansen


...
John Kane wrote:
> Simple example
> 
> cat <- c( 3,5,6,8,0)
> dog <- c(3,5,3,6, 0)
> rat <- c (5, 5, 4, 9, 0)
> bat <- c( 12, 42, 45, 32, 54)
> 
> Df <- data.frame(cbind(cat, dog, rat, bat))
...


From murray.logan at sci.monash.edu.au  Thu Aug  3 00:25:03 2006
From: murray.logan at sci.monash.edu.au (Murray Logan)
Date: Thu, 03 Aug 2006 08:25:03 +1000
Subject: [R] unbalanced mixed effects models for fully factorial designs
Message-ID: <44D1263F.3050306@sci.monash.edu.au>

Does anyone know of a way of dealing with unbalanced mixed effects 
(fixed and random factors) for fully factorial designs.

An example of such data is given below;

The response variable is SQRTRECRUITS
SEASON is a random factor
DENSITY is a fixed factor
Thus DENSITY:SEASON is a fixed factor.

Therefore, whereas the effects of SEASON and DENSITY:SEASON should be 
tested against the overall residual (error) term, the effect of DENSITY 
should be tested against the DENSITY:SEASON interaction.
To complicate matters, the data are unbalanced, and thus Type III SS are 
preferable

quinn <-
structure(list(SEASON = structure(as.integer(c(2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4)), .Label = c("Autumn",
"Spring", "Summer", "Winter"), class = "factor", contrasts = "contr.sum"),
    DENSITY = structure(as.integer(c(2, 2, 2, 2, 2, 1, 1, 1,
    1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,
    1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1)), .Label = c("High",
    "Low"), class = "factor"), RECRUITS = as.integer(c(15, 10,
    13, 13, 5, 11, 10, 15, 10, 13, 1, 21, 31, 21, 18, 14, 27,
    34, 49, 69, 55, 28, 54, 14, 18, 20, 21, 4, 22, 30, 36, 13,
    13, 8, 0, 0, 10, 1, 5, 9, 4, 5)), SQRTRECRUITS = c(3.872983,
    3.162278, 3.605551, 3.605551, 2.236068, 3.316625, 3.162278,
    3.872983, 3.162278, 3.605551, 1, 4.582576, 5.567764, 4.582576,
    4.242641, 3.741657, 5.196152, 5.830952, 7, 8.306624, 7.416198,
    5.291503, 7.348469, 3.741657, 4.242641, 4.472136, 4.582576,
    2, 4.690416, 5.477226, 6, 3.605551, 3.605551, 2.828427, 0,
    0, 3.162278, 1, 2.236068, 3, 2, 2.236068), GROUP = 
structure(as.integer(c(4,
    4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 5, 5, 5,
    5, 5, 5, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 8, 8, 8, 7, 7, 7,
    7, 7, 7)), .Label = c("AutumnHigh", "AutumnLow", "SpringHigh",
    "SpringLow", "SummerHigh", "SummerLow", "WinterHigh", "WinterLow"
    ), class = "factor")), .Names = c("SEASON", "DENSITY", "RECRUITS",
"SQRTRECRUITS", "GROUP"), row.names = c("1", "2", "3", "4", "5",
"6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
"17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27",
"28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38",
"39", "40", "41", "42"), class = "data.frame")

I realise that Anova (car package) calculated Type III SS (given the 
correct contrasts), however, this does not permit mixed models.  
Conversely, if I was to specify a aov model such as;
summary(aov(SQRTRECRUITS ~ SEASON+DENSITY+Error(DENSITY:SEASON), 
data=quinn))
purely to obtain a test for DENSITY (ignoring the test for SEASON), the 
SS are Type I.

Although it is possible to calculate out the F-ratio (and p-value) 
calculations manually and substitute them into the anova tables, I cant 
help think that there must be a better solution.

Is there any expectation that there will be a summary routine that 
provides Type II and Type II SS, and or is aov ever likely to 
accommodate non-hierarchical mixed models?

Regards

Murray


From ggrothendieck at gmail.com  Thu Aug  3 01:03:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Aug 2006 19:03:36 -0400
Subject: [R] ggplot facet label font size
In-Reply-To: <1F5C95F1B887EF42B14F31AEF36AD22F05FEB78F@dlee04.ent.ti.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F05FEB78F@dlee04.ent.ti.com>
Message-ID: <971536df0608021603n6d68b4c2kf26661dac0ce998b@mail.gmail.com>

On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> How do I change the font size in the facet labels along the edges of the
> plot?
>
> For example (from the ggplot help file):
>     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
>     gghistogram(p)
>
> In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> Female", "sex: Male".  What command can I use to reduce the font size of
> these labels?
>
> In lattice terminology, cex is used to scale these strip labels.  But I
> couldn't find the equivalent in ggplot.
>
> The reason I'm asking is I have a 9x7 array of plots which I've been
> plotting with lattice.  I wanted to use ggplot because I like having the
> labels on the edge of the plots

Note that lattice can do that by using custom strip functions:

library(ggplot) # data resides here
library(lattice)

my.strip <- function(which.given, which.panel, ...)
   if (which.given == 1 && which.panel[2] == 2)
      strip.default(which.given, which.panel, ...)

my.strip.left <- function(which.given, which.panel, ..., horizontal)
   if (which.given == 2 && which.panel[1] == 1)
      strip.default(which.given, which.panel, horizontal = FALSE, ...)

histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
     strip.left = my.strip.left, par.settings = list(add.text =
list(cex = 0.7)))


From jfox at mcmaster.ca  Thu Aug  3 02:00:58 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 2 Aug 2006 20:00:58 -0400
Subject: [R] Syntax of Levene's test
In-Reply-To: <6ade6f6c0608021501m3ad73f54yf505dba93f75575e@mail.gmail.com>
Message-ID: <20060803000057.UKNI13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Paul,

Levene's test tests the null hypothesis that the variance are equal, so a
small p-value suggests that they are not. Looking at your output, it seems
odd that you have as many as 96 groups.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Wednesday, August 02, 2006 5:02 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Syntax of Levene's test
> 
> On 8/2/06, John Fox <jfox at mcmaster.ca> wrote:
> > The argument y is the response variable and group is a 
> factor defining 
> > groups (as ?levene.test says). If you have more than one 
> factor, then 
> > you can use interaction() to create from them a factor with levels 
> > given by the product set of the levels of the individual factors. 
> > Here's an example
> >
> > > library(car)
> > > data(Moore)
> > > attach(Moore)
> > > levene.test(conformity, interaction(fcategory, partner.status))
> > Levene's Test for Homogeneity of Variance
> >       Df F value Pr(>F)
> > group  5  1.4694 0.2219
> >       39
> > > levels(interaction(fcategory, partner.status))
> > [1] "high.high"   "low.high"    "medium.high" "high.low"    
> "low.low"
> > [6] "medium.low"
> > > levels(fcategory)
> > [1] "high"   "low"    "medium"
> > > levels(partner.status)
> > [1] "high" "low"
> >
> > I'll add a couple of examples to the help page.
> 
> Thanks, John. Now, I understand how to use levene.test. There 
> is only a question remaining: is the null hypothesis 
> corresponding to homogeneity of variances, i.e., should one 
> conclude that
> 
> Levene's Test for Homogeneity of Variance
>        Df F value    Pr(>F)
> group  95  3.5919 < 2.2e-16 ***
>       864
> 
> tell us that the hypothesis that the variances are equal is 
> (highly) significant?
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phhs80 at gmail.com  Thu Aug  3 02:17:26 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 3 Aug 2006 01:17:26 +0100
Subject: [R] Syntax of Levene's test
In-Reply-To: <20060803000057.UKNI13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <6ade6f6c0608021501m3ad73f54yf505dba93f75575e@mail.gmail.com>
	<20060803000057.UKNI13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <6ade6f6c0608021717o1f1bcbecn9365c3ec33a9b320@mail.gmail.com>

On 8/3/06, John Fox <jfox at mcmaster.ca> wrote:
> Levene's test tests the null hypothesis that the variance are equal, so a
> small p-value suggests that they are not. Looking at your output, it seems
> odd that you have as many as 96 groups.

Thanks again, John. I have 4 factors with 3, 4, 4 and 2 levels
(resulting in 96 groups), respectively. For each combination of the 4
factors, I have 10 observations.

Paul


From michael_bibo at health.qld.gov.au  Thu Aug  3 03:05:40 2006
From: michael_bibo at health.qld.gov.au (Michael Bibo)
Date: Thu, 3 Aug 2006 01:05:40 +0000 (UTC)
Subject: [R] read.spss  'error reading system-file header'
References: <44CE57B3.9090901@fs-analyse.dk>
Message-ID: <loom.20060803T030216-196@post.gmane.org>

Finn Sand? <fs <at> fs-analyse.dk> writes:

> 
> When I try to import an spss sav file with read.spss() I am getting the 
> following error
> 'Error in read.spss("X:\\xxxx.sav") : error reading system-file header' 
> and the import process is aborted.
> I have tried in v. 2.3.0 and 2.3.1
> The sav-file loads without problems in spss v14 I have tried saving in 
> older spss v7 but are getting the same result.
> The read.spss() has other errors (the 'Unrecognized record type 7, 
> subtype 7 encountered in system file') but it does not seem to have any 
> impact.


These are also the error messages you get when the .sav file in question was 
created with the SPSS Data Entry product.  If that is the case, then it is 
covered by section 3.1 of the R Data Import/Export document.

Michael


From maj at waikato.ac.nz  Thu Aug  3 06:30:33 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Thu, 03 Aug 2006 16:30:33 +1200
Subject: [R] Fitting models in a loop
In-Reply-To: <971536df0608011937y6de3878ak4c68cbd6d5f9fe73@mail.gmail.com>
References: <B998A44C8986644EA8029CFE6396A9245475CD@exqld2-bne.qld.csiro.au>	
	<971536df0608011900k36c60f1ax8522fcb675eaca2f@mail.gmail.com>
	<971536df0608011937y6de3878ak4c68cbd6d5f9fe73@mail.gmail.com>
Message-ID: <44D17BE9.4090200@waikato.ac.nz>

Thanks to all who helped me with this problem, especially Bill Venables 
and Gabor Grothendieck. I hope one day to learn more about the advanced 
features of the language used by Bill.

 From a practical standpoint I think I will just avoid doing things like 
this in my teaching. It is hard enough just getting across the 
elementary ideas.

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From calstats05 at yahoo.com  Thu Aug  3 06:43:23 2006
From: calstats05 at yahoo.com (Cal Stats)
Date: Wed, 2 Aug 2006 21:43:23 -0700 (PDT)
Subject: [R] Error in step()
Message-ID: <20060803044323.75086.qmail@web34012.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/8ee4a8d6/attachment.pl 

From spencer.graves at pdf.com  Thu Aug  3 08:19:10 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Aug 2006 23:19:10 -0700
Subject: [R] (robust) mixed-effects model with covariate
In-Reply-To: <44C7B31E.5080503@emory.edu>
References: <44C50E9A.30905@emory.edu>
	<200607250937.03578.thilo@izkf.rwth-aachen.de>
	<44C7B31E.5080503@emory.edu>
Message-ID: <44D1955E.3060707@pdf.com>

	  What do you get from the following:

	  max(with(df1, table(Subj, Time)))?

	  With cases like this, "lme" gives an answer with a bogus distinction 
between variance components for "time" and "residuals".  I don't know 
about "aov" or "JMP", but I know that "varcomp" in S-Plus also produces 
garbage answers in such cases as well.

	  With mixed models, there seem to be many ways to specify models with 
random effects that are not estimable.  Some some standard software 
(like "varcomp" in S-Plus or "lme") does not (adequately?) test for 
these situations, with the result that the algorithm sometimes returns 
answers that are not correct, at least with the distinction between 
"time" and "residual".

	  I don't know if this applies to your example since it is not self 
contained.

	  Your code raises other question.  For example, what is the class of 
"Time" in "df1"?  Might it be treated differently between "aov" and 
"lme"?  How many levels does "Group" have, etc.?

	  Hope this helps.
	  Spencer Graves
p.s.  If you'd like more help from this listserve, please submit another 
post that includes commented, minimal, self-contained, reproducible 
code, as suggested in  the posting guide 
"www.R-project.org/posting-guide.html".

Giuseppe Pagnoni wrote:
> Dear Thilo,
> 
> many thanks for your reply.  I realized that there was an error in my 
> formula which should have been:
> 
> aov(y ~ Group * (Time + Age) + Error (Subj/Time), data=df1)
> 
> or alternatively:
> 
> lme(RVP.A ~ Group*(Time+Age), random = ~ 1|Subj/Time,data=df1)
> 
> but I get different results in each case, and different still from the 
> results of another stat program (JMP).
> The problem is that I am not sure which one (if one indeed is) correct!
> 
> Also, in the model you proposed:
> 
> lme(y~Group*Time, random ~ age | Subj, data = df1)
> 
> it appears that age is not between the effects of interests, so I do not 
> get an estimate of the significance of the Age or the Age*Group effect.
> 
> I have Pinheiro & Bates, and I read the first chapter but it didn't seem 
> to provide an example analogous to my case.  Also, it looks like it 
> would take me some months to study the book thoroughly and frankly that 
> seems a bit excessive for such a (apparently?) simple problem....  I was 
> hoping somebody would magically provide the correct syntax :-)  !
> 
> thanks again anyway for your help
> 
> best regards
> 
>    giuseppe
> 
> 
> 
> Thilo Kellermann wrote:
>> On Monday 24 July 2006 20:16, Giuseppe Pagnoni wrote:
>>   
>>> Dear all,
>>>
>>> First of all I apologize if you received this twice: I was checking the
>>> archive and I noticed that the text was scrubbed from the message,
>>> probably due to some setting in my e-mail program.
>>>
>>>
>>> I am unsure about how to specify a model in R and I thought of asking
>>> some advice to the list. I have two groups ("Group"= A, B) of subjects,
>>> with each subject undertaking a test before and after a certain
>>> treatment ("Time"= pre, post). Additionally, I want to enter
>>> the age of the subject as a covariate (the performance on the test is
>>> affected by age), and I also want to allow different slopes for the
>>> effect of age in the two groups of subjects (age might affect the
>>> performance of the two groups differentially).
>>>
>>> Is the right model to use something like the following?
>>>
>>> aov (y ~ Group*Time + Group*Age + Error(Subj/Group), data=df1 )
>>>
>>> (If I enter that command, within summary, I get the following:
>>> Error() model is singular in: aov(y ~ Group * Time + Group * Age +
>>> Error(Subj/Group), data = df1))
>>>
>>>     
>> try:
>> aov(y~Group*Time*Age + Error(Subj*Time*Age), data = df1)
>> which specifies an ANOVA (but not with mixed effects) with three main effects 
>> and all interaction terms plus an error term that is independent between 
>> groups (!) and relates to within subjects variability.
>>
>> For a "real" mixed effects analysis you should use the (n)lme function from 
>> the nlme package and one possible model could look like this:
>>
>> lme(y~Group*Time, random ~ age | Subj, data = df1)
>>
>> but the exact specification depends on your assumptions, in which it is 
>> possible to specify two or three models and compare their fits with anova(). 
>> For more information on mixed effects you should consult:
>> Jose C. Pinheiro & Douglas M. Bates (2000) Mixed-Effects Models in S and 
>> S-PLUS. Springer, New York.
>>
>> Good luck,
>> Thilo
>>
>>   
>>> As a second question: I have an outlier in one of the two groups. The
>>> outlier is not due to a measurement error but simply to the performance
>>> of the subject (possibly related to his medical history, but I have no
>>> way to determine that with certainty). This subject is
>>> signaled to be an outlier within its group: averaging the pre and post
>>> values for the performance of the subjects in his group, the Grubbs test
>>> yields a probability of 0.002 for the subject to be an outlier (the
>>> subject is marked as a significant outlier also if I
>>> perform the test separately on the pre and the post data).
>>>
>>> If I remove this subject from its group, I get significant effects of
>>> Group and Group X Age (not using the R formula above, but another stat
>>> software), but if I leave the subject in those effects disappear. Since
>>> I understand that removing outliers is always worrysome, I would like to
>>> know if it is possible in R to estimate a model similar to that outlined
>>> above but in a resistant/robust fashion, and what would be the actual
>>> syntax to do that. I will very much appreciate any help or suggestion
>>> about this.
>>>
>>> thanks in advance and best regards
>>>
>>> giuseppe
>>>     
>>   
> 
>


From niederlein-rstat at yahoo.de  Thu Aug  3 09:36:58 2006
From: niederlein-rstat at yahoo.de (Antje)
Date: Thu, 03 Aug 2006 09:36:58 +0200
Subject: [R] run self written functions
Message-ID: <44D1A79A.90602@yahoo.de>

Hello,

I'm not sure if I'm in the right place with my question...
I'm running R on Windows and wrote a function and saved it as .R file.
It looks like this:

bmi <- function(weight, height) {
    bmi <- weight / height^2
    bmi
}

If I want to use this function, I have to mark everything and then press 
Ctrl-R. But then everything single line is executed on the command line, 
which means that I will "loose" my history when the code becomes longer.
Further, I wonder if there is any way to do some output for control 
within the function (or any other possibilities to debug in a way).

Maybe, I have chosen a completely wrong way? I only want to make it easy 
to create some graphical visualizations of data which will be read in by 
csv. files, has to be converted and then displayed depending on some 
"displaying parameters".

Ciao,
Antje


From Antonio_Paredes at aphis.usda.gov  Wed Aug  2 23:21:01 2006
From: Antonio_Paredes at aphis.usda.gov (Antonio_Paredes at aphis.usda.gov)
Date: Wed, 2 Aug 2006 16:21:01 -0500
Subject: [R] From 2.2.1 to 2.3
Message-ID: <OF420B0470.3A625A21-ON862571BE.0075113E-862571BE.0074C6B3@aphis.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060802/12a3d1d7/attachment.pl 

From adrian at maths.uwa.edu.au  Thu Aug  3 09:37:21 2006
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 3 Aug 2006 15:37:21 +0800
Subject: [R] [R-pkgs] spatstat 1.9-4
Message-ID: <17617.42929.317124.668398@maths.uwa.edu.au>


Version 1.9-4 of package 'spatstat' has been sent to CRAN.

It includes new code for perfect simulation of point processes
and various improvements.

The release notes are available at
    <http://www.spatstat.org/spatstat/current/spatstatRELEASE-NOTES-1.9-4>

Adrian Baddeley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From dargosch at gmail.com  Thu Aug  3 10:25:20 2006
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Thu, 3 Aug 2006 10:25:20 +0200
Subject: [R] Math elements in panel headers of lattice plots?
Message-ID: <376e97ec0608030125p6a53c159x3b9d70826e24bc77@mail.gmail.com>

Hi,

I would like to put a math expression in the header of a panel.
Acctually, I need to substitute the 'a' to a script-a in a
transcription contained in the factor by which the data set is
divided. So, "/spak/" should be "/spAk/" where A should be a script-a.
I guessed that math expressions would be the way to go on this..


Is this possible?

/Fredrik

-- 

"Give up learning, and put an end to your troubles."


From ripley at stats.ox.ac.uk  Thu Aug  3 10:36:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Aug 2006 09:36:48 +0100 (BST)
Subject: [R] From 2.2.1 to 2.3
In-Reply-To: <OF420B0470.3A625A21-ON862571BE.0075113E-862571BE.0074C6B3@aphis.usda.gov>
References: <OF420B0470.3A625A21-ON862571BE.0075113E-862571BE.0074C6B3@aphis.usda.gov>
Message-ID: <Pine.LNX.4.64.0608030935370.5985@gannet.stats.ox.ac.uk>

On Wed, 2 Aug 2006, Antonio_Paredes at aphis.usda.gov wrote:

> Hello everyone.
> 
> Currently I am running R 2.2.1 (windows), and I will like to update to 
> 2.3. I wanted to ask if it is possible to update without having to removed 
> 2.2.1; or do I first need to delete 2.2.1?

There is no such version as `2.3', as the posting guide points out.

Updating is discussed in detail the rw-FAQ, to which the posting guide 
referred you.

> 
> Thank you very much.
> 
> Tony
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From uhkeller at web.de  Thu Aug  3 10:37:21 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Thu, 03 Aug 2006 10:37:21 +0200
Subject: [R] From 2.2.1 to 2.3
In-Reply-To: <OF420B0470.3A625A21-ON862571BE.0075113E-862571BE.0074C6B3@aphis.usda.gov>
References: <OF420B0470.3A625A21-ON862571BE.0075113E-862571BE.0074C6B3@aphis.usda.gov>
Message-ID: <44D1B5C1.6010107@web.de>

http://cran.r-project.org/bin/windows/base/rw-FAQ.html#What_0027s-the-best-way-to-upgrade_003f

Antonio_Paredes at aphis.usda.gov wrote:
> Hello everyone.
>
> Currently I am running R 2.2.1 (windows), and I will like to update to 
> 2.3. I wanted to ask if it is possible to update without having to removed 
> 2.2.1; or do I first need to delete 2.2.1?
>
> Thank you very much.
>
> Tony
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From r at fam-kuster.ch  Thu Aug  3 10:38:13 2006
From: r at fam-kuster.ch (Thomas Kuster)
Date: Thu, 3 Aug 2006 10:38:13 +0200
Subject: [R] read.spss and umlaut
In-Reply-To: <Pine.LNX.4.64.0608020752400.2107@homer23.u.washington.edu>
References: <200608021531.56340.r@fam-kuster.ch>
	<Pine.LNX.4.64.0608020752400.2107@homer23.u.washington.edu>
Message-ID: <200608031038.14141.r@fam-kuster.ch>

Hello

Am Mittwoch, 2. August 2006 17.11 schrieb Thomas Lumley:
> This sounds like a conflict between encodings -- eg if R is assuming UTF-8
> and the file is encoding in Latin-1 then the sequence
> U+00FC : LATIN SMALL LETTER U WITH DIAERESIS
> U+0072 : LATIN SMALL LETTER R
> is coded as FC72 in the file, which is an illegal byte sequence in UTF-8.

Hex:  74 65 20 66 fc 72 20 61 6c 6c 65 53 45 2f 31 36
Text:  t  e     f  ?  r     a  l  l  e  S  E  /  1  6

> The underlying C code (being written in the US quite a long time ago)
> doesn't know about encodings, and I don't know what the rules are in SPSS
> for valid characters (I suspect that in these old portable file formats it
> probably just reads and writes bytes, leaving it up to the OS to interpret
> them.

But why stopp the C code reading? Is "/" not the endmark of the string? What 
is the problem, if I chance that in the source?

> You could try running R in a non-UTF-8 locale to see if it helps.

I think my local is non-UTF-8 (de_CH, isolatin). How can I check that, and set 
an other temporary?

A dirty hack like this:
sed s/?/ae/g | sed s/?/oe/g | sed s/?/ue/g | sed s/?/Ae/g | sed s/?/Oe/g | sed 
s/?/Ue/g
didn't work (file 'projets_non_umlaut.por' is not in any supported SPSS 
format).

Thomas

> If anyone has definitive information about how SPSS represents strings and
> decides on valid characters that might be useful too.
>
>  	-thomas
>
> >> library("foreign")
> >> spssdaten <- read.spss("projets.por")
> >> attr(spssdaten$PROJETX, "value.labels")[1:20]
> >
> >              Bg Stammzellenforschung                                  Bb
> >                                  863                                  
> > 862 Bb Neugestaltung des Finanzausgleichs
> >                                  861                                  
> > 854 EV Postdienste f                                   Bb 853            
> >                       852 Bb                         Bg Steuerpaket 851  
> >                                 843 Bb Anhebung der Mehrwertsteuer s     
> >                 11. AHV-Revision 842                                  
> > 841 Volkinitiative Lebenslange Verwahrung
> >                                  833                                  
> > 832 Gegenentwurf zur Avanti             EV Lehrstellen-Initiative 831    
> >                               824 EV Moratorium Plus                   
> > EV Strom ohne Atom 823                                   822 EV Ja zu
> > fairen Mieten                   EV Gleiche Rechte f 821                  
> >                 815 EV Gesundheitsinitiative                EV
> > Sonntags-Initiative 814                                   813
> >
> > The SPSS-File is okay:
> >> system("cat projets.por |grep Postdienste")
> >
> > echtserwerb 3. GenerationSD/N/EV Postdienste f?r alleSE/16/?nderrung Bg 
> > EOG Mut
> >
> > How can I read the SPSS-File with the Umlaut?
> >
> > Bye
> > Thomas Kuster
> >
> > R: 2.1.0 (2005-04-18)
> > OS: Debian Linux, 2.6.10-isgee-neptun-1
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle


From tuechler at gmx.at  Thu Aug  3 10:39:35 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 03 Aug 2006 09:39:35 +0100
Subject: [R] Summary method needed?
In-Reply-To: <Pine.LNX.4.64.0608022103430.31307@gannet.stats.ox.ac.uk>
References: <Pine.LNX.4.64.0608021953560.2736@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608021914430.27720@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0608021953560.2736@egon.stats.ucl.ac.uk>
Message-ID: <3.0.6.32.20060803093935.00ae6e48@pop.gmx.net>

At 21:04 02.08.2006 +0100, Prof Brian Ripley wrote:
>On Wed, 2 Aug 2006, Christian Hennig wrote:
>
>> Thank you Brian!
>> 
>> > > I'm updating my fpc package at the moment and will add some new
functions.
>> > > I learned that there should be print and summary methods for the key
>> > > functions.
>> >
>> > for 'classes', I think.
>> 
>> Yes.
>> 
>> > > But in some cases the print method will make use of more or less all
the
>> > > output information of the function. Is there any reason to implement a
>> > > summary method in these cases?
>> >
>> > Would a more concise print() method be useful?  If so the existing
print()
>> > could become summary().
>> 
>> :-)
>> What I initially did some years ago was to write summary methods to
print out
>> the required informations. Then M. Maechler told me that this is not the
>> purpose of a summary method and I should write a print.summary method for
>> this. Now I realise that I actually just want to print, and I don't really
>> need the extra "synopsis" to be done by summary().
>> 
>> Now is there any recommendation on this? My intuition would be to write a
>> print, but not a summary method.
>
>That sounds fine for your purposes.
>

Maybe I am wrong, but as far as I see, print() has the disadvantage that it
has to return x and must not return the summarized results as an object.
You remember the difficulties with print.survfit.
Instead it seems to be allowed that summary does not only print but also
return summarized results.
Is that right?

Greetings,
Heinz

>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>


From thomas at fam-kuster.ch  Thu Aug  3 10:46:30 2006
From: thomas at fam-kuster.ch (Thomas Kuster)
Date: Thu, 3 Aug 2006 10:46:30 +0200
Subject: [R] run self written functions
In-Reply-To: <44D1A79A.90602@yahoo.de>
References: <44D1A79A.90602@yahoo.de>
Message-ID: <200608031046.30399.thomas@fam-kuster.ch>

Hello

Am Donnerstag, 3. August 2006 09.36 schrieb Antje:
> Hello,
>
> I'm not sure if I'm in the right place with my question...
> I'm running R on Windows and wrote a function and saved it as .R file.
> It looks like this:
>
> bmi <- function(weight, height) {
>     bmi <- weight / height^2
>     bmi
> }
>
> If I want to use this function, I have to mark everything and then press
> Ctrl-R. But then everything single line is executed on the command line,
> which means that I will "loose" my history when the code becomes longer.
> Further, I wonder if there is any way to do some output for control
> within the function (or any other possibilities to debug in a way).

source("your-r-file.R")
mybmi <- bmi(180, 70)

> Maybe, I have chosen a completely wrong way? I only want to make it easy
> to create some graphical visualizations of data which will be read in by
> csv. files, has to be converted and then displayed depending on some
> "displaying parameters".

look here (directory R):
http://tomix.homelinux.org/~thomas/eth/5_semester/semesterarbeit_WS_2005_2006/
auswertung.R is the main file.


> Ciao,
> Antje
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From stefano.iacus at unimi.it  Thu Aug  3 11:14:43 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Thu, 3 Aug 2006 11:14:43 +0200
Subject: [R] geodesic distance
Message-ID: <69A62C51-EEEB-494A-A9C8-289276EC82E1@unimi.it>

Hi,
has anyone ever seen implemented in R the following "geodesic"  
distance between positive definite pxp matrices A and B?

d(A,B) = \sum_{i=1}^p (\log \lambda_i)^2

were \lambda is the solution of det(A -\lambda B)  = 0

thanks
stefano


From maechler at stat.math.ethz.ch  Thu Aug  3 11:25:46 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 3 Aug 2006 11:25:46 +0200
Subject: [R] Summary method needed?
In-Reply-To: <3.0.6.32.20060803093935.00ae6e48@pop.gmx.net>
References: <Pine.LNX.4.64.0608021953560.2736@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608021746440.2736@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608021914430.27720@gannet.stats.ox.ac.uk>
	<3.0.6.32.20060803093935.00ae6e48@pop.gmx.net>
Message-ID: <17617.49434.928935.207407@stat.math.ethz.ch>

>>>>> "HeinzT" == Heinz Tuechler <tuechler at gmx.at>
>>>>>     on Thu, 03 Aug 2006 09:39:35 +0100 writes:

    HeinzT> At 21:04 02.08.2006 +0100, Prof Brian Ripley wrote:
    >> On Wed, 2 Aug 2006, Christian Hennig wrote:
    >> 
    >>> Thank you Brian!
    >>> 
    >>> > > I'm updating my fpc package at the moment and will add some new functions.
    >>> > > I learned that there should be print and summary methods for the key
    >>> > > functions.
    >>> >
    >>> > for 'classes', I think.
    >>> 
    >>> Yes.
    >>> 
    >>> > > But in some cases the print method will make use of
    >>> > > more or less all the output information of the
    >>> > > function. Is there any reason to implement a summary
    >>> > > method in these cases?
    >>> >
    >>> > Would a more concise print() method be useful?  If so
    >>> > the existing print() could become summary().
    >>> 
    >>> :-)

    >>> What I initially did some years ago was to write summary
    >>> methods to print out the required informations. Then
    >>> M. Maechler told me that this is not the purpose of a
    >>> summary method and I should write a print.summary method
    >>> for this. Now I realise that I actually just want to
    >>> print, and I don't really need the extra "synopsis" to
    >>> be done by summary().
    >>> 
    >>> Now is there any recommendation on this? My intuition
    >>> would be to write a print, but not a summary method.
    >> 
    >> That sounds fine for your purposes.
    >> 

    HeinzT> Maybe I am wrong, but as far as I see, print() has
    HeinzT> the disadvantage that it has to return x and must
    HeinzT> not return the summarized results as an object.  You
    HeinzT> remember the difficulties with print.survfit.

    HeinzT> Instead it seems to be allowed that summary does not
    HeinzT> only print but also return summarized results.

    HeinzT> Is that right?

yes, that's right,  But let me tell more on the story:

I think I had recommended  summary.FOO() and print.summary.FOO()
to Christian because he *did* compute a few `useful' things on
his "FOO" object. In such a situation, we (R-core) recommend and
usually (very rarely not; for back-compatibility reasons only) implement
the following:

print.FOO: gives a one (sometimes two) paragraph overview of
	   the fitted model object, and
	   --yes-- *should* return its *unchanged* argument invisibly.

summary.FOO: computes more interesting things from the original
	     FOO object, does *NOT print* anything (explicitly)
	     and returns an object of class "summary.FOO".

print.summary.FOO: now prints (an overview of) the summary.FOO
	     object and -- since it's a print() method -- also
	     returns its argument unchanged and invisibly.

  Examples of the above, can be inspected e.g., for 
  'FOO' in  { lm, glm, aov, nls, loess, princomp, prcomp, ...... }

For Joe Average User, of course it *looks* like 
summary( <FOO> ) would print , but that's just because it
returns a summary.FOO object which is auto-printed subsequently
-- unless it's assigned or used in another expression.

My reasons for *not* providing a  summary.* and print.summary.*
method would have to be either *both* of 1) and 2)  *or* 3) :

1) print(obj) gives enough information 

2) the 'obj' already contains the interesting quantities, or
   these are either trivially computable from the contents of 'obj'
   or are already provided by other FOO methods, e.g., vcov.FOO().

3) lack of time and/or motivation; lazyness


Martin Maechler, ETH Zurich


From gregd at stats.uct.ac.za  Thu Aug  3 11:58:58 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Thu, 3 Aug 2006 11:58:58 +0200
Subject: [R] NLME: Problem with plotting ranef vs a factor
Message-ID: <009501c6b6e3$70d6baf0$6f179e89@UCTPCGREGD>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/600bd82d/attachment.pl 

From bennfine at yahoo.com  Thu Aug  3 12:02:08 2006
From: bennfine at yahoo.com (Benn Fine)
Date: Thu, 3 Aug 2006 03:02:08 -0700 (PDT)
Subject: [R] question about dll crashing R
Message-ID: <20060803100208.78789.qmail@web61312.mail.yahoo.com>

I have ported some R code to C to make it faster.

I can perform .Call("foobar",....) once and it
works fine. Absolutely correct answer.

 If I put a loop inside foobar and run the  main code
routine more than 100 times, it crashes R.

Or if I call .Call("foobar"....) seperately more than
two tims it crashes R.


For the most part I am doing matirx multiplies using

EXP matrixprod (SEXP x , SEXP y )
{ int nrx , ncx , nry , ncy , mode;
SEXP xdims , ydims , ans ;
char *transa = "N" , *transb = "N" ;
double one = 1.0 , zero = 0.0 ;
xdims = getAttrib (x , R_DimSymbol ) ;
ydims = getAttrib (y , R_DimSymbol ) ;
mode = REALSXP;
nrx = INTEGER( xdims ) [ 0 ] ;
ncx = INTEGER( xdims ) [ 1 ] ;
nry = INTEGER( ydims ) [ 0 ] ;
ncy = INTEGER( ydims ) [ 1 ] ;
PROTECT( ans = allocMatrix (mode, nrx , ncy ) ) ;
F77_CALL(dgemm) ( transa , transb , &nrx , &ncy , &ncx
, &one ,
REAL( x ) , &nrx , REAL( y ) , &nry , &zero ,
REAL( ans ) , &nrx ) ;
UNPROTECT( 1 ) ;
return( ans ) ;
}

I am also generating random multiavriate normals using
the (not pretty) code

*
 1) Generate P independent standard normal deviates -
Ei ~ N(0,1)
     2) Using Cholesky decomposition find A s.t.
trans(A)*A = COVM
     3) trans(A)E + MEANV ~ N(MEANV,COVM)
*/

SEXP mvntest (SEXP mean, SEXP cov, SEXP temp)


{ int nrx , ncx , nry , ncy ,info,mode;
SEXP xdims , ydims , ans;

int i,j, one=1;
info = 1;

xdims = getAttrib (mean , R_DimSymbol ) ;
ydims = getAttrib (cov , R_DimSymbol ) ;
mode = REALSXP;
nrx = INTEGER( xdims ) [ 0 ] ;
ncx = INTEGER( xdims ) [ 1 ] ;
nry = INTEGER( ydims ) [ 0 ] ;
ncy = INTEGER( ydims ) [ 1 ] ;



/* create the upper trianglular matrix A */

/* such that t(A) %*% A = Sigma */

GetRNGstate();	

F77_CALL(dpofa) ( REAL( cov ), &nry , &ncy , &info);
Rprintf("Info = %d\n",info);


for(i=0;i<nry;i++)
  for(j=0;j<i;j++)
    REAL(cov)[i+j*ncy] = 0.0;


PROTECT( ans = allocMatrix (mode, nrx , one ) ) ;
for(i=0;i<nry;i++)
  REAL(temp)[i] = rnorm(0,1);
ans = tmatrixprod(cov,temp);
for(i=0;i<nry;i++)
  REAL(ans)[i] = REAL(ans)[i]+REAL(mean)[i];
UNPROTECT( 1 ) ;
PutRNGstate();
return( ans ) ;


}


I have a feeling I am messing up memory usage
somewhere but haven't a clue. Do I need to do garbage
collecting inside the C program ?The fact that the
code
runs a few times before R crashes is driving me nuts. 
I send most of what I need into the C routine from R,
so I am not creating that many SEXP objects within the
program.

Any hints or ideas ?

Thanks!

Benn


From rjvbertin at gmail.com  Thu Aug  3 12:34:07 2006
From: rjvbertin at gmail.com (=?ISO-8859-1?Q?Ren=E9_J.V._Bertin?=)
Date: Thu, 3 Aug 2006 12:34:07 +0200
Subject: [R] best way to calculate per-parameter differences in
	across-subject means
In-Reply-To: <797664590608020741m241c9782v34386e01811ea58@mail.gmail.com>
References: <797664590608020741m241c9782v34386e01811ea58@mail.gmail.com>
Message-ID: <797664590608030334t37d611a6v975c1ba58d8ba66e@mail.gmail.com>

Thanks, I'll look at that.

In the meantime, the code below is what I came up with myself. It does
what I want

# SelectCases(dat,crit) == subset(dat, crit, drop=FALSE)

SENSICK.AvScores<- function( dat=SENSICK.items.tr )
{
	n<-nlevels(dat$Symptom)
	data.frame(
		Patient=c( rep(1,n), rep(0, 3*n)),
		WasSick=c( rep(1,2*n), rep(NA,2*n)),
		StrictSick=c( rep(NA,2*n), rep(-1,n), rep(1,n)),
		Symptom=rep(levels(dat$Symptom),4),
		AvScore=c(
			with( SelectCases(dat, 'Patient==1 & WasSick==1'), tapply(Score,
Symptom , mean) ),
			with( SelectCases(dat, 'Patient==0 & WasSick==1'), tapply(Score,
Symptom , mean) ),
			with( SelectCases(dat, 'Patient==0 & StrictSick==-1'),
tapply(Score, Symptom , mean) ),
			with( SelectCases(dat, 'Patient==0 & StrictSick==1'), tapply(Score,
Symptom , mean) )
		)
	)
}

AvScores<-SENSICK.AvScores

with( AvScores, (barchart( AvScore[Patient==1] - AvScore[Patient==0 &
WasSick==1]) ~ Symptom, scales=list( rot=c(45,0)) )

------ Dieter wrote:
Maybe it's a bit more than you want, but possibly you are happy with it: see the
example under TukeyHSD.

summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
TukeyHSD(fm1, "tension", ordered = TRUE)
plot(TukeyHSD(fm1, "tension"))


Dieter


From spencer.graves at pdf.com  Thu Aug  3 13:07:10 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Aug 2006 04:07:10 -0700
Subject: [R] mult comp significance
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778570601@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778570601@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <44D1D8DE.6050108@pdf.com>

	  I read two questions:  physical meaning and how to display only 
significant differences.


PHYSICAL MEANING

	  A p value is the probability of obtaining by chance alone a result at 
least as extreme as what we observe.  A "simultaneous" or "adjusted" p 
value is the probability that at least one of the multiple tests was at 
least as extreme as what observed.

	  A (1-alpha) confidence interval or region is a random set that 
includes the true but unknown value with probability at least (1-alpha). 
  A set of (1-alpha) simultaneous confidence intervals is a random set 
in the multidimensional space that includes the true but unknown values 
of all parameter comparisons with probability at least (1-alpha).


SELECTING ONLY SIGNIFICANT DIFFERENCES

	  Consider the following modification of the first example in the 
"simint" help page:

library(multcomp)
data(recovery)
RecInts <- simint(minutes~blanket, data=recovery, type="Dunnett",
                     conf.level=0.9, alternative="less",eps=0.0001)

	  To see the structure of "RecInts", look at "str(RecInts)".  This 
indicates that "RecInts" is a list, and one of its components is a named 
vector called "p.value.adj".  This suggests we try the following:

 > with(RecInts, p.value.adj[p.value.adj<0.05])
blanketb2-blanketb0
        5.484862e-05

	  Hope this helps.
	  Spencer Graves

Nair, Murlidharan T wrote:
> This has a stats question and a R question.  I am sure there are many
> core statisticians here how would know the answer to this simple
> question. In determining the significant comparisons using the methods
> in multcomp, the ones that are designated as significant are the ones
> that do not intersect the zero line. What is the physical meaning of
> this and why are those considered significant? I can sort those out and
> pick out by their adjusted pvalues to pick the top ones correct? Is
> there an method the multcomp that will output only the significant ones?
> 
> Thanks ../Murli
> 
>    
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tuechler at gmx.at  Thu Aug  3 13:08:57 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 03 Aug 2006 12:08:57 +0100
Subject: [R] Default first argument in assignment function possible?
Message-ID: <3.0.6.32.20060803120857.00ad8fa8@pop.gmx.net>

Dear All,

is there a possibility to provide a default for the first argument in an
assignment function?
I could not find out if and how. You see in the example below that not
explicitly stating the first argument leads to errors.

Thanks,
Heinz T?chler

### example of assignment function
'testfun<-' <- function(x=a, y=b, value)
  { x <- x+ y+ value
    print(x)
    x }

a <- 0; b <- 1; c <- 2
testfun(c, 2) <- 2  # result: 6
c <- 2
testfun(c   ) <- 2  # result: 5
testfun( , 2) <- 2  # Error: argument is missing, with no default
testfun(y=2) <- 2   # Error: target of assignment expands to non-language
object
testfun() <- 2      # Error: invalid (NULL) left side of assignment

               _                                        
platform       i386-pc-mingw32                          
arch           i386                                     
os             mingw32                                  
system         i386, mingw32                            
status         Patched                                  
major          2                                        
minor          3.1                                      
year           2006                                     
month          07                                       
day            23                                       
svn rev        38687                                    
language       R                                        
version.string Version 2.3.1 Patched (2006-07-23 r38687)


From adi at roda.ro  Thu Aug  3 13:21:35 2006
From: adi at roda.ro (Adrian DUSA)
Date: Thu, 3 Aug 2006 14:21:35 +0300
Subject: [R] Tcltk package
In-Reply-To: <20060801162417.29995.qmail@web35412.mail.mud.yahoo.com>
References: <20060801162417.29995.qmail@web35412.mail.mud.yahoo.com>
Message-ID: <200608031421.35804.adi@roda.ro>

On Tuesday 01 August 2006 19:24, John McHenry wrote:
> [...]
> Yes, I built R myself. I couldn't find a debian package for R 2.3.1. The
> latest available is 2.2.1.

Oh, but there is... right on CRAN. For Dapper just add this line to your 
sources.list:

deb http://cran.R-project.org/bin/linux/ubuntu/ dapper/

This repository has lots of other packages compiled for Ubuntu, feel free to 
take a look.

HTH,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From dirk.enzmann at uni-hamburg.de  Thu Aug  3 13:24:52 2006
From: dirk.enzmann at uni-hamburg.de (Dirk Enzmann)
Date: Thu, 03 Aug 2006 13:24:52 +0200
Subject: [R] NLME: Problem with plotting ranef vs a factor
Message-ID: <44D1DD04.7050909@uni-hamburg.de>

Greg,

be careful using attach() and detach(). From the syntax snippets you 
showed it seems that you did create an object "pcat" ("factor 
variable"), but you did not change the respective "variable" in your 
data frame.

Try to remove "pcat" and see what happens do the results of lme()!

Dirk


From spencer.graves at pdf.com  Thu Aug  3 13:39:41 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Aug 2006 04:39:41 -0700
Subject: [R] random effects with  lmer() and lme(), three random factors
In-Reply-To: <425B10826271C048A4F65FC6C810F5D60142B5@mail.arcturusag.local>
References: <425B10826271C048A4F65FC6C810F5D60142B5@mail.arcturusag.local>
Message-ID: <44D1E07D.9060103@pdf.com>

	  In theory, 'lme' may be able to handle crossed random effects, but I 
don't know how to do it.  I've used 'lmer' for such models, and your 
'lmer' code looks plausible to me.  Therefore, I would be inclined to 
believe the 'lmer' results.

	  To remove any lingering doubt, I suggest you construct Monte Carlo 
simulations where you know the structure.  Then feed the simulations to 
'lme' and 'lmer' and see what you get.

	  Hope this helps.
	  Spencer Graves

Xianqun (Wilson) Wang wrote:
> Hi, all,
> 
>  
> 
> I have a question about random effects model. I am dealing with a
> three-factor experiment dataset. The response variable y is modeled
> against three factors: Samples, Operators, and Runs. The experimental
> design is as follow:
> 
>  
> 
> 4 samples were randomly chosen from a large pool of test samples. Each
> of the 4 samples was analyzed by 4 operators, randomly selected from a
> group of operators. Each operator independently analyzed same samples
> over 5 runs (runs nested in operator). I would like to know the
> following things:
> 
>  
> 
> (1)                     the standard deviation within each run;
> 
> (2)                     the standard deviation between runs;
> 
> (3)                     the standard deviation within operator
> 
> (4)                     the standard deviation between operator.
> 
>  
> 
> With this data, I assumed the three factors are all random effects. So
> the model I am looking for is
> 
>  
> 
> Model:  y  = Samples(random) + Operator(random) + Operator:Run(random) +
> Error(Operator) + Error(Operator:Run)  + Residuals
> 
>  
> 
> I am using lme function in nlme package. Here is the R code I have
> 
>  
> 
> 1.	 using lme:
> 
> First I created a grouped data using
> 
> gx <- groupedData(y ~ 1 | Sample, data=x)
> 
> gx$dummy <- factor(rep(1,nrow(gx)))
> 
>  
> 
> then I run the lme
> 
>  
> 
> fm<- lme(y ~ 1, data=gx,
> random=list(dummy=pdBlocked(list(pdIdent(~Sample-1),
> 
>             pdIdent(~Operator-1), 
> 
>             pdIdent(~Operator:Run-1)))))
> 
>  
> 
>     finally, I use VarCorr to extract the variance components
> 
>  
> 
>            vc <- VarCorr(fm)
> 
>  
> 
>                      Variance           StdDev  
> 
> Operator:Run 1.595713e-10:20   1.263215e-05:20  
> 
> Sample       5.035235e+00: 4   2.243933e+00: 4  
> 
> Operator     5.483145e-04: 4   2.341612e-02: 4  
> 
> Residuals    8.543601e-02: 1   2.922944e-01: 1  
> 
>  
> 
>  
> 
> 2.	Using lmer in Matrix package:
> 
>  
> 
> fm <- lmer(y ~ (1 | Sample) + (1 | Operator) + 
> 
>            (1|Operator:Run), data=x)
> 
>      summary(fm)
> 
>  
> 
> Linear mixed-effects model fit by REML 
> 
> Formula: H.I.Index ~ (1 | Sample.Name) + (1 | Operator) + (1 |
> Operator:Run) 
> 
>           Data: x 
> 
>       AIC      BIC    logLik MLdeviance REMLdeviance
> 
>  96.73522 109.0108 -44.36761   90.80064     88.73522
> 
> Random effects:
> 
>  Groups       Name        Variance   Std.Dev.  
> 
>  Operator:Run (Intercept) 4.2718e-11 6.5359e-06
> 
>  Operator     (Intercept) 5.4821e-04 2.3414e-02
> 
>  Sample       (Intercept) 5.0352e+00 2.2439e+00
> 
>  Residual                 8.5436e-02 2.9229e-01
> 
> number of obs: 159, groups: Operator:Run, 20; Operator, 4; Sample.Name,
> 4
> 
>  
> 
> Fixed effects:
> 
>              Estimate Std. Error  t value
> 
> (Intercept) 0.0020818  1.1222683 0.001855
> 
>  
> 
>  
> 
> There is a difference between lmer and lme is for the factor
> Operator:Run.  I cannot find where the problem is. Could anyone point me
> out if my model specification is correct for the problem I am dealing
> with. I am pretty new user to lme and lmer. Thanks for your help!
> 
>  
> 
>  
> 
> Wilson Wang
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From coursol at mathens.u-psud.fr  Thu Aug  3 12:06:51 2006
From: coursol at mathens.u-psud.fr (Jean Coursol)
Date: Thu, 3 Aug 2006 12:06:51 +0200 (CEST)
Subject: [R] tcl/tk bind destroy event
In-Reply-To: <44D105B9.3000306@gmx.de>
Message-ID: <Pine.LNX.4.31.0608031202540.799-100000@pc00.mathens.u-psud.fr>

Perhaps Destroy key is unknown by Tcl; it is not in the
Event modifiers table in Welch Book...
But try with <Control-L> or <Shift-Control_L>, it runs
(but not with <Control_L-Shift> ??).

Use xmodmap to see the current mappings from keys to modifiers.

Jean Coursol

On Wed, 2 Aug 2006, Franco Mendolia wrote:

> Hello!
>
> I want to create a messagebox whenever the user wants to destroy the window
> (e.g. <Alt-F4> or the 'x' in the right top corner) and ask if a modified file
> should be saved or not.
>
> If 'cancel' is chosen then nothing should happen and the windows still should be
> existing. This doesn't work. When I press cancel the window will be destroyed
> although.
>
> I also implemented a menu item 'Quit' where I show the same messagebox and there
> it works fine.
>
> How can I make it work or is there another method to do this? I'm very new to R
> and tcl/tk.
>
> Here is part of my code:
>
>
>    exitProg <- function()
>    {
>      returnVal <- tkmessageBox(title="Question",
>        message="Save modified file?",
>        icon="question", type="yesnocancel", default="yes")
>
>      returnVal <- as.character(returnVal)
>
>      if( returnVal == "yes" )
>      {
>        # save file
>        value <- saveFile()
>        # destroy window when save was successfull
>        if( value == 1 )
>          tkdestroy(mw)
>      }
>      if( returnVal == "no" )
>      {
>        tkdestroy(mw)
>      }
>      if( returnVal == "cancel" )
>      {
>        # do nothing
>        cat("Cancel was pressed.\n")
>      }
>    }
>
>    # bind the destroy event in order to show a message box
>    tkbind(mw,"<Destroy>",exitProg)
>
>    # menu item which works fine
>    tkadd(fileMenu, "command", label="Quit", command=exitProg)
>
>
>
> Thank you.
>
> Franco Mendolia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wsimpson at utsc.utoronto.ca  Thu Aug  3 14:01:18 2006
From: wsimpson at utsc.utoronto.ca (William Simpson)
Date: Thu, 3 Aug 2006 08:01:18 -0400 (EDT)
Subject: [R] between-within anova: aov and lme
Message-ID: <33271.131.136.242.1.1154606478.squirrel@webmail.utsc.utoronto.ca>

I have 2 questions on ANOVA with 1 between subjects factor and 2 within factors.

1. I am confused on how to do the analysis with aov because I have seen two examples
on the web with different solutions.

a) Jon Baron (http://www.psych.upenn.edu/~baron/rpsych/rpsych.html) does
6.8.5 Example 5: Stevens pp. 468 - 474 (one between, two within)

between: gp
within: drug, dose
aov(effect ~ gp * drug * dose + Error(subj/(dose*drug)), data=Ela.uni)

b) Bill Venables answered a question on R help as follows.

- factor A between subjects
- factors B*C within subjects.

aov(response ~ A*B*C + Error(subject), Kirk)
"An alternative formula would be response ~ A/(B*C) + Error(subject), which
would only change things by grouping together some of the sums of squares."

-------------------------------------------------------
SO: which should I do????
aov(response ~ A*B*C + Error(subject), Kirk)
aov(response ~ A/(B*C) + Error(subject), Kirk)
aov(response ~ A*B*C + Error(subject/(B*C)), Kirk)
--------------------------------------------------------

2. How would I do the analysis in lme()?
Something like
lme(response~A*B*C,random=~1|subject/(B*C))???


Thanks very much for any help!
Bill Simpson


From franco.mendolia at gmx.de  Thu Aug  3 14:04:31 2006
From: franco.mendolia at gmx.de (Franco Mendolia)
Date: Thu, 03 Aug 2006 14:04:31 +0200
Subject: [R] tcl/tk bind destroy event
In-Reply-To: <Pine.LNX.4.31.0608031202540.799-100000@pc00.mathens.u-psud.fr>
References: <Pine.LNX.4.31.0608031202540.799-100000@pc00.mathens.u-psud.fr>
Message-ID: <44D1E64F.4010301@gmx.de>

Hi!

> Perhaps Destroy key is unknown by Tcl; it is not in the
> Event modifiers table in Welch Book...

I think the Destroy key is known, because when destroying the window with Alt-F4 
or the littel x in the topcorner my function exitProg is executed.

What I actually search for is a possibility to let the window not be destroyed 
when user closes the window by Alt-F4 or the little x.

Franco Mendolia

>>    exitProg <- function()
>>    {
>>      returnVal <- tkmessageBox(title="Question",
>>        message="Save modified file?",
>>        icon="question", type="yesnocancel", default="yes")
>>
>>      returnVal <- as.character(returnVal)
>>
>>      if( returnVal == "yes" )
>>      {
>>        # save file
>>        value <- saveFile()
>>        # destroy window when save was successfull
>>        if( value == 1 )
>>          tkdestroy(mw)
>>      }
>>      if( returnVal == "no" )
>>      {
>>        tkdestroy(mw)
>>      }
>>      if( returnVal == "cancel" )
>>      {
>>        # do nothing
>>        cat("Cancel was pressed.\n")
>>      }
>>    }
>>
>>    # bind the destroy event in order to show a message box
>>    tkbind(mw,"<Destroy>",exitProg)
>>
>>    # menu item which works fine
>>    tkadd(fileMenu, "command", label="Quit", command=exitProg)


From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Aug  3 14:05:57 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 3 Aug 2006 14:05:57 +0200 (CEST)
Subject: [R] Correlation adjusted Bonferroni? (was: Multiple tests
	onrepeated measurements)
In-Reply-To: <44D0615D.9090702@pdf.com>
References: <63E04C5ADEDACB4989972239CDABF05782BEEE@chlsne01.nestle.com>
	<44D0615D.9090702@pdf.com>
Message-ID: <Pine.LNX.4.64.0608031401520.6432@imbe153.imbe.med.uni-erlangen.de>

|_______________________________________________________________________|

On Wed, 2 Aug 2006, Spencer Graves wrote:

> 	  I'm not familiar with the correlation adjustment to Bonferroni you 
> mention below, though it sounds interesting.  However, I think there is 
> something not right about it or about how you have interpreted it.  Your code 
> produced the following for me:
>
> 	 p.value.raw p.value.bon p.value.adj
>          = raw.p      = bon.p   =multcomp.p "bon.cor.p"
> diff/v=0 0.028572509 0.057145019 0.054951102 0.034934913
> diff/v=1 0.001727993 0.003455987 0.003415545 0.002119276
>
> 	  In the absence of other information, I'd be inclined to believe 
> csimint(..)$p.value.adj or ..$p.value.bon over your "bon.cor.p".
>

hm, I recall that we had some discussions if multiple comparisons
for fixed effects (as performed by Dominik) using 
the `multcomp' functionality are admissible and none of the experts was 
really sure about that -- and I was not able to find any helpful reference 
when I looked into that problem two or three years ago. And thats the 
reason why a simint method for lme objects is still missing ...

Best wishes,

Torsten

> 		  Hope this helps.
> 	  Spencer Graves
>
> Grathwohl, Dominik, LAUSANNE, NRC-BAS wrote:
>> Dear R-helpers:
>> 
>> My question is how do I efficient and valid correct for multiple tests in a 
>> repeated measurement design: Suppose we measure at two distinct visits with 
>> repeated subjects a treatment difference on the same variable. The 
>> treatment differences are assessed with a mixed model and adjusted by two 
>> methods for multiple tests:
>> 
>> # 1. Method: Adjustment with library(multcomp)
>> 
>> library(nlme)
>> library(multcomp)
>> 
>> n <- 30 # number of subjects
>> sd1 <- 0.5 # Standard deviation of the random intercept
>> sd2 <- 0.8 # Standard deviation of the residuals
>> id <- rep(1:n,times=2); v <- rep(0:1, each=n); trt <- rep(sample(rep(0:1, 
>> each=n/2), n), times=2)
>> df <- data.frame(id, v, trt, y=2 + rep(rnorm(10,0,sd1), times=2) + 0.5*v + 
>> 0.7*trt + 0.2*v*trt + rnorm(2*n, 0, sd2))
>> m1 <- lme(y ~ v + trt + v*trt, data=df, random= ~ 1|id)
>> summary(m1)
>> par4 <- m1$coef$fixed
>> cov4 <- vcov(m1)
>> cm4 <- matrix(c(0, 0, 1, 0, 0, 0, 1, 1), nrow = 2, ncol=4, byrow=TRUE, 
>> dimnames = list(c("diff/v=0", "diff/v=1"), c("C.1", "C.2", "C.3", "C.4")))
>> v4 <- csimint(estpar=par4, df=n-6, # I'm not sure whether I found      # 
>> the correct degrees of freedom
>> 	covm=cov4,
>> 	cmatrix=cm4, conf.level=0.95)
>> sv4 <- summary(v4)
>> 
>> # 2. Method: I found in Handbook of Statistics Vol 13, p.616,
>> # same can be found in http://home.clara.net/sisa/bonhlp.htm
>> # Bonferroni on correlated outcomes:
>> 
>> raw.p <- sv4$p.value.raw
>> co4 <- cor(df$y[df$v==0],df$y[df$v==1])
>> rho <- mean(c(1,co4,co4,1))
>> pai <- 1-(1-raw.p)^2^(1-rho) 
>> # The results of two methods are presented in the following lines:
>> out <- cbind(raw.p, sv4$p.value.bon, sv4$p.value.adj, pai)
>> colnames(out) <- c("raw.p", "bon.p", "multcomp.p", "bon.cor.p")
>> out
>> 
>> As you can see there are quite big differences between the two ways 
>> adjusting for multiple tests on repeated measurements. I guess that the 
>> multcomp library is not appropriate for this kind of hypotheses. However I 
>> could not find an explanation in the help files. May be one of the experts 
>> can point me in the right direction?
>> 
>> Kind regards,
>> 
>> Dominik
>> 
>> platform i386-pc-mingw32
>> arch     i386           os       mingw32        system   i386, mingw32 
>> status                  major    2              minor    2.1 
>> year     2005           month    12             day      20             svn 
>> rev  36812          language R
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From sridharshalini at hotmail.com  Thu Aug  3 14:11:30 2006
From: sridharshalini at hotmail.com (Shalini Sridhar)
Date: Thu, 03 Aug 2006 12:11:30 +0000
Subject: [R] Invoking R on UNIX Command line from a PERL CGI
In-Reply-To: <mailman.0.1151508533.21761.r-help@stat.math.ethz.ch>
Message-ID: <BAY103-F16BAB38EAE83FFD8E24E3BC0530@phx.gbl>


From p.dalgaard at biostat.ku.dk  Thu Aug  3 14:13:10 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Aug 2006 14:13:10 +0200
Subject: [R] tcl/tk bind destroy event
In-Reply-To: <Pine.LNX.4.31.0608031202540.799-100000@pc00.mathens.u-psud.fr>
References: <Pine.LNX.4.31.0608031202540.799-100000@pc00.mathens.u-psud.fr>
Message-ID: <x2u04unhk9.fsf@viggo.kubism.ku.dk>

Jean Coursol <coursol at mathens.u-psud.fr> writes:

> Perhaps Destroy key is unknown by Tcl; it is not in the
> Event modifiers table in Welch Book...
> But try with <Control-L> or <Shift-Control_L>, it runs
> (but not with <Control_L-Shift> ??).

I don't think that is it. As I read it, Tk catches the Destroy event
alright, but the problem is that it is too late: the window
destruction is already in motion at the time and there's no way to
"undestroy" it from within binding. I think you need to create a "wm
protocol" handler for WM_DELETE_WINDOW. As in
 
x <- tktoplevel()
tcl("wm", "protocol", x, "WM_DELETE_WINDOW", quote(cat("I'm staying!\n")))

--- now try to destroy the window using window controls ---

tcl("wm", "protocol", x, "WM_DELETE_WINDOW", "")

--- things should be back to normal now ---


> Use xmodmap to see the current mappings from keys to modifiers.
> 
> Jean Coursol
> 
> On Wed, 2 Aug 2006, Franco Mendolia wrote:
> 
> > Hello!
> >
> > I want to create a messagebox whenever the user wants to destroy the window
> > (e.g. <Alt-F4> or the 'x' in the right top corner) and ask if a modified file
> > should be saved or not.
> >
> > If 'cancel' is chosen then nothing should happen and the windows still should be
> > existing. This doesn't work. When I press cancel the window will be destroyed
> > although.
> >
> > I also implemented a menu item 'Quit' where I show the same messagebox and there
> > it works fine.
> >
> > How can I make it work or is there another method to do this? I'm very new to R
> > and tcl/tk.
> >
> > Here is part of my code:
> >
> >
> >    exitProg <- function()
> >    {
> >      returnVal <- tkmessageBox(title="Question",
> >        message="Save modified file?",
> >        icon="question", type="yesnocancel", default="yes")
> >
> >      returnVal <- as.character(returnVal)
> >
> >      if( returnVal == "yes" )
> >      {
> >        # save file
> >        value <- saveFile()
> >        # destroy window when save was successfull
> >        if( value == 1 )
> >          tkdestroy(mw)
> >      }
> >      if( returnVal == "no" )
> >      {
> >        tkdestroy(mw)
> >      }
> >      if( returnVal == "cancel" )
> >      {
> >        # do nothing
> >        cat("Cancel was pressed.\n")
> >      }
> >    }
> >
> >    # bind the destroy event in order to show a message box
> >    tkbind(mw,"<Destroy>",exitProg)
> >
> >    # menu item which works fine
> >    tkadd(fileMenu, "command", label="Quit", command=exitProg)
> >
> >
> >
> > Thank you.
> >
> > Franco Mendolia
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From franco.mendolia at gmx.de  Thu Aug  3 14:25:03 2006
From: franco.mendolia at gmx.de (Franco Mendolia)
Date: Thu, 03 Aug 2006 14:25:03 +0200
Subject: [R] tcl/tk bind destroy event
In-Reply-To: <x2u04unhk9.fsf@viggo.kubism.ku.dk>
References: <Pine.LNX.4.31.0608031202540.799-100000@pc00.mathens.u-psud.fr>
	<x2u04unhk9.fsf@viggo.kubism.ku.dk>
Message-ID: <44D1EB1F.5000408@gmx.de>


> I don't think that is it. As I read it, Tk catches the Destroy event
> alright, but the problem is that it is too late: the window
> destruction is already in motion at the time and there's no way to
> "undestroy" it from within binding. I think you need to create a "wm
> protocol" handler for WM_DELETE_WINDOW. As in
>  
> x <- tktoplevel()
> tcl("wm", "protocol", x, "WM_DELETE_WINDOW", quote(cat("I'm staying!\n")))
> 
> --- now try to destroy the window using window controls ---
> 
> tcl("wm", "protocol", x, "WM_DELETE_WINDOW", "")
> 
> --- things should be back to normal now ---

Yeah! That's it! Thanks!


From r at fam-kuster.ch  Thu Aug  3 14:34:17 2006
From: r at fam-kuster.ch (Thomas Kuster)
Date: Thu, 3 Aug 2006 14:34:17 +0200
Subject: [R] problem with factor in list (same name for diffrent category)
Message-ID: <200608031434.18227.r@fam-kuster.ch>

Hello

I try to ignore the problems with the umlaut.
I read the file in with:

> library("foreign")
> daten <- read.spss(filename)

Then I want select a "Vorlage" (vote), but if I want select for example [6] 
and not 6 and 7 how can I select it? The real name is a other problem, but I 
can solve this via date.

> levels(daten$PROJETX)
  [1] "Bg Stammzellenforschung"
  [2] "Bb  "
  [3] "Bb Neugestaltung des Finanzausgleichs"
  [4] ""
  [5] "EV Postdienste f"
  [6] "Bb "
  [7] "Bb "
  [8] "Bg Steuerpaket"
  [9] "Bb Anhebung der Mehrwertsteuer s"
 [10] "11. AHV-Revision"
 [11] "Volkinitiative Lebenslange Verwahrung"
 [12] ""
   .
   .
   .

Select with:
> pos <- daten$PROJETX=="11. AHV-Revision"
> summary(pos)
   Mode   FALSE    TRUE
logical  200373    1002

This is okay.

> pos <- daten$PROJETX==(levels(daten$PROJETX)[6])
> summary(pos)
   Mode   FALSE    TRUE
logical  194904    6471

Too many TRUE's.

Thomas


From jrkrideau at yahoo.ca  Thu Aug  3 14:42:03 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 3 Aug 2006 08:42:03 -0400 (EDT)
Subject: [R] Vector DF [a,
	b] to replace values :was Finding the position of a variable in a
	data.frame
In-Reply-To: <644e1f320608021410g53c0fc7bx5183867fd7c6ea55@mail.gmail.com>
Message-ID: <20060803124203.27177.qmail@web33811.mail.mud.yahoo.com>


--- jim holtman <jholtman at gmail.com> wrote:

> ?which
> 
> > which(Df >= 50, arr.ind=T)
>   row col
> 5   5   4

This works very nicely as has some other suggestions
on how to replace a value. Assuming that I have more
than one correction to make where Df >= 50, can I use
vectors in the Df[] to do this.

My attempt shows that I can use vectors but it appears
thatthere is something wrong with my logic 

Eample

cat <- c( 3,5,6,8,0)
dog <- c(3,5,3,6, 0)
rat <- c (5, 5, 4, 9, 51)
bat <- c( 12, 42, 45, 32, 54)

Df <- data.frame(cbind(cat, dog, rat, bat))
post <- which(Df >= 50, arr.ind=T)  # find values .=
50
post
correction <- c(77, 88)  # new correct values
row <- post[ ,1]      # vector for row number
col <- post[ ,2]       # vector for column number

Df[row,col] <-  correction
Df

-------result---------
  cat dog rat bat
1   3   3   5  12
2   5   5   5  42
3   6   3   4  45
4   8   6   9  32
5   0   0  88  88

I am replacing both instances with 88 which is not
correct

Thanks



> On 8/2/06, John Kane <jrkrideau at yahoo.ca> wrote:
> >
> > Simple problem but I don't see the answer. I'm
> trying
> > to clean up some data
> > I have 120 columns in a data.frame.  I have one
> value
> > in a column named "blaw" that I want to change.
> How do
> > I find the coordinates. I can find the row by
> doing a
> > subset on the data.frame but how do I find out
> here
> > "blaw " is in columns without manually counting
> them
> > or converting names(Df) to a list and reading down
> the
> > list.
> >
> > Simple example
> >
> > cat <- c( 3,5,6,8,0)
> > dog <- c(3,5,3,6, 0)
> > rat <- c (5, 5, 4, 9, 0)
> > bat <- c( 12, 42, 45, 32, 54)
> >
> > Df <- data.frame(cbind(cat, dog, rat, bat))
> > Df
> > subset(Df, bat >= 50)
> >
> > ----results
> > cat dog rat bat
> > 5   0   0   0  54
> >
> >
> > Thus I know that my target is in row 5 but how do
> I
> > figure out where 'bat' is?
> >
> > All I want to do is be able to say
> > Df[5,4] <- 100
> >
> > Is there some way to have function(bat) return the
> > column number: some kind of a colnum() function? 
> I
> > had thought that I had found somthing  in
> > library(gdata) matchcols but no luck.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
>


From epistat at gmail.com  Thu Aug  3 14:53:49 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 3 Aug 2006 20:53:49 +0800
Subject: [R] how to use the EV AND condEV from BMA's results?
Message-ID: <2fc17e30608030553q4dcc0fefqaddfe363bd40db64@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/85f75daf/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Aug  3 15:03:01 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Aug 2006 15:03:01 +0200
Subject: [R] Vector DF [a,
	b] to replace values :was Finding the position of a variable in a
	data.frame
In-Reply-To: <20060803124203.27177.qmail@web33811.mail.mud.yahoo.com>
References: <20060803124203.27177.qmail@web33811.mail.mud.yahoo.com>
Message-ID: <x2psfinf96.fsf@viggo.kubism.ku.dk>

John Kane <jrkrideau at yahoo.ca> writes:

> --- jim holtman <jholtman at gmail.com> wrote:
> 
> > ?which
> > 
> > > which(Df >= 50, arr.ind=T)
> >   row col
> > 5   5   4
> 
> This works very nicely as has some other suggestions
> on how to replace a value. Assuming that I have more
> than one correction to make where Df >= 50, can I use
> vectors in the Df[] to do this.
> 
> My attempt shows that I can use vectors but it appears
> thatthere is something wrong with my logic 
> 
> Eample
> 
> cat <- c( 3,5,6,8,0)
> dog <- c(3,5,3,6, 0)
> rat <- c (5, 5, 4, 9, 51)
> bat <- c( 12, 42, 45, 32, 54)
> 
> Df <- data.frame(cbind(cat, dog, rat, bat))
> post <- which(Df >= 50, arr.ind=T)  # find values .=
> 50
> post
> correction <- c(77, 88)  # new correct values
> row <- post[ ,1]      # vector for row number
> col <- post[ ,2]       # vector for column number
> 
> Df[row,col] <-  correction
> Df
> 
> -------result---------
>   cat dog rat bat
> 1   3   3   5  12
> 2   5   5   5  42
> 3   6   3   4  45
> 4   8   6   9  32
> 5   0   0  88  88
> 
> I am replacing both instances with 88 which is not
> correct

You're being bitten by two issues here: One is that Df[row,col] does
not vectorize in parallel in the two indices: Df[1:2,1:2] has *four*
elements, not two. Another is that data frames are not matrices: you
can have different types of values in different columns, so you cannot
expect to pick an arbitrary set of elements and assign a vector into
it (well, it doesn't work, anyway).

This sort of stuff works much easier with matrices, where we also have
the wonderful feature of indexing with a matrix:

> M <- cbind(cat, dog, rat, bat)
> M[post]<- correction
> M
     cat dog rat bat
[1,]   3   3   5  12
[2,]   5   5   5  42
[3,]   6   3   4  45
[4,]   8   6   9  32
[5,]   0   0  77  88


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From christian.ritter at shell.com  Thu Aug  3 15:13:01 2006
From: christian.ritter at shell.com (christian.ritter at shell.com)
Date: Thu, 3 Aug 2006 15:13:01 +0200
Subject: [R] bringToTop without focus?
Message-ID: <156CDC8CCFD1894295D2907F16337A48014208F8@bru-s-006.europe.shell.com>

Hi all who know R on Windows,

Quick question: Is there a way to do "bringToTop(stay=TRUE)" without giving "focus"? I would like to pop a graph window but I would like to preserve focus in the window which I was in before. 

Thanks for any lead,

Chris


From murdoch at stats.uwo.ca  Thu Aug  3 15:18:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 03 Aug 2006 09:18:01 -0400
Subject: [R] bringToTop without focus?
In-Reply-To: <156CDC8CCFD1894295D2907F16337A48014208F8@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A48014208F8@bru-s-006.europe.shell.com>
Message-ID: <44D1F789.4020404@stats.uwo.ca>

On 8/3/2006 9:13 AM, christian.ritter at shell.com wrote:
> Hi all who know R on Windows,
> 
> Quick question: Is there a way to do "bringToTop(stay=TRUE)" without giving "focus"? I would like to pop a graph window but I would like to preserve focus in the window which I was in before. 

No, there's no way to do that.  The closest is

{ bringToTop(stay=TRUE); bringToTop(-1) }

but that only works if the previous window was the console.

Duncan Murdoch


From phhs80 at gmail.com  Thu Aug  3 15:33:28 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 3 Aug 2006 14:33:28 +0100
Subject: [R] Looking for transformation to overcome heterogeneity of
	variances
Message-ID: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>

Dear All

My data consists in 96 groups, each one with 10 observations. Levene's
test suggests that the variances are not equal, and therefore I have
tried to apply the classical transformations to have homocedasticity
in order to be able to use ANOVA. Unfortunately, no transformation
that I have used transforms my data into data with homocedasticity.
The histogram of variances is at

http://phhs80.googlepages.com/hist1.png

Is someone able to suggest to me a transformation to overcome the
problem of heterocedasticity?

Thanks in advance,

Paul


From tlumley at u.washington.edu  Thu Aug  3 15:34:04 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Aug 2006 06:34:04 -0700 (PDT)
Subject: [R] read.spss and umlaut
In-Reply-To: <200608031038.14141.r@fam-kuster.ch>
References: <200608021531.56340.r@fam-kuster.ch>
	<Pine.LNX.4.64.0608020752400.2107@homer23.u.washington.edu>
	<200608031038.14141.r@fam-kuster.ch>
Message-ID: <Pine.LNX.4.64.0608030622130.20946@homer23.u.washington.edu>

On Thu, 3 Aug 2006, Thomas Kuster wrote:

> Hello
>
> Am Mittwoch, 2. August 2006 17.11 schrieb Thomas Lumley:
>> This sounds like a conflict between encodings -- eg if R is assuming UTF-8
>> and the file is encoding in Latin-1 then the sequence
>> U+00FC : LATIN SMALL LETTER U WITH DIAERESIS
>> U+0072 : LATIN SMALL LETTER R
>> is coded as FC72 in the file, which is an illegal byte sequence in UTF-8.
>
> Hex:  74 65 20 66 fc 72 20 61 6c 6c 65 53 45 2f 31 36
> Text:  t  e     f  ?  r     a  l  l  e  S  E  /  1  6

Ok, so that looks like Latin-1 encoding in the file

>> The underlying C code (being written in the US quite a long time ago)
>> doesn't know about encodings, and I don't know what the rules are in SPSS
>> for valid characters (I suspect that in these old portable file formats it
>> probably just reads and writes bytes, leaving it up to the OS to interpret
>> them.
>
> But why stopp the C code reading? Is "/" not the endmark of the string? What
> is the problem, if I chance that in the source?

You haven't shown anything that indicates that the C code stopped reading. 
More likely R just stops displaying when it gets to an illegal byte 
sequence.  You could use nchar() to count the bytes in the string to find 
out.

>> You could try running R in a non-UTF-8 locale to see if it helps.
>
> I think my local is non-UTF-8 (de_CH, isolatin). How can I check that, and set
> an other temporary?

You can use charToRaw() to see what R thinks the byte sequence is for a 
word with a u-umlaut.

Sys.setlocale() will let you change the locale, but your locale does look 
non-UTF-8.

This is all guesswork since we can't see the file.

 	-thomas

From p.dalgaard at biostat.ku.dk  Thu Aug  3 15:45:10 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Aug 2006 15:45:10 +0200
Subject: [R] Looking for transformation to overcome heterogeneity of
	variances
In-Reply-To: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
References: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
Message-ID: <x2lkq6ndax.fsf@viggo.kubism.ku.dk>

"Paul Smith" <phhs80 at gmail.com> writes:

> Dear All
> 
> My data consists in 96 groups, each one with 10 observations. Levene's
> test suggests that the variances are not equal, and therefore I have
> tried to apply the classical transformations to have homocedasticity
> in order to be able to use ANOVA. Unfortunately, no transformation
> that I have used transforms my data into data with homocedasticity.
> The histogram of variances is at
> 
> http://phhs80.googlepages.com/hist1.png
> 
> Is someone able to suggest to me a transformation to overcome the
> problem of heterocedasticity?

Not based on that information. Try the following instead:

fit <- lm(y~g)
par(mfrow=c(2,2)); plot(fit)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tuechler at gmx.at  Thu Aug  3 15:45:19 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 03 Aug 2006 14:45:19 +0100
Subject: [R] get() in sapply() in with()
Message-ID: <3.0.6.32.20060803144519.00ad2dd8@pop.gmx.net>

Dear All,

applying some function within a with() function I wanted to use also
sapply() and get() to form a data.frame, but did not succede.
Below is a simplified example.
It is possible to use sapply() within a with() function, it is also
possible to use get() within a with() function, but when I try to use get
within sapply within with I arrive at "Error in get(x, envir, mode,
inherits) : variable "v5" was not found".
Is there a solution?

Thanks,
Heinz T?chler


## example
df1 <- data.frame(v5=16:20, v6=21:25, v7=I(letters[16:20]), v8=letters[16:20])

with(df1, sapply(c('v5', 'v6'), get) ) ## Error, see next line
## Error in get(x, envir, mode, inherits) : variable "v5" was not found

with(df1, sapply(list(v5, v6), mean) ) # does work
with(df1, get('v5') ) # does work

platform       i386-pc-mingw32                          
arch           i386                                     
os             mingw32                                  
system         i386, mingw32                            
status         Patched                                  
major          2                                        
minor          3.1                                      
year           2006                                     
month          07                                       
day            23                                       
svn rev        38687                                    
language       R                                        
version.string Version 2.3.1 Patched (2006-07-23 r38687)


From henrik.parn at bio.ntnu.no  Thu Aug  3 15:46:32 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Thu, 03 Aug 2006 15:46:32 +0200
Subject: [R] efficient way to make NAs of empty cells in a factor (or
	character)
Message-ID: <44D1FE38.3050100@bio.ntnu.no>

Dear all,

I have some csv-files (originating from Excel-files) containing empty 
cells. In my example file I have four variables of different classes, 
each with some empty cells in the original csv-file:

 > test <- read.csv2("test.csv", dec=".")

 > test
  id id2  x   y
1  a      1  NA
2  b   e NA 2.2
3      f  3 3.3
4  c   g  4 4.4


 > class(test$id)
[1] "factor"
 > class(test$id2)
[1] "factor"
 > class(test$x)
[1] "integer"
 > class(test$y)
[1] "numeric"

In the help text of read.csv2 you can read 'Blank fields are also 
considered to be missing values in logical, integer, numeric and complex 
fields.'. Thus, empty cells in a factor (or a character I assume) is not 
considered as missing values but an own level:

 > is.na(test$id)
[1] FALSE FALSE FALSE FALSE
 > levels(test$id)
[1] ""  "a" "b" "c"

When I work with my real (larger) dataset I would like to use functions 
like 'is.na' and '!is.na' on factors. Now I wonder if there is an R 
alternativ to do 'search (for empty cells) and replace (with NA)' in Excel?

I have tried a modification of Uwe Ligges suggestion on missing value 
posted 2 Aug:
 > is.na(test[test==""]) <- TRUE

...but it did not work on the data set:

Error in "[<-.data.frame"(`*tmp*`, test == "", value = c(NA, NA, NA, NA :
        rhs is the wrong length for indexing by a logical matrix


However it worked fine when applied to a single vector:

 > is.na(test$id[test$id==""]) <- TRUE
 > test$id
[1] a    b    <NA> c  
Levels:  a b c

 > is.na(test$id)
[1] FALSE FALSE  TRUE FALSE

Is there a more efficient way to fill empty cells in all my factors in R 
or should I just do it in advance in Excel by 'search and replace'?

Thanks in advance!

-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)


From christian.ritter at shell.com  Thu Aug  3 15:47:32 2006
From: christian.ritter at shell.com (christian.ritter at shell.com)
Date: Thu, 3 Aug 2006 15:47:32 +0200
Subject: [R] bringToTop without focus?
Message-ID: <156CDC8CCFD1894295D2907F16337A48014208F9@bru-s-006.europe.shell.com>

Thanks for fast response.

'Had another look at the problem and found a partial but nice solution which will solve my present problem and might be useful to others.

Here is the context:
In fact I have all my data and stuff in Excel and I use R(D)COM with Rexcel to communicate with R and do some of the work there. One of the things I do is to create a graph in R and to display it. To make sure it stays visible, I use bringToTop(stay=TRUE) from R. But then I would like to keep working in Excel without having to re-activate the workbook window. This is where the problem originated. Within excel there is a somewhat crude solution:
Submit the code which makes the graph via a VBA procedure (macro) and end it with SendKeys "%{TAB}". Here is an example:

  Sub Refilter
  Range("Alldata").AdvancedFilter Action:=xlFilterCopy, CriteriaRange:=Range( _
        "Criteria"), CopyToRange:=Range("Extractrange"), Unique:=False
  SendCommands Range("DiagPlot")
  SendKeys "%{TAB}"
  End sub

Here "DiagPlot" is a range which contains the code to construct the plot such as plot(x,y). 

The VBA macro should be run when the worksheet of interest has focus. The call SendKeys "%{TAB}" simulates pressing ALT TAB which goes back to the previous window on the stack. This was the one which had focus when bringToTop. There we go. 

Have a nice day,

Chris.

Morale: if impossible / change the point of view

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
Sent: Thursday, 03 August, 2006 3:18 PM
To: Ritter, Christian C GSMCIL-GSTMS/2
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] bringToTop without focus?


On 8/3/2006 9:13 AM, christian.ritter at shell.com wrote:
> Hi all who know R on Windows,
> 
> Quick question: Is there a way to do "bringToTop(stay=TRUE)" without giving "focus"? I would like to pop a graph window but I would like to preserve focus in the window which I was in before. 

No, there's no way to do that.  The closest is

{ bringToTop(stay=TRUE); bringToTop(-1) }

but that only works if the previous window was the console.

Duncan Murdoch


From scheidler at informatik.uni-leipzig.de  Thu Aug  3 15:56:17 2006
From: scheidler at informatik.uni-leipzig.de (scheidler)
Date: Thu, 03 Aug 2006 15:56:17 +0200
Subject: [R] questions on plotting dedrograms
Message-ID: <44D20081.9060206@informatik.uni-leipzig.de>

Hi,

i've two questions concerning the plot of a dendrogram. first, i use 
hclust for clustering and if i plot the dendrogram, then the maximal 
height is the maximal dissimilarity found in my data. but i want to have 
a arbitary maximal height. for example if the maximal dissimilarity in 
my data is 50 and i want a height of 100, the plot should be compressed 
by 1/2 and the line to the root node should be as long as the rest of 
the plot. How can i do that ? (i want to use it for comparing 2 
clusterings with the same maximal possible dissimilarity)
The other question is, how can i plot the dendrogram horizontal, but 
with the root node to the right ?

thx for your efford
alexander scheidler


From scheidler at informatik.uni-leipzig.de  Thu Aug  3 15:56:43 2006
From: scheidler at informatik.uni-leipzig.de (scheidler)
Date: Thu, 03 Aug 2006 15:56:43 +0200
Subject: [R] questions on plotting dedrograms
Message-ID: <44D2009B.3080207@informatik.uni-leipzig.de>

Hi,

i've two questions concerning the plot of a dendrogram. first, i use 
hclust for clustering and if i plot the dendrogram, then the maximal 
height is the maximal dissimilarity found in my data. but i want to have 
a arbitary maximal height. for example if the maximal dissimilarity in 
my data is 50 and i want a height of 100, the plot should be compressed 
by 1/2 and the line to the root node should be as long as the rest of 
the plot. How can i do that ? (i want to use it for comparing 2 
clusterings with the same maximal possible dissimilarity)
The other question is, how can i plot the dendrogram horizontal, but 
with the root node to the right ?

thx for your efford
alexander scheidler


From phhs80 at gmail.com  Thu Aug  3 15:58:10 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 3 Aug 2006 14:58:10 +0100
Subject: [R] Looking for transformation to overcome heterogeneity of
	variances
In-Reply-To: <x2lkq6ndax.fsf@viggo.kubism.ku.dk>
References: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
	<x2lkq6ndax.fsf@viggo.kubism.ku.dk>
Message-ID: <6ade6f6c0608030658y6d330380oe4345a12081ebc37@mail.gmail.com>

On 03 Aug 2006 15:45:10 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > My data consists in 96 groups, each one with 10 observations. Levene's
> > test suggests that the variances are not equal, and therefore I have
> > tried to apply the classical transformations to have homocedasticity
> > in order to be able to use ANOVA. Unfortunately, no transformation
> > that I have used transforms my data into data with homocedasticity.
> > The histogram of variances is at
> >
> > http://phhs80.googlepages.com/hist1.png
> >
> > Is someone able to suggest to me a transformation to overcome the
> > problem of heterocedasticity?
>
> Not based on that information. Try the following instead:
>
> fit <- lm(y~g)
> par(mfrow=c(2,2)); plot(fit)

Thanks, Peter. By 'g', you mean

factor1* factor2*factor3*factor4

?

Paul


From dgerlanc at gmail.com  Thu Aug  3 16:10:46 2006
From: dgerlanc at gmail.com (Daniel Gerlanc)
Date: Thu, 3 Aug 2006 10:10:46 -0400
Subject: [R] Vectorizing a "for" loop
Message-ID: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>

Hello all,

Consider the following problem:

There are two vectors:

rows <- c(1, 2, 3, 4, 5)
columns <- c(10, 11, 12, 13, 14)

I want to create a matrix with dimensions length(rows) x length(columns):

res <- matrix(nrow = length(rows), ncol = length(columns))

If "i" and "j" are the row and column indexes respectively, the values
of the cells are abs(rows[i] - columns[j]).  The resultant matrix
follows:

     [,1] [,2] [,3] [,4] [,5]
[1,]    9   10   11   12   13
[2,]    8    9    10   11   12
[3,]    7    8      9   10   11
[4,]    6    7      8     9   10
[5,]    5    6      7     8    9

This matrix may be generated by using a simple "for" loop:

for(i in 1:length(rows)){
  for(j in 1:length(columns)){
    res[i,j] <- abs(rows[i] - columns[j])
  }
}

Is there a quicker, vector-based approach for doing this or a function
included in the recommended packages that does this?

Thanks!

-- Dan Gerlanc
Williams College


From dimitris.rizopoulos at med.kuleuven.be  Thu Aug  3 16:20:41 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 3 Aug 2006 16:20:41 +0200
Subject: [R] efficient way to make NAs of empty cells in a factor
	(orcharacter)
References: <44D1FE38.3050100@bio.ntnu.no>
Message-ID: <001f01c6b708$00197e90$0540210a@www.domain>

try to use the 'na.strings' argument of read.csv(), e.g.,

test <- read.csv("test.csv", na.strings = "")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Henrik Parn" <henrik.parn at bio.ntnu.no>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Thursday, August 03, 2006 3:46 PM
Subject: [R] efficient way to make NAs of empty cells in a factor 
(orcharacter)


Dear all,

I have some csv-files (originating from Excel-files) containing empty
cells. In my example file I have four variables of different classes,
each with some empty cells in the original csv-file:

 > test <- read.csv2("test.csv", dec=".")

 > test
  id id2  x   y
1  a      1  NA
2  b   e NA 2.2
3      f  3 3.3
4  c   g  4 4.4


 > class(test$id)
[1] "factor"
 > class(test$id2)
[1] "factor"
 > class(test$x)
[1] "integer"
 > class(test$y)
[1] "numeric"

In the help text of read.csv2 you can read 'Blank fields are also
considered to be missing values in logical, integer, numeric and 
complex
fields.'. Thus, empty cells in a factor (or a character I assume) is 
not
considered as missing values but an own level:

 > is.na(test$id)
[1] FALSE FALSE FALSE FALSE
 > levels(test$id)
[1] ""  "a" "b" "c"

When I work with my real (larger) dataset I would like to use 
functions
like 'is.na' and '!is.na' on factors. Now I wonder if there is an R
alternativ to do 'search (for empty cells) and replace (with NA)' in 
Excel?

I have tried a modification of Uwe Ligges suggestion on missing value
posted 2 Aug:
 > is.na(test[test==""]) <- TRUE

...but it did not work on the data set:

Error in "[<-.data.frame"(`*tmp*`, test == "", value = c(NA, NA, NA, 
NA :
        rhs is the wrong length for indexing by a logical matrix


However it worked fine when applied to a single vector:

 > is.na(test$id[test$id==""]) <- TRUE
 > test$id
[1] a    b    <NA> c
Levels:  a b c

 > is.na(test$id)
[1] FALSE FALSE  TRUE FALSE

Is there a more efficient way to fill empty cells in all my factors in 
R
or should I just do it in advance in Excel by 'search and replace'?

Thanks in advance!

-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From neuro3000 at hotmail.com  Thu Aug  3 16:22:29 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Thu, 03 Aug 2006 10:22:29 -0400
Subject: [R] Vectorizing a "for" loop
In-Reply-To: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <BAY112-F4C2AA07E01F6C850ABBBAAF530@phx.gbl>

Hi,

Try this:

rows <- c(1, 2, 3, 4, 5)
columns <- c(10, 11, 12, 13, 14)
expanded <-expand.grid(rows,columns)
abslist <-abs(ex$Var1-ex$Var2)
res <- matrix(abslist,nrow = length(rows), ncol = length(columns))

Neurorox


>From: "Daniel Gerlanc" <dgerlanc at gmail.com>
>Reply-To: dgerlanc at gmail.com
>To: r-help at stat.math.ethz.ch
>Subject: [R] Vectorizing a "for" loop
>Date: Thu, 3 Aug 2006 10:10:46 -0400
>
>Hello all,
>
>Consider the following problem:
>
>There are two vectors:
>
>rows <- c(1, 2, 3, 4, 5)
>columns <- c(10, 11, 12, 13, 14)
>
>I want to create a matrix with dimensions length(rows) x length(columns):
>
>res <- matrix(nrow = length(rows), ncol = length(columns))
>
>If "i" and "j" are the row and column indexes respectively, the values
>of the cells are abs(rows[i] - columns[j]).  The resultant matrix
>follows:
>
>      [,1] [,2] [,3] [,4] [,5]
>[1,]    9   10   11   12   13
>[2,]    8    9    10   11   12
>[3,]    7    8      9   10   11
>[4,]    6    7      8     9   10
>[5,]    5    6      7     8    9
>
>This matrix may be generated by using a simple "for" loop:
>
>for(i in 1:length(rows)){
>   for(j in 1:length(columns)){
>     res[i,j] <- abs(rows[i] - columns[j])
>   }
>}
>
>Is there a quicker, vector-based approach for doing this or a function
>included in the recommended packages that does this?
>
>Thanks!
>
>-- Dan Gerlanc
>Williams College
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From tlumley at u.washington.edu  Thu Aug  3 16:22:39 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Aug 2006 07:22:39 -0700 (PDT)
Subject: [R] Vectorizing a "for" loop
In-Reply-To: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
References: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608030722040.20946@homer23.u.washington.edu>

On Thu, 3 Aug 2006, Daniel Gerlanc wrote:

> Hello all,
>
> Consider the following problem:
>
> There are two vectors:
>
> rows <- c(1, 2, 3, 4, 5)
> columns <- c(10, 11, 12, 13, 14)
>
> I want to create a matrix with dimensions length(rows) x length(columns):
>
> res <- matrix(nrow = length(rows), ncol = length(columns))
>
> If "i" and "j" are the row and column indexes respectively, the values
> of the cells are abs(rows[i] - columns[j]).  The resultant matrix
> follows:
>
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    9   10   11   12   13
> [2,]    8    9    10   11   12
> [3,]    7    8      9   10   11
> [4,]    6    7      8     9   10
> [5,]    5    6      7     8    9
>
> This matrix may be generated by using a simple "for" loop:
>
> for(i in 1:length(rows)){
>  for(j in 1:length(columns)){
>    res[i,j] <- abs(rows[i] - columns[j])
>  }
> }
>
> Is there a quicker, vector-based approach for doing this or a function
> included in the recommended packages that does this?
>

abs(outer(rows,columns,"-"))


 	-thomas


From petr.pikal at precheza.cz  Thu Aug  3 16:24:30 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 03 Aug 2006 16:24:30 +0200
Subject: [R] Vectorizing a "for" loop
In-Reply-To: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <44D2233E.13303.9D447F@localhost>

Hi

outer(rows, columns, function(x,y) abs(x-y))
shall do it.

HTH
Petr


On 3 Aug 2006 at 10:10, Daniel Gerlanc wrote:

Date sent:      	Thu, 3 Aug 2006 10:10:46 -0400
From:           	"Daniel Gerlanc" <dgerlanc at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Vectorizing a "for" loop
Send reply to:  	dgerlanc at gmail.com
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hello all,
> 
> Consider the following problem:
> 
> There are two vectors:
> 
> rows <- c(1, 2, 3, 4, 5)
> columns <- c(10, 11, 12, 13, 14)
> 
> I want to create a matrix with dimensions length(rows) x
> length(columns):
> 
> res <- matrix(nrow = length(rows), ncol = length(columns))
> 
> If "i" and "j" are the row and column indexes respectively, the values
> of the cells are abs(rows[i] - columns[j]).  The resultant matrix
> follows:
> 
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    9   10   11   12   13
> [2,]    8    9    10   11   12
> [3,]    7    8      9   10   11
> [4,]    6    7      8     9   10
> [5,]    5    6      7     8    9
> 
> This matrix may be generated by using a simple "for" loop:
> 
> for(i in 1:length(rows)){
>   for(j in 1:length(columns)){
>     res[i,j] <- abs(rows[i] - columns[j])
>   }
> }
> 
> Is there a quicker, vector-based approach for doing this or a function
> included in the recommended packages that does this?
> 
> Thanks!
> 
> -- Dan Gerlanc
> Williams College
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From mschwartz at mn.rr.com  Thu Aug  3 16:23:42 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 03 Aug 2006 09:23:42 -0500
Subject: [R] Vectorizing a "for" loop
In-Reply-To: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
References: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <1154615022.4739.6.camel@localhost.localdomain>

On Thu, 2006-08-03 at 10:10 -0400, Daniel Gerlanc wrote:
> Hello all,
> 
> Consider the following problem:
> 
> There are two vectors:
> 
> rows <- c(1, 2, 3, 4, 5)
> columns <- c(10, 11, 12, 13, 14)
> 
> I want to create a matrix with dimensions length(rows) x length(columns):
> 
> res <- matrix(nrow = length(rows), ncol = length(columns))
> 
> If "i" and "j" are the row and column indexes respectively, the values
> of the cells are abs(rows[i] - columns[j]).  The resultant matrix
> follows:
> 
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    9   10   11   12   13
> [2,]    8    9    10   11   12
> [3,]    7    8      9   10   11
> [4,]    6    7      8     9   10
> [5,]    5    6      7     8    9
> 
> This matrix may be generated by using a simple "for" loop:
> 
> for(i in 1:length(rows)){
>   for(j in 1:length(columns)){
>     res[i,j] <- abs(rows[i] - columns[j])
>   }
> }
> 
> Is there a quicker, vector-based approach for doing this or a function
> included in the recommended packages that does this?
> 
> Thanks!

See ?outer

> outer(rows, columns, function(x, y) abs(x - y))
     [,1] [,2] [,3] [,4] [,5]
[1,]    9   10   11   12   13
[2,]    8    9   10   11   12
[3,]    7    8    9   10   11
[4,]    6    7    8    9   10
[5,]    5    6    7    8    9

HTH,

Marc Schwartz


From neuro3000 at hotmail.com  Thu Aug  3 16:25:32 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Thu, 03 Aug 2006 10:25:32 -0400
Subject: [R] Vectorizing a "for" loop
In-Reply-To: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <BAY112-F2365292AB6F794D8517AD5AF530@phx.gbl>

Sorry,

I used ex instead of expanded (was working name).

rows <- c(1, 2, 3, 4, 5)
columns <- c(10, 11, 12, 13, 14)
expanded <-expand.grid(rows,columns)
abslist <-abs(expanded$Var1-expanded$Var2)
res <- matrix(abslist,nrow = length(rows), ncol = length(columns))

D?sol? encore

Neuro


>From: "Daniel Gerlanc" <dgerlanc at gmail.com>
>Reply-To: dgerlanc at gmail.com
>To: r-help at stat.math.ethz.ch
>Subject: [R] Vectorizing a "for" loop
>Date: Thu, 3 Aug 2006 10:10:46 -0400
>
>Hello all,
>
>Consider the following problem:
>
>There are two vectors:
>
>rows <- c(1, 2, 3, 4, 5)
>columns <- c(10, 11, 12, 13, 14)
>
>I want to create a matrix with dimensions length(rows) x length(columns):
>
>res <- matrix(nrow = length(rows), ncol = length(columns))
>
>If "i" and "j" are the row and column indexes respectively, the values
>of the cells are abs(rows[i] - columns[j]).  The resultant matrix
>follows:
>
>      [,1] [,2] [,3] [,4] [,5]
>[1,]    9   10   11   12   13
>[2,]    8    9    10   11   12
>[3,]    7    8      9   10   11
>[4,]    6    7      8     9   10
>[5,]    5    6      7     8    9
>
>This matrix may be generated by using a simple "for" loop:
>
>for(i in 1:length(rows)){
>   for(j in 1:length(columns)){
>     res[i,j] <- abs(rows[i] - columns[j])
>   }
>}
>
>Is there a quicker, vector-based approach for doing this or a function
>included in the recommended packages that does this?
>
>Thanks!
>
>-- Dan Gerlanc
>Williams College
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From vettorazzi at econ.uni-hamburg.de  Thu Aug  3 16:28:18 2006
From: vettorazzi at econ.uni-hamburg.de (Eik Vettorazzi)
Date: Thu, 03 Aug 2006 16:28:18 +0200
Subject: [R] Vectorizing a "for" loop
In-Reply-To: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
References: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <op.tdpo5gcvj3tevv@econ.uni-hamburg.de>

res<-outer(rows,columns,FUN=function(x,y) abs(x-y))

will help you.

Am Thu, 03 Aug 2006 16:10:46 +0200 schrieb Daniel Gerlanc  
<dgerlanc at gmail.com>:

> Hello all,
>
> Consider the following problem:
>
> There are two vectors:
>
> rows <- c(1, 2, 3, 4, 5)
> columns <- c(10, 11, 12, 13, 14)
>
> I want to create a matrix with dimensions length(rows) x length(columns):
>
> res <- matrix(nrow = length(rows), ncol = length(columns))
>
> If "i" and "j" are the row and column indexes respectively, the values
> of the cells are abs(rows[i] - columns[j]).  The resultant matrix
> follows:
>
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    9   10   11   12   13
> [2,]    8    9    10   11   12
> [3,]    7    8      9   10   11
> [4,]    6    7      8     9   10
> [5,]    5    6      7     8    9
>
> This matrix may be generated by using a simple "for" loop:
>
> for(i in 1:length(rows)){
>   for(j in 1:length(columns)){
>     res[i,j] <- abs(rows[i] - columns[j])
>   }
> }
>
> Is there a quicker, vector-based approach for doing this or a function
> included in the recommended packages that does this?
>
> Thanks!
>
> -- Dan Gerlanc
> Williams College
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
--------------------

Universit?t Hamburg
Institut f?r Statistik und ?konometrie
Dipl.-Wi.-Math. Eik Vettorazzi
Von-Melle-Park 5
20146 Hamburg

Tel.: +49 40-42838-3540


From dimitris.rizopoulos at med.kuleuven.be  Thu Aug  3 16:36:35 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 3 Aug 2006 16:36:35 +0200
Subject: [R] Vectorizing a "for" loop
References: <84c9e3cb0608030710t3b5e3ce0j6a5d8d47c7bc8264@mail.gmail.com>
Message-ID: <004001c6b70a$38d95140$0540210a@www.domain>

try this:

abs(outer(rows, columns, "-"))


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Daniel Gerlanc" <dgerlanc at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 03, 2006 4:10 PM
Subject: [R] Vectorizing a "for" loop


> Hello all,
>
> Consider the following problem:
>
> There are two vectors:
>
> rows <- c(1, 2, 3, 4, 5)
> columns <- c(10, 11, 12, 13, 14)
>
> I want to create a matrix with dimensions length(rows) x 
> length(columns):
>
> res <- matrix(nrow = length(rows), ncol = length(columns))
>
> If "i" and "j" are the row and column indexes respectively, the 
> values
> of the cells are abs(rows[i] - columns[j]).  The resultant matrix
> follows:
>
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    9   10   11   12   13
> [2,]    8    9    10   11   12
> [3,]    7    8      9   10   11
> [4,]    6    7      8     9   10
> [5,]    5    6      7     8    9
>
> This matrix may be generated by using a simple "for" loop:
>
> for(i in 1:length(rows)){
>  for(j in 1:length(columns)){
>    res[i,j] <- abs(rows[i] - columns[j])
>  }
> }
>
> Is there a quicker, vector-based approach for doing this or a 
> function
> included in the recommended packages that does this?
>
> Thanks!
>
> -- Dan Gerlanc
> Williams College
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From p.dalgaard at biostat.ku.dk  Thu Aug  3 16:32:38 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Aug 2006 16:32:38 +0200
Subject: [R] Looking for transformation to overcome heterogeneity of
	variances
In-Reply-To: <6ade6f6c0608030658y6d330380oe4345a12081ebc37@mail.gmail.com>
References: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
	<x2lkq6ndax.fsf@viggo.kubism.ku.dk>
	<6ade6f6c0608030658y6d330380oe4345a12081ebc37@mail.gmail.com>
Message-ID: <x2hd0topo9.fsf@viggo.kubism.ku.dk>

"Paul Smith" <phhs80 at gmail.com> writes:

> On 03 Aug 2006 15:45:10 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > > My data consists in 96 groups, each one with 10 observations. Levene's
> > > test suggests that the variances are not equal, and therefore I have
> > > tried to apply the classical transformations to have homocedasticity
> > > in order to be able to use ANOVA. Unfortunately, no transformation
> > > that I have used transforms my data into data with homocedasticity.
> > > The histogram of variances is at
> > >
> > > http://phhs80.googlepages.com/hist1.png
> > >
> > > Is someone able to suggest to me a transformation to overcome the
> > > problem of heterocedasticity?
> >
> > Not based on that information. Try the following instead:
> >
> > fit <- lm(y~g)
> > par(mfrow=c(2,2)); plot(fit)
> 
> Thanks, Peter. By 'g', you mean
> 
> factor1* factor2*factor3*factor4

If that defines your 96 groups, yes.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tlumley at u.washington.edu  Thu Aug  3 16:33:47 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Aug 2006 07:33:47 -0700 (PDT)
Subject: [R] get() in sapply() in with()
In-Reply-To: <3.0.6.32.20060803144519.00ad2dd8@pop.gmx.net>
References: <3.0.6.32.20060803144519.00ad2dd8@pop.gmx.net>
Message-ID: <Pine.LNX.4.64.0608030726170.20946@homer23.u.washington.edu>

On Thu, 3 Aug 2006, Heinz Tuechler wrote:

> Dear All,
>
> applying some function within a with() function I wanted to use also
> sapply() and get() to form a data.frame, but did not succede.
> Below is a simplified example.
> It is possible to use sapply() within a with() function, it is also
> possible to use get() within a with() function, but when I try to use get
> within sapply within with I arrive at "Error in get(x, envir, mode,
> inherits) : variable "v5" was not found".
> Is there a solution?

This isn't what you want to hear, but the solution is probably to not use 
get(). How to not use get() will depend on what your problem really is.

>
> ## example
> df1 <- data.frame(v5=16:20, v6=21:25, v7=I(letters[16:20]), v8=letters[16:20])
>
> with(df1, sapply(c('v5', 'v6'), get) ) ## Error, see next line
> ## Error in get(x, envir, mode, inherits) : variable "v5" was not found

get() looks in the environment it was called from, which is the 
environment inside lapply(), whose parent is the environment of 
the base package. There is no "v5" there.

> with(df1, sapply(list(v5, v6), mean) ) # does work

This works because list(v5,v6) is evaluated in df1.

> with(df1, get('v5') ) # does work

This works because get() looks in the environment it was called from, 
which is df1.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From tlumley at u.washington.edu  Thu Aug  3 16:38:16 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Aug 2006 07:38:16 -0700 (PDT)
Subject: [R] question about dll crashing R
In-Reply-To: <20060803100208.78789.qmail@web61312.mail.yahoo.com>
References: <20060803100208.78789.qmail@web61312.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608030736250.20946@homer23.u.washington.edu>

On Thu, 3 Aug 2006, Benn Fine wrote:

> I have ported some R code to C to make it faster.
>
> I can perform .Call("foobar",....) once and it
> works fine. Absolutely correct answer.
>
> If I put a loop inside foobar and run the  main code
> routine more than 100 times, it crashes R.
>
> Or if I call .Call("foobar"....) seperately more than
> two tims it crashes R.
>
<snip>
> SEXP mvntest (SEXP mean, SEXP cov, SEXP temp)
>
>
> { int nrx , ncx , nry , ncy ,info,mode;
> SEXP xdims , ydims , ans;
>
> int i,j, one=1;
> info = 1;
>
> xdims = getAttrib (mean , R_DimSymbol ) ;
> ydims = getAttrib (cov , R_DimSymbol ) ;
> mode = REALSXP;
> nrx = INTEGER( xdims ) [ 0 ] ;
> ncx = INTEGER( xdims ) [ 1 ] ;
> nry = INTEGER( ydims ) [ 0 ] ;
> ncy = INTEGER( ydims ) [ 1 ] ;
>
>
>
> /* create the upper trianglular matrix A */
>
> /* such that t(A) %*% A = Sigma */
>
> GetRNGstate();
>
> F77_CALL(dpofa) ( REAL( cov ), &nry , &ncy , &info);
> Rprintf("Info = %d\n",info);
>
>
> for(i=0;i<nry;i++)
>  for(j=0;j<i;j++)
>    REAL(cov)[i+j*ncy] = 0.0;
>
>
> PROTECT( ans = allocMatrix (mode, nrx , one ) ) ;
> for(i=0;i<nry;i++)
>  REAL(temp)[i] = rnorm(0,1);
> ans = tmatrixprod(cov,temp);

^^^^^^^^^
Here you are returning a new SEXP from tmatrixprod but not protecting it. 
Remember, PROTECT() operates on the *value* of its argument, so it 
protects the thing that ans points to. When ans points to a new thing, it 
is still the old thing that is protected.

 	-thomas

> for(i=0;i<nry;i++)
>  REAL(ans)[i] = REAL(ans)[i]+REAL(mean)[i];
> UNPROTECT( 1 ) ;
> PutRNGstate();
> return( ans ) ;
>
>
> }
>
>
> I have a feeling I am messing up memory usage
> somewhere but haven't a clue. Do I need to do garbage
> collecting inside the C program ?The fact that the
> code
> runs a few times before R crashes is driving me nuts.
> I send most of what I need into the C routine from R,
> so I am not creating that many SEXP objects within the
> program.
>
> Any hints or ideas ?
>
> Thanks!
>
> Benn
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From petr.pikal at precheza.cz  Thu Aug  3 16:40:42 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 03 Aug 2006 16:40:42 +0200
Subject: [R] efficient way to make NAs of empty cells in a factor
	(or	character)
In-Reply-To: <44D1FE38.3050100@bio.ntnu.no>
Message-ID: <44D2270A.26938.AC24AA@localhost>

Hi

try to set

na.strings = ""

in calling read.csv2. Works for me

> is.na(read.delim("clipboard", na.strings="")$mono)
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE

> read.delim("clipboard", na.strings="")$mono
[1] hruby     hruby     jemny     jemny     nejhrubsi nejhrubsi 
standard  standard  <NA>     
Levels: hruby jemny nejhrubsi standard

or you can try

test[(test=="")] <- NA

HTH
Petr


On 3 Aug 2006 at 15:46, Henrik Parn wrote:

Date sent:      	Thu, 03 Aug 2006 15:46:32 +0200
From:           	Henrik Parn <henrik.parn at bio.ntnu.no>
Organization:   	NTNU
To:             	R-help <r-help at stat.math.ethz.ch>
Subject:        	[R] efficient way to make NAs of empty cells in a factor (or
	character)
Send reply to:  	henrik.parn at bio.ntnu.no
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Dear all,
> 
> I have some csv-files (originating from Excel-files) containing empty
> cells. In my example file I have four variables of different classes,
> each with some empty cells in the original csv-file:
> 
>  > test <- read.csv2("test.csv", dec=".")
> 
>  > test
>   id id2  x   y
> 1  a      1  NA
> 2  b   e NA 2.2
> 3      f  3 3.3
> 4  c   g  4 4.4
> 
> 
>  > class(test$id)
> [1] "factor"
>  > class(test$id2)
> [1] "factor"
>  > class(test$x)
> [1] "integer"
>  > class(test$y)
> [1] "numeric"
> 
> In the help text of read.csv2 you can read 'Blank fields are also
> considered to be missing values in logical, integer, numeric and
> complex fields.'. Thus, empty cells in a factor (or a character I
> assume) is not considered as missing values but an own level:
> 
>  > is.na(test$id)
> [1] FALSE FALSE FALSE FALSE
>  > levels(test$id)
> [1] ""  "a" "b" "c"
> 
> When I work with my real (larger) dataset I would like to use
> functions like 'is.na' and '!is.na' on factors. Now I wonder if there
> is an R alternativ to do 'search (for empty cells) and replace (with
> NA)' in Excel?
> 
> I have tried a modification of Uwe Ligges suggestion on missing value
> posted 2 Aug:
>  > is.na(test[test==""]) <- TRUE
> 
> ...but it did not work on the data set:
> 
> Error in "[<-.data.frame"(`*tmp*`, test == "", value = c(NA, NA, NA,
> NA :
>         rhs is the wrong length for indexing by a logical matrix
> 
> 
> However it worked fine when applied to a single vector:
> 
>  > is.na(test$id[test$id==""]) <- TRUE
>  > test$id
> [1] a    b    <NA> c  
> Levels:  a b c
> 
>  > is.na(test$id)
> [1] FALSE FALSE  TRUE FALSE
> 
> Is there a more efficient way to fill empty cells in all my factors in
> R or should I just do it in advance in Excel by 'search and replace'?
> 
> Thanks in advance!
> 
> -- 
> ************************
> Henrik P?rn
> Department of Biology
> NTNU
> 7491 Trondheim
> Norway
> 
> +47 735 96282 (office)
> +47 909 89 255 (mobile)
> +47 735 96100 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From phhs80 at gmail.com  Thu Aug  3 16:41:51 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 3 Aug 2006 15:41:51 +0100
Subject: [R] Looking for transformation to overcome heterogeneity of
	variances
In-Reply-To: <x2hd0topo9.fsf@viggo.kubism.ku.dk>
References: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
	<x2lkq6ndax.fsf@viggo.kubism.ku.dk>
	<6ade6f6c0608030658y6d330380oe4345a12081ebc37@mail.gmail.com>
	<x2hd0topo9.fsf@viggo.kubism.ku.dk>
Message-ID: <6ade6f6c0608030741icc85d69sb4a4cf38b7fb5eb6@mail.gmail.com>

On 03 Aug 2006 16:32:38 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > > > My data consists in 96 groups, each one with 10 observations. Levene's
> > > > test suggests that the variances are not equal, and therefore I have
> > > > tried to apply the classical transformations to have homocedasticity
> > > > in order to be able to use ANOVA. Unfortunately, no transformation
> > > > that I have used transforms my data into data with homocedasticity.
> > > > The histogram of variances is at
> > > >
> > > > http://phhs80.googlepages.com/hist1.png
> > > >
> > > > Is someone able to suggest to me a transformation to overcome the
> > > > problem of heterocedasticity?
> > >
> > > Not based on that information. Try the following instead:
> > >
> > > fit <- lm(y~g)
> > > par(mfrow=c(2,2)); plot(fit)
> >
> > Thanks, Peter. By 'g', you mean
> >
> > factor1* factor2*factor3*factor4
>
> If that defines your 96 groups, yes.

Thanks, Peter. The result of

> fit <- lm(tardiness ~ interaction(factor1,factor2,factor3,factor4))
> par(mfrow=c(2,2)); plot(fit)
Warning message:
X11 used font size 8 when 7 was requested
>

is at

http://phhs80.googlepages.com/2transform1.png

Paul


From jeroschh at ohsu.edu  Thu Aug  3 16:46:49 2006
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Thu, 03 Aug 2006 07:46:49 -0700
Subject: [R] bullseye or polar display of "circular" data
Message-ID: <s4d1a9f6.087@ohsu.edu>


I have data for several rings of a left heart chamber, and which I would like to display in concentric rings, with color-encoding of the values. Each ring corresponds to one slice through the heart, and the rings correspond to positions from the base to the apex of the heart as you move from the outermost ring to the innermost one. The data have a circular pattern. These types of displays are referred to as bullseye displays in the nuclear medicine literature. Does any reader of these messages know of a R function/package that offers this functionality?

Also I noticed that in some contexts you can define a "circular" attribute for your data. Are there plot routines for such "circular" data?

thank you!

Michael Jerosch-Herold


From dieter.menne at menne-biomed.de  Thu Aug  3 16:57:19 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 3 Aug 2006 14:57:19 +0000 (UTC)
Subject: [R] run self written functions
References: <44D1A79A.90602@yahoo.de>
Message-ID: <loom.20060803T164913-998@post.gmane.org>

Antje <niederlein-rstat <at> yahoo.de> writes:

> I'm not sure if I'm in the right place with my question...
> I'm running R on Windows and wrote a function and saved it as .R file.
> It looks like this:
> 
> bmi <- function(weight, height) {
>     bmi <- weight / height^2
>     bmi
> }
> 
> If I want to use this function, I have to mark everything and then press 
> Ctrl-R. But then everything single line is executed on the command line, 
> which means that I will "loose" my history when the code becomes longer.

I suggest that you forget the "history" function, it tends to accumulate all the
nonsense we make. Use the following method instead: Keep a RGui (assuming
Windows) on the left half of your screen, and an editor (Tinn-R, I suggest, if
you are no emacian) on the right half. Write everything you plan to keep in the
editor, and send it to RGui using the menu items of the editor provided. That
way, you end up with a nice little program that you can take home and re-run
tomorry, when your BMI has changed (I hope it stays constant).

R is a bit different from other programming languages, it does not need the
assignment to the function name. The following is the same as your function.

bmi <- function(weight, height) weight / height^2

Dieter
(T?)


From tuechler at gmx.at  Thu Aug  3 17:08:45 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 03 Aug 2006 16:08:45 +0100
Subject: [R] get() in sapply() in with()
In-Reply-To: <Pine.LNX.4.64.0608030726170.20946@homer23.u.washington.edu
 >
References: <3.0.6.32.20060803144519.00ad2dd8@pop.gmx.net>
	<3.0.6.32.20060803144519.00ad2dd8@pop.gmx.net>
Message-ID: <3.0.6.32.20060803160845.00a62d00@pop.gmx.net>

Thank you, Thomas!
It helps a lot to know that something is impossible and that I have to look
for a different kind of solution.
Heinz

At 07:33 03.08.2006 -0700, Thomas Lumley wrote:
>On Thu, 3 Aug 2006, Heinz Tuechler wrote:
>
>> Dear All,
>>
>> applying some function within a with() function I wanted to use also
>> sapply() and get() to form a data.frame, but did not succede.
>> Below is a simplified example.
>> It is possible to use sapply() within a with() function, it is also
>> possible to use get() within a with() function, but when I try to use get
>> within sapply within with I arrive at "Error in get(x, envir, mode,
>> inherits) : variable "v5" was not found".
>> Is there a solution?
>
>This isn't what you want to hear, but the solution is probably to not use 
>get(). How to not use get() will depend on what your problem really is.
>
>>
>> ## example
>> df1 <- data.frame(v5=16:20, v6=21:25, v7=I(letters[16:20]),
v8=letters[16:20])
>>
>> with(df1, sapply(c('v5', 'v6'), get) ) ## Error, see next line
>> ## Error in get(x, envir, mode, inherits) : variable "v5" was not found
>
>get() looks in the environment it was called from, which is the 
>environment inside lapply(), whose parent is the environment of 
>the base package. There is no "v5" there.
>
>> with(df1, sapply(list(v5, v6), mean) ) # does work
>
>This works because list(v5,v6) is evaluated in df1.
>
>> with(df1, get('v5') ) # does work
>
>This works because get() looks in the environment it was called from, 
>which is df1.
>
>
> 	-thomas
>
>Thomas Lumley			Assoc. Professor, Biostatistics
>tlumley at u.washington.edu	University of Washington, Seattle
>
>


From mschwartz at mn.rr.com  Thu Aug  3 17:31:38 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 03 Aug 2006 10:31:38 -0500
Subject: [R] bullseye or polar display of "circular" data
In-Reply-To: <s4d1a9f6.087@ohsu.edu>
References: <s4d1a9f6.087@ohsu.edu>
Message-ID: <1154619098.4739.29.camel@localhost.localdomain>

On Thu, 2006-08-03 at 07:46 -0700, Michael Jerosch-Herold wrote:
> I have data for several rings of a left heart chamber, and which I
> would like to display in concentric rings, with color-encoding of the
> values. Each ring corresponds to one slice through the heart, and the
> rings correspond to positions from the base to the apex of the heart
> as you move from the outermost ring to the innermost one. The data
> have a circular pattern. These types of displays are referred to as
> bullseye displays in the nuclear medicine literature. Does any reader
> of these messages know of a R function/package that offers this
> functionality?
> 
> Also I noticed that in some contexts you can define a "circular"
> attribute for your data. Are there plot routines for such "circular"
> data?
> 
> thank you!
> 
> Michael Jerosch-Herold

You might want to take a look at the 'circular' or 'CircStats' packages
on CRAN:

http://cran.us.r-project.org/src/contrib/Descriptions/circular.html

http://cran.us.r-project.org/src/contrib/Descriptions/CircStats.html

There are some examples of plots generated using the packages in the R
Graphics Gallery here:

http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=121
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=97


HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Thu Aug  3 17:43:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Aug 2006 16:43:06 +0100 (BST)
Subject: [R] question about dll crashing R
In-Reply-To: <Pine.LNX.4.64.0608030736250.20946@homer23.u.washington.edu>
References: <20060803100208.78789.qmail@web61312.mail.yahoo.com>
	<Pine.LNX.4.64.0608030736250.20946@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0608031637330.29143@gannet.stats.ox.ac.uk>

On Thu, 3 Aug 2006, Thomas Lumley wrote:

> On Thu, 3 Aug 2006, Benn Fine wrote:
> 
> > I have ported some R code to C to make it faster.
> >
> > I can perform .Call("foobar",....) once and it
> > works fine. Absolutely correct answer.
> >
> > If I put a loop inside foobar and run the  main code
> > routine more than 100 times, it crashes R.
> >
> > Or if I call .Call("foobar"....) seperately more than
> > two tims it crashes R.
> >
> <snip>
> > SEXP mvntest (SEXP mean, SEXP cov, SEXP temp)
> >
> >
> > { int nrx , ncx , nry , ncy ,info,mode;
> > SEXP xdims , ydims , ans;
> >
> > int i,j, one=1;
> > info = 1;
> >
> > xdims = getAttrib (mean , R_DimSymbol ) ;
> > ydims = getAttrib (cov , R_DimSymbol ) ;
> > mode = REALSXP;
> > nrx = INTEGER( xdims ) [ 0 ] ;
> > ncx = INTEGER( xdims ) [ 1 ] ;
> > nry = INTEGER( ydims ) [ 0 ] ;
> > ncy = INTEGER( ydims ) [ 1 ] ;
> >
> >
> >
> > /* create the upper trianglular matrix A */
> >
> > /* such that t(A) %*% A = Sigma */
> >
> > GetRNGstate();
> >
> > F77_CALL(dpofa) ( REAL( cov ), &nry , &ncy , &info);
> > Rprintf("Info = %d\n",info);
> >
> >
> > for(i=0;i<nry;i++)
> >  for(j=0;j<i;j++)
> >    REAL(cov)[i+j*ncy] = 0.0;
> >
> >
> > PROTECT( ans = allocMatrix (mode, nrx , one ) ) ;
> > for(i=0;i<nry;i++)
> >  REAL(temp)[i] = rnorm(0,1);
> > ans = tmatrixprod(cov,temp);
> 
> ^^^^^^^^^
> Here you are returning a new SEXP from tmatrixprod but not protecting it. 
> Remember, PROTECT() operates on the *value* of its argument, so it 
> protects the thing that ans points to. When ans points to a new thing, it 
> is still the old thing that is protected.

and the old value of 'ans' is never used.

> > for(i=0;i<nry;i++)
> >  REAL(ans)[i] = REAL(ans)[i]+REAL(mean)[i];
> > UNPROTECT( 1 ) ;
> > PutRNGstate();
> > return( ans ) ;

but the next two lines do not do any allocations.  However, PutRNGstate 
does, so you need to UNPROTECT *after* that call.

While we are at it, REAL(ans) is a function call, and you will do better 
to assign the value once outside the loop.

> > I have a feeling I am messing up memory usage
> > somewhere but haven't a clue. Do I need to do garbage
> > collecting inside the C program ?The fact that the
> > code
> > runs a few times before R crashes is driving me nuts.
> > I send most of what I need into the C routine from R,
> > so I am not creating that many SEXP objects within the
> > program.
> >
> > Any hints or ideas ?

See the debugging sections in `Writing R Extensions'.  Using 
gctorture(TRUE) and valgrind (if possible) will find these things faster.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gunter.berton at gene.com  Thu Aug  3 17:56:28 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 3 Aug 2006 08:56:28 -0700
Subject: [R] Looking for transformation to overcome heterogeneity
	ofvariances
In-Reply-To: <x2lkq6ndax.fsf@viggo.kubism.ku.dk>
Message-ID: <002201c6b715$621271d0$711f210a@gne.windows.gene.com>

I know I'm coming late to this, but ...

> > Is someone able to suggest to me a transformation to overcome the
> > problem of heterocedasticity?

It is not usually useful to worry about this. In my experience, the gain in
efficiency from using an essentially ideal weighted analysis vs. an
approximate unweighted one is usually small and unimportant (transformation
to simplify a model is another issue ...). Of far greater importance usually
is the loss in efficiency due to the presence of a few "unusual" extreme
values; have you carefully checked to make sure that none of the large
sample variances you have are due merely to the presence of a small number
of highly discrepant values?


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box


From alessandro at idsia.ch  Thu Aug  3 18:11:04 2006
From: alessandro at idsia.ch (Alessandro Antonucci)
Date: Thu, 3 Aug 2006 18:11:04 +0200
Subject: [R] levels of an array (strings and numbers)
Message-ID: <20060803161104.GA9270@idsia.ch>

Reading a csv file (db.csv) as:

yes,full
no,full
no,empty

I can use the command 'levels' to extract
the different values appearing for each
column as follows:

> d<-read.csv("db.csv")
> levels(d[,2])

which returns

> [1] "empty" "full" 

while, doing the same with a numerical csv file as:

1,6
0,6
0,7

the same instruction returns an empty output instead of 6 7

Any idea about that?

Thanks in advance,
Alessandro

-- 
============================================================
Alessandro Antonucci
Dalle Molle Institute for Artificial Intelligence (IDSIA)
at Idsia			e-mail: alessandro at idsia.ch
Galleria 2			web:   idsia.ch/~alessandro
Via Cantonale			mobile:   +39 339-567-23-28
CH-6928				tel:       +41 58-666-66-70
Manno - Lugano			fax:       +41 58-666-66-61
Switzerland			skype: alessandro.antonucci


From jrkrideau at yahoo.ca  Thu Aug  3 18:09:56 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 3 Aug 2006 12:09:56 -0400 (EDT)
Subject: [R] Vector DF [a,
	b] to replace values :was Finding the position of a variable in a
	data.frame
In-Reply-To: <x2psfinf96.fsf@viggo.kubism.ku.dk>
Message-ID: <20060803160956.59341.qmail@web33810.mail.mud.yahoo.com>


--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> John Kane <jrkrideau at yahoo.ca> writes:
> 
> > --- jim holtman <jholtman at gmail.com> wrote:
> > 
> > > ?which
> > > 
> > > > which(Df >= 50, arr.ind=T)
> > >   row col
> > > 5   5   4
> > 
> > This works very nicely as has some other
> suggestions
> > on how to replace a value. Assuming that I have
> more
> > than one correction to make where Df >= 50, can I
> use
> > vectors in the Df[] to do this.
> > 
> > My attempt shows that I can use vectors but it
> appears
> > thatthere is something wrong with my logic 
> > 
> > Eample
> > 
> > cat <- c( 3,5,6,8,0)
> > dog <- c(3,5,3,6, 0)
> > rat <- c (5, 5, 4, 9, 51)
> > bat <- c( 12, 42, 45, 32, 54)
> > 
> > Df <- data.frame(cbind(cat, dog, rat, bat))
> > post <- which(Df >= 50, arr.ind=T)  # find values
> .=
> > 50
> > post
> > correction <- c(77, 88)  # new correct values
> > row <- post[ ,1]      # vector for row number
> > col <- post[ ,2]       # vector for column number
> > 
> > Df[row,col] <-  correction
> > Df
> > 
> > -------result---------
> >   cat dog rat bat
> > 1   3   3   5  12
> > 2   5   5   5  42
> > 3   6   3   4  45
> > 4   8   6   9  32
> > 5   0   0  88  88
> > 
> > I am replacing both instances with 88 which is not
> > correct
> 
> You're being bitten by two issues here: One is that
> Df[row,col] does
> not vectorize in parallel in the two indices:
> Df[1:2,1:2] has *four*
> elements, not two. 

It took a moment to see it but it's rather obvious
now. I'm still fuzzy on the differences in behaviour
between a data frame and a matrix.  


>  Another is that data frames are
> not matrices: you
> can have different types of values in different
> columns, so you cannot
> expect to pick an arbitrary set of elements and
> assign a vector into
> it (well, it doesn't work, anyway).
> 
> This sort of stuff works much easier with matrices,
> where we also have
> the wonderful feature of indexing with a matrix:
> 
> > M <- cbind(cat, dog, rat, bat)
> > M[post]<- correction
> > M
>      cat dog rat bat
> [1,]   3   3   5  12
> [2,]   5   5   5  42
> [3,]   6   3   4  45
> [4,]   8   6   9  32
> [5,]   0   0  77  88

Which of course I, stupidly  thought that I was doing
in the data frame.

Thank you very much.


From tlumley at u.washington.edu  Thu Aug  3 18:53:07 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Aug 2006 09:53:07 -0700 (PDT)
Subject: [R] read.spss and umlaut
In-Reply-To: <200608031038.14141.r@fam-kuster.ch>
References: <200608021531.56340.r@fam-kuster.ch>
	<Pine.LNX.4.64.0608020752400.2107@homer23.u.washington.edu>
	<200608031038.14141.r@fam-kuster.ch>
Message-ID: <Pine.LNX.4.64.0608030929560.27187@homer23.u.washington.edu>


I have gone and looked at the code for reading SPSS portable files, and 
the file format appears to specify that you cannot read many legal 
characters.

Part of the header information in the file format is a 256-byte 
translation table apparently designed for translating between character 
representations.  It can mark characters as "untranslateable", and the 
code for reading character strings replaces untranslateable characters 
with NULs.

In the example file in the foreign package the only translatable 
characters are the ASCII alphanumeric characters and 
.<(+0&[]!$*);^-/|,%_>?`:#@'="~{}\

So, it looks as though your SPSS portable file may be marking character 
code FC as untranslatable.  This is easy to check -- look at the start of 
the file and find the sequence ABCDEF..., which is in the middle of the 
translation table. See if u-umlaut is in the table.  It might even work to 
modify the translation table to allow the accented characters.

It looks as though SPSS .sav files don't have this limitation.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From dmauldin at fhcrc.org  Thu Aug  3 19:09:13 2006
From: dmauldin at fhcrc.org (Denise Mauldin)
Date: Thu, 3 Aug 2006 10:09:13 -0700 (PDT)
Subject: [R] Help Building packages for windows
Message-ID: <Pine.LNX.4.61.0608030953430.32097@jade.fhcrc.org>


Hello all,

I've tried to build my linux source package with the following commands 
and then run it through the RCrossBuild package with R 2.3.1, but I get 
errors about how the built field is incorrect and that my Rd file has a 
non-empty \name.  I've looked through the Rd file and it seems to not have 
any empty \name values, but I'm not sure how to tell if any are 'missing'.  
The built field is: Built: R 2.3.1; ; 2006-08-02 21:42:59; unix

R CMD INSTALL --build pkgName
R CMD INSTALL --build --use-zip pkgName
R CMD BUILD --use-zip pkgName
R CMD BUILD pkgName

An RCHECK on the package gives back the warning that subdirectory data 
contains no data sets, despite the presence of Rdata.rdb, Rdata.rds, and 
Rdata.rdx in the directory.  This package is an annotation package for an 
array and can be used fine in *nix R.  If I try to install the package in 
R for windows it gives me a bunch of errors with package names that look 
like random strings of characters.

Any help would be appreciated.

Thanks,
Denise

> sessionInfo()
Version 2.3.1 (2006-06-01)
i686-pc-linux-gnu
 
attached base packages: 
[1] "methods"  "stats"  "graphics"  "grDevices" "utils"  "datasets"
[7] "base"
 
other attached packages: 
pkgName
   "2.3"

Linux 2.4.20-31.9 #1 Tue Apr 13 17:38:16 EDT 2004 i686 athlon i386 GNU/Linux


From ligges at statistik.uni-dortmund.de  Thu Aug  3 19:18:29 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 03 Aug 2006 19:18:29 +0200
Subject: [R] levels of an array (strings and numbers)
In-Reply-To: <20060803161104.GA9270@idsia.ch>
References: <20060803161104.GA9270@idsia.ch>
Message-ID: <44D22FE5.5080503@statistik.uni-dortmund.de>



Alessandro Antonucci wrote:
> Reading a csv file (db.csv) as:
> 
> yes,full
> no,full
> no,empty
> 
> I can use the command 'levels' to extract
> the different values appearing for each
> column as follows:
> 
>> d<-read.csv("db.csv")
>> levels(d[,2])
> 
> which returns
> 
>> [1] "empty" "full" 
> 
> while, doing the same with a numerical csv file as:
> 
> 1,6
> 0,6
> 0,7

R (i.e. read.csv) assumes this is numeric rather than categorical data.
Hence specify the class of each variable in read.cvs' argument "colClasses".

Uwe Ligges



> the same instruction returns an empty output instead of 6 7
> 
> Any idea about that?
> 
> Thanks in advance,
> Alessandro
>


From spencer.graves at pdf.com  Thu Aug  3 19:50:45 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Aug 2006 10:50:45 -0700
Subject: [R] DOE in R
In-Reply-To: <44CDD99B.24295.80EA80@localhost>
References: <44CDD99B.24295.80EA80@localhost>
Message-ID: <44D23775.80703@pdf.com>

<see in line>

Petr Pikal wrote:
> See
> 
> ?aov
> ?lm
> 
> something like
>  
> lm(result~yourfactor1+yourfactor2+yourfactor3+yourfactor5, 
> data=yourdataframe)
>
or

lm(result~(yourfactor1+yourfactor2+yourfactor3+yourfactor5)^2,
data=yourdataframe)

if you want all interactions.  Make sure all "yourfactors" are of class 
"factor".

	  Hope this helps.
	  Spencer Graves

  or
> aov(result~yourfactor1+yourfactor2+yourfactor3+yourfactor5, 
> data=yourdataframe)
> 
> but the exact structure of lm or aov construction depends on what you 
> want to test.
> 
> HTH
> Petr
> 
> 
> On 28 Jul 2006 at 21:43, dvrecko at sfu.ca wrote:
> 
> Date sent:      	Fri, 28 Jul 2006 21:43:38 -0700
> From:           	dvrecko at sfu.ca
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] DOE in R
> Send reply to:  	dvrecko at sfu.ca
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
> 
>> Hi.
>>
>> I'm a student in a graduate program at Simon Fraser University in
>> Canada.
>>
>> I am trying to run a simple screening experiment with some simulated
>> data.
>>
>> I simply want to do an ANOVA of an experiemnt with 5 factors (4 have 2
>> levels, the last has 3 levels) and 48 runs (ie, full factorial).
>>
>> The thing is that I have multiple observations for each level
>> combination (run).
>>
>> So,
>>
>> 1) How do I do the anova based on the setup above?
>>
>> and 
>>
>> 2) More importantly, because of convergence issues for my simulations,
>> I will likely have an unequal number of observations for the 48 runs.
>> How can I do this?
>>
>> Seems like a straightforward enough situation.
>>
>> I am trying to avaoid writing my own C code to do the analysis since I
>> am working under some pretty tight time constraints.
>>
>> ANy help would be appreciated.
>>
>> Thanks very much.
>>
>> Dean Vrecko
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jsorkin at grecc.umaryland.edu  Thu Aug  3 19:51:18 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 03 Aug 2006 13:51:18 -0400
Subject: [R] Looking for transformation to overcome
	heterogeneity	ofvariances
Message-ID: <s4d1ff67.052@MEDICINE.umaryland.edu>

Peter
You question is difficult to answer without more information about the
distribution of your residuals. Different residual patterns call for
different transformations to stabilize the variance. One very common
form of  heterocedasticity is increasing variance with increasing values
of an independent predictor, i.e. the variance of the residuals of y=x
increase as x increases. In this case a log transformation of some, or
all, of the independent variables of the helps. Please also note the
comment by Bert Gunter (included below) in which some important points
are raised, particularly about extreme values. 

If you want more help, please describe the pattern of your residuals. 


John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

>>> Berton Gunter <gunter.berton at gene.com> 8/3/2006 11:56:28 AM >>>
I know I'm coming late to this, but ...

> > Is someone able to suggest to me a transformation to overcome the
> > problem of heterocedasticity?

It is not usually useful to worry about this. In my experience, the
gain in
efficiency from using an essentially ideal weighted analysis vs. an
approximate unweighted one is usually small and unimportant
(transformation
to simplify a model is another issue ...). Of far greater importance
usually
is the loss in efficiency due to the presence of a few "unusual"
extreme
values; have you carefully checked to make sure that none of the large
sample variances you have are due merely to the presence of a small
number
of highly discrepant values?


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific
learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From fs at fs-analyse.dk  Thu Aug  3 20:04:25 2006
From: fs at fs-analyse.dk (=?ISO-8859-1?Q?Finn_Sand=F8?=)
Date: Thu, 03 Aug 2006 20:04:25 +0200
Subject: [R] read.spss  'error reading system-file header'
In-Reply-To: <44CF34B8.7000000@web.de>
References: <44CE57B3.9090901@fs-analyse.dk> <44CF34B8.7000000@web.de>
Message-ID: <44D23AA9.1040902@fs-analyse.dk>

Thank you for your answer.
I did try this but got similar errors. I tried all the other 
spss-specific formats with same result.

A way to get access to the data  is: save in sas-xport format (though 
all labels are lost) then save in another sas-format and asking spss to 
also save value labels in a sas-format file which can then be edited to 
extract both value labels and variable labels.
These methods are quite laborious and quite complicated, though, and you 
need access to spss.

Ulrich Keller wrote:
> Question 2: Try saving the data as an SPSS portable file. I never had 
> trouble reading these in R.


From fs at fs-analyse.dk  Thu Aug  3 20:04:35 2006
From: fs at fs-analyse.dk (=?ISO-8859-1?Q?Finn_Sand=F8?=)
Date: Thu, 03 Aug 2006 20:04:35 +0200
Subject: [R] read.spss 'error reading system-file header'
In-Reply-To: <s4d1d5f2.094@health.qld.gov.au>
References: <s4d1d5f2.094@health.qld.gov.au>
Message-ID: <44D23AB3.607@fs-analyse.dk>

Thank you Michael
I think you point to the real cause of the problem.
I solved my own immediate problem by using StatTransfer to transfer from 
sav to sav.

My real concern is that the read.spss() function will become obsolete. 
Most of the data I have received in the last year have had those 
problems (and increasingly so), now they have become so serious that the 
import process fails all together.
Therefore I believe it is important to do something about



Michael Bibo wrote:
> I  don't know if this is relevant in your particular case, but the error
> messages you quote are precisely what you get if the SPSS .sav file has
> been created with the SPSS Data Entry product.  If this is the case, it
> is covered by section 3.1 of the R Data Import/Export document.
>
> If that is the problem, and if you don't have access to SPSS Data
> Entry, I could do a data export for you.  If I can help, please respond
> on both my work email (below) and mbibo at aanet.com.au, as I'm not sure
> which one I would see first.
>
>
>
>
>
>
>
> Michael Bibo
> Research Officer
> Projects and Research Service
> West Moreton Community Health Services
> West Moreton Health Service District
> Queensland Health
>
> Michael_Bibo at health.qld.gov.au
>
> Ph. +61 7 3817 2400
>
> P.O. Box 878
> Ipswich,  4305
> Australia
>
> This e-mail, including any attachments sent with it, is confidential 
> and for the sole use of the intended recipient(s). This confidentiality
>
> is not waived or lost if you receive it and you are not the intended 
> recipient(s), or if it is transmitted/ received in error.  
>  
> Any unauthorised use, alteration, disclosure, distribution or review 
> of this e-mail is prohibited.  It may be subject to a statutory duty of
>
> confidentiality if it relates to health service matters.
>  
> If you are not the intended recipient(s), or if you have received this
>
> e-mail in error, you are asked to immediately notify the sender by 
> telephone or by return e-mail.  You should also delete this e-mail 
> message and destroy any hard copies produced.
>
>
> *****************************************************************
> This email, including any attachments sent with it, is
> confidential and for the sole use of the intended recipient(s).
> This confidentiality is not waived or lost, if you receive it and
> you are not the intended recipient(s), or if it is transmitted/
> received in error.
>
> Any unauthorised use, alteration, disclosure, distribution or
> review of this email is strictly prohibited.  The information
> contained in this email, including any attachment sent with
> it, may be subject to a statutory duty of confidentiality if it
> relates to health service matters.
>
> If you are not the intended recipient(s), or if you have
> received this email in error, you are asked to immediately
> notify the sender by telephone collect on Australia
> +61 1800 198 175 or by return email.  You should also
> delete this email, and any copies, from your computer
> system network and destroy any hard copies produced.
>
> If not an intended recipient of this email, you must not copy,
> distribute or take any action(s) that relies on it; any form of
> disclosure, modification, distribution and/or publication of this
> email is also prohibited.
>
> Although Queensland Health takes all reasonable steps to
> ensure this email does not contain malicious software,
> Queensland Health does not accept responsibility for the
> consequences if any person's computer inadvertently suffers
> any disruption to services, loss of information, harm or is
> infected with a virus, other malicious computer programme or
> code that may occur as a consequence of receiving this
> email.
>
> Unless stated otherwise, this email represents only the views
> of the sender and not the views of the Queensland Government.
> ****************************************************************
>
>
>


From mike_saunders at umenfa.maine.edu  Thu Aug  3 20:22:33 2006
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Thu, 3 Aug 2006 14:22:33 -0400
Subject: [R] gsummary
Message-ID: <000801c6b729$c9d2b7d0$b5a76f82@CFRU0204>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/00a06517/attachment.pl 

From fjohannes at fastmail.fm  Thu Aug  3 20:23:24 2006
From: fjohannes at fastmail.fm (Frank Johannes)
Date: Thu, 03 Aug 2006 11:23:24 -0700
Subject: [R] fitting a model with the nlme package
Message-ID: <1154629404.30793.267578388@webmail.messagingengine.com>

Dear all,
I am analyzing some data that requires a mixed model. I have been
reading Pinheiro and Bates' book,
but cannot find the notation to fit the following model:

Suppose I have the dataset below. Here I am fitting variable p as a
fixed effect, variable h
as a random effect and variable t as nested within h.  I would like to
include the variable j as well
as an independent (non-nested) random effect. I can't find the notation
in the book to tell R
to do this. Does anybody know? 

library(nlme)

h<-c(1,1,1,2,2,2,3,3,3)
j<-c(2,3,8,3,4,3,9,5,4)
t<-c(1,2,3,1,2,3,1,2,3)
f<-c(1,2,1,2,1,2,1,2,1)
p<-c(45,32,35,12,23,12,2,9,12)
set<-data.frame(p,h,j,t)

out<-lme(p~ -1 + f,data=set, random=~1|h/t)

Thanks a alot,
frank.

-- 

                          unladen european swallow


From bates at stat.wisc.edu  Thu Aug  3 20:43:12 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 3 Aug 2006 13:43:12 -0500
Subject: [R] gsummary
In-Reply-To: <000801c6b729$c9d2b7d0$b5a76f82@CFRU0204>
References: <000801c6b729$c9d2b7d0$b5a76f82@CFRU0204>
Message-ID: <40e66e0b0608031143k9754026v2571c02e243c6ef7@mail.gmail.com>

The gsummary function was intended to apply a summary function to each
column in each of the groups of observations.  The (implicit)
definition of a summary function is that it produces a scalar from a
vector.  (See the groupings of functions in Table A.1, p. 472 of
"Statistical Models in S", a.k.a. "The White Book".)

I don't think you need to use gsummary for your application.  You may
be able to use tapply or a combination of split and lapply instead.


On 8/3/06, Mike Saunders <mike_saunders at umenfa.maine.edu> wrote:
> Could someone give me a hand with the format of the gsummary function?  Basically, I have a large set of xyz coordinates generated by LiDAR data (>37 million points) and I am trying to derive various summary statistics on the z-coordinates by a grid cell.  I wrote a function to do this by creating factors from the x- and y- coordinates and then using gsummary.  However, I want the function to calculate BOTH the max z-value in a grid cell and the number of z-values in that cell.  gsummary is supposed to be able to use a list a functions as input to FUN, but I am having trouble coding it correctly.  Here is what I came up to so far.
>
> gr<-gsummary(xyz.packet,FUN=list(numeric=max,numeric=length),omitGroupingFactor=T,form=~X/Y)
>
> The help for the function gives no examples using multiple functions as a FUN value, so the help pages haven't been overly useful in this case.
>
> Thanks in advance,
> Mike
>
> Mike R. Saunders
> Forest Biometrician
> Cooperative Forest Research Unit
> University of Maine
> 5755 Nutting Hall
> Orono, ME  04469-5755
>
> 207-581-2763 (O)
> 207-581-2833 (F)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Thu Aug  3 20:43:58 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Aug 2006 20:43:58 +0200
Subject: [R] Looking for transformation to overcome heterogeneity
	ofvariances
In-Reply-To: <s4d1ff67.048@MEDICINE.umaryland.edu>
References: <s4d1ff67.048@MEDICINE.umaryland.edu>
Message-ID: <x2vep9oe1d.fsf@turmalin.kubism.ku.dk>

[Resending -- recipient list length issue]

"John Sorkin" <jsorkin at grecc.umaryland.edu> writes:

> Peter

Erm, that was Paul's question, not mine! If you want to help, please
look at the pattern of residuals which he put up on the web on my
request.... 

> You question is difficult to answer without more information about the
> distribution of your residuals. Different residual patterns call for
> different transformations to stabilize the variance. One very common
> form of  heterocedasticity is increasing variance with increasing values
> of an independent predictor, i.e. the variance of the residuals of y=x
> increase as x increases. In this case a log transformation of some, or
> all, of the independent variables of the helps. Please also note the
> comment by Bert Gunter (included below) in which some important points
> are raised, particularly about extreme values. 
> 
> If you want more help, please describe the pattern of your residuals. 
> 
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
> 
> >>> Berton Gunter <gunter.berton at gene.com> 8/3/2006 11:56:28 AM >>>
> I know I'm coming late to this, but ...
> 
> > > Is someone able to suggest to me a transformation to overcome the
> > > problem of heterocedasticity?
> 
> It is not usually useful to worry about this. In my experience, the
> gain in
> efficiency from using an essentially ideal weighted analysis vs. an
> approximate unweighted one is usually small and unimportant
> (transformation
> to simplify a model is another issue ...). Of far greater importance
> usually
> is the loss in efficiency due to the presence of a few "unusual"
> extreme
> values; have you carefully checked to make sure that none of the large
> sample variances you have are due merely to the presence of a small
> number
> of highly discrepant values?
> 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific
> learning
> process."  - George E. P. Box
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From neil.a.mcleod at gmail.com  Thu Aug  3 20:44:49 2006
From: neil.a.mcleod at gmail.com (Neil McLeod)
Date: Thu, 3 Aug 2006 14:44:49 -0400
Subject: [R] How to access a column by its label?
Message-ID: <2c4b23d60608031144g565add66t692935678b24194a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/4d6b7dde/attachment.pl 

From vincent at 7d4.com  Thu Aug  3 20:51:33 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Thu, 03 Aug 2006 20:51:33 +0200
Subject: [R] How to access a column by its label?
In-Reply-To: <2c4b23d60608031144g565add66t692935678b24194a@mail.gmail.com>
References: <2c4b23d60608031144g565add66t692935678b24194a@mail.gmail.com>
Message-ID: <44D245B5.2090705@7d4.com>

?colnames
hih


From tlumley at u.washington.edu  Thu Aug  3 20:52:32 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Aug 2006 11:52:32 -0700 (PDT)
Subject: [R] read.spss 'error reading system-file header'
In-Reply-To: <44D23AB3.607@fs-analyse.dk>
References: <s4d1d5f2.094@health.qld.gov.au> <44D23AB3.607@fs-analyse.dk>
Message-ID: <Pine.LNX.4.64.0608031120030.27187@homer23.u.washington.edu>

On Thu, 3 Aug 2006, Finn Sand? wrote:
> My real concern is that the read.spss() function will become obsolete.
> Most of the data I have received in the last year have had those
> problems (and increasingly so), now they have become so serious that the
> import process fails all together.
> Therefore I believe it is important to do something about
>

Well, we would be happy if someone did something about it.  It still reads 
all the files it used to read (which includes all the SPSS files I have 
ever encountered in my work).

As has been pointed out several times, PSPP now has newer code to read 
SPSS files than the code in the foreign package, and that could be 
adapted.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From bates at stat.wisc.edu  Thu Aug  3 20:54:33 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 3 Aug 2006 13:54:33 -0500
Subject: [R] fitting a model with the nlme package
In-Reply-To: <1154629404.30793.267578388@webmail.messagingengine.com>
References: <1154629404.30793.267578388@webmail.messagingengine.com>
Message-ID: <40e66e0b0608031154m71050f95p33e7f0d1e3d7d9fd@mail.gmail.com>

On 8/3/06, Frank Johannes <fjohannes at fastmail.fm> wrote:
> Dear all,
> I am analyzing some data that requires a mixed model. I have been
> reading Pinheiro and Bates' book,
> but cannot find the notation to fit the following model:
>
> Suppose I have the dataset below. Here I am fitting variable p as a
> fixed effect, variable h
> as a random effect and variable t as nested within h.  I would like to
> include the variable j as well
> as an independent (non-nested) random effect. I can't find the notation
> in the book to tell R
> to do this. Does anybody know?
>
> library(nlme)
>
> h<-c(1,1,1,2,2,2,3,3,3)
> j<-c(2,3,8,3,4,3,9,5,4)
> t<-c(1,2,3,1,2,3,1,2,3)
> f<-c(1,2,1,2,1,2,1,2,1)
> p<-c(45,32,35,12,23,12,2,9,12)
> set<-data.frame(p,h,j,t)
>
> out<-lme(p~ -1 + f,data=set, random=~1|h/t)

It is not easy to fit mixed models with non-nested grouping factors
for the random effects using lme.  I would recommend using lmer from
the lme4/Matrix package instead.  Even with lmer you can't fit the
model you want to fit because you have 9 levels of the interaction h:t
and only 9 observations.  The version of lmer in the
soon-to-be-released Matrix_0.995-13 even produces a warning about
this.

> library(lme4)
Loading required package: Matrix
Loading required package: lattice
Loading required package: lattice
> set <- data.frame(h = factor(c(1,1,1,2,2,2,3,3,3)),
+                   j = factor(c(2,3,8,3,4,3,9,5,4)),
+                   t = factor(c(1,2,3,1,2,3,1,2,3)),
+                   f = c(1,2,1,2,1,2,1,2,1),
+                   p = c(45,32,35,12,23,12,2,9,12))
> (out <- lmer(p ~ f - 1 + (1|h/t) + (1|j), set))
Linear mixed-effects model fit by REML
Formula: p ~ f - 1 + (1 | h/t) + (1 | j)
   Data: set
      AIC      BIC    logLik MLdeviance REMLdeviance
 39.21916 40.00806 -15.60958   36.17505     31.21916
Random effects:
 Groups   Name        Variance   Std.Dev.
 t:h      (Intercept) 1.7053e-22 1.3059e-11
 j        (Intercept) 1.0067e+02 1.0033e+01
 h        (Intercept) 1.2655e+02 1.1249e+01
 Residual             3.4106e-13 5.8400e-07
number of obs: 9, groups: t:h, 9; j, 6; h, 3

Fixed effects:
  Estimate Std. Error t value
f   12.000      4.899  2.4495
Warning messages:
1: Estimated variance for group(s) t:h is zero in:
"LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance =
1.49011611938477e-08,
2: nlminb returned message singular convergence (7)
 in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance =
1.49011611938477e-08,


From mschwartz at mn.rr.com  Thu Aug  3 21:00:01 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 03 Aug 2006 14:00:01 -0500
Subject: [R] How to access a column by its label?
In-Reply-To: <2c4b23d60608031144g565add66t692935678b24194a@mail.gmail.com>
References: <2c4b23d60608031144g565add66t692935678b24194a@mail.gmail.com>
Message-ID: <1154631601.4739.40.camel@localhost.localdomain>

On Thu, 2006-08-03 at 14:44 -0400, Neil McLeod wrote:
> Hi all,
> 
> Is there any way to access a column of a data frame by its label (title)
> rather than its column index? For example, I'd like to be able to select
> animals[,"weight"] rather than animals[,3], if the third column of the
> "animals" data frame has the label "weight".
> 
> Thank you!

You answered your own question...animals[,"weight"]

You can also do:

animals$weight

or

animals[["weight"]]

or 

subset(animals, select = weight)


See ?Extract and ?subset for more information.

HTH,

Marc Schwartz


From s-walker at ti.com  Thu Aug  3 21:11:00 2006
From: s-walker at ti.com (Walker, Sam)
Date: Thu, 3 Aug 2006 14:11:00 -0500
Subject: [R] ggplot facet label font size
Message-ID: <1F5C95F1B887EF42B14F31AEF36AD22F060661C1@dlee04.ent.ti.com>


This works OK, but there is some extra spacing between the panels, the
top axis and the strip on the top, and the left labels and panel.

How can I remove these extra spaces?

I've tried changing various layout.widths settings with no luck.  It
seems the spaces are calculated based on the number of conditioning
variables, in this case 2 (sex+smoker).


Thanks in advance...
-Sam 


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, August 02, 2006 6:04 PM
To: Walker, Sam
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] ggplot facet label font size

On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> How do I change the font size in the facet labels along the edges of
the
> plot?
>
> For example (from the ggplot help file):
>     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
>     gghistogram(p)
>
> In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> Female", "sex: Male".  What command can I use to reduce the font size
of
> these labels?
>
> In lattice terminology, cex is used to scale these strip labels.  But
I
> couldn't find the equivalent in ggplot.
>
> The reason I'm asking is I have a 9x7 array of plots which I've been
> plotting with lattice.  I wanted to use ggplot because I like having
the
> labels on the edge of the plots

Note that lattice can do that by using custom strip functions:

library(ggplot) # data resides here
library(lattice)

my.strip <- function(which.given, which.panel, ...)
   if (which.given == 1 && which.panel[2] == 2)
      strip.default(which.given, which.panel, ...)

my.strip.left <- function(which.given, which.panel, ..., horizontal)
   if (which.given == 2 && which.panel[1] == 1)
      strip.default(which.given, which.panel, horizontal = FALSE, ...)

histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
     strip.left = my.strip.left, par.settings = list(add.text =
list(cex = 0.7)))


From brian_cade at usgs.gov  Thu Aug  3 21:11:26 2006
From: brian_cade at usgs.gov (Brian S Cade)
Date: Thu, 3 Aug 2006 13:11:26 -0600
Subject: [R] Looking for transformation to overcome heterogeneity
	of	variances
In-Reply-To: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
Message-ID: <OF954B12A1.08D9CFCE-ON872571BF.00684FA5-872571BF.0069BC4D@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/731892a5/attachment.pl 

From Qinghong.Li at rdmo.nestle.com  Thu Aug  3 22:02:22 2006
From: Qinghong.Li at rdmo.nestle.com (Li,Qinghong,ST.LOUIS,Molecular Biology)
Date: Thu, 3 Aug 2006 15:02:22 -0500
Subject: [R] meta characters in file path
Message-ID: <2BA2B7291D5DC6409FD53CB7C01F0D990183B88E@usslre00.nestle.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/58c67132/attachment.pl 

From tplate at acm.org  Thu Aug  3 22:41:55 2006
From: tplate at acm.org (Tony Plate)
Date: Thu, 03 Aug 2006 14:41:55 -0600
Subject: [R] meta characters in file path
In-Reply-To: <2BA2B7291D5DC6409FD53CB7C01F0D990183B88E@usslre00.nestle.com>
References: <2BA2B7291D5DC6409FD53CB7C01F0D990183B88E@usslre00.nestle.com>
Message-ID: <44D25F93.6090707@acm.org>

What is the problem you are having?  Seems to work fine for me running 
under Windows2000:

 > write.table(data.frame(a=1:3,b=4:6), file="@# x.csv", sep=",")
 > read.csv(file="@# x.csv")
   a b
1 1 4
2 2 5
3 3 6
 > sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      XML
"0.99-8"
 >

Li,Qinghong,ST.LOUIS,Molecular Biology wrote:
> Hi,
> 
> I need to read in some files. The file names contain come meta characters such as @, #, and white spaces etc, In read.csv, file= option, is there any way that one can make the function to recognize a file path with those characters?
> 
> Thanks
> Johnny
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From HDoran at air.org  Thu Aug  3 22:46:14 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 3 Aug 2006 16:46:14 -0400
Subject: [R] fitting a model with the nlme package
Message-ID: <2323A6D37908A847A7C32F1E3662C80E276BBC@dc1ex01.air.org>

 
 > (out <- lmer(p ~ f - 1 + (1|h/t) + (1|j), set))

Doug:

It seems the nesting syntax is handled a bit differently. Is (1|h/t)
equivalent to the old lme nesting syntax?

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From goran.brostrom at gmail.com  Thu Aug  3 23:04:43 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Thu, 3 Aug 2006 23:04:43 +0200
Subject: [R] Weibull distribution
In-Reply-To: <Pine.LNX.4.64.0607210831420.17202@homer21.u.washington.edu>
References: <20060721150307.87753.qmail@web30806.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0607210831420.17202@homer21.u.washington.edu>
Message-ID: <148ed8180608031404o1529f2d8pc311afd2a46e5e59@mail.gmail.com>

On 7/21/06, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Fri, 21 Jul 2006, Valentin Dimitrov wrote:
>
> > Dear Leaf,
> >
> > I modified your code as follows:
> >
> > gamma.fun <- function(mu,sd,start=100)
> > {
> > f.fn <- function(alpha)
> > {abs(sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/alpha)-(gamma(1+1/alpha))^2))}
> > alpha <- optim(start, f.fn)
> > beta <- mu/gamma(1+1/alpha$par)
> > return(list=c(a=alpha$par,b=beta));
> > }
> >
> > Now it works properly.
> >
> > First, I added an abs(). You tried to solve an
> > equation by means of the R-function optim(), which
> > finds a minimum. That's why you can find the solution
> > of f(x)=a through minimization of abs(f(x)-a).
> > Second, I deleted the optim-method BFGS from the
> > optim() function, because it is not appropriate in
> > this case.
>
> optim() is not appropriate at all in this case -- its help page says to
> use optimize() for one-dimensional problems.

Just to clarify: The help page says:

'optim' will work with one-dimensional 'par's, but the default
method does not work well (and will warn).  Use 'optimize' instead.

In other words, if you for instance use the 'BFGS' method, optim is
perfectly OK for one-dimensional problems.
>
> In fact, in one dimension there isn't any need to resort to optimization
> when you really want root-finding, and uniroot() is more appropriate than
> optimize().

One reason to use optim instead of uniroot or optimize is that you
need not specify a finite interval that covers the solution.

G?ran


From stefano.iacus at unimi.it  Thu Aug  3 23:39:41 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Thu, 3 Aug 2006 23:39:41 +0200
Subject: [R]  geodesic distance (solved)
Message-ID: <865DB46B-895B-4D06-A4A9-F5E3D1104CD0@unimi.it>

I found the answer to my problem.
stefano


From h.wickham at gmail.com  Thu Aug  3 23:43:46 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 3 Aug 2006 16:43:46 -0500
Subject: [R] ggplot facet label font size
In-Reply-To: <1F5C95F1B887EF42B14F31AEF36AD22F05FEB78F@dlee04.ent.ti.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F05FEB78F@dlee04.ent.ti.com>
Message-ID: <f8e6ff050608031443i20e6521fic478b103179f9133@mail.gmail.com>

Hi Sam,

> How do I change the font size in the facet labels along the edges of the
> plot?

Unfortunately, you can't currently change the size of those fonts.
However, it is on my todo list (as well as completely custom strip
functions) and should be available in the near future.

One thing you could do is have a look at ggopt, where you can at least
change the strip text, if not the size.

Regards,

Hadley

On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
>
> For example (from the ggplot help file):
>      p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
>      gghistogram(p)
>
> In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> Female", "sex: Male".  What command can I use to reduce the font size of
> these labels?
>
> In lattice terminology, cex is used to scale these strip labels.  But I
> couldn't find the equivalent in ggplot.
>
> The reason I'm asking is I have a 9x7 array of plots which I've been
> plotting with lattice.  I wanted to use ggplot because I like having the
> labels on the edge of the plots, but the label font size is too large
> and exceeding the size of the label box.
>
> Thanks in advance...
> -Sam
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From macq at llnl.gov  Fri Aug  4 00:24:02 2006
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 3 Aug 2006 15:24:02 -0700
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
References: <20060802210153.46892.qmail@web33806.mail.mud.yahoo.com>
Message-ID: <p0623090ec0f82713ad02@[128.115.153.6]>

You don't need to find out the column index. This works:

    Df[5,'bat'] <- 100

-Don

At 5:01 PM -0400 8/2/06, John Kane wrote:
>Simple problem but I don't see the answer. I'm trying
>to clean up some data
>I have 120 columns in a data.frame.  I have one value
>in a column named "blaw" that I want to change. How do
>I find the coordinates. I can find the row by doing a
>subset on the data.frame but how do I find out here
>"blaw " is in columns without manually counting them
>or converting names(Df) to a list and reading down the
>list.
>
>Simple example
>
>cat <- c( 3,5,6,8,0)
>dog <- c(3,5,3,6, 0)
>rat <- c (5, 5, 4, 9, 0)
>bat <- c( 12, 42, 45, 32, 54)
>
>Df <- data.frame(cbind(cat, dog, rat, bat))
>Df
>subset(Df, bat >= 50)
>
>----results
>   cat dog rat bat
>5   0   0   0  54
>
>
>Thus I know that my target is in row 5 but how do I
>figure out where 'bat' is? 
>
>All I want to do is be able to say
>Df[5,4] <- 100
>
>Is there some way to have function(bat) return the
>column number: some kind of a colnum() function?  I
>had thought that I had found somthing  in
>library(gdata) matchcols but no luck.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From john_d_mchenry at yahoo.com  Fri Aug  4 00:25:08 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Thu, 3 Aug 2006 15:25:08 -0700 (PDT)
Subject: [R] Tcltk package
In-Reply-To: <200608031421.35804.adi@roda.ro>
Message-ID: <20060803222508.95452.qmail@web35406.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/f78aa637/attachment.pl 

From jrkrideau at yahoo.ca  Fri Aug  4 00:38:51 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 3 Aug 2006 18:38:51 -0400 (EDT)
Subject: [R] Finding the position of a  variable in a data.frame
In-Reply-To: <p0623090ec0f82713ad02@[128.115.153.6]>
Message-ID: <20060803223851.45208.qmail@web33813.mail.mud.yahoo.com>


--- Don MacQueen <macq at llnl.gov> wrote:

> You don't need to find out the column index. This
> works:
> 
>     Df[5,'bat'] <- 100
> 
> -Don
> 

Thanks, I'd tried 
Df[5, bat] <- 100  :(

I never thought of the ' ' being needed.



> At 5:01 PM -0400 8/2/06, John Kane wrote:
> >Simple problem but I don't see the answer. I'm
> trying
> >to clean up some data
> >I have 120 columns in a data.frame.  I have one
> value
> >in a column named "blaw" that I want to change. How
> do
> >I find the coordinates. I can find the row by doing
> a
> >subset on the data.frame but how do I find out here
> >"blaw " is in columns without manually counting
> them
> >or converting names(Df) to a list and reading down
> the
> >list.
> >
> >Simple example
> >
> >cat <- c( 3,5,6,8,0)
> >dog <- c(3,5,3,6, 0)
> >rat <- c (5, 5, 4, 9, 0)
> >bat <- c( 12, 42, 45, 32, 54)
> >
> >Df <- data.frame(cbind(cat, dog, rat, bat))
> >Df
> >subset(Df, bat >= 50)
> >
> >----results
> >   cat dog rat bat
> >5   0   0   0  54
> >
> >
> >Thus I know that my target is in row 5 but how do I
> >figure out where 'bat' is? 
> >
> >All I want to do is be able to say
> >Df[5,4] <- 100
> >
> >Is there some way to have function(bat) return the
> >column number: some kind of a colnum() function?  I
> >had thought that I had found somthing  in
> >library(gdata) matchcols but no luck.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained,
> reproducible code.
> 
> 
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> --------------------------------------
>


From mmalten at gmail.com  Fri Aug  4 01:22:37 2006
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Thu, 3 Aug 2006 19:22:37 -0400
Subject: [R] Ambitious newbie with some ongoing Q's
Message-ID: <8913fde30608031622p27a510c8hf33b98e07ea05658@mail.gmail.com>

I'm new to the list and I've been playing about with R for some months
now, mostly using the power analysis routines including the "pwr"
package.

I'm currently looking at a project which will require a
repeated-measures MANCOVA.

I've been reviewing the files available at CRAN, including
http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_repms.html
and http://tolstoy.newcastle.edu.au/R/help/03b/7663.html which
describe repeated-measures ANOVA

Is making this a repeated-measures MANCOVA as simple as adding a
covariate to the fixed model and then making the Y and covariate
variables matrices?

If not, how do I do it?  Can I do it?

And if I can do it, can I also do a power analysis?  (OK, I'm a greedy
little newbie...)

Thanks in advance.



-- 
I can answer any question.
"I don't know" is an answer.
"I don't know yet" is a better answer.


From spencer.graves at pdf.com  Fri Aug  4 01:35:20 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Aug 2006 16:35:20 -0700
Subject: [R] Random structure of nested design in lme
In-Reply-To: <E632249B3E11B14AAABA75FDB5F32B24132A0F@EXCHANGE4.unifr.ch>
References: <2323A6D37908A847A7C32F1E3662C80E132CEE@dc1ex01.air.org>	<E632249B3E11B14AAABA75FDB5F32B24132A0B@EXCHANGE4.unifr.ch>	<44C26882.2090209@pdf.com>
	<E632249B3E11B14AAABA75FDB5F32B24132A0F@EXCHANGE4.unifr.ch>
Message-ID: <44D28838.1090009@pdf.com>

	  I'm not familiar with 'aov', but I have two observations that might 
help you:

1.  UNESTIMABLE VARIANCE COMPONENT

	  The variance component 'soiltype' is not estimable in your 'lme' model:

	lme(NA.1~soiltype*habitat,random=~1|destination/soiltype)

That's because each level of 'soiltype' occurs only once within each 
level of 'destination' in the self-contained example you provided below.

	  To confirm this, I deleted 'soiltype' from this model:

fit.lme <- lme(response~soiltype*habitat, random=~1|destination/origin)
fit.lme0 <- lme(response~soiltype*habitat, random=~1|destination)

	  The answers seemed to be identical except for one thing:

 > VarCorr(fit.lme)
               Variance     StdDev
destination = pdLogChol(1)
(Intercept)   0.004149471  0.06441639
origin =      pdLogChol(1)
(Intercept)   0.060968550  0.24691810
Residual      0.007265180  0.08523603
 > VarCorr(fit.lme0)
destination = pdLogChol(1)
             Variance    StdDev
(Intercept) 0.004149471 0.06441639
Residual    0.068233730 0.26121587

	  The "Residual" variance in "fit.lme0" equals the sum of "origin" and 
"Residual" variances in "fit.lme".

	  It would help if 'lme' checked for situations like this and either 
refused to run or dropped inestimable variance components.  However, 
it's possible that there are so many ways that variance components can 
be inestimable that it's just not feasible to check for them all.  (The 
function 'varcomp' in S-Plus 6.2 has the same problem.)


2.  CROSSED OR NESTED?

	  Are 'destination' and 'origin' crossed or nested in your 'aov' model:

	  aov(response~soiltype*habitat+Error(destination+origin))

	  I have not used 'aov', and I don't think I should take the time now 
to try to figure this out.  However, this model specification suggests 
to me that 'destination' and 'origin' might be crossed not nested.  (The 
difference is the 'destination:origin' interaction:  If 
'destination+origin' is crossed, their interaction is used as the error 
term;  otherwise, it looks to me like you have a saturated model.)  By 
contrast, 'destination/origin' in lme is 'nested', which means that the 
variance component for 'origin' is in essence the crossed term and the 
interaction combined.

	  I believe there is a way to estimate crossed random effects using 
'lme', but I don't understand how.  Fortunately, we can do it using 
'lmer' in the 'lme4' and 'Matrix' packages.

	  Because of potential conflicts between 'nlme' and 'lme4', I always 
quit R and restart when I switch from one to another.  The following 
will then fit something using 'lmer' that looks like it might match your 
'aov' fit:

library(lme4)
fit.lme4 <- lmer(
   response~soiltype*habitat
      +(1|destination)+(1|origin), Dat0)

where Dat0 is a data.frame with columns 'response', 'soiltype', 
'habitat', 'destination' and 'origin'.
	
	  I don't know 'aov' well enough to determine easily if the results 
from this 'lmer' fit match those from 'aov', but I hope this helps.

	  Spencer Graves	

ESCHEN Rene wrote:
> Spencer,
> 
> Thank you for the kind and elaborate reply to my previous post.
> 
> I did consider the option you suggested and many variations. 
Depending on the order of the random factors, lme will either
give the same output as the aov model for soiltype or for habitat,
but not both in the same model.
> 
> The closest I came was 
> 
>           anova(lme(NA.1~soiltype*habitat,random=~1|destination/soiltype))
> 
> However, it apppears that in this case the interaction is tested at the same level as soiltype.
> 
> In this post, a small sample dataset with a brief explanation of the meaning of the different column titles is included below. Also, I included both the aov model and the lme model.
> 
> Hopefully, this will help to get closer to a solution to my problem.
> 
> Best regards,
> 
> Ren? Eschen.
> 
> ___
> 
> #Small sample dataset
> #
> data=read.table("Sample dataset.csv",header=T) 
> require(nlme)
> soiltype=factor(soiltype)
> habitat=factor(habitat)
> destination=factor(destination)
> origin=factor(origin)
> summary(aov(response~soiltype*habitat+Error(destination+origin)))
> anova(lme(response~soiltype*habitat,random=~1|destination/origin))
> #
> #"habitat" type is either 'arable' or 'grassland'
> #"destination" indicates what site the soil was transplanted into, and is considered a random factor within habitat type
> #"soiltype" is either 'arable' or 'grassland'
> #"origin" indicates what site the soil was taken from, and is considered a random factor within soil type
> #"response" is the response variable, typically some plant parameter such as growth rate or number of leaves, but in this example it is a random number between 0 and 1.
> #
> "habitat"	"destination"	"soiltype"	"origin"	"response"
> 1	1	1	1	0.63
> 1	2	1	1	0.76
> 1	3	1	1	0.14
> 2	4	1	1	0.27
> 2	5	1	1	0.88
> 2	6	1	1	0.41
> 1	1	1	2	0.47
> 1	2	1	2	0.48
> 1	3	1	2	0.76
> 2	4	1	2	0.83
> 2	5	1	2	0.88
> 2	6	1	2	0.57
> 1	1	1	3	0.80
> 1	2	1	3	0.31
> 1	3	1	3	0.22
> 2	4	1	3	0.53
> 2	5	1	3	0.97
> 2	6	1	3	0.30
> 1	1	2	4	0.46
> 1	2	2	4	0.99
> 1	3	2	4	0.56
> 2	4	2	4	0.32
> 2	5	2	4	0.46
> 2	6	2	4	0.64
> 1	1	2	5	0.03
> 1	2	2	5	0.41
> 1	3	2	5	0.24
> 2	4	2	5	0.60
> 2	5	2	5	0.04
> 2	6	2	5	0.30
> 1	1	2	6	0.97
> 1	2	2	6	0.60
> 1	3	2	6	0.22
> 2	4	2	6	0.16
> 2	5	2	6	0.58
> 2	6	2	6	0.21
> 
> 
> 
> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com]
> Sent: Sat 2006-07-22 20:03
> To: ESCHEN Rene
> Cc: Doran, Harold; r-help at stat.math.ethz.ch
> Subject: Re: [R] Random structure of nested design in lme
>  
> 	  Have you considered the following:
> 
> 	  anova(lme(NA.1~soiltype*habitat,random=~1|destination/origin))
> 
> 	  This seems more closely to match the 'aov' command in your original 
> post.  This model might be written in more detail as follows:
> 
> 	  NA.1[s, h, i,j,k] = b0 + ST[s] + H[h] +
> 		ST.H[s[i],j[j] j] + d[i] + o[i,j] + e[i,j,k]
> 
> where 	  b0 = a constant to be estimated,
> 
> 	  s = the soil type for that particular sample,
> 
> 	  h = the habitat for that sample,
> 
> 	  ST = soil type coefficients to be estimated subject to a constraint 
> that they sum to 0,
> 
> 	  H = habitat coefficients to be estimated subject to the constraint 
> that they sum to 0,
> 
> 	  ST.H = soil type by habitat interaction coefficients to be estimated 
> subject to constraints that ST.H[s,.] sum to 0 and ST.H[., h] also sum 
> to 0,
> 
> 	  d[i] = a random deviation associated with each destination, assuming 
> the d's are all normal, independent, with mean 0 and unknown but 
> constant variance s2.d
> 
> 	  o[i, j] = a random deviation associated with each destination / 
> origin combination, assuming the o's are all normal, independent, with 
> mean 0 and unknown variance s2.o,
> 
> and 	  e[i,j,j] = the standard unknown noise term, normal, independent 
> with mean 0 and unknown variance s2.e.
> 
> 	  The model you wrote includes nested noise terms for soil type and 
> habitat as well.  These terms are not estimable, which makes the answers 
> garbage, but the 'lme' function does not check for replicates and 
> therefore sometimes gives garbage answers without warning.
> 
> 	  To get more information from the fit, I suggest you first try 
> 'methods(class="lme")', and review help pages associated with what you 
> see listed there.
> 
> 	  Have you looked at Pinheiro and Bates (2000) Mixed-Effects Models in 
> S and S-Plus (Springer)?  This is my all-time favorite reference on 
> Bates has been one of the leading original contributors in variance 
> components analysis and nonlinear estimation more generally for over 25 
> years.  The 'nlme' package is the product of his work and the work of 
> many of his graduate students prior to 2000.  The book, at least from my 
> perspective, is very well written.  Moreover, the standard R 
> distribution includes files named "ch01.R", "ch02.R", ..., "ch06.R", 
> "ch08.R" with the R scripts accompanying each chapter in the book in 
> "~\library\nlme\scripts" under the R installation directory on your hard 
> drive, e.g. "D:\Program files\R\R-2.3.1\library\nlme\scripts", on my 
> computer.  There are minor changes in the syntax in a few places between 
> the book and the current R implementation that make it impossible to get 
> some of the published answers.  Using these script files increases the 
> likelihood that you will get essentially the book's answers and won't be 
> defeated by subtle typographical errors or by the difference between x^2 
> and I(x^2), for example.
> 
> 	  If you would like further information from this listserver, please 
> submit another post, preferably including a "commented, minimal, 
> self-contained, reproducible code", as suggested in the posting guide 
> "www.R-project.org/posting-guide.html".
> 
> 	  Hope this helps.
> 	  Spencer Graves
> 
> ESCHEN Rene wrote:
>> Although I know it's not correct, this is what I tried in lme:
>>
>> anova(lme(NA.1~soiltype*habitat,random=~1|destination/habitat/origin/soiltype))
>>
>> #                 numDF denDF   F-value p-value
>> #(Intercept)          1   130 12.136195  0.0007
>> #soiltype             1   130 15.099792  0.0002
>> #habitat              1    10  0.699045  0.4226
>> #soiltype:habitat     1   130  2.123408  0.1475
>>
>> Ren?.
>>
>> -----Original Message-----
>> From: Doran, Harold [mailto:HDoran at air.org]
>> Sent: Wed 2006-07-19 13:53
>> To: ESCHEN Rene; r-help at stat.math.ethz.ch
>> Subject: RE: [R] Random structure of nested design in lme
>>  
>> Can you provide an example of what you have done with lme so we might be able to evaluate the issue? 
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch 
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ESCHEN Rene
>>> Sent: Wednesday, July 19, 2006 7:37 AM
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] Random structure of nested design in lme
>>>
>>> All,
>>>
>>> I'm trying to analyze the results of a reciprocal transplant 
>>> experiment using lme(). While I get the error-term right in 
>>> aov(), in lme() it appears impossible to get as expected. I 
>>> would be greatful for any help.
>>>
>>> My experiment aimed to identify whether two fixed factors 
>>> (habitat type and soil type) affect the development of 
>>> plants. I took soil from six random sites each of two types 
>>> (arable and grassland) and transplanted them back into the 
>>> sites of origin in such way that in each of the sites there 
>>> were six pots containing arable soil and six pots of 
>>> grassland soil, each containing a seedling.
>>>
>>> With aov(), I got the analysis as I expected, with habitat 
>>> type tested against destination site, and soil type tested 
>>> against origin site:
>>>
>>> summary(aov(response~soiltype*habitat+Error(destination+origin)))
>>> #
>>> #Error: destination
>>> #          Df  Sum Sq Mean Sq F value Pr(>F)
>>> #habitat    1  1.0000  1.0000   0.699 0.4226
>>> #Residuals 10 14.3056  1.4306               
>>> #
>>> #Error: origin
>>> #          Df  Sum Sq Mean Sq F value   Pr(>F)   
>>> #soiltype   1 1.77778 1.77778  11.636 0.006645 **
>>> #Residuals 10 1.52778 0.15278                    
>>> #---
>>> #Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #
>>> #Error: Within
>>> #                  Df  Sum Sq Mean Sq F value Pr(>F)
>>> #soiltype:habitat   1  0.2500  0.2500  2.1774 0.1427
>>> #Residuals        120 13.7778  0.1148     
>>>
>>> However, when I try to replicate this analysis in lme, I am 
>>> unable to get the structure of the random factors (origin and 
>>> destination) correct. Does anyone have a suggestion how to 
>>> resolve this problem?
>>>
>>> Thanks in advance.
>>>
>>> Ren? Eschen
>>>
>>> CABI Bioscience Centre Switzerland
>>> Rue des Grillons 1
>>> 2800 Del?mont
>>> Switzerland
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>>
>>
>> 	[[alternative HTML version deleted]]
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Aug  4 02:22:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 3 Aug 2006 20:22:12 -0400
Subject: [R] Finding the position of a variable in a data.frame
In-Reply-To: <20060803223851.45208.qmail@web33813.mail.mud.yahoo.com>
References: <p0623090ec0f82713ad02@128.115.153.6>
	<20060803223851.45208.qmail@web33813.mail.mud.yahoo.com>
Message-ID: <971536df0608031722g6f039809ia647ac7da2f67c5a@mail.gmail.com>

On 8/3/06, John Kane <jrkrideau at yahoo.ca> wrote:
>
> --- Don MacQueen <macq at llnl.gov> wrote:
>
> > You don't need to find out the column index. This
> > works:
> >
> >     Df[5,'bat'] <- 100
> >
> > -Don
> >
>
> Thanks, I'd tried
> Df[5, bat] <- 100  :(
>
> I never thought of the ' ' being needed.

Right -- the quotes are not needed if you use $ but they are needed if
you use [.


From matt at overlook.homelinux.net  Fri Aug  4 03:17:15 2006
From: matt at overlook.homelinux.net (Matthew Wilson)
Date: Fri, 4 Aug 2006 01:17:15 +0000 (UTC)
Subject: [R] Building a random walk vector
Message-ID: <slrned580r.5dk.matt@mwilson.umlcoop.net>

I'm studying R in my free time.  I want to build a vector where each
element is equal to the element before it in the sequence plus some
random tweak.

In python, I would write:

vec = [100] * 50 # make a 50-element list with each element set to 100
from random import randint
for i, v in enumerate(vec):
    if i is not 0: # if we're not on the first element
        vec[i] = vec[i-1] + randint(-2, 2)

I suspect R has some fancier way of doing this.  How to?

TIA


-- 
A better way of running series of SAS programs:
http://overlook.homelinux.net/wilsonwiki/SasAndMakefiles


From ff809 at ncf.ca  Fri Aug  4 03:21:03 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Thu, 03 Aug 2006 21:21:03 -0400
Subject: [R] Questions about sweave...
Message-ID: <44D2A0FF.2090706@ncf.ca>

Evening all:

I'm taking a little time to experiment with R, Sweave, and Miktex/LaTex but 
I've run up against some problems and -well- I hope that there are some on 
the list who might have some suggestions. This will be kind of wordy as I 
will include the complete files involved as I'm just not sure what I'm 
looking for. Apologies at the outset.

I created the source file (example.Snw):

\documentclass[12pt,letterpaper]{article}
\title{Sweave Example 1}
\author{Friedrich Leisch}
\begin{document}
\maketitle

In this example we embed parts of the examples from the 
\texttt{kruskal.test} help page into a \LaTeX{} document:

<<>>=
data(airquality)
library(stats)
kruskal.test(Ozone ~ Month , data=airquality)
@

which shows that the location parameter of the Ozone distribution varies 
significantly from month to month. Finally we include a boxplot of the data:
\begin{center}
<<fig=TRUE , echo=FALSE>>=
boxplot(Ozone ~ Month , data=airquality)
@
\end{center}
\end{document}

which R seems to accept gracefully to produce the tex file (example.tex):

\documentclass[12pt,letterpaper]{article}
\title{Sweave Example 1}
\author{Friedrich Leisch}
\usepackage{C:/PROGRA~1/R/R-23~1.1/share/texmf/Sweave}
\begin{document}
\maketitle

In this example we embed parts of the examples from the 
\texttt{kruskal.test} help page into a \LaTeX{} document:

\begin{Schunk}
\begin{Sinput}
 > data(airquality)
 > library(stats)
 > kruskal.test(Ozone ~ Month, data = airquality)
\end{Sinput}
\begin{Soutput}
	Kruskal-Wallis rank sum test

data:  Ozone by Month
Kruskal-Wallis chi-squared = 29.2666, df = 4, p-value = 6.901e-06
\end{Soutput}
\end{Schunk}

which shows that the location parameter of the Ozone distribution varies 
significantly from month to month. Finally we include a boxplot of the data:
\begin{center}
\includegraphics{example-002}
\end{center}
\end{document}

but when I try to run Latex on it I get errors (example.log):

This is e-TeX, Version 3.141592-2.2 (MiKTeX 2.4) (preloaded format=latex 
2006.8.3)  3 AUG 2006 20:57
entering extended mode
**example.tex
(example.tex
LaTeX2e <2005/12/01>
Babel <v3.8g> and hyphenation patterns for english, usenglishmax, dumylang, noh
yphenation, french, ukenglish, loaded.
(C:\texmf\tex\latex\base\article.cls
Document Class: article 2005/09/16 v1.4f Standard LaTeX document class
(C:\texmf\tex\latex\base\size12.clo
File: size12.clo 2005/09/16 v1.4f Standard LaTeX file (size option)
)
\c at part=\count79
\c at section=\count80
\c at subsection=\count81
\c at subsubsection=\count82
\c at paragraph=\count83
\c at subparagraph=\count84
\c at figure=\count85
\c at table=\count86
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
)
! Missing \endcsname inserted.
<to be read again>
                    \protect
l.5 \begin
           {document}
The control sequence marked <to be read again> should
not appear between \csname and \endcsname.


! LaTeX Error: Missing \begin{document}.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
  ...

l.5 \begin
           {document}
You're in trouble here.  Try typing  <return>  to proceed.
If that doesn't work, type  X <return>  to quit.

! Extra \endcsname.
\@onefilewithoptions ...\@currext -h@@k\endcsname
                                                   \@empty \let 
\CurrentOptio...
l.5 \begin
           {document}
I'm ignoring this, since I wasn't doing a \csname.

! Missing \endcsname inserted.
<to be read again>
                    \protect
l.5 \begin
           {document}
The control sequence marked <to be read again> should
not appear between \csname and \endcsname.

! Extra \endcsname.
\@ifl at aded ...er \ifx \csname ver@#2.#1\endcsname
                                                   \relax \expandafter 
\@seco...
l.5 \begin
           {document}
I'm ignoring this, since I wasn't doing a \csname.

! Missing \endcsname inserted.
<to be read again>
                    \protect
l.5 \begin
           {document}
The control sequence marked <to be read again> should
not appear between \csname and \endcsname.

! Extra \endcsname.
\@pass at ptions ...xdef \csname opt@#3.#1\endcsname
                                                   {\@ifundefined 
{opt@#3.#1}...
l.5 \begin
           {document}
I'm ignoring this, since I wasn't doing a \csname.

! Missing \endcsname inserted.
<to be read again>
                    \protect
l.5 \begin
           {document}
The control sequence marked <to be read again> should
not appear between \csname and \endcsname.

! Missing \endcsname inserted.
<to be read again>
                    \protect
l.5 \begin
           {document}
The control sequence marked <to be read again> should
not appear between \csname and \endcsname.

! Extra \endcsname.
<argument> ...e/texmf/Sweave.\@currext \endcsname
                                                   ,
l.5 \begin
           {document}
I'm ignoring this, since I wasn't doing a \csname.

! Missing \endcsname inserted.
<to be read again>
                    \protect
l.5 \begin
           {document}
The control sequence marked <to be read again> should
not appear between \csname and \endcsname.

! Extra \endcsname.
<argument> ...er@\@currname .\@currext \endcsname
                                                   \@empty 
\InputIfFileExists...
l.5 \begin
           {document}
I'm ignoring this, since I wasn't doing a \csname.


! LaTeX Error: File `C:/PROGRA\unhbox\voidb at x \penalty \@M \ {}1/R/R-23\unhbox\
voidb at x \penalty \@M \ {}1.1/share/texmf/Sweave.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.5 \begin
           {document}

*** (cannot \read from terminal in nonstop modes)


Here is how much of TeX's memory you used:
  207 strings out of 95856
  1955 string characters out of 1194845
  47360 words of memory out of 1051005
  3348 multiletter control sequences out of 60000
  3938 words of font info for 15 fonts, out of 1000000 for 2000
  36 hyphenation exceptions out of 4999
  22i,0n,19p,148b,36s stack positions out of 5000i,500n,10000p,200000b,32768s
No pages of output.

I have tried adding the appropriate R directory where sweave.sty exists 
variously to the path in autoexec.bat and to the roots list in the options 
program of Miktex. Neither seems to work. Does anybody out there who has 
used sweave successfully have any advice on this situation?

Regards...
-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0631-2, 2006-08-02
Tested on: 2006-08-03 21:21:05
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From coleman.daniel at gene.com  Fri Aug  4 03:47:20 2006
From: coleman.daniel at gene.com (Daniel Coleman)
Date: Thu, 3 Aug 2006 18:47:20 -0700
Subject: [R] gnlsControl
Message-ID: <000001c6b767$ed270630$3f1f210a@gne.windows.gene.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060803/ed3a5066/attachment.pl 

From murdoch at stats.uwo.ca  Fri Aug  4 03:50:10 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 03 Aug 2006 21:50:10 -0400
Subject: [R] Building a random walk vector
In-Reply-To: <slrned580r.5dk.matt@mwilson.umlcoop.net>
References: <slrned580r.5dk.matt@mwilson.umlcoop.net>
Message-ID: <44D2A7D2.7020200@stats.uwo.ca>

On 8/3/2006 9:17 PM, Matthew Wilson wrote:
> I'm studying R in my free time.  I want to build a vector where each
> element is equal to the element before it in the sequence plus some
> random tweak.
> 
> In python, I would write:
> 
> vec = [100] * 50 # make a 50-element list with each element set to 100
> from random import randint
> for i, v in enumerate(vec):
>     if i is not 0: # if we're not on the first element
>         vec[i] = vec[i-1] + randint(-2, 2)
> 
> I suspect R has some fancier way of doing this.  How to?

Assuming randint(-2, 2) gives a value uniform on (-2, 2) a quick way to 
do what you want is

vec <- 100 + c(0, cumsum(runif(49, -2, 2)))

Duncan Murdoch


From edd at debian.org  Fri Aug  4 03:56:17 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 3 Aug 2006 20:56:17 -0500
Subject: [R] Building a random walk vector
In-Reply-To: <slrned580r.5dk.matt@mwilson.umlcoop.net>
References: <slrned580r.5dk.matt@mwilson.umlcoop.net>
Message-ID: <17618.43329.642932.819998@basebud.nulle.part>


On 4 August 2006 at 01:17, Matthew Wilson wrote:
| I'm studying R in my free time.  I want to build a vector where each
| element is equal to the element before it in the sequence plus some
| random tweak.
| 
| In python, I would write:
| 
| vec = [100] * 50 # make a 50-element list with each element set to 100
| from random import randint
| for i, v in enumerate(vec):
|     if i is not 0: # if we're not on the first element
|         vec[i] = vec[i-1] + randint(-2, 2)
| 
| I suspect R has some fancier way of doing this.  How to?

Yup, cumsum() is your friend. You only need the first scalar of 100,
vectorisation does the rest. Try

> set.seed(12345) 
> Z <- 100 + cumsum(runif(50,-2,2))
> summary(Z)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  98.54  100.80  102.30  102.10  103.40  105.70 
> plot(Z, type='l')

Lastly, I think N(0, some_sd) is more customary that U(-2,2) but that is easy
to change.

Cheers, Dik

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ggrothendieck at gmail.com  Fri Aug  4 04:24:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 3 Aug 2006 22:24:02 -0400
Subject: [R] ggplot facet label font size
In-Reply-To: <1F5C95F1B887EF42B14F31AEF36AD22F060661C1@dlee04.ent.ti.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F060661C1@dlee04.ent.ti.com>
Message-ID: <971536df0608031924u369dafaajb204f3533815b3e5@mail.gmail.com>

If you are willing to use grid then you could create only the sex
factor in the left strips since its already in the desired position
but when displaying it output a factor.level, i.e. label of "A".
(my.strip.left is modified from the prior post to do that.)

Then after the plot is drawn, looping through all grobs looking for
those with a label component of "A" producing a list of grob names,
strip.left.names.  We then mapply the real factor levels with
those grobs editing them in reset.levels(), defined below.
(I have used the fact, empirically determined that the stripts
are produced in order of the factor levels.)

Everything is the same as the last post except my.strip.left
which has been modified and everything which comes after the
call to histogram.

Although this seems to work, maybe Deepayan or Paul can think of
something slicker.


library(ggplot) # data resides here
library(lattice)
library(grid)

my.strip <- function(which.given, which.panel, ...)
   if (which.given == 1 && which.panel[2] == 2)
      strip.default(which.given, which.panel, ...)

my.strip.left <- function(which.given, which.panel, ...,
   factor.levels, horizontal)
   if (which.given == 1 && which.panel[1] == 1)
      strip.default(which.given, which.panel, factor.levels = LETTERS,
      horizontal = FALSE, ...)


histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
     strip.left = my.strip.left, par.settings = list(add.text =
list(cex = 0.7)))

is.strip.left <- function(name) identical(grid.get(name)$label, "A")
strip.left.names <- getNames()[sapply(getNames(), is.strip.left)]

reset.levels <- function(nam, lev) grid.edit(nam, label = lev)
mapply(reset.levels , strip.left.names, levels(tips$smoker))






On 8/3/06, Walker, Sam <s-walker at ti.com> wrote:
>
> This works OK, but there is some extra spacing between the panels, the
> top axis and the strip on the top, and the left labels and panel.
>
> How can I remove these extra spaces?
>
> I've tried changing various layout.widths settings with no luck.  It
> seems the spaces are calculated based on the number of conditioning
> variables, in this case 2 (sex+smoker).
>
>
> Thanks in advance...
> -Sam
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Wednesday, August 02, 2006 6:04 PM
> To: Walker, Sam
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ggplot facet label font size
>
> On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> > How do I change the font size in the facet labels along the edges of
> the
> > plot?
> >
> > For example (from the ggplot help file):
> >     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
> >     gghistogram(p)
> >
> > In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> > Female", "sex: Male".  What command can I use to reduce the font size
> of
> > these labels?
> >
> > In lattice terminology, cex is used to scale these strip labels.  But
> I
> > couldn't find the equivalent in ggplot.
> >
> > The reason I'm asking is I have a 9x7 array of plots which I've been
> > plotting with lattice.  I wanted to use ggplot because I like having
> the
> > labels on the edge of the plots
>
> Note that lattice can do that by using custom strip functions:
>
> library(ggplot) # data resides here
> library(lattice)
>
> my.strip <- function(which.given, which.panel, ...)
>   if (which.given == 1 && which.panel[2] == 2)
>      strip.default(which.given, which.panel, ...)
>
> my.strip.left <- function(which.given, which.panel, ..., horizontal)
>   if (which.given == 2 && which.panel[1] == 1)
>      strip.default(which.given, which.panel, horizontal = FALSE, ...)
>
> histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
>     strip.left = my.strip.left, par.settings = list(add.text =
> list(cex = 0.7)))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Aug  4 05:32:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 3 Aug 2006 23:32:21 -0400
Subject: [R] ggplot facet label font size
In-Reply-To: <f8e6ff050608031443i20e6521fic478b103179f9133@mail.gmail.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F05FEB78F@dlee04.ent.ti.com>
	<f8e6ff050608031443i20e6521fic478b103179f9133@mail.gmail.com>
Message-ID: <971536df0608032032w1b67bc7sd59c0f8e4f379c87@mail.gmail.com>

Just sending this to you.  One thing that might be easy to do
yet give a lot of flexibility is to:

1. put meaningful names on the grobs.  Even with just this it would be
possible to do a getNames() in grid and then from inspection grid.edit
the appropriate one(s).
2. create a routine that retrieves grobs so one does not have to use
getNames with grep.

trellis.focus and friends do this in lattice.

Regards.



On 8/3/06, hadley wickham <h.wickham at gmail.com> wrote:
> Hi Sam,
>
> > How do I change the font size in the facet labels along the edges of the
> > plot?
>
> Unfortunately, you can't currently change the size of those fonts.
> However, it is on my todo list (as well as completely custom strip
> functions) and should be available in the near future.
>
> One thing you could do is have a look at ggopt, where you can at least
> change the strip text, if not the size.
>
> Regards,
>
> Hadley
>
> On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> >
> > For example (from the ggplot help file):
> >      p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
> >      gghistogram(p)
> >
> > In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> > Female", "sex: Male".  What command can I use to reduce the font size of
> > these labels?
> >
> > In lattice terminology, cex is used to scale these strip labels.  But I
> > couldn't find the equivalent in ggplot.
> >
> > The reason I'm asking is I have a 9x7 array of plots which I've been
> > plotting with lattice.  I wanted to use ggplot because I like having the
> > labels on the edge of the plots, but the label font size is too large
> > and exceeding the size of the label box.
> >
> > Thanks in advance...
> > -Sam
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Fri Aug  4 06:10:43 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Aug 2006 21:10:43 -0700
Subject: [R] standard dev in glmmPQL
In-Reply-To: <3AA0B59C9640784C8956888BF8AFC5DD1FF247@fc-mailserver01.ul.pt>
References: <3AA0B59C9640784C8956888BF8AFC5DD1FF247@fc-mailserver01.ul.pt>
Message-ID: <44D2C8C3.8000102@pdf.com>

	  Let's look at the first example on the help page for "glmmQPL":

library(MASS)
library(nlme) # will be loaded automatically if omitted
fitPQL <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
                      family = binomial, data = bacteria)
fitPQL
VarCorr(fitPQL)
VarCorr(fitPQL)[2,2]

	  You might also be interested in some other functions that might help 
you solve problems like this in the future:

	  'str(fitPQL)' provides a compact summary of 'fitPQL'.
	  'class(fitPQL)' tells us it is of class 'glmmPQL' and 'lme'
	  'methods(class="lme")' identifies all the functions with special 
methods written for 'lme' objects.

	  Hope this helps.
	  Spencer Graves

Maria Salom? Esteves Cabral wrote:
> Hi!
>  
> Can anyone let me know how can I get the stdDev of the random intercept from the output  of glmmPQL?
>  
>  
> Thanks
>  
> Salom?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From spencer.graves at pdf.com  Fri Aug  4 06:31:10 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Aug 2006 21:31:10 -0700
Subject: [R] Replacing NA in fSeries
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D03059EEF@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D03059EEF@MAIL1.infores.com>
Message-ID: <44D2CD8E.9040108@pdf.com>

	  Will 'na.locf' in the 'zoo' package do what you want?

	  If not, please provide commented, minimal, self-contained, 
reproducible code as suggested in the posting guide 
(www.R-project.org/posting-guide.html).

	  Hope this helps.
	  Spencer Gaves
p.s.  The 'zoo' package contains an excellent vignette.  If you are not 
familiar with vignettes, I suggest you read 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html" and then 
try the zoo vignette.

Kerpel, John wrote:
> Hi folks!
> 
>  
> 
> What's a good way to replace NAs with a prior value in a time series
> created with the fSeries package?
> 
>  
> 
> Thanks,
> 
> John
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From emailtogauravyadav at gmail.com  Fri Aug  4 07:07:29 2006
From: emailtogauravyadav at gmail.com (GauravMailExpress)
Date: Fri, 4 Aug 2006 10:37:29 +0530
Subject: [R] Question regarding extrapolation
Message-ID: <fd6b689f0608032207j7a8b9050s7ba616b95948f386@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/04b66ef4/attachment.pl 

From r at fam-kuster.ch  Fri Aug  4 08:20:11 2006
From: r at fam-kuster.ch (Thomas Kuster)
Date: Fri, 4 Aug 2006 08:20:11 +0200
Subject: [R] read.spss and umlaut
In-Reply-To: <Pine.LNX.4.64.0608030622130.20946@homer23.u.washington.edu>
References: <200608021531.56340.r@fam-kuster.ch>
	<200608031038.14141.r@fam-kuster.ch>
	<Pine.LNX.4.64.0608030622130.20946@homer23.u.washington.edu>
Message-ID: <200608040820.13403.r@fam-kuster.ch>

Hello

Am Donnerstag, 3. August 2006 15.34 schrieb Thomas Lumley:
> On Thu, 3 Aug 2006, Thomas Kuster wrote:
> > Hello
> >
> > Am Mittwoch, 2. August 2006 17.11 schrieb Thomas Lumley:
...
> You haven't shown anything that indicates that the C code stopped reading.
> More likely R just stops displaying when it gets to an illegal byte
> sequence.  You could use nchar() to count the bytes in the string to find
> out.

If I change the translatable characters (overwrite the 0 between :#@'=" and 
~000 with ??????). I can read in the file an every ?????? ist a withspace:
> daten <- read.spss("projets_umlaut.por")
> levels(daten$PROJETX)
  [1] "Bg Stammzellenforschung"
  [2] "Bb   ber eine neue Finanzordnung"
  [3] "Bb Neugestaltung des Finanzausgleichs"
  [4] " nderrung Bg  EOG Mutterschafturlaub"
  [5] "EV Postdienste f r alle"
  [6] "Bb  ber B rgerrechtserwerb 3. Generation"
  [7] "Bb  ber erleichterte Einb rung 2. Generation"
  [8] "Bg Steuerpaket"
   .
   .
   .
> levels(daten$PROJETX)[208]
[1] "EV Gleiche Rechte f r Mann und Frau Gegenvorschlag"
> charToRaw(levels(daten$PROJETX)[208])
 [1] 45 56 20 47 6c 65 69 63 68 65 20 52 65 63 68 74 65 20 66 20 72 20 4d 61 
6e
[26] 6e 20 75 6e 64 20 46 72 61 75 20 47 65 67 65 6e 76 6f 72 73 63 68 6c 61 
67

without change the table I get:
> daten <- read.spss("projets.por")
> charToRaw(levels(daten$PROJETX)[208])
 [1] 45 56 20 47 6c 65 69 63 68 65 20 52 65 63 68 74 65 20 66

The SPSS file is from:
http://voxit.sidos.ch/update.asp?lang=d
-> Download der kumulierten Dateien Version 2.0

You must accept this:
The Standardized Post-Vote Surveys:
http://voxit.sidos.ch/agreement.asp?lang=e&menu=0

Thomas


From dieter.menne at menne-biomed.de  Fri Aug  4 08:46:58 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 4 Aug 2006 06:46:58 +0000 (UTC)
Subject: [R] gnlsControl
References: <000001c6b767$ed270630$3f1f210a@gne.windows.gene.com>
Message-ID: <loom.20060804T083931-150@post.gmane.org>

Daniel Coleman <coleman.daniel <at> gene.com> writes:

> 
> When I run gnls I get the error:
> 
> Error in nls(y ~ cbind(1, 1/(1 + exp((xmid - x)/exp(lscal)))), data = xy,  :
> 
>         step factor 0.000488281 reduced below 'minFactor' of 0.000976563
> 
> My first thought was to decrease minFactor but gnlsControl does not contain
> minFactor nor nlsMinFactor (see below).  It does however contain nlsMaxIter
> and nlsTol which I assume are the analogs of maxiter and tol in nls.control.
> I would be happy to hear from anyone who has an idea on what parameters in
> gnlsControl to change to get convergence.  

Try nlsTol with a large value, e.g. 0.3. If I am really desparate, I put the
gnls in a try() structure, halving nlsTol until it fails, and take the last
successful. 

Dieter


From dieter.menne at menne-biomed.de  Fri Aug  4 08:57:44 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 4 Aug 2006 06:57:44 +0000 (UTC)
Subject: [R] Questions about sweave...
References: <44D2A0FF.2090706@ncf.ca>
Message-ID: <loom.20060804T085513-88@post.gmane.org>

Brian Lunergan <ff809 <at> ncf.ca> writes:

> 
> Evening all:
> 
> I'm taking a little time to experiment with R, Sweave, and Miktex/LaTex but 
> I've run up against some problems and -well- I hope that there are some on 
> the list who might have some suggestions. This will be kind of wordy as I 
> will include the complete files involved as I'm just not sure what I'm 
> looking for. Apologies at the outset.

This is Windows special

See A.12: 
http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html

And put a commented (!sic) \usepackage into the header (don't remember where I
found this)

% \usepackage{Sweave} 


Dieter


From stefano.iacus at unimi.it  Fri Aug  4 09:14:09 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Fri, 4 Aug 2006 09:14:09 +0200
Subject: [R]  geodesic distance (solution)
Message-ID: <D20E1AD2-2589-4138-96D8-12EB491040D9@unimi.it>

 > Hi,
 > has anyone ever seen implemented in R the following "geodesic"
 > distance between positive definite pxp matrices A and B?
 >
 > d(A,B) = \sum_{i=1}^p (\log \lambda_i)^2
 >
 > were \lambda is the solution of det(A -\lambda B)  = 0
 >
 > thanks
 > stefano

as I received few private email on the claimed solution, I'm posting  
it to r-help.

when matrix B is invertible (which is always my case), one approach  
is to notice that
solving

det(A -\lambda * B)  = 0

is equivalent to solve

det(B^-1*A -\lambda *I)  = 0

which is a standard eigen value problem for the matrix B^-1 * A, hence

eigen(solve(B) %*% A)$values

is the answer.

I'm pretty sure that the problem can also be solved using some svd  
decomposition when B is not invertible.

hope it helps
stefano


From bolker at ufl.edu  Fri Aug  4 03:27:22 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 4 Aug 2006 01:27:22 +0000 (UTC)
Subject: [R] Building a random walk vector
References: <slrned580r.5dk.matt@mwilson.umlcoop.net>
Message-ID: <loom.20060804T032448-59@post.gmane.org>

Matthew Wilson <matt <at> overlook.homelinux.net> writes:

> 
> I'm studying R in my free time.  I want to build a vector where each
> element is equal to the element before it in the sequence plus some
> random tweak.
> 

  You will probably get many answers to this, but
I think

vec <- 100+c(0,cumsum(runif(49,-2,2)))

works.

  Ben Bolker


From e.deomano at cgiar.org  Fri Aug  4 09:45:52 2006
From: e.deomano at cgiar.org (Deomano, Emily (IRRI))
Date: Fri, 4 Aug 2006 15:45:52 +0800
Subject: [R] need sample parallelized R scripts
Message-ID: <99564AEBF1939B4994111DF0C921946FC23627@HERMES>

Good day to everyone.

I'm working on computing correlation for several datasets (one dataset for each chromosome).? Computation is done several thousand times for each dataset which at present takes around 13 hours. We have a HPC machine with MPI.? snow package and R 2.3.1 running in Linux (Rocks) are installed in the machine.? I need to modify the script to run it on several nodes. ?I do not have experience in writing parallel scripts.

Anybody knows where I can find sample R scripts that are designed to run on parallel machines? ?

Thanks.
Ems



Emily Deomano
Crop Research Informatics Laboratory
The International Rice Research Institute DAPO Box 7777 Metro Manila, Philippines
Email: e.deomano at cgiar.org;? Tel No: (63-2) 580-5600;? Fax No: (63-2) 580-5699


From lorenzo.isella at gmail.com  Fri Aug  4 10:23:06 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Fri, 4 Aug 2006 10:23:06 +0200
Subject: [R] Integration and Loop in R
Message-ID: <a2b3004b0608040123j3c8bdaa5oc026432a0e3da045@mail.gmail.com>

Dear All,
I have seldom needed to use loops in R, but now I need to code a loop
with a stride different from one.
In the R manual I downloaded I have the example:
> xc <- split(x, ind)
> yc <- split(y, ind)
> for (i in 1:length(yc)) {
    plot(xc[[i]], yc[[i]]);
    abline(lsfit(xc[[i]], yc[[i]]))
  }
but in my case I'd like to add a condition so that i varies by 4 from
one go to the following one. I cannot figure out the right syntax, can
anyone help here?
Another thing (which could possibly solve my problem): I had a look at
integrate command in R.
It seems to require an object defined as a function to carry out the
integration.
What if I simply have a list of data values? How can I coerce them
into a function recognized by R? Furthermore, are there R routines to
carry out the integration on a non-equally spaced 1D grid?
Best Regards

Lorenzo


From petr.pikal at precheza.cz  Fri Aug  4 10:50:24 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 04 Aug 2006 10:50:24 +0200
Subject: [R] Integration and Loop in R
In-Reply-To: <a2b3004b0608040123j3c8bdaa5oc026432a0e3da045@mail.gmail.com>
Message-ID: <44D32670.20262.360E03@localhost>



On 4 Aug 2006 at 10:23, Lorenzo Isella wrote:

Date sent:      	Fri, 4 Aug 2006 10:23:06 +0200
From:           	"Lorenzo Isella" <lorenzo.isella at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Integration and Loop in R

> Dear All,
> I have seldom needed to use loops in R, but now I need to code a loop
> with a stride different from one. In the R manual I downloaded I have
> the example: > xc <- split(x, ind) > yc <- split(y, ind) > for (i in
> 1:length(yc)) {
>     plot(xc[[i]], yc[[i]]);
>     abline(lsfit(xc[[i]], yc[[i]]))
>   }
> but in my case I'd like to add a condition so that i varies by 4 from

You can make i vary as you wish.
e.g.
steps <- seq(1,40,4)

for (i in steps) ......

> one go to the following one. I cannot figure out the right syntax, can
> anyone help here? Another thing (which could possibly solve my
> problem): I had a look at integrate command in R. It seems to require
> an object defined as a function to carry out the integration. What if
> I simply have a list of data values? How can I coerce them into a
> function recognized by R? Furthermore, are there R routines to carry
> out the integration on a non-equally spaced 1D grid? Best Regards

Try to look at splinefun or approxfun.

HTH
Petr

> 
> Lorenzo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From jim at bitwrit.com.au  Sat Aug  5 01:17:49 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 04 Aug 2006 19:17:49 -0400
Subject: [R] bullseye or polar display of "circular" data
In-Reply-To: <s4d1a9f6.087@ohsu.edu>
References: <s4d1a9f6.087@ohsu.edu>
Message-ID: <44D3D59D.4090203@bitwrit.com.au>

Michael Jerosch-Herold wrote:
> I have data for several rings of a left heart chamber, and which I would like to display in concentric rings, with color-encoding of the values. Each ring corresponds to one slice through the heart, and the rings correspond to positions from the base to the apex of the heart as you move from the outermost ring to the innermost one. The data have a circular pattern. These types of displays are referred to as bullseye displays in the nuclear medicine literature. Does any reader of these messages know of a R function/package that offers this functionality?
> 
> Also I noticed that in some contexts you can define a "circular" attribute for your data. Are there plot routines for such "circular" data?
> 
I'm not quite sure that I understand the display you want, but 
radial.plot in the plotrix package might do what you want. Also you 
could check the circular and CircStats packages.

Jim


From jim at bitwrit.com.au  Sat Aug  5 01:37:34 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 04 Aug 2006 19:37:34 -0400
Subject: [R] prettyR arrives
Message-ID: <44D3DA3E.6040204@bitwrit.com.au>

Hi all,

I have finally gotten the prettyR package going (many thanks to Kurt 
Hornik for his patience).

prettyR is a set of functions that allows the user to produce HTML 
output from R scripts. Given an R script that runs properly, an HTML 
listing complete with embedded graphics can be produced simply by 
passing the script to the core function htmlize (Phillipe Grosjean has 
not only offered great suggestions, but provided a fancier function 
named R2html). It is even possible to have the output magically appear 
in your friendly local HTML browser when the script has been processed.

The package includes some basic descriptive functions that display "the 
usual suspects" in formats that should not agitate those accustomed to 
the vanilla listings that abound in the real world.

prettyR is intended to assist the R beginner in producing basic stats 
right from the word "go". No knowledge beyond that of writing an R 
script is required, but there is quite a bit of room to learn and 
innovate. Have fun and please let me know if you break it.

Jim


From phhs80 at gmail.com  Fri Aug  4 12:21:32 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Fri, 4 Aug 2006 11:21:32 +0100
Subject: [R] Looking for transformation to overcome heterogeneity of
	variances
In-Reply-To: <OF954B12A1.08D9CFCE-ON872571BF.00684FA5-872571BF.0069BC4D@usgs.gov>
References: <6ade6f6c0608030633w30571a11gb7966987bd36070c@mail.gmail.com>
	<OF954B12A1.08D9CFCE-ON872571BF.00684FA5-872571BF.0069BC4D@usgs.gov>
Message-ID: <6ade6f6c0608040321ydf3d497n850002191bf38948@mail.gmail.com>

Thanks to all contributors for the fruitfulness of this discussion. I
am speculating about a simpler solution: to use a non-parametric
approach. To avoid the requirement of having normal residuals, Frank
Harrell has suggested here the following non-parametric procedure:

library(Design)  # also requires library(Hmisc)
f <- lrm(y ~ a*b*c*d)
f
anova(f)

Could someone please tell me whether that also works when there is no
homoscedasticity? What are the assumptions of that method?

Paul


From chiya31 at gmail.com  Fri Aug  4 13:36:38 2006
From: chiya31 at gmail.com (chiya sharma)
Date: Fri, 4 Aug 2006 17:06:38 +0530
Subject: [R] User input from keyboard
Message-ID: <92cc2b7b0608040436w277b2006n994bc22d185a5fda@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/de6a1988/attachment.pl 

From slist at oomvanlieshout.net  Fri Aug  4 13:48:47 2006
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 04 Aug 2006 13:48:47 +0200
Subject: [R] Data frame referencing?
Message-ID: <44D3341F.20107@oomvanlieshout.net>

Dear R users,

When you do:
> x <- rnorm(10)
> y <- rnorm(10)
> z <- rnorm(10)
> a <- data.frame(x,y,z)
> a$x
 [1]  1.37821893  0.21152756 -0.55453182 -2.10426048 -0.08967880  0.03712110
 [7] -0.80592149  0.07413450  0.15557671  1.22165341

Why does this not work:
> a[a$y>0.5,y] <-1
Error in "[<-.data.frame"(`*tmp*`, a$y > 0.5, y, value = 1) :
        only 0's may be mixed with negative subscripts

While this works:
> a[a$y>0.5,2] <-1

> a
             x          y          z
1   1.37821893 -1.0887363  1.7340522
2   0.21152756 -0.7256467 -1.3165373
3  -0.55453182  1.0000000 -2.1116072
4  -2.10426048 -0.4898596 -1.5863823
5  -0.08967880  1.0000000 -0.9139706
6   0.03712110  1.0000000 -1.3004970
7  -0.80592149 -0.7004193 -0.1958059
8   0.07413450  1.0000000 -1.3574303
9   0.15557671 -0.3335407 -2.1991236
10  1.22165341  1.0000000 -0.7576708

For a complex loop I would prefer to reference the right colomn by name,
not by number! Now, when the colomns change, I need to check my code to
make sure that the right colomns are referenced.

Suggestions much appreciated!

Thanks in advance,

Sander.


From ggrothendieck at gmail.com  Fri Aug  4 13:53:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 4 Aug 2006 07:53:36 -0400
Subject: [R] Data frame referencing?
In-Reply-To: <44D3341F.20107@oomvanlieshout.net>
References: <44D3341F.20107@oomvanlieshout.net>
Message-ID: <971536df0608040453r1314e21dlf2296f24a0fc5a5d@mail.gmail.com>

When specifying a column name with [ the name must be quoted (unlike
when using it with $):

   a[a$y > 0.5, "y"] <- 1

On 8/4/06, Sander Oom <slist at oomvanlieshout.net> wrote:
> Dear R users,
>
> When you do:
> > x <- rnorm(10)
> > y <- rnorm(10)
> > z <- rnorm(10)
> > a <- data.frame(x,y,z)
> > a$x
>  [1]  1.37821893  0.21152756 -0.55453182 -2.10426048 -0.08967880  0.03712110
>  [7] -0.80592149  0.07413450  0.15557671  1.22165341
>
> Why does this not work:
> > a[a$y>0.5,y] <-1
> Error in "[<-.data.frame"(`*tmp*`, a$y > 0.5, y, value = 1) :
>        only 0's may be mixed with negative subscripts
>
> While this works:
> > a[a$y>0.5,2] <-1
>
> > a
>             x          y          z
> 1   1.37821893 -1.0887363  1.7340522
> 2   0.21152756 -0.7256467 -1.3165373
> 3  -0.55453182  1.0000000 -2.1116072
> 4  -2.10426048 -0.4898596 -1.5863823
> 5  -0.08967880  1.0000000 -0.9139706
> 6   0.03712110  1.0000000 -1.3004970
> 7  -0.80592149 -0.7004193 -0.1958059
> 8   0.07413450  1.0000000 -1.3574303
> 9   0.15557671 -0.3335407 -2.1991236
> 10  1.22165341  1.0000000 -0.7576708
>
> For a complex loop I would prefer to reference the right colomn by name,
> not by number! Now, when the colomns change, I need to check my code to
> make sure that the right colomns are referenced.
>
> Suggestions much appreciated!
>
> Thanks in advance,
>
> Sander.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From chonghuitan at smu.edu.sg  Fri Aug  4 13:58:54 2006
From: chonghuitan at smu.edu.sg (TAN Chong Hui)
Date: Fri, 4 Aug 2006 19:58:54 +0800
Subject: [R] Problem with installing R under Windows
Message-ID: <FA3090E732DC6A4EB8E2D875EEB486A508CC04@EX01.staff.smu.edu.sg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/52a4107e/attachment.pl 

From wsimpson at utsc.utoronto.ca  Fri Aug  4 14:03:48 2006
From: wsimpson at utsc.utoronto.ca (William Simpson)
Date: Fri, 4 Aug 2006 08:03:48 -0400 (EDT)
Subject: [R] between-within anova: aov and lme
Message-ID: <30089.131.136.242.1.1154693028.squirrel@webmail.utsc.utoronto.ca>

Well nobody answered :-(
Nobody on R-help is doing anovas I guess -- I don't blame them! (It's just for aggies.)

In the absence of any response and for no good reason I am doing:
fitn1 <- aov(amplitude ~ stereo*site*stimulus + Error(subject), stereon1) This is
Bill Venables's way.
And when the data are unbalanced I am doing:
lme(amplitude ~ site+stimulus+stereo*stimulus, random=~1|subject, method="ML",
stereon1)

And I have no clue why.

Every discussion of between-within ANOVA I have read (practical or mathematical) is
either vacuous or opaque...

Cheers
Bill
> I have 2 questions on ANOVA with 1 between subjects factor and 2 within factors.
>
> 1. I am confused on how to do the analysis with aov because I have seen two
examples on the web with different solutions.
>
> a) Jon Baron (http://www.psych.upenn.edu/~baron/rpsych/rpsych.html) does 6.8.5
Example 5: Stevens pp. 468 - 474 (one between, two within)
>
> between: gp
> within: drug, dose
> aov(effect ~ gp * drug * dose + Error(subj/(dose*drug)), data=Ela.uni)
>
> b) Bill Venables answered a question on R help as follows.
>
> - factor A between subjects
> - factors B*C within subjects.
>
> aov(response ~ A*B*C + Error(subject), Kirk)
> "An alternative formula would be response ~ A/(B*C) + Error(subject), which would
only change things by grouping together some of the sums of squares."
>
> -------------------------------------------------------
> SO: which should I do????
> aov(response ~ A*B*C + Error(subject), Kirk)
> aov(response ~ A/(B*C) + Error(subject), Kirk)
> aov(response ~ A*B*C + Error(subject/(B*C)), Kirk)
> --------------------------------------------------------
>
> 2. How would I do the analysis in lme()?
> Something like
> lme(response~A*B*C,random=~1|subject/(B*C))???
>
>
> Thanks very much for any help!
> Bill Simpson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html and
provide commented, minimal, self-contained, reproducible code.
>


From dimitris.rizopoulos at med.kuleuven.be  Fri Aug  4 14:11:08 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 4 Aug 2006 14:11:08 +0200
Subject: [R] Data frame referencing?
References: <44D3341F.20107@oomvanlieshout.net>
Message-ID: <013301c6b7bf$119ee3a0$0540210a@www.domain>

you need to use quotes, i.e.,

a[a$y > 0.5, "y"] <- 1

you can also use

a$y[a$y > 0.5] <- 1


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sander Oom" <slist at oomvanlieshout.net>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 04, 2006 1:48 PM
Subject: [R] Data frame referencing?


> Dear R users,
>
> When you do:
>> x <- rnorm(10)
>> y <- rnorm(10)
>> z <- rnorm(10)
>> a <- data.frame(x,y,z)
>> a$x
> [1]  1.37821893  0.21152756 -0.55453182 -2.10426048 -0.08967880 
> 0.03712110
> [7] -0.80592149  0.07413450  0.15557671  1.22165341
>
> Why does this not work:
>> a[a$y>0.5,y] <-1
> Error in "[<-.data.frame"(`*tmp*`, a$y > 0.5, y, value = 1) :
>        only 0's may be mixed with negative subscripts
>
> While this works:
>> a[a$y>0.5,2] <-1
>
>> a
>             x          y          z
> 1   1.37821893 -1.0887363  1.7340522
> 2   0.21152756 -0.7256467 -1.3165373
> 3  -0.55453182  1.0000000 -2.1116072
> 4  -2.10426048 -0.4898596 -1.5863823
> 5  -0.08967880  1.0000000 -0.9139706
> 6   0.03712110  1.0000000 -1.3004970
> 7  -0.80592149 -0.7004193 -0.1958059
> 8   0.07413450  1.0000000 -1.3574303
> 9   0.15557671 -0.3335407 -2.1991236
> 10  1.22165341  1.0000000 -0.7576708
>
> For a complex loop I would prefer to reference the right colomn by 
> name,
> not by number! Now, when the colomns change, I need to check my code 
> to
> make sure that the right colomns are referenced.
>
> Suggestions much appreciated!
>
> Thanks in advance,
>
> Sander.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From petr.pikal at precheza.cz  Fri Aug  4 14:19:46 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 04 Aug 2006 14:19:46 +0200
Subject: [R] User input from keyboard
In-Reply-To: <92cc2b7b0608040436w277b2006n994bc22d185a5fda@mail.gmail.com>
Message-ID: <44D35782.22692.F5C7BD@localhost>

Hi

cat("\n","Enter x","\n") # prompt
y<-scan(n=1)

prompts for user imput and scans 1 line from console.

HTH
Petr



On 4 Aug 2006 at 17:06, chiya sharma wrote:

Date sent:      	Fri, 4 Aug 2006 17:06:38 +0530
From:           	"chiya sharma" <chiya31 at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] User input from keyboard

> Dear All,
> 
>  Can anybody tell me the syntax for "User input from keyboard" in R. I
>  mean
> to say that if I run the program it should ask "Please enter the date"
> at the begining of the program. I am using R-2.2.1 for windows.
> 
> Any help will be greatly appreciated.
> thanks in advance.
> 
> Regards,
> Chiya
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From slist at oomvanlieshout.net  Fri Aug  4 14:26:48 2006
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 04 Aug 2006 14:26:48 +0200
Subject: [R] Data frame referencing?
In-Reply-To: <013301c6b7bf$119ee3a0$0540210a@www.domain>
References: <44D3341F.20107@oomvanlieshout.net>
	<013301c6b7bf$119ee3a0$0540210a@www.domain>
Message-ID: <44D33D08.9070207@oomvanlieshout.net>

Dear Gabor and Dimitris,

Simple, once you know! It is these little exceptions on the R notation
that get me stuck. Now I am on the loose again!

Thanks,

Sander.

Dimitris Rizopoulos wrote:
> you need to use quotes, i.e.,
> 
> a[a$y > 0.5, "y"] <- 1
> 
> you can also use
> 
> a$y[a$y > 0.5] <- 1
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Sander Oom" <slist at oomvanlieshout.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, August 04, 2006 1:48 PM
> Subject: [R] Data frame referencing?
> 
> 
>> Dear R users,
>>
>> When you do:
>>> x <- rnorm(10)
>>> y <- rnorm(10)
>>> z <- rnorm(10)
>>> a <- data.frame(x,y,z)
>>> a$x
>> [1]  1.37821893  0.21152756 -0.55453182 -2.10426048 -0.08967880 
>> 0.03712110
>> [7] -0.80592149  0.07413450  0.15557671  1.22165341
>>
>> Why does this not work:
>>> a[a$y>0.5,y] <-1
>> Error in "[<-.data.frame"(`*tmp*`, a$y > 0.5, y, value = 1) :
>>        only 0's may be mixed with negative subscripts
>>
>> While this works:
>>> a[a$y>0.5,2] <-1
>>> a
>>             x          y          z
>> 1   1.37821893 -1.0887363  1.7340522
>> 2   0.21152756 -0.7256467 -1.3165373
>> 3  -0.55453182  1.0000000 -2.1116072
>> 4  -2.10426048 -0.4898596 -1.5863823
>> 5  -0.08967880  1.0000000 -0.9139706
>> 6   0.03712110  1.0000000 -1.3004970
>> 7  -0.80592149 -0.7004193 -0.1958059
>> 8   0.07413450  1.0000000 -1.3574303
>> 9   0.15557671 -0.3335407 -2.1991236
>> 10  1.22165341  1.0000000 -0.7576708
>>
>> For a complex loop I would prefer to reference the right colomn by 
>> name,
>> not by number! Now, when the colomns change, I need to check my code 
>> to
>> make sure that the right colomns are referenced.
>>
>> Suggestions much appreciated!
>>
>> Thanks in advance,
>>
>> Sander.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Jan.Wijffels at ucs.kuleuven.be  Fri Aug  4 14:40:05 2006
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Fri, 04 Aug 2006 14:40:05 +0200
Subject: [R] Sweave special token \\ from R to latex
Message-ID: <20060804144005.fu4o7us7erk0wooo@webmail4.kuleuven.be>

Dear helpeRs,

I would like to specify a newline command in R and pass it to latex  
via Sweave such that it corresponds to latex' \\ command. But that  
doesn't seem to be possible. If I Sweave the \n character, it just  
makes a new line in latex but not the \\ command.

Is there a way such that the following code would result in latex in
blablabla \\ blablabla on different line


<<echo=FALSE>>=
   string <- "blablabla \\ blablabla on different line"
@
\Sexpr{string}




Thanks for the help,
Jan

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Fri Aug  4 14:43:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Aug 2006 13:43:11 +0100 (BST)
Subject: [R] User input from keyboard
In-Reply-To: <44D35782.22692.F5C7BD@localhost>
References: <44D35782.22692.F5C7BD@localhost>
Message-ID: <Pine.LNX.4.64.0608041340380.19526@gannet.stats.ox.ac.uk>

On Fri, 4 Aug 2006, Petr Pikal wrote:

> Hi
> 
> cat("\n","Enter x","\n") # prompt
> y<-scan(n=1)
> 
> prompts for user imput and scans 1 line from console.
(that scans one number: use readLines(n=1) to get a string).

But readline() is probably easier.

> 
> HTH
> Petr
> 
> 
> 
> On 4 Aug 2006 at 17:06, chiya sharma wrote:
> 
> Date sent:      	Fri, 4 Aug 2006 17:06:38 +0530
> From:           	"chiya sharma" <chiya31 at gmail.com>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] User input from keyboard
> 
> > Dear All,
> > 
> >  Can anybody tell me the syntax for "User input from keyboard" in R. I
> >  mean
> > to say that if I run the program it should ask "Please enter the date"
> > at the begining of the program. I am using R-2.2.1 for windows.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From MSchwartz at mn.rr.com  Fri Aug  4 14:57:25 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 04 Aug 2006 07:57:25 -0500
Subject: [R] Sweave special token \\ from R to latex
In-Reply-To: <20060804144005.fu4o7us7erk0wooo@webmail4.kuleuven.be>
References: <20060804144005.fu4o7us7erk0wooo@webmail4.kuleuven.be>
Message-ID: <1154696245.4554.9.camel@localhost.localdomain>

On Fri, 2006-08-04 at 14:40 +0200, Jan Wijffels wrote:
> Dear helpeRs,
> 
> I would like to specify a newline command in R and pass it to latex  
> via Sweave such that it corresponds to latex' \\ command. But that  
> doesn't seem to be possible. If I Sweave the \n character, it just  
> makes a new line in latex but not the \\ command.
> 
> Is there a way such that the following code would result in latex in
> blablabla \\ blablabla on different line
> 
> 
> <<echo=FALSE>>=
>    string <- "blablabla \\ blablabla on different line"
> @
> \Sexpr{string}
> 
> 
> 
> 
> Thanks for the help,
> Jan

In R, the "\" is an escape character, so to get a single "\" output, you
actually need to double it, "\\".

Thus, to get "\\" output as part of a character vector, you would need
"\\\\":

> string <- "blablabla \\\\ blablabla on different line"

> cat(string, "\n")
blablabla \\ blablabla on different line


HTH,

Marc Schwartz


From markus.jantti at iki.fi  Fri Aug  4 15:07:32 2006
From: markus.jantti at iki.fi (Markus Jntti)
Date: Fri, 04 Aug 2006 16:07:32 +0300
Subject: [R] Sweave special token \\ from R to latex
In-Reply-To: <20060804144005.fu4o7us7erk0wooo@webmail4.kuleuven.be>
References: <20060804144005.fu4o7us7erk0wooo@webmail4.kuleuven.be>
Message-ID: <44D34694.2060002@iki.fi>

Jan Wijffels wrote:
> Dear helpeRs,
>
> I would like to specify a newline command in R and pass it to latex
> via Sweave such that it corresponds to latex' \\ command. But that
> doesn't seem to be possible. If I Sweave the \n character, it just
> makes a new line in latex but not the \\ command.
>
> Is there a way such that the following code would result in latex in
> blablabla \\ blablabla on different line
>
>
> <<echo=FALSE>>=
>    string <- "blablabla \\ blablabla on different line"
> @
> \Sexpr{string}
>

For reasons that I am unable to account for, you get the desired string
in your .tex file if you do

<<echo=FALSE>>=

    string <- "blablabla \\\\\\\\ blablabla on different line"

@

\Sexpr{print(string)}

resulting in the .tex file as

blablabla \\ blablabla on different line


It would seem that using <<echo=FALSE,results=tex>>
would be a good solution.

Markus

>
>
>
> Thanks for the help,
> Jan
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti
###########################################

This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}


From anthony at stat.sdu.dk  Fri Aug  4 15:12:36 2006
From: anthony at stat.sdu.dk (Gichangi, Anthony)
Date: Fri, 4 Aug 2006 15:12:36 +0200
Subject: [R] plotting picture data
Message-ID: <001501c6b7c7$a75e7c40$ce83e182@yatesvmware>

Hi R users

I have a dataset which represents points that are market by patients as the
source of pain.
Basically the patients indicates by a cross on a chest pictures where he/she
thinks is the
source of pain. The data was then digitalized by divinding the chest into
small squares and each
square was give value 1 if it was the center 2 if it was touched by the
markings and 3 if it was not
touched.  I would like to plot this data on the chest like graph showing the
intesities of different
points and later stratify the grouping variables to see the difference.

Has anybody got an idea how I can go around this ?

Help is highly appreciated.

Regards

Anthony Gichangi, M. sc.
Department of Statistics.
JB. Winsl?vej 9B,
DK 5000 Odense C.
Tel:       00 45 6550 3379
Mobile:  00 45 61105805


From ritwik.sinha at gmail.com  Fri Aug  4 15:38:04 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Fri, 4 Aug 2006 09:38:04 -0400
Subject: [R] plotting picture data
In-Reply-To: <001501c6b7c7$a75e7c40$ce83e182@yatesvmware>
References: <001501c6b7c7$a75e7c40$ce83e182@yatesvmware>
Message-ID: <42bc98300608040638w73f1a7f6nbb5456064fedccd1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/cd78ef0c/attachment.pl 

From petr.pikal at precheza.cz  Fri Aug  4 15:51:16 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 04 Aug 2006 15:51:16 +0200
Subject: [R] plotting picture data
In-Reply-To: <001501c6b7c7$a75e7c40$ce83e182@yatesvmware>
Message-ID: <44D36CF4.13626.1498B96@localhost>

Hi

seems to me that it can be done by image. See ?image.

Just as illustration.
mat<-matrix(sample(c(1,rep(2,10), rep(3,50)), 1000, replace=T), 
100,100)
for(i in 1:6) mat[i,c(1:(50-5*i),(50+5*i):100)]<-NA
for(i in 14:9) mat[i,c(1:(50-5*(15-i)),(50+5*(15-i)):100)]<-NA
image(1:100,1:100,mat)

HTH
Petr


On 4 Aug 2006 at 15:12, Gichangi, Anthony wrote:

From:           	"Gichangi, Anthony" <anthony at stat.sdu.dk>
To:             	"R-help" <r-help at stat.math.ethz.ch>
Date sent:      	Fri, 4 Aug 2006 15:12:36 +0200
Subject:        	[R] plotting picture data

> Hi R users
> 
> I have a dataset which represents points that are market by patients
> as the source of pain. Basically the patients indicates by a cross on
> a chest pictures where he/she thinks is the source of pain. The data
> was then digitalized by divinding the chest into small squares and
> each square was give value 1 if it was the center 2 if it was touched
> by the markings and 3 if it was not touched.  I would like to plot
> this data on the chest like graph showing the intesities of different
> points and later stratify the grouping variables to see the
> difference.
> 
> Has anybody got an idea how I can go around this ?
> 
> Help is highly appreciated.
> 
> Regards
> 
> Anthony Gichangi, M. sc.
> Department of Statistics.
> JB. Winsl?vej 9B,
> DK 5000 Odense C.
> Tel:       00 45 6550 3379
> Mobile:  00 45 61105805
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From tlumley at u.washington.edu  Fri Aug  4 16:16:26 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 4 Aug 2006 07:16:26 -0700 (PDT)
Subject: [R] read.spss and umlaut
In-Reply-To: <200608040820.13403.r@fam-kuster.ch>
References: <200608021531.56340.r@fam-kuster.ch>
	<200608031038.14141.r@fam-kuster.ch>
	<Pine.LNX.4.64.0608030622130.20946@homer23.u.washington.edu>
	<200608040820.13403.r@fam-kuster.ch>
Message-ID: <Pine.LNX.4.64.0608040712510.1404@homer23.u.washington.edu>

On Fri, 4 Aug 2006, Thomas Kuster wrote:
>
> If I change the translatable characters (overwrite the 0 between :#@'=" and
> ~000 with ??????). I can read in the file an every ?????? ist a withspace:

Ok, that's the problem then.  The file format says that the umlauts are 
unreadable and R is believing the file format.

I will look at adding an option to specify an encoding and ignore the 
translation table, but not very urgently.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From jessn2 at hotmail.com  Fri Aug  4 17:12:24 2006
From: jessn2 at hotmail.com (Jessica G.)
Date: Fri, 04 Aug 2006 17:12:24 +0200
Subject: [R] question
Message-ID: <BAY102-F31BE714A68A119AAFCBE8D9C500@phx.gbl>

Hi Mr Plate,

I have a little question

How to convert a rowname vector of numbers into a real column of the matrix,

My problem is that I applied a rowsum function on a matrix.
Then I get a matrix in which the names of the columns are the values of the 
group (numbers)
Now I need to make calculation on the groups row.
How to convert this vector of (rownames) into a real column in the matrix ?


thanks by advance

Jessica Gervais

_________________________________________________________________

www.windowslivemessenger.fr


From akniss at uwyo.edu  Fri Aug  4 17:47:06 2006
From: akniss at uwyo.edu (Andrew Kniss)
Date: Fri, 04 Aug 2006 09:47:06 -0600
Subject: [R] expression() - Superscript in y-axis,
	keeping line break in string
Message-ID: <1154706426.19124.12.camel@localhost>

I've tried several different ways to accomplish this, but as yet to no
avail.  My y-axis for a plot has a rather long label, and thus I have
been using "/n" to break it into two lines.  However, to make it
technically correct for publication, I also need to use superscript in
the label.  For example:

     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
     plot(1:10,
          ylab="14C-glyphosate line1\n line2")

will provide the text in two lines as I would like it.  However, I am
trying to keep those same line breaks when using expression() to get my
superscript number.  This will not work, as it aligns the "14C" section
with the bottom line of the expression making little sense to the
reader.

     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
     plot(1:10,
          ylab=expression(" "^14*C*"-glyphosate line1\n line2"))

Is there a way to align the "14C" portion of the expression with the top
line of the string rather than the bottom line?  Any suggestions are
greatly appreciated.
Andrew


-- 
Andrew Kniss
Assistant Research Scientist
University of Wyoming 
Department of Plant Sciences

akniss at uwyo.edu
Office: (307) 766-3949
Fax:    (307) 766-5549


From mschwartz at mn.rr.com  Fri Aug  4 18:01:06 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 04 Aug 2006 11:01:06 -0500
Subject: [R] expression() - Superscript in y-axis,
	keeping line break	in string
In-Reply-To: <1154706426.19124.12.camel@localhost>
References: <1154706426.19124.12.camel@localhost>
Message-ID: <1154707267.4089.11.camel@localhost.localdomain>

On Fri, 2006-08-04 at 09:47 -0600, Andrew Kniss wrote:
> I've tried several different ways to accomplish this, but as yet to no
> avail.  My y-axis for a plot has a rather long label, and thus I have
> been using "/n" to break it into two lines.  However, to make it
> technically correct for publication, I also need to use superscript in
> the label.  For example:
> 
>      par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>      plot(1:10,
>           ylab="14C-glyphosate line1\n line2")
> 
> will provide the text in two lines as I would like it.  However, I am
> trying to keep those same line breaks when using expression() to get my
> superscript number.  This will not work, as it aligns the "14C" section
> with the bottom line of the expression making little sense to the
> reader.
> 
>      par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>      plot(1:10,
>           ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
> 
> Is there a way to align the "14C" portion of the expression with the top
> line of the string rather than the bottom line?  Any suggestions are
> greatly appreciated.
> Andrew

plotmath, as has been covered many times previously, does not support
multi-line expressions. A note should probably be added to ?plotmath on
this.

Thus, you need to create each line in the label separately:

  par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
  
  plot(1:10, ylab = "")

  # Now use mtext() to place each line of the y axis label

  mtext(2, text = expression(" "^14*C*"-glyphosate line1"), line = 3)

  mtext(2, text = "line2", line = 2)

See ?mtext for more information.

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Fri Aug  4 18:04:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 4 Aug 2006 12:04:45 -0400
Subject: [R] expression() - Superscript in y-axis,
	keeping line break in string
In-Reply-To: <1154706426.19124.12.camel@localhost>
References: <1154706426.19124.12.camel@localhost>
Message-ID: <971536df0608040904q3fe1a9bbn3581bad2c513d335@mail.gmail.com>

Use atop:

   plot(1, main = expression(atop(" "^14*C*"-glyphosate line", "line2")))

On 8/4/06, Andrew Kniss <akniss at uwyo.edu> wrote:
> I've tried several different ways to accomplish this, but as yet to no
> avail.  My y-axis for a plot has a rather long label, and thus I have
> been using "/n" to break it into two lines.  However, to make it
> technically correct for publication, I also need to use superscript in
> the label.  For example:
>
>     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>     plot(1:10,
>          ylab="14C-glyphosate line1\n line2")
>
> will provide the text in two lines as I would like it.  However, I am
> trying to keep those same line breaks when using expression() to get my
> superscript number.  This will not work, as it aligns the "14C" section
> with the bottom line of the expression making little sense to the
> reader.
>
>     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>     plot(1:10,
>          ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
>
> Is there a way to align the "14C" portion of the expression with the top
> line of the string rather than the bottom line?  Any suggestions are
> greatly appreciated.
> Andrew
>
>
> --
> Andrew Kniss
> Assistant Research Scientist
> University of Wyoming
> Department of Plant Sciences
>
> akniss at uwyo.edu
> Office: (307) 766-3949
> Fax:    (307) 766-5549
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Qinghong.Li at rdmo.nestle.com  Fri Aug  4 17:57:26 2006
From: Qinghong.Li at rdmo.nestle.com (Li,Qinghong,ST.LOUIS,Molecular Biology)
Date: Fri, 4 Aug 2006 10:57:26 -0500
Subject: [R] meta characters in file path
Message-ID: <2BA2B7291D5DC6409FD53CB7C01F0D990183B88F@usslre00.nestle.com>

Thanks. I tried them, it works for most of those characters except "*" and "?". 

Does regular expression work in file names in windows? e.g. I have a machine-generated file named "021706 matrix#1479 @50.csv", of which "1479" is kinda random. Will I be able to match "1479" with some sort of "wild card" chars?

Thanks
Johnny

-----Original Message-----
From: Tony Plate [mailto:tplate at acm.org]
Sent: Thursday, August 03, 2006 3:42 PM
To: Li,Qinghong,ST.LOUIS,Molecular Biology
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] meta characters in file path


What is the problem you are having?  Seems to work fine for me running 
under Windows2000:

 > write.table(data.frame(a=1:3,b=4:6), file="@# x.csv", sep=",")
 > read.csv(file="@# x.csv")
   a b
1 1 4
2 2 5
3 3 6
 > sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      XML
"0.99-8"
 >

Li,Qinghong,ST.LOUIS,Molecular Biology wrote:
> Hi,
> 
> I need to read in some files. The file names contain come meta characters such as @, #, and white spaces etc, In read.csv, file= option, is there any way that one can make the function to recognize a file path with those characters?
> 
> Thanks
> Johnny
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Aug  4 18:09:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 4 Aug 2006 12:09:42 -0400
Subject: [R] expression() - Superscript in y-axis,
	keeping line break in string
In-Reply-To: <971536df0608040904q3fe1a9bbn3581bad2c513d335@mail.gmail.com>
References: <1154706426.19124.12.camel@localhost>
	<971536df0608040904q3fe1a9bbn3581bad2c513d335@mail.gmail.com>
Message-ID: <971536df0608040909w14174d2aqb5e69c1fd7003833@mail.gmail.com>

Sorry, you wanted a ylab=, not a main=.  Try using xyplot in lattice:

library(lattice)
xyplot(1~1, ylab = expression(atop(phantom(0)^14*C*"-glyphosate line",
"line2")))


On 8/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Use atop:
>
>   plot(1, main = expression(atop(" "^14*C*"-glyphosate line", "line2")))
>
> On 8/4/06, Andrew Kniss <akniss at uwyo.edu> wrote:
> > I've tried several different ways to accomplish this, but as yet to no
> > avail.  My y-axis for a plot has a rather long label, and thus I have
> > been using "/n" to break it into two lines.  However, to make it
> > technically correct for publication, I also need to use superscript in
> > the label.  For example:
> >
> >     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> >     plot(1:10,
> >          ylab="14C-glyphosate line1\n line2")
> >
> > will provide the text in two lines as I would like it.  However, I am
> > trying to keep those same line breaks when using expression() to get my
> > superscript number.  This will not work, as it aligns the "14C" section
> > with the bottom line of the expression making little sense to the
> > reader.
> >
> >     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> >     plot(1:10,
> >          ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
> >
> > Is there a way to align the "14C" portion of the expression with the top
> > line of the string rather than the bottom line?  Any suggestions are
> > greatly appreciated.
> > Andrew
> >
> >
> > --
> > Andrew Kniss
> > Assistant Research Scientist
> > University of Wyoming
> > Department of Plant Sciences
> >
> > akniss at uwyo.edu
> > Office: (307) 766-3949
> > Fax:    (307) 766-5549
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From mschwartz at mn.rr.com  Fri Aug  4 18:22:21 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 04 Aug 2006 11:22:21 -0500
Subject: [R] expression() - Superscript in y-axis,
	keeping line break	in string
In-Reply-To: <971536df0608040909w14174d2aqb5e69c1fd7003833@mail.gmail.com>
References: <1154706426.19124.12.camel@localhost>
	<971536df0608040904q3fe1a9bbn3581bad2c513d335@mail.gmail.com>
	<971536df0608040909w14174d2aqb5e69c1fd7003833@mail.gmail.com>
Message-ID: <1154708541.4089.18.camel@localhost.localdomain>

Actually Gabor, using your solution with 'atop', which I had not
considered, it will work with base graphics:

 par(oma = c(0, 0, 2, 0), mar = c(5, 6, 0.25, 2), lheight = 1)

 plot(1:10, ylab = expression(atop(" "^14*C*"-glyphosate line1",
                                   line2)))

HTH,

Marc

On Fri, 2006-08-04 at 12:09 -0400, Gabor Grothendieck wrote:
> Sorry, you wanted a ylab=, not a main=.  Try using xyplot in lattice:
> 
> library(lattice)
> xyplot(1~1, ylab = expression(atop(phantom(0)^14*C*"-glyphosate line",
> "line2")))
> 
> 
> On 8/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Use atop:
> >
> >   plot(1, main = expression(atop(" "^14*C*"-glyphosate line", "line2")))
> >
> > On 8/4/06, Andrew Kniss <akniss at uwyo.edu> wrote:
> > > I've tried several different ways to accomplish this, but as yet to no
> > > avail.  My y-axis for a plot has a rather long label, and thus I have
> > > been using "/n" to break it into two lines.  However, to make it
> > > technically correct for publication, I also need to use superscript in
> > > the label.  For example:
> > >
> > >     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> > >     plot(1:10,
> > >          ylab="14C-glyphosate line1\n line2")
> > >
> > > will provide the text in two lines as I would like it.  However, I am
> > > trying to keep those same line breaks when using expression() to get my
> > > superscript number.  This will not work, as it aligns the "14C" section
> > > with the bottom line of the expression making little sense to the
> > > reader.
> > >
> > >     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> > >     plot(1:10,
> > >          ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
> > >
> > > Is there a way to align the "14C" portion of the expression with the top
> > > line of the string rather than the bottom line?  Any suggestions are
> > > greatly appreciated.
> > > Andrew
> > >


From ssj1364 at gmail.com  Fri Aug  4 18:33:52 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Fri, 4 Aug 2006 10:33:52 -0600
Subject: [R] Simulate an Overdispersed(extra-variance poisson process)?
Message-ID: <1c6126db0608040933r530be4dcu17b4b90c318c109f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/07625385/attachment.pl 

From akniss at gmail.com  Fri Aug  4 18:44:35 2006
From: akniss at gmail.com (Andrew Kniss)
Date: Fri, 04 Aug 2006 10:44:35 -0600
Subject: [R] expression() - Superscript in y-axis,
 keeping line break in string
In-Reply-To: <1154708541.4089.18.camel@localhost.localdomain>
References: <1154706426.19124.12.camel@localhost>	
	<971536df0608040904q3fe1a9bbn3581bad2c513d335@mail.gmail.com>	
	<971536df0608040909w14174d2aqb5e69c1fd7003833@mail.gmail.com>
	<1154708541.4089.18.camel@localhost.localdomain>
Message-ID: <44D37973.5020802@gmail.com>

Thank you Marc and Gabor.  Both suggestions work well.  I will use the 
'atop' solution, as it requires the least amount of typing to change my 
current ylabs.
Andrew

Marc Schwartz (via MN) wrote:
> Actually Gabor, using your solution with 'atop', which I had not
> considered, it will work with base graphics:
> 
>  par(oma = c(0, 0, 2, 0), mar = c(5, 6, 0.25, 2), lheight = 1)
> 
>  plot(1:10, ylab = expression(atop(" "^14*C*"-glyphosate line1",
>                                    line2)))
> 
> HTH,
> 
> Marc
> 
> On Fri, 2006-08-04 at 12:09 -0400, Gabor Grothendieck wrote:
>> Sorry, you wanted a ylab=, not a main=.  Try using xyplot in lattice:
>>
>> library(lattice)
>> xyplot(1~1, ylab = expression(atop(phantom(0)^14*C*"-glyphosate line",
>> "line2")))
>>
>>
>> On 8/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>> Use atop:
>>>
>>>   plot(1, main = expression(atop(" "^14*C*"-glyphosate line", "line2")))
>>>
>>> On 8/4/06, Andrew Kniss <akniss at uwyo.edu> wrote:
>>>> I've tried several different ways to accomplish this, but as yet to no
>>>> avail.  My y-axis for a plot has a rather long label, and thus I have
>>>> been using "/n" to break it into two lines.  However, to make it
>>>> technically correct for publication, I also need to use superscript in
>>>> the label.  For example:
>>>>
>>>>     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>>>>     plot(1:10,
>>>>          ylab="14C-glyphosate line1\n line2")
>>>>
>>>> will provide the text in two lines as I would like it.  However, I am
>>>> trying to keep those same line breaks when using expression() to get my
>>>> superscript number.  This will not work, as it aligns the "14C" section
>>>> with the bottom line of the expression making little sense to the
>>>> reader.
>>>>
>>>>     par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>>>>     plot(1:10,
>>>>          ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
>>>>
>>>> Is there a way to align the "14C" portion of the expression with the top
>>>> line of the string rather than the bottom line?  Any suggestions are
>>>> greatly appreciated.
>>>> Andrew
>>>>
>


From dgerlanc at gmail.com  Fri Aug  4 18:46:15 2006
From: dgerlanc at gmail.com (Daniel Gerlanc)
Date: Fri, 4 Aug 2006 12:46:15 -0400
Subject: [R] Sampling from a Matrix
Message-ID: <84c9e3cb0608040946l4f02881boa809947e0676a3e6@mail.gmail.com>

Hello all,

Consider the following problem:

There is a matrix of probabilities:

> set.seed(1)
> probs <- array(abs(rnorm(25, sd = 0.33)), dim = c(5,5), dimnames = list(1:5, letters[1:5]))
> probs
        a      b       c         d        e
1 0.21 0.27 0.50 0.0148 0.303
2 0.06 0.16 0.13 0.0053 0.258
3 0.28 0.24 0.21 0.3115 0.025
4 0.53 0.19 0.73 0.2710 0.656
5 0.11 0.10 0.37 0.1960 0.205

I want to sample 3 values from each row.

One way to do this follows:

index <- 1:ncol(probs)

for(i in 1:nrow(probs)){

## gets the indexes of the values chosen

sample(index, size = 3, replace = TRUE, prob = probs[i, ])

}

Is there a another way to do this?

Thanks!

Dan Gerlanc

-- 
Daniel Gerlanc
Williams College '07


From wuming.gong at gmail.com  Fri Aug  4 18:47:02 2006
From: wuming.gong at gmail.com (Wuming Gong)
Date: Sat, 5 Aug 2006 00:47:02 +0800
Subject: [R] Error when loading odesolve
Message-ID: <b428d06d0608040947l3990c780w9a38a61edf67b8ee@mail.gmail.com>

Dear list,

I installed odesolve package (0.5-15) in R 2.3.1 in a Solaris server
(Generic_118558-11 sun4u sparc SUNW,Sun-Blade-1000).  The installing
progress completed without errors, though several warnings like
"Warning: Option -fPIC passed to ld, if ld is invoked, ignored
otherwise" were outputed.

However, when loading the odesolve package by library(odesolve),
following error messages pop out:

> library(odesolve)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/project/scratch/ligroup/R1/lib/R/library/odesolve/libs/odesolve.so':
  ld.so.1: R: fatal: relocation error: file
/project/scratch/ligroup/R1/lib/R/library/odesolve/libs/odesolve.so:
symbol __f90_ssfw: referenced symbol not found
Error: package/namespace load failed for 'odesolve'

Could any one tell me how to fix this problem?

Thanks very much.

Wuming


From mschwartz at mn.rr.com  Fri Aug  4 18:49:52 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 04 Aug 2006 11:49:52 -0500
Subject: [R] Simulate an Overdispersed(extra-variance poisson process)?
In-Reply-To: <1c6126db0608040933r530be4dcu17b4b90c318c109f@mail.gmail.com>
References: <1c6126db0608040933r530be4dcu17b4b90c318c109f@mail.gmail.com>
Message-ID: <1154710192.4089.22.camel@localhost.localdomain>

On Fri, 2006-08-04 at 10:33 -0600, Spencer Jones wrote:
> Is there a function in R comparable to rpois that can simulate random
> variables from an overdispersed poisson distribution? If there is not a
> function any ideas/references on how to program one?

Take a look at

  ?rnbinom

or

  library(MASS)
  ?rnegbin


HTH,

Marc Schwartz


From mschwartz at mn.rr.com  Fri Aug  4 19:00:15 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 04 Aug 2006 12:00:15 -0500
Subject: [R] Sampling from a Matrix
In-Reply-To: <84c9e3cb0608040946l4f02881boa809947e0676a3e6@mail.gmail.com>
References: <84c9e3cb0608040946l4f02881boa809947e0676a3e6@mail.gmail.com>
Message-ID: <1154710815.4089.25.camel@localhost.localdomain>

On Fri, 2006-08-04 at 12:46 -0400, Daniel Gerlanc wrote:
> Hello all,
> 
> Consider the following problem:
> 
> There is a matrix of probabilities:
> 
> > set.seed(1)
> > probs <- array(abs(rnorm(25, sd = 0.33)), dim = c(5,5), dimnames = list(1:5, letters[1:5]))
> > probs
>         a      b       c         d        e
> 1 0.21 0.27 0.50 0.0148 0.303
> 2 0.06 0.16 0.13 0.0053 0.258
> 3 0.28 0.24 0.21 0.3115 0.025
> 4 0.53 0.19 0.73 0.2710 0.656
> 5 0.11 0.10 0.37 0.1960 0.205
> 
> I want to sample 3 values from each row.
> 
> One way to do this follows:
> 
> index <- 1:ncol(probs)
> 
> for(i in 1:nrow(probs)){
> 
> ## gets the indexes of the values chosen
> 
> sample(index, size = 3, replace = TRUE, prob = probs[i, ])
> 
> }
> 
> Is there a another way to do this?
> 
> Thanks!

> t(apply(probs, 1, function(x) sample(x, 3)))
   [,1]   [,2]   [,3]
1 0.210 0.5000 0.0148
2 0.258 0.0053 0.1300
3 0.025 0.2800 0.3115
4 0.190 0.5300 0.2710
5 0.196 0.1000 0.1100

See ?apply and ?t

HTH,

Marc Schwartz


From bahamon at icm.csic.es  Fri Aug  4 19:01:18 2006
From: bahamon at icm.csic.es (Nixon Bahamon)
Date: Fri, 04 Aug 2006 19:01:18 +0200
Subject: [R] GAM 2D-plotting
Message-ID: <7.0.1.0.0.20060804185227.01a69218@icm.csic.es>

Hi,

When I fit a GAM model (using mgvc) with overlapping terms, such as

gam(y~s(x,z)+s(z,w))

and afterwards I pretend to plot the component smooth functions that make it up
using plot.gam, I achieve a couple of 2D plots.

My question is: What's the meaning of those 2D plots in terms of y?

Regards,

Nixon


From jessiet at stanford.edu  Fri Aug  4 19:20:20 2006
From: jessiet at stanford.edu (Jessie Tenenbaum)
Date: Fri, 4 Aug 2006 10:20:20 -0700
Subject: [R] training svm's with probability flag
Message-ID: <013301c6b7ea$4317c7a0$ac0941ab@JESSIET>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/d8197bf8/attachment.pl 

From jhainm at fas.harvard.edu  Fri Aug  4 19:26:49 2006
From: jhainm at fas.harvard.edu (Jens Hainmueller)
Date: Fri, 4 Aug 2006 19:26:49 +0200
Subject: [R] why does lm() not allow for negative weights?
Message-ID: <000a01c6b7eb$2e7624d0$0501a8c0@JANK>

Dear List,

Why do commonly used estimator functions (such as lm(), glm(), etc.) not
allow negative case weights? I suspect that there is a good reason for this.
Yet, I can see reasonable cases when one wants to use negative case weights.

Take lm() for example:

###

n <- 20
Y <- rnorm(n)
X <- cbind(rep(1,n),runif(n),rnorm(n))
Weights <- rnorm(n)
# Includes Pos and Neg Weights
Weights

# Now do Weighted LS and get beta coeffs:
b <- solve(t(X)%*%diag(Weights)%*%X) %*% t(X) %*% diag(Weights)%*%Y
b

# This seems like a valid model, but when I try
lm(Y ~ X[,2:3],weights=Weights)

# I get: "missing or negative weights not allowed"

###

What is the rationale for not allowing negative weights? I ask this, because
I am currently trying to implement a (two stage) estimator into R that
involves negative case weights. Weights are generated in the first stage, so
it would be nice if I could use canned functions such as
lm(,weights=Weights) in the second stage.

Thank you for your help.

Best,
Jens


From ripley at stats.ox.ac.uk  Fri Aug  4 19:27:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Aug 2006 18:27:04 +0100 (BST)
Subject: [R] meta characters in file path
In-Reply-To: <2BA2B7291D5DC6409FD53CB7C01F0D990183B88F@usslre00.nestle.com>
References: <2BA2B7291D5DC6409FD53CB7C01F0D990183B88F@usslre00.nestle.com>
Message-ID: <Pine.LNX.4.64.0608041821040.22596@gannet.stats.ox.ac.uk>

On Fri, 4 Aug 2006, Li,Qinghong,ST.LOUIS,Molecular Biology wrote:

> Thanks. I tried them, it works for most of those characters except "*" 
> and "?".

Those are not valid characters in Windows file paths
(/  " * : < > ? \ | are invalid in file or dir names).

> Does regular expression work in file names in windows? 

No, and I think you may mean wildcards (which is what work on the command 
line).

> e.g. I have a machine-generated file named "021706 matrix#1479 @50.csv", 
> of which "1479" is kinda random. Will I be able to match "1479" with 
> some sort of "wild card" chars?

Yes, use dir(), with regexp pattern patching to find the name(s) you 
want.  glob2rx() might be useful here.

> Thanks
> Johnny
> 
> -----Original Message-----
> From: Tony Plate [mailto:tplate at acm.org]
> Sent: Thursday, August 03, 2006 3:42 PM
> To: Li,Qinghong,ST.LOUIS,Molecular Biology
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] meta characters in file path
> 
> 
> What is the problem you are having?  Seems to work fine for me running 
> under Windows2000:
> 
>  > write.table(data.frame(a=1:3,b=4:6), file="@# x.csv", sep=",")
>  > read.csv(file="@# x.csv")
>    a b
> 1 1 4
> 2 2 5
> 3 3 6
>  > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> 
> other attached packages:
>       XML
> "0.99-8"
>  >
> 
> Li,Qinghong,ST.LOUIS,Molecular Biology wrote:
> > Hi,
> > 
> > I need to read in some files. The file names contain come meta characters such as @, #, and white spaces etc, In read.csv, file= option, is there any way that one can make the function to recognize a file path with those characters?
> > 
> > Thanks
> > Johnny
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Aug  4 19:33:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Aug 2006 18:33:02 +0100 (BST)
Subject: [R] Error when loading odesolve
In-Reply-To: <b428d06d0608040947l3990c780w9a38a61edf67b8ee@mail.gmail.com>
References: <b428d06d0608040947l3990c780w9a38a61edf67b8ee@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608041827540.22596@gannet.stats.ox.ac.uk>

Looking at odesolve, src/Makevars is

PKG_LIBS=$(BLAS_LIBS)

Now, the documentation says that if you have $(BLAS_LIBS) you must also 
have $(FLIBS), so please change this to

PKG_LIBS=$(BLAS_LIBS) $(FLIBS)

and take this up with the package maintainer (which is what the posting 
guide asked you to do in the first place).


On Sat, 5 Aug 2006, Wuming Gong wrote:

> Dear list,
> 
> I installed odesolve package (0.5-15) in R 2.3.1 in a Solaris server
> (Generic_118558-11 sun4u sparc SUNW,Sun-Blade-1000).  The installing
> progress completed without errors, though several warnings like
> "Warning: Option -fPIC passed to ld, if ld is invoked, ignored
> otherwise" were outputed.
> 
> However, when loading the odesolve package by library(odesolve),
> following error messages pop out:
> 
> > library(odesolve)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> '/project/scratch/ligroup/R1/lib/R/library/odesolve/libs/odesolve.so':
>   ld.so.1: R: fatal: relocation error: file
> /project/scratch/ligroup/R1/lib/R/library/odesolve/libs/odesolve.so:
> symbol __f90_ssfw: referenced symbol not found
> Error: package/namespace load failed for 'odesolve'
> 
> Could any one tell me how to fix this problem?
> 
> Thanks very much.
> 
> Wuming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Fri Aug  4 19:35:51 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 04 Aug 2006 13:35:51 -0400
Subject: [R] why does lm() not allow for negative weights?
In-Reply-To: <000a01c6b7eb$2e7624d0$0501a8c0@JANK>
References: <000a01c6b7eb$2e7624d0$0501a8c0@JANK>
Message-ID: <44D38577.3030405@stats.uwo.ca>

On 8/4/2006 1:26 PM, Jens Hainmueller wrote:
> Dear List,
> 
> Why do commonly used estimator functions (such as lm(), glm(), etc.) not
> allow negative case weights? 

Residual sums of squares (or deviances) could be negative with negative 
case weights.  This doesn't seem like a good thing:  would you really 
want the fit to be far from those points?

 > I suspect that there is a good reason for this.
> Yet, I can see reasonable cases when one wants to use negative case weights.
> 
> Take lm() for example:
> 
> ###
> 
> n <- 20
> Y <- rnorm(n)
> X <- cbind(rep(1,n),runif(n),rnorm(n))
> Weights <- rnorm(n)
> # Includes Pos and Neg Weights
> Weights
> 
> # Now do Weighted LS and get beta coeffs:
> b <- solve(t(X)%*%diag(Weights)%*%X) %*% t(X) %*% diag(Weights)%*%Y

That formula does not necessarily give least squares estimates in the 
case where weights might be negative.  For example, with a single 
observation y, a single parameter mu, design matrix X = 1, and weight 
-1, that formula becomes

b <- y,

but that is the worst possible estimator in a least squares sense.  The 
residual sum of squares can be made arbitrarily large and negative by 
setting b to a large value.

Duncan Murdoch


> b
> 
> # This seems like a valid model, but when I try
> lm(Y ~ X[,2:3],weights=Weights)
> 
> # I get: "missing or negative weights not allowed"
> 
> ###
> 
> What is the rationale for not allowing negative weights? I ask this, because
> I am currently trying to implement a (two stage) estimator into R that
> involves negative case weights. Weights are generated in the first stage, so
> it would be nice if I could use canned functions such as
> lm(,weights=Weights) in the second stage.
> 
> Thank you for your help.
> 
> Best,
> Jens
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jhainm at fas.harvard.edu  Fri Aug  4 19:45:28 2006
From: jhainm at fas.harvard.edu (Jens Hainmueller)
Date: Fri, 4 Aug 2006 19:45:28 +0200
Subject: [R] why does lm() not allow for negative weights?
In-Reply-To: <44D38577.3030405@stats.uwo.ca>
Message-ID: <000b01c6b7ed$c828d850$0501a8c0@JANK>

Thanks Duncan Murdoch,

> > Why do commonly used estimator functions (such as lm(), 
> > glm(), etc.) 
> > not allow negative case weights?
 
> Residual sums of squares (or deviances) could be negative 
> with negative case weights.  This doesn't seem like a good 
> thing:  would you really want the fit to be far from those points?

Yes, this is actually what I want for this particular estimator. But I can
see now why this generally doesn't seem like a a good idea.

Best,
Jens



> -----Urspr?ngliche Nachricht-----
> Von: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Gesendet: Friday, August 04, 2006 7:36 PM
> An: Jens Hainmueller
> Cc: r-help at stat.math.ethz.ch
> Betreff: Re: [R] why does lm() not allow for negative weights?
> 
> On 8/4/2006 1:26 PM, Jens Hainmueller wrote:
> > Dear List,
> > 

> 

> 
>  > I suspect that there is a good reason for this.
> > Yet, I can see reasonable cases when one wants to use 
> negative case weights.
> > 
> > Take lm() for example:
> > 
> > ###
> > 
> > n <- 20
> > Y <- rnorm(n)
> > X <- cbind(rep(1,n),runif(n),rnorm(n)) Weights <- rnorm(n) 
> # Includes 
> > Pos and Neg Weights Weights
> > 
> > # Now do Weighted LS and get beta coeffs:
> > b <- solve(t(X)%*%diag(Weights)%*%X) %*% t(X) %*% diag(Weights)%*%Y
> 
> That formula does not necessarily give least squares 
> estimates in the case where weights might be negative.  For 
> example, with a single observation y, a single parameter mu, 
> design matrix X = 1, and weight -1, that formula becomes
> 
> b <- y,
> 
> but that is the worst possible estimator in a least squares 
> sense.  The residual sum of squares can be made arbitrarily 
> large and negative by setting b to a large value.
> 
> Duncan Murdoch
> 
> 
> > b
> > 
> > # This seems like a valid model, but when I try lm(Y ~ 
> > X[,2:3],weights=Weights)
> > 
> > # I get: "missing or negative weights not allowed"
> > 
> > ###
> > 
> > What is the rationale for not allowing negative weights? I 
> ask this, 
> > because I am currently trying to implement a (two stage) estimator 
> > into R that involves negative case weights. Weights are 
> generated in 
> > the first stage, so it would be nice if I could use canned 
> functions 
> > such as
> > lm(,weights=Weights) in the second stage.
> > 
> > Thank you for your help.
> > 
> > Best,
> > Jens
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From edd at debian.org  Fri Aug  4 20:10:09 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 4 Aug 2006 13:10:09 -0500
Subject: [R] why does lm() not allow for negative weights?
In-Reply-To: <000a01c6b7eb$2e7624d0$0501a8c0@JANK>
References: <000a01c6b7eb$2e7624d0$0501a8c0@JANK>
Message-ID: <17619.36225.960874.811479@basebud.nulle.part>


On 4 August 2006 at 19:26, Jens Hainmueller wrote:
| Why do commonly used estimator functions (such as lm(), glm(), etc.) not
| allow negative case weights? I suspect that there is a good reason for this.

That came up on r-sig-finance a little while ago. As usual, Gabor won the
contest for most precise and concise answer with:

   [..] At any rate, note that if the weights can be negative then the sum of
   squares to be optimized is no longer a convex function of the coefficients
   so we really don't have a conventional least squares model and uniqueness
   and existence have possibly different answers.

See the r-sig-finance archives for April 2006 and a thread entitled 'negative
weights'. 

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From rab45+ at pitt.edu  Fri Aug  4 20:22:51 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Fri, 04 Aug 2006 14:22:51 -0400
Subject: [R] prettyR arrives
In-Reply-To: <44D3DA3E.6040204@bitwrit.com.au>
References: <44D3DA3E.6040204@bitwrit.com.au>
Message-ID: <1154715771.3719.8.camel@localhost.localdomain>

On Fri, 2006-08-04 at 19:37 -0400, Jim Lemon wrote:
> Hi all,
> 
> I have finally gotten the prettyR package going (many thanks to Kurt 
> Hornik for his patience).
> 
> prettyR is a set of functions that allows the user to produce HTML 
> output from R scripts. Given an R script that runs properly, an HTML 
> listing complete with embedded graphics can be produced simply by 
> passing the script to the core function htmlize (Phillipe Grosjean has 
> not only offered great suggestions, but provided a fancier function 
> named R2html). It is even possible to have the output magically appear 
> in your friendly local HTML browser when the script has been processed.
> 
> The package includes some basic descriptive functions that display "the 
> usual suspects" in formats that should not agitate those accustomed to 
> the vanilla listings that abound in the real world.
> 
> prettyR is intended to assist the R beginner in producing basic stats 
> right from the word "go". No knowledge beyond that of writing an R 
> script is required, but there is quite a bit of room to learn and 
> innovate. Have fun and please let me know if you break it.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

Thanks but I could not get R2html in prettyR to work:

> R2html(Rfile="/ophth/cornea/R/lme_4.R",
+   HTMLfile="/ophth/cornea/Reports/lme_4.html")
Error in CreateIndexFile(HTMLfile, basenavfile, baselistfile, title) :
        unused argument(s) ( ...)

lme_3.r has the R script and lme_3.html is the html file I'd like to
create. The help file for R2html does not give an example.

> args(R2html)
function (Rfile, HTMLfile, echo = TRUE, split = FALSE, browse = TRUE,
    title = "R listing", bgcolor = "#dddddd", ...)

What am I doing wrong? I can source the script file.

> args(CreateIndexFile)
function (HTMLbase, HTMLdir, title = "R listing")

Is there a problem in R2html's call to CreateIndexFile? The arguments
don't seem to match.

Rick B.


From ggrothendieck at gmail.com  Fri Aug  4 20:33:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 4 Aug 2006 14:33:58 -0400
Subject: [R] ggplot facet label font size
In-Reply-To: <971536df0608031924u369dafaajb204f3533815b3e5@mail.gmail.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F060661C1@dlee04.ent.ti.com>
	<971536df0608031924u369dafaajb204f3533815b3e5@mail.gmail.com>
Message-ID: <971536df0608041133h55bfb134q1671f901831af7bf@mail.gmail.com>

With ggplot its possible to do this too but in that case it seems
necessary to recurse through the grobs.  Here we look for
grobs that have a label component which contains a colon
and grid.edit those changing the value of cex.  Note that
getNames() give a single grob named "pretty" and we start
from that:

# run code
library(ggplot)
library(grid)

p <- ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
gghistogram(p)

recurse <- function(x) {
   if (!is.null(x$label) && regexpr(":", x$label) > 0)
      grid.edit(x$name, gp = gpar(cex = 0.7))
   for (ch in x$children) recurse(ch)
}
recurse(grid.get("pretty"))


On 8/3/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If you are willing to use grid then you could create only the sex
> factor in the left strips since its already in the desired position
> but when displaying it output a factor.level, i.e. label of "A".
> (my.strip.left is modified from the prior post to do that.)
>
> Then after the plot is drawn, looping through all grobs looking for
> those with a label component of "A" producing a list of grob names,
> strip.left.names.  We then mapply the real factor levels with
> those grobs editing them in reset.levels(), defined below.
> (I have used the fact, empirically determined that the stripts
> are produced in order of the factor levels.)
>
> Everything is the same as the last post except my.strip.left
> which has been modified and everything which comes after the
> call to histogram.
>
> Although this seems to work, maybe Deepayan or Paul can think of
> something slicker.
>
>
> library(ggplot) # data resides here
> library(lattice)
> library(grid)
>
> my.strip <- function(which.given, which.panel, ...)
>   if (which.given == 1 && which.panel[2] == 2)
>      strip.default(which.given, which.panel, ...)
>
> my.strip.left <- function(which.given, which.panel, ...,
>   factor.levels, horizontal)
>   if (which.given == 1 && which.panel[1] == 1)
>      strip.default(which.given, which.panel, factor.levels = LETTERS,
>      horizontal = FALSE, ...)
>
>
> histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
>     strip.left = my.strip.left, par.settings = list(add.text =
> list(cex = 0.7)))
>
> is.strip.left <- function(name) identical(grid.get(name)$label, "A")
> strip.left.names <- getNames()[sapply(getNames(), is.strip.left)]
>
> reset.levels <- function(nam, lev) grid.edit(nam, label = lev)
> mapply(reset.levels , strip.left.names, levels(tips$smoker))
>
>
>
>
>
>
> On 8/3/06, Walker, Sam <s-walker at ti.com> wrote:
> >
> > This works OK, but there is some extra spacing between the panels, the
> > top axis and the strip on the top, and the left labels and panel.
> >
> > How can I remove these extra spaces?
> >
> > I've tried changing various layout.widths settings with no luck.  It
> > seems the spaces are calculated based on the number of conditioning
> > variables, in this case 2 (sex+smoker).
> >
> >
> > Thanks in advance...
> > -Sam
> >
> >
> > -----Original Message-----
> > From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > Sent: Wednesday, August 02, 2006 6:04 PM
> > To: Walker, Sam
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] ggplot facet label font size
> >
> > On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> > > How do I change the font size in the facet labels along the edges of
> > the
> > > plot?
> > >
> > > For example (from the ggplot help file):
> > >     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
> > >     gghistogram(p)
> > >
> > > In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> > > Female", "sex: Male".  What command can I use to reduce the font size
> > of
> > > these labels?
> > >
> > > In lattice terminology, cex is used to scale these strip labels.  But
> > I
> > > couldn't find the equivalent in ggplot.
> > >
> > > The reason I'm asking is I have a 9x7 array of plots which I've been
> > > plotting with lattice.  I wanted to use ggplot because I like having
> > the
> > > labels on the edge of the plots
> >
> > Note that lattice can do that by using custom strip functions:
> >
> > library(ggplot) # data resides here
> > library(lattice)
> >
> > my.strip <- function(which.given, which.panel, ...)
> >   if (which.given == 1 && which.panel[2] == 2)
> >      strip.default(which.given, which.panel, ...)
> >
> > my.strip.left <- function(which.given, which.panel, ..., horizontal)
> >   if (which.given == 2 && which.panel[1] == 1)
> >      strip.default(which.given, which.panel, horizontal = FALSE, ...)
> >
> > histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
> >     strip.left = my.strip.left, par.settings = list(add.text =
> > list(cex = 0.7)))
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ripley at stats.ox.ac.uk  Fri Aug  4 20:44:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Aug 2006 19:44:21 +0100 (BST)
Subject: [R] expression() - Superscript in y-axis,
 keeping line break in string
In-Reply-To: <1154707267.4089.11.camel@localhost.localdomain>
References: <1154706426.19124.12.camel@localhost>
	<1154707267.4089.11.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0608041942080.23492@gannet.stats.ox.ac.uk>

On Fri, 4 Aug 2006, Marc Schwartz (via MN) wrote:

> On Fri, 2006-08-04 at 09:47 -0600, Andrew Kniss wrote:
> > I've tried several different ways to accomplish this, but as yet to no
> > avail.  My y-axis for a plot has a rather long label, and thus I have
> > been using "/n" to break it into two lines.  However, to make it
> > technically correct for publication, I also need to use superscript in
> > the label.  For example:
> > 
> >      par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> >      plot(1:10,
> >           ylab="14C-glyphosate line1\n line2")
> > 
> > will provide the text in two lines as I would like it.  However, I am
> > trying to keep those same line breaks when using expression() to get my
> > superscript number.  This will not work, as it aligns the "14C" section
> > with the bottom line of the expression making little sense to the
> > reader.
> > 
> >      par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> >      plot(1:10,
> >           ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
> > 
> > Is there a way to align the "14C" portion of the expression with the top
> > line of the string rather than the bottom line?  Any suggestions are
> > greatly appreciated.
> > Andrew
> 
> plotmath, as has been covered many times previously, does not support
> multi-line expressions. A note should probably be added to ?plotmath on
> this.

I've added a note.  I think what is exact is that control chars are not 
interpreted ('expresssion' is an overloaded work in this context).

Thanks for the nudge (and please do continue to make such remarks).

Brian

> 
> Thus, you need to create each line in the label separately:
> 
>   par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
>   
>   plot(1:10, ylab = "")
> 
>   # Now use mtext() to place each line of the y axis label
> 
>   mtext(2, text = expression(" "^14*C*"-glyphosate line1"), line = 3)
> 
>   mtext(2, text = "line2", line = 2)
> 
> See ?mtext for more information.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ff809 at ncf.ca  Fri Aug  4 20:52:16 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Fri, 04 Aug 2006 14:52:16 -0400
Subject: [R] Questions about sweave... question answered and problem
	solved
In-Reply-To: <loom.20060804T085513-88@post.gmane.org>
References: <44D2A0FF.2090706@ncf.ca> <loom.20060804T085513-88@post.gmane.org>
Message-ID: <44D39760.8040002@ncf.ca>

* Dieter Menne wrote, On 2006-08-04 02:57:
> Brian Lunergan <ff809 <at> ncf.ca> writes:
> 
>> Evening all:
>>
>> I'm taking a little time to experiment with R, Sweave, and Miktex/LaTex but 
>> I've run up against some problems and -well- I hope that there are some on 
>> the list who might have some suggestions. This will be kind of wordy as I 
>> will include the complete files involved as I'm just not sure what I'm 
>> looking for. Apologies at the outset.
> 
> This is Windows special
> 
> See A.12: 
> http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html
> 
> And put a commented (!sic) \usepackage into the header (don't remember where I
> found this)
> 
> % \usepackage{Sweave} 
> 
> 
> Dieter

Thanks for the lead. Reinstalled R outside of 'c:\program files'. Reran the 
source through Sweave to generate a new example.tex with the preamble:

\documentclass[letterpaper]{article}
\title{Sweave Example 1}
\author{Friedrich Leisch}
\usepackage{C:/R-2.3.1/share/texmf/Sweave}
\begin{document}

Ran it through Latex and everything is as it should be. Problem solved.

-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0631-3, 2006-08-04
Tested on: 2006-08-04 14:52:19
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From mschwartz at mn.rr.com  Fri Aug  4 21:05:34 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 04 Aug 2006 14:05:34 -0500
Subject: [R] expression() - Superscript in y-axis,
	keeping line break	in string
In-Reply-To: <Pine.LNX.4.64.0608041942080.23492@gannet.stats.ox.ac.uk>
References: <1154706426.19124.12.camel@localhost>
	<1154707267.4089.11.camel@localhost.localdomain>
	<Pine.LNX.4.64.0608041942080.23492@gannet.stats.ox.ac.uk>
Message-ID: <1154718335.4089.30.camel@localhost.localdomain>

On Fri, 2006-08-04 at 19:44 +0100, Prof Brian Ripley wrote:
> On Fri, 4 Aug 2006, Marc Schwartz (via MN) wrote:
> 
> > On Fri, 2006-08-04 at 09:47 -0600, Andrew Kniss wrote:
> > > I've tried several different ways to accomplish this, but as yet to no
> > > avail.  My y-axis for a plot has a rather long label, and thus I have
> > > been using "/n" to break it into two lines.  However, to make it
> > > technically correct for publication, I also need to use superscript in
> > > the label.  For example:
> > > 
> > >      par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> > >      plot(1:10,
> > >           ylab="14C-glyphosate line1\n line2")
> > > 
> > > will provide the text in two lines as I would like it.  However, I am
> > > trying to keep those same line breaks when using expression() to get my
> > > superscript number.  This will not work, as it aligns the "14C" section
> > > with the bottom line of the expression making little sense to the
> > > reader.
> > > 
> > >      par(oma=c(0,0,2,0),mar=c(5,6,0.25,2),lheight=1)
> > >      plot(1:10,
> > >           ylab=expression(" "^14*C*"-glyphosate line1\n line2"))
> > > 
> > > Is there a way to align the "14C" portion of the expression with the top
> > > line of the string rather than the bottom line?  Any suggestions are
> > > greatly appreciated.
> > > Andrew
> > 
> > plotmath, as has been covered many times previously, does not support
> > multi-line expressions. A note should probably be added to ?plotmath on
> > this.
> 
> I've added a note.  I think what is exact is that control chars are not 
> interpreted ('expresssion' is an overloaded work in this context).
> 
> Thanks for the nudge (and please do continue to make such remarks).
> 
> Brian

<snip>

Happy to help and thanks for both noticing and taking the time to
incorporate the update.

Best regards,

Marc


From jessiet at stanford.edu  Fri Aug  4 21:14:17 2006
From: jessiet at stanford.edu (Jessie Tenenbaum)
Date: Fri, 4 Aug 2006 12:14:17 -0700
Subject: [R] training svm's with probability flag (re-send in plain text)
Message-ID: <016501c6b7fa$2e943650$ac0941ab@JESSIET>

Hi-

I'm seeing some weirdness with svm and tune.svm that I can't figure out- was
wondering if anyone else has seen this? Perhaps I'm failing to make
something the expected class?
Below is my repro case, though it *sometimes* doesn't repro. I'm using
R2.3.1 on WindowsXP. I was also seeing it happen with R2.1.1 and have seen
it on 2 different machines.
 
data(iris)
attach(iris)
library(e1071)
train<- iris[c(1:30,50:80,100:130),]
test<- iris[-c(1:30,50:80,100:130),]
y.train<- train$Species
y.test<- test$Species
obj<- tune.svm(train[,-5], y.train, gamma = 2^(-1:1), cost = 2^(2:4),
probability=T)
my.svm<- obj$best.model
pred1<- predict(my.svm, test[,-5])
pred2<- predict(my.svm, test[,-5], probability=T)
table(pred1, y.test)
table(pred2, y.test)

When I do this, the two different tables often come out different, as below:
> table(pred1, y.test)
            y.test
pred1        setosa versicolor virginica
  setosa         19          0         0
  versicolor      0         18         1
  virginica       0          1        19
> table(pred2, y.test)
            y.test
pred2        setosa versicolor virginica
  setosa         18          0         0
  versicolor      1         18         1
  virginica       0          1        19
> 

I'm not sure 1. why the results would differ based on whether I choose to
calculate the probabilities, and 2. which one to trust??
Anyone come across this before, or have any ideas?
 
thanks,
jessie


From joseclaudio.faria at terra.com.br  Fri Aug  4 22:09:23 2006
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Fri, 04 Aug 2006 17:09:23 -0300
Subject: [R] Doubt about Student t distribution simulation
Message-ID: <44D3A973.3020407@terra.com.br>

Dear R list,

I would like to illustrate the origin of the Student t distribution using R.

So, if (sample.mean - pop.mean) / standard.error(sample.mean) has t 
distribution with (sample.size - 1) degree free, what is wrong with the 
simulation below? I think that the theoretical curve should agree with 
the relative frequencies of the t values calculated:

#== begin options=====
# parameters
   mu    = 10
   sigma =  5

# size of sample
   n = 3

# repetitions
   nsim = 10000

# histogram parameter
   nchist = 150
#== end options=======

t   = numeric()
pop = rnorm(10000, mean = mu, sd = sigma)

for (i in 1:nsim) {
   amo.i = sample(pop, n, replace = TRUE)
   t[i]  = (mean(amo.i) - mu) / (sigma / sqrt(n))
}

win.graph(w = 5, h = 7)
split.screen(c(2,1))
screen(1)
hist(t,
      main     = "histogram",
      breaks   = nchist,
      col      = "lightgray",
      xlab     = '', ylab = "Fi",
      font.lab = 2, font = 2)

screen(2)
hist(t,
      probability = T,
      main        = 'f.d.p and histogram',
      breaks      = nchist,
      col         = 'lightgray',
      xlab        = 't', ylab = 'f(t)',
      font.lab    = 2, font = 2)

x = t
curve(dt(x, df = n-1), add = T, col = "red", lwd = 2)

Many thanks for any help,
___
Jose Claudio Faria
Brasil/Bahia/Ilheus/UESC/DCET
Estat?stica Experimental/Prof. Adjunto
mails: joseclaudio.faria em terra.com.br
        jc_faria em uesc.br
        jc_faria em uol.com.br


From Qinghong.Li at rdmo.nestle.com  Fri Aug  4 22:05:37 2006
From: Qinghong.Li at rdmo.nestle.com (Li,Qinghong,ST.LOUIS,Molecular Biology)
Date: Fri, 4 Aug 2006 15:05:37 -0500
Subject: [R] meta characters in file path
Message-ID: <2BA2B7291D5DC6409FD53CB7C01F0D990183B890@usslre00.nestle.com>

thanks Prof Ripley. dir() returns the path with full names (wildcards replaced) that are exactly what I need.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
Sent: Friday, August 04, 2006 12:27 PM
To: Li,Qinghong,ST.LOUIS,Molecular Biology
Cc: r-help at stat.math.ethz.ch; Tony Plate
Subject: Re: [R] meta characters in file path


On Fri, 4 Aug 2006, Li,Qinghong,ST.LOUIS,Molecular Biology wrote:

> Thanks. I tried them, it works for most of those characters except "*" 
> and "?".

Those are not valid characters in Windows file paths
(/  " * : < > ? \ | are invalid in file or dir names).

> Does regular expression work in file names in windows? 

No, and I think you may mean wildcards (which is what work on the command 
line).

> e.g. I have a machine-generated file named "021706 matrix#1479 @50.csv", 
> of which "1479" is kinda random. Will I be able to match "1479" with 
> some sort of "wild card" chars?

Yes, use dir(), with regexp pattern patching to find the name(s) you 
want.  glob2rx() might be useful here.

> Thanks
> Johnny
> 
> -----Original Message-----
> From: Tony Plate [mailto:tplate at acm.org]
> Sent: Thursday, August 03, 2006 3:42 PM
> To: Li,Qinghong,ST.LOUIS,Molecular Biology
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] meta characters in file path
> 
> 
> What is the problem you are having?  Seems to work fine for me running 
> under Windows2000:
> 
>  > write.table(data.frame(a=1:3,b=4:6), file="@# x.csv", sep=",")
>  > read.csv(file="@# x.csv")
>    a b
> 1 1 4
> 2 2 5
> 3 3 6
>  > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> 
> other attached packages:
>       XML
> "0.99-8"
>  >
> 
> Li,Qinghong,ST.LOUIS,Molecular Biology wrote:
> > Hi,
> > 
> > I need to read in some files. The file names contain come meta characters such as @, #, and white spaces etc, In read.csv, file= option, is there any way that one can make the function to recognize a file path with those characters?
> > 
> > Thanks
> > Johnny
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Fri Aug  4 22:18:14 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 4 Aug 2006 16:18:14 -0400
Subject: [R] Doubt about Student t distribution simulation
In-Reply-To: <44D3A973.3020407@terra.com.br>
Message-ID: <20060804201814.MNIH1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Jose,

The problem is that you're using the population standard deviation (sigma)
rather than the sample SD of each sample [i.e., t[i]  = (mean(amo.i) - mu) /
(sd(amo.i) / sqrt(n)) ], so your values should be normally distributed, as
they appear to be.

A couple of smaller points: (1) Even after this correction, you're sampling
from a discrete population (albeit with replacement) and so the values won't
be exactly t-distributed. You could draw the samples directly from N(mu,
sigma) instead. (2) It would be preferable to make a quantile-comparison
plot against the t-distribution, since you'd get a better picture of what's
going on in the tails.

I hope this helps,
 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jose 
> Claudio Faria
> Sent: Friday, August 04, 2006 3:09 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Doubt about Student t distribution simulation
> 
> Dear R list,
> 
> I would like to illustrate the origin of the Student t 
> distribution using R.
> 
> So, if (sample.mean - pop.mean) / standard.error(sample.mean) 
> has t distribution with (sample.size - 1) degree free, what 
> is wrong with the simulation below? I think that the 
> theoretical curve should agree with the relative frequencies 
> of the t values calculated:
> 
> #== begin options=====
> # parameters
>    mu    = 10
>    sigma =  5
> 
> # size of sample
>    n = 3
> 
> # repetitions
>    nsim = 10000
> 
> # histogram parameter
>    nchist = 150
> #== end options=======
> 
> t   = numeric()
> pop = rnorm(10000, mean = mu, sd = sigma)
> 
> for (i in 1:nsim) {
>    amo.i = sample(pop, n, replace = TRUE)
>    t[i]  = (mean(amo.i) - mu) / (sigma / sqrt(n)) }
> 
> win.graph(w = 5, h = 7)
> split.screen(c(2,1))
> screen(1)
> hist(t,
>       main     = "histogram",
>       breaks   = nchist,
>       col      = "lightgray",
>       xlab     = '', ylab = "Fi",
>       font.lab = 2, font = 2)
> 
> screen(2)
> hist(t,
>       probability = T,
>       main        = 'f.d.p and histogram',
>       breaks      = nchist,
>       col         = 'lightgray',
>       xlab        = 't', ylab = 'f(t)',
>       font.lab    = 2, font = 2)
> 
> x = t
> curve(dt(x, df = n-1), add = T, col = "red", lwd = 2)
> 
> Many thanks for any help,
> ___
> Jose Claudio Faria
> Brasil/Bahia/Ilheus/UESC/DCET
> Estat?stica Experimental/Prof. Adjunto
> mails: joseclaudio.faria at terra.com.br
>         jc_faria at uesc.br
>         jc_faria at uol.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Fri Aug  4 22:21:08 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Aug 2006 22:21:08 +0200
Subject: [R] Doubt about Student t distribution simulation
In-Reply-To: <44D3A973.3020407@terra.com.br>
References: <44D3A973.3020407@terra.com.br>
Message-ID: <x2d5bggsln.fsf@turmalin.kubism.ku.dk>

Jose Claudio Faria <joseclaudio.faria at terra.com.br> writes:

> Dear R list,
> 
> I would like to illustrate the origin of the Student t distribution using R.
> 
> So, if (sample.mean - pop.mean) / standard.error(sample.mean) has t 
> distribution with (sample.size - 1) degree free, what is wrong with the 
> simulation below? I think that the theoretical curve should agree with 
> the relative frequencies of the t values calculated:
> 
> #== begin options=====
> # parameters
>    mu    = 10
>    sigma =  5
> 
> # size of sample
>    n = 3
> 
> # repetitions
>    nsim = 10000
> 
> # histogram parameter
>    nchist = 150
> #== end options=======
> 
> t   = numeric()
> pop = rnorm(10000, mean = mu, sd = sigma)
> 
> for (i in 1:nsim) {
>    amo.i = sample(pop, n, replace = TRUE)
>    t[i]  = (mean(amo.i) - mu) / (sigma / sqrt(n))

At the very least, you need a sample-based standard error: sd(amo.i),
not sigma. Also, resampling from "pop" is not really what the
t-distribution is based on, but I don't think that matters much.


> }
> 
> win.graph(w = 5, h = 7)
> split.screen(c(2,1))
> screen(1)
> hist(t,
>       main     = "histogram",
>       breaks   = nchist,
>       col      = "lightgray",
>       xlab     = '', ylab = "Fi",
>       font.lab = 2, font = 2)
> 
> screen(2)
> hist(t,
>       probability = T,
>       main        = 'f.d.p and histogram',
>       breaks      = nchist,
>       col         = 'lightgray',
>       xlab        = 't', ylab = 'f(t)',
>       font.lab    = 2, font = 2)
> 
> x = t
> curve(dt(x, df = n-1), add = T, col = "red", lwd = 2)
> 
> Many thanks for any help,
> ___
> Jose Claudio Faria
> Brasil/Bahia/Ilheus/UESC/DCET
> Estat?stica Experimental/Prof. Adjunto
> mails: joseclaudio.faria at terra.com.br
>         jc_faria at uesc.br
>         jc_faria at uol.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Setzer.Woodrow at epamail.epa.gov  Fri Aug  4 22:22:06 2006
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow at epamail.epa.gov)
Date: Fri, 04 Aug 2006 16:22:06 -0400
Subject: [R] Error when loading odesolve
In-Reply-To: <Pine.LNX.4.64.0608041827540.22596@gannet.stats.ox.ac.uk>
Message-ID: <OF259A3E05.9FAFEC79-ON852571C0.006DA21C-852571C0.006FE32B@epamail.epa.gov>

Prof. Ripley:
Thanks for diagnosing Wuming Gong's problem.
 I'm not sure I would have recognized the solution so quickly.  I have
uploaded to CRAN a new version of odesolve with Makevars fixed.

R. Woodrow Setzer, Ph. D.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B205-01/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-1194


                                                                        
             Prof Brian                                                 
             Ripley                                                     
             <ripley at stats.ox                                        To 
             .ac.uk>                  Wuming Gong                       
             Sent by:                 <wuming.gong at gmail.com>           
             r-help-bounces at s                                        cc 
             tat.math.ethz.ch         r-help at stat.math.ethz.ch          
                                                                Subject 
                                      Re: [R] Error when loading        
             08/04/2006 01:33         odesolve                          
             PM                                                         
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        




Looking at odesolve, src/Makevars is

PKG_LIBS=$(BLAS_LIBS)

Now, the documentation says that if you have $(BLAS_LIBS) you must also
have $(FLIBS), so please change this to

PKG_LIBS=$(BLAS_LIBS) $(FLIBS)

and take this up with the package maintainer (which is what the posting
guide asked you to do in the first place).


On Sat, 5 Aug 2006, Wuming Gong wrote:

> Dear list,
>
> I installed odesolve package (0.5-15) in R 2.3.1 in a Solaris server
> (Generic_118558-11 sun4u sparc SUNW,Sun-Blade-1000).  The installing
> progress completed without errors, though several warnings like
> "Warning: Option -fPIC passed to ld, if ld is invoked, ignored
> otherwise" were outputed.
>
> However, when loading the odesolve package by library(odesolve),
> following error messages pop out:
>
> > library(odesolve)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> '/project/scratch/ligroup/R1/lib/R/library/odesolve/libs/odesolve.so':
>   ld.so.1: R: fatal: relocation error: file
> /project/scratch/ligroup/R1/lib/R/library/odesolve/libs/odesolve.so:
> symbol __f90_ssfw: referenced symbol not found
> Error: package/namespace load failed for 'odesolve'
>
> Could any one tell me how to fix this problem?
>
> Thanks very much.
>
> Wuming
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From sundar.dorai-raj at pdf.com  Fri Aug  4 22:27:12 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 04 Aug 2006 15:27:12 -0500
Subject: [R] Doubt about Student t distribution simulation
In-Reply-To: <20060804201814.MNIH1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20060804201814.MNIH1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <44D3ADA0.30904@pdf.com>

Hi, Jose/John,

Here's an example to help Jose and highlights John's advice. Also 
includes set.seed which should be included in all simulations posted to 
R-help.

set.seed(42)
mu <- 10
sigma <-  5
n <- 3
nsim <- 10000
m <- matrix(rnorm(n * nsim, mu, sigma), nsim, n)
t <- apply(m, 1, function(x) (mean(x) - mu)/(sd(x)/sqrt(n)))

library(lattice)
qqmath(t, distribution = function(x) qt(x, n - 1),
        panel = function(x, ...) {
          panel.qqmath(x, col = "darkblue", ...)
          panel.qqmathline(x, col = "darkred", ...)
        })


With n = 3, expect a few outliers.

--sundar


John Fox wrote:
> Dear Jose,
> 
> The problem is that you're using the population standard deviation (sigma)
> rather than the sample SD of each sample [i.e., t[i]  = (mean(amo.i) - mu) /
> (sd(amo.i) / sqrt(n)) ], so your values should be normally distributed, as
> they appear to be.
> 
> A couple of smaller points: (1) Even after this correction, you're sampling
> from a discrete population (albeit with replacement) and so the values won't
> be exactly t-distributed. You could draw the samples directly from N(mu,
> sigma) instead. (2) It would be preferable to make a quantile-comparison
> plot against the t-distribution, since you'd get a better picture of what's
> going on in the tails.
> 
> I hope this helps,
>  John 
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jose 
>>Claudio Faria
>>Sent: Friday, August 04, 2006 3:09 PM
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] Doubt about Student t distribution simulation
>>
>>Dear R list,
>>
>>I would like to illustrate the origin of the Student t 
>>distribution using R.
>>
>>So, if (sample.mean - pop.mean) / standard.error(sample.mean) 
>>has t distribution with (sample.size - 1) degree free, what 
>>is wrong with the simulation below? I think that the 
>>theoretical curve should agree with the relative frequencies 
>>of the t values calculated:
>>
>>#== begin options=====
>># parameters
>>   mu    = 10
>>   sigma =  5
>>
>># size of sample
>>   n = 3
>>
>># repetitions
>>   nsim = 10000
>>
>># histogram parameter
>>   nchist = 150
>>#== end options=======
>>
>>t   = numeric()
>>pop = rnorm(10000, mean = mu, sd = sigma)
>>
>>for (i in 1:nsim) {
>>   amo.i = sample(pop, n, replace = TRUE)
>>   t[i]  = (mean(amo.i) - mu) / (sigma / sqrt(n)) }
>>
>>win.graph(w = 5, h = 7)
>>split.screen(c(2,1))
>>screen(1)
>>hist(t,
>>      main     = "histogram",
>>      breaks   = nchist,
>>      col      = "lightgray",
>>      xlab     = '', ylab = "Fi",
>>      font.lab = 2, font = 2)
>>
>>screen(2)
>>hist(t,
>>      probability = T,
>>      main        = 'f.d.p and histogram',
>>      breaks      = nchist,
>>      col         = 'lightgray',
>>      xlab        = 't', ylab = 'f(t)',
>>      font.lab    = 2, font = 2)
>>
>>x = t
>>curve(dt(x, df = n-1), add = T, col = "red", lwd = 2)
>>
>>Many thanks for any help,
>>___
>>Jose Claudio Faria
>>Brasil/Bahia/Ilheus/UESC/DCET
>>Estat?stica Experimental/Prof. Adjunto
>>mails: joseclaudio.faria at terra.com.br
>>        jc_faria at uesc.br
>>        jc_faria at uol.com.br
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From joseclaudio.faria at terra.com.br  Fri Aug  4 22:42:37 2006
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Fri, 04 Aug 2006 17:42:37 -0300
Subject: [R] Doubt about Student t distribution simulation
In-Reply-To: <20060804201814.MNIH1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20060804201814.MNIH1543.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <44D3B13D.7060508@terra.com.br>

Dears John, Peter and Sundar,

Many thanks for the quick answers!!!

.. and sorry for all..

[]s
___
Jose Claudio Faria
Brasil/Bahia/Ilheus/UESC/DCET
Estat?stica Experimental/Prof. Adjunto
mails: joseclaudio.faria em terra.com.br
        jc_faria em uesc.br
        jc_faria em uol.com.br

John Fox escreveu:
> Dear Jose,
> 
> The problem is that you're using the population standard deviation (sigma)
> rather than the sample SD of each sample [i.e., t[i]  = (mean(amo.i) - mu) /
> (sd(amo.i) / sqrt(n)) ], so your values should be normally distributed, as
> they appear to be.
> 
> A couple of smaller points: (1) Even after this correction, you're sampling
> from a discrete population (albeit with replacement) and so the values won't
> be exactly t-distributed. You could draw the samples directly from N(mu,
> sigma) instead. (2) It would be preferable to make a quantile-comparison
> plot against the t-distribution, since you'd get a better picture of what's
> going on in the tails.
> 
> I hope this helps,
>  John 
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
>> -----Original Message-----
>> From: r-help-bounces em stat.math.ethz.ch 
>> [mailto:r-help-bounces em stat.math.ethz.ch] On Behalf Of Jose 
>> Claudio Faria
>> Sent: Friday, August 04, 2006 3:09 PM
>> To: R-help em stat.math.ethz.ch
>> Subject: [R] Doubt about Student t distribution simulation
>>
>> Dear R list,
>>
>> I would like to illustrate the origin of the Student t 
>> distribution using R.
>>
>> So, if (sample.mean - pop.mean) / standard.error(sample.mean) 
>> has t distribution with (sample.size - 1) degree free, what 
>> is wrong with the simulation below? I think that the 
>> theoretical curve should agree with the relative frequencies 
>> of the t values calculated:
>>
>> #== begin options=====
>> # parameters
>>    mu    = 10
>>    sigma =  5
>>
>> # size of sample
>>    n = 3
>>
>> # repetitions
>>    nsim = 10000
>>
>> # histogram parameter
>>    nchist = 150
>> #== end options=======
>>
>> t   = numeric()
>> pop = rnorm(10000, mean = mu, sd = sigma)
>>
>> for (i in 1:nsim) {
>>    amo.i = sample(pop, n, replace = TRUE)
>>    t[i]  = (mean(amo.i) - mu) / (sigma / sqrt(n)) }
>>
>> win.graph(w = 5, h = 7)
>> split.screen(c(2,1))
>> screen(1)
>> hist(t,
>>       main     = "histogram",
>>       breaks   = nchist,
>>       col      = "lightgray",
>>       xlab     = '', ylab = "Fi",
>>       font.lab = 2, font = 2)
>>
>> screen(2)
>> hist(t,
>>       probability = T,
>>       main        = 'f.d.p and histogram',
>>       breaks      = nchist,
>>       col         = 'lightgray',
>>       xlab        = 't', ylab = 'f(t)',
>>       font.lab    = 2, font = 2)
>>
>> x = t
>> curve(dt(x, df = n-1), add = T, col = "red", lwd = 2)
>>
>> Many thanks for any help,
>> ___
>> Jose Claudio Faria
>> Brasil/Bahia/Ilheus/UESC/DCET
>> Estat?stica Experimental/Prof. Adjunto
>> mails: joseclaudio.faria em terra.com.br
>>         jc_faria em uesc.br
>>         jc_faria em uol.com.br
>>
>> ______________________________________________
>> R-help em stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> Esta mensagem foi verificada pelo E-mail Protegido Terra.
> Scan engine: McAfee VirusScan / Atualizado em 04/08/2006 / Vers?o: 4.4.00/4822
> Proteja o seu e-mail Terra: http://mail.terra.com.br/
> 
> 
>


From jfox at mcmaster.ca  Fri Aug  4 23:44:29 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 4 Aug 2006 17:44:29 -0400
Subject: [R] Doubt about Student t distribution simulation
In-Reply-To: <44D3ADA0.30904@pdf.com>
Message-ID: <20060804214429.UPK10262.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Sundar,

Try qq.plot(t, dist="t", df=n-1) from the car package, which include a
95-percent point-wise confidence envelope that helps you judge how extreme
the outliers are relative to expectations.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com] 
> Sent: Friday, August 04, 2006 3:27 PM
> To: John Fox
> Cc: joseclaudio.faria at terra.com.br; R-help at stat.math.ethz.ch
> Subject: Re: [R] Doubt about Student t distribution simulation
> 
> Hi, Jose/John,
> 
> Here's an example to help Jose and highlights John's advice. 
> Also includes set.seed which should be included in all 
> simulations posted to R-help.
> 
> set.seed(42)
> mu <- 10
> sigma <-  5
> n <- 3
> nsim <- 10000
> m <- matrix(rnorm(n * nsim, mu, sigma), nsim, n) t <- 
> apply(m, 1, function(x) (mean(x) - mu)/(sd(x)/sqrt(n)))
> 
> library(lattice)
> qqmath(t, distribution = function(x) qt(x, n - 1),
>         panel = function(x, ...) {
>           panel.qqmath(x, col = "darkblue", ...)
>           panel.qqmathline(x, col = "darkred", ...)
>         })
> 
> 
> With n = 3, expect a few outliers.
> 
> --sundar
> 
> 
> John Fox wrote:
> > Dear Jose,
> > 
> > The problem is that you're using the population standard deviation 
> > (sigma) rather than the sample SD of each sample [i.e., t[i]  = 
> > (mean(amo.i) - mu) /
> > (sd(amo.i) / sqrt(n)) ], so your values should be normally 
> > distributed, as they appear to be.
> > 
> > A couple of smaller points: (1) Even after this correction, you're 
> > sampling from a discrete population (albeit with 
> replacement) and so 
> > the values won't be exactly t-distributed. You could draw 
> the samples 
> > directly from N(mu,
> > sigma) instead. (2) It would be preferable to make a 
> > quantile-comparison plot against the t-distribution, since 
> you'd get a 
> > better picture of what's going on in the tails.
> > 
> > I hope this helps,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jose Claudio 
> >>Faria
> >>Sent: Friday, August 04, 2006 3:09 PM
> >>To: R-help at stat.math.ethz.ch
> >>Subject: [R] Doubt about Student t distribution simulation
> >>
> >>Dear R list,
> >>
> >>I would like to illustrate the origin of the Student t distribution 
> >>using R.
> >>
> >>So, if (sample.mean - pop.mean) / standard.error(sample.mean) has t 
> >>distribution with (sample.size - 1) degree free, what is wrong with 
> >>the simulation below? I think that the theoretical curve 
> should agree 
> >>with the relative frequencies of the t values calculated:
> >>
> >>#== begin options=====
> >># parameters
> >>   mu    = 10
> >>   sigma =  5
> >>
> >># size of sample
> >>   n = 3
> >>
> >># repetitions
> >>   nsim = 10000
> >>
> >># histogram parameter
> >>   nchist = 150
> >>#== end options=======
> >>
> >>t   = numeric()
> >>pop = rnorm(10000, mean = mu, sd = sigma)
> >>
> >>for (i in 1:nsim) {
> >>   amo.i = sample(pop, n, replace = TRUE)
> >>   t[i]  = (mean(amo.i) - mu) / (sigma / sqrt(n)) }
> >>
> >>win.graph(w = 5, h = 7)
> >>split.screen(c(2,1))
> >>screen(1)
> >>hist(t,
> >>      main     = "histogram",
> >>      breaks   = nchist,
> >>      col      = "lightgray",
> >>      xlab     = '', ylab = "Fi",
> >>      font.lab = 2, font = 2)
> >>
> >>screen(2)
> >>hist(t,
> >>      probability = T,
> >>      main        = 'f.d.p and histogram',
> >>      breaks      = nchist,
> >>      col         = 'lightgray',
> >>      xlab        = 't', ylab = 'f(t)',
> >>      font.lab    = 2, font = 2)
> >>
> >>x = t
> >>curve(dt(x, df = n-1), add = T, col = "red", lwd = 2)
> >>
> >>Many thanks for any help,
> >>___
> >>Jose Claudio Faria
> >>Brasil/Bahia/Ilheus/UESC/DCET
> >>Estat?stica Experimental/Prof. Adjunto
> >>mails: joseclaudio.faria at terra.com.br
> >>        jc_faria at uesc.br
> >>        jc_faria at uol.com.br
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From simone.vincenzi at nemo.unipr.it  Fri Aug  4 23:45:19 2006
From: simone.vincenzi at nemo.unipr.it (Simone Vincenzi)
Date: Fri, 4 Aug 2006 23:45:19 +0200
Subject: [R] Help with short time series
Message-ID: <20060804214445.M9473@nemo.unipr.it>

Dear R-list, 
I have a statistical problem with the comparison of two short time-series of 
density data in an ecological framework. I have to compare two short time 
series (5 years, one value for each year) of species density data (it is the 
density of fish in two different streams) to test if the two means of the 
five densities are significantly different, so basically if the two mean 
stream-specific fish densities are significantly different. 
I don't think I can use a straight t-test due to the problem of 
autocorrelation and I don't think I can use a repeated measure ANOVA as I 
don't have any replicates. 
Any help would be greatly appreciated. 

Thanks 

Simone Vincenzi  


--
Universita' degli Studi di Parma (http://www.unipr.it)


From erich.neuwirth at univie.ac.at  Sat Aug  5 00:02:06 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sat, 05 Aug 2006 00:02:06 +0200
Subject: [R] Postscript fonts
Message-ID: <44D3C3DE.3090509@univie.ac.at>

How can I find out what fonts are available for
par(family=
for the postscript device?



-- 
Erich Neuwirth, Didactic Center for Computer Science
University of Vienna
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-9394


From daniel.jeske at ucr.edu  Sat Aug  5 00:43:21 2006
From: daniel.jeske at ucr.edu (Daniel Jeske)
Date: Fri, 4 Aug 2006 15:43:21 -0700
Subject: [R] Variance-Covariance matrix from glm()
Message-ID: <200608042243.ASH98094@sentoku.ucr.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/a044fcdc/attachment.pl 

From jrosenba at rand.org  Sat Aug  5 00:48:46 2006
From: jrosenba at rand.org (Rosenbaum, Janet)
Date: Fri, 4 Aug 2006 15:48:46 -0700
Subject: [R] polychoric correlation error
Message-ID: <EF29C0E2A43A744E9C8F0CA717B4F1C380EC16@smmail8.rand.org>


Dear all,

I get a strange error when I find polychoric correlations with the ML method, which I have been able to reproduce using randomly-generated data.

What is wrong?  
I realize that the data that I generated randomly is a bit strange, but it is the only way that I duplicate the error message.


> n<-100
> test.x<-rnorm(n, mean=0, sd=1)
> test.c<-test.x + rnorm(n, mean=0, sd=.5)
> thresh.x<-c(-2.5, -1, -.5, .5, 1000)
> thresh.c<-c(-1, 1, 2, 3, 1000)
> 
> discrete.x<-discrete.c<-vector(length=n)
> 
> for (i in 1:n) {
+ 	discrete.x[i]<-which.min(thresh.x < test.x[i] )
+ 	discrete.c[i]<-which.min(thresh.c < test.c[i] )
+ }
> pc<-polychor(discrete.x, discrete.c, std.err=T, ML=T)
Error in optim(c(optimise(f, interval = c(-1, 1))$minimum, rc, cc), f,  : 
	non-finite finite-difference value [1]
In addition: There were 50 or more warnings (use warnings() to see the first 50)
> print(pc)
Error in print(pc) : object "pc" not found
> warnings()
Warning messages:
1: NaNs produced in: log(x) 
2: NA/Inf replaced by maximum positive value
3: NaNs produced in: log(x) 


---

Thanks,

Janet

--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From Achim.Zeileis at wu-wien.ac.at  Sat Aug  5 01:15:53 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 5 Aug 2006 01:15:53 +0200 (CEST)
Subject: [R] Variance-Covariance matrix from glm()
In-Reply-To: <200608042243.ASH98094@sentoku.ucr.edu>
References: <200608042243.ASH98094@sentoku.ucr.edu>
Message-ID: <Pine.LNX.4.58.0608050115100.19918@luthien.ci.tuwien.ac.at>

On Fri, 4 Aug 2006, Daniel Jeske wrote:

> We are trying to find out how to get the variance-covariance matrix of the
> MLEs out of the glm function.  Can anyone help?

It can be extracted with the corresponding vcov() method.

Best,
Z


From jpu at apple.com  Sat Aug  5 01:44:44 2006
From: jpu at apple.com (Jia Pu)
Date: Fri, 4 Aug 2006 16:44:44 -0700
Subject: [R] (... not defined because of singularities) in lm()
Message-ID: <ED91D8A0-28DE-44BC-B084-A45D87541A3F@apple.com>

I got the message, " Coefficients: (1 not defined because of  
singularities)", in the returned result of lm(). What does it mean?  
And where should I start investigating why it happens?

thanks.


/// Complete result of lm()
Call:
lm(formula = durationRatio ~ isHStar + isWordFinal + oneSyllToEOW +
     isInterNuclear + isIntonNuclear + isInterLP + isIntonLF +
     zeroSyllToNA + oneSyllToNA + zeroSyllToEOP + oneSyllToEOP +
     isInterNuclear:zeroSyllToEOP)

Residuals:
       Min        1Q    Median        3Q       Max
-3.031538 -0.126943 -0.002909  0.121606  4.322135

Coefficients: (1 not defined because of singularities)
                               Estimate Std. Error t value Pr(>|t|)
(Intercept)                   1.044481   0.010694  97.669  < 2e-16 ***
isHStar                      -0.055227   0.005748  -9.607  < 2e-16 ***
isWordFinal                  -0.262071   0.009982 -26.254  < 2e-16 ***
oneSyllToEOW                 -0.031313   0.009917  -3.158  0.00160 **
isInterNuclear                0.063470   0.015699   4.043 5.32e-05 ***
isIntonNuclear                0.054570   0.011711   4.660 3.21e-06 ***
isInterLP                    -0.262302   0.011868 -22.102  < 2e-16 ***
isIntonLF                    -0.160018   0.012537 -12.764  < 2e-16 ***
zeroSyllToNA                 -0.160767   0.011105 -14.477  < 2e-16 ***
oneSyllToNA                  -0.012633   0.010779  -1.172  0.24123
zeroSyllToEOP                -0.212474   0.011987 -17.726  < 2e-16 ***
oneSyllToEOP                 -0.123324   0.010998 -11.214  < 2e-16 ***
isInterNuclear:zeroSyllToEOP        NA         NA      NA       NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 0.2662 on 9650 degrees of freedom
Multiple R-Squared: 0.3957,	Adjusted R-squared: 0.395
F-statistic: 574.5 on 11 and 9650 DF,  p-value: < 2.2e-16


From lgilbert at berkeley.edu  Sat Aug  5 02:51:06 2006
From: lgilbert at berkeley.edu (Betty Gilbert)
Date: Fri, 4 Aug 2006 17:51:06 -0700
Subject: [R] cor of two matrices whose columns got shuffled
Message-ID: <p06230901c0f98b45c94d@[128.32.8.36]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060804/b4a8905c/attachment.pl 

From jfox at mcmaster.ca  Sat Aug  5 04:48:16 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 4 Aug 2006 22:48:16 -0400
Subject: [R] polychoric correlation error
In-Reply-To: <EF29C0E2A43A744E9C8F0CA717B4F1C380EC16@smmail8.rand.org>
Message-ID: <20060805024815.MBY16051.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Janet,

Because you didn't set the value of the random-number generator seed, your
example isn't precisely reproducible, but the problem is apparent anyway:

> set.seed(12345)
> n<-100
> test.x<-rnorm(n, mean=0, sd=1)
> test.c<-test.x + rnorm(n, mean=0, sd=.5) 
> thresh.x<-c(-2.5, -1, -.5, .5, 1000) 
> thresh.c<-c(-1, 1, 2, 3, 1000)
> 
> discrete.x<-discrete.c<-vector(length=n)
> 
> for (i in 1:n) {
+ discrete.x[i]<-which.min(thresh.x < test.x[i] )
+ discrete.c[i]<-which.min(thresh.c < test.c[i] ) }
> 
> table(discrete.x, discrete.c)
          discrete.c
discrete.x  1  2  3  4  5
         2 12  1  0  0  0
         3  3 12  0  0  0
         4  2 19  2  0  0
         5  0 18 21  9  1
> 
> cor(test.x, test.c)
[1] 0.9184189
> 
> pc <- polychor(discrete.x, discrete.c, std.err=T, ML=T)
Warning messages:
1: NaNs produced in: log(x) 
2: NaNs produced in: log(x) 
3: NaNs produced in: log(x) 
> pc

Polychoric Correlation, ML est. = 0.9077 (0.03314)
Test of bivariate normality: Chisquare = 3.103, df = 11, p = 0.9893

  Row Thresholds
  Threshold Std.Err.
1  -1.12200   0.1609
2  -0.56350   0.1309
3   0.03318   0.1235


  Column Thresholds
  Threshold Std.Err.
1   -0.9389   0.1489
2    0.4397   0.1292
3    1.2790   0.1707
4    2.3200   0.3715
> 

The variables that you've created are indeed bivariate normal, but they are
highly correlated, and your choice of cut points makes it hard to estimate
the correlation from the contingency tables, apparently producing some
difficulty in the maximization of the likelihood. Nevertheless, the ML
estimates of the correlation and thresholds for the set of data above are
pretty good. (In your case, the optimization failed.)

BTW, a more straightforward way to create the categorical variables would be

discrete.x <- cut(test.x, c(-Inf, -2.5, -1, -.5, .5, Inf))
discrete.c <- cut(test.c, c(-Inf, -1, 1, 2, 3, Inf))

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Rosenbaum, Janet
> Sent: Friday, August 04, 2006 5:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] polychoric correlation error
> 
> 
> Dear all,
> 
> I get a strange error when I find polychoric correlations 
> with the ML method, which I have been able to reproduce using 
> randomly-generated data.
> 
> What is wrong?  
> I realize that the data that I generated randomly is a bit 
> strange, but it is the only way that I duplicate the error message.
> 
> 
> > n<-100
> > test.x<-rnorm(n, mean=0, sd=1)
> > test.c<-test.x + rnorm(n, mean=0, sd=.5) thresh.x<-c(-2.5, -1, -.5, 
> > .5, 1000) thresh.c<-c(-1, 1, 2, 3, 1000)
> > 
> > discrete.x<-discrete.c<-vector(length=n)
> > 
> > for (i in 1:n) {
> + 	discrete.x[i]<-which.min(thresh.x < test.x[i] )
> + 	discrete.c[i]<-which.min(thresh.c < test.c[i] ) }
> > pc<-polychor(discrete.x, discrete.c, std.err=T, ML=T)
> Error in optim(c(optimise(f, interval = c(-1, 1))$minimum, 
> rc, cc), f,  : 
> 	non-finite finite-difference value [1]
> In addition: There were 50 or more warnings (use warnings() 
> to see the first 50)
> > print(pc)
> Error in print(pc) : object "pc" not found
> > warnings()
> Warning messages:
> 1: NaNs produced in: log(x)
> 2: NA/Inf replaced by maximum positive value
> 3: NaNs produced in: log(x) 
> 
> 
> ---
> 
> Thanks,
> 
> Janet
> 
> --------------------
> 
> This email message is for the sole use of the intended\ > ...{{dropped}}


From ripley at stats.ox.ac.uk  Sat Aug  5 08:25:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Aug 2006 07:25:26 +0100 (BST)
Subject: [R] Postscript fonts
In-Reply-To: <44D3C3DE.3090509@univie.ac.at>
References: <44D3C3DE.3090509@univie.ac.at>
Message-ID: <Pine.LNX.4.64.0608050722200.30652@gannet.stats.ox.ac.uk>

On Sat, 5 Aug 2006, Erich Neuwirth wrote:

> How can I find out what fonts are available for
> par(family=
> for the postscript device?

This is dynamic: for the current value

> names(postscriptFonts())
 [1] "serif"                "sans"                 "mono"
 [4] "symbol"               "AvantGarde"           "Bookman"
 [7] "Courier"              "Helvetica"            "Helvetica-Narrow"
[10] "NewCenturySchoolbook" "Palatino"             "Times"
[13] "URWGothic"            "URWBookman"           "NimbusMon"
[16] "NimbusSan"            "URWHelvetica"         "NimbusSanCond"
[19] "CenturySch"           "URWPalladio"          "NimbusRom"
[22] "URWTimes"             "ComputerModern"       "ComputerModernItalic"
[25] "Japan1"               "Japan1HeiMin"         "Japan1GothicBBB"
[28] "Japan1Ryumin"         "Korea1"               "Korea1deb"
[31] "CNS1"                 "GB1"

and for more details see the current R-News (6/2).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ffenics2002 at yahoo.co.uk  Sat Aug  5 12:27:10 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Sat, 5 Aug 2006 11:27:10 +0100 (BST)
Subject: [R] formating for dist function
Message-ID: <20060805102710.12596.qmail@web25504.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060805/ef3c4d3e/attachment.pl 

From erich.neuwirth at univie.ac.at  Sat Aug  5 14:14:10 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sat, 05 Aug 2006 14:14:10 +0200
Subject: [R] Postscript fonts
Message-ID: <44D48B92.6040003@univie.ac.at>

Thanks,

The following piece of code demonstrates a problem I still have.

pdf(file="fonttest.pdf",fonts=c("Helvetica"))
plot(1:10,main="",xlab="")
par(family="Helvetica",font=1)
title(main="Helvetica")
par(family="Helvetica",font=2)
title(sub="Helvetica-Bold")
par(family="Helvetica",font=3)
title(xlab="Helvetica-Oblique")
dev.off()

The font=  parameter seems not to be respected.
I do not get bold or italics.
What am I doing wrong or not understand here?

My original problem was that I had not understood that the
fonts parameter is absolutely necessary when opening the device
when one wants to use par(family= ).


BTW, this is R 2.3.1 on Windows XP.



Prof Brian Ripley wrote:
> > On Sat, 5 Aug 2006, Erich Neuwirth wrote:
> >
>> >> How can I find out what fonts are available for
>> >> par(family=
>> >> for the postscript device?
> >
> > This is dynamic: for the current value
> >
>> >> names(postscriptFonts())
> >  [1] "serif"                "sans"                 "mono"
> >  [4] "symbol"               "AvantGarde"           "Bookman"
> >  [7] "Courier"              "Helvetica"            "Helvetica-Narrow"
> > [10] "NewCenturySchoolbook" "Palatino"             "Times"
> > [13] "URWGothic"            "URWBookman"           "NimbusMon"
> > [16] "NimbusSan"            "URWHelvetica"         "NimbusSanCond"
> > [19] "CenturySch"           "URWPalladio"          "NimbusRom"
> > [22] "URWTimes"             "ComputerModern"
"ComputerModernItalic"
> > [25] "Japan1"               "Japan1HeiMin"         "Japan1GothicBBB"
> > [28] "Japan1Ryumin"         "Korea1"               "Korea1deb"
> > [31] "CNS1"                 "GB1"
> >
> > and for more details see the current R-News (6/2).
-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-39459


From JeeBee at troefpunt.nl  Sat Aug  5 14:15:58 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Sat, 05 Aug 2006 14:15:58 +0200
Subject: [R] Interpretation of call to aov()
Message-ID: <pan.2006.08.05.12.15.56.290792@troefpunt.nl>

Hi all,

I've been reading about aov() at
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html and
http://davidmlane.com/hyperstat/intro_ANOVA.html and
I try to use this test in experiments with my simulator.

What I would like Anova to tell me is whether the differences I see
when plotting the means of performance per method are significant.
And also, whether this is dependent on the problem size (bigger is
more complex).
I would be very grateful if there's somebody more mathematically skilled
on this list who could tell me whether I'm drawing correct conclusions.

> data
    performance  method problem
1   146780.0000      -f     960
2     4654.0000      -f     160
3    45840.0000      -f     320
4    54750.0000      -f     320
5    91750.0000      -f     480
6     7452.0000      -f     160
7     8866.0000      -f     160
8     8513.0000      -f     160
9   139520.0000      -f     960
10   85380.0000      -f     480
<snip>

> str(data)
`data.frame':   419 obs. of  3 variables:
 $ performance: num  146780   4654  45840  54750  91750 ...
 $ method     : Factor w/ 7 levels "-f","-f -q","-h0 -r0",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ problem    : int  960 160 320 320 480 160 160 160 960 480 ...

>   summary(aov(performance ~ method * problem, data=data))
                Df     Sum Sq    Mean Sq F value    Pr(>F)
method           6 3.3185e+11 5.5308e+10  416.91 < 2.2e-16 ***
problem          1 5.7141e+11 5.7141e+11 4307.26 < 2.2e-16 ***
method:problem   6 9.8891e+10 1.6482e+10  124.24 < 2.2e-16 ***
Residuals      405 5.3728e+10 1.3266e+08
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

I interpret this data as follows:

-1- The performance depends on the chosen method.
If I compute the overall performance means for each method, this will
give me numbers that are significantly different. This means, the method
with the greatest mean is significantly better than at least some other
methods (and not worse than any other method).

-2- The performance depends on the problem complexity.
This is not so interesting. In my setting it is trivial that performance
is worse for more complex problems.

-3- There is interaction between method and complexity, in other words,
when trying to order the methods from bad to good, one cannot simply do
this without taking the problem complexity into account. (for simple
problems method A might be the best, for complex problems, another method
might be the better).

I have not used Error() in my call to aov().
I've seen this one being used: Error(subj/(shape * color)
But I do not have subjects. Or in fact, I believe I have only 1, which is
my simulator. Am I correct about that? Or should I use something like
Error(method * problem) ?

Thanks in advance,
JeeBee.


From JeeBee at troefpunt.nl  Sat Aug  5 14:33:39 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Sat, 05 Aug 2006 14:33:39 +0200
Subject: [R] Interpretation of call to aov()
References: <pan.2006.08.05.12.15.56.290792@troefpunt.nl>
Message-ID: <pan.2006.08.05.12.33.37.643064@troefpunt.nl>


Or, perhaps I can use Error(subject/(method * problem))
where subject = paste(method, problem)

Because I repeated each simulation several times for
different problems of the same problem.
This gives me the following output:

> summary(aov(performance ~ method * problem + Error(subj/(method *
> problem)), data=data))

Error: subj
          Df     Sum Sq    Mean Sq F value    Pr(>F)
method        6 3.3185e+11 5.5308e+10  46.179 3.020e-13 ***
problem        1 5.7141e+11 5.7141e+11 477.094 < 2.2e-16 ***
method:problem    6 9.8891e+10 1.6482e+10  13.761 3.123e-07 ***
Residuals 28 3.3535e+10 1.1977e+09
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Error: Within
           Df     Sum Sq    Mean Sq F value Pr(>F)
Residuals 377 2.0193e+10 5.3562e+07
Warning message:
Error() model is singular in: aov(performance ~ method * problem +
Error(subj/(method * problem)), data = data)


From JeeBee at troefpunt.nl  Sat Aug  5 14:46:29 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Sat, 05 Aug 2006 14:46:29 +0200
Subject: [R] question
References: <BAY102-F31BE714A68A119AAFCBE8D9C500@phx.gbl>
Message-ID: <pan.2006.08.05.12.46.26.872716@troefpunt.nl>

Hi Jessica,

I think you want to use names()

See:
?names
?colnames (or rownames)

If not, it's perhaps a good idea to send us a more complete example.
JeeBee.


On Fri, 04 Aug 2006 17:12:24 +0200, Jessica G. wrote:
> How to convert a rowname vector of numbers into a real column of the matrix,
> 
> My problem is that I applied a rowsum function on a matrix.
> Then I get a matrix in which the names of the columns are the values of the 
> group (numbers)
> Now I need to make calculation on the groups row.
> How to convert this vector of (rownames) into a real column in the matrix ?


From ggrothendieck at gmail.com  Sat Aug  5 15:08:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 5 Aug 2006 09:08:33 -0400
Subject: [R] formating for dist function
In-Reply-To: <20060805102710.12596.qmail@web25504.mail.ukl.yahoo.com>
References: <20060805102710.12596.qmail@web25504.mail.ukl.yahoo.com>
Message-ID: <971536df0608050608t41c7ef60q64aa2df7f4dc60c6@mail.gmail.com>

Here are three ways:

# read in data
Lines <- "object1 object1 78
object1 object2 45
object1 object3 34
object1 object4 45
object2 object2 89
object2 object3 32
object2 object4 13
"
DF <- read.table(textConnection(Lines))

# 1 - xtabs
xt <- as.matrix(xtabs(V3 ~., DF))

# 2 - reshape
wide <- reshape(DF, direction = "wide", idvar = "V1", timevar = "V2")
rownames(wide) <- wide$V1
colnames(wide) <- sub(".*[.]", "", colnames(wide))
wide <- as.matrix(wide[,-1])

# 3 - [
mat <- matrix(0, nlevels(DF$V1), nlevels(DF$V2),
	dimnames = list(levels(DF$V1), levels(DF$V2)))
mat[cbind(DF$V1, DF$V2)] <- DF$V3


On 8/5/06, Ffenics <ffenics2002 at yahoo.co.uk> wrote:
> Hi there
> I have a list that looks like this
> object1 object1 78
> object1 object2 45
> object1 object3 34
> object1 object4 45
> object2 object2 89
> object2 object3 32
> object2 object4 13
>
> but i want to create a matrix like this in order to use the dist function of R
>
>
>             object1       object2         object3          object4
> object1   78          45          34            45
>
> object2        45                     89                     32                        13
>
> Is there a method in R that will take a list and format it in this way?
>
> Any help much appreciated.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jc at or.psychology.dal.ca  Sat Aug  5 17:09:57 2006
From: jc at or.psychology.dal.ca (John Christie)
Date: Sat, 05 Aug 2006 12:09:57 -0300
Subject: [R] R GUI for Mac OS X bug
Message-ID: <8DB79988-6A79-4A67-920B-2058EC6EB704@or.psychology.dal.ca>

Hi,
	I found a highly reproduceable bug in the version that comes with  
2.3.1 and I just thought I'd let the authors know.  Pressing " and  
then tab instantly crashes it every time.


From andreas.beyerlein at gmx.de  Sat Aug  5 18:29:16 2006
From: andreas.beyerlein at gmx.de (Andreas Beyerlein)
Date: Sat, 5 Aug 2006 18:29:16 +0200
Subject: [R] AIC for lognormal model
Message-ID: <000a01c6b8ac$4be62ee0$ab098752@intra.arcisnet.mhn.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060805/516153c5/attachment.pl 

From ffenics2002 at yahoo.co.uk  Sat Aug  5 19:08:23 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Sat, 5 Aug 2006 18:08:23 +0100 (BST)
Subject: [R] Kmeans - how to display results
Message-ID: <20060805170823.24531.qmail@web25514.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060805/d7efcd70/attachment.pl 

From andrej.kastrin at siol.net  Sat Aug  5 20:01:05 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sat, 05 Aug 2006 20:01:05 +0200
Subject: [R] Kmeans - how to display results
In-Reply-To: <20060805170823.24531.qmail@web25514.mail.ukl.yahoo.com>
References: <20060805170823.24531.qmail@web25514.mail.ukl.yahoo.com>
Message-ID: <44D4DCE1.6030407@siol.net>

What's wrong with cross-tabs?


From ripley at stats.ox.ac.uk  Sat Aug  5 20:04:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Aug 2006 19:04:02 +0100 (BST)
Subject: [R] Kmeans - how to display results
In-Reply-To: <20060805170823.24531.qmail@web25514.mail.ukl.yahoo.com>
References: <20060805170823.24531.qmail@web25514.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608051903160.24379@gannet.stats.ox.ac.uk>

There are worked examples in MASS (the book, see the FAQ).

On Sat, 5 Aug 2006, Ffenics wrote:

> I'm very new as regards to R. I have managed to work out how to use dist and kmeans but am now wondering how best to display the results from kmeans in a graphical form.
> If anyone has any general advice/tips, I would be most grateful.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sat Aug  5 20:08:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Aug 2006 19:08:36 +0100 (BST)
Subject: [R] AIC for lognormal model
In-Reply-To: <000a01c6b8ac$4be62ee0$ab098752@intra.arcisnet.mhn.de>
References: <000a01c6b8ac$4be62ee0$ab098752@intra.arcisnet.mhn.de>
Message-ID: <Pine.LNX.4.64.0608051900430.24379@gannet.stats.ox.ac.uk>

On Sat, 5 Aug 2006, Andreas Beyerlein wrote:

> Dear all,
> 
> I want to compare some different models for a dataset by QQ plots and 
> AIC. I get the following AICs:
> 
> - linear model: 19759.66
> - GAMLSS model: 18702.7
> - linear model with lognormal response: -7862.182
> 
> The QQ plots show that the lognormal model fits better than the linear 
> model, but still much worse than the GAMLSS. So, in my opinion, the AIC 
> of the lognormal model should be between the AICs of the both other 
> models. What happens here?

> Btw: For the lognormal model, I transformed the response variable by 
> log(). Apart from that, I used the same formula as for the linear model.

So you got the AIC for the logged data, which is not comparable to the 
others.  You need to convert to a likelihood and hence AIC for the 
original data.  (I think anyone using AIC needs to know how to do that, as 
it is part of the basic understanding of what a likelihood is.  It is also 
part of the derivation of the estimation of the Box-Cox transformation, 
something which you might well want to consider here.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Gregor.Gorjanc at bfro.uni-lj.si  Sun Aug  6 01:36:14 2006
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 6 Aug 2006 01:36:14 +0200
Subject: [R] R CMD check and RUnit
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31C3B@pollux.bfro.uni-lj.si>

Hi!

I appologize for crossposting, but this might be of broader interest.

In case you are interested in fusing RUnit with R CMD check under unix
alike OS, here is one way of doing/hacking this.

My aim was to perform unit tests:
(1) during R CMD check
(2) at any other time

Say you have a package PKG in a map PKG. I use the following structure

PKG
 |- R
 |- ...
 |- inst
 |   |- doc
 |   `- unitTests
 |- ...
 `- tests

Then the following files (content is at the end of the mail) 
are used:
 - PKG/tests/doRUnit.R
 - PKG/inst/unitTests/Makefile
 - PKG/inst/unitTests/runit.*.R

Now when I launch 

cd /some/path/PKG/..
R CMD check PKG 

doRUnit.R in map tests is sourced and all unit tests in 
PKG/inst/unitTests are issued - aim (1). Output of tests can be 
seen in either:
  - PKG.Rcheck/tests/doRUnit.Rout
  - PKG.Rcheck/PKG/unitTests/report.txt
  - PKG.Rcheck/PKG/unitTests/report.html

R CMD check will say OK also when some unit tests fails, but
unit tests are at least issued and one should check above outputs
to see the results of unit testing. I use the following in my
~/.Makefile to see these results imidiately

NAME := $(shell basename $(PWD)) # Get current map name

Rcheck: # Check R package of current map
	(cd ..; \
	R CMD check $(NAME) && \
	if [ -d $(NAME)/inst/unitTests ]; then \
	  cat $(NAME).Rcheck/tests/doRUnit.Rout; \
	fi)

Then only the following is necesarry to start R CME check and
print results of unit testing (if it is used)

mymake Rcheck

where mymake is a shell function

mymake ()
{
    make -f ~/.Makefile $*
}
export -f mymake

----------------------------------------------------------------------

If one wants to run only unit tests - aim (2) against currently 
installed code, use

cd PKG/inst/unitTests
make test

or the following if package should be first re-installed, due
to changes

cd PKG/inst/unitTests
make

----------------------------------------------------------------------

Please note that this hack is not only my work as I have taken 
the structure and initial idea from package graph in BioC.

----------------------------------------------------------------------

Content of above mentioned files:

PKG/tests/doRUnit.R
----------------------------------------------------------------------
if(require("RUnit", quietly=TRUE)) {

  ## --- Setup ---

  library("PKG")
  testDir <- "unitTests"
  path <- "../PKG"
  ## Path for standalone i.e. not by R CMD check testing
  if(Sys.getenv("RCMDCHECK") == "FALSE") path <- "../inst"
  path <- file.path(getwd(), path, testDir)
  pathReport <- file.path(path, "report")

  ## --- Testing ---

  ## Define tests
  testsuite.PKG <- defineTestSuite(name="PKG unit testing",
                                   dirs=path)
  ## Run
  tests <- runTestSuite(testsuite.PKG)

  ## Print results
  printTextProtocol(tests)
  printTextProtocol(tests, fileName=paste(pathReport, ".txt", sep=""))

  ## Print HTML version to a file
  printHTMLProtocol(tests, fileName=paste(pathReport, ".html", sep=""))
}
----------------------------------------------------------------------

PKG/inst/unitTests/Makefile
----------------------------------------------------------------------
PKG=PKG
TOP=../../..
SUITE=doRUnit.R

all: inst test

inst: # Install package
        cd ${TOP};\
        R CMD INSTALL ${PKG}

test: # Run unit tests
        export RCMDCHECK=FALSE;\
        cd ${TOP}/${PKG}/tests;\
        R --slave < ${SUITE}
----------------------------------------------------------------------

PKG/inst/unitTests/runit.*.R
----------------------------------------------------------------------
# As many files with RUnit testing as you want
----------------------------------------------------------------------

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From bgreen at dyson.brisnet.org.au  Sun Aug  6 06:42:35 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sun, 06 Aug 2006 14:42:35 +1000
Subject: [R] ordering by a datframe date
In-Reply-To: <mailman.11.1153562403.16192.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060806140813.02a25070@pop3.brisnet.org.au>


I am hoping for some advice regarding ordering a dataframe, by date.

The dataframe is in the format below.

$story		$datepub
story10      1 April 1999
story 90    1 March 2002
story 37    10 July 1985

	
I want to reorder the entire dataframe so the earliest story is first, and 
save the reordered dataframe. The command, 'class' (datepub) reveals 
$datepub is a factor variable.

I tried the following:

d2 <- as.character(datepub)
rank(d2)

Any assistance is appreciated,

regards

Bob Green


From ggrothendieck at gmail.com  Sun Aug  6 07:37:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Aug 2006 01:37:21 -0400
Subject: [R] ordering by a datframe date
In-Reply-To: <5.1.0.14.0.20060806140813.02a25070@pop3.brisnet.org.au>
References: <mailman.11.1153562403.16192.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060806140813.02a25070@pop3.brisnet.org.au>
Message-ID: <971536df0608052237s5e529dd8v9404c98d7eeafa5f@mail.gmail.com>

Try this:


Lines <- "story,datepub
story10,1 April 1999
story 90,1 March 2002
story 37,10 July 1985
"
DF <- read.csv(textConnection(Lines))

DF[order(as.Date(DF$datepub, "%d %B %Y")),]


On 8/6/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
>
> I am hoping for some advice regarding ordering a dataframe, by date.
>
> The dataframe is in the format below.
>
> $story          $datepub
> story10      1 April 1999
> story 90    1 March 2002
> story 37    10 July 1985
>
>
> I want to reorder the entire dataframe so the earliest story is first, and
> save the reordered dataframe. The command, 'class' (datepub) reveals
> $datepub is a factor variable.
>
> I tried the following:
>
> d2 <- as.character(datepub)
> rank(d2)
>
> Any assistance is appreciated,
>
> regards
>
> Bob Green
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From deepayan.sarkar at gmail.com  Sun Aug  6 08:17:56 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 6 Aug 2006 01:17:56 -0500
Subject: [R] ggplot facet label font size
In-Reply-To: <1F5C95F1B887EF42B14F31AEF36AD22F060661C1@dlee04.ent.ti.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F060661C1@dlee04.ent.ti.com>
Message-ID: <eb555e660608052317n3921ddbfq59386b693ef27990@mail.gmail.com>

On 8/3/06, Walker, Sam <s-walker at ti.com> wrote:
>
> This works OK, but there is some extra spacing between the panels, the
> top axis and the strip on the top, and the left labels and panel.
>
> How can I remove these extra spaces?

This turns out to be surprisingly easy (surprising to me at least):


#########

my.strip <-
    function(which.given, which.panel, var.name, ...)
{
    if (which.given == 1 && which.panel[2] == 2)
        strip.default(1, which.panel[1],
                      var.name = var.name[1],
                      ...)
}


my.strip.left <-
    function(which.given, which.panel, var.name, ..., horizontal)
{
    if (which.given == 2 && which.panel[1] == 1)
        strip.default(1, which.panel[2],
                      var.name = var.name[2],
                      horizontal = FALSE, ...)
}


histogram(~ tip/total_bill | sex + smoker, tips,
          strip = my.strip,
          strip.left = my.strip.left,
          par.strip.text = list(lines = 0.6),
          par.settings =
          list(layout.heights = list(strip = c(0, 1)),
               layout.widths = list(strip.left = c(1, 0)),
               add.text = list(cex = 0.7)))


#########

The trick of changing two-line strips to one-line strips is not
obvious from the documentation (such as it is), it depends on the
implementation of strips.

HTH,

Deepayan

> I've tried changing various layout.widths settings with no luck.  It
> seems the spaces are calculated based on the number of conditioning
> variables, in this case 2 (sex+smoker).
>
>
> Thanks in advance...
> -Sam
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Wednesday, August 02, 2006 6:04 PM
> To: Walker, Sam
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ggplot facet label font size
>
> On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> > How do I change the font size in the facet labels along the edges of
> the
> > plot?
> >
> > For example (from the ggplot help file):
> >     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
> >     gghistogram(p)
> >
> > In this plot, the facet labels are "smoker: No", "smoker: Yes", "sex:
> > Female", "sex: Male".  What command can I use to reduce the font size
> of
> > these labels?
> >
> > In lattice terminology, cex is used to scale these strip labels.  But
> I
> > couldn't find the equivalent in ggplot.
> >
> > The reason I'm asking is I have a 9x7 array of plots which I've been
> > plotting with lattice.  I wanted to use ggplot because I like having
> the
> > labels on the edge of the plots
>
> Note that lattice can do that by using custom strip functions:
>
> library(ggplot) # data resides here
> library(lattice)
>
> my.strip <- function(which.given, which.panel, ...)
>    if (which.given == 1 && which.panel[2] == 2)
>       strip.default(which.given, which.panel, ...)
>
> my.strip.left <- function(which.given, which.panel, ..., horizontal)
>    if (which.given == 2 && which.panel[1] == 1)
>       strip.default(which.given, which.panel, horizontal = FALSE, ...)
>
> histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
>      strip.left = my.strip.left, par.settings = list(add.text =
> list(cex = 0.7)))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://www.stat.wisc.edu/~deepayan/


From henrik.parn at bio.ntnu.no  Sun Aug  6 16:13:29 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Sun, 06 Aug 2006 16:13:29 +0200
Subject: [R] paired t-test. Need to rearrange data?
Message-ID: <44D5F909.9040507@bio.ntnu.no>

Dear all,

I have received some data arranged like this:

# original data
id <- rep(letters[1:6], each=2)
time <- as.factor(rep(1:2, 6))
y <- as.vector(replicate(6, c(rnorm(n=1, mean=1), rnorm(n=1, mean=2))))
test.data <- data.frame(id, time, y)
test.data

I would like to perform a paired t-test of the y-values at time=1 
against those at time=2, with samples paired by their id. Is it 
necessary to arrange the data in a format like this:   

# rearranged data
id <- letters[1:6]
y1 <- replicate(6, rnorm(n=1, mean=1)) # y-value at time = 1
y2 <- replicate(6, rnorm(n=1, mean=2)) #  y-value at time = 2
test.data2 <- data.frame(id, y1, y2)
test.data2

...in order to perform a paired t-test?
t.test(y1, y2, paired=T)

If yes, which is the most convenient way to rearrange the data?
Or is it possible to apply the paired t-test function to the original 
data set?

And a side question: In my examples, I suppose can I use set.seed to 
reproduce the 'rnorm-values' created in the 'original data' also in my 
the 'rearranged data'. Can someone give me a hint of how to apply the 
same 'seed' to all the rnorms?


Thanks a lot in advance!


Henrik


From h.wickham at gmail.com  Sun Aug  6 16:53:43 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 6 Aug 2006 07:53:43 -0700
Subject: [R] [R-pkgs] Reshape package: new version 0.7
Message-ID: <f8e6ff050608060753w28e532cby139b9abdf579844@mail.gmail.com>

Reshape 0.7
===================

Reshape is an R package for flexibly restructuring and aggregating
data. It is inspired by Excel's pivot tables, and it  makes it very
easy to view your data the way you want. The reshape package (along
with ggplot) received the John Chambers Award for Statistical
Computing. You can find out more at http://had.co.nz/reshape.

Reshape (hopefully) makes it easy to do what you have been struggling
to do with tapply, by, aggregate, xtabs, apply and summarise.  It is
also useful for getting your data into the correct structure for
lattice or ggplot plots.

This new version of reshape substantially expands the type of output
you can cast into.  You can now make nested lists, cast(m, a ~ b | c)
or cast(m, a ~ b | c + d), and multidimensional arrays, cast(m, a ~ b
~ c) (or any combination of the two).  See the examples in ?cast.

What else is new?:

 * iapply, an idempotent apply function that returns results with the
same dimensionality as the input (very useful in conjunction with
sweep and multidimensional arrays)

 * rescaler, a function to rescale data.frames variable by variable
using a range of different scaling methods

 * combine_factor and reorder_factor to make it easier to combine and
reorder factor levels

 * bug fixes!

All feedback is welcomed, and if you are struggling with reshaping
your data and the reshape package isn't helping, please let me know.

Regards,

Hadley
http://had.co.nz/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Sun Aug  6 17:24:52 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 6 Aug 2006 16:24:52 +0100 (BST)
Subject: [R] paired t-test. Need to rearrange data?
In-Reply-To: <44D5F909.9040507@bio.ntnu.no>
References: <44D5F909.9040507@bio.ntnu.no>
Message-ID: <Pine.LNX.4.64.0608061610300.8253@gannet.stats.ox.ac.uk>

On Sun, 6 Aug 2006, Henrik Parn wrote:

> Dear all,
> 
> I have received some data arranged like this:
> 
> # original data
> id <- rep(letters[1:6], each=2)
> time <- as.factor(rep(1:2, 6))
> y <- as.vector(replicate(6, c(rnorm(n=1, mean=1), rnorm(n=1, mean=2))))
> test.data <- data.frame(id, time, y)
> test.data
> 
> I would like to perform a paired t-test of the y-values at time=1 
> against those at time=2, with samples paired by their id. Is it 
> necessary to arrange the data in a format like this:   
> 
> # rearranged data
> id <- letters[1:6]
> y1 <- replicate(6, rnorm(n=1, mean=1)) # y-value at time = 1

Really, rnorm(6, 1) suffices here.

> y2 <- replicate(6, rnorm(n=1, mean=2)) #  y-value at time = 2
> test.data2 <- data.frame(id, y1, y2)
> test.data2
> 
> ...in order to perform a paired t-test?
> t.test(y1, y2, paired=T)
> 
> If yes, which is the most convenient way to rearrange the data?
> Or is it possible to apply the paired t-test function to the original 
> data set?

t.test(y ~ time, test.data, paired=TRUE)

> And a side question: In my examples, I suppose can I use set.seed to 
> reproduce the 'rnorm-values' created in the 'original data' also in my 
> the 'rearranged data'. Can someone give me a hint of how to apply the 
> same 'seed' to all the rnorms?

Using the same seed will give identical values from rnorm, surely not what 
you want.  You need to generate the rnorms in the same order, that is all.
matrix(rnorm(12) + c(1,2), 6, 2, byrow=TRUE) will do the trick.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.y.wong at leeds.ac.uk  Sun Aug  6 20:10:52 2006
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Sun, 6 Aug 2006 19:10:52 +0100
Subject: [R] extractAIC using surf.ls
Message-ID: <DE508AFC-D7C8-412F-9C1F-FA7DB49B164F@leeds.ac.uk>

Although the 'spatial' documentation doesn't mention that extractAIC  
works, it does seem to give an output.
I may have misunderstood, but shouldn't the following give at least  
the same d.f.?

 > library(spatial)
 > data(topo, package="MASS")
 > extractAIC(surf.ls(2, topo))
[1]  46.0000 437.5059
 > extractAIC(lm(z ~ x+I(x^2)+y+I(y^2)+x:y, topo))
[1]   6.0000 357.5059

# and if the AIC values differ, shouldn't they do so by an additive  
constant?

 > (extractAIC(surf.ls(2, topo))-extractAIC(lm(z ~ x+I(x^2)+y+I(y^2) 
+x:y, topo)))[2]
[1] 80
 > (extractAIC(surf.ls(1, topo))-extractAIC(lm(z ~ x+y, topo)))[2]
[1] 92

# Using R 2.3.1 (OS X), spatial version 7.2-27.1

Thanks

Yan


From kubovy at virginia.edu  Sun Aug  6 20:40:05 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 6 Aug 2006 14:40:05 -0400
Subject: [R] Beamer and Sweave
Message-ID: <6E6114C7-7AEB-40C6-9A83-5C6B939F51BC@virginia.edu>

Dear R-helpers,

Here is a minimal .Rnw file which shows that builds do not work in  
frames that contain chunks of verbatim code:

\documentclass[]{beamer}

\author{}
\date{}

\title{Title}

\begin{document}

\frame[containsverbatim]{\frametitle{Here the build doesn't work}
\begin{enumerate}[<+->]
	\item A
	\item \alert{B}
	\item C
\end{enumerate}
<<generateIQ>>=
	iq <- c(96, 102, 104, 104, 108, 110)
@
}

\frame{\frametitle{Here it does}
\begin{enumerate}[<+->]
	\item A
	\item \alert{B}
	\item C
\end{enumerate}
}

\end{document}

Is this a Beamer problem or an Sweave problem? Suggestions?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From msubianto at gmail.com  Sun Aug  6 21:45:15 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Sun, 6 Aug 2006 21:45:15 +0200
Subject: [R] Take random sample from class variable
Message-ID: <c7c17cef0608061245m78bd4417md2a3df3dd18ad97d@mail.gmail.com>

Dear all,
Suppose I have a dataset like below, then I take for example, 100
random sample "class" variable where contains "yes" and "no"
respectively, 70% and 30%.
I need a new 100 random sample from mydat dataset, but I can't get the result.
Thanks you very much for any helps.
Best, Muhammad Subianto

mydat <- data.frame(size=c(30,12,15,10,12,12,25,30,20,14),
                       A=c(0,1,0,1,0,1,1,1,0,0),
                       B=c(1,1,0,1,0,1,1,0,0,1),
                       C=c(0,0,1,1,0,0,1,1,0,0),
                       D=c(1,1,1,1,0,1,0,0,1,1),
                       E=c(1,1,0,1,1,1,1,1,1,0),

Class=c("yes","yes","no","yes","yes","no","yes","no","yes","yes"))
mydat
# Maximal data from dataset
max.size <- sum(mydat$size);max.size
# I need sample random
nof.sample <- 100
set.seed(123)
sample.class <- sample(c("yes","no"), nof.sample, prob=c(.7, .3), replace=TRUE)
sample.class
sampledat.class <- mydat[sample.class,]
sampledat.class


From deepayan.sarkar at gmail.com  Sun Aug  6 22:21:30 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 6 Aug 2006 15:21:30 -0500
Subject: [R] Beamer and Sweave
In-Reply-To: <6E6114C7-7AEB-40C6-9A83-5C6B939F51BC@virginia.edu>
References: <6E6114C7-7AEB-40C6-9A83-5C6B939F51BC@virginia.edu>
Message-ID: <eb555e660608061321rf7d1d2dv15252826c1386148@mail.gmail.com>

On 8/6/06, Michael Kubovy <kubovy at virginia.edu> wrote:
> Dear R-helpers,
>
> Here is a minimal .Rnw file which shows that builds do not work in
> frames that contain chunks of verbatim code:
>
> \documentclass[]{beamer}
>
> \author{}
> \date{}
>
> \title{Title}
>
> \begin{document}
>
> \frame[containsverbatim]{\frametitle{Here the build doesn't work}
> \begin{enumerate}[<+->]
>         \item A
>         \item \alert{B}
>         \item C
> \end{enumerate}
> <<generateIQ>>=
>         iq <- c(96, 102, 104, 104, 108, 110)
> @
> }
>
> \frame{\frametitle{Here it does}
> \begin{enumerate}[<+->]
>         \item A
>         \item \alert{B}
>         \item C
> \end{enumerate}
> }
>
> \end{document}
>
> Is this a Beamer problem or an Sweave problem? Suggestions?

Beamer. You need

\begin{frame}[fragile]
\frametitle{Should work now}
 \begin{enumerate}[<+->]
         \item A
         \item \alert{B}
         \item C
 \end{enumerate}
<<generateIQ>>=
         iq <- c(96, 102, 104, 104, 108, 110)
@
\end{frame}

(not sure what the \frame version is, probably \frame[fragile]

-Deepayan


From kubovy at virginia.edu  Sun Aug  6 23:09:32 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 6 Aug 2006 17:09:32 -0400
Subject: [R] Beamer and Sweave
In-Reply-To: <eb555e660608061321rf7d1d2dv15252826c1386148@mail.gmail.com>
References: <6E6114C7-7AEB-40C6-9A83-5C6B939F51BC@virginia.edu>
	<eb555e660608061321rf7d1d2dv15252826c1386148@mail.gmail.com>
Message-ID: <1FD10378-06EE-4CDB-B203-7799C72C84D9@virginia.edu>

Thanks Deepayan,

\begin{frame}[fragile]
...
\end{frame}

works, but

\frame[fragile]{
...
}

doesn't.

\frame[containsverbatim]{
...
}

works when there's no build, but there's a
\begin{Schunk}
...
\end{frame}

in the frame body.

On Aug 6, 2006, at 4:21 PM, Deepayan Sarkar wrote:

> On 8/6/06, Michael Kubovy <kubovy at virginia.edu> wrote:
>> Dear R-helpers,
>>
>> Here is a minimal .Rnw file which shows that builds do not work in
>> frames that contain chunks of verbatim code:
>>
>> \documentclass[]{beamer}
>>
>> \author{}
>> \date{}
>>
>> \title{Title}
>>
>> \begin{document}
>>
>> \frame[containsverbatim]{\frametitle{Here the build doesn't work}
>> \begin{enumerate}[<+->]
>>         \item A
>>         \item \alert{B}
>>         \item C
>> \end{enumerate}
>> <<generateIQ>>=
>>         iq <- c(96, 102, 104, 104, 108, 110)
>> @
>> }
>>
>> \frame{\frametitle{Here it does}
>> \begin{enumerate}[<+->]
>>         \item A
>>         \item \alert{B}
>>         \item C
>> \end{enumerate}
>> }
>>
>> \end{document}
>>
>> Is this a Beamer problem or an Sweave problem? Suggestions?
>
> Beamer. You need
>
> \begin{frame}[fragile]
> \frametitle{Should work now}
> \begin{enumerate}[<+->]
>         \item A
>         \item \alert{B}
>         \item C
> \end{enumerate}
> <<generateIQ>>=
>         iq <- c(96, 102, 104, 104, 108, 110)
> @
> \end{frame}
>
> (not sure what the \frame version is, probably \frame[fragile]
>
> -Deepayan

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From sunyata at uvic.ca  Mon Aug  7 00:04:38 2006
From: sunyata at uvic.ca (Graham Watt-Gremm)
Date: Sun, 6 Aug 2006 15:04:38 -0700
Subject: [R] R GUI for Mac OS X bug
In-Reply-To: <8DB79988-6A79-4A67-920B-2058EC6EB704@or.psychology.dal.ca>
References: <8DB79988-6A79-4A67-920B-2058EC6EB704@or.psychology.dal.ca>
Message-ID: <29B54FAC-57CA-4655-8F84-CDEEBC4CA799@uvic.ca>

In my case
R.app Version 1.14 (2129), Tiger (10.4.7), R.app burps the following  
warning message, but does not crash:

2006-08-06 15:01:05.644 R[1822] -[NSCFString substringFromIndex:]  
called with out-of-bounds index. For apps linked on Tiger this will  
raise an exception. For earlier apps it will produce this one-time  
warning and continue with existing behavior (which is undefined).

hope this helps

Graham




On 5-Aug-06, at 8:09 AM, John Christie wrote:

> Hi,
> 	I found a highly reproduceable bug in the version that comes with
> 2.3.1 and I just thought I'd let the authors know.  Pressing " and
> then tab instantly crashes it every time.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lebouton at msu.edu  Mon Aug  7 00:21:03 2006
From: lebouton at msu.edu (Joseph LeBouton)
Date: Sun, 06 Aug 2006 17:21:03 -0500
Subject: [R] removing intercept from lm() results in oddly high Rsquared
Message-ID: <44D66B4F.9070500@msu.edu>

Can anyone help me understand why an lm model summary would return an 
r.squared of ~0.18 with an intercept term, and an r.squared of ~0.98 
without the intercept?   The fit is NOT that much better, according to 
plot.lm: residuals are similar between the two models, and a plot of 
observed x predicted is almost identical.

Thanks,

-Joseph

-- 
************************************
Joseph P. LeBouton
Forest Ecology PhD Candidate
Department of Forestry
Michigan State University
East Lansing, Michigan 48824

Office phone: 517-355-7744
email: lebouton at msu.edu


From sell_mirage_ne at hotmail.com  Mon Aug  7 04:24:32 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Sun, 06 Aug 2006 21:24:32 -0500
Subject: [R] classification tables
Message-ID: <BAY110-F35898EEE02BFAB50688A5BC7570@phx.gbl>

Dear R-users

I have two vectors. One vector includes true values and the other vector has 
estimated values. Values are all integers from 1 to 4.

For example,

x <- c(1,2,3,4,2,3,3,1,2,3)
y <- c(2,1,3,4,1,3,3,2,2,3)

I would like to a classfication table x by y. With the table, I would like 
to calculate what percentage is correct classfication.

Which R function do I need to use for creating a 4 * 4 classification table?

Thank you.

Taka,


From epistat at gmail.com  Mon Aug  7 04:54:13 2006
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 7 Aug 2006 10:54:13 +0800
Subject: [R] how to generate this simulation dataset in R
Message-ID: <2fc17e30608061954o2c3e1a0etfff36b506924caf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/f773029d/attachment.pl 

From mikeumo at sbcglobal.net  Mon Aug  7 04:46:58 2006
From: mikeumo at sbcglobal.net (Mike)
Date: Sun, 06 Aug 2006 21:46:58 -0500
Subject: [R] Source installation error: "gfortran and gcc disagree on int
	and double ...
Message-ID: <eb69j3$cmo$1@sea.gmane.org>


...  configure: error: Maybe change CFLAGS or FFLAGS?"

Dear list, 

This problem has been posted before
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/7982.html), but
suggestions did not help. My machine:  AMD Duron 800 MHz, MandrivaLinux
10.2, gcc (and gfortran): 4.0.1, R source: 2.3.1. I had to set CPICFLAGS
and FPICFLAGS to "-fPIC". I tried ./configure with default CFLAGS and
FFLAGS flags and with "-O3 -g" with the same error message. 

How can I pass the error?

Thank you in advance, 

Mike.


From phgrosjean at sciviews.org  Mon Aug  7 08:06:02 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 07 Aug 2006 08:06:02 +0200
Subject: [R] classification tables
In-Reply-To: <BAY110-F35898EEE02BFAB50688A5BC7570@phx.gbl>
References: <BAY110-F35898EEE02BFAB50688A5BC7570@phx.gbl>
Message-ID: <44D6D84A.3040809@sciviews.org>


 > x <- c(1,2,3,4,2,3,3,1,2,3)
 > y <- c(2,1,3,4,1,3,3,2,2,3)
 > table(x, y)
    y
x   1 2 3 4
   1 0 2 0 0
   2 2 1 0 0
   3 0 0 4 0
   4 0 0 0 1
 > ?table

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Taka Matzmoto wrote:
> Dear R-users
> 
> I have two vectors. One vector includes true values and the other vector has 
> estimated values. Values are all integers from 1 to 4.
> 
> For example,
> 
> x <- c(1,2,3,4,2,3,3,1,2,3)
> y <- c(2,1,3,4,1,3,3,2,2,3)
> 
> I would like to a classfication table x by y. With the table, I would like 
> to calculate what percentage is correct classfication.
> 
> Which R function do I need to use for creating a 4 * 4 classification table?
> 
> Thank you.
> 
> Taka,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From petr.pikal at precheza.cz  Mon Aug  7 08:18:47 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 07 Aug 2006 08:18:47 +0200
Subject: [R] paired t-test. Need to rearrange data?
In-Reply-To: <44D5F909.9040507@bio.ntnu.no>
Message-ID: <44D6F767.10680.1D61AA@localhost>

Hi

one possibility is

t(unstack(test.data, y~id))

HTH
Petr

On 6 Aug 2006 at 16:13, Henrik Parn wrote:

Date sent:      	Sun, 06 Aug 2006 16:13:29 +0200
From:           	Henrik Parn <henrik.parn at bio.ntnu.no>
Organization:   	NTNU
To:             	R-help <r-help at stat.math.ethz.ch>
Subject:        	[R] paired t-test. Need to rearrange data?
Send reply to:  	henrik.parn at bio.ntnu.no
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Dear all,
> 
> I have received some data arranged like this:
> 
> # original data
> id <- rep(letters[1:6], each=2)
> time <- as.factor(rep(1:2, 6))
> y <- as.vector(replicate(6, c(rnorm(n=1, mean=1), rnorm(n=1,
> mean=2)))) test.data <- data.frame(id, time, y) test.data
> 
> I would like to perform a paired t-test of the y-values at time=1
> against those at time=2, with samples paired by their id. Is it
> necessary to arrange the data in a format like this:   
> 
> # rearranged data
> id <- letters[1:6]
> y1 <- replicate(6, rnorm(n=1, mean=1)) # y-value at time = 1
> y2 <- replicate(6, rnorm(n=1, mean=2)) #  y-value at time = 2
> test.data2 <- data.frame(id, y1, y2)
> test.data2
> 
> ...in order to perform a paired t-test?
> t.test(y1, y2, paired=T)
> 
> If yes, which is the most convenient way to rearrange the data?
> Or is it possible to apply the paired t-test function to the original
> data set?
> 
> And a side question: In my examples, I suppose can I use set.seed to
> reproduce the 'rnorm-values' created in the 'original data' also in my
> the 'rearranged data'. Can someone give me a hint of how to apply the
> same 'seed' to all the rnorms?
> 
> 
> Thanks a lot in advance!
> 
> 
> Henrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Mon Aug  7 08:23:51 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 07 Aug 2006 08:23:51 +0200
Subject: [R] (... not defined because of singularities) in lm()
In-Reply-To: <ED91D8A0-28DE-44BC-B084-A45D87541A3F@apple.com>
Message-ID: <44D6F897.13987.2205A7@localhost>

Hi

On 4 Aug 2006 at 16:44, Jia Pu wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Jia Pu <jpu at apple.com>
Date sent:      	Fri, 4 Aug 2006 16:44:44 -0700
Subject:        	[R] (... not defined because of singularities) in lm()

> I got the message, " Coefficients: (1 not defined because of  
> singularities)", in the returned result of lm(). What does it mean? 
> And where should I start investigating why it happens?

one of your variables (probably isInterNuclear:zeroSyllToEOP) is a 
linear combination of other explanatory variables or is constant.

HTH
Petr



> 
> thanks.
> 
> 
> /// Complete result of lm()
> Call:
> lm(formula = durationRatio ~ isHStar + isWordFinal + oneSyllToEOW +
>      isInterNuclear + isIntonNuclear + isInterLP + isIntonLF +
>      zeroSyllToNA + oneSyllToNA + zeroSyllToEOP + oneSyllToEOP +
>      isInterNuclear:zeroSyllToEOP)
> 
> Residuals:
>        Min        1Q    Median        3Q       Max
> -3.031538 -0.126943 -0.002909  0.121606  4.322135
> 
> Coefficients: (1 not defined because of singularities)
>                                Estimate Std. Error t value Pr(>|t|)
> (Intercept)                   1.044481   0.010694  97.669  < 2e-16 ***
> isHStar                      -0.055227   0.005748  -9.607  < 2e-16 ***
> isWordFinal                  -0.262071   0.009982 -26.254  < 2e-16 ***
> oneSyllToEOW                 -0.031313   0.009917  -3.158  0.00160 **
> isInterNuclear                0.063470   0.015699   4.043 5.32e-05 ***
> isIntonNuclear                0.054570   0.011711   4.660 3.21e-06 ***
> isInterLP                    -0.262302   0.011868 -22.102  < 2e-16 ***
> isIntonLF                    -0.160018   0.012537 -12.764  < 2e-16 ***
> zeroSyllToNA                 -0.160767   0.011105 -14.477  < 2e-16 ***
> oneSyllToNA                  -0.012633   0.010779  -1.172  0.24123
> zeroSyllToEOP                -0.212474   0.011987 -17.726  < 2e-16 ***
> oneSyllToEOP                 -0.123324   0.010998 -11.214  < 2e-16 ***
> isInterNuclear:zeroSyllToEOP        NA         NA      NA       NA ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 0.2662 on 9650 degrees of freedom
> Multiple R-Squared: 0.3957,	Adjusted R-squared: 0.395
> F-statistic: 574.5 on 11 and 9650 DF,  p-value: < 2.2e-16
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From buser at stat.math.ethz.ch  Mon Aug  7 09:26:58 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 7 Aug 2006 09:26:58 +0200
Subject: [R] removing intercept from lm() results in oddly high Rsquared
In-Reply-To: <44D66B4F.9070500@msu.edu>
References: <44D66B4F.9070500@msu.edu>
Message-ID: <17622.60226.146002.860846@stat.math.ethz.ch>

Dear Joseph

Have a look at the questions and answers in the two links 
below. There the topic has been discussed.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68905.html

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/6943.html

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Joseph LeBouton writes:
 > Can anyone help me understand why an lm model summary would return an 
 > r.squared of ~0.18 with an intercept term, and an r.squared of ~0.98 
 > without the intercept?   The fit is NOT that much better, according to 
 > plot.lm: residuals are similar between the two models, and a plot of 
 > observed x predicted is almost identical.
 > 
 > Thanks,
 > 
 > -Joseph
 > 
 > -- 
 > ************************************
 > Joseph P. LeBouton
 > Forest Ecology PhD Candidate
 > Department of Forestry
 > Michigan State University
 > East Lansing, Michigan 48824
 > 
 > Office phone: 517-355-7744
 > email: lebouton at msu.edu
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Mon Aug  7 09:29:17 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Aug 2006 08:29:17 +0100 (BST)
Subject: [R] Source installation error: "gfortran and gcc disagree on
 int and double ...
In-Reply-To: <eb69j3$cmo$1@sea.gmane.org>
References: <eb69j3$cmo$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.64.0608070821180.20401@gannet.stats.ox.ac.uk>

First, you do not need -fPIC: it is the same as -fpic which R selects on 
your platform.

Second, please look at config.log to find the exact problem: it well be 
that your compilers are not properly installed (as was the case in the 
reference you quote below).

Finally, your compilers are pretty obselete (there have been 4.0.2, 4.0.3, 
4.1.0 and 4.1.1), so you should be updating them.

On Sun, 6 Aug 2006, Mike wrote:

> 
> ...  configure: error: Maybe change CFLAGS or FFLAGS?"
> 
> Dear list, 
> 
> This problem has been posted before
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/7982.html), 

That was over three years ago, for R 1.6.0 on Solaris, so I am not sure 
why you thought it would help.

> but suggestions did not help. My machine:  AMD Duron 800 MHz, 
> MandrivaLinux 10.2, gcc (and gfortran): 4.0.1, R source: 2.3.1. I had to 
> set CPICFLAGS and FPICFLAGS to "-fPIC". I tried ./configure with default 
> CFLAGS and FFLAGS flags and with "-O3 -g" with the same error message.
> 
> How can I pass the error?
> 
> Thank you in advance, 
> 
> Mike.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bioconductor.cn at gmail.com  Mon Aug  7 09:36:00 2006
From: bioconductor.cn at gmail.com (Jiantao Shi)
Date: Mon, 7 Aug 2006 15:36:00 +0800
Subject: [R] Is there a function in R can help me to plot such a figure?
Message-ID: <cedaa40b0608070036i105037d2q3e09e64abab9bdc8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/f266113f/attachment.pl 

From dieter.menne at menne-biomed.de  Mon Aug  7 09:38:10 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 7 Aug 2006 07:38:10 +0000 (UTC)
Subject: [R] removing intercept from lm() results in oddly high Rsquared
References: <44D66B4F.9070500@msu.edu>
Message-ID: <loom.20060807T093427-634@post.gmane.org>

Joseph LeBouton <lebouton <at> msu.edu> writes:

> 
> Can anyone help me understand why an lm model summary would return an 
> r.squared of ~0.18 with an intercept term, and an r.squared of ~0.98 
> without the intercept?   The fit is NOT that much better, according to 
> plot.lm: residuals are similar between the two models, and a plot of 
> observed x predicted is almost identical.

There are reasons why the standard textbooks and Bill Venables

http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf

tell you that removing Intercepts can be dangerous for your health.

Dieter

##
set.seed(10)
x = runif(20,5,10)
y = 2 * x + rnorm(20,0,0.3)

# a fit with good data
summary(lm(y~x))$r.squared
# 0.98

# add one outlier at 0
x = c(x,0)
y = c(y,20)
summary(lm(y~x))$r.squared
# 0.00008

# removing the intercept: perfect correlation again
summary(lm(y~x-1))$r.squared
# 0.91

#... because it is similar to adding MANY data points
# at (0,0)
x = c(x,rep(0,1000))
y = c(y,rep(0,1000))
summary(lm(y~x))$r.squared
# 0.90


From jasonshi510 at hotmail.com  Mon Aug  7 10:38:05 2006
From: jasonshi510 at hotmail.com (Xin)
Date: Mon, 7 Aug 2006 09:38:05 +0100
Subject: [R] Combination (a large numbers)
Message-ID: <BAY117-DAV650F837BC7E27C21EDDE0F0570@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/153370bb/attachment.pl 

From Gregor.Gorjanc at bfro.uni-lj.si  Mon Aug  7 10:37:16 2006
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 7 Aug 2006 10:37:16 +0200
Subject: [R] Constrain coefs. in linear model to sum to 0
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31C41@pollux.bfro.uni-lj.si>

Hello!

I would like to use constrain to sum coeficients of a factor to 0 instead
of classical corner contraint i.e. I would like to fit a model like

lm(y ~ 1 + effectA + effectB)

and say get parameters

intercept
effectA_1
effectA_2
effectB_1
effectB_2
effectB_3

where effectA_1 represents deviation of level A_1 from intercept and 
sum(effectA_1, effectA_2) = 0 and the same for factor B.

Is this possible to do?

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From dieter.menne at menne-biomed.de  Mon Aug  7 10:46:09 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 7 Aug 2006 08:46:09 +0000 (UTC)
Subject: [R] Combination (a large numbers)
References: <BAY117-DAV650F837BC7E27C21EDDE0F0570@phx.gbl>
Message-ID: <loom.20060807T104526-601@post.gmane.org>

Xin <jasonshi510 <at> hotmail.com> writes:

>     I try to list all of combination among (a,b,c,d,e,f,g,h,i,j,k)....

Check function combinations in gtools.

Dieter


From p.dalgaard at biostat.ku.dk  Mon Aug  7 11:16:57 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Aug 2006 11:16:57 +0200
Subject: [R] Constrain coefs. in linear model to sum to 0
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31C41@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31C41@pollux.bfro.uni-lj.si>
Message-ID: <x24pwolxbq.fsf@viggo.kubism.ku.dk>

"Gorjanc Gregor" <Gregor.Gorjanc at bfro.uni-lj.si> writes:

> Hello!
> 
> I would like to use constrain to sum coeficients of a factor to 0 instead
> of classical corner contraint i.e. I would like to fit a model like
> 
> lm(y ~ 1 + effectA + effectB)
> 
> and say get parameters
> 
> intercept
> effectA_1
> effectA_2
> effectB_1
> effectB_2
> effectB_3
> 
> where effectA_1 represents deviation of level A_1 from intercept and 
> sum(effectA_1, effectA_2) = 0 and the same for factor B.
> 
> Is this possible to do?

This is what contr.sum does. There are multiple ways to set this
instead of the default contr.treatment:

(a) options(contrast=....) 

(b) contrasts(effectA) <-  contr.sum 

(c) lm(y~C(effectA,contr.sum)+.....)

Notice though that you only get parameters for a linearly independent
subset of groups (i.e. the first n-1 of them). The last one is defined
as -sum(the other levels).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Ted.Harding at nessie.mcc.ac.uk  Mon Aug  7 11:25:41 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Aug 2006 10:25:41 +0100 (BST)
Subject: [R] Backquote in R syntax
Message-ID: <XFMail.060807102541.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,
Can someone satisfy my curiosity (well, at least about
the following query!)

Reading the draft proofs of an article I've been asked
to look through, I find the typesetter has set what
would normally be entered as

  source("xyz.R")   or   source('xyz.R')

as

  source(`xyz.R')

i.e. it has come out with an opening backquote, then
xyz.R, then a closing forward quote. I suspect the
intervention of "intelligent" software (? la Word's
"clever quotes").

Well, the cure is clear and I'm not asking about that.
But I got curious about what role the backquote might
play in R syntax (if any). As a start I tried typing
that in as it stands:

   source(`xyz.R')
## and then you get the "continuation +" as if it were
##   incomplete, so I tried a closing parenthesis:
+ )
Error: unprotect_ptr: pointer not found

So it wasn't a mere syntax error (which would have caused
an error message saying just that) -- using the backquote
caused R to try to do something.

So now I'm wondering what the effect of "`" is, in R.

Statutory Declaration: I have performed an R Site Search
for "backquote" obtaining 9 hits none of which seems to
address this question.

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Aug-06                                       Time: 10:25:31
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Mon Aug  7 11:36:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Aug 2006 10:36:49 +0100 (BST)
Subject: [R] Backquote in R syntax
In-Reply-To: <XFMail.060807102541.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060807102541.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.64.0608071035120.10346@gannet.stats.ox.ac.uk>

?Quote tells you all about quotes in R (and so does ?backquote in 
R-devel, but many people call it backtick and that is all 2.3.1 has).

On Mon, 7 Aug 2006, Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
> Can someone satisfy my curiosity (well, at least about
> the following query!)
> 
> Reading the draft proofs of an article I've been asked
> to look through, I find the typesetter has set what
> would normally be entered as
> 
>   source("xyz.R")   or   source('xyz.R')
> 
> as
> 
>   source(`xyz.R')
> 
> i.e. it has come out with an opening backquote, then
> xyz.R, then a closing forward quote. I suspect the
> intervention of "intelligent" software (? la Word's
> "clever quotes").
> 
> Well, the cure is clear and I'm not asking about that.
> But I got curious about what role the backquote might
> play in R syntax (if any). As a start I tried typing
> that in as it stands:
> 
>    source(`xyz.R')
> ## and then you get the "continuation +" as if it were
> ##   incomplete, so I tried a closing parenthesis:
> + )
> Error: unprotect_ptr: pointer not found
> 
> So it wasn't a mere syntax error (which would have caused
> an error message saying just that) -- using the backquote
> caused R to try to do something.
> 
> So now I'm wondering what the effect of "`" is, in R.
> 
> Statutory Declaration: I have performed an R Site Search
> for "backquote" obtaining 9 hits none of which seems to
> address this question.
> 
> Best wishes to all,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 07-Aug-06                                       Time: 10:25:31
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From p.dalgaard at biostat.ku.dk  Mon Aug  7 11:40:30 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Aug 2006 11:40:30 +0200
Subject: [R] Backquote in R syntax
In-Reply-To: <XFMail.060807102541.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060807102541.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2zmegkho1.fsf@viggo.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> Hi Folks,
> Can someone satisfy my curiosity (well, at least about
> the following query!)
> 
> Reading the draft proofs of an article I've been asked
> to look through, I find the typesetter has set what
> would normally be entered as
> 
>   source("xyz.R")   or   source('xyz.R')
> 
> as
> 
>   source(`xyz.R')
> 
> i.e. it has come out with an opening backquote, then
> xyz.R, then a closing forward quote. I suspect the
> intervention of "intelligent" software (? la Word's
> "clever quotes").
> 
> Well, the cure is clear and I'm not asking about that.
> But I got curious about what role the backquote might
> play in R syntax (if any). As a start I tried typing
> that in as it stands:
> 
>    source(`xyz.R')
> ## and then you get the "continuation +" as if it were
> ##   incomplete, so I tried a closing parenthesis:
> + )
> Error: unprotect_ptr: pointer not found
> 
> So it wasn't a mere syntax error (which would have caused
> an error message saying just that) -- using the backquote
> caused R to try to do something.
> 
> So now I'm wondering what the effect of "`" is, in R.
> 
> Statutory Declaration: I have performed an R Site Search
> for "backquote" obtaining 9 hits none of which seems to
> address this question.

?Quotes should put you straight. (And help.search("`") gets you there,
as does help.search("backtick"), although not help.search("back
quote"))

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dieter.menne at menne-biomed.de  Mon Aug  7 11:53:11 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 7 Aug 2006 09:53:11 +0000 (UTC)
Subject: [R] Help with short time series
References: <20060804214445.M9473@nemo.unipr.it>
Message-ID: <loom.20060807T114827-72@post.gmane.org>

Simone Vincenzi <simone.vincenzi <at> nemo.unipr.it> writes:

> 
> Dear R-list, 
> I have a statistical problem with the comparison of two short time-series of 
> density data in an ecological framework. I have to compare two short time 
> series (5 years, one value for each year) of species density data (it is the 
> density of fish in two different streams) to test if the two means of the 
> five densities are significantly different, so basically if the two mean 
> stream-specific fish densities are significantly different. 
> I don't think I can use a straight t-test due to the problem of 
> autocorrelation and I don't think I can use a repeated measure ANOVA as I 
> don't have any replicates. 
> Any help would be greatly appreciated. 

try something like 

library(nlme)
summary(lme(dens~stream+year,data=mystreamdata,random=~year|stream))

This should also give you an estimate if the slopes are different if you test
against the simplified model

summary(lme(dens~stream+year,data=mystreamdata,random=~1|stream))

Since you did not provide a short example data set, this is only approximatively
right.

Dieter


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Mon Aug  7 12:35:20 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Mon,  7 Aug 2006 11:35:20 +0100
Subject: [R] Finding points with equal probability between normal
	distributions
Message-ID: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>

Dear mailing list, 

For two normal distributions, e.g:

r1 =rnorm(20,5.2,2.1)
r2 =rnorm(20,4.2,1.1)
plot(density(r2), col="blue")
lines(density(r1), col="red")

Is there a way in R to compute/estimate the point(s) x where the density of the
two distributions cross (ie where x has equal probability of belonging to
either of the two distributions)?

Many Thanks

Eleni Rapsomaniki
PhD student
Birkbeck College, UK


From vincent at 7d4.com  Mon Aug  7 12:40:57 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 07 Aug 2006 12:40:57 +0200
Subject: [R] Is there a function in R can help me to plot such a figure?
In-Reply-To: <cedaa40b0608070036i105037d2q3e09e64abab9bdc8@mail.gmail.com>
References: <cedaa40b0608070036i105037d2q3e09e64abab9bdc8@mail.gmail.com>
Message-ID: <44D718B9.7060204@7d4.com>

Jiantao Shi a ?crit :

> i want to plot figure like this,
> http://www.cis.hut.fi/projects/somtoolbox/download/pics2/shotvs2_colorcode.png
> So is there a function or package in R can help me to do this.
> Any suggestion will be appreciated.
> Thanks in advance

have a look here
http://addictedtor.free.fr/graphiques/allgraph.php
hih


From ligges at statistik.uni-dortmund.de  Mon Aug  7 13:01:52 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Aug 2006 13:01:52 +0200
Subject: [R] Finding points with equal probability between
	normal	distributions
In-Reply-To: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>
References: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>
Message-ID: <44D71DA0.8060001@statistik.uni-dortmund.de>



Eleni Rapsomaniki wrote:
> Dear mailing list, 
> 
> For two normal distributions, e.g:
> 
> r1 =rnorm(20,5.2,2.1)
> r2 =rnorm(20,4.2,1.1)
> plot(density(r2), col="blue")
> lines(density(r1), col="red")


dr1 <- density(r1, from=-3, to=14)
dr2 <- density(r2, from=-3, to=14)
w <- which(diff(sign(dr2$y -  dr1$y)) != 0)
dr1$x[w]

plot(dr1, ylim = c(0, 0.33))
lines(dr2, col = "red")
abline(v = dr1$x[w], col = "blue")

Uwe Ligges



> Is there a way in R to compute/estimate the point(s) x where the density of the
> two distributions cross (ie where x has equal probability of belonging to
> either of the two distributions)?
> 
> Many Thanks
> 
> Eleni Rapsomaniki
> PhD student
> Birkbeck College, UK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Mon Aug  7 13:04:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 07:04:21 -0400
Subject: [R] classification tables
In-Reply-To: <44D6D84A.3040809@sciviews.org>
References: <BAY110-F35898EEE02BFAB50688A5BC7570@phx.gbl>
	<44D6D84A.3040809@sciviews.org>
Message-ID: <971536df0608070404l356ebe9s41e1e9f038d2fc2c@mail.gmail.com>

Also check out CrossTable in the gmodels package.

Regarding your other question, assuming we have
tab<-table(x,y) as in Philippe's post, the fraction of
pairs in x and y that match can be calculated via
any of these:

  sum(x==y) / length(x)

  sum(diag(tab)) / sum(tab)

  library(e1071)
  classAgreement(tab) # tab from above

  sum(diag(prop.table(tab)))


On 8/7/06, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
>
>  > x <- c(1,2,3,4,2,3,3,1,2,3)
>  > y <- c(2,1,3,4,1,3,3,2,2,3)
>  > table(x, y)
>    y
> x   1 2 3 4
>   1 0 2 0 0
>   2 2 1 0 0
>   3 0 0 4 0
>   4 0 0 0 1
>  > ?table
>
> Best,
>
> Philippe Grosjean
>
> ..............................................<?}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................
>
> Taka Matzmoto wrote:
> > Dear R-users
> >
> > I have two vectors. One vector includes true values and the other vector has
> > estimated values. Values are all integers from 1 to 4.
> >
> > For example,
> >
> > x <- c(1,2,3,4,2,3,3,1,2,3)
> > y <- c(2,1,3,4,1,3,3,2,2,3)
> >
> > I would like to a classfication table x by y. With the table, I would like
> > to calculate what percentage is correct classfication.
> >
> > Which R function do I need to use for creating a 4 * 4 classification table?
> >
> > Thank you.
> >
> > Taka,
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dimitris.rizopoulos at med.kuleuven.be  Mon Aug  7 13:18:50 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 7 Aug 2006 13:18:50 +0200
Subject: [R] Finding points with equal probability between
	normaldistributions
References: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>
Message-ID: <001e01c6ba13$426443d0$0540210a@www.domain>

if you want to base it on Normality, then you can use:

prob <- function(x){
    dnorm((x - 5.2) / 2.1) / 2.1 - dnorm((x - 4.2) / 1.1) / 1.1
}

uniroot(prob, c(-3, 3))

otherwise if you want to estimate it you can use Uwe's solution.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Eleni Rapsomaniki" <e.rapsomaniki at mail.cryst.bbk.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, August 07, 2006 12:35 PM
Subject: [R] Finding points with equal probability between 
normaldistributions


> Dear mailing list,
>
> For two normal distributions, e.g:
>
> r1 =rnorm(20,5.2,2.1)
> r2 =rnorm(20,4.2,1.1)
> plot(density(r2), col="blue")
> lines(density(r1), col="red")
>
> Is there a way in R to compute/estimate the point(s) x where the 
> density of the
> two distributions cross (ie where x has equal probability of 
> belonging to
> either of the two distributions)?
>
> Many Thanks
>
> Eleni Rapsomaniki
> PhD student
> Birkbeck College, UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From chrishold at psyctc.org  Mon Aug  7 14:00:07 2006
From: chrishold at psyctc.org (Chris Evans)
Date: Mon, 07 Aug 2006 13:00:07 +0100
Subject: [R] Finding points with equal probability between
	normal	distributions
In-Reply-To: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>
References: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>
Message-ID: <44D72B47.9000206@psyctc.org>

Eleni Rapsomaniki sent the following  at 07/08/2006 11:35:
> Dear mailing list, 
> 
> For two normal distributions, e.g:
> 
> r1 =rnorm(20,5.2,2.1)
> r2 =rnorm(20,4.2,1.1)
> plot(density(r2), col="blue")
> lines(density(r1), col="red")
> 
> Is there a way in R to compute/estimate the point(s) x where the density of the
> two distributions cross (ie where x has equal probability of belonging to
> either of the two distributions)?

I worry about showing my statistical incompetence or incomprehension but
isn't what you need Jacobson et al.'s criterion C for clinical change?
I.e. the point at which the misclassification rates in two Normal
distributions, one with a higher mean than the other, match.

It's at (sd1*mean2 + sd2*mean1)/(sd1 + sd2)

So for Eleni's example I think that comes out at 4.544 and if I use:
> r1b <- rnorm(200,5.2,2.1)
> r2b <- rnorm(200,4.2,1.1)
> plot(density(r2b), col="blue")
> plot(density(r1b), col="red")
> plot(density(r2b), col="blue")
> lines(density(r1b), col="red")
> cscc <- 4.544
> abline(v=cscc)

It happened to work out beautifully:

> sum(r1b > cscc)
[1] 126
> sum(r2b < cscc)
[1] 126

of course, set a different seed (I broke the posting rules and didn't
set one, yes,  I know) you won't get such a nice result every time and
with n=20 in each group you'll get much more wobble.

Or am I missing something.  The original paper, which got reliable
change wrong, was:

Jacobson, N. S., Follette, W. C. & Revenstorf, D. (1984) Psychotherapy
outcome research: methods for reporting variability and evaluating
clinical significance. Behavior Therapy, 15, 336-352.

There's a summary most people cite at:
Jacobson, N. S. & Truax, P. (1991) Clinical significance: a statistical
approach to defining meaningful change in psychotherapy research.
Journal of Consulting and Clinical Psychology, 59, 12-19.

and shameless self-promotion here, I tried to summarise it:
Evans, C., Margison, F. & Barkham, M. (1998) The contribution of
reliable and clinically significant change methods to evidence-based
mental health. Evidence Based Mental Health, 1, 70-72.

I hadn't twigged that what the criterion gives is balanced
missclassification when I wrote that.  I've played with some simulations
and it's not as vulnerable to non-Gaussian distributions as I'd expected
but someone can probably point to published work, simulation or
analytic, on that.

Cheers all,

Chris


From jasonshi510 at hotmail.com  Mon Aug  7 14:09:09 2006
From: jasonshi510 at hotmail.com (Xin)
Date: Mon, 7 Aug 2006 13:09:09 +0100
Subject: [R] How to export data to Excel Spreadsheet?
Message-ID: <BAY117-DAV1D6FB179C8AE099AEFA00F0570@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/5f54caf6/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Mon Aug  7 14:15:57 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 7 Aug 2006 14:15:57 +0200
Subject: [R] Finding points with equal probability
	betweennormaldistributions
References: <1154946920.44d7176871994@webmail.cryst.bbk.ac.uk>
	<001e01c6ba13$426443d0$0540210a@www.domain>
Message-ID: <006301c6ba1b$3cca92f0$0540210a@www.domain>


----- Original Message ----- 
From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
To: "Eleni Rapsomaniki" <e.rapsomaniki at mail.cryst.bbk.ac.uk>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, August 07, 2006 1:18 PM
Subject: Re: [R] Finding points with equal probability 
betweennormaldistributions


> if you want to base it on Normality, then you can use:
>
> prob <- function(x){
>    dnorm((x - 5.2) / 2.1) / 2.1 - dnorm((x - 4.2) / 1.1) / 1.1
> }
>
> uniroot(prob, c(-3, 3))
>

sorry but the above gives you only the one solution, probably you need 
both:

dn <- function(x){
    dnorm(x, 5.2, 2.1) - dnorm(x, 4.2, 1.1)
}

rout1 <- uniroot(dn, c(0, 5))$root
rout2 <- uniroot(dn, c(5, 10))$root

x <- seq(-3, 14, 0.1)
plot(x, dnorm(x, 4.2,  1.1), type = "l", col = "red")
lines(x, dnorm(x, 5.2, 2.1), col = "blue")
abline(v = c(rout1, rout2))



Best,
Dimitris



> otherwise if you want to estimate it you can use Uwe's solution.
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>     http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- 
> From: "Eleni Rapsomaniki" <e.rapsomaniki at mail.cryst.bbk.ac.uk>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, August 07, 2006 12:35 PM
> Subject: [R] Finding points with equal probability between
> normaldistributions
>
>
>> Dear mailing list,
>>
>> For two normal distributions, e.g:
>>
>> r1 =rnorm(20,5.2,2.1)
>> r2 =rnorm(20,4.2,1.1)
>> plot(density(r2), col="blue")
>> lines(density(r1), col="red")
>>
>> Is there a way in R to compute/estimate the point(s) x where the
>> density of the
>> two distributions cross (ie where x has equal probability of
>> belonging to
>> either of the two distributions)?
>>
>> Many Thanks
>>
>> Eleni Rapsomaniki
>> PhD student
>> Birkbeck College, UK
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From phhs80 at gmail.com  Mon Aug  7 14:23:26 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 7 Aug 2006 13:23:26 +0100
Subject: [R] How to export data to Excel Spreadsheet?
In-Reply-To: <BAY117-DAV1D6FB179C8AE099AEFA00F0570@phx.gbl>
References: <BAY117-DAV1D6FB179C8AE099AEFA00F0570@phx.gbl>
Message-ID: <6ade6f6c0608070523u2e3c69aeq6168120051354600@mail.gmail.com>

On 8/7/06, Xin <jasonshi510 at hotmail.com> wrote:
>    I try to export my output's data to Excel spreadsheet. My outputs are:
>
>  >comb3
>        [,1] [,2] [,3]
>   [1,] "a"  "b"  "c"
>   [2,] "a"  "b"  "d"
>   [3,] "a"  "b"  "e"
>   [4,] "a"  "b"  "f"
>   [5,] "a"  "b"  "g"

See

? write.table
? write.csv

Paul


From Ralf.Finne at syh.fi  Mon Aug  7 12:43:18 2006
From: Ralf.Finne at syh.fi (Ralf Finne)
Date: Mon, 07 Aug 2006 13:43:18 +0300
Subject: [R] Plotting logarithmic and semiloarithmic charts.
Message-ID: <44D74376020000EE0000135F@valhall.syh.fi>

Dear all,

Can anyone help me to find functions 
like LOGLOG and SEMILOG in Matlab.

I am sure that they are out there.
Thanks in advance

Ralf Finne


From Markus.Preisetanz at clientvela.com  Mon Aug  7 14:31:48 2006
From: Markus.Preisetanz at clientvela.com (Markus Preisetanz)
Date: Mon, 7 Aug 2006 14:31:48 +0200
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
Message-ID: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/7f387274/attachment.pl 

From bioconductor.cn at gmail.com  Mon Aug  7 14:40:53 2006
From: bioconductor.cn at gmail.com (Jiantao Shi)
Date: Mon, 7 Aug 2006 20:40:53 +0800
Subject: [R] Is there a function in R can help me to plot such a figure?
In-Reply-To: <44D7F060.3090503@bitwrit.com.au>
References: <cedaa40b0608070036i105037d2q3e09e64abab9bdc8@mail.gmail.com>
	<44D7F060.3090503@bitwrit.com.au>
Message-ID: <cedaa40b0608070540q4e28cd4emb378ab1818fd6d55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/cdcd4b1e/attachment.pl 

From ligges at statistik.uni-dortmund.de  Mon Aug  7 14:41:21 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Aug 2006 14:41:21 +0200
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
In-Reply-To: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
Message-ID: <44D734F1.2060303@statistik.uni-dortmund.de>



Markus Preisetanz wrote:
> ... I have exactly the same issue with R 2.3.1 . The Question is: Why
> is R unable to take more CPU space to make the calculation process go
> faster?


You have either 2 CPUs, a dual core CPU or a P4/Xeon processor with 
hyperthreading enabled. In most regular cases, each R process runs with 
only one thread, i.e. on only one (virtual) processor.

Uwe Ligges


> In my case no hardisk nor any network device is involved (data in
> RAM, 600 of 1024 MB filled) - and the CPU usage of the rgui-process
> does not exceed 50%.
> 
> Has anybody an idea? Is there a setting a can change?
> 
> Sincerely, Markus ____________________________ Markus Preisetanz 
> Consultant
> 
> Client Vela GmbH Albert-Ro?haupter-Str. 32 81369 M?nchen fon: +49
> (89) 74217-113 main: +49 (89) 74217-150 fax: +49 (89) 74217-250 
> markus.preisetanz at clientvela.com
> <mailto:markus.preisetanz at clientvela.com> http://www.clientvela.com
> <http://www.clientvela.com/>
> 
> Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte
> Informationen. Wenn Sie nicht der richtige Adressat sind oder diese
> E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den
> Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie
> die unbefugte Weitergabe dieser E-Mail ist nicht gestattet.
> 
> This e-mail may contain confidential and/or privileged information.
> If you are not the intended recipient (or have received the e-mail in
> error) please notify the sender immediately and destroy this e-mail.
> Any unauthorized copying, disclosure or distribution of the material
> in this e-mail is strictly forbidden
> 
> xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
>  From: Doran, Harold <HDoran_at_air.org
> <mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows>
> > Date: Wed 20 Jul 2005 - 04:59:34 EST
> 
> 
> 
> Dear Michael:
> 
> Why is it a problem that R is not using more CPU space than it seems
> to need?
> 
> -----Original Message----- From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene,
> Michael Sent: Tuesday, July 19, 2005 2:29 PM To:
> 'R-help at lists.R-project.org' Subject: [R] CPU Usage with R 2.1.0 in
> Windows
> 
> Hi,
> 
> I'm using a fairly simple HP Compaq desktop PC running Windows 2K.
> When running a large process in R, the process "RGUI.exe" will never
> exceed 50% of the CPU usage.
> 
> The program used to be able to use more of the computer, but does not
> now. I don't believe this is a multiple processor machine.
> 
> Can anyone give any advice on how to solve the problem?
> 
> Thanks,
> 
> Michael Greene
> 
> Product Management Plymouth Rock Assurance Corp 617-951-1682
> 
> 
> [[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From mike_saunders at umenfa.maine.edu  Mon Aug  7 14:40:16 2006
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Mon, 7 Aug 2006 08:40:16 -0400
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
Message-ID: <001201c6ba1e$a25e1da0$b5a76f82@CFRU0204>

Markus:

My guess is that you have either a dual-core or a hyperthreaded processor. 
Hyperthreading caps the CPU usage for any process at 50%.  It can be shut 
off by going through the BIOS setup for your machine.  For the dual-core, I 
am not sure if you can configure the BIOS to use both processors for one 
particular task (I don't have a dual core).

Mike

Mike R. Saunders
Forest Biometrician
Cooperative Forest Research Unit
University of Maine
5755 Nutting Hall
Orono, ME  04469-5755

207-581-2763 (O)
207-581-2833 (F)

----- Original Message ----- 
From: "Markus Preisetanz" <Markus.Preisetanz at clientvela.com>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, August 07, 2006 8:31 AM
Subject: Re: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)


... I have exactly the same issue with R 2.3.1 . The Question is: Why is R 
unable to take more CPU space to make the calculation process go faster?

In my case no hardisk nor any network device is involved (data in RAM, 600 
of 1024 MB filled) - and the CPU usage of the rgui-process does not exceed 
50%.

Has anybody an idea? Is there a setting a can change?

Sincerely, Markus
____________________________
Markus Preisetanz
Consultant

Client Vela GmbH
Albert-Ro?haupter-Str. 32
81369 M?nchen
fon: +49 (89) 74217-113
main: +49 (89) 74217-150
fax: +49 (89) 74217-250
markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com>
http://www.clientvela.com <http://www.clientvela.com/>

Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte 
Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail 
irrt?mlich erhalten haben, informieren Sie bitte sofort den Absender und 
vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die unbefugte 
Weitergabe dieser E-Mail ist nicht gestattet.

This e-mail may contain confidential and/or privileged information. If you 
are not the intended recipient (or have received the e-mail in error) please 
notify the sender immediately and destroy this e-mail. Any unauthorized 
copying, disclosure or distribution of the material in this e-mail is 
strictly forbidden

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
From: Doran, Harold <HDoran_at_air.org 
<mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows> 
 >
Date: Wed 20 Jul 2005 - 04:59:34 EST



Dear Michael:

Why is it a problem that R is not using more CPU space than it seems to 
need?

-----Original Message----- 
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael Sent: 
Tuesday, July 19, 2005 2:29 PM
To: 'R-help at lists.R-project.org'
Subject: [R] CPU Usage with R 2.1.0 in Windows

Hi,

I'm using a fairly simple HP Compaq desktop PC running Windows 2K. When 
running a large process in R, the process "RGUI.exe" will never exceed 50% 
of the CPU usage.

The program used to be able to use more of the computer, but does not now.
I don't believe this is a multiple processor machine.

Can anyone give any advice on how to solve the problem?

Thanks,

Michael Greene

Product Management
Plymouth Rock Assurance Corp
617-951-1682


[[alternative HTML version deleted]]




--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Mon Aug  7 14:42:50 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 07 Aug 2006 08:42:50 -0400
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
In-Reply-To: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
Message-ID: <44D7354A.8070500@stats.uwo.ca>

Markus Preisetanz wrote:
> ... I have exactly the same issue with R 2.3.1 . The Question is: Why is R unable to take more CPU space to make the calculation process go faster?
>  
> In my case no hardisk nor any network device is involved (data in RAM, 600 of 1024 MB filled) - and the CPU usage of the rgui-process does not exceed 50%.
>  
> Has anybody an idea? Is there a setting a can change?
>   
You are probably on a dual core or hyperthreaded machine.  R does almost 
all of its work in one thread, so it will max out the usage on one cpu, 
but not more than one.

If you are on a hyperthreaded machine, you can disable hyperthreading 
and the report should show 100% usage, but that doesn't mean things will 
be going twice as fast.  (I have heard that hyperthreading does slow 
down some computations, so turning it off might be a good idea, but I 
have never done the measurements myself.)

Duncan Murdoch
>  
> Sincerely, Markus
> ____________________________
> Markus Preisetanz
> Consultant
>
> Client Vela GmbH
> Albert-Ro?haupter-Str. 32
> 81369 M?nchen
> fon: +49 (89) 74217-113
> main: +49 (89) 74217-150
> fax: +49 (89) 74217-250
> markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com> 
> http://www.clientvela.com <http://www.clientvela.com/> 
>  
> Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die unbefugte Weitergabe dieser E-Mail ist nicht gestattet.
>  
> This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received the e-mail in error) please notify the sender immediately and destroy this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden
>  
> xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
> From: Doran, Harold <HDoran_at_air.org <mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows> > 
> Date: Wed 20 Jul 2005 - 04:59:34 EST
>
>
>
> Dear Michael: 
>
> Why is it a problem that R is not using more CPU space than it seems to need? 
>
> -----Original Message----- 
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael Sent: Tuesday, July 19, 2005 2:29 PM 
> To: 'R-help at lists.R-project.org' 
> Subject: [R] CPU Usage with R 2.1.0 in Windows 
>
> Hi, 
>
> I'm using a fairly simple HP Compaq desktop PC running Windows 2K. When running a large process in R, the process "RGUI.exe" will never exceed 50% of the CPU usage. 
>
> The program used to be able to use more of the computer, but does not now. 
> I don't believe this is a multiple processor machine. 
>
> Can anyone give any advice on how to solve the problem? 
>
> Thanks, 
>
> Michael Greene 
>
> Product Management 
> Plymouth Rock Assurance Corp 
> 617-951-1682 
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mike_saunders at umenfa.maine.edu  Mon Aug  7 14:50:14 2006
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Mon, 7 Aug 2006 08:50:14 -0400
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
	<44D7354A.8070500@stats.uwo.ca>
Message-ID: <002001c6ba20$071786e0$b5a76f82@CFRU0204>

I have done the measurements, at least with some of the code I have built 
for spatial pattern analysis.  If you are on a networked machine, with lots 
of network processes running in the background, turning off hyperthreading 
increases speed by 30-50% (at least on my network).  I have tried it on a 
non-networked machine, and speed increased by 60% or better.

Mike

Mike R. Saunders
Forest Biometrician
Cooperative Forest Research Unit
University of Maine
5755 Nutting Hall
Orono, ME  04469-5755

207-581-2763 (O)
207-581-2833 (F)

----- Original Message ----- 
From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
To: "Markus Preisetanz" <Markus.Preisetanz at clientvela.com>
Cc: <R-help at stat.math.ethz.ch>
Sent: Monday, August 07, 2006 8:42 AM
Subject: Re: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)


Markus Preisetanz wrote:
> ... I have exactly the same issue with R 2.3.1 . The Question is: Why is R 
> unable to take more CPU space to make the calculation process go faster?
>
> In my case no hardisk nor any network device is involved (data in RAM, 600 
> of 1024 MB filled) - and the CPU usage of the rgui-process does not exceed 
> 50%.
>
> Has anybody an idea? Is there a setting a can change?
>
You are probably on a dual core or hyperthreaded machine.  R does almost
all of its work in one thread, so it will max out the usage on one cpu,
but not more than one.

If you are on a hyperthreaded machine, you can disable hyperthreading
and the report should show 100% usage, but that doesn't mean things will
be going twice as fast.  (I have heard that hyperthreading does slow
down some computations, so turning it off might be a good idea, but I
have never done the measurements myself.)

Duncan Murdoch
>
> Sincerely, Markus
> ____________________________
> Markus Preisetanz
> Consultant
>
> Client Vela GmbH
> Albert-Ro?haupter-Str. 32
> 81369 M?nchen
> fon: +49 (89) 74217-113
> main: +49 (89) 74217-150
> fax: +49 (89) 74217-250
> markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com>
> http://www.clientvela.com <http://www.clientvela.com/>
>
> Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte 
> Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail 
> irrt?mlich erhalten haben, informieren Sie bitte sofort den Absender und 
> vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die unbefugte 
> Weitergabe dieser E-Mail ist nicht gestattet.
>
> This e-mail may contain confidential and/or privileged information. If you 
> are not the intended recipient (or have received the e-mail in error) 
> please notify the sender immediately and destroy this e-mail. Any 
> unauthorized copying, disclosure or distribution of the material in this 
> e-mail is strictly forbidden
>
> xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
> From: Doran, Harold <HDoran_at_air.org 
> <mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows> 
>  >
> Date: Wed 20 Jul 2005 - 04:59:34 EST
>
>
>
> Dear Michael:
>
> Why is it a problem that R is not using more CPU space than it seems to 
> need?
>
> -----Original Message----- 
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael 
> Sent: Tuesday, July 19, 2005 2:29 PM
> To: 'R-help at lists.R-project.org'
> Subject: [R] CPU Usage with R 2.1.0 in Windows
>
> Hi,
>
> I'm using a fairly simple HP Compaq desktop PC running Windows 2K. When 
> running a large process in R, the process "RGUI.exe" will never exceed 50% 
> of the CPU usage.
>
> The program used to be able to use more of the computer, but does not now.
> I don't believe this is a multiple processor machine.
>
> Can anyone give any advice on how to solve the problem?
>
> Thanks,
>
> Michael Greene
>
> Product Management
> Plymouth Rock Assurance Corp
> 617-951-1682
>
>
> [[alternative HTML version deleted]]
>
>
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Mon Aug  7 14:57:48 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Aug 2006 14:57:48 +0200
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
In-Reply-To: <44D7354A.8070500@stats.uwo.ca>
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
	<44D7354A.8070500@stats.uwo.ca>
Message-ID: <44D738CC.9020301@statistik.uni-dortmund.de>



Duncan Murdoch wrote:
> Markus Preisetanz wrote:
>> ... I have exactly the same issue with R 2.3.1 . The Question is: Why is R unable to take more CPU space to make the calculation process go faster?
>>  
>> In my case no hardisk nor any network device is involved (data in RAM, 600 of 1024 MB filled) - and the CPU usage of the rgui-process does not exceed 50%.
>>  
>> Has anybody an idea? Is there a setting a can change?
>>   
> You are probably on a dual core or hyperthreaded machine.  R does almost 
> all of its work in one thread, so it will max out the usage on one cpu, 
> but not more than one.
> 
> If you are on a hyperthreaded machine, you can disable hyperthreading 
> and the report should show 100% usage, but that doesn't mean things will 
> be going twice as fast.  (I have heard that hyperthreading does slow 
> down some computations, so turning it off might be a good idea, but I 
> have never done the measurements myself.)

Highly depends on the task. I have seen two WinBUGS processes on a 
hyperthreading machine making full use of hyperthreading and surpisingly 
completing *both* in the same time as one on the same machine with 
hyperthreading disabled. I don't think Intel folks will find many of 
such impressive examples ...

Uwe Ligges



> 
> Duncan Murdoch
>>  
>> Sincerely, Markus
>> ____________________________
>> Markus Preisetanz
>> Consultant
>>
>> Client Vela GmbH
>> Albert-Ro?haupter-Str. 32
>> 81369 M?nchen
>> fon: +49 (89) 74217-113
>> main: +49 (89) 74217-150
>> fax: +49 (89) 74217-250
>> markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com> 
>> http://www.clientvela.com <http://www.clientvela.com/> 
>>  
>> Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die unbefugte Weitergabe dieser E-Mail ist nicht gestattet.
>>  
>> This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received the e-mail in error) please notify the sender immediately and destroy this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden
>>  
>> xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
>> From: Doran, Harold <HDoran_at_air.org <mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows> > 
>> Date: Wed 20 Jul 2005 - 04:59:34 EST
>>
>>
>>
>> Dear Michael: 
>>
>> Why is it a problem that R is not using more CPU space than it seems to need? 
>>
>> -----Original Message----- 
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael Sent: Tuesday, July 19, 2005 2:29 PM 
>> To: 'R-help at lists.R-project.org' 
>> Subject: [R] CPU Usage with R 2.1.0 in Windows 
>>
>> Hi, 
>>
>> I'm using a fairly simple HP Compaq desktop PC running Windows 2K. When running a large process in R, the process "RGUI.exe" will never exceed 50% of the CPU usage. 
>>
>> The program used to be able to use more of the computer, but does not now. 
>> I don't believe this is a multiple processor machine. 
>>
>> Can anyone give any advice on how to solve the problem? 
>>
>> Thanks, 
>>
>> Michael Greene 
>>
>> Product Management 
>> Plymouth Rock Assurance Corp 
>> 617-951-1682 
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>>   
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Mon Aug  7 14:58:42 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Aug 2006 14:58:42 +0200
Subject: [R] Plotting logarithmic and semiloarithmic charts.
In-Reply-To: <44D74376020000EE0000135F@valhall.syh.fi>
References: <44D74376020000EE0000135F@valhall.syh.fi>
Message-ID: <x2mzagk8hp.fsf@viggo.kubism.ku.dk>

"Ralf Finne" <Ralf.Finne at syh.fi> writes:

> Dear all,
> 
> Can anyone help me to find functions 
> like LOGLOG and SEMILOG in Matlab.
> 
> I am sure that they are out there.
> Thanks in advance

plot(....., log="y")
plot(....., log="x")
plot(....., log="xy")

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From singularitaet at gmx.net  Mon Aug  7 15:37:35 2006
From: singularitaet at gmx.net (Singu)
Date: Mon, 07 Aug 2006 15:37:35 +0200
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
In-Reply-To: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
Message-ID: <44D7421F.5090606@gmx.net>

You could compile R using specific libraries if threading/dual core is
the issue

Look eg here:
http://tolstoy.newcastle.edu.au/R/devel/05/12/3355.html
and in R Administration Guide Section A.(2.2)
.
And probably you are better of with Linux if you rely on speed:
http://www.r-project.org/useR-2006/Slides/IacusEtAl.pdf

Stefan

PS For Windows the precompiled Atlas  Rblas.dll seems to be
single-threaded. I use it on my P4 3Ghz HT Processor and CPU usage is at
50% as well.


Markus Preisetanz schrieb:
> ... I have exactly the same issue with R 2.3.1 . The Question is: Why is R unable to take more CPU space to make the calculation process go faster?
>  
> In my case no hardisk nor any network device is involved (data in RAM, 600 of 1024 MB filled) - and the CPU usage of the rgui-process does not exceed 50%.
>  
> Has anybody an idea? Is there a setting a can change?
>  
> Sincerely, Markus
> ____________________________
> Markus Preisetanz
> Consultant
>
> Client Vela GmbH
> Albert-Ro?haupter-Str. 32
> 81369 M?nchen
> fon: +49 (89) 74217-113
> main: +49 (89) 74217-150
> fax: +49 (89) 74217-250
> markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com> 
> http://www.clientvela.com <http://www.clientvela.com/> 
>  
> Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die unbefugte Weitergabe dieser E-Mail ist nicht gestattet.
>  
> This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received the e-mail in error) please notify the sender immediately and destroy this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden
>  
> xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
> From: Doran, Harold <HDoran_at_air.org <mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows> > 
> Date: Wed 20 Jul 2005 - 04:59:34 EST
>
>
>
> Dear Michael: 
>
> Why is it a problem that R is not using more CPU space than it seems to need? 
>
> -----Original Message----- 
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael Sent: Tuesday, July 19, 2005 2:29 PM 
> To: 'R-help at lists.R-project.org' 
> Subject: [R] CPU Usage with R 2.1.0 in Windows 
>
> Hi, 
>
> I'm using a fairly simple HP Compaq desktop PC running Windows 2K. When running a large process in R, the process "RGUI.exe" will never exceed 50% of the CPU usage. 
>
> The program used to be able to use more of the computer, but does not now. 
> I don't believe this is a multiple processor machine. 
>
> Can anyone give any advice on how to solve the problem? 
>
> Thanks, 
>
> Michael Greene 
>
> Product Management 
> Plymouth Rock Assurance Corp 
> 617-951-1682 
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Mon Aug  7 15:46:35 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 7 Aug 2006 14:46:35 +0100
Subject: [R] Plotting logarithmic and semiloarithmic charts.
In-Reply-To: <x2mzagk8hp.fsf@viggo.kubism.ku.dk>
References: <44D74376020000EE0000135F@valhall.syh.fi>
	<x2mzagk8hp.fsf@viggo.kubism.ku.dk>
Message-ID: <f8e6ff050608070646re75509fh4c66c79a51b0b6d@mail.gmail.com>

> > Can anyone help me to find functions
> > like LOGLOG and SEMILOG in Matlab.
> >
> > I am sure that they are out there.
> > Thanks in advance
>
> plot(....., log="y")
> plot(....., log="x")
> plot(....., log="xy")

or

install.packages("ggplot")
library(ggplot)
qplot(....., log="y")
qplot(....., log="x")
qplot(....., log="xy")

which work with all types of graphics, not just scatterplots.

You can transform the axes using any monotone function, see
?pscontinuous for examples.

Hadley


From gynmeerut at indiatimes.com  Mon Aug  7 16:19:34 2006
From: gynmeerut at indiatimes.com (gynmeerut)
Date: Mon, 07 Aug 2006 19:49:34 +0530
Subject: [R] Variance-Covariance matrix from glm()
Message-ID: <200608071308.SAA28635@WS0005.indiatimes.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/5cd2d0cf/attachment.pl 

From ffenics2002 at yahoo.co.uk  Mon Aug  7 16:38:27 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 7 Aug 2006 15:38:27 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
Message-ID: <20060807143827.78369.qmail@web25506.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/772686eb/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Aug  7 16:44:31 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 7 Aug 2006 16:44:31 +0200
Subject: [R] Is there a function in R can help me to plot such a figure?
In-Reply-To: <cedaa40b0608070540q4e28cd4emb378ab1818fd6d55@mail.gmail.com>
References: <cedaa40b0608070036i105037d2q3e09e64abab9bdc8@mail.gmail.com>
	<44D7F060.3090503@bitwrit.com.au>
	<cedaa40b0608070540q4e28cd4emb378ab1818fd6d55@mail.gmail.com>
Message-ID: <17623.20943.929867.475877@stat.math.ethz.ch>

>>>>> "Jiantao" == Jiantao Shi <bioconductor.cn at gmail.com>
>>>>>     on Mon, 7 Aug 2006 20:40:53 +0800 writes:

    Jiantao> This figure is the plot of prototype of a Self Organizing Maps(SOM)
    Jiantao> result.The numeric data is a M*N matrix, M rows and N
    Jiantao> columns.color2D.matplot can transform the numeric matrix to a color matrix
    Jiantao> ,but i have to put them  in  hexagon cells to reveal their topological
    Jiantao> structure( M  hexagon cells in a column,N  hexagon cells in a row).

    Jiantao> It seems that i have to use the GRID package in R.

yes, but only through use of the  'hexbin'  package
(part of Bioconductor, not CRAN yet) which is itself based on
using "grid".

'hexbin' does hexagon plots as you seem to want.

Martin Maechler, ETH Zurich


    Jiantao> On 8/8/06, Jim Lemon <jim at bitwrit.com.au> wrote:
    >> 
    >> Jiantao Shi wrote:
    >> > Hi,
    >> > i want to plot figure like this,
    >> >
    >> http://www.cis.hut.fi/projects/somtoolbox/download/pics2/shotvs2_colorcode.png
    >> > So is there a function or package in R can help me to do this.
    >> >
    >> color2D.matplot transforms a numeric matrix into a color matrix, as does
    >> "image". I don't know of an R function that does hexagon cells, although
    >> it wouldn't be impossible to program. What is the numeric data behind
    >> such a plot?
    >> 
    >> Jim


From chrish at stats.ucl.ac.uk  Mon Aug  7 16:46:40 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Mon, 7 Aug 2006 15:46:40 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
In-Reply-To: <20060807143827.78369.qmail@web25506.mail.ukl.yahoo.com>
References: <20060807143827.78369.qmail@web25506.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608071544580.7590@egon.stats.ucl.ac.uk>

First of all, kmeans doesn't work on distance matrices.

On Mon, 7 Aug 2006, Ffenics wrote:

> Hi there
> I have been using R to perform kmeans on a dataset. The data is fed in using read.table and then a matrix (x) is created
>
> i.e:
>
> [
> mat <- matrix(0, nlevels(DF$V1), nlevels(DF$V2),
> dimnames = list(levels(DF$V1), levels(DF$V2)))
> mat[cbind(DF$V1, DF$V2)] <- DF$V3
> This matrix is then taken and a distance matrix (y) created using dist() before performing the kmeans clustering.
>
> My query is this: not all the data for the initial matrix (x) exists and therefore the matrix is not fully populated - empty cells are populated with '0's.
>
> Could someone please tell me how this may affect the result from the dist() command - because a '0' in a distance matrix means that the two variables are identical doesnt it(?) - but I dont want tthings clustered together simply because there was no information.
>
> Is this a problem and are there ways to circumnavigate them? Thanks
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From ffenics2002 at yahoo.co.uk  Mon Aug  7 16:55:27 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 7 Aug 2006 15:55:27 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
In-Reply-To: <Pine.LNX.4.64.0608071544580.7590@egon.stats.ucl.ac.uk>
Message-ID: <20060807145527.41183.qmail@web25503.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/cc859010/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Aug  7 16:59:32 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Aug 2006 15:59:32 +0100 (BST)
Subject: [R] CPU Usage with R 2.1.0 in Windows (and with R 2.3.1)
In-Reply-To: <44D7421F.5090606@gmx.net>
References: <161A202DD5D3394FB46136A00CF844B719DA8E@mucmsrv.hq.clientvela.net>
	<44D7421F.5090606@gmx.net>
Message-ID: <Pine.LNX.4.64.0608071554380.5542@gannet.stats.ox.ac.uk>

On Mon, 7 Aug 2006, Singu wrote:

> You could compile R using specific libraries if threading/dual core is
> the issue
> 
> Look eg here:
> http://tolstoy.newcastle.edu.au/R/devel/05/12/3355.html

Hmm, quoting me.  None of those libraries for Windows is multithreaded, 
nor does it say anthing about threads for Windows.

> and in R Administration Guide Section A.(2.2)
> .
> And probably you are better of with Linux if you rely on speed:
> http://www.r-project.org/useR-2006/Slides/IacusEtAl.pdf
> 
> Stefan
> 
> PS For Windows the precompiled Atlas  Rblas.dll seems to be
> single-threaded. I use it on my P4 3Ghz HT Processor and CPU usage is at
> 50% as well.

Yes, as Windows does not have pthreads which is what ATLAS would use.


> Markus Preisetanz schrieb:
> > ... I have exactly the same issue with R 2.3.1 . The Question is: Why is R unable to take more CPU space to make the calculation process go faster?
> >  
> > In my case no hardisk nor any network device is involved (data in RAM, 600 of 1024 MB filled) - and the CPU usage of the rgui-process does not exceed 50%.
> >  
> > Has anybody an idea? Is there a setting a can change?
> >  
> > Sincerely, Markus
> > ____________________________
> > Markus Preisetanz
> > Consultant
> >
> > Client Vela GmbH
> > Albert-Ro?haupter-Str. 32
> > 81369 M?nchen
> > fon: +49 (89) 74217-113
> > main: +49 (89) 74217-150
> > fax: +49 (89) 74217-250
> > markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com> 
> > http://www.clientvela.com <http://www.clientvela.com/> 
> >  
> > Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die unbefugte Weitergabe dieser E-Mail ist nicht gestattet.
> >  
> > This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received the e-mail in error) please notify the sender immediately and destroy this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden
> >  
> > xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
> > From: Doran, Harold <HDoran_at_air.org <mailto:HDoran_at_air.org?Subject=Re:%20%5BR%5D%20CPU%20Usage%20with%20R%202.1.0%20in%20Windows> > 
> > Date: Wed 20 Jul 2005 - 04:59:34 EST
> >
> >
> >
> > Dear Michael: 
> >
> > Why is it a problem that R is not using more CPU space than it seems to need? 
> >
> > -----Original Message----- 
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael Sent: Tuesday, July 19, 2005 2:29 PM 
> > To: 'R-help at lists.R-project.org' 
> > Subject: [R] CPU Usage with R 2.1.0 in Windows 
> >
> > Hi, 
> >
> > I'm using a fairly simple HP Compaq desktop PC running Windows 2K. When running a large process in R, the process "RGUI.exe" will never exceed 50% of the CPU usage. 
> >
> > The program used to be able to use more of the computer, but does not now. 
> > I don't believe this is a multiple processor machine. 
> >
> > Can anyone give any advice on how to solve the problem? 
> >
> > Thanks, 
> >
> > Michael Greene 
> >
> > Product Management 
> > Plymouth Rock Assurance Corp 
> > 617-951-1682 
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> >   
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From gunter.berton at gene.com  Mon Aug  7 17:00:00 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 7 Aug 2006 08:00:00 -0700
Subject: [R] How to export data to Excel Spreadsheet?
In-Reply-To: <6ade6f6c0608070523u2e3c69aeq6168120051354600@mail.gmail.com>
Message-ID: <001401c6ba32$288ae440$8564a8c0@gne.windows.gene.com>

You can also usually copy and paste to/from the Windows clipboard by using
file='clipboard' in file i/o or via description = 'clipboard' using
connections. I haven't checked all details of this, so there may be some
glitches.  

-- Bert Gunter

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
Sent: Monday, August 07, 2006 5:23 AM
To: R-Help
Subject: Re: [R] How to export data to Excel Spreadsheet?

On 8/7/06, Xin <jasonshi510 at hotmail.com> wrote:
>    I try to export my output's data to Excel spreadsheet. My outputs are:
>
>  >comb3
>        [,1] [,2] [,3]
>   [1,] "a"  "b"  "c"
>   [2,] "a"  "b"  "d"
>   [3,] "a"  "b"  "e"
>   [4,] "a"  "b"  "f"
>   [5,] "a"  "b"  "g"

See

? write.table
? write.csv

Paul

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From lorenzo.logi at hsbc.fr  Mon Aug  7 17:05:33 2006
From: lorenzo.logi at hsbc.fr (lorenzo.logi at hsbc.fr)
Date: Mon, 7 Aug 2006 17:05:33 +0200
Subject: [R] unwanted conversion of date formats in 2.3.1 to character
Message-ID: <OF36C08F4E.FDF5D783-ONC12571C3.0051C80D@ccf.com>


good morning
I'm using R 2.2.1 on windows (2000/NT/XP) and trying to upgrade to 2.3.1

I am finding the curious problem of date/time data (both if stocked as date and as POSIX) that gets converted into character format whenever touched: it seems that date/time data just does not want to remain that way.

For example, a data.frame containing dates or POSIXct as row.names that is returned as result from a function arrives to the calling function with characters as row.names:
      pippo<-function(...) {
            result<-data.frame( ... )
            attributes(result)$row.names<- ... (some dates or POSIX)
            #mode(attributes(xx)$row.names) is now numeric, = date or POSIX
            return(result)
      }

      xx <- pippo(...)
      #mode(attributes(xx)$row.names) is now character !!!!

This is new to 2.3.1 (and to 2.3.0 i suppose), i.e. everything worked like a charm in 2.2.1; it looks like a bug to me (but might notbe), and I have not been able to find any reference to such a problem in the archives (which is strange,
but I might have got blind).

Any idea pls?
Thanks a lot in advance


Ensemble adoptons des gestes responsables : N'imprimez ce mail que si necessaire.

Les informations contenues dans ce message et les pieces jointes (ci-apres denomme le message) sont confidentielles et peuvent etre couvertes par le secret professionnel. Si vous n'etes pas le destinataire de ce message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d'en utiliser tout ou partie. Si vous avez recu ce message par erreur, nous vous remercions de le supprimer de votre systeme, ainsi que toutes ses copies, et d'en avertir immediatement HSBC France et ses filiales par message de retour. Il est impossible de garantir que les communications par messagerie electronique arrivent en temps utile, sont securisees ou denuees de toute erreur, alteration, falsification ou virus. En consequence, HSBC France et ses filiales declinent toute responsabilite du fait des erreurs, alterations, falsifications ou omissions qui pourraient en resulter.

Consider the environment before printing this mail.

The information contained in this e-mail is confidential. It...{{dropped}}


From kamila.naxerova at childrens.harvard.edu  Mon Aug  7 17:17:58 2006
From: kamila.naxerova at childrens.harvard.edu (Kamila Naxerova)
Date: Mon, 07 Aug 2006 11:17:58 -0400
Subject: [R] Running out of memory when using lapply
In-Reply-To: <mailman.7.1154944803.26285.r-help@stat.math.ethz.ch>
References: <mailman.7.1154944803.26285.r-help@stat.math.ethz.ch>
Message-ID: <44D759A6.1000600@childrens.harvard.edu>

Hi all!

I'm afraid I programmed something totally non-sensical and inefficient, 
but I can't figure out how to do it better.

I have a list of ~ 40 000 characters. I want to take each element at a 
time, map it to a large data frame with 
hit=which(data.frame$column==elementFromList), then compute some 
statistic on data.frame[hit,] and return a result that consists of 
either 1) a list of integers or 2) a character.

res=lapply(listof40000,myfunction,dataframeToSearchIn)

On a small scale, this works and returns something like

str(res)
[[1]]
[1] "UNIQUE"
[[2]]
[1]   405   406   407 16351
[[3]]
[1] "REMOVE"
[[4]]
[1] "REMOVE"

If I try this with the entire 40 000 character list, though, I get the 
"Reached total allocation of 1022Mb: see help(memory.size)" error message.

Can someone please give me a hint how to solve this problem correctly? 
THANKS!

Kamila


From ffenics2002 at yahoo.co.uk  Mon Aug  7 17:43:10 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 7 Aug 2006 16:43:10 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
Message-ID: <20060807154310.39886.qmail@web25513.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/c3a04bd5/attachment.pl 

From ggrothendieck at gmail.com  Mon Aug  7 17:43:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 11:43:33 -0400
Subject: [R] kmeans and incom,plete distance matrix concern
In-Reply-To: <20060807145527.41183.qmail@web25503.mail.ukl.yahoo.com>
References: <Pine.LNX.4.64.0608071544580.7590@egon.stats.ucl.ac.uk>
	<20060807145527.41183.qmail@web25503.mail.ukl.yahoo.com>
Message-ID: <971536df0608070843r58f4cef7ub9cdcb2432d7da82@mail.gmail.com>

There are many clustering functions in R and R packages and some
take distance objects whereas others do not.  You likely read about
hclust or some different clustering function.  See ?kmeans for the
kmeans function and also look at the CRAN Task View on clustering for
other clustering functions:

  http://cran.r-project.org/src/contrib/Views/

On 8/7/06, Ffenics <ffenics2002 at yahoo.co.uk> wrote:
> well then i dont understand because everything i have read so far suggests that you use the dist() function to create a matrix based on the euclideam distance and then the kmeans() function.
>
> If this is incorrect, then any suggestins as to how to do this properly would be much appreciated.
>
> Christian Hennig <chrish at stats.ucl.ac.uk> wrote: First of all, kmeans doesn't work on distance matrices.
>
> On Mon, 7 Aug 2006, Ffenics wrote:
>
> > Hi there
> > I have been using R to perform kmeans on a dataset. The data is fed in using read.table and then a matrix (x) is created
> >
> > i.e:
> >
> > [
> > mat <- matrix(0, nlevels(DF$V1), nlevels(DF$V2),
> > dimnames = list(levels(DF$V1), levels(DF$V2)))
> > mat[cbind(DF$V1, DF$V2)] <- DF$V3
> > This matrix is then taken and a distance matrix (y) created using dist() before performing the kmeans clustering.
> >
> > My query is this: not all the data for the initial matrix (x) exists and therefore the matrix is not fully populated - empty cells are populated with '0's.
> >
> > Could someone please tell me how this may affect the result from the dist() command - because a '0' in a distance matrix means that the two variables are identical doesnt it(?) - but I dont want tthings clustered together simply because there was no information.
> >
> > Is this a problem and are there ways to circumnavigate them? Thanks
> >
> >  [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Aug  7 17:45:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Aug 2006 16:45:49 +0100 (BST)
Subject: [R] unwanted conversion of date formats in 2.3.1 to character
In-Reply-To: <OF36C08F4E.FDF5D783-ONC12571C3.0051C80D@ccf.com>
References: <OF36C08F4E.FDF5D783-ONC12571C3.0051C80D@ccf.com>
Message-ID: <Pine.LNX.4.64.0608071641550.6064@gannet.stats.ox.ac.uk>

Row names of data frames are supposed to be character: if this worked in 
2.2.1 it was a bug there.

attributes(result)$row.names<- is not the way to assign an attribute 
(attr("row.names") <- is) and definitely not the way to assign row names 
(row.names(xx) <- is), and this may explain how you managed to circumvent 
the checks until now.

On Mon, 7 Aug 2006, lorenzo.logi at hsbc.fr wrote:

> 
> good morning
> I'm using R 2.2.1 on windows (2000/NT/XP) and trying to upgrade to 2.3.1
> 
> I am finding the curious problem of date/time data (both if stocked as date and as POSIX) that gets converted into character format whenever touched: it seems that date/time data just does not want to remain that way.
> 
> For example, a data.frame containing dates or POSIXct as row.names that is returned as result from a function arrives to the calling function with characters as row.names:
>       pippo<-function(...) {
>             result<-data.frame( ... )
>             attributes(result)$row.names<- ... (some dates or POSIX)
>             #mode(attributes(xx)$row.names) is now numeric, = date or POSIX
>             return(result)
>       }
> 
>       xx <- pippo(...)
>       #mode(attributes(xx)$row.names) is now character !!!!
> 
> This is new to 2.3.1 (and to 2.3.0 i suppose), i.e. everything worked 
> like a charm in 2.2.1; it looks like a bug to me (but might notbe), and 
> I have not been able to find any reference to such a problem in the 
> archives (which is strange, but I might have got blind).
> 
> Any idea pls?
> Thanks a lot in advance


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ffenics2002 at yahoo.co.uk  Mon Aug  7 17:55:24 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 7 Aug 2006 16:55:24 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
Message-ID: <20060807155524.44085.qmail@web25513.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/3f1c2225/attachment.pl 

From ggrothendieck at gmail.com  Mon Aug  7 17:58:50 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 11:58:50 -0400
Subject: [R] kmeans and incom,plete distance matrix concern
In-Reply-To: <20060807155524.44085.qmail@web25513.mail.ukl.yahoo.com>
References: <20060807155524.44085.qmail@web25513.mail.ukl.yahoo.com>
Message-ID: <971536df0608070858h5f676b41r3c387e60694b26c2@mail.gmail.com>

?kmeans says the following.  Note that x is a matrix of ***data***.
Also look at the examples at the end of the help page if its still
not clear.

Usage:

     kmeans(x, centers, iter.max = 10, nstart = 1,
            algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"))

Arguments:

       x: A numeric matrix of data, or an object that can be coerced to
          such a matrix (such as a numeric vector or a data frame with
          all numeric columns).



On 8/7/06, Ffenics <ffenics2002 at yahoo.co.uk> wrote:
>         Thanks. I had a look at that and it says:
>
>
> Partitioning Clustering:
>
>                Function                            kmeans()                          from package stats provides   several algorithms   for computing partitions with respect to   Euclidean distance.
> Hence why I am using a euclidean distance matrix. Why is this incorrect?
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> There are many clustering functions in R and R packages and some
> take distance objects whereas others do not.  You likely read about
> hclust or some different clustering function.  See ?kmeans for the
> kmeans function and also look at the CRAN Task View on clustering for
> other clustering functions:
>
>  http://cran.r-project.org/src/contrib/Views/
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ffenics2002 at yahoo.co.uk  Mon Aug  7 18:01:53 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 7 Aug 2006 17:01:53 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
In-Reply-To: <971536df0608070858h5f676b41r3c387e60694b26c2@mail.gmail.com>
Message-ID: <20060807160153.15015.qmail@web25515.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/7ada713f/attachment.pl 

From ffenics2002 at yahoo.co.uk  Mon Aug  7 18:29:29 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Mon, 7 Aug 2006 17:29:29 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
Message-ID: <20060807162929.57195.qmail@web25513.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/583b2f5b/attachment.pl 

From jpu at apple.com  Mon Aug  7 18:29:48 2006
From: jpu at apple.com (Jia Pu)
Date: Mon, 7 Aug 2006 09:29:48 -0700
Subject: [R] (... not defined because of singularities) in lm()
In-Reply-To: <44D6F897.13987.2205A7@localhost>
References: <44D6F897.13987.2205A7@localhost>
Message-ID: <BC6B95EB-8E64-4799-AF76-BBB63251BE4C@apple.com>

Thanks, Petr.

isInterNuclear and zeroSyllToEOP are both binary, and it turns out :
isInterNuclear & zeroSyllToEOP == zeroSyllToEOP.

On Aug 6, 2006, at 11:23 PM, Petr Pikal wrote:

> Hi
>
> On 4 Aug 2006 at 16:44, Jia Pu wrote:
>
> To:             	r-help at stat.math.ethz.ch
> From:           	Jia Pu <jpu at apple.com>
> Date sent:      	Fri, 4 Aug 2006 16:44:44 -0700
> Subject:        	[R] (... not defined because of singularities) in  
> lm()
>
>> I got the message, " Coefficients: (1 not defined because of
>> singularities)", in the returned result of lm(). What does it mean?
>> And where should I start investigating why it happens?
>
> one of your variables (probably isInterNuclear:zeroSyllToEOP) is a
> linear combination of other explanatory variables or is constant.
>
> HTH
> Petr
>


From jrkrideau at yahoo.ca  Mon Aug  7 18:46:20 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 7 Aug 2006 12:46:20 -0400 (EDT)
Subject: [R] Trying to do aseries of subsets with function or for loop
Message-ID: <20060807164620.88022.qmail@web33804.mail.mud.yahoo.com>

I want to  calculate the "scat" etc,  values for each
level of id, output a  vector of these values that is
a <- c(scat, sdog, srat, sbat ) and do an rbind on
them.  Each level of id has  a different value of
rate.  
 
So far it it looks to me like I can do this by a
series of repeated subsets of Df butI would think that
I should be able to do this with a function but I am
not 
having any luck.  Thus far, I'm not even sure if the
function is doing anything

The other approach seems to be a for loop but I don't
see how to handle it there either.  I am sure I have
seen something about exporting multiple .csv files a
couple of days ago that would be relevant but I cannot
find it. 

Can anyone suggest anything?

Thanks

Example:

cata <- c( 3,5,6,8,0, NA)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)
dogb <- c(2,4,6,8,10, 12)
rata <- c (NA, 5, 5, 4, 9, 0)
ratb <- c( 1,2,3,4,5,6)
bata <- c( 12, 42,NA, 45, 32, 54)
batb <- c( 13, 15, 17,19,21,23)
id <- Cs(a,b,b,c,a,b)
site <- c(1,1,4,4,1,4)

Df <- data.frame(cbind(site, id, cata, catb, doga,
dogb, rata, ratb, bata, batb))
Df
attach(Df)
rate <- c(2,3,4,5,6,7,8,9)
st <- Df$site

prog <- function ( y, z) { df <-subset(Df, site == y)
detach(Df)
attach(df)
scat <- sum(c(cata,catb), na.rm=T)* z
sdog <- sum(c(doga,dogb), na.rm=T)* z
srat <- sum(c(rata,ratb), na.rm=T)* z
sbat <- sum(c(bata,batb), na.rm=T)* z
san <- c(scat,sdog, srat,sbat)
detach(df)
attach(Df)
}
prog(st, rate)


From chrish at stats.ucl.ac.uk  Mon Aug  7 18:49:57 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Mon, 7 Aug 2006 17:49:57 +0100 (BST)
Subject: [R] kmeans and incom,plete distance matrix concern
In-Reply-To: <20060807162929.57195.qmail@web25513.mail.ukl.yahoo.com>
References: <20060807162929.57195.qmail@web25513.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608071748300.7590@egon.stats.ucl.ac.uk>

> Thanks everyone for their help so far. I'm very appreciative of the fact that
> people have pointed out that I was heading in the wrong direction.
> I would be most grateful if someone could look over the following simple
> example for me and tell me if this is how to do it.
> I'm assuming by data matrix you mean the 'raw data' organised as a matrix
> Data (not euclidean distance) matrix
>> DF
>  V1 V2 V3 V4
> 1 78 45 34 45
> 2 97 23 67 12
> 3  9 56 12 67
> 4 19 67 23 90
> 5 34 12 78 56
>
> and then
>>  clusters.kmeans <-kmeans(DF, 2)

This should work. (But why don't you try instead of asking first?)

> if I want 2 clusters for example.
>
> Am I also right in thinking that I can say which 'centriods' I want the clustering to be done?

You can specify from which centroids the kmeans iteration should start, if 
you want to.

Christian

>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From vbokony at enternet.hu  Mon Aug  7 19:09:27 2006
From: vbokony at enternet.hu (Veronika =?iso-8859-1?Q?B=F3kony?=)
Date: Mon, 07 Aug 2006 19:09:27 +0200
Subject: [R] failed to load gplots
Message-ID: <5.2.0.9.0.20060806143404.02995258@pop3.enternet.hu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060807/a23f3a61/attachment.pl 

From dieter.menne at menne-biomed.de  Mon Aug  7 19:16:43 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 7 Aug 2006 17:16:43 +0000 (UTC)
Subject: [R] failed to load gplots
References: <5.2.0.9.0.20060806143404.02995258@pop3.enternet.hu>
Message-ID: <loom.20060807T191548-560@post.gmane.org>

Veronika B?kony <vbokony <at> enternet.hu> writes:

> 
> Hi everyone!
> I get the following message anyhow I try to load the package gplots:
> 
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
>          ReadItem: unknown type 241
> Error in library(pkg, character.only = TRUE) :
>          package/namespace load failed for 'gplots'
> 
> Could any of you tell me what goes wrong? (I am using R 2.0.1. on Windows)
> 
First update your R-version to the current. gplots is a fairly new package,
don't know if it was tested with old versions of R.

Dieter


From sonal at deepfoo.com  Mon Aug  7 20:23:58 2006
From: sonal at deepfoo.com (Sonal Darbari)
Date: Mon, 07 Aug 2006 14:23:58 -0400
Subject: [R] Plots
Message-ID: <44D7853E.4010401@deepfoo.com>

Hi,

What commands are needed to get an output like this:


1. On X-Axis : 2 Indices ex. S&P500 and DOW JONES
2. Their repective dates


If I use the "plot" command, I get one output & if I use it again, I lose
the previous output. I need both of them on one graph only(As seen in 
the attachment).



Thanks,
Sonal

From Steward-owner at scoug.com  Mon Aug  7 20:25:30 2006
From: Steward-owner at scoug.com (Steward-owner)
Date: Mon, 7 Aug 2006 11:25:30 -0700
Subject: [R] Your Message To scoug-general
Message-ID: <20060807112531-21967-7@scoug.com>

Your message to the list scoug-general has been rejected.

You are not a member of the list. For help on subscribing to
the list, please send a message to Steward-request at scoug.com with
the word "help" in the body of the message.

Your humble mailing list software,

Steward


From ggrothendieck at gmail.com  Mon Aug  7 20:37:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 14:37:15 -0400
Subject: [R] Plots
In-Reply-To: <44D7853E.4010401@deepfoo.com>
References: <44D7853E.4010401@deepfoo.com>
Message-ID: <971536df0608071137k234fdaefl31530351726abc4d@mail.gmail.com>

Try:

   RSiteSearch("Horses and Hounds")


On 8/7/06, Sonal Darbari <sonal at deepfoo.com> wrote:
> Hi,
>
> What commands are needed to get an output like this:
>
>
> 1. On X-Axis : 2 Indices ex. S&P500 and DOW JONES
> 2. Their repective dates
>
>
> If I use the "plot" command, I get one output & if I use it again, I lose
> the previous output. I need both of them on one graph only(As seen in
> the attachment).
>
>
>
> Thanks,
> Sonal
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Mon Aug  7 20:43:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 14:43:14 -0400
Subject: [R] Plots
In-Reply-To: <971536df0608071137k234fdaefl31530351726abc4d@mail.gmail.com>
References: <44D7853E.4010401@deepfoo.com>
	<971536df0608071137k234fdaefl31530351726abc4d@mail.gmail.com>
Message-ID: <971536df0608071143h1c39e8cdy7bc0a498e4929767@mail.gmail.com>

Also RSiteSearch("ts.plot.2Axis")


On 8/7/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try:
>
>   RSiteSearch("Horses and Hounds")
>
>
> On 8/7/06, Sonal Darbari <sonal at deepfoo.com> wrote:
> > Hi,
> >
> > What commands are needed to get an output like this:
> >
> >
> > 1. On X-Axis : 2 Indices ex. S&P500 and DOW JONES
> > 2. Their repective dates
> >
> >
> > If I use the "plot" command, I get one output & if I use it again, I lose
> > the previous output. I need both of them on one graph only(As seen in
> > the attachment).
> >
> >
> >
> > Thanks,
> > Sonal
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From Robert.McGehee at geodecapital.com  Mon Aug  7 21:00:32 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Mon, 7 Aug 2006 15:00:32 -0400
Subject: [R] Capturing stderr from system()
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C9468C2@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,
I have a system command that occasionally fails and writes output to
standard error, which R will print to the screen when ignore.stderr =
FALSE.

For example:
> system("BadCommand")
sh: line 1: BadCommand: command not found

I would like to know if the above command fails, and can presumably do
this by parsing the stderr message that R prints to the screen. My
(hopefully simple) problem is that I can't figure out how to capture
this output.

I tried intern = TRUE, capture.output, and sink (with either/both type =
"message" and type = "output"). I'm perplexed why sink doesn't get this
output (since it's printed to the screen), so wanted to ask the R
community how to grab the error message generated by trying to run
"BadCommand".

To be clear, below is the sink() syntax I tried. Notice the output of
the sink is character(0) rather than the line "sh: line 1: BadCommand:
command not found".

> conn <- sink(z <- tempfile())
> sink(conn, type = "message")
> system("BadCommand")
sh: line 1: BadCommand: command not found
> sink(type = "message")
> sink()
> readLines(z)
character(0)

Thanks in advance,
Robert

> R.version
               _                                        
platform       i686-pc-linux-gnu                        
arch           i686                                     
os             linux-gnu                                
system         i686, linux-gnu                          
status         Patched                                  
major          2                                        
minor          3.1                                      
year           2006                                     
month          07                                       
day            27                                       
svn rev        38721                                    
language       R                                        
version.string Version 2.3.1 Patched (2006-07-27 r38721)


Robert McGehee
Quantitative Analyst
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}


From ripley at stats.ox.ac.uk  Mon Aug  7 21:28:31 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Aug 2006 20:28:31 +0100 (BST)
Subject: [R] Capturing stderr from system()
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C9468C2@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020C9468C2@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.64.0608072013590.8047@gannet.stats.ox.ac.uk>

On Mon, 7 Aug 2006, McGehee, Robert wrote:

> Hello,
> I have a system command that occasionally fails and writes output to
> standard error, which R will print to the screen when ignore.stderr =
> FALSE.

No, R does not print to the screen, your shell does. You can capture shell 
and command errors by

	system("command 2>&1", intern=TRUE) 

or whatever the appropriate syntax for your shell is.  E.g.

> system("BadCommand 2>&1", intern=TRUE)
[1] "sh: BadCommand: command not found"

> For example:
> > system("BadCommand")
> sh: line 1: BadCommand: command not found

(My shell does not give `line 1' here.)

> I would like to know if the above command fails, and can presumably do
> this by parsing the stderr message that R prints to the screen. My
> (hopefully simple) problem is that I can't figure out how to capture
> this output.

You are starting from a false assertion ....

[...]

> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do note what it says about where to send programming questions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mtb954 at gmail.com  Mon Aug  7 22:05:29 2006
From: mtb954 at gmail.com (Mark Na)
Date: Mon, 7 Aug 2006 14:05:29 -0600
Subject: [R] Retain only those records from a dataframe that exist in
	another dataframe
Message-ID: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>

Dear R community,

I have two dataframes "first" and "second" which share a unique identifier.

I wish to make a new dataframe "third" retaining only the rows in
"first" which also occur in "second".

I have tried using merge but can't seem to figure it out. Any ideas?

Thanks!

Mark


From simone.vincenzi at nemo.unipr.it  Mon Aug  7 22:07:21 2006
From: simone.vincenzi at nemo.unipr.it (Simone Vincenzi)
Date: Mon, 7 Aug 2006 22:07:21 +0200
Subject: [R] Help with short time series
Message-ID: <20060807200238.M58400@nemo.unipr.it>

Thanks for the help. 
I provide below the dataset I'm using, it's a little bit different from what 
I was describing (sorry for that). The streams are 3 and I have an unequal 
number of years for each stream.

Stream Density Year 
1 ? ? Zak ? ?0.20 2000 
2 ? ? Zak ? ?0.36 2001 
3 ? ? Zak ? ?0.41 2002 
4 ? ? Zak ? ?0.34 2003 
5 ? ? Zak ? ?0.28 2004 
6 ? ? Gor ? ?0.08 1999 
7 ? ? Gor ? ?0.05 2000 
8 ? ? Gor ? ?0.14 2001 
9 ? ? Gor ? ?0.16 2002 
10 ? ?Gor ? ?0.13 2003 
11 ? ?Gat ? ?0.18 2004 
12 ? ?Gat ? ?0.10 2001 
13 ? ?Gat ? ?0.37 2002 
14 ? ?Gat ? ?0.57 2003 
15 ? ?Gat ? ?0.47 2004

I tried to follow the suggestions of Dieter, but the model does not fit. 
Any suggestion will be appreciated



Dear R-list, 
> I have a statistical problem with the comparison of two short time-series 
of 
> density data in an ecological framework. I have to compare two short time 
> series (5 years, one value for each year) of species density data (it is 
the 
> density of fish in two different streams) to test if the two means of the 
> five densities are significantly different, so basically if the two mean 
> stream-specific fish densities are significantly different. 
> I don't think I can use a straight t-test due to the problem of 
> autocorrelation and I don't think I can use a repeated measure ANOVA as I 
> don't have any replicates. 
> Any help would be greatly appreciated.

try something like

library(nlme) 
summary(lme(dens~stream+year,data=mystreamdata,random=~year|stream))

This should also give you an estimate if the slopes are different if you 
test 
against the simplified model

summary(lme(dens~stream+year,data=mystreamdata,random=~1|stream))

Since you did not provide a short example data set, this is only 
approximatively 
right.

Dieter

-- 
Universita' degli Studi di Parma (http://www.unipr.it)


From p.dalgaard at biostat.ku.dk  Mon Aug  7 22:14:05 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Aug 2006 22:14:05 +0200
Subject: [R] Retain only those records from a dataframe that exist in
	another dataframe
In-Reply-To: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>
References: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>
Message-ID: <x2psfcnw1e.fsf@turmalin.kubism.ku.dk>

"Mark Na" <mtb954 at gmail.com> writes:

> Dear R community,
> 
> I have two dataframes "first" and "second" which share a unique identifier.
> 
> I wish to make a new dataframe "third" retaining only the rows in
> "first" which also occur in "second".
> 
> I have tried using merge but can't seem to figure it out. Any ideas?

Doesn't sound like a merge problem. Will this do it?:

first[first$ID %in% second$ID,]

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From mschwartz at mn.rr.com  Mon Aug  7 22:17:27 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 07 Aug 2006 15:17:27 -0500
Subject: [R] Retain only those records from a dataframe that
	exist	in	another dataframe
In-Reply-To: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>
References: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>
Message-ID: <1154981847.4146.37.camel@localhost.localdomain>

On Mon, 2006-08-07 at 14:05 -0600, Mark Na wrote:
> Dear R community,
> 
> I have two dataframes "first" and "second" which share a unique identifier.
> 
> I wish to make a new dataframe "third" retaining only the rows in
> "first" which also occur in "second".
> 
> I have tried using merge but can't seem to figure it out. Any ideas?
> 
> Thanks!
> 
> Mark

Do you want to actually join (merge) matching rows from 'first' and
'second' into 'third', or just get a subset of the rows from 'first'
where there is a matching UniqueID in 'second'?

In the first case:

  third <- merge(first, second, by = "UniqueID")

Note that the UniqueID column is quoted.


In the second case:

  third <- subset(first, UniqueID %in% second$UniqueID)

See ?merge, ?"%in%" and ?subset

HTH,

Marc Schwartz


From mtb954 at gmail.com  Mon Aug  7 22:49:23 2006
From: mtb954 at gmail.com (Mark Na)
Date: Mon, 7 Aug 2006 14:49:23 -0600
Subject: [R] Retain only those records from a dataframe that exist in
	another dataframe
In-Reply-To: <1154981847.4146.37.camel@localhost.localdomain>
References: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>
	<1154981847.4146.37.camel@localhost.localdomain>
Message-ID: <e40d78ce0608071349h40b4bc53ha49da6a1d6ce21a8@mail.gmail.com>

Thanks Peter and Mark, the subset and %in% commands did the job.

For future reference, or for others reading this message, the code I
ended up using was:

> third <- subset(first, ID %in% second$ID)

Mark



On 8/7/06, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> On Mon, 2006-08-07 at 14:05 -0600, Mark Na wrote:
> > Dear R community,
> >
> > I have two dataframes "first" and "second" which share a unique identifier.
> >
> > I wish to make a new dataframe "third" retaining only the rows in
> > "first" which also occur in "second".
> >
> > I have tried using merge but can't seem to figure it out. Any ideas?
> >
> > Thanks!
> >
> > Mark
>
> Do you want to actually join (merge) matching rows from 'first' and
> 'second' into 'third', or just get a subset of the rows from 'first'
> where there is a matching UniqueID in 'second'?
>
> In the first case:
>
>   third <- merge(first, second, by = "UniqueID")
>
> Note that the UniqueID column is quoted.
>
>
> In the second case:
>
>   third <- subset(first, UniqueID %in% second$UniqueID)
>
> See ?merge, ?"%in%" and ?subset
>
> HTH,
>
> Marc Schwartz
>
>
>


From leog at anicca-vijja.de  Tue Aug  8 00:13:19 2006
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Tue, 08 Aug 2006 00:13:19 +0200
Subject: [R] mathematica -> r (gamma function + integration)
Message-ID: <44D7BAFF.2030305@anicca-vijja.de>

Dear R-list,

I try to transform a mathematica script to R.

#######relevant part of the Mathematica script
(* p_sv *)
dd = NN (DsD - DD^2);
lownum = NN (L-DD)^2;
upnum  = NN (H-DD)^2;
low = lownum/(2s^2);
up  = upnum/(2s^2);
psv = NIntegrate[1/(s^NN) Exp[-dd/(2s^2)]
    (Gamma[1/2,0,up] + Gamma[1/2,0,low]),{s,sL,sH},
    MinRecursion->3];
PSV = psv/Sqrt[2NN];
Print["------------- Results ------------------------------------"];
Print[" "];
Print["p(sv|D_1D_2I)   = const. ",N[PSV,6]];
########

# R part
library(fOptions)

###raw values for reproduction
NN <- 58
dd <- 0.411769
lownum <- 20.81512
upnum <- 6.741643
sL <- 0.029
sH <- 0.092
###

integpsv <- function(s) { 1 / (s^NN) * exp(-dd / (2 * s^2)) *
  ( (igamma((upnum/(2*s^2)),1/2) - igamma(0,1/2) ) +
    (igamma((lownum/(2*s^2)),1/2) - igamma(0,1/2) ) )
}
psv <- integrate(integpsv, lower=sL, upper=sH)
PSV <- psv$value / sqrt(2*NN)
print("------------- Results ------------------------------------\n")
print(paste("p(sv|D_1D_2I)   = const. ",PSV, sep=""))


The results of variable "PSV" are not the same.

In mathematica -> PSV ~ 2.67223e+47
with rounding errors due to the initial values, in R -> PSV ~ 1.5e+47

I am not that familiar with gamma functions and integration, thus I 
assume there the source of the problem can be located.
Thanks for helping me to adjust the sript.

best wishes
leo


From aolinto_r at bignet.com.br  Tue Aug  8 00:49:42 2006
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Mon,  7 Aug 2006 19:49:42 -0300
Subject: [R] finding x values to meet a y
Message-ID: <1154990982.44d7c3867617e@webmail.bignet.com.br>

Hi,

I'd like to find which values of x will give me a y.

In other words, in the example of a gaussian curve, I want to find the values of
x that will give me a density, let's say, of 0.02.

curve(((1/(sqrt(2*pi)*10))*exp(-((x-50)^2)/(2*10^2))),xlim=c(0,100))

Thanks for any help,

Antonio Olinto




-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br


From ggrothendieck at gmail.com  Tue Aug  8 02:37:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 20:37:14 -0400
Subject: [R] Retain only those records from a dataframe that exist in
	another dataframe
In-Reply-To: <x2psfcnw1e.fsf@turmalin.kubism.ku.dk>
References: <e40d78ce0608071305l6ab5e1c7wcad5261419f9dd3e@mail.gmail.com>
	<x2psfcnw1e.fsf@turmalin.kubism.ku.dk>
Message-ID: <971536df0608071737v62759dabp230af3a0c88b8fa1@mail.gmail.com>

Although this is probably not directly applicable to this problem
I might mention here that merge.zoo does support left and right
joins and that handles problems similar to this.  z3t, z3ft,
z3tf and z3f below have times of both unioned, the times of
z2, the times of z1 and the times of both z1 and z2 intersected
respectively:

library(zoo)
z1 <- zoo(1:5, 1:5)
z2 <- zoo(2:6, 2:6)
z3t <- merge(z1, z2, all = TRUE)
z3ft <- merge(z1, z2, all = c(FALSE, TRUE))
z3tf <- merge(z1, z2, all = c(TRUE, FALSE))
z3f - merge(z1, z2, all = FALSE)


On 07 Aug 2006 22:14:05 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Mark Na" <mtb954 at gmail.com> writes:
>
> > Dear R community,
> >
> > I have two dataframes "first" and "second" which share a unique identifier.
> >
> > I wish to make a new dataframe "third" retaining only the rows in
> > "first" which also occur in "second".
> >
> > I have tried using merge but can't seem to figure it out. Any ideas?
>
> Doesn't sound like a merge problem. Will this do it?:
>
> first[first$ID %in% second$ID,]
>
> --
>   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pbarata at infolink.com.br  Tue Aug  8 03:46:18 2006
From: pbarata at infolink.com.br (Paulo Barata)
Date: Mon, 07 Aug 2006 22:46:18 -0300
Subject: [R] parameter yaxs / function hist (graphics)
In-Reply-To: <mailman.9.1154685603.13659.r-help@stat.math.ethz.ch>
References: <mailman.9.1154685603.13659.r-help@stat.math.ethz.ch>
Message-ID: <44D7ECEA.10009@infolink.com.br>


Dear R users,

The parameters xaxs and yaxs (function par, package graphics)
seem not to work with the function hist (package graphics),
even when the parameters xlim and ylim are defined.

Is there any way to make yaxs="i" and xaxs="i" work properly
with the function hist, mainly to produce histograms that
"touch" the horizontal axis? The R documentation and the
R mailing lists archive don't seem to be of help here.

I am using R 2.3.1, running under Windows XP.

## Example:
x <- rnorm(100)
hist(x,breaks=seq(-4,4,0.5),ylim=c(0,40),yaxs="i",
   xlim=c(-4,4),xaxs="i")
box()

Thank you very much.

Paulo Barata

--------------------------------------------------------------
Paulo Barata
Fundacao Oswaldo Cruz / Oswaldo Cruz Foundation
Rua Leopoldo Bulhoes 1480 - 8A
21041-210  Rio de Janeiro - RJ
Brasil
E-mail: pbarata at infolink.com.br


From paulojus at est.ufpr.br  Tue Aug  8 04:06:24 2006
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Mon, 7 Aug 2006 23:06:24 -0300 (BRT)
Subject: [R] parameter yaxs / function hist (graphics)
In-Reply-To: <44D7ECEA.10009@infolink.com.br>
References: <mailman.9.1154685603.13659.r-help@stat.math.ethz.ch>
	<44D7ECEA.10009@infolink.com.br>
Message-ID: <Pine.LNX.4.63.0608072304570.7707@est.ufpr.br>

Paulo

One possibility is to draw the histogram without axes and then add them 
wherever you want.

For instance with something along the lines:

x <- rnorm(500)
hist(x, axes=F)
axis(1, line=-1)

For more details: ?axis

best
P.J.


Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus

On Mon, 7 Aug 2006, Paulo Barata wrote:

>
> Dear R users,
>
> The parameters xaxs and yaxs (function par, package graphics)
> seem not to work with the function hist (package graphics),
> even when the parameters xlim and ylim are defined.
>
> Is there any way to make yaxs="i" and xaxs="i" work properly
> with the function hist, mainly to produce histograms that
> "touch" the horizontal axis? The R documentation and the
> R mailing lists archive don't seem to be of help here.
>
> I am using R 2.3.1, running under Windows XP.
>
> ## Example:
> x <- rnorm(100)
> hist(x,breaks=seq(-4,4,0.5),ylim=c(0,40),yaxs="i",
>   xlim=c(-4,4),xaxs="i")
> box()
>
> Thank you very much.
>
> Paulo Barata
>
> --------------------------------------------------------------
> Paulo Barata
> Fundacao Oswaldo Cruz / Oswaldo Cruz Foundation
> Rua Leopoldo Bulhoes 1480 - 8A
> 21041-210  Rio de Janeiro - RJ
> Brasil
> E-mail: pbarata at infolink.com.br
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

From adik at ilovebacon.org  Tue Aug  8 04:03:41 2006
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Mon, 7 Aug 2006 19:03:41 -0700 (PDT)
Subject: [R] Pairwise n for large correlation tables?
In-Reply-To: <mailman.7.1154944803.26285.r-help@stat.math.ethz.ch>
References: <mailman.7.1154944803.26285.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0608071836330.4867@parser.ilovebacon.org>

Hello,

I'm using a very large data set (n > 100,000 for 7 columns), for which I'm
pretty happy dealing with pairwise-deleted correlations to populate my
correlation table. E.g.,

a <- cor(cbind(col1, col2, col3),use="pairwise.complete.obs")

...however, I am interested in the number of cases used to compute each
cell of the correlation table. I am unable to find such a function via
google searches, so I wrote one of my own. This turns out to be highly
inefficient (e.g., it takes much, MUCH longer than the correlations do). Any
hints, regarding other functions to use or ways to maket his speedier, would
be much appreciated!

pairwise.n <- function(df=stop("Must provide data frame!")) {
   if (!is.data.frame(df)) {
     df <- as.data.frame(df)
   }
   colNum <- ncol(df)
   result <- matrix(data=NA,nrow=colNum,ncol=ncolNum,dimnames=list(colnames(df),colnames(df)))
   for(i in 1:colNum) {
     for (j in i:colNum) {
       result[i,j] <- length(df[!is.na(df[i])&!is.na(df[j])])/colNum
     }
   }
   result
}

--
Adam D. I. Kramer
University of Oregon


From ggrothendieck at gmail.com  Tue Aug  8 04:40:19 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Aug 2006 22:40:19 -0400
Subject: [R] Pairwise n for large correlation tables?
In-Reply-To: <Pine.LNX.4.64.0608071836330.4867@parser.ilovebacon.org>
References: <mailman.7.1154944803.26285.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0608071836330.4867@parser.ilovebacon.org>
Message-ID: <971536df0608071940u66becac3g70613b7bc56b2bf2@mail.gmail.com>

Try this:

# mat is test matrix
mat <- matrix(1:25, 5)
mat[2,2] <- mat[3,4] <- NA
crossprod(!is.na(mat))


On 8/7/06, Adam D. I. Kramer <adik at ilovebacon.org> wrote:
> Hello,
>
> I'm using a very large data set (n > 100,000 for 7 columns), for which I'm
> pretty happy dealing with pairwise-deleted correlations to populate my
> correlation table. E.g.,
>
> a <- cor(cbind(col1, col2, col3),use="pairwise.complete.obs")
>
> ...however, I am interested in the number of cases used to compute each
> cell of the correlation table. I am unable to find such a function via
> google searches, so I wrote one of my own. This turns out to be highly
> inefficient (e.g., it takes much, MUCH longer than the correlations do). Any
> hints, regarding other functions to use or ways to maket his speedier, would
> be much appreciated!
>
> pairwise.n <- function(df=stop("Must provide data frame!")) {
>   if (!is.data.frame(df)) {
>     df <- as.data.frame(df)
>   }
>   colNum <- ncol(df)
>   result <- matrix(data=NA,nrow=colNum,ncol=ncolNum,dimnames=list(colnames(df),colnames(df)))
>   for(i in 1:colNum) {
>     for (j in i:colNum) {
>       result[i,j] <- length(df[!is.na(df[i])&!is.na(df[j])])/colNum
>     }
>   }
>   result
> }
>
> --
> Adam D. I. Kramer
> University of Oregon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From christos at nuverabio.com  Tue Aug  8 04:44:03 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Mon, 7 Aug 2006 22:44:03 -0400
Subject: [R] Pairwise n for large correlation tables?
In-Reply-To: <Pine.LNX.4.64.0608071836330.4867@parser.ilovebacon.org>
Message-ID: <002001c6ba94$83554ed0$0202a8c0@headquarters.silicoinsights>

Hi,

You can use complete.cases
It should run faster than the code you suggested.

See following example:

x <- matrix(runif(30),10,3)

# introduce missing values
x[sample(1:10,3),1] <- NA
x[sample(1:10,3),2] <- NA
x[sample(1:10,3),3] <- NA

cor(x,use="pairwise.complete.obs")

n <- ncol(x)
n.na <- matrix(0, n, n)
for (i in seq(1, n)) {
    n.na[i,i] <- sum( complete.cases(x[, i]) )
    for (j in seq(i+1, length=n-i)) {
        ok <- sum( complete.cases(x[, i], x[, j]) )
        n.na[i,j] <- n.na[j,i] <- ok
    }
}
 
HTH

-Christos

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adam D. I. Kramer
Sent: Monday, August 07, 2006 10:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Pairwise n for large correlation tables?

Hello,

I'm using a very large data set (n > 100,000 for 7 columns), for which I'm
pretty happy dealing with pairwise-deleted correlations to populate my
correlation table. E.g.,

a <- cor(cbind(col1, col2, col3),use="pairwise.complete.obs")

...however, I am interested in the number of cases used to compute each cell
of the correlation table. I am unable to find such a function via google
searches, so I wrote one of my own. This turns out to be highly inefficient
(e.g., it takes much, MUCH longer than the correlations do). Any hints,
regarding other functions to use or ways to maket his speedier, would be
much appreciated!

pairwise.n <- function(df=stop("Must provide data frame!")) {
   if (!is.data.frame(df)) {
     df <- as.data.frame(df)
   }
   colNum <- ncol(df)
   result <-
matrix(data=NA,nrow=colNum,ncol=ncolNum,dimnames=list(colnames(df),colnames(
df)))
   for(i in 1:colNum) {
     for (j in i:colNum) {
       result[i,j] <- length(df[!is.na(df[i])&!is.na(df[j])])/colNum
     }
   }
   result
}

--
Adam D. I. Kramer
University of Oregon

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rab at nauticom.net  Tue Aug  8 05:23:46 2006
From: rab at nauticom.net (Rick Bilonick)
Date: Mon, 07 Aug 2006 23:23:46 -0400
Subject: [R] oodraw command line usage
Message-ID: <1155007426.4465.8.camel@localhost.localdomain>

I use R to create .eps graphics and then use oodraw to convert them
to .emf versions. (One reason I do this is that OOo tends to
re-size .eps files and I haven't found a way to stop it or change it
once the graph is resized. .emf files are not distorted by OOo -
fortunately.) I use a command like:

> oodraw filename.eps &

The gui opens and then I select .emf and do an export. I haven't found a
way to eliminate the gui and do everything from the command line. Is
this possible? I need to convert a large number of files and it would be
convenient to do it all from the command line. (I also note that I
sometimes get warning messages that OOo can't create .emf files - but
there is never a problem creating the .emf files.) I'm using the latest
version of OOo. I've looked at the arguments for oodraw and I don't see
any way to specify that I want a .emf file as an output.

Rick B.


From bilonickra at upmc.edu  Tue Aug  8 05:57:20 2006
From: bilonickra at upmc.edu (Rick Bilonick)
Date: Mon, 07 Aug 2006 23:57:20 -0400
Subject: [R] oodraw command line usage
In-Reply-To: <1155007426.4465.8.camel@localhost.localdomain>
References: <1155007426.4465.8.camel@localhost.localdomain>
Message-ID: <1155009440.4465.10.camel@localhost.localdomain>

Please ignore this - it was sent to the wrong list by mistake.

Rick B.

On Mon, 2006-08-07 at 23:23 -0400, Rick Bilonick wrote:
> I use R to create .eps graphics and then use oodraw to convert them
> to .emf versions. (One reason I do this is that OOo tends to
> re-size .eps files and I haven't found a way to stop it or change it
> once the graph is resized. .emf files are not distorted by OOo -
> fortunately.) I use a command like:
> 
> > oodraw filename.eps &
> 
> The gui opens and then I select .emf and do an export. I haven't found a
> way to eliminate the gui and do everything from the command line. Is
> this possible? I need to convert a large number of files and it would be
> convenient to do it all from the command line. (I also note that I
> sometimes get warning messages that OOo can't create .emf files - but
> there is never a problem creating the .emf files.) I'm using the latest
> version of OOo. I've looked at the arguments for oodraw and I don't see
> any way to specify that I want a .emf file as an output.
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Assistant Professor
University of Pittsburgh School of Medicine
Department of Ophthalmology
412 648 9138
BST S 207


From mikeumo at sbcglobal.net  Tue Aug  8 06:15:36 2006
From: mikeumo at sbcglobal.net (Mike)
Date: Mon, 07 Aug 2006 23:15:36 -0500
Subject: [R] Source installation error: "gfortran and gcc disagree on
	int and double ...
References: <eb69j3$cmo$1@sea.gmane.org>
	<Pine.LNX.4.64.0608070821180.20401@gannet.stats.ox.ac.uk>
Message-ID: <eb935b$ihp$1@sea.gmane.org>

Prof Brian Ripley wrote:

> First, you do not need -fPIC: it is the same as -fpic which R selects on
> your platform.
> 

Right, I commented the flags and configure goes just as far


> Second, please look at config.log to find the exact problem: it well be
> that your compilers are not properly installed (as was the case in the
> reference you quote below).

from config.log it seem that there are a few missing inclides and syntax
errors in confdefs.h. (I can quote the specifics, if necessary). The last
lines before the error message are:

configure:27770: checking whether gfortran and gcc agree on int and double
conftest.c: In function 'main':
conftest.c:28: warning: implicit declaration of function 'printf'
conftest.c:28: warning: incompatible implicit declaration of built-in
function 'printf'
conftest.c:29: warning: implicit declaration of function 'exit'
conftest.c:29: warning: incompatible implicit declaration of built-in
function 'exit'
conftestf.o: In function `cftest_':
/home/mike/Desktop/R-2.3.1/conftestf.f:9: undefined reference to
`_gfortran_pow_r8_i4'
collect2: ld returned 1 exit status
configure:27848: WARNING: gfortran and gcc disagree on int and double
configure:27850: error: Maybe change CFLAGS or FFLAGS?

> 
> Finally, your compilers are pretty obselete (there have been 4.0.2, 4.0.3,
> 4.1.0 and 4.1.1), so you should be updating them.
> 
Changing compiler is not an option for me. Could you advise me how to
configure the flags?

Thanks, 

Mike


From Bill.Venables at csiro.au  Tue Aug  8 07:52:20 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 8 Aug 2006 15:52:20 +1000
Subject: [R] Constrain coefs. in linear model to sum to 0
Message-ID: <B998A44C8986644EA8029CFE6396A924547676@exqld2-bne.qld.csiro.au>

 
Gorjanc Gregor asks:

> 
> Hello!
> 
> I would like to use constrain to sum coeficients of a factor to 0
instead
> of classical corner contraint i.e. I would like to fit a model like
> 
> lm(y ~ 1 + effectA + effectB)
> 
> and say get parameters
> 
> intercept
> effectA_1
> effectA_2
> effectB_1
> effectB_2
> effectB_3
> 
> where effectA_1 represents deviation of level A_1 from intercept and 
> sum(effectA_1, effectA_2) = 0 and the same for factor B.
> 
> Is this possible to do?

Yes, but not quite as simply as you would like.  If you set

options(contrasts = c("contr.sum", "contr.poly"))

for example, then factor models are parametrised as you wish above,
BUT you don't get all the effects directly

In your case above, for example, if fm is the fitted model object, then

coef(fm)

Will give you intercept, effectA_2, effectB_2, effectB_3.  The
remaining effects*_1 you will need to calculate as the negative of the
sum of all the others.

This gets a bit more complicated when you have crossed terms, a*b, but
the same principle applies.

Bill Venables.


> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.


Well, now's your chance!


From ligges at statistik.uni-dortmund.de  Tue Aug  8 08:43:24 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Aug 2006 08:43:24 +0200
Subject: [R] finding x values to meet a y
In-Reply-To: <1154990982.44d7c3867617e@webmail.bignet.com.br>
References: <1154990982.44d7c3867617e@webmail.bignet.com.br>
Message-ID: <44D8328C.8060902@statistik.uni-dortmund.de>



Antonio Olinto wrote:
> Hi,
> 
> I'd like to find which values of x will give me a y.
> 
> In other words, in the example of a gaussian curve, I want to find the values of
> x that will give me a density, let's say, of 0.02.
> 
> curve(((1/(sqrt(2*pi)*10))*exp(-((x-50)^2)/(2*10^2))),xlim=c(0,100))

Numerical optimization might help:

optimize(
   function(x) (1/(sqrt(2*pi)*10) * exp(-((x-50)^2)/(2*10^2)) - 0.02)^2,
   interval = c(0, 50))

Uwe Ligges

> Thanks for any help,
> 
> Antonio Olinto
> 
> 
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral
> www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ogabbrie at tin.it  Tue Aug  8 09:53:12 2006
From: ogabbrie at tin.it (Simone Gabbriellini)
Date: Tue, 8 Aug 2006 09:53:12 +0200
Subject: [R] geodesic distance
In-Reply-To: <69A62C51-EEEB-494A-A9C8-289276EC82E1@unimi.it>
References: <69A62C51-EEEB-494A-A9C8-289276EC82E1@unimi.it>
Message-ID: <A684141C-2564-41B8-B9E8-5A1F437E400B@tin.it>

I don't know exactly, but you can have a look at the sna package,  
there should be a function for geodesic computation in social network  
analysis...

regards,
Simone

Il giorno 03/ago/06, alle ore 11:14, stefano iacus ha scritto:

> Hi,
> has anyone ever seen implemented in R the following "geodesic"
> distance between positive definite pxp matrices A and B?
>
> d(A,B) = \sum_{i=1}^p (\log \lambda_i)^2
>
> were \lambda is the solution of det(A -\lambda B)  = 0
>
> thanks
> stefano
>
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Tue Aug  8 09:55:50 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Aug 2006 09:55:50 +0200
Subject: [R] integrate() problem {was "mathematica -> r ..."}
In-Reply-To: <44D7BAFF.2030305@anicca-vijja.de>
References: <44D7BAFF.2030305@anicca-vijja.de>
Message-ID: <17624.17286.610035.45098@stat.math.ethz.ch>

>>>>> "Leo" == Leo G?rtler <leog at anicca-vijja.de>
>>>>>     on Tue, 08 Aug 2006 00:13:19 +0200 writes:

    Leo> Dear R-list,
    Leo> I try to transform a mathematica script to R.

    Leo> #######relevant part of the Mathematica script
    Leo> (* p_sv *)
    Leo> dd = NN (DsD - DD^2);
    Leo> lownum = NN (L-DD)^2;
    Leo> upnum  = NN (H-DD)^2;
    Leo> low = lownum/(2s^2);
    Leo> up  = upnum/(2s^2);
    Leo> psv = NIntegrate[1/(s^NN) Exp[-dd/(2s^2)]
    Leo>      (Gamma[1/2,0,up] + Gamma[1/2,0,low]),{s,sL,sH}, MinRecursion-> 3];
    Leo> PSV = psv/Sqrt[2NN];
    Leo> Print["------------- Results ------------------------------------"];
    Leo> Print[" "];
    Leo> Print["p(sv|D_1D_2I)   = const. ",N[PSV,6]];
    Leo> ########

    Leo> # R part
    Leo> library(fOptions)

    Leo> ###raw values for reproduction
    Leo> NN <- 58
    Leo> dd <- 0.411769
    Leo> lownum <- 20.81512
    Leo> upnum <- 6.741643
    Leo> sL <- 0.029
    Leo> sH <- 0.092
    Leo> ###

    Leo> integpsv <- function(s) { 1 / (s^NN) * exp(-dd / (2 * s^2)) *
    Leo>    ( (igamma((upnum/(2*s^2)),1/2) - igamma(0,1/2) ) +
    Leo>    (igamma((lownum/(2*s^2)),1/2) - igamma(0,1/2) ) )
    Leo> }
    Leo> psv <- integrate(integpsv, lower=sL, upper=sH)
    Leo> PSV <- psv$value / sqrt(2*NN)
    Leo> print("------------- Results ------------------------------------\n")
    Leo> print(paste("p(sv|D_1D_2I)   = const. ",PSV, sep=""))


    Leo> The results of variable "PSV" are not the same.

    Leo> In mathematica -> PSV ~ 2.67223e+47
    Leo> with rounding errors due to the initial values, in R -> PSV ~ 1.5e+47

    Leo> I am not that familiar with gamma functions and integration, thus I 
    Leo> assume there the source of the problem can be located.

Yes.
A few remarks

1) No need to use package "fOptions" and igamma(); 
   standard R's  pgamma() is all you need
   {igamma() has added value only for *complex* arguments!}

2) igamma(0, 1/2) == pgamma(0, 1/2) == 0 , so you can really
   drop them from your integrand.

integpsv <- function(s) { 
  1 / (s^NN) * exp(-dd / (2 * s^2)) *
  ( pgamma(upnum/(2*s^2), 1/2) + pgamma(lownum/(2*s^2), 1/2) )
}

3) But then the problem could really be with the algorithm used in
   integrate(), and indeed if you plot your integrand

      plot(integpsv, from= sL, to = sH)

   you see that indeed your integrand looks ``almost
   constant'' in the left half --- whereas that is actually not
   true but the range of the integrand varies so dramatically
   that it ``looks like'' constant 0 upto about x= .06.

   Something which   help(integrate)   warns about.

However, if I experiment, using integrate() in two parts, or using many other
numerical integration approximators,
all methods give ( your 'psv', not PSV )

    integrate(integpsv, lower=sL, upper=sH)

a value of   1.623779e+48   (which leads to your PSV of 1.5076e+47)

Could it be that you are not using the same definition of
incomplete gamma in Mathematica and R ?

Martin Maechler, ETH Zurich

    Leo> Thanks for helping me to adjust the sript.

    Leo> best wishes
    Leo> leo


From maechler at stat.math.ethz.ch  Tue Aug  8 10:05:21 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Aug 2006 10:05:21 +0200
Subject: [R] finding x values to meet a y
In-Reply-To: <1154990982.44d7c3867617e@webmail.bignet.com.br>
References: <1154990982.44d7c3867617e@webmail.bignet.com.br>
Message-ID: <17624.17857.883597.454624@stat.math.ethz.ch>

>>>>> "AOlinto" == Antonio Olinto <aolinto_r at bignet.com.br>
>>>>>     on Mon,  7 Aug 2006 19:49:42 -0300 writes:

    AOlinto> Hi,
    AOlinto> I'd like to find which values of x will give me a y.

    AOlinto> In other words, in the example of a gaussian curve, I want to find the values of
    AOlinto> x that will give me a density, let's say, of 0.02.

    AOlinto> curve(((1/(sqrt(2*pi)*10))*exp(-((x-50)^2)/(2*10^2))),xlim=c(0,100))

In other words, you are trying to  *invert a function*  
numerically, i.e.,

find  x  such that  f(x) = y0

Now, this is trivially the same as finding the "root" (or
"zero") of the function  f~(x) = f(x) - y0

==> Use *the* R root finding function :
    uniroot()   [or polyroot() if  f(.) is a polynomial]

Martin Maechler, ETH Zurich


From maechler at stat.math.ethz.ch  Tue Aug  8 10:23:48 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Aug 2006 10:23:48 +0200
Subject: [R] Constrain coefs. in linear model to sum to 0
In-Reply-To: <B998A44C8986644EA8029CFE6396A924547676@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924547676@exqld2-bne.qld.csiro.au>
Message-ID: <17624.18964.441669.688969@stat.math.ethz.ch>

>>>>> "BillV" ==   <Bill.Venables at csiro.au>
>>>>>     on Tue, 8 Aug 2006 15:52:20 +1000 writes:

    BillV> Gorjanc Gregor asks:

    >> Hello!
    >> 
    >> I would like to use constrain to sum coeficients of a
    >> factor to 0 instead of classical corner contraint i.e. I
    >> would like to fit a model like
    >> 
    >> lm(y ~ 1 + effectA + effectB)
    >> 
    >> and say get parameters
    >> 
    >> intercept
    >> effectA_1
    >> effectA_2
    >> effectB_1
    >> effectB_2
    >> effectB_3
    >> 
    >> where effectA_1 represents deviation of level A_1 from intercept and 
    >> sum(effectA_1, effectA_2) = 0 and the same for factor B.
    >> 
    >> Is this possible to do?

    BillV> Yes, but not quite as simply as you would like.  If you set

    BillV> options(contrasts = c("contr.sum", "contr.poly"))

    BillV> for example, then factor models are parametrised as
    BillV> you wish above, BUT you don't get all the effects
    BillV> directly

    BillV> In your case above, for example, if fm is the fitted
    BillV> model object, then

    BillV> coef(fm)

    BillV> Will give you intercept, effectA_2, effectB_2,
    BillV> effectB_3.  The remaining effects*_1 you will need to
    BillV> calculate as the negative of the sum of all the
    BillV> others.

    BillV> This gets a bit more complicated when you have
    BillV> crossed terms, a*b, but the same principle applies.

Further, note that there have been functions

    dummy.coef() 
and 
    model.tables()

that have been intended to do exactly this.
Unfortunately, these two functions only work correctly in "simpler"
cases (e.g., complete balanced designs, IIRC)

E.g. help(model.tables) has

 >> Warning:
 >> 
 >>      The implementation is incomplete, and only the simpler cases have
 >>      been tested thoroughly.

and  help(dummy.coef) 

  >> Details:
  >> 
  >>   [........]
  >> 
  >>      The method used has some limitations, and will give incomplete
  >>      results for terms such as 'poly(x, 2))'.  However, it is adequate
  >>      for its main purpose, 'aov' models.
  >> 
  >>   [........]
  >> 
  >> Warning:
  >> 
  >>      This function is intended for human inspection of the output: it
  >>      should not be used for calculations.  Use coded variables for all
  >>      calculations.
  >> 
  >>      The results differ from S for singular values, where S can be
  >>      incorrect.

When we last discussed this, IIRC,
we (some within R-core)  were waiting for interested (and
knowledgable) users to provide improved code for these functions.
Hey, if you want to become immortal in the R annals, now is your
chance! ;-)

    BillV> Bill Venables.

    >> 
    >> Lep pozdrav / With regards,
    >> Gregor Gorjanc
> [.........]

    >> ----------------------------------------------------------------------
    >> "One must learn by doing the thing; for though you think you know it,
    >> you have no certainty until you try." Sophocles ~ 450 B.C.

    BillV> Well, now's your chance!

indeed!

Martin Maechler, ETH Zurich


From jonne at troefpunt.nl  Tue Aug  8 10:25:59 2006
From: jonne at troefpunt.nl (Jonne)
Date: Tue, 08 Aug 2006 10:25:59 +0200
Subject: [R] OT: RBanking
Message-ID: <pan.2006.08.08.08.25.59.496978@troefpunt.nl>


Hi all,

This might be a little off-topic, but I wanted to let you all know
about a project for which I used R.
I started an (free for all, open source) offline banking tool, programmed
in R and using the tcltk library for the user interface. It has several
nice features already
:) It can use regular expressions to match/group bank transactions and can
obviously use R's power to plot nice graphs.

Personally, I found it too difficult to make nice graphs with a program
like gnucash (which seems more for experts instead of home-users??)

There is support for several Dutch banks at the moment (I don't know
about internationalization).
The program is not yet user-friendly and not trivial to install,
but if you are interested I invite you to try it.

It can be checked out with the command:
svn co https://svn.sourceforge.net/svnroot/rbanking/trunk rbanking
And the website is at http://rbanking.sourceforge.net
(thereafter you would need to install several dependencies like BWidgets
and tablelist.

It would be great if there's a group of people who would slowly like
to further develop this, like myself. Any help is appreciated :)

Jonne.


From ripley at stats.ox.ac.uk  Tue Aug  8 11:08:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Aug 2006 10:08:42 +0100 (BST)
Subject: [R] Source installation error: "gfortran and gcc disagree on
 int and double ...
In-Reply-To: <eb935b$ihp$1@sea.gmane.org>
References: <eb69j3$cmo$1@sea.gmane.org>
	<Pine.LNX.4.64.0608070821180.20401@gannet.stats.ox.ac.uk>
	<eb935b$ihp$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.64.0608081003260.23674@gannet.stats.ox.ac.uk>

First, you replied to the list and not to me, which was discourteous.

Your error does indeed appear to be an incorrectly installed compiler.

> conftestf.o: In function `cftest_':
> /home/mike/Desktop/R-2.3.1/conftestf.f:9: undefined reference to
> `_gfortran_pow_r8_i4'

this is in -lgfortran, so that is not being found or is broken.
One possibility is a missing symlink that is in a -devel RPM.

This is not an R issue.

On Mon, 7 Aug 2006, Mike wrote:

> Prof Brian Ripley wrote:
> 
> > First, you do not need -fPIC: it is the same as -fpic which R selects on
> > your platform.
> > 
> 
> Right, I commented the flags and configure goes just as far
> 
> 
> > Second, please look at config.log to find the exact problem: it well be
> > that your compilers are not properly installed (as was the case in the
> > reference you quote below).
> 
> from config.log it seem that there are a few missing inclides and syntax
> errors in confdefs.h. (I can quote the specifics, if necessary). The last
> lines before the error message are:
> 
> configure:27770: checking whether gfortran and gcc agree on int and double
> conftest.c: In function 'main':
> conftest.c:28: warning: implicit declaration of function 'printf'
> conftest.c:28: warning: incompatible implicit declaration of built-in
> function 'printf'
> conftest.c:29: warning: implicit declaration of function 'exit'
> conftest.c:29: warning: incompatible implicit declaration of built-in
> function 'exit'
> conftestf.o: In function `cftest_':
> /home/mike/Desktop/R-2.3.1/conftestf.f:9: undefined reference to
> `_gfortran_pow_r8_i4'
> collect2: ld returned 1 exit status
> configure:27848: WARNING: gfortran and gcc disagree on int and double
> configure:27850: error: Maybe change CFLAGS or FFLAGS?
> 
> > 
> > Finally, your compilers are pretty obselete (there have been 4.0.2, 4.0.3,
> > 4.1.0 and 4.1.1), so you should be updating them.
> > 
> Changing compiler is not an option for me. Could you advise me how to
> configure the flags?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.dalgaard at biostat.ku.dk  Tue Aug  8 11:42:51 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Aug 2006 11:42:51 +0200
Subject: [R] Netiquette, was Re:  ... "gfortran and gcc...
In-Reply-To: <Pine.LNX.4.64.0608081003260.23674@gannet.stats.ox.ac.uk>
References: <eb69j3$cmo$1@sea.gmane.org>
	<Pine.LNX.4.64.0608070821180.20401@gannet.stats.ox.ac.uk>
	<eb935b$ihp$1@sea.gmane.org>
	<Pine.LNX.4.64.0608081003260.23674@gannet.stats.ox.ac.uk>
Message-ID: <x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> First, you replied to the list and not to me, which was discourteous.

You mean that he replied to the list *only*, I hope. 

I usually consider it offensive when people reply to me and not the
list (reasons including: It feels like being grabbed by the sleeve, I
might not actually be the best source for the answer, and it's
withholding the answer from the rest of the subscribers.)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Tue Aug  8 11:47:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Aug 2006 10:47:49 +0100 (BST)
Subject: [R] Netiquette, was Re:  ... "gfortran and gcc...
In-Reply-To: <x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>
References: <eb69j3$cmo$1@sea.gmane.org>
	<Pine.LNX.4.64.0608070821180.20401@gannet.stats.ox.ac.uk>
	<eb935b$ihp$1@sea.gmane.org>
	<Pine.LNX.4.64.0608081003260.23674@gannet.stats.ox.ac.uk>
	<x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>

On Tue, 8 Aug 2006, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > First, you replied to the list and not to me, which was discourteous.
> 
> You mean that he replied to the list *only*, I hope.

Yes, and it was written as if to me, and was a reply to an email from me.

> I usually consider it offensive when people reply to me and not the
> list (reasons including: It feels like being grabbed by the sleeve, I
> might not actually be the best source for the answer, and it's
> withholding the answer from the rest of the subscribers.)

We do ask people to copy to the list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From henrik.parn at bio.ntnu.no  Tue Aug  8 12:11:40 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Tue, 08 Aug 2006 12:11:40 +0200
Subject: [R] fixed effects following lmer and mcmcsamp - which to present?
Message-ID: <44D8635C.4090808@bio.ntnu.no>

Dear all,

I am running a mixed model using lmer. In order to obtain CI of 
individual coefficients I use mcmcsamp. However, I need advice which 
values that are most appropriate to present in result section of a 
paper. I have not used mixed models and lmer so much before so my 
question is probably very naive. However, to avoid to much problems with 
journal editors and referees addicted to p-values, I would appreciate 
advice of which values of the output for the fixed factor that would be 
most appropriate to present in a result section, in order to convince 
them of the p-value free 'lmer-mcmcsamp'-approach!

Using the example from the help page on lmer:

fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)

...I obtain the following for 'Days':


summary(fm1)
...
            Estimate Std. Error  t value

Days         10.4673     1.5458    6.771


...and from mcmcsamp:

summary(mcmcsamp(fm1 , n = 10000))

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

                        Mean     SD Naive SE Time-series SE
Days                 10.4695 1.7354 0.017354       0.015921

2. Quantiles for each variable:
                        2.5%       25%       50%       75%    97.5%
Days                  7.0227    9.3395   10.4712   11.5719   13.957



The standard way of presenting coefficients following a 'non-lmer' 
output is often (beta=..., SE=..., statistic=..., P=...). What would be 
the best equivalent in a 'lmer-mcmcsamp-context'? (beta=..., CI=...) is 
a good start I believe. But which beta? And what else?

I assume that the a 95% CI in this case would be 7.0227-13.957 (please, 
do correct me I have completely misunderstood!). But which would be the 
corresponding beta? 10.4673?, 10.4695? 10.4712? Is the t-value worth 
presenting or is it 'useless' without corresponding degrees of freedom 
and P-value? If I present the mcmcsamp-CI, does it make sense to present 
any of the three SE obtained in the output above? BTW, I have no idea 
what Naive SE, Time-series SE means. Could not find much in help and 
pdfs to coda or Matrix, or in Google.

Thanks in advance for any advice and hints to help-texts I have missed!


Best regards,

Henrik


From pj.bell at yahoo.co.uk  Tue Aug  8 12:33:13 2006
From: pj.bell at yahoo.co.uk (Piet Bell)
Date: Tue, 8 Aug 2006 11:33:13 +0100 (BST)
Subject: [R] gamm question
Message-ID: <20060808103313.43924.qmail@web27609.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060808/af7432e2/attachment.pl 

From farewelld at Cardiff.ac.uk  Tue Aug  8 11:58:12 2006
From: farewelld at Cardiff.ac.uk (Daniel Farewell)
Date: Tue, 08 Aug 2006 10:58:12 +0100
Subject: [R] fixed effects constant in mcmcsamp
Message-ID: <44D86E45020000B6000071AA@ZGRW50.cf.ac.uk>

I'm fitting a GLMM to some questionnaire data. The structure is J individuals,
nested within I areas, all of whom answer the same K (ordinal) questions. The
model I'm using is based on so-called continuation ratios, so that it can be
fitted using the lme4 package.

The lmer function fits the model just fine, but using mcmcsamp to judge the
variability of the parameter estimates produces some strange results. The
posterior sample is constant for the fixed effects, and the estimates of the
variance components are way out in the tails of their posterior samples.

The model I'm using says (for l = 1, ..., L - 1)

logit P(X[ijk] = l | X[ijk] >= l, U[i], V[j], W[k]) = U[i] + V[j] + W[k] + a[l]

where X[ijk] is the ordinal response to question k for individual j in area i.
The U, V, and W are random effects and the a's are fixed effects. Here's a
function to simulate data which mimics this setup (with a sequence of binary
responses Y[ijkl] = 1 iff X[ijk] = l):

sim <- function(n = c(10, 10, 10), sd = c(0.5, 2, 0.5), a = seq(-5, 1, 2)) {

 u <- rnorm(n[1], 0, sd[1])
 v <- rnorm(prod(n[1:2]), 0, sd[2])
 w <- rnorm(n[3], 0, sd[3])

 i <- factor(rep(1:n[1], each = prod(n[2:3])))
 j <- factor(rep(1:prod(n[1:2]), each = n[3]))
 k <- factor(rep(1:n[3], prod(n[1:2])))

 df <- NULL

 for(l in 1:length(a)) {
    
  y <- rbinom(length(i), 1, plogis(u[i] + v[j] + w[k] + a[l]))
  df <- rbind(df, data.frame(i, j, k, l, y))

  i <- i[!y]
  j <- j[!y]
  k <- k[!y]

 }

 df$l <- factor(df$l)
 return(df)
 
}

And here's a function which shows the difficulties I've been having:

test <- function(seed = 10, ...) {

 require(lme4)
 set.seed(seed)

 df <- sim(...)
 df.lmer <- lmer(y ~ 0 + l + (1 | i) + (1 | j) + (1 | k), family = binomial,
data = df) 
 df.mcmc <- mcmcsamp(df.lmer, 1000, trans = FALSE)

 print(summary(df.lmer))
 print(summary(df.mcmc))
 densityplot(~ as.numeric(df.mcmc) | factor(rep(colnames(df.mcmc), each =
1000)), scales = list(relation = "free"))

}

Running

test()

gives the following:

Generalized linear mixed model fit using PQL
Formula: y ~ 0 + l + (1 | i) + (1 | j) + (1 | k)
Data: df
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 2316.133 2359.166 -1151.066 2302.133
Random effects:
 Groups Name        Variance Std.Dev.
 j      (Intercept) 4.15914  2.03940
 k      (Intercept) 0.25587  0.50584
 i      (Intercept) 0.56962  0.75473
number of obs: 3455, groups: j, 100; k, 10; i, 10

Estimated scale (compare to 1)  0.8747598

Fixed effects:
   Estimate Std. Error  z value  Pr(>|z|)
l1 -4.50234    0.40697 -11.0632 < 2.2e-16 ***
l2 -3.27643    0.38177  -8.5821 < 2.2e-16 ***
l3 -1.05277    0.36566  -2.8791  0.003988 **
l4  0.76538    0.36832   2.0780  0.037706 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
   l1    l2    l3
l2 0.843
l3 0.841 0.900
l4 0.805 0.865 0.921
       l1               l2               l3               l4
 Min.   :-4.502   Min.   :-3.276   Min.   :-1.053   Min.   :0.7654
 1st Qu.:-4.502   1st Qu.:-3.276   1st Qu.:-1.053   1st Qu.:0.7654
 Median :-4.502   Median :-3.276   Median :-1.053   Median :0.7654
 Mean   :-4.502   Mean   :-3.276   Mean   :-1.053   Mean   :0.7654
 3rd Qu.:-4.502   3rd Qu.:-3.276   3rd Qu.:-1.053   3rd Qu.:0.7654
 Max.   :-4.502   Max.   :-3.276   Max.   :-1.053   Max.   :0.7654
     j.(In)          k.(In)           i.(In)           deviance
 Min.   :1.911   Min.   :0.0509   Min.   :0.06223   Min.   :2011
 1st Qu.:2.549   1st Qu.:0.1310   1st Qu.:0.19550   1st Qu.:2044
 Median :2.789   Median :0.1756   Median :0.25581   Median :2054
 Mean   :2.824   Mean   :0.2085   Mean   :0.29948   Mean   :2054
 3rd Qu.:3.070   3rd Qu.:0.2463   3rd Qu.:0.34640   3rd Qu.:2064
 Max.   :4.615   Max.   :0.8804   Max.   :3.62168   Max.   :2107

As you can see, the posterior samples from the fixed effects are constant (at
the inital estimates) and the estimates of the variance components aren't within
the IQ ranges of their posterior samples.

I understand from various posts that mcmcsamp is still a work in progress, and
may not work on every model. Is this one of those cases? I'm using R 2.3.1 and
lme4 0.995-2 on Windows XP.

Daniel Farewell
Cardiff University


From r-help at botelho-machado.de  Tue Aug  8 12:59:50 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Tue, 08 Aug 2006 12:59:50 +0200
Subject: [R] How to convert list elements to data.frames or vectors?
Message-ID: <44D86EA6.3060300@botelho-machado.de>

Dear R mailing-list comunity!



I'm currently trying to implement an R method. I have two sets of data
that I convert into a data.frame each. These data.frames I'd like to
append to a list:

# generate a list
listTable<-list()

# add one set of data
x<-1000 ;y<-1 ;listTable[[length(listTable) + 1]] <-
data.frame(matrix(rnorm(x*y), nrow=y)); rm(x); rm(y)

# add another set of data (same command)
x<-1000 ;y<-1 ;listTable[[length(listTable) + 1]] <-
data.frame(matrix(rnorm(x*y), nrow=y)); rm(x); rm(y)

My objective is to performed some hypothesis tests on the data. To test
if that works out correctly, I tried first using an unpaired t-test
(therfore the data.frames consist only of one row each in the example).

# alternative, var.equal and conf.level shall
# be arguments of my method as well (alternative="two.sided",
# var.equal=TRUE, conf.level=0.95)
t.test(listTable[[1]][1,], listTable[[2]][1,], alternative=alternative,
paired=FALSE, var.equal=var.equal, conf.level=conf.level)

And an F-test, throwing an error, like there were not enough
observations in the x vector of the test's input.

# The F-test (ratio=1, alternative="two.sided", conf.level=0.95)
var.test(listTable[[1]][1,], listTable[[2]][1,], ratio=ratio,
alternative=alternative, conf.level=conf.level)

I figured out, those tests work perfectly, using vectors instead of my
list elements with the same argument values, hence there should be no
problem with the parameters, I guess.

So, my problems would be the list elements like "listTable[[1]][1,]".
They are no vectors but "list"s themselves containing each only one
element?! I tried several things without any success to change that.
I need to have a list like structure and couldn't find a way how to
convert the list elements back to data.frames or vectors.


Thus I now have a bunch of basic questions on R:

1. If I put a data.frame into a list, how can I get it back as data.frame?

2. How can I get a single row of a data.frame, stored in a list, as
vector and not as list of elements?

3. Is a "list" at all the correct structure for my deeds?

4. Why is this only a problem for the F-test and it seems to be no
problem for the t-test?


Regards and TIA,
Lothar Rubusch


From gyadav at ccilindia.co.in  Tue Aug  8 13:04:35 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Tue, 8 Aug 2006 16:34:35 +0530
Subject: [R] Spline Extrapolation(NURBS)
In-Reply-To: <A684141C-2564-41B8-B9E8-5A1F437E400B@tin.it>
Message-ID: <OFCC229FB8.0A3BE570-ON652571C4.003C8FD7-652571AA.003D0094@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060808/b8e03d94/attachment.pl 

From r-help at botelho-machado.de  Tue Aug  8 13:51:48 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Tue, 08 Aug 2006 13:51:48 +0200
Subject: [R] How to convert list elements to data.frames or vectors?
In-Reply-To: <44D874D7.6030905@pburns.seanet.com>
References: <44D86EA6.3060300@botelho-machado.de>
	<44D874D7.6030905@pburns.seanet.com>
Message-ID: <44D87AD4.1020108@botelho-machado.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Patrick Burns wrote:
> Chapter 1 of S Poetry might help you come to
> grips with data structures in R.
> 
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Lothar Botelho-Machado wrote:
> 
>> Dear R mailing-list comunity!
>>
>>
>>
>> I'm currently trying to implement an R method. I have two sets of data
>> that I convert into a data.frame each. These data.frames I'd like to
>> append to a list:
>>
>> # generate a list
>> listTable<-list()
>>
>> # add one set of data
>> x<-1000 ;y<-1 ;listTable[[length(listTable) + 1]] <-
>> data.frame(matrix(rnorm(x*y), nrow=y)); rm(x); rm(y)
>>
>> # add another set of data (same command)
>> x<-1000 ;y<-1 ;listTable[[length(listTable) + 1]] <-
>> data.frame(matrix(rnorm(x*y), nrow=y)); rm(x); rm(y)
>>
>> My objective is to performed some hypothesis tests on the data. To test
>> if that works out correctly, I tried first using an unpaired t-test
>> (therfore the data.frames consist only of one row each in the example).
>>
>> # alternative, var.equal and conf.level shall
>> # be arguments of my method as well (alternative="two.sided",
>> # var.equal=TRUE, conf.level=0.95)
>> t.test(listTable[[1]][1,], listTable[[2]][1,], alternative=alternative,
>> paired=FALSE, var.equal=var.equal, conf.level=conf.level)
>>
>> And an F-test, throwing an error, like there were not enough
>> observations in the x vector of the test's input.
>>
>> # The F-test (ratio=1, alternative="two.sided", conf.level=0.95)
>> var.test(listTable[[1]][1,], listTable[[2]][1,], ratio=ratio,
>> alternative=alternative, conf.level=conf.level)
>>
>> I figured out, those tests work perfectly, using vectors instead of my
>> list elements with the same argument values, hence there should be no
>> problem with the parameters, I guess.
>>
>> So, my problems would be the list elements like "listTable[[1]][1,]".
>> They are no vectors but "list"s themselves containing each only one
>> element?! I tried several things without any success to change that.
>> I need to have a list like structure and couldn't find a way how to
>> convert the list elements back to data.frames or vectors.
>>
>>
>> Thus I now have a bunch of basic questions on R:
>>
>> 1. If I put a data.frame into a list, how can I get it back as
>> data.frame?
>>
>> 2. How can I get a single row of a data.frame, stored in a list, as
>> vector and not as list of elements?
>>
>> 3. Is a "list" at all the correct structure for my deeds?
>>
>> 4. Why is this only a problem for the F-test and it seems to be no
>> problem for the t-test?
>>
>>
>> Regards and TIA,
>> Lothar Rubusch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>
> 


Thank you!

I was rather concentrated in finding something especially for "R" and
didn't search for "S" as well. I'll have a look in "S Poetry" now, It
seems like that's exactly the thing I was looking for, the last days!

Great!
 Lothar Rubusch
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFE2HrUHRf7N9c+X7sRAvEeAJwPQTjDm0qmtz15mWJIPEmF5atIzQCfa0kC
YyzgDhUMoeXdeJ4LzRHWTc4=
=MIq/
-----END PGP SIGNATURE-----


From maechler at stat.math.ethz.ch  Tue Aug  8 13:53:55 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Aug 2006 13:53:55 +0200
Subject: [R] Incomplete gamma function (was  "integrate()",
	{was "mathematica -> r ..."})
In-Reply-To: <17624.17286.610035.45098@stat.math.ethz.ch>
References: <44D7BAFF.2030305@anicca-vijja.de>
	<17624.17286.610035.45098@stat.math.ethz.ch>
Message-ID: <17624.31571.865663.700495@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 8 Aug 2006 09:55:50 +0200 writes:

 >>>>> "Leo" == Leo G?rtler <leog at anicca-vijja.de>
 >>>>>     on Tue, 08 Aug 2006 00:13:19 +0200 writes:

     Leo> Dear R-list,
     Leo> I try to transform a mathematica script to R.

     Leo> #######relevant part of the Mathematica script
     Leo> (* p_sv *)
     Leo> dd = NN (DsD - DD^2);
     Leo> lownum = NN (L-DD)^2;
     Leo> upnum  = NN (H-DD)^2;
     Leo> low = lownum/(2s^2);
     Leo> up  = upnum/(2s^2);
     Leo> psv = NIntegrate[1/(s^NN) Exp[-dd/(2s^2)]
     Leo>    (Gamma[1/2,0,up] + Gamma[1/2,0,low]),{s,sL,sH}, MinRecursion-> 3];
     Leo>    PSV = psv/Sqrt[2NN];
     Leo> Print["------------- Results ------------------------------------"];
     Leo> Print[" "];
     Leo> Print["p(sv|D_1D_2I)   = const. ",N[PSV,6]];
     Leo> ########

     Leo> # R part
     Leo> library(fOptions)

     Leo> ###raw values for reproduction
     Leo> NN <- 58
     Leo> dd <- 0.411769
     Leo> lownum <- 20.81512
     Leo> upnum <- 6.741643
     Leo> sL <- 0.029
     Leo> sH <- 0.092
     Leo> ###

     Leo> integpsv <- function(s) { 1 / (s^NN) * exp(-dd / (2 * s^2)) *
     Leo>    ( (igamma((upnum/(2*s^2)),1/2) - igamma(0,1/2) ) +
     Leo>    (igamma((lownum/(2*s^2)),1/2) - igamma(0,1/2) ) )
     Leo> }
     Leo> psv <- integrate(integpsv, lower=sL, upper=sH)
     Leo> PSV <- psv$value / sqrt(2*NN)
     Leo> print("------------- Results ------------------------------------\n")
     Leo> print(paste("p(sv|D_1D_2I)   = const. ",PSV, sep=""))


     Leo> The results of variable "PSV" are not the same.

     Leo> In mathematica -> PSV ~ 2.67223e+47
     Leo> with rounding errors due to the initial values, in R -> PSV ~ 1.5e+47

     Leo> I am not that familiar with gamma functions and integration, thus I 
     Leo> assume there the source of the problem can be located.

    MM> Yes.
    MM> A few remarks

    MM> 1) No need to use package "fOptions" and igamma(); 
    MM> standard R's  pgamma() is all you need
    MM> {igamma() has added value only for *complex* arguments!}

    MM> 2) igamma(0, 1/2) == pgamma(0, 1/2) == 0 , so you can really
    MM> drop them from your integrand.

    MM> integpsv <- function(s) { 
    MM>    1 / (s^NN) * exp(-dd / (2 * s^2)) *
    MM>    ( pgamma(upnum/(2*s^2), 1/2) + pgamma(lownum/(2*s^2), 1/2) )
    MM> }

    
    [............]

    MM> However, if I experiment, using integrate() in two parts, or using many other
    MM> numerical integration approximators,
    MM> all methods give ( your 'psv', not PSV )

    MM> integrate(integpsv, lower=sL, upper=sH)

    MM> a value of   1.623779e+48   (which leads to your PSV of 1.5076e+47)

    MM> Could it be that you are not using the same definition of
    MM> incomplete gamma in Mathematica and R ?

Offlist, Leo sent me Mathematica's definition
of
   Gamma[a, z0, z1]  :=  integral_z0^z1 t^(a-1) exp(-t) dt

Now if you compare this with what  ?pgamma (not ?gamma !) tells you,
namely that R uses (Abramowitz and Stegun's definition of the
incomplete gamma function) 

   pgamma(x, a) =  1/ Gamma(a) * integral_0^x  t^(a-1) exp(-t) dt

which has a normalizing factor:  In your case above, it is
Gamma(1/2) with its well-known value of sqrt(pi).

And indeed, if you multiply the current result by sqrt(pi), you
get what you want -- and did get from Mathematica:

> (1.623779e+48 / sqrt(2*NN)) * sqrt(pi)
[1] 2.672224e+47


Regards,
Martin Maechler, ETH Zurich


From mikeumo at sbcglobal.net  Tue Aug  8 15:09:44 2006
From: mikeumo at sbcglobal.net (Mike)
Date: Tue, 8 Aug 2006 08:09:44 -0500
Subject: [R] Netiquette, was Re:  ... "gfortran and gcc...
In-Reply-To: <Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
References: <eb69j3$cmo$1@sea.gmane.org> <x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
Message-ID: <200608080809.44967.mikeumo@sbcglobal.net>

Thank you both.

I would prefer to communicate through the list only.

Mike.

On Tue August 8 2006 04:47, Prof Brian Ripley wrote:
> On Tue, 8 Aug 2006, Peter Dalgaard wrote:
> > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> > > First, you replied to the list and not to me, which was discourteous.
> >
> > You mean that he replied to the list *only*, I hope.
>
> Yes, and it was written as if to me, and was a reply to an email from me.
>
> > I usually consider it offensive when people reply to me and not the
> > list (reasons including: It feels like being grabbed by the sleeve, I
> > might not actually be the best source for the answer, and it's
> > withholding the answer from the rest of the subscribers.)
>
> We do ask people to copy to the list.


From miczat at yahoo.com  Tue Aug  8 16:00:50 2006
From: miczat at yahoo.com (Michael Zatorsky)
Date: Wed, 9 Aug 2006 00:00:50 +1000 (EST)
Subject: [R] Frequency Distribution
Message-ID: <20060808140050.59187.qmail@web51108.mail.yahoo.com>

Hi,

Could someone please suggest where I might find some
instructions / tutorials / FAQs that describe how to 
create a frequency distribution and cumulative
frequency distribution in R using different class
withs.

I have about a 2-million observations (distances
between points ranging from sub-millimetre to about
400km, and I want to get a feel for how they are
distributed).

I'd like the output as a table / data rather than an
graph.

I've searched Google and R's help for obvious terms,
and while I've found much information on
graphing/plotting, I haven't hit on anything for this.

(I only downloaded R about 2 hours ago, apologies if
this is obviously documented somewhere I missed.)

Regards
Michael.

Send instant messages to your online friends http://au.messenger.yahoo.com


From petr.pikal at precheza.cz  Tue Aug  8 16:18:15 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 08 Aug 2006 16:18:15 +0200
Subject: [R] How to export data to Excel Spreadsheet?
In-Reply-To: <001401c6ba32$288ae440$8564a8c0@gne.windows.gene.com>
References: <6ade6f6c0608070523u2e3c69aeq6168120051354600@mail.gmail.com>
Message-ID: <44D8B947.30838.1C4198C@localhost>

Hi

On 7 Aug 2006 at 8:00, Berton Gunter wrote:

From:           	Berton Gunter <gunter.berton at gene.com>
To:             	"'Paul Smith'" <phhs80 at gmail.com>,
	"'R-Help'" <R-help at stat.math.ethz.ch>
Date sent:      	Mon, 7 Aug 2006 08:00:00 -0700
Organization:   	Genentech Inc.
Subject:        	Re: [R] How to export data to Excel Spreadsheet?

> You can also usually copy and paste to/from the Windows clipboard by
> using file='clipboard' in file i/o or via description = 'clipboard'
> using connections. I haven't checked all details of this, so there may
> be some glitches.  

No problem

write.excel<-function(tab, ...) write.table( tab, "clipboard", 
sep="\t", row.names=F)

works, at least with my version of Excel. Of course after Ctrl-Ving 
in Excel

HTH
Petr


> 
> -- Bert Gunter
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Monday, August 07, 2006 5:23 AM To: R-Help Subject: Re: [R] How
> to export data to Excel Spreadsheet?
> 
> On 8/7/06, Xin <jasonshi510 at hotmail.com> wrote:
> >    I try to export my output's data to Excel spreadsheet. My outputs
> >    are:
> >
> >  >comb3
> >        [,1] [,2] [,3]
> >   [1,] "a"  "b"  "c"
> >   [2,] "a"  "b"  "d"
> >   [3,] "a"  "b"  "e"
> >   [4,] "a"  "b"  "f"
> >   [5,] "a"  "b"  "g"
> 
> See
> 
> ? write.table
> ? write.csv
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Tue Aug  8 16:32:28 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 08 Aug 2006 16:32:28 +0200
Subject: [R] (Fwd) Re:  paired t-test. Need to rearrange data?
Message-ID: <44D8BC9C.1680.1D11EAD@localhost>


------- Forwarded message follows -------
From:           	Petr Pikal <petr.pikal at precheza.cz>
To:             	Henrik Parn <henrik.parn at bio.ntnu.no>
Subject:        	Re: [R] paired t-test. Need to rearrange data?
Date sent:      	Tue, 08 Aug 2006 16:13:47 +0200

Hi

Uff, it takes me a bit headache but this shall do it
?unstack
?table
?t

new.list<-unstack(test.data,y~id)
new.test.data<-t(as.data.frame(new.list[table(test.data$id)>1]))
# if you have more than 2 catches you need to modify this^^^
t.test(new.test.data[,1], new.test.data[,2], paired=T)

HTH
Petr



On 7 Aug 2006 at 16:43, Henrik Parn wrote:

Date sent:      	Mon, 07 Aug 2006 16:43:45 +0200
From:           	Henrik Parn <henrik.parn at bio.ntnu.no>
Send reply to:  	henrik.parn at bio.ntnu.no
Organization:   	NTNU
To:             	Petr Pikal <petr.pikal at precheza.cz>
Subject:        	Re: [R] paired t-test. Need to rearrange data?

> Dear Peter,
> 
> Thanks a lot for your rapid answer! I am afraid that I presented an
> example data set in my previous question that was  to nicely
> organised. In fact it more looked like the data I want to acheive
> rather than the data I recieved...
> 
> So,may I bother you with a follow-up question:
> 
> I have data on morphology on birds from several different years.
> Each year about 50-100 birds are captured, marked with a unique id
> and measured. The recapture rate is very low and on average 1-2
> birds are recaptured the subsequent year. Again a small example data
> set, which hopefully is more realistic:
> 
> ####
> year <- as.factor(rep(1:4,each= 5)) # the four study years
> id <- c("a", "b", "c", "d", "e",  "b", "f", "g", "h", "a",  "i",
> "g", "j", "k", "l",  "j", "m", "n", "l", "o") # id of the birds
> captured y <- rnorm(20) # the measure taken test.data <-
> data.frame(year, id, y)
> ####
> 
> 
> So, in this small example, bird "a" and "b" is captured on year 1
> and recaptured year 2, bird "g" captured on year 2 and 3, and so on.
> On birds that are captured more than once, I would like to do a
> paired t.test where I compare the measure taken at first capture
> with that taken on the second time.
> 
> I suppose I need to add a vector indicating whether it is the first
> or second time a bird is captured:
> 
> ####
> test.data$time <- ifelse(duplicated(test.data$id), 2, 1)
> ####
> 
> But then I am stuck...Do you have a hint of how to proceed? How to
> select the y-values for which I have both a capture and a recapture?
> 
> Thanks in advance for taking your time!
> 
> Best regards,
> Henrik
> 
> Petr Pikal wrote:
> 
> >Hi
> >
> >one possibility is
> >
> >t(unstack(test.data, y~id))
> >
> >HTH
> >Petr
> >
> >On 6 Aug 2006 at 16:13, Henrik Parn wrote:
> >
> >Date sent:      	Sun, 06 Aug 2006 16:13:29 +0200
> >From:           	Henrik Parn <henrik.parn at bio.ntnu.no>
> >Organization:   	NTNU
> >To:             	R-help <r-help at stat.math.ethz.ch>
> >Subject:        	[R] paired t-test. Need to rearrange data?
> >Send reply to:  	henrik.parn at bio.ntnu.no
> >	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> >	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
> >
> >  
> >
> >>Dear all,
> >>
> >>I have received some data arranged like this:
> >>
> >># original data
> >>id <- rep(letters[1:6], each=2)
> >>time <- as.factor(rep(1:2, 6))
> >>y <- as.vector(replicate(6, c(rnorm(n=1, mean=1), rnorm(n=1,
> >>mean=2)))) test.data <- data.frame(id, time, y) test.data
> >>
> >>I would like to perform a paired t-test of the y-values at time=1
> >>against those at time=2, with samples paired by their id. Is it
> >>necessary to arrange the data in a format like this:   
> >>
> >># rearranged data
> >>id <- letters[1:6]
> >>y1 <- replicate(6, rnorm(n=1, mean=1)) # y-value at time = 1
> >>y2 <- replicate(6, rnorm(n=1, mean=2)) #  y-value at time = 2
> >>test.data2 <- data.frame(id, y1, y2) test.data2
> >>
> >>...in order to perform a paired t-test?
> >>t.test(y1, y2, paired=T)
> >>
> >>If yes, which is the most convenient way to rearrange the data? Or
> >>is it possible to apply the paired t-test function to the original
> >>data set?
> >>
> >>And a side question: In my examples, I suppose can I use set.seed
> >>to reproduce the 'rnorm-values' created in the 'original data'
> >>also in my the 'rearranged data'. Can someone give me a hint of
> >>how to apply the same 'seed' to all the rnorms?
> >>
> >>
> >>Thanks a lot in advance!
> >>
> >>
> >>Henrik
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html and provide commented,
> >>minimal, self-contained, reproducible code.
> >>    
> >>
> >
> >Petr Pikal
> >petr.pikal at precheza.cz
> >
> >  
> >
> 
> -- 
> ************************
> Henrik P?rn
> Department of Biology
> NTNU
> 7491 Trondheim
> Norway
> 
> +47 735 96282 (office)
> +47 909 89 255 (mobile)
> +47 735 96100 (fax)
> ************************
> 


------- End of forwarded message -------Petr Pikal
petr.pikal at precheza.cz


From jen at xlsolutions-corp.com  Tue Aug  8 17:21:11 2006
From: jen at xlsolutions-corp.com (jen at xlsolutions-corp.com)
Date: Tue, 08 Aug 2006 08:21:11 -0700
Subject: [R] Call for Beta Testers: R+ (read R plus) for Solaris and Linux:
Message-ID: <20060808082111.61cf36fc0b73de5f0c7f07b3effe7558.54baf86559.wbe@email.secureserver.net>

Code-named R+Team, our commercially supported R group is looking for
beta testers for R+ on  Solaris and R+ on Linux. We'd love to get your
feedback for our  R+Professional version that has additional
funcationalities for enterprise support.

The sooner we get your feedback and inputs, the faster we'll make
changes for the final release!

To participate in our beta test program, please email
dsmith at xlsolutions-corp.com or go online
www.xlsolutions-corp.com/contact.htm

The windows (Vista) version of R+ is also into development with a great
graphical user interface, and we'll be calling for beta testers this
fall or after Vista release. If you want to beta test the vista
version, please email Jennifer McDonald, jen at xlsolutions-corp.com.

We look forward to hearing your comments and inputs on R+ ... please
feel free to suggest a final name for our commercially supported R.

Drew Smith
R+ Group Manager
206 686 1578
dsmith at xlsolutions-corp.com
www.xlsolutions-corp.com


From sonal at deepfoo.com  Tue Aug  8 17:43:24 2006
From: sonal at deepfoo.com (sonal at deepfoo.com)
Date: Tue, 8 Aug 2006 11:43:24 -0400
Subject: [R] More Plots
Message-ID: <20060808154114.M9520@deepfoo.com>

Hi,

How can we plot two graphs ex. lets say correlation & ratio in the same 
window?

I mean in the window I have :

1. Graph of correlation having X & Y axes

2. Graph of ratio having A & B axes

one above the other.

Thanks,
Sonal


From bates at stat.wisc.edu  Tue Aug  8 17:45:56 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 8 Aug 2006 10:45:56 -0500
Subject: [R] fixed effects following lmer and mcmcsamp - which to
	present?
In-Reply-To: <44D8635C.4090808@bio.ntnu.no>
References: <44D8635C.4090808@bio.ntnu.no>
Message-ID: <40e66e0b0608080845g7a4e6a6dx47b046b6d8e05f75@mail.gmail.com>

On 8/8/06, Henrik Parn <henrik.parn at bio.ntnu.no> wrote:
> Dear all,
>
> I am running a mixed model using lmer. In order to obtain CI of
> individual coefficients I use mcmcsamp. However, I need advice which
> values that are most appropriate to present in result section of a
> paper. I have not used mixed models and lmer so much before so my
> question is probably very naive. However, to avoid to much problems with
> journal editors and referees addicted to p-values, I would appreciate
> advice of which values of the output for the fixed factor that would be
> most appropriate to present in a result section, in order to convince
> them of the p-value free 'lmer-mcmcsamp'-approach!
>
> Using the example from the help page on lmer:
>
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>
> ...I obtain the following for 'Days':
>
>
> summary(fm1)
> ...
>             Estimate Std. Error  t value
>
> Days         10.4673     1.5458    6.771
>
>
> ...and from mcmcsamp:
>
> summary(mcmcsamp(fm1 , n = 10000))
>
> 1. Empirical mean and standard deviation for each variable,
>    plus standard error of the mean:
>
>                         Mean     SD Naive SE Time-series SE
> Days                 10.4695 1.7354 0.017354       0.015921
>
> 2. Quantiles for each variable:
>                         2.5%       25%       50%       75%    97.5%
> Days                  7.0227    9.3395   10.4712   11.5719   13.957
>
>
>
> The standard way of presenting coefficients following a 'non-lmer'
> output is often (beta=..., SE=..., statistic=..., P=...). What would be
> the best equivalent in a 'lmer-mcmcsamp-context'? (beta=..., CI=...) is
> a good start I believe. But which beta? And what else?
>
> I assume that the a 95% CI in this case would be 7.0227-13.957 (please,
> do correct me I have completely misunderstood!). But which would be the
> corresponding beta? 10.4673?, 10.4695? 10.4712? Is the t-value worth
> presenting or is it 'useless' without corresponding degrees of freedom
> and P-value? If I present the mcmcsamp-CI, does it make sense to present
> any of the three SE obtained in the output above? BTW, I have no idea
> what Naive SE, Time-series SE means. Could not find much in help and
> pdfs to coda or Matrix, or in Google.

I would definitely report the REML value 10.4673 as the estimate.
This is a reproducible estimate - the other two are stochastic in that
you will get different values from different mcmcsamp runs.  However,
the reproducibility is high.  Notice that they all round to 10.47 and
two decimal places is quite enough precision for reporting this value
when you consider that  a 95% confidence interval is  [7.02,13.96].

Another way of evaluating a confidence interval using the coda package
is the HPDinterval function.  (HPD stands for "Highest Posterior
Density")  It returns the shortest interval with a 95% probability
content in the empirical distribution.

Here are values from a sample that I evaluated

> data(sleepstudy)
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> fs1 <- mcmcsamp(fm1, 10000)
> library(coda)
> summary(fs1)
...
1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

                        Mean     SD Naive SE Time-series SE
Days                 10.4810 1.6910 0.016910       0.015762

2. Quantiles for each variable:

                        2.5%       25%       50%       75%    97.5%
Days                  7.0975    9.3971   10.4851   11.5751   13.810

> fixef(fm1)
(Intercept)        Days
  251.40510    10.46729
> HPDinterval(fs1)
                         lower       upper
Days                  7.038076   13.734493
attr(,"Probability")
[1] 0.95

If you would like to use a t-distribution to calculate a confidence
interval, I would argue that the degrees of freedom for the t should
be somewhere between n - p = 178 in this case (n = number of
observations, p = number of coefficients in the fixed effects or
rank(X) where X is the model matrix for the fixed effects) and n -
rank([Z:X]) = 144.  (there are 36 random effects and 2 fixed effects
but the rank of [Z:X] = 36)

If we use the lower value of 144 we produce a confidence interval of

> 10.4673 + c(-1,1) * qt(0.975, df = 144) * 1.5458
[1]  7.41191 13.52269

Notice that this interval is contained in the HPD interval and in the
interval obtained from the 2.5% and 97.5% quantiles of the empirical
distribution.  I attribute this to the fact that the standard errors
are calculated conditional on the variance of the random effects.
Thus the t-based confidence interval takes into account imprecision of
the estimate of \sigma^2 but it assumes that the variance of the
random effects is fixed at the estimated values.  (That's not quite
true - it is the relative variance that is assumed fixed - but the
effect is the same.) The MCMC sample allows all the parameters to vary
and I would claim is therefore a better measure of the marginal
variability of this parameter.

However, as stated above, the HPD interval is stochastic.  I would
create more than one sample to check the reproducibility of the
intervals.  In this case intervals based on a chain of length 10000
are not wonderfully consistent

Days                  6.9389037   13.851661
Days                  6.9968306   13.768851

and I might go to chains of length 50000 to check further

Days                  7.0880422   13.903068
Days                  7.0758948   13.965146

For the benefit of editors of a biological journal I think I would
report both the interval derived from the t-distribution and a
representative interval from the MCMC samples to show that the MCMC
samples show variation in excess of that reported by the t interval.



I hope this helps.

--Doug


From laurent.deniau at cern.ch  Tue Aug  8 18:25:07 2006
From: laurent.deniau at cern.ch (Laurent Deniau)
Date: Tue, 08 Aug 2006 18:25:07 +0200
Subject: [R] prefixing list names in print
Message-ID: <44D8BAE3.4060603@cern.ch>

With

print(list(A="a",B="b"))

it displays

$A
[1] "a"

$B
[1] "b"

I would like to add a common prefix to all the list tags after the $. 
Pasting the prefix to the "names" does not work (appear after the $). 
For example if the prefix would be "P", it should display:

P$A
[1] "a"

P$B
[1] "b"

I tried to add a "name" attribute to the list or to add a prefix="P" to 
print but nothing works. Any hint?

Thanks,

	Laurent.


From tuechler at gmx.at  Tue Aug  8 18:41:37 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 08 Aug 2006 17:41:37 +0100
Subject: [R] Netiquette, was Re:  ... "gfortran and gcc...
In-Reply-To: <200608080809.44967.mikeumo@sbcglobal.net>
References: <Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
	<eb69j3$cmo$1@sea.gmane.org> <x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
Message-ID: <3.0.6.32.20060808174137.00adc780@pop.gmx.net>

What could be the reason, to respond not only to the list? I did not see an
advantage, to receive a response twice, once directly, once by the list.
Is it wrong, to assume that someone who writes to the list, does also
receive all the postings on the list?

Heinz

At 08:09 08.08.2006 -0500, Mike wrote:
>Thank you both.
>
>I would prefer to communicate through the list only.
>
>Mike.
>
>On Tue August 8 2006 04:47, Prof Brian Ripley wrote:
>> On Tue, 8 Aug 2006, Peter Dalgaard wrote:
>> > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>> > > First, you replied to the list and not to me, which was discourteous.
>> >
>> > You mean that he replied to the list *only*, I hope.
>>
>> Yes, and it was written as if to me, and was a reply to an email from me.
>>
>> > I usually consider it offensive when people reply to me and not the
>> > list (reasons including: It feels like being grabbed by the sleeve, I
>> > might not actually be the best source for the answer, and it's
>> > withholding the answer from the rest of the subscribers.)
>>
>> We do ask people to copy to the list.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>


From andy_liaw at merck.com  Tue Aug  8 18:42:45 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Aug 2006 12:42:45 -0400
Subject: [R] Take random sample from class variable
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB65B@usctmx1106.merck.com>

There may be better ways, but this should work:

R> p.yes <- 0.7
R> n.yes <- rbinom(1, nof.sample, p.yes)
R> n.no <- nof.sample - n.yes
R> dat.yes <- mydat[sample(which(mydat$Class == "yes"), n.yes,
replace=TRUE),]
R> dat.no <- mydat[sample(which(mydat$Class == "no"), n.no, replace=TRUE),]

You can rbind() them, and shuffle the rows if you wish.

Andy  

From: Muhammad Subianto
> 
> Dear all,
> Suppose I have a dataset like below, then I take for example, 
> 100 random sample "class" variable where contains "yes" and "no"
> respectively, 70% and 30%.
> I need a new 100 random sample from mydat dataset, but I 
> can't get the result.
> Thanks you very much for any helps.
> Best, Muhammad Subianto
> 
> mydat <- data.frame(size=c(30,12,15,10,12,12,25,30,20,14),
>                        A=c(0,1,0,1,0,1,1,1,0,0),
>                        B=c(1,1,0,1,0,1,1,0,0,1),
>                        C=c(0,0,1,1,0,0,1,1,0,0),
>                        D=c(1,1,1,1,0,1,0,0,1,1),
>                        E=c(1,1,0,1,1,1,1,1,1,0),
> 
> Class=c("yes","yes","no","yes","yes","no","yes","no","yes","yes"))
> mydat
> # Maximal data from dataset
> max.size <- sum(mydat$size);max.size
> # I need sample random
> nof.sample <- 100
> set.seed(123)
> sample.class <- sample(c("yes","no"), nof.sample, prob=c(.7, 
> .3), replace=TRUE) sample.class sampledat.class <- 
> mydat[sample.class,] sampledat.class
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From halldor.bjornsson at gmail.com  Tue Aug  8 18:47:18 2006
From: halldor.bjornsson at gmail.com (halldor bjornsson)
Date: Tue, 8 Aug 2006 16:47:18 +0000
Subject: [R] locating intervals
Message-ID: <696224530608080947x77afb1adl4b49fc2fff3ce529@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060808/8d12dd85/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Aug  8 18:53:40 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Aug 2006 18:53:40 +0200
Subject: [R] prefixing list names in print
In-Reply-To: <44D8BAE3.4060603@cern.ch>
References: <44D8BAE3.4060603@cern.ch>
Message-ID: <44D8C194.8000709@statistik.uni-dortmund.de>



Laurent Deniau wrote:
> With
> 
> print(list(A="a",B="b"))
> 
> it displays
> 
> $A
> [1] "a"
> 
> $B
> [1] "b"
> 
> I would like to add a common prefix to all the list tags after the $. 
> Pasting the prefix to the "names" does not work (appear after the $). 
> For example if the prefix would be "P", it should display:
> 
> P$A
> [1] "a"
> 
> P$B
> [1] "b"
> 
> I tried to add a "name" attribute to the list or to add a prefix="P" to 
> print but nothing works. Any hint?

This is a very internal feature of print(). At a first quick look, I 
think you will have to change the R sources and recompile.
Conclusion: Don't do it.

Uwe Ligges




> Thanks,
> 
> 	Laurent.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tristan.rouyer at ifremer.fr  Tue Aug  8 18:54:34 2006
From: tristan.rouyer at ifremer.fr (tristan rouyer)
Date: Tue, 08 Aug 2006 18:54:34 +0200
Subject: [R] More Plots
In-Reply-To: <20060808154114.M9520@deepfoo.com>
References: <20060808154114.M9520@deepfoo.com>
Message-ID: <44D8C1CA.2060807@ifremer.fr>

sonal at deepfoo.com a ?crit :
> Hi,
>
> How can we plot two graphs ex. lets say correlation & ratio in the same 
> window?
>
> I mean in the window I have :
>
> 1. Graph of correlation having X & Y axes
>
> 2. Graph of ratio having A & B axes
>
> one above the other.
>
> Thanks,
> Sonal
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
Hi,
you can simply do that using the command par(new=TRUE) between your two 
graphs.
example :
x<-rnorm(100,0,2)
y<-2*x+rnorm(100,0,2)
plot(y~x)
par(new=TRUE)
z<-rnorm(100,10,2)
plot(z,col='red')

As you can notice the second plot is roughly added to the new one, even 
if the scales are different. So you basically have to specify the axes 
of the second plot, using the axis() command (after specifying xaxt='n' 
and yaxt='n' in the second plot for removing its axes).


Tristan


From halldor.bjornsson at gmail.com  Tue Aug  8 18:54:37 2006
From: halldor.bjornsson at gmail.com (halldor bjornsson)
Date: Tue, 8 Aug 2006 16:54:37 +0000
Subject: [R] locating intervals (corrected version)
Message-ID: <696224530608080954u44aebef9t2a6af2ab62d9246c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060808/adf0638d/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Aug  8 18:55:37 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Aug 2006 17:55:37 +0100 (BST)
Subject: [R] prefixing list names in print
In-Reply-To: <44D8BAE3.4060603@cern.ch>
References: <44D8BAE3.4060603@cern.ch>
Message-ID: <Pine.LNX.4.64.0608081749160.30539@gannet.stats.ox.ac.uk>

On Tue, 8 Aug 2006, Laurent Deniau wrote:

> With
> 
> print(list(A="a",B="b"))
> 
> it displays
> 
> $A
> [1] "a"
> 
> $B
> [1] "b"
> 
> I would like to add a common prefix to all the list tags after the $.

`prefix' ... `after'?  You seem to want to prefix component names: why?
What do you want for component $a$b$c?  For unnamed components?
 
> Pasting the prefix to the "names" does not work (appear after the $). 
> For example if the prefix would be "P", it should display:
> 
> P$A
> [1] "a"
> 
> P$B
> [1] "b"
> 
> I tried to add a "name" attribute to the list or to add a prefix="P" to 
> print but nothing works. Any hint?

You will need to alter the C code to do this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mschwartz at mn.rr.com  Tue Aug  8 19:17:14 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 08 Aug 2006 12:17:14 -0500
Subject: [R] Netiquette, was Re:  ... "gfortran and gcc...
In-Reply-To: <3.0.6.32.20060808174137.00adc780@pop.gmx.net>
References: <Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
	<eb69j3$cmo$1@sea.gmane.org> <x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
	<3.0.6.32.20060808174137.00adc780@pop.gmx.net>
Message-ID: <1155057434.30651.2.camel@localhost.localdomain>

[Re-sending to the list only for archiving, as my original reply had too
many recipients and I cancelled it.]


1. One need not be subscribed to the list to be able to post. Thus,
indeed, a poster may not see all postings.

2. On the relatively rare occasion (thanks to Martin) where the server
seems to incur delays in sending out posts and replies, copying the
original poster on your reply ensures that they will get the reply in a
timely fashion.

HTH,

Marc Schwartz

On Tue, 2006-08-08 at 17:41 +0100, Heinz Tuechler wrote:
> What could be the reason, to respond not only to the list? I did not see an
> advantage, to receive a response twice, once directly, once by the list.
> Is it wrong, to assume that someone who writes to the list, does also
> receive all the postings on the list?
> 
> Heinz
> 
> At 08:09 08.08.2006 -0500, Mike wrote:
> >Thank you both.
> >
> >I would prefer to communicate through the list only.
> >
> >Mike.
> >
> >On Tue August 8 2006 04:47, Prof Brian Ripley wrote:
> >> On Tue, 8 Aug 2006, Peter Dalgaard wrote:
> >> > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> >> > > First, you replied to the list and not to me, which was discourteous.
> >> >
> >> > You mean that he replied to the list *only*, I hope.
> >>
> >> Yes, and it was written as if to me, and was a reply to an email from me.
> >>
> >> > I usually consider it offensive when people reply to me and not the
> >> > list (reasons including: It feels like being grabbed by the sleeve, I
> >> > might not actually be the best source for the answer, and it's
> >> > withholding the answer from the rest of the subscribers.)
> >>
> >> We do ask people to copy to the list.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bruno.giordano at music.mcgill.ca  Tue Aug  8 19:38:56 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Tue, 8 Aug 2006 13:38:56 -0400
Subject: [R]  multinom details
Message-ID: <003e01c6bb11$94335cb0$d70ece84@brungio>

Hello,
the multinom() procedure prints a lot of information during the fitting 
process.
Is there a way to disable this "verbose" mode?
Thanks,
    Bruno


From andy_liaw at merck.com  Tue Aug  8 19:54:12 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Aug 2006 13:54:12 -0400
Subject: [R] multinom details
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB678@usctmx1106.merck.com>

Try adding trace=FALSE to the call.  ?multinom says "..." is passed to
nnet(), and you'd find "trace" documented in ?nnet.

Please remember to mention the add-on package(s) you're using.

Andy 

From: Bruno L. Giordano
> 
> Hello,
> the multinom() procedure prints a lot of information during 
> the fitting process.
> Is there a way to disable this "verbose" mode?
> Thanks,
>     Bruno
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From bates at stat.wisc.edu  Tue Aug  8 20:05:29 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 8 Aug 2006 13:05:29 -0500
Subject: [R] fixed effects constant in mcmcsamp
In-Reply-To: <44D86E45020000B6000071AA@ZGRW50.cf.ac.uk>
References: <44D86E45020000B6000071AA@ZGRW50.cf.ac.uk>
Message-ID: <40e66e0b0608081105s25d4387dw34f1a5a7511f9f07@mail.gmail.com>

Thank you for the thorough report.

On 8/8/06, Daniel Farewell <farewelld at cardiff.ac.uk> wrote:
> I'm fitting a GLMM to some questionnaire data. The structure is J individuals,
> nested within I areas, all of whom answer the same K (ordinal) questions. The
> model I'm using is based on so-called continuation ratios, so that it can be
> fitted using the lme4 package.
>
> The lmer function fits the model just fine, but using mcmcsamp to judge the
> variability of the parameter estimates produces some strange results. The
> posterior sample is constant for the fixed effects, and the estimates of the
> variance components are way out in the tails of their posterior samples.
>
> The model I'm using says (for l = 1, ..., L - 1)
>
> logit P(X[ijk] = l | X[ijk] >= l, U[i], V[j], W[k]) = U[i] + V[j] + W[k] + a[l]
>
> where X[ijk] is the ordinal response to question k for individual j in area i.
> The U, V, and W are random effects and the a's are fixed effects. Here's a
> function to simulate data which mimics this setup (with a sequence of binary
> responses Y[ijkl] = 1 iff X[ijk] = l):
>
> sim <- function(n = c(10, 10, 10), sd = c(0.5, 2, 0.5), a = seq(-5, 1, 2)) {
>
>  u <- rnorm(n[1], 0, sd[1])
>  v <- rnorm(prod(n[1:2]), 0, sd[2])
>  w <- rnorm(n[3], 0, sd[3])
>
>  i <- factor(rep(1:n[1], each = prod(n[2:3])))
>  j <- factor(rep(1:prod(n[1:2]), each = n[3]))
>  k <- factor(rep(1:n[3], prod(n[1:2])))
>
>  df <- NULL
>
>  for(l in 1:length(a)) {
>
>   y <- rbinom(length(i), 1, plogis(u[i] + v[j] + w[k] + a[l]))
>   df <- rbind(df, data.frame(i, j, k, l, y))
>
>   i <- i[!y]
>   j <- j[!y]
>   k <- k[!y]
>
>  }
>
>  df$l <- factor(df$l)
>  return(df)
>
> }
>
> And here's a function which shows the difficulties I've been having:
>
> test <- function(seed = 10, ...) {
>
>  require(lme4)
>  set.seed(seed)
>
>  df <- sim(...)
>  df.lmer <- lmer(y ~ 0 + l + (1 | i) + (1 | j) + (1 | k), family = binomial,
> data = df)
>  df.mcmc <- mcmcsamp(df.lmer, 1000, trans = FALSE)
>
>  print(summary(df.lmer))
>  print(summary(df.mcmc))
>  densityplot(~ as.numeric(df.mcmc) | factor(rep(colnames(df.mcmc), each =
> 1000)), scales = list(relation = "free"))
>
> }
>
> Running
>
> test()
>
> gives the following:
>
> Generalized linear mixed model fit using PQL
> Formula: y ~ 0 + l + (1 | i) + (1 | j) + (1 | k)
> Data: df
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  2316.133 2359.166 -1151.066 2302.133
> Random effects:
>  Groups Name        Variance Std.Dev.
>  j      (Intercept) 4.15914  2.03940
>  k      (Intercept) 0.25587  0.50584
>  i      (Intercept) 0.56962  0.75473
> number of obs: 3455, groups: j, 100; k, 10; i, 10
>
> Estimated scale (compare to 1)  0.8747598
>
> Fixed effects:
>    Estimate Std. Error  z value  Pr(>|z|)
> l1 -4.50234    0.40697 -11.0632 < 2.2e-16 ***
> l2 -3.27643    0.38177  -8.5821 < 2.2e-16 ***
> l3 -1.05277    0.36566  -2.8791  0.003988 **
> l4  0.76538    0.36832   2.0780  0.037706 *
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>    l1    l2    l3
> l2 0.843
> l3 0.841 0.900
> l4 0.805 0.865 0.921
>        l1               l2               l3               l4
>  Min.   :-4.502   Min.   :-3.276   Min.   :-1.053   Min.   :0.7654
>  1st Qu.:-4.502   1st Qu.:-3.276   1st Qu.:-1.053   1st Qu.:0.7654
>  Median :-4.502   Median :-3.276   Median :-1.053   Median :0.7654
>  Mean   :-4.502   Mean   :-3.276   Mean   :-1.053   Mean   :0.7654
>  3rd Qu.:-4.502   3rd Qu.:-3.276   3rd Qu.:-1.053   3rd Qu.:0.7654
>  Max.   :-4.502   Max.   :-3.276   Max.   :-1.053   Max.   :0.7654
>      j.(In)          k.(In)           i.(In)           deviance
>  Min.   :1.911   Min.   :0.0509   Min.   :0.06223   Min.   :2011
>  1st Qu.:2.549   1st Qu.:0.1310   1st Qu.:0.19550   1st Qu.:2044
>  Median :2.789   Median :0.1756   Median :0.25581   Median :2054
>  Mean   :2.824   Mean   :0.2085   Mean   :0.29948   Mean   :2054
>  3rd Qu.:3.070   3rd Qu.:0.2463   3rd Qu.:0.34640   3rd Qu.:2064
>  Max.   :4.615   Max.   :0.8804   Max.   :3.62168   Max.   :2107
>
> As you can see, the posterior samples from the fixed effects are constant (at
> the inital estimates) and the estimates of the variance components aren't within
> the IQ ranges of their posterior samples.
>
> I understand from various posts that mcmcsamp is still a work in progress, and
> may not work on every model. Is this one of those cases? I'm using R 2.3.1 and
> lme4 0.995-2 on Windows XP.

In recent versions of the lme4/Matrix packages setting verbose = TRUE
in a call to mcmcsamp for a generalized linear mixed model causes the
Metropolis-Hastings ratio for the proposed change in the fixed effects
and random effects to be printed.  When I ran your example those
values were always 0 which is either extremely bad luck or a bug.  My
guess is that it's a bug.

Thanks for the report with the reproducible example.


From andy_liaw at merck.com  Tue Aug  8 20:10:26 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Aug 2006 14:10:26 -0400
Subject: [R] Sampling from a Matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB685@usctmx1106.merck.com>

From: Marc Schwartz
 
> On Fri, 2006-08-04 at 12:46 -0400, Daniel Gerlanc wrote:
> > Hello all,
> > 
> > Consider the following problem:
> > 
> > There is a matrix of probabilities:
> > 
> > > set.seed(1)
> > > probs <- array(abs(rnorm(25, sd = 0.33)), dim = c(5,5), 
> dimnames = 
> > > list(1:5, letters[1:5])) probs
> >         a      b       c         d        e
> > 1 0.21 0.27 0.50 0.0148 0.303
> > 2 0.06 0.16 0.13 0.0053 0.258
> > 3 0.28 0.24 0.21 0.3115 0.025
> > 4 0.53 0.19 0.73 0.2710 0.656
> > 5 0.11 0.10 0.37 0.1960 0.205
> > 
> > I want to sample 3 values from each row.
> > 
> > One way to do this follows:
> > 
> > index <- 1:ncol(probs)
> > 
> > for(i in 1:nrow(probs)){
> > 
> > ## gets the indexes of the values chosen
> > 
> > sample(index, size = 3, replace = TRUE, prob = probs[i, ])
> > 
> > }
> > 
> > Is there a another way to do this?
> > 
> > Thanks!
> 
> > t(apply(probs, 1, function(x) sample(x, 3)))
>    [,1]   [,2]   [,3]
> 1 0.210 0.5000 0.0148
> 2 0.258 0.0053 0.1300
> 3 0.025 0.2800 0.3115
> 4 0.190 0.5300 0.2710
> 5 0.196 0.1000 0.1100

Hmm... If I read Daniel's code (which is different from his description)
correctly, that doesn't seem to be what he wanted.  Perhaps something like
this:

apply(probs, 1, function(p) sample(1:ncol(probs), 3, replace=TRUE, prob=p))

Andy

 
> See ?apply and ?t
> 
> HTH,
> 
> Marc Schwartz


From leslou at ctbp.ucsd.edu  Tue Aug  8 20:22:06 2006
From: leslou at ctbp.ucsd.edu (Leslie Chavez)
Date: Tue, 8 Aug 2006 11:22:06 -0700 (PDT)
Subject: [R] Fitting data with optim or nls--different time scales
Message-ID: <Pine.LNX.4.44.0608081108390.27357-100000@ctbp0.ucsd.edu>


Hi,

I have a system of ODE's I can solve with lsoda.

Model=function(t,x,parms) 
{
    #parameter definitions
    lambda=parms[1]; beta=parms[2]; 
    d = parms[3]; delta = parms[4]; 
     p=parms[5];    c=parms[6]
     
      xdot[1] = lambda - (d*x[1])- (beta*x[3]*x[1])
      xdot[2] = (beta*x[3]*x[1]) - (delta*x[2])
      xdot[3] = (p*x[2]) - (c*x[3])
     
    return(list(xdot))
}

I want to fit the output out[,4] to experimental data that is only 
available on days 0, 7, 12, 14, 17, and 20. I don't know how to set up 
optim or nls so that it takes out[,4] on the appropriate day, but still 
runs lsoda on a time scale of 0.01 day.

Below is the function I've been using to run 'optim', at the 
course-grained time scale:

Modelfit=function(s) {
	parms[1:4]=s[1:4]; times=c(0,7,12,14,17,20,25)
	out=lsoda(x0,times,Model,parms)
	mse=mean((log10(out[,4])-log10(i(times)))^2)
#	cat(times)
	return(mse)
}
#x0=c(T0,I0,V0)
x0=c(2249,0,1)
#parms(lambda, beta, d, delta, p, c)
parms[5:6]=c(1.0,23)

s0=c(49994,8456,6.16E-8,0.012) #initial values

fit=optim(s0,Modelfit)

Right now, lsoda is being run on too course-grained a time scale in the 
function Modelfit. Most examples of optim and nls I have found compare 
two data sets at the same times, and run lsoda on the time scale the 
data is available at, but I would like to run lsoda at a finer scale, and 
only compare the appropriate time points with the experiment.  I have also 
tried using nls, but I have the same problem. Does anyone have 
suggestions?

Thank you very much,

Leslie


From Dimitris.Rizopoulos at med.kuleuven.be  Tue Aug  8 20:39:31 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Tue, 08 Aug 2006 20:39:31 +0200
Subject: [R] locating intervals (corrected version)
In-Reply-To: <696224530608080954u44aebef9t2a6af2ab62d9246c@mail.gmail.com>
References: <696224530608080954u44aebef9t2a6af2ab62d9246c@mail.gmail.com>
Message-ID: <20060808203931.3o3d5nvs8beo4gw0@webmail4.kuleuven.be>

?findInterval() could be of help in this case.

Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting halldor bjornsson <halldor.bjornsson at gmail.com>:

> I have corrected a typo in my previous posting. In what follows the
> line with the inequality is correct
>
> Hi ,
>
> I have two sorted vectors X and Xi, where the range of Xi lies within the
> range of X.
>
> For an element in Xi, I want to find the neigbouring data in X, e.g. find an
> index ix
> so that for element number k, then
> X[ix[k]] < Xi[k] < X[ix[k] +1]  # also OK with "<=" on either one, but not
> both
>
> This is easy to code by looping over the data in X,Xi, but I suspect there
> may be a faster and more elegant way to do this in R.
>
> In Python (Numeric) the same can be achieved with
> ix=Numeric.searchsorted(X[1:-1],Xi),
> which is quite compact.
>
> So, does anyone know of a corresponding R  call that can achive the same?
>
> Sincerely,
> Halld?r
>
>
> --
> Halld?r Bj?rnsson
> Deildarstj. Ranns. & ?r?un
> Ve?ursvi? Ve?urstofu ?slands
>
> Halld?r Bjornsson
> Weatherservice R & D
> Icelandic Met. Office
>
> 	[[alternative HTML version deleted]]
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jrkrideau at yahoo.ca  Tue Aug  8 20:59:10 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 8 Aug 2006 14:59:10 -0400 (EDT)
Subject: [R] More Plots
In-Reply-To: <20060808154114.M9520@deepfoo.com>
Message-ID: <20060808185910.10347.qmail@web33803.mail.mud.yahoo.com>


--- sonal at deepfoo.com wrote:

> Hi,
> 
> How can we plot two graphs ex. lets say correlation
> & ratio in the same 
> window?
> 
> I mean in the window I have :
> 
> 1. Graph of correlation having X & Y axes
> 
> 2. Graph of ratio having A & B axes
> 
> one above the other.
> 
> Thanks,
> Sonal


?par
and have a look at mfcol or mfrow.

Example

a <- c(1:5)
b <- c(11:15)
x <- c(21:30)
y <- c(31:40)

par(mfcol=c(2,1))
plot(a,b)
plot (x,y)


From br44114 at gmail.com  Tue Aug  8 22:27:05 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 8 Aug 2006 16:27:05 -0400
Subject: [R] prefixing list names in print
Message-ID: <8d5a36350608081327r3f681aaft5b4e414cbc64fd75@mail.gmail.com>

A simple function will do what you want, customize this as needed:
lprint <- function(lst,prefix)
{
for (i in 1:length(lst)) {
   cat(paste(prefix,"$",names(lst)[i],sep=""),"\n")
   print(lst[[i]])
   cat("\n")
}
}
P <- list(A="a",B="b")
lprint(P,"Prefix")


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Deniau
> Sent: Tuesday, August 08, 2006 12:25 PM
> To: R-help
> Subject: [R] prefixing list names in print
>
> With
>
> print(list(A="a",B="b"))
>
> it displays
>
> $A
> [1] "a"
>
> $B
> [1] "b"
>
> I would like to add a common prefix to all the list tags after the $.
> Pasting the prefix to the "names" does not work (appear after the $).
> For example if the prefix would be "P", it should display:
>
> P$A
> [1] "a"
>
> P$B
> [1] "b"
>
> I tried to add a "name" attribute to the list or to add a
> prefix="P" to
> print but nothing works. Any hint?
>
> Thanks,
>
> 	Laurent.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dsonneborn at ucdavis.edu  Tue Aug  8 23:02:16 2006
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Tue, 08 Aug 2006 14:02:16 -0700
Subject: [R] trellis in black and white
Message-ID: <44D8FBD8.8060609@yellow.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060808/1ad2a284/attachment.pl 

From miczat at yahoo.com  Tue Aug  8 23:41:01 2006
From: miczat at yahoo.com (Michael Zatorsky)
Date: Wed, 9 Aug 2006 07:41:01 +1000 (EST)
Subject: [R] Frequency Distribution
In-Reply-To: <219889DD-3CD3-45AE-BCF5-6E907CB91169@austin.rr.com>
Message-ID: <20060808214102.77628.qmail@web51114.mail.yahoo.com>

Thankyou William.

I found the package and read through the
documentation.  I'm not a statistican, so it was
largely over my head.  I was looking for a
command/function that described itself as performing a
frequency distribution, and could not find anything
obvious enough.

What did you have in mind in the package that you
thought may help? 

All I'm looking to do is ask it to give me frequencies
and cumulative frequencies for the whole dataset,
using intervale widths of 100 or 1000 (in much the
same way the data would have to have been binned
before producing a histogram.


Regards
Michael.

--- William Asquith <wasquith at austin.rr.com> wrote:

> You might be interested in the lmomco package that
> supports many  
> nontraditional and traditional distributions.
> 
> William A.
> 
> 
> On Aug 8, 2006, at 9:00 AM, Michael Zatorsky wrote:
> 
> > Hi,
> >
> > Could someone please suggest where I might find
> some
> > instructions / tutorials / FAQs that describe how
> to
> > create a frequency distribution and cumulative
> > frequency distribution in R using different class
> > withs.
> >
> > I have about a 2-million observations (distances
> > between points ranging from sub-millimetre to
> about
> > 400km, and I want to get a feel for how they are
> > distributed).
> >
> > I'd like the output as a table / data rather than
> an
> > graph.
> >
> > I've searched Google and R's help for obvious
> terms,
> > and while I've found much information on
> > graphing/plotting, I haven't hit on anything for
> this.
> >
> > (I only downloaded R about 2 hours ago, apologies
> if
> > this is obviously documented somewhere I missed.)
> >
> > Regards
> > Michael.
> >
> > Send instant messages to your online friends
> http:// 
> > au.messenger.yahoo.com
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting- 
> > guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 
> 


Send instant messages to your online friends http://au.messenger.yahoo.com


From simone.vincenzi at nemo.unipr.it  Tue Aug  8 23:44:49 2006
From: simone.vincenzi at nemo.unipr.it (Simone Vincenzi)
Date: Tue, 8 Aug 2006 23:44:49 +0200
Subject: [R]  Help with short time series
Message-ID: <20060808214338.M50933@nemo.unipr.it>

Thanks for the help. 
I provide below the dataset I'm using, it's a little bit different from what 
I was describing (sorry for that). The streams are 3 and I have an unequal 
number of years for each stream.

Stream Density Year 
1     Zak    0.20 2000 
2     Zak    0.36 2001 
3     Zak    0.41 2002 
4     Zak    0.34 2003 
5     Zak    0.28 2004 
6     Gor    0.08 1999 
7     Gor    0.05 2000 
8     Gor    0.14 2001 
9     Gor    0.16 2002 
10    Gor    0.13 2003 
11    Gat    0.18 2004 
12    Gat    0.10 2001 
13    Gat    0.37 2002 
14    Gat    0.57 2003 
15    Gat    0.47 2004

I tried to follow the suggestions of Dieter, but the model does not fit. 
Any suggestion will be appreciated



Dear R-list, 
> I have a statistical problem with the comparison of two short time-series 
of 
> density data in an ecological framework. I have to compare two short time 
> series (5 years, one value for each year) of species density data (it is 
the 
> density of fish in two different streams) to test if the two means of the 
> five densities are significantly different, so basically if the two mean 
> stream-specific fish densities are significantly different. 
> I don't think I can use a straight t-test due to the problem of 
> autocorrelation and I don't think I can use a repeated measure ANOVA as I 
> don't have any replicates. 
> Any help would be greatly appreciated.

try something like

library(nlme) 
summary(lme(dens~stream+year,data=mystreamdata,random=~year|stream))

This should also give you an estimate if the slopes are different if you 
test 
against the simplified model

summary(lme(dens~stream+year,data=mystreamdata,random=~1|stream))

Since you did not provide a short example data set, this is only 
approximatively 
right.

Dieter

-- 
Universita' degli Studi di Parma (http://www.unipr.it)

_________________________________________
Simone Vincenzi, PhD Student 
Department of Environmental Sciences
University of Parma
Parco Area delle Scienze, 33/A, 43100 Parma, Italy
Phone: +39 0521 905696
Fax: +39 0521 906611
e.mail: svincenz at nemo.unipr.it 


-- 



 


--
Universita' degli Studi di Parma (http://www.unipr.it)


From pbarata at infolink.com.br  Tue Aug  8 23:49:59 2006
From: pbarata at infolink.com.br (Paulo Barata)
Date: Tue, 08 Aug 2006 18:49:59 -0300
Subject: [R] parameter yaxs / function hist (graphics)
In-Reply-To: <Pine.LNX.4.63.0608072304570.7707@est.ufpr.br>
References: <mailman.9.1154685603.13659.r-help@stat.math.ethz.ch>
	<44D7ECEA.10009@infolink.com.br>
	<Pine.LNX.4.63.0608072304570.7707@est.ufpr.br>
Message-ID: <44D90707.1060002@infolink.com.br>


Dear Paulo,

Thank you for your reply. But I doubt yours is a proper
solution to my request, for some reasons:

1. The position of the axis graphed with the command axis(1, line=-1)
depends on the size of the graphics window.

2. After your graph is on the screen, in case one may want a boxed
graph, a box() command will produce a histogram "floating in the air",
not "touching" the horizontal axis.

Of course, one could build a proper box (with labels, etc.) by means
of more primitive graphics functions like lines (package graphics)
and others, but I think that would mean a lot of work.

Thank you again.

Paulo Barata

-----------------------------------------------------------------------
Paulo Barata
Fundacao Oswaldo Cruz / Oswaldo Cruz Foundation
Rua Leopoldo Bulhoes 1480 - 8A
21041-210  Rio de Janeiro - RJ
Brasil
E-mail: pbarata at infolink.com.br
-----------------------------------------------------------------------

Paulo Justiniano Ribeiro Jr wrote:
> Paulo
> 
> One possibility is to draw the histogram without axes and then add them 
> wherever you want.
> 
> For instance with something along the lines:
> 
> x <- rnorm(500)
> hist(x, axes=F)
> axis(1, line=-1)
> 
> For more details: ?axis
> 
> best
> P.J.
> 
> 
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat?rio de Estat?stica e Geoinforma??o)
> Departamento de Estat?stica
> Universidade Federal do Paran?
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 3361 3573
> Fax: (+55) 41 3361 3141
> e-mail: paulojus at est.ufpr.br
> http://www.est.ufpr.br/~paulojus
> 
> On Mon, 7 Aug 2006, Paulo Barata wrote:
> 
>>
>> Dear R users,
>>
>> The parameters xaxs and yaxs (function par, package graphics)
>> seem not to work with the function hist (package graphics),
>> even when the parameters xlim and ylim are defined.
>>
>> Is there any way to make yaxs="i" and xaxs="i" work properly
>> with the function hist, mainly to produce histograms that
>> "touch" the horizontal axis? The R documentation and the
>> R mailing lists archive don't seem to be of help here.
>>
>> I am using R 2.3.1, running under Windows XP.
>>
>> ## Example:
>> x <- rnorm(100)
>> hist(x,breaks=seq(-4,4,0.5),ylim=c(0,40),yaxs="i",
>>   xlim=c(-4,4),xaxs="i")
>> box()
>>
>> Thank you very much.
>>
>> Paulo Barata
>>
>> --------------------------------------------------------------
>> Paulo Barata
>> Fundacao Oswaldo Cruz / Oswaldo Cruz Foundation
>> Rua Leopoldo Bulhoes 1480 - 8A
>> 21041-210  Rio de Janeiro - RJ
>> Brasil
>> E-mail: pbarata at infolink.com.br
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>


From jrkrideau at yahoo.ca  Wed Aug  9 00:00:23 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 8 Aug 2006 18:00:23 -0400 (EDT)
Subject: [R] Getting data out of a loop
Message-ID: <20060808220023.73412.qmail@web33814.mail.mud.yahoo.com>

A stupid question but I just cannot see how to do
this.

I have a loop that does some calculations and puts the
results in a vector for each iteration, but I cannot
see how to get the data out of the loop in such a way
that I can use it.  I can print it but how do I get it
into a set of vectors or what ever.

Any help gratefully received.  Thanks

Example

cata <- c( 3,5,6,8,0, NA)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)
dogb <- c(2,4,6,8,10, 12)
rata <- c (NA, 5, 5, 4, 9, 0)
ratb <- c( 1,2,3,4,5,6)
bata <- c( 12, 42,NA, 45, 32, 54)
batb <- c( 13, 15, 17,19,21,23)
id <- Cs(a,b,b,c,a,b)
site <- c(1,1,4,4,1,4)
mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
bata, batb)
Df <- data.frame(site, id, mat1)
nn <- levels(Df$id)

Df
nn
rate <- c(2,3,4)

for (i in 1: length(nn)) {
dd<- subset(Df, id==nn[i])   
scat <- sum(c(dd$cata,dd$catb), na.rm=T)
sdog <- sum(c(dd$doga,dd$dogb), na.rm=T) 
srat <- sum(c(dd$rata, dd$ratb), na.rm=T)
sbat <- sum(c(dd$bata,dd$batb), na.rm=T)
sss <- c(scat,sdog, srat,sbat) * rate[i]
print(sss)
}


From jholtman at gmail.com  Wed Aug  9 00:10:46 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 8 Aug 2006 18:10:46 -0400
Subject: [R] Getting data out of a loop
In-Reply-To: <20060808220023.73412.qmail@web33814.mail.mud.yahoo.com>
References: <20060808220023.73412.qmail@web33814.mail.mud.yahoo.com>
Message-ID: <644e1f320608081510q5ee38c0bhc16aa0dd506cc78c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060808/ee397d14/attachment.pl 

From arvicolinae at hotmail.com  Wed Aug  9 00:11:17 2006
From: arvicolinae at hotmail.com (Chreis Habeck)
Date: Tue, 08 Aug 2006 17:11:17 -0500
Subject: [R] Split-plot model
Message-ID: <BAY23-F135B0827D0C64DA86BACFDA0540@phx.gbl>

How do I set up my model equation in aov to analyze a split-plot design?

I have two factors (CO2 and NITROGEN), each with two levels (high and 
ambient).   CO2 is my whole-plot factor with three replicates for each level 
(i.e., 6 rooms total).

Is this syntax below correct?

summary(aov(response ~ ROOM + CO2*NITROGEN + Error(ROOM/CO2)))


From ggrothendieck at gmail.com  Wed Aug  9 00:39:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 8 Aug 2006 18:39:56 -0400
Subject: [R] Netiquette, was Re: ... "gfortran and gcc...
In-Reply-To: <1155057434.30651.2.camel@localhost.localdomain>
References: <eb69j3$cmo$1@sea.gmane.org> <x2d5bba7hg.fsf_-_@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0608081045480.18142@gannet.stats.ox.ac.uk>
	<3.0.6.32.20060808174137.00adc780@pop.gmx.net>
	<1155057434.30651.2.camel@localhost.localdomain>
Message-ID: <971536df0608081539j1ec9f179h81f0a0db80ce0c80@mail.gmail.com>

I agree.  Also, sending a copy to the poster means that they are
likely to get it first which seems like a desirable courtesy.

On 8/8/06, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> [Re-sending to the list only for archiving, as my original reply had too
> many recipients and I cancelled it.]
>
>
> 1. One need not be subscribed to the list to be able to post. Thus,
> indeed, a poster may not see all postings.
>
> 2. On the relatively rare occasion (thanks to Martin) where the server
> seems to incur delays in sending out posts and replies, copying the
> original poster on your reply ensures that they will get the reply in a
> timely fashion.
>
> HTH,
>
> Marc Schwartz
>
> On Tue, 2006-08-08 at 17:41 +0100, Heinz Tuechler wrote:
> > What could be the reason, to respond not only to the list? I did not see an
> > advantage, to receive a response twice, once directly, once by the list.
> > Is it wrong, to assume that someone who writes to the list, does also
> > receive all the postings on the list?
> >
> > Heinz
> >
> > At 08:09 08.08.2006 -0500, Mike wrote:
> > >Thank you both.
> > >
> > >I would prefer to communicate through the list only.
> > >
> > >Mike.
> > >
> > >On Tue August 8 2006 04:47, Prof Brian Ripley wrote:
> > >> On Tue, 8 Aug 2006, Peter Dalgaard wrote:
> > >> > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> > >> > > First, you replied to the list and not to me, which was discourteous.
> > >> >
> > >> > You mean that he replied to the list *only*, I hope.
> > >>
> > >> Yes, and it was written as if to me, and was a reply to an email from me.
> > >>
> > >> > I usually consider it offensive when people reply to me and not the
> > >> > list (reasons including: It feels like being grabbed by the sleeve, I
> > >> > might not actually be the best source for the answer, and it's
> > >> > withholding the answer from the rest of the subscribers.)
> > >>
> > >> We do ask people to copy to the list.
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Wed Aug  9 01:08:24 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 8 Aug 2006 16:08:24 -0700
Subject: [R] More Plots
In-Reply-To: <20060808154114.M9520@deepfoo.com>
References: <20060808154114.M9520@deepfoo.com>
Message-ID: <f8e6ff050608081608nc685f2frd91f0737234d1c6d@mail.gmail.com>

> How can we plot two graphs ex. lets say correlation & ratio in the same
> window?
>
> I mean in the window I have :
>
> 1. Graph of correlation having X & Y axes
>
> 2. Graph of ratio having A & B axes
>
> one above the other.


Why do you want to do this?  It is not a good idea unless you are
trying to confusing or mislead people.

Hadey


From neuro3000 at hotmail.com  Wed Aug  9 01:22:02 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 08 Aug 2006 19:22:02 -0400
Subject: [R] Getting data out of a loop
In-Reply-To: <20060808220023.73412.qmail@web33814.mail.mud.yahoo.com>
Message-ID: <BAY112-F317A963CB9EE3E08486F95AF540@phx.gbl>

I'd create an empty dataframe prior to the loop.

cata <- c( 3,5,6,8,0, NA)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)
dogb <- c(2,4,6,8,10, 12)
rata <- c (NA, 5, 5, 4, 9, 0)
ratb <- c( 1,2,3,4,5,6)
bata <- c( 12, 42,NA, 45, 32, 54)
batb <- c( 13, 15, 17,19,21,23)
id <- c('a', 'b', 'b', 'c', 'a', 'b')
site <- c(1,1,4,4,1,4)
mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
bata, batb)
Df <- data.frame(site, id, mat1)
nn <- levels(Df$id)

Df
nn
rate <- c(2,3,4)

Result <- data.frame(matrix(NA,length(nn),4))
for (i in 1: length(nn)) {
dd<- subset(Df, id==nn[i])
scat <- sum(c(dd$cata,dd$catb), na.rm=T)
sdog <- sum(c(dd$doga,dd$dogb), na.rm=T)
srat <- sum(c(dd$rata, dd$ratb), na.rm=T)
sbat <- sum(c(dd$bata,dd$batb), na.rm=T)
sss <- c(scat,sdog, srat,sbat) * rate[i]
Result[i,] <- sss
print(sss)
}
Result



>From: John Kane <jrkrideau at yahoo.ca>
>To: R R-help <r-help at stat.math.ethz.ch>
>Subject: [R] Getting data out of a loop
>Date: Tue, 8 Aug 2006 18:00:23 -0400 (EDT)
>
>A stupid question but I just cannot see how to do
>this.
>
>I have a loop that does some calculations and puts the
>results in a vector for each iteration, but I cannot
>see how to get the data out of the loop in such a way
>that I can use it.  I can print it but how do I get it
>into a set of vectors or what ever.
>
>Any help gratefully received.  Thanks
>
>Example
>
>cata <- c( 3,5,6,8,0, NA)
>catb <- c( 1,2,3,4,5,6)
>doga <- c(3,5,3,6,4, 0)
>dogb <- c(2,4,6,8,10, 12)
>rata <- c (NA, 5, 5, 4, 9, 0)
>ratb <- c( 1,2,3,4,5,6)
>bata <- c( 12, 42,NA, 45, 32, 54)
>batb <- c( 13, 15, 17,19,21,23)
>id <- Cs(a,b,b,c,a,b)
>site <- c(1,1,4,4,1,4)
>mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
>bata, batb)
>Df <- data.frame(site, id, mat1)
>nn <- levels(Df$id)
>
>Df
>nn
>rate <- c(2,3,4)
>
>for (i in 1: length(nn)) {
>dd<- subset(Df, id==nn[i])
>scat <- sum(c(dd$cata,dd$catb), na.rm=T)
>sdog <- sum(c(dd$doga,dd$dogb), na.rm=T)
>srat <- sum(c(dd$rata, dd$ratb), na.rm=T)
>sbat <- sum(c(dd$bata,dd$batb), na.rm=T)
>sss <- c(scat,sdog, srat,sbat) * rate[i]
>print(sss)
>}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From eklypse at gmail.com  Wed Aug  9 02:38:59 2006
From: eklypse at gmail.com (Alessandro Gagliardi)
Date: Tue, 8 Aug 2006 20:38:59 -0400
Subject: [R] unable to restore saved data
Message-ID: <af2c8600608081738o4c8c8821t7c976d5f314ad088@mail.gmail.com>

Lately, when I try to open R I get the following error message:

Error: object 'time' not found whilst loading namespace 'tseries'
Fatal error: unable to restore saved data in .RData

If I rename .RData to RData.RData and then try opening R again it
works.  Then I can load("RData.RData") without a problem.  But if I
try saving my workspace (as the default, ".RData") and reload R it
crashes all over again.  I don't know how to get this 'time' object
back.  (I must have removed it by accident at some point.)  Any ideas?

Thanks in advance,
-- 
Alessandro Gagliardi
Integrative Neuroscience Program
Rutgers University Mind Brain Analysis
alessandro at gagliardi.name

"The opposite of a correct statement is a false statement.
But the opposite of a profound truth may well be another profound truth."
-Niels Bohr


From andy_liaw at merck.com  Wed Aug  9 05:37:09 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Aug 2006 23:37:09 -0400
Subject: [R] Frequency Distribution
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB293E@usctmx1106.merck.com>

You could just do table(cut(...)) and cumsum(table(cut(...))).  See the help
pages for those functions.

Example:

R> x <- rnorm(1e4)
R> breaks <- c(-Inf, -3:3, Inf)
R> table(cut(x, breaks))

(-Inf,-3]   (-3,-2]   (-2,-1]    (-1,0]     (0,1]     (1,2]     (2,3]  (3,
Inf] 
       16       253      1389      3349      3419      1339       220
15 
R> cumsum(table(cut(x, breaks)))
(-Inf,-3]   (-3,-2]   (-2,-1]    (-1,0]     (0,1]     (1,2]     (2,3]  (3,
Inf] 
       16       269      1658      5007      8426      9765      9985
10000 

Andy

From: Michael Zatorsky
> 
> Thankyou William.
> 
> I found the package and read through the documentation.  I'm 
> not a statistican, so it was largely over my head.  I was 
> looking for a command/function that described itself as 
> performing a frequency distribution, and could not find 
> anything obvious enough.
> 
> What did you have in mind in the package that you thought may help? 
> 
> All I'm looking to do is ask it to give me frequencies and 
> cumulative frequencies for the whole dataset, using intervale 
> widths of 100 or 1000 (in much the same way the data would 
> have to have been binned before producing a histogram.
> 
> 
> Regards
> Michael.
> 
> --- William Asquith <wasquith at austin.rr.com> wrote:
> 
> > You might be interested in the lmomco package that supports many 
> > nontraditional and traditional distributions.
> > 
> > William A.
> > 
> > 
> > On Aug 8, 2006, at 9:00 AM, Michael Zatorsky wrote:
> > 
> > > Hi,
> > >
> > > Could someone please suggest where I might find
> > some
> > > instructions / tutorials / FAQs that describe how
> > to
> > > create a frequency distribution and cumulative frequency 
> > > distribution in R using different class withs.
> > >
> > > I have about a 2-million observations (distances between points 
> > > ranging from sub-millimetre to
> > about
> > > 400km, and I want to get a feel for how they are distributed).
> > >
> > > I'd like the output as a table / data rather than
> > an
> > > graph.
> > >
> > > I've searched Google and R's help for obvious
> > terms,
> > > and while I've found much information on graphing/plotting, I 
> > > haven't hit on anything for
> > this.
> > >
> > > (I only downloaded R about 2 hours ago, apologies
> > if
> > > this is obviously documented somewhere I missed.)
> > >
> > > Regards
> > > Michael.
> > >
> > > Send instant messages to your online friends
> > http://
> > > au.messenger.yahoo.com
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-
> > > guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code.
> > 
> > 
> 
> 
> Send instant messages to your online friends 
> http://au.messenger.yahoo.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From andy_liaw at merck.com  Wed Aug  9 05:52:04 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Aug 2006 23:52:04 -0400
Subject: [R] boxplot
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB293F@usctmx1106.merck.com>

When I tried:

R> x <- c(5, rnorm(30))
R> boxplot(x)
R> identify(x)
[1] 1
(after I clicked on the obvious outlier, and R labeled it `1')

it seems to work just fine.  What do you mean by can't apply it boxplot?

Andy

  

From: Ana Patricia Martins
 
> Hello R-users and developers,
> 
>  
> 
> Once again, I'm asking for your help.
> 
> I've used "identify" to identify points in a scatter plot. 
> However, I can't apple in the boxplot.....
> 
>  
> 
> I need to identify the outlier's id in the boxplot. Can 
> anyone help me?
> 
>  
> 
> Thanks in advance,
> 
> Ana Patricia Martins
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From gyadav at ccilindia.co.in  Wed Aug  9 06:21:55 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 9 Aug 2006 09:51:55 +0530
Subject: [R] Spline Extrapolation(NURBS) 2nd Post
In-Reply-To: <OFCC229FB8.0A3BE570-ON652571C4.003C8FD7-652571AA.003D0094@ccil
	india.co.in>
Message-ID: <OF629979A6.2449255F-ON652571C5.0017A38F-652571AB.0018229E@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/a220ed33/attachment.pl 

From amstat2006 at gmail.com  Wed Aug  9 07:16:17 2006
From: amstat2006 at gmail.com (Am Stat)
Date: Wed, 9 Aug 2006 01:16:17 -0400
Subject: [R] How to draw the decision boundaries for LDA and Rpart object
Message-ID: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/5a7c816b/attachment.pl 

From revansx at cox.net  Wed Aug  9 07:19:55 2006
From: revansx at cox.net (Richard Evans)
Date: Wed, 9 Aug 2006 01:19:55 -0400
Subject: [R] debug print() commands not showing during file write loop
Message-ID: <000201c6bb73$7b105960$6601a8c0@revansx>

Hello,

I have a for loop that takes about and hour to complete each loop. I
added a "print()" command at the end of the looped code as a way to see
its progress, but the output is suppressed until the entire for-loop is
finished. why is that? can it be changed such that the print() output is
echoed to the screen when it is processed?

thanks in advance,
-rich


From spencer.graves at pdf.com  Wed Aug  9 07:31:49 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Aug 2006 22:31:49 -0700
Subject: [R] NLME: Problem with plotting ranef vs a factor
In-Reply-To: <009501c6b6e3$70d6baf0$6f179e89@UCTPCGREGD>
References: <009501c6b6e3$70d6baf0$6f179e89@UCTPCGREGD>
Message-ID: <44D97345.20901@pdf.com>

	  Your question is entirely too complex for me to try to answer in a 
reasonable amount of time, especially since your example in not self 
contained.

	  If you would still like help on this, I suggest you try to generate a 
self contained example that is as simple as you can make it that 
illustrates your problem, as suggested in the posting guide 
"www.R-project.org/posting-guide.html".  With only a modest amount of 
luck, the things you try to simplify your example will lead to 
enlightenment.  If that fails, please submit another question that is 
self contained, simple and clear.  Doing so should substantially 
increase your chances of getting a quick, useful reply.

	  I know this doesn't answer your question, but I hope it helps.

	  Spencer Graves

Greg Distiller wrote:
> Hi
> I am following the model building strategy that is outlined in the Pinheiro and Bates book wrt including covariates but am having a problem with the plot. Basically I am using 4 covariates (1 of them is continuous) and 3 of them are fine but the 4th one is being shown as a scatterplot despite the fact that it is a factor. I have explicitly declared this to be a factor (pcat<-as.factor(pcat)) and have also checked by using the "is.factor" and the "levels" command that it is a factor. Yet despite this the plot command is not recognising it as a factor.
>  
> Here is more information about my problem:
> 
> I am reading in the data by:
> 
> Data<-read.csv("Data1_93_2.csv",header=T)
> attach(Data)
> Data1_93<-transform(Data,log2game=log2(gamedens+1))
> pcat<-as.factor(pcat)
> Data1_93<-groupedData(log2game ~ day | subjectno, data=Data1_93)
> detach(Data)
> 
> Here is the code to check that the covariate called pcat is indeed a factor:
>> levels(pcat)
> [1] "1" "2" "3"
> 
>> is.factor(pcat)
> [1] TRUE
> 
> and then after the model is fitted I extract the random effects:
> 
> D1C2.ran <- ranef(mod11.103nlme,augFrame=T)
> 
> and here is an extract from the object:
> 
>                     C             R      day   gamedens pcat       site   mutcat1  pdens0  log2game
> NA02_259 -1.016987007  0.0162825099 15.75000   23.50000    1   Namaacha     Mixed   15018  3.761099
> NA02_073 -0.939355374  0.0132589702 10.50000   23.75000    1   Namaacha Resistant    6170  3.675543
> M00_12   -0.775048474  0.0047124742 10.50000   25.00000    1 Mpumulanga Sensitive   17525  3.768326
> M00_93   -0.555801118  0.0053872868 14.00000   37.50000    2 Mpumulanga Sensitive  332000  4.254319
> NA02_053 -0.327990343 -0.0037659864  6.00000   39.25000    1   Namaacha Resistant   65529  4.292481
> 
> Note that this output also seems to indicate that pcat is a factor as it is summarised correctly.
> 
> I then generate plots for my random effects:
> 
> plot(D1C2.ran,form= C ~site+mutcat2+pcat+pdens0)
> 
> and the problem is that the panel for my random effects vs pcat is displayed as a scatterplot rather than as a boxplot.
> I am getting told to check warnings and these warnings look like:
> 
> Warning messages:
> 1: at  0.99
> 2: radius  0.0001
> 3: all data on boundary of neighborhood. make span bigger
> 4: pseudoinverse used at 0.99
> 5: neighborhood radius 0.01
> 6: reciprocal condition number  -1.#IND
> 7: zero-width neighborhood. make span bigger
> 
> I do not get these warnings if I exclude the problematic variable pcat so must be something to do with this. Any ideas?
> 
> Many thanks
> 
> Greg
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vincent at 7d4.com  Wed Aug  9 08:12:15 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Wed, 09 Aug 2006 08:12:15 +0200
Subject: [R] debug print() commands not showing during file write loop
In-Reply-To: <000201c6bb73$7b105960$6601a8c0@revansx>
References: <000201c6bb73$7b105960$6601a8c0@revansx>
Message-ID: <44D97CBF.3050105@7d4.com>

Richard Evans a ?crit :

> can it be changed such that the print() output is
> echoed to the screen when it is processed?

?flush.console
hih


From ripley at stats.ox.ac.uk  Wed Aug  9 08:58:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Aug 2006 07:58:45 +0100 (BST)
Subject: [R] problem with tseries (was unable to restore saved data)
In-Reply-To: <af2c8600608081738o4c8c8821t7c976d5f314ad088@mail.gmail.com>
References: <af2c8600608081738o4c8c8821t7c976d5f314ad088@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608090742520.7250@gannet.stats.ox.ac.uk>

Hmm.  'time' is in package stats, and you seem to have saved an object 
that is pulling in the namespace 'tseries'.

Loading the workspace is done before loading the standard set of packages.
This appears to be a bug in tseries, for if its namespace needs 'time', it 
should import it from 'stats', and it is not even depending on 'stats'.

I can confirm that:

gannet% env R_DEFAULT_PACKAGES=NULL R
...
> loadNamespace("tseries")
Error: object 'time' not found whilst loading namespace 'tseries'

That is a matter for the 'tseries' maintainer (Cc:ed here).  If you
try library(tseries) at that point you find

> library(tseries)
Loading required package: zoo
Error: object 'aggregate' not found whilst loading namespace 'zoo'
Error: package 'zoo' could not be loaded

so package zoo has a similar problem (maintainer Cc:ed).


What can you do?  The simplest is to rename the workspace and load() 
afterwards as you have done.  But before saving, remove any objects which 
have a dependence on tseries.


On Tue, 8 Aug 2006, Alessandro Gagliardi wrote:

> Lately, when I try to open R I get the following error message:
> 
> Error: object 'time' not found whilst loading namespace 'tseries'
> Fatal error: unable to restore saved data in .RData
> 
> If I rename .RData to RData.RData and then try opening R again it
> works.  Then I can load("RData.RData") without a problem.  But if I
> try saving my workspace (as the default, ".RData") and reload R it
> crashes all over again.

R does NOT crash.  It is objecting (correctly) to your saved workspace.
Please see the posting guide, which specifically asked you not to abuse 
that word.

> I don't know how to get this 'time' object back.  (I must have removed 
> it by accident at some point.)  Any ideas?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Aug  9 09:04:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Aug 2006 08:04:55 +0100 (BST)
Subject: [R] How to draw the decision boundaries for LDA and Rpart object
In-Reply-To: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>
References: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>
Message-ID: <Pine.LNX.4.64.0608090800030.7250@gannet.stats.ox.ac.uk>

On Wed, 9 Aug 2006, Am Stat wrote:

> Hello useR,
> 
> Could you please tell me how to draw the decision boundaries in a 
> scatterplot of the original data for a LDA or Rpart object.

There are examples in MASS (the book).

> For example:
> > library(rpart)
> >fit.rpart <- rpart(as.factor(group.id)~., data=data.frame(Data) )
> 
> 
> How can I draw the cutting lines on the orignial Data?
> 
> Or is there any built in functions that can read the rpart object 
> 'fit.rpart' to do that?

See partition.tree in package tree.

> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mpiktas at gmail.com  Wed Aug  9 09:58:48 2006
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Wed, 9 Aug 2006 10:58:48 +0300
Subject: [R] nlme iteration process, few questions
Message-ID: <e47808320608090058n3c8be53ahcab9552345e0e91f@mail.gmail.com>

Hi all,

Recently I started using nlme intensively, and since it is all new for
me, I have some questions. I am running nlme with
control=list(verbose=TRUE) and during one lengthy fitting, I started
watching the output for some clues, how to speed up the process. I
noticed that in one case, the iteration process is alternating between
two solutions. Here is an output of 5 iterations, after which I
stopped the calculation:

**Iteration 22
LME step: Loglik: -1580.5 , nlm iterations: 50
reStruct  parameters:
        id1         id2         id3         id4         id5         id6
 -2.6387745   5.7964164   3.4153271   4.1734349  48.7541909  -0.1250377
        id7         id8         id9        id10
-36.7011651  30.1780697 -56.0266217 127.3234221

PNLS step: RSS =  137743.3
 fixed effects:24.0656  0.105708  -2.86501  0.542384
 iterations: 7

Convergence:
    fixed  reStruct
0.1353935 3.3655468

**Iteration 23
LME step: Loglik: -1579.346 , nlm iterations: 50
reStruct  parameters:
       id1        id2        id3        id4        id5        id6        id7
 -2.285671   6.380246   3.583532   4.248227  -1.398577  -1.318335 -47.773224
       id8        id9       id10
 12.834766 -63.669545 136.063383

PNLS step: RSS =  151492.2
 fixed effects:20.8075  0.105065  -2.85908  0.544615
 iterations: 7

Convergence:
    fixed  reStruct
0.1565794 1.5396836

**Iteration 24
LME step: Loglik: -1580.500 , nlm iterations: 50
reStruct  parameters:
        id1         id2         id3         id4         id5         id6
 -2.6387708   5.7964464   3.4153368   4.1734411  48.7509855  -0.1246523
        id7         id8         id9        id10
-36.7012641  30.1788064 -56.0254345 127.3242285

PNLS step: RSS =  137743.5
 fixed effects:24.0665  0.105706  -2.86503  0.542385
 iterations: 7

Convergence:
    fixed  reStruct
0.1354136 3.3647287

**Iteration 25
LME step: Loglik: -1579.346 , nlm iterations: 50
reStruct  parameters:
       id1        id2        id3        id4        id5        id6        id7
 -2.285674   6.380234   3.583527   4.248225  -1.399981  -1.318104 -47.772885
       id8        id9       id10
 12.835424 -63.669219 136.063111

PNLS step: RSS =  151495.6
 fixed effects:20.8062  0.105066  -2.85907  0.544616
 iterations: 7

Convergence:
    fixed  reStruct
0.1566962 1.5398072

**Iteration 26
LME step: Loglik: -1580.500 , nlm iterations: 50
reStruct  parameters:
        id1         id2         id3         id4         id5         id6
 -2.6387698   5.7964218   3.4153246   4.1734379  48.7501373  -0.1249832
        id7         id8         id9        id10
-36.7020319  30.1775554 -56.0315196 127.3225180

PNLS step: RSS =  137744.8
 fixed effects:24.066  0.105707  -2.86501  0.542384
 iterations: 7

Convergence:
    fixed  reStruct
0.1354528 3.3650687


Is there any particular strategy I can pursue in such case? Besides
the usual ones: changing the starting values, changing the model,
using different scaling. I am not familiar with inner workings of nlme
algorithm, maybe this an indication of some known problem?

Thanks for all answers.

Vaidotas Zemlys
--
Doctorate student, Vilnius University
http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php


From Daniel.Gutknecht at gmx.de  Wed Aug  9 10:01:06 2006
From: Daniel.Gutknecht at gmx.de (Daniel Gutknecht)
Date: Wed, 09 Aug 2006 10:01:06 +0200
Subject: [R] exponential proportional hazard model
Message-ID: <20060809080106.278270@gmx.net>

Dear R-users,

I am looking for a function designed to handle parametric proportional hazard models with a piecewise constant baseline hazard (i.e. dummies for annual intervals) and time-dependent covariates since I'm especially interested about the effect of those covariates on the baseline hazard.
I tried survreg() but this doesn't seem to work since my data looks like this

id/ start/ stop/ censoring/ covariates  
 1/   0/     5/     0/      ...
 1/   5/     8/     1/
 2/   0/     4/     0/

and survreg(Surv(start,stop,censoring)~covariates,...) always returns 'invalid survival type' (specifying the type="right" returns "wrong number of args...").
Am I missing something or is there a function that can handle this kind of data.
Thanks
Daniel   
--


From Bernhard_Pfaff at fra.invesco.com  Wed Aug  9 10:14:00 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Wed, 9 Aug 2006 09:14:00 +0100
Subject: [R] CRAN package: update of 'vars' submitted
Message-ID: <E4A9111DA23BA048B9A46686BF727CF40391EB@DEFRAXMB01.corp.amvescap.net>

Dear useR!

an updated version of package 'vars' has been shipped to CRAN lately.

Information on package 'vars':
==============================

Title:         VAR Modelling
Version:       0.1.3
Date:          2006-07-27
Author:        Bernhard Pfaff
Maintainer:    Bernhard Pfaff <bernhard at pfaffikus.de>
Depends:       R (>= 2.0.0), MASS, strucchange
Saveimage:     yes
Description:   Estimation, lag selection, diagnsotic testing,
               forecasting, causality analysis, forecast error variance
               decomposition and impulse response functions of VAR
               models and estimation of SVAR models (A-model, B-model,
               AB-model).
License:       GPL 2 or newer
URL:           http://www.pfaffikus.de


The package is shipped with a NAMESPACE and S3-classes/methods have been
employed.
It should be noted, that this package is still in its infancy, and more
features and functionalities are in the pipeline. Hence, I would
appreciate your feedback -- off list, adressed to the email adress in
the DESCRIPTION file.

Best,
Bernhard

Dr. Bernhard Pfaff
Global Structured Products Group
(Europe)

Invesco Asset Management Deutschland GmbH
Bleichstrasse 60-62
D-60313 Frankfurt am Main

Tel: +49(0)69 29807 230
Fax: +49(0)69 29807 178
Email: bernhard_pfaff at fra.invesco.com 
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From spencer.graves at pdf.com  Wed Aug  9 10:22:45 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Aug 2006 01:22:45 -0700
Subject: [R] RE
In-Reply-To: <44D18E90@webmail>
References: <44D18E90@webmail>
Message-ID: <44D99B55.5000106@pdf.com>

	  I've never used the 'bayesm' package, but your question sounded 
interesting and I've not seen a reply, so I decided to look at it. 
Unfortunately, I couldn't make sense of your example in the time 
available.  Instead, I will offer a few more general questions / 
suggests that I hope might help you.

	  1.  Have you worked through the example in the 'rmnlIndepMetrop' help 
file?

	  2.  Have you worked through the documentation in 
'~library\basesm\doc' subdirectory of your installed version of R?

	  3.  Have you tried the web site for the companion book, cited in the 
'References' to the 'rmnlIndepMetrop' help file?

	  If you've tried all these things and you would still like help from 
this listserve, I suggest you submit another post that is more 
consistent with the suggestions in the posting guide 
'www.R-project.org/posting-guide.html'.  This includes (a) using an 
informative subject (b) including the email address of the maintainer 
for the 'bayesm' package, listed in help(package='bayesm') in the 
distribution, and (c) providing commented, minimal, self-contained, 
reproducible code.

	  The example you provided was helpful be inadequate for someone like 
me to diagnose the problem in a reasonable period of time.  I suggest 
you include a call to 'set.seed' before your random number generation, 
and including attempted function calls to 'llmnl' and 'rmnlIndepMetrop' 
with comments explaining your confusion.

	  I know this doesn't answer your question, but I hope it helps.
	  Spencer Graves

nmi13 wrote:
> Hi any,
> 
> Can some please detail me the createX command in bayesm package?
> 
> To make things easy for you to help me, let me put forward my problem
> 
> Suppose I have 3 covariates (say X matrix) and my Y has 3 categories say 
> (1,2,3). Now from the CreateX I understand that the data matrix say 'Xa' must 
> be of dimension n* (naxp), where 'na' is the number of variables and 'p' is 
> the number of categories that Y has and 'n' is the number of observations. Now 
> the Xa matrix will have 9 columns if I give na=3 and p=3, and 6 columns if I 
> give na =2 and p=3. I understand this part. In order to create Xa with a 
> dimension of n*9 or n*6 we have to create Xa as cbind(Xa,-Xa) and now when I 
> get the design matrix say XD then I have 5 or 4 variables, which will be same 
> as the beta matrix that  I intend to get, I get this, but my question is when 
> I get the XD matrix as explained below the covariate matrix (X) which 
> initially had  3 columns now changed to a  9 rows and one column and two 
> additional variables X4 and X5 as explained belwo which I guess for the beta, 
> what is the role of these variables in the 'rmnlIndepMetrop'analysis.
> example
> x1<-runif(3,-1,1)
> x2<-runif(3,0,1)
> x3<-runif(3,10,50)
> X<-cbind(x1,x2,x3)
> X
>              x1        x2       x3
> [1,] -0.9701396 0.4084203 41.31097
> [2,]  0.3844539 0.4791997 36.85861
> [3,]  0.2732056 0.5433642 13.14610
> 
> Xa<-cbind(X,-X)
> XD<-createX(p=3,na=2,nd=NULL,Xa=Xa,Xd=NULL)
> XD
>       [,1] [,2]       [,3]        [,4]
>  [1,]    1    0 -0.9701396   0.9701396
>  [2,]    0    1  0.4084203  -0.4084203
>  [3,]    0    0 41.3109655 -41.3109655
>  [4,]    1    0  0.3844539  -0.3844539
>  [5,]    0    1  0.4791997  -0.4791997
>  [6,]    0    0 36.8586070 -36.8586070
>  [7,]    1    0  0.2732056  -0.2732056
>  [8,]    0    1  0.5433642  -0.5433642
>  [9,]    0    0 13.1461040 -13.1461040
> Xa<-cbind(X,-X,X^2) (is this a correct way)
> XD<-createX(p=3,na=3,nd=NULL,Xa=Xa,Xd=NULL)
> XD
>       [,1] [,2]       [,3]        [,4]         [,5]
>  [1,]    1    0 -0.9701396   0.9701396    0.9411709
>  [2,]    0    1  0.4084203  -0.4084203    0.1668071
>  [3,]    0    0 41.3109655 -41.3109655 1706.5958746
>  [4,]    1    0  0.3844539  -0.3844539    0.1478048
>  [5,]    0    1  0.4791997  -0.4791997    0.2296324
>  [6,]    0    0 36.8586070 -36.8586070 1358.5569127
>  [7,]    1    0  0.2732056  -0.2732056    0.0746413
>  [8,]    0    1  0.5433642  -0.5433642    0.2952447
>  [9,]    0    0 13.1461040 -13.1461040  172.8200512
> 
> In the above example my X matrix as you can see has 3 columns with 3 
> observations, which now in XD are 9 observations in 3rd column, I don't know 
> how col 4 and col 5 of XD play a role in computing the llmnl and 
> rmnlIndepMetrop.
> 
> Thanks for all your help and time.
> 
> Regards,
> Murthy.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From d.semmens at pgrad.unimelb.edu.au  Wed Aug  9 11:03:49 2006
From: d.semmens at pgrad.unimelb.edu.au (David Semmens)
Date: Wed, 09 Aug 2006 19:03:49 +1000
Subject: [R] nested ANOVA using lme
Message-ID: <d222dc863b4d64cd03d440853686523e@pgrad.unimelb.edu.au>

I have an ANOVA model with 2 factors "Environment" and "Site", 
"Diameter" is the response variable. Site should be nested within 
Environment. Site is also a random factor while Environment is fixed. I 
can do this analysis using the "aov" function by using these commands:

 >model<-aov(Diam~Env+Error(Env%in%Site),data=environ)
 >summary(model)
 >summary(aov(Diam~Env/Site,data=environ))

But the model is unbalanced and I want to calculate estimates of the 
variance components. In this case the REML approach should give more 
reliable estimates than the method I have used. But I am unable to work 
out how to specify a nested factor using the "lme" function. I have 
tried:

 >firstgo<-lme(Diam~Env+Env:Site,data=environ)
 >secondgo<-lme(Diam~Env+Env%in%Site,data=environ)
 >thirdgo<-lme(Diam~Env+Env/Site,data=environ)

But I keep getting this error:

Error in getGroups.data.frame(dataMix, groups) :
	Invalid formula for groups

If anyone could help me with code for specifying a random nested factor 
using lme that would be great.

Thanks,
David Semmens.


From msubianto at gmail.com  Wed Aug  9 11:17:30 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 9 Aug 2006 11:17:30 +0200
Subject: [R] Take random sample from class variable
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB65B@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB65B@usctmx1106.merck.com>
Message-ID: <c7c17cef0608090217n36b00cebkb7aff6ec9359615a@mail.gmail.com>

Dear all,
Andy, thanks you for your help and suggestions.
This is exactly what I was looking for.

Kindly regards, Muhammad Subianto


On 8/8/06, Liaw, Andy <andy_liaw at merck.com> wrote:
> There may be better ways, but this should work:
>
> R> p.yes <- 0.7
> R> n.yes <- rbinom(1, nof.sample, p.yes)
> R> n.no <- nof.sample - n.yes
> R> dat.yes <- mydat[sample(which(mydat$Class == "yes"), n.yes,
> replace=TRUE),]
> R> dat.no <- mydat[sample(which(mydat$Class == "no"), n.no, replace=TRUE),]
>
> You can rbind() them, and shuffle the rows if you wish.
>
> Andy
>
> From: Muhammad Subianto
> >
> > Dear all,
> > Suppose I have a dataset like below, then I take for example,
> > 100 random sample "class" variable where contains "yes" and "no"
> > respectively, 70% and 30%.
> > I need a new 100 random sample from mydat dataset, but I
> > can't get the result.
> > Thanks you very much for any helps.
> > Best, Muhammad Subianto
> >
> > mydat <- data.frame(size=c(30,12,15,10,12,12,25,30,20,14),
> >                        A=c(0,1,0,1,0,1,1,1,0,0),
> >                        B=c(1,1,0,1,0,1,1,0,0,1),
> >                        C=c(0,0,1,1,0,0,1,1,0,0),
> >                        D=c(1,1,1,1,0,1,0,0,1,1),
> >                        E=c(1,1,0,1,1,1,1,1,1,0),
> >
> > Class=c("yes","yes","no","yes","yes","no","yes","no","yes","yes"))
> > mydat
> > # Maximal data from dataset
> > max.size <- sum(mydat$size);max.size
> > # I need sample random
> > nof.sample <- 100
> > set.seed(123)
> > sample.class <- sample(c("yes","no"), nof.sample, prob=c(.7,
> > .3), replace=TRUE) sample.class sampledat.class <-
> > mydat[sample.class,] sampledat.class
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug  9 11:24:42 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 9 Aug 2006 11:24:42 +0200
Subject: [R] nested ANOVA using lme
References: <d222dc863b4d64cd03d440853686523e@pgrad.unimelb.edu.au>
Message-ID: <00b001c6bb95$a5ab3560$0540210a@www.domain>

in lme() you need to specify the random-effects with a formula passed 
to the 'random' argument, e.g., something like

fit <- lme(Diam ~ Env + Site, random = ~ 1 | Env / Site, data = 
environ)
summary(fit)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "David Semmens" <d.semmens at pgrad.unimelb.edu.au>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 09, 2006 11:03 AM
Subject: [R] nested ANOVA using lme


>I have an ANOVA model with 2 factors "Environment" and "Site",
> "Diameter" is the response variable. Site should be nested within
> Environment. Site is also a random factor while Environment is 
> fixed. I
> can do this analysis using the "aov" function by using these 
> commands:
>
> >model<-aov(Diam~Env+Error(Env%in%Site),data=environ)
> >summary(model)
> >summary(aov(Diam~Env/Site,data=environ))
>
> But the model is unbalanced and I want to calculate estimates of the
> variance components. In this case the REML approach should give more
> reliable estimates than the method I have used. But I am unable to 
> work
> out how to specify a nested factor using the "lme" function. I have
> tried:
>
> >firstgo<-lme(Diam~Env+Env:Site,data=environ)
> >secondgo<-lme(Diam~Env+Env%in%Site,data=environ)
> >thirdgo<-lme(Diam~Env+Env/Site,data=environ)
>
> But I keep getting this error:
>
> Error in getGroups.data.frame(dataMix, groups) :
> Invalid formula for groups
>
> If anyone could help me with code for specifying a random nested 
> factor
> using lme that would be great.
>
> Thanks,
> David Semmens.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From maechler at stat.math.ethz.ch  Wed Aug  9 11:25:09 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 9 Aug 2006 11:25:09 +0200
Subject: [R] locating intervals
In-Reply-To: <696224530608080947x77afb1adl4b49fc2fff3ce529@mail.gmail.com>
References: <696224530608080947x77afb1adl4b49fc2fff3ce529@mail.gmail.com>
Message-ID: <17625.43509.689323.838196@stat.math.ethz.ch>

I'm pretty sure you want to use

findInterval() 

Why did you not find it?
In other words,  what did you try to find it?

Regards,
Martin Maechler, ETH Zurich

>>>>> "halldor" == halldor bjornsson <halldor.bjornsson at gmail.com>
>>>>>     on Tue, 8 Aug 2006 16:47:18 +0000 writes:

    halldor> Hi , I have two sorted vectors X and Xi, where the
    halldor> range of Xi lies within the range of X.

    halldor> For an element in Xi, I want to find the
    halldor> neigbouring data in X, e.g. find an index ix so
    halldor> that for element number k, then X[ix[k]] < X[k] <
    halldor> X[ix[k] +1] # also OK with "<=" on either one, but
    halldor> not both

    halldor> This is easy to code by looping over the data in
    halldor> X,Xi, but I suspect there may be a faster and more
    halldor> elegant way to do this in R.

    halldor> In Python (Numeric) the same can be achieved with
    halldor> ix=Numeric.searchsorted(X[1:-1],Xi), which is quite
    halldor> compact.

    halldor> So, does anyone know of a corresponding R call that
    halldor> can achive the same?

    halldor> Sincerely, Halld?r

    halldor> -- Halldor Bjornsson Weatherservice R & D Icelandic
    halldor> Met. Office


From john.janmaat at acadiau.ca  Wed Aug  9 11:35:24 2006
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Wed,  9 Aug 2006 06:35:24 -0300
Subject: [R] NLS and IV
Message-ID: <1155116124.44d9ac5ca4f33@webmail.acadiau.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/de339b59/attachment.pl 

From laurent.deniau at cern.ch  Wed Aug  9 11:39:13 2006
From: laurent.deniau at cern.ch (Laurent Deniau)
Date: Wed, 09 Aug 2006 11:39:13 +0200
Subject: [R] prefixing list names in print
In-Reply-To: <8d5a36350608081327r3f681aaft5b4e414cbc64fd75@mail.gmail.com>
References: <8d5a36350608081327r3f681aaft5b4e414cbc64fd75@mail.gmail.com>
Message-ID: <44D9AD41.4040604@cern.ch>

bogdan romocea wrote:
> A simple function will do what you want, customize this as needed:
> lprint <- function(lst,prefix)
> {
> for (i in 1:length(lst)) {
>   cat(paste(prefix,"$",names(lst)[i],sep=""),"\n")
>   print(lst[[i]])
>   cat("\n")
> }
> }
> P <- list(A="a",B="b")
> lprint(P,"Prefix")

I thought that there is a way to 'setup' print.

Thanks.

ld.


From demirtas at uic.edu  Wed Aug  9 12:39:19 2006
From: demirtas at uic.edu (Demirtas, Hakan)
Date: Wed, 9 Aug 2006 05:39:19 -0500 (CDT)
Subject: [R] solving nonlinear equations in R
Message-ID: <1232.75.4.39.137.1155119959.squirrel@webmail.uic.edu>

Hi, I can't seem to get computationally stable estimates for the following
system:

Y=a+bX+cX^2+dX^3, where X~N(0,1). (Y is expressed as a linear combination
of the first three powers of a standard normal variable.) Assuming that
E(Y)=0 and Var(Y)=1, one can obtain the following equations after tedious
algebraic calculations:

1) b^2+6bd+2c^2+15d^2=1
2) 2c(b^2+24bd+105d^2+2)=E(Y^3)
3) 24[bd+c^2(1+b^2+28bd)+d^2(12+48bd+141c^2+225d^2)]=E(Y^4)-3

Obviously, a=-c. Suppose that distributional form of Y is given so we know
E(Y^3) and E(Y^4). In other words, we have access to the third and fourth
raw moments. How do we solve for these four coefficients? I reduced the
number of unknowns/equations to two, and subsequently used a grid
approach. It works well when I am close to the center of the support, but
fails miserably at the tails. Any ideas? Hopefully, there is a nice R
function that does this.

Hakan Demirtas


From kennedy_david at bah.com  Wed Aug  9 12:39:11 2006
From: kennedy_david at bah.com (Kennedy David)
Date: Wed, 9 Aug 2006 06:39:11 -0400
Subject: [R] Joint confidence intervals for GLS models?
Message-ID: <5BBCB098FE36644C90E7836C296362880244FCAA@MCLNEXVS03.resource.ds.bah.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/709ece7d/attachment.pl 

From adi at roda.ro  Wed Aug  9 12:46:55 2006
From: adi at roda.ro (Adrian Dusa)
Date: Wed, 9 Aug 2006 13:46:55 +0300
Subject: [R] missing documentation entries
Message-ID: <200608091346.55313.adi@roda.ro>


Dear list,

When creating a package, there are always many little utility functions that 
belong to the "internal kitchen" of the main, documented functions.
Now, when checking the sources with R CMD check, I get a warning for those 
little functions that are not documented.
I would have two questions:
- is it mandatory to document _all_ functions (will the source package be 
rejected by CRAN if otherwise)?
- if not, is there a way to tell R which are the functions that I don't want 
to document?

Thanks,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From hkag at novonordisk.com  Wed Aug  9 13:14:23 2006
From: hkag at novonordisk.com (=?iso-8859-1?Q?HKAG_=28Henrik_Agers=F8=29?=)
Date: Wed, 9 Aug 2006 13:14:23 +0200
Subject: [R] legend on trellis plot
Message-ID: <9E5264DDB6137048AC96028B0DEE553E013543D7@EXDKBA021.corp.novocorp.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/b02300f2/attachment.pl 

From spencer.graves at pdf.com  Wed Aug  9 13:32:17 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Aug 2006 04:32:17 -0700
Subject: [R] between-within anova: aov and lme
In-Reply-To: <33271.131.136.242.1.1154606478.squirrel@webmail.utsc.utoronto.ca>
References: <33271.131.136.242.1.1154606478.squirrel@webmail.utsc.utoronto.ca>
Message-ID: <44D9C7C1.1030608@pdf.com>

	  I can't answer your question about 'aov', but have you tried the 
following with 'lme':

	  lme(response~A*B*C,random=~1|subject)

	  This assumes that A, B, and C are fixed effects, either continuous 
variables or factors present at only a very few levels whose effects are 
not reasonably modeled as a random sample from some other distribution. 
  It also assumes that the effect of each level of subject can be 
reasonable modeled as a random adjustment to the intercept following a 
common distribution with mean 0 and variance = 'var.subj'.

	   The function 'aov' is old and mostly obsoleted by 'nlme'.  There may 
be things that can be done in 'aov' that can not be done more or less as 
easily and usually better and more generally with 'lme', but I'm not 
familiar with such cases.

	  Your question suggests you may not be familiar with Pinheiro and 
Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).  The 
standard R distribution comes with a directory "~library\nlme\scripts" 
containing script files 'ch01.R', 'ch02.R', ..., 'ch06.R', and 'ch08.R'. 
  These contain R script files with the R code for each chapter in the 
book.  I've learned a lot from walking through the script files line by 
line while reviewing the corresponding text in the book.  Doing so 
protects me from problems with silly typographical errors as well as 
subtle problems where the S-Plus syntax in the book gives a different 
answer in R because of the few differences in the syntax between S-Plus 
and R.

	  Hope this helps.
	  Spencer Graves

William Simpson wrote:
> I have 2 questions on ANOVA with 1 between subjects factor and 2 within factors.
> 
> 1. I am confused on how to do the analysis with aov because I have seen two examples
> on the web with different solutions.
> 
> a) Jon Baron (http://www.psych.upenn.edu/~baron/rpsych/rpsych.html) does
> 6.8.5 Example 5: Stevens pp. 468 - 474 (one between, two within)
> 
> between: gp
> within: drug, dose
> aov(effect ~ gp * drug * dose + Error(subj/(dose*drug)), data=Ela.uni)
> 
> b) Bill Venables answered a question on R help as follows.
> 
> - factor A between subjects
> - factors B*C within subjects.
> 
> aov(response ~ A*B*C + Error(subject), Kirk)
> "An alternative formula would be response ~ A/(B*C) + Error(subject), which
> would only change things by grouping together some of the sums of squares."
> 
> -------------------------------------------------------
> SO: which should I do????
> aov(response ~ A*B*C + Error(subject), Kirk)
> aov(response ~ A/(B*C) + Error(subject), Kirk)
> aov(response ~ A*B*C + Error(subject/(B*C)), Kirk)
> --------------------------------------------------------
> 
> 2. How would I do the analysis in lme()?
> Something like
> lme(response~A*B*C,random=~1|subject/(B*C))???
> 
> 
> Thanks very much for any help!
> Bill Simpson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Aug  9 13:37:18 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Aug 2006 12:37:18 +0100 (BST)
Subject: [R] missing documentation entries
In-Reply-To: <200608091346.55313.adi@roda.ro>
References: <200608091346.55313.adi@roda.ro>
Message-ID: <Pine.LNX.4.64.0608091232390.10598@gannet.stats.ox.ac.uk>

This is discussed in `Writing R Extensions', which both points you to the 
'internal' keyword, and (in the current version) mentions using name 
spaces to hide such functions.

This really was a question for R-devel: please do study the posting guide. 

  `R-devel is intended for questions and discussion about code development 
  in R.'


On Wed, 9 Aug 2006, Adrian Dusa wrote:

> 
> Dear list,
> 
> When creating a package, there are always many little utility functions that 
> belong to the "internal kitchen" of the main, documented functions.
> Now, when checking the sources with R CMD check, I get a warning for those 
> little functions that are not documented.
> I would have two questions:
> - is it mandatory to document _all_ functions (will the source package be 
> rejected by CRAN if otherwise)?
> - if not, is there a way to tell R which are the functions that I don't want 
> to document?
> 
> Thanks,
> Adrian
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Wed Aug  9 13:54:24 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Aug 2006 07:54:24 -0400
Subject: [R] legend on trellis plot
In-Reply-To: <9E5264DDB6137048AC96028B0DEE553E013543D7@EXDKBA021.corp.novocorp.net>
References: <9E5264DDB6137048AC96028B0DEE553E013543D7@EXDKBA021.corp.novocorp.net>
Message-ID: <971536df0608090454p30323c60x581d2b5194ba3330@mail.gmail.com>

1. Use the x, y and corner components to the key= list to specify
    the legend position, and
2. pass the panel.number in the panel function and test that as shown
    in the panel function below.
    Alternately you can place the horizontal line on afterwards using
    trellis.focus/trellis.unfocus as shown below.

Read the material under key= and panel= in ?xyplot for more information
on the key and panel arguments and read ?trellis.focus for more
information on trellis.focus/trellis.unfocus.


xyplot(DV~TIME | DOSE, data=data, groups=ID, layout=c(2,1),
      key=list(x=.1,y=.8,corner=c(0,0),border=TRUE,colums=2,text=list(c("ID1","ID2"),col=c(1,4)),
             lines=list(type="o",pch=c(1,16),lty=c(1,2), col=c(1,4)),
             layout.heights=list(key.axis.padding=15)),

      panel = function(x,y,groups,...,panel.number) {
                panel.superpose.2(x,y,groups,...,type="o",pch=c(1,16),
                lty=c(1,2), col=c(1,4), cex=0.8)
	if (panel.number == 1) panel.abline(h=0.301,col=5,lty=1,lwd=2)
   }
)


# add a red horizontal line only to panel 2, 1
    trellis.focus("panel", 2, 1, highlight = FALSE)
    panel.abline(h=0.301,col=2,lty=1,lwd=2)
    trellis.unfocus()


On 8/9/06, HKAG (Henrik Agers?) <hkag at novonordisk.com> wrote:
>
> Dear all
>
> I have two questions regarding trellis plots - which I hope you may be able to help me with.
>
> Is it possible to place the key in a trellis plot on the panel (instead of beside the panel)? This will cause the same key to be reproduced on each panel. Please see the plot below - here I placed the legend below the plot. I tried moving the key to the function statement, but it did not really work out the way I expected.
>
> One last thing, in the plot below I placed a horizontal line on the plot, is it possible to only have the horizontal line on the left panel (I remember that in S it was possible to state something like "if(get("cell",fr=9)==2)" in the function statement to include the line on only one of the panels)?
>
> All suggestions will highly appreciated.
>
> Br Henrik
>
>
>
> ###############################
>
> data           <- as.data.frame(cbind(rep(1:4,each=25),
>                  rep(1:2,each=50) ,rep(1:25,4),
>                  rnorm(100,0,1) ))
> names(data)   <- c("ID","DOSE","TIME","DV")
>
>
>
> xyplot(DV~TIME | DOSE, data=data, groups=ID, layout=c(2,1),
>
>       key=list(space="bottom",border=TRUE,colums=2,text=list(c("ID1","ID2"),col=c(1,4)),
>              lines=list(type="o",pch=c(1,16),lty=c(1,2), col=c(1,4)),
>              layout.heights=list(key.axis.padding=15)),
>
>       panel = function(x,y,groups,...) {
>                 panel.superpose.2(x,y,groups,...,type="o",pch=c(1,16),
>                 lty=c(1,2), col=c(1,4), cex=0.8)
>                 panel.abline(h=0.301,col=5,lty=1,lwd=2)
>    }
> )
>
> ###############################
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Wed Aug  9 13:59:02 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 9 Aug 2006 07:59:02 -0400 (EDT)
Subject: [R] Getting data out of a loop
In-Reply-To: <BAY112-F317A963CB9EE3E08486F95AF540@phx.gbl>
Message-ID: <20060809115902.61147.qmail@web32801.mail.mud.yahoo.com>

My thanks to, in order of the postings appearance at
my reader, Jim Holtman, Andrew Robinson & Neuro
LeSuperH?ros for thre different and very useful
solutions to my problem.  Your help is greatly
appreciated.



--- Neuro LeSuperH?ros <neuro3000 at hotmail.com> wrote:

> I'd create an empty dataframe prior to the loop.
> 
> cata <- c( 3,5,6,8,0, NA)
> catb <- c( 1,2,3,4,5,6)
> doga <- c(3,5,3,6,4, 0)
> dogb <- c(2,4,6,8,10, 12)
> rata <- c (NA, 5, 5, 4, 9, 0)
> ratb <- c( 1,2,3,4,5,6)
> bata <- c( 12, 42,NA, 45, 32, 54)
> batb <- c( 13, 15, 17,19,21,23)
> id <- c('a', 'b', 'b', 'c', 'a', 'b')
> site <- c(1,1,4,4,1,4)
> mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> bata, batb)
> Df <- data.frame(site, id, mat1)
> nn <- levels(Df$id)
> 
> Df
> nn
> rate <- c(2,3,4)
> 
> Result <- data.frame(matrix(NA,length(nn),4))
> for (i in 1: length(nn)) {
> dd<- subset(Df, id==nn[i])
> scat <- sum(c(dd$cata,dd$catb), na.rm=T)
> sdog <- sum(c(dd$doga,dd$dogb), na.rm=T)
> srat <- sum(c(dd$rata, dd$ratb), na.rm=T)
> sbat <- sum(c(dd$bata,dd$batb), na.rm=T)
> sss <- c(scat,sdog, srat,sbat) * rate[i]
> Result[i,] <- sss
> print(sss)
> }
> Result
> 
> 
> 
> >From: John Kane <jrkrideau at yahoo.ca>
> >To: R R-help <r-help at stat.math.ethz.ch>
> >Subject: [R] Getting data out of a loop
> >Date: Tue, 8 Aug 2006 18:00:23 -0400 (EDT)
> >
> >A stupid question but I just cannot see how to do
> >this.
> >
> >I have a loop that does some calculations and puts
> the
> >results in a vector for each iteration, but I
> cannot
> >see how to get the data out of the loop in such a
> way
> >that I can use it.  I can print it but how do I get
> it
> >into a set of vectors or what ever.
> >
> >Any help gratefully received.  Thanks
> >
> >Example
> >
> >cata <- c( 3,5,6,8,0, NA)
> >catb <- c( 1,2,3,4,5,6)
> >doga <- c(3,5,3,6,4, 0)
> >dogb <- c(2,4,6,8,10, 12)
> >rata <- c (NA, 5, 5, 4, 9, 0)
> >ratb <- c( 1,2,3,4,5,6)
> >bata <- c( 12, 42,NA, 45, 32, 54)
> >batb <- c( 13, 15, 17,19,21,23)
> >id <- Cs(a,b,b,c,a,b)
> >site <- c(1,1,4,4,1,4)
> >mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> >bata, batb)
> >Df <- data.frame(site, id, mat1)
> >nn <- levels(Df$id)
> >
> >Df
> >nn
> >rate <- c(2,3,4)
> >
> >for (i in 1: length(nn)) {
> >dd<- subset(Df, id==nn[i])
> >scat <- sum(c(dd$cata,dd$catb), na.rm=T)
> >sdog <- sum(c(dd$doga,dd$dogb), na.rm=T)
> >srat <- sum(c(dd$rata, dd$ratb), na.rm=T)
> >sbat <- sum(c(dd$bata,dd$batb), na.rm=T)
> >sss <- c(scat,sdog, srat,sbat) * rate[i]
> >print(sss)
> >}
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide 
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained,
> reproducible code.
> 
> 
>


From dusa.adrian at gmail.com  Wed Aug  9 14:27:06 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 9 Aug 2006 15:27:06 +0300
Subject: [R] missing documentation entries
In-Reply-To: <Pine.LNX.4.64.0608091232390.10598@gannet.stats.ox.ac.uk>
References: <200608091346.55313.adi@roda.ro>
	<Pine.LNX.4.64.0608091232390.10598@gannet.stats.ox.ac.uk>
Message-ID: <200608091527.07022.dusa.adrian@gmail.com>

On Wednesday 09 August 2006 14:37, Prof Brian Ripley wrote:
> This is discussed in `Writing R Extensions', which both points you to the
> 'internal' keyword, and (in the current version) mentions using name
> spaces to hide such functions.
>
> This really was a question for R-devel: please do study the posting guide.
>
>   `R-devel is intended for questions and discussion about code development
>   in R.'

Thank you very much for your reply, I'll post to R-devel from now on.
It seems to me that name spaces are the solution for my problem.
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From christian.miehle at wiwi.uni-augsburg.de  Wed Aug  9 14:24:56 2006
From: christian.miehle at wiwi.uni-augsburg.de (Christian Miehle)
Date: Wed, 9 Aug 2006 14:24:56 +0200
Subject: [R] evolutionary computing in R
Message-ID: <04df01c6bbae$d2d07be0$732cfa89@wiwi.uniaugsburg.de>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/7c586020/attachment.pl 

From ggrothendieck at gmail.com  Wed Aug  9 14:30:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Aug 2006 08:30:23 -0400
Subject: [R] evolutionary computing in R
In-Reply-To: <04df01c6bbae$d2d07be0$732cfa89@wiwi.uniaugsburg.de>
References: <04df01c6bbae$d2d07be0$732cfa89@wiwi.uniaugsburg.de>
Message-ID: <971536df0608090530v46e7d554j424dfa1b65b72dc3@mail.gmail.com>

Check out the machining learning task view at:

http://cran.r-project.org/src/contrib/Views/

On 8/9/06, Christian Miehle <christian.miehle at wiwi.uni-augsburg.de> wrote:
> Hallo,
>
> Ich bin auf der Suche nach umgesetzten evolution?ren Algorithmen in R. Leider habe ich kein entsprechendes Package oder Funktionen dieser Verfahrensgruppe gefunden. Wei? zuf?llig jemand, ob Funktionen oder Pakete zu dieser Problematik in R existieren?
>
> Vielen Dank,
> Christian
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From Rau at demogr.mpg.de  Wed Aug  9 14:35:12 2006
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 9 Aug 2006 14:35:12 +0200
Subject: [R] evolutionary computing in R
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6013DB605@HERMES.demogr.mpg.de>

Hi Christian, 

the language spoken on this mailing list is English not German.

As far as I know there is one package called "gafit" which is intended for...
library(gafit)
?gafit
...Genetic Algorithm for Curve Fitting

Is this something you are looking for?

For non-German speakers who are probably the majority on this list: The original poster asked whether there are any functions or packages which have evolutionary algorithms implemented.


Best,
Roland


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christian Miehle
> Sent: Wednesday, August 09, 2006 2:25 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] evolutionary computing in R
> 
> Hallo,
> 
> Ich bin auf der Suche nach umgesetzten evolution?ren 
> Algorithmen in R. Leider habe ich kein entsprechendes Package 
> oder Funktionen dieser Verfahrensgruppe gefunden. Wei? 
> zuf?llig jemand, ob Funktionen oder Pakete zu dieser 
> Problematik in R existieren?
> 
> Vielen Dank,
> Christian
> 
> 	[[alternative HTML version deleted]]
> 
> 

----------
This mail has been sent through the MPI for Demographic Rese...{{dropped}}


From s.ruegg at access.unizh.ch  Wed Aug  9 14:52:23 2006
From: s.ruegg at access.unizh.ch (Simon Ruegg)
Date: Wed, 9 Aug 2006 14:52:23 +0200
Subject: [R] scaling constant in optim("L-BFGS-B")
Message-ID: <200608091252.k79CqOrD007142@idmailgate2.unizh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/80b43551/attachment.pl 

From highstat at highstat.com  Wed Aug  9 15:02:14 2006
From: highstat at highstat.com (Highland Statistics Ltd.)
Date: Wed, 09 Aug 2006 14:02:14 +0100
Subject: [R] 2 places on R course
Message-ID: <7.0.0.16.0.20060809140158.01d42db0@highstat.com>

Apologies for cross-posting

There are two places available on a 3-day R course in Newburgh, UK 
(28-30 August 2006).

Full details can be found at: www.brodgar.com



Kind regards,

Alain Zuur
Highland Statistics Ltd.


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug  9 15:10:14 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 9 Aug 2006 15:10:14 +0200
Subject: [R] scaling constant in optim("L-BFGS-B")
References: <200608091252.k79CqOrD007142@idmailgate2.unizh.ch>
Message-ID: <002b01c6bbb5$26e75f90$0540210a@www.domain>

maybe it would be better to scale the parameters (using 'parscale') 
instead of NLL.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Simon Ruegg" <s.ruegg at access.unizh.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 09, 2006 2:52 PM
Subject: [R] scaling constant in optim("L-BFGS-B")


> Hi all,
>
>
>
> I am trying to find estimates for 7 parameters of a model which 
> should fit
> real data. I have a function for the negative log likelihood (NLL) 
> of the
> data. With optim(method="L-BFGS-B",lower=0) I am now minimizing the 
> NLL to
> find the best fitting parameters.
>
>
>
> My problem is that the algorithm does not converge for certain data 
> sets. I
> have read that one should scale the fn (i.e. the NLL in my case), 
> however I
> am having trouble with the scaling constant: If I change it, the 
> algorithm
> converges for certain data sets, for which it didn't before, but for 
> others
> it doesn't converge although it did before. In addition, the scaling
> constant affects the value of the optimal parameters and the 
> converging
> value of the NLL (evidently). So to be able to compare the 
> parameters
> between different data sets I need to use the same scaling constant. 
> Trying
> out all values between 0.1 and 1 is very laborious and is not quite 
> a
> systematic approach.
>
>
>
> My question is: Are there any rules of thumb to choose a scaling 
> constant?
> And how do I justify it's application (it looks a bit like a "magic
> constant" that tricks the algo to converge, but does not have a 
> systematic
> justification)?
>
>
>
> I'd greatly appreciate any hints, tricks or references about the 
> scaling
> constant
>
>
>
> Thanks for your help!
>
>
>
> Simon
>
>
>
> ********************************************************************
>
> Simon Ruegg
>
> Dr.med.vet.,  PhD student
>
> Institute for Parasitology
>
> Winterthurstr. 266a
>
> 8057 Zurich
>
> Switzerland
>
>
>
> phone: +41 44 635 85 93
>
> fax: +41 44 635 89 07
>
> e-mail: s.ruegg at access.unizh.ch
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From kubovy at virginia.edu  Wed Aug  9 15:19:31 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 9 Aug 2006 09:19:31 -0400
Subject: [R] R2HTML: request for an extended example
Message-ID: <F061CE75-17E6-4647-8613-2DA559B49B3B@virginia.edu>

Dear R-helpers,

If you know of an extended example of the use of R2HTML, in which the  
various constructs are present in one place, could you please point  
me to it or send it to me?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From tobias.verbeke at telenet.be  Wed Aug  9 15:23:06 2006
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Wed, 09 Aug 2006 15:23:06 +0200
Subject: [R] R2HTML: request for an extended example
In-Reply-To: <F061CE75-17E6-4647-8613-2DA559B49B3B@virginia.edu>
References: <F061CE75-17E6-4647-8613-2DA559B49B3B@virginia.edu>
Message-ID: <44D9E1BA.7010808@telenet.be>

Michael Kubovy wrote:
> Dear R-helpers,
>
> If you know of an extended example of the use of R2HTML, in which the  
> various constructs are present in one place, could you please point  
> me to it or send it to me?
>   
There was an article in the R News of December 2003

http://cran.r-project.org/doc/Rnews/

HTH,
Tobias

> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From lami at faunalia.it  Wed Aug  9 15:32:41 2006
From: lami at faunalia.it (Leonardo Lami)
Date: Wed, 9 Aug 2006 15:32:41 +0200
Subject: [R] data.frame to shape
Message-ID: <200608091532.42647.lami@faunalia.it>

Hi all,
I have a simple question:
I have a data.frame like this:

   id       x       y
1  50 1647685 4815259
2  50 1647546 4815196
3  50 1647454 4815294
4  50 1647405 4815347
5  50 1647292 4815552
6 50 1647737 4815410
7 74 1647555 4815201
8 74 1647464 4815023
9 74 1646970 4815129
10 74 1646895 4815264
11 74 1646762 4815513

and I'd like to trasform it with the "convert.to.shapefile" function 
(shapefiles package) but to make this I must have a data.frame like this:

   id       x       y
1  50 1647685 4815259
2  50 1647546 4815196
3  50 1647454 4815294
4  50 1647405 4815347
5  50 1647292 4815552
6  50 1647737 4815410
7  50 1647685 4815259
8  74 1647555 4815201
9  74 1647464 4815023
10 74 1646970 4815129
11 74 1646895 4815264
12 74 1646762 4815513
13 74 1646762 4815513

with the first point of every id repeated to close the polygon.
There is a function to make this indipendently by the number of the id

Best regards
Leonardo

-- 
Leonardo Lami
email + jabber: lami at faunalia.it
www.faunalia.it
Cell: (+39)349-1310164  Tel+Fax: (+39) 0587-213742
Piazza Garibaldi 5 - 56025 Pontedera (PI), Italy


From Stefano.Guazzetti at ausl.re.it  Wed Aug  9 16:00:38 2006
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Wed, 9 Aug 2006 16:00:38 +0200
Subject: [R] R:  data.frame to shape
Message-ID: <B8A1EED732379B44A7E59D22E82E4442020D6BF6@IMHOTEP.ausl.org>

assuming that the rows are sorted correctly
dat
   id       x       y
1  50 1647685 4815259
2  50 1647546 4815196
3  50 1647454 4815294
4  50 1647405 4815347
5  50 1647292 4815552
6  50 1647737 4815410
7  74 1647555 4815201
8  74 1647464 4815023
9  74 1646970 4815129
10 74 1646895 4815264
11 74 1646762 4815513

> list.dat<-split(dat, dat$id)
> 
>  closed.polygons<-lapply(list.dat, function(x) rbind(x, x[1,]))
> 
>  do.call("rbind", closed.polygons)
      id       x       y
50.1  50 1647685 4815259
50.2  50 1647546 4815196
50.3  50 1647454 4815294
50.4  50 1647405 4815347
50.5  50 1647292 4815552
50.6  50 1647737 4815410
50.11 50 1647685 4815259
74.7  74 1647555 4815201
74.8  74 1647464 4815023
74.9  74 1646970 4815129
74.10 74 1646895 4815264
74.11 74 1646762 4815513
74.71 74 1647555 4815201

but maybe there are better ways to do what you want


   >-----Messaggio originale-----
   >Da: r-help-bounces at stat.math.ethz.ch
   >[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Leonardo Lami
   >Inviato: 09 August, 2006 15:33
   >A: r-help at stat.math.ethz.ch
   >Oggetto: [R] data.frame to shape
   >
   >
   >Hi all,
   >I have a simple question:
   >I have a data.frame like this:
   >
   >   id       x       y
   >1  50 1647685 4815259
   >2  50 1647546 4815196
   >3  50 1647454 4815294
   >4  50 1647405 4815347
   >5  50 1647292 4815552
   >6 50 1647737 4815410
   >7 74 1647555 4815201
   >8 74 1647464 4815023
   >9 74 1646970 4815129
   >10 74 1646895 4815264
   >11 74 1646762 4815513
   >
   >and I'd like to trasform it with the "convert.to.shapefile" 
   >function 
   >(shapefiles package) but to make this I must have a 
   >data.frame like this:
   >
   >   id       x       y
   >1  50 1647685 4815259
   >2  50 1647546 4815196
   >3  50 1647454 4815294
   >4  50 1647405 4815347
   >5  50 1647292 4815552
   >6  50 1647737 4815410
   >7  50 1647685 4815259
   >8  74 1647555 4815201
   >9  74 1647464 4815023
   >10 74 1646970 4815129
   >11 74 1646895 4815264
   >12 74 1646762 4815513
   >13 74 1646762 4815513
   >
   >with the first point of every id repeated to close the polygon.
   >There is a function to make this indipendently by the 
   >number of the id
   >
   >Best regards
   >Leonardo
   >
   >-- 
   >Leonardo Lami
   >email + jabber: lami at faunalia.it
   >www.faunalia.it
   >Cell: (+39)349-1310164  Tel+Fax: (+39) 0587-213742
   >Piazza Garibaldi 5 - 56025 Pontedera (PI), Italy
   >
   >______________________________________________
   >R-help at stat.math.ethz.ch mailing list
   >https://stat.ethz.ch/mailman/listinfo/r-help
   >PLEASE do read the posting guide 
   >http://www.R-project.org/posting-guide.html
   >and provide commented, minimal, self-contained, reproducible code.
   >
   >


From ggrothendieck at gmail.com  Wed Aug  9 16:11:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Aug 2006 10:11:30 -0400
Subject: [R] data.frame to shape
In-Reply-To: <200608091532.42647.lami@faunalia.it>
References: <200608091532.42647.lami@faunalia.it>
Message-ID: <971536df0608090711k477f55bep5d7e9ded6ffa83b4@mail.gmail.com>

Try this:

DF[unlist(tapply(rownames(DF), DF$id, function(x) c(x, x[1]))),]


On 8/9/06, Leonardo Lami <lami at faunalia.it> wrote:
> Hi all,
> I have a simple question:
> I have a data.frame like this:
>
>   id       x       y
> 1  50 1647685 4815259
> 2  50 1647546 4815196
> 3  50 1647454 4815294
> 4  50 1647405 4815347
> 5  50 1647292 4815552
> 6 50 1647737 4815410
> 7 74 1647555 4815201
> 8 74 1647464 4815023
> 9 74 1646970 4815129
> 10 74 1646895 4815264
> 11 74 1646762 4815513
>
> and I'd like to trasform it with the "convert.to.shapefile" function
> (shapefiles package) but to make this I must have a data.frame like this:
>
>   id       x       y
> 1  50 1647685 4815259
> 2  50 1647546 4815196
> 3  50 1647454 4815294
> 4  50 1647405 4815347
> 5  50 1647292 4815552
> 6  50 1647737 4815410
> 7  50 1647685 4815259
> 8  74 1647555 4815201
> 9  74 1647464 4815023
> 10 74 1646970 4815129
> 11 74 1646895 4815264
> 12 74 1646762 4815513
> 13 74 1646762 4815513
>
> with the first point of every id repeated to close the polygon.
> There is a function to make this indipendently by the number of the id
>
> Best regards
> Leonardo
>
> --
> Leonardo Lami
> email + jabber: lami at faunalia.it
> www.faunalia.it
> Cell: (+39)349-1310164  Tel+Fax: (+39) 0587-213742
> Piazza Garibaldi 5 - 56025 Pontedera (PI), Italy
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mschwartz at mn.rr.com  Wed Aug  9 16:22:38 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 09 Aug 2006 09:22:38 -0500
Subject: [R] Sampling from a Matrix
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB685@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB685@usctmx1106.merck.com>
Message-ID: <1155133358.3790.5.camel@localhost.localdomain>

On Tue, 2006-08-08 at 14:10 -0400, Liaw, Andy wrote:
> From: Marc Schwartz
>  
> > On Fri, 2006-08-04 at 12:46 -0400, Daniel Gerlanc wrote:
> > > Hello all,
> > > 
> > > Consider the following problem:
> > > 
> > > There is a matrix of probabilities:
> > > 
> > > > set.seed(1)
> > > > probs <- array(abs(rnorm(25, sd = 0.33)), dim = c(5,5), 
> > dimnames = 
> > > > list(1:5, letters[1:5])) probs
> > >         a      b       c         d        e
> > > 1 0.21 0.27 0.50 0.0148 0.303
> > > 2 0.06 0.16 0.13 0.0053 0.258
> > > 3 0.28 0.24 0.21 0.3115 0.025
> > > 4 0.53 0.19 0.73 0.2710 0.656
> > > 5 0.11 0.10 0.37 0.1960 0.205
> > > 
> > > I want to sample 3 values from each row.
> > > 
> > > One way to do this follows:
> > > 
> > > index <- 1:ncol(probs)
> > > 
> > > for(i in 1:nrow(probs)){
> > > 
> > > ## gets the indexes of the values chosen
> > > 
> > > sample(index, size = 3, replace = TRUE, prob = probs[i, ])
> > > 
> > > }
> > > 
> > > Is there a another way to do this?
> > > 
> > > Thanks!
> > 
> > > t(apply(probs, 1, function(x) sample(x, 3)))
> >    [,1]   [,2]   [,3]
> > 1 0.210 0.5000 0.0148
> > 2 0.258 0.0053 0.1300
> > 3 0.025 0.2800 0.3115
> > 4 0.190 0.5300 0.2710
> > 5 0.196 0.1000 0.1100
> 
> Hmm... If I read Daniel's code (which is different from his description)
> correctly, that doesn't seem to be what he wanted.  Perhaps something like
> this:
> 
> apply(probs, 1, function(p) sample(1:ncol(probs), 3, replace=TRUE, prob=p))
> 
> Andy

Andy,

You are of course correct. I had focused on the description of the
problem, rather than the code provided, presuming that the code was not
correct, including the use of 'replace' and 'prob' in sample().

I suppose it would be up to Daniel for clarification.

Regards,

Marc


From chrish at stats.ucl.ac.uk  Wed Aug  9 16:34:42 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 9 Aug 2006 15:34:42 +0100 (BST)
Subject: [R] Rd question: itemize in arguments
Message-ID: <Pine.LNX.4.64.0608091530380.28888@egon.stats.ucl.ac.uk>

Dear list,

I'm writing Rd files and I'd like to have a list inside the description of 
an argument:

(...)
\arguments{
(...)
   \item{bootmethod}{vector of strings, defining the methods used for
     resampling. Possible methods:
     \itemize{
       \item{"boot"} {Nonparametric bootstrap (precise behaviour is
       controlled by parameters \code{bscompare} and
       \code{multipleboot}).}
       \item{"subset"} {Selecting random subsets from the dataset. Size
       determined by \code{subtuning}.}
       \item{"noise"} {Replacing a certain percentage of the points by
       random noise, see \code{noisetuning}.}
       \item{"jitter"} {Add random noise to all points, see
       \code{jittertuning}. (This didn't perform well in Hennig (2006),
       but you may want to get your own experience.)}
       \item{"bojit"} {Nonparametric bootstrap first, and then adding
       noise to the points, see \code{jittertuning}.}
     }
(...)
}

Seemingly this leads to the following warning, running R CMD check:
Found the following significant warnings:
   Warning: missing text for item '"boot"' in \describe
   Warning: missing text for item '"subset"' in \describe
   Warning: missing text for item '"noise"' in \describe
   Warning: missing text for item '"jitter"' in \describe
   Warning: missing text for item '"bojit"' in \describe

(The code above is the only place in any Rd file of my package where
these five strings occurr.)

Is it forbidden to use \itemize inside of \argument?
What went wrong here?

Best,
Christian


*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From spencer.graves at pdf.com  Wed Aug  9 16:35:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Aug 2006 07:35:19 -0700
Subject: [R] lme4 and lmeSplines
In-Reply-To: <adf71a630608021225u73a2a547i9ba43fb930980a04@mail.gmail.com>
References: <adf71a630608021225u73a2a547i9ba43fb930980a04@mail.gmail.com>
Message-ID: <44D9F2A7.5070603@pdf.com>

	   At least at one time, to use lmer after lme, you had to quit R 
before loading 'lme4' because of substantive conflicts between 'nlme' 
and 'lme4' / 'Matrix'.  That may be the source of your problem.  I can 
think of two possible ways to get around this:

	  1.  I might try quitting R and restarting after your use of 
'lmeSplines' but before 'lmer'.

	  2.  If that failed, I might try making local copies of everything I 
needed from 'lmeSplines' and using them with 'lmer'.  If that failed, 
I'd use 'debug' to walk through the code (or local copies of whatever I 
needed, possibly with name changes) until I figured it out.

	  I know this is not what you wanted to hear, but I hope it helps.
	  Spencer Graves

Kevin Wright wrote:
> I'm trying to use the lmeSplines package together with lme4.
> 
> Below is (1) an example of lmeSplines together with nlme (2) an
> attempt to use lmeSplines with lme4 (3) then a comparison of the
> random effects from the two different methods.
> 
> (1)
> 
> require(lmeSplines)
> data(smSplineEx1)
> dat <- smSplineEx1
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> dat$all <- rep(1,nrow(dat))
> times20 <- seq(1,100,length=20)
> Zt20 <- smspline(times20)
> dat$Zt20 <- approx.Z(Zt20, times20, dat$time)
> fit1.20 <- lme(y~time, data=dat, random=list(all=pdIdent(~Zt20-1)))
> # Loess model
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> # Spline model
> with(dat, lines(fitted(fit1.20)~time, col="red"))
> # Save random effects for later
> ranef.nlme <- unlist(ranef(fit1.20))
> 
> (2) Now an attempt to use lme4:
> 
> library(lmeSplines)
> detach(package:nlme)
> library(lme4)
> data(smSplineEx1)
> # Use 20 spline in lme4
> dat <- smSplineEx1
> times20 <- seq(1,100,length=20)
> Zt20 <- smspline(times20)
> dat <- cbind(dat, approx.Z(Zt20, times20, dat$time))
> names(dat)[4:21] <- paste("Zt",names(dat)[4:21],sep="")
> dat$all <- rep(1, nrow(dat))
> fit1.20 <- lmer(y~time
>              +(-1+Zt1|all)+(-1+Zt2|all)+(-1+Zt3|all)+(-1+Zt4|all)+(-1+Zt5|all)+(-1+Zt6|all)
>              +(-1+Zt7|all)+(-1+Zt8|all)+(-1+Zt9|all)+(-1+Zt10|all)+(-1+Zt11|all)+(-1+Zt12|all)
>              +(-1+Zt13|all)+(-1+Zt14|all)+(-1+Zt15|all)+(-1+Zt16|all)+(-1+Zt17|all)+(-1+Zt18|all),
>              data=dat)
> #summary(fit1)
> # Plot the data and loess fit
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> # Fitting with splines
> with(dat, lines(fitted(fit1.20)~time, col="red"))
> ranef.lme4 <- unlist(ranef(fit1.20))
> 
> (3) Compare nlme lme4 random effects
> 
> plot(ranef.nlme~ranef.lme4)
> 
> The plot of fitted values from lme4 is visually appealing, but the
> random effects from lme4 are peculiar--three are non-zero and the rest
> are essentially zero.
> 
> Any help in getting lme4 + lmeSplines working would be appreciated.
> It is not unlikely that I have the lmer syntax wrong.
> 
> Kevin Wright
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erich.neuwirth at univie.ac.at  Wed Aug  9 16:36:17 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Wed, 09 Aug 2006 16:36:17 +0200
Subject: [R] [Rcom-l] GARCH(1,1) optimization with R
In-Reply-To: <1a350e7b0608090704m5a15596aoa0cb6f32b2ed8dd4@mail.gmail.com>
References: <1a350e7b0608090704m5a15596aoa0cb6f32b2ed8dd4@mail.gmail.com>
Message-ID: <44D9F2E1.0@univie.ac.at>

This is not the right list for your question.
Question on how to use R and R's libraries should be posted to
R-help at stat.math.ethz.ch




Patrick Zhang wrote:
> Hello all,
> 
> Trying to implement GARCH(1,1) with ASP.NET <http://asp.net/> and
> VB.NET<http://vb.net/>.
> It involves optimization of a three-variate function with some constraints.
> Learned from Wilmott.com <http://wilmott.com/> that R might be able to
> do it
> but have no idea how.  Could anyone help me out please.  Thanks in advance.
> 
> Additional info:
> 1. Tried calling Excel Solver from within my web application - it works
> fine
> except that Excel.exe won't go away from task manager although the Quit()
> method has been used;
> 2. Also tried running (Process.Start) a separate console application that
> calls Excel Solver from the code, getting error message: The application
> failed to initialize properly (0xc0000142).
> Any thought of an alternative?
> 
> Best wishes,
> Pat
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Rcom-l mailing list
> Rcom-l at mailman.csd.univie.ac.at
> http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l
> Wiki at http://rcom.csd.univie.ac.at/rcomwiki/


-- 
Erich Neuwirth, Didactic Center for Computer Science
University of Vienna
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-9394


From swidanf at janelia.hhmi.org  Wed Aug  9 16:36:49 2006
From: swidanf at janelia.hhmi.org (Swidan, Firas)
Date: Wed, 09 Aug 2006 10:36:49 -0400
Subject: [R] Speeding indexing and sub-sectioning of 3d array
Message-ID: <C0FF6B41.110A%swidanf@janelia.hhmi.org>

Hi,

I am having a problem with a very slow indexing and sub-sectioning of a 3d
array:

> dim(arr)
[1] 245 175 150

For each point in the array, I am trying to calculate the mean of the values
in its surrounding:


mean( arr[ (i - radius):(i + radius),
                                (j - radius):(j + radius),
                                (k - radius):(k + radius)] )

Putting that code in 3 for loops

calculateKMedian <- function( arr, radius){

  for( i in (radius + 1):(dim(arr)[1] - radius - 1) ){
    for( j in (radius + 1):(dim(arr)[2] - radius - 1) )
      for( k in (radius + 1):(dim(arr)[3] - radius - 1) ){


        mediansArr <- mean( arr[ (i - radius):(i + radius),
                                (j - radius):(j + radius),
                                (k - radius):(k + radius)] )

      }
  }
  return(mediansArr)
}

Results in a very slow run:

> system.time( calculateKMedian( a, 3))
[1] 423.468   0.096 423.631   0.000   0.000

If I replace 

        mediansArr <- mean( arr[ (i - radius):(i + radius),
                                (j - radius):(j + radius),
                                (k - radius):(k + radius)] )

With an access to the (I,j,k) cell's value

 mediansArr <- arr[i,j,k]

The running time decreases to

> system.time( calculateKMedian( a, 3))
[1] 14.821  0.005 14.829  0.000  0.000



But 14 seconds are still pretty expensive for just scanning the array.

Is there anything I can do to speed the indexing and the sub-sectioning of
the 3d array in this case?

Thanks for the help,
Firas.


From mschwartz at mn.rr.com  Wed Aug  9 16:39:36 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 09 Aug 2006 09:39:36 -0500
Subject: [R] parameter yaxs / function hist (graphics)
In-Reply-To: <44D90707.1060002@infolink.com.br>
References: <mailman.9.1154685603.13659.r-help@stat.math.ethz.ch>
	<44D7ECEA.10009@infolink.com.br>
	<Pine.LNX.4.63.0608072304570.7707@est.ufpr.br>
	<44D90707.1060002@infolink.com.br>
Message-ID: <1155134376.3790.18.camel@localhost.localdomain>

Paulo,

Try the following:

x <- rnorm(100)

par(xaxs = "i")
par(yaxs = "i")

hist(x, breaks = seq(-4, 4, 0.5), ylim = c(0, 40),
     xlim = c(-4, 4))

box()


The problem is that graphics:::plot.histogram() is coded in such a way
that the use of 'xaxs' and 'yaxs' are ineffectual, as they are not
passed to the internal call to plot.window(), which does not provide for
'...' args in this case. So they cannot be passed 'inline' as you
attempted.

Thus, you need to set these prior to the plotting of the histogram.

BTW, a better option for the call to axis() relative to using the 'line'
argument, is to use the 'pos' argument and set it to 0:

x <- rnorm(100)

par(xaxs = "i")
par(yaxs = "i")

hist(x, breaks = seq(-4, 4, 0.5), ylim = c(0, 40),
     xlim = c(-4, 4), xaxt = "n")

axis(1, pos = 0)

box()

HTH,

Marc Schwartz

On Tue, 2006-08-08 at 18:49 -0300, Paulo Barata wrote:
> Dear Paulo,
> 
> Thank you for your reply. But I doubt yours is a proper
> solution to my request, for some reasons:
> 
> 1. The position of the axis graphed with the command axis(1, line=-1)
> depends on the size of the graphics window.
> 
> 2. After your graph is on the screen, in case one may want a boxed
> graph, a box() command will produce a histogram "floating in the air",
> not "touching" the horizontal axis.
> 
> Of course, one could build a proper box (with labels, etc.) by means
> of more primitive graphics functions like lines (package graphics)
> and others, but I think that would mean a lot of work.
> 
> Thank you again.

> Paulo Justiniano Ribeiro Jr wrote:
> > Paulo
> > 
> > One possibility is to draw the histogram without axes and then add them 
> > wherever you want.
> > 
> > For instance with something along the lines:
> > 
> > x <- rnorm(500)
> > hist(x, axes=F)
> > axis(1, line=-1)
> > 
> > For more details: ?axis
> > 
> > best
> > P.J.
> > > 
> > On Mon, 7 Aug 2006, Paulo Barata wrote:
> > 
> >>
> >> Dear R users,
> >>
> >> The parameters xaxs and yaxs (function par, package graphics)
> >> seem not to work with the function hist (package graphics),
> >> even when the parameters xlim and ylim are defined.
> >>
> >> Is there any way to make yaxs="i" and xaxs="i" work properly
> >> with the function hist, mainly to produce histograms that
> >> "touch" the horizontal axis? The R documentation and the
> >> R mailing lists archive don't seem to be of help here.
> >>
> >> I am using R 2.3.1, running under Windows XP.
> >>
> >> ## Example:
> >> x <- rnorm(100)
> >> hist(x,breaks=seq(-4,4,0.5),ylim=c(0,40),yaxs="i",
> >>   xlim=c(-4,4),xaxs="i")
> >> box()
> >>
> >> Thank you very much.
> >>
> >> Paulo Barata
> >>


From franco.mendolia at gmx.de  Wed Aug  9 16:44:16 2006
From: franco.mendolia at gmx.de (Franco Mendolia)
Date: Wed, 09 Aug 2006 16:44:16 +0200
Subject: [R] tk combobox question
Message-ID: <44D9F4C0.3080106@gmx.de>


Hello!

The IWigdets::combobox has an option "selectioncommand" to specify a procedure 
which is called when an item in the combobox is selected.

Does ComboBox from Bwidgets have an equal option or is there a simple method to 
do thi? I haven't found one but i would like to use Bwidgets instead of Iwidgets.

Another question which should not be a problem, but at the moment I just don't 
see the answer:

How can I get the index of the selected item in an Iwidgets combobox? Till now I 
am only able to get the contents of the selected combobox item.

Regards,
Franco Mendolia


From spencer.graves at pdf.com  Wed Aug  9 16:44:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Aug 2006 07:44:21 -0700
Subject: [R] unbalanced mixed effects models for fully factorial designs
In-Reply-To: <44D1263F.3050306@sci.monash.edu.au>
References: <44D1263F.3050306@sci.monash.edu.au>
Message-ID: <44D9F4C5.8020105@pdf.com>

Dear Prof. Logan:

	  Are you familiar with 'lme' in the 'nlme' package?  Superlative 
documentation for the 'nlme' package is Pinheiro and Bates (2000) Mixed 
effects models in S and S-PLUS (Springer, listed as 'available' in the 
catalog of your university library).  The standard R distribution comes 
with a directory "~library\nlme\scripts" containing script files 
'ch01.R', 'ch02.R', ..., 'ch06.R', and 'ch08.R'.  These contain R script 
files with the R code for each chapter in the book.  I've learned a lot 
from walking through the script files line by line while reviewing the 
corresponding text in the book.  Doing so protects me from problems with 
silly typographical errors as well as subtle problems where the S-Plus 
syntax in the book gives a different answer in R because of the few 
differences in the syntax between S-Plus and R.

       Hope this helps.
       Spencer Graves

Murray Logan wrote:
> Does anyone know of a way of dealing with unbalanced mixed effects 
> (fixed and random factors) for fully factorial designs.
> 
> An example of such data is given below;
> 
> The response variable is SQRTRECRUITS
> SEASON is a random factor
> DENSITY is a fixed factor
> Thus DENSITY:SEASON is a fixed factor.
> 
> Therefore, whereas the effects of SEASON and DENSITY:SEASON should be 
> tested against the overall residual (error) term, the effect of DENSITY 
> should be tested against the DENSITY:SEASON interaction.
> To complicate matters, the data are unbalanced, and thus Type III SS are 
> preferable
> 
> quinn <-
> structure(list(SEASON = structure(as.integer(c(2, 2, 2, 2, 2,
> 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4)), .Label = c("Autumn",
> "Spring", "Summer", "Winter"), class = "factor", contrasts = "contr.sum"),
>     DENSITY = structure(as.integer(c(2, 2, 2, 2, 2, 1, 1, 1,
>     1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,
>     1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1)), .Label = c("High",
>     "Low"), class = "factor"), RECRUITS = as.integer(c(15, 10,
>     13, 13, 5, 11, 10, 15, 10, 13, 1, 21, 31, 21, 18, 14, 27,
>     34, 49, 69, 55, 28, 54, 14, 18, 20, 21, 4, 22, 30, 36, 13,
>     13, 8, 0, 0, 10, 1, 5, 9, 4, 5)), SQRTRECRUITS = c(3.872983,
>     3.162278, 3.605551, 3.605551, 2.236068, 3.316625, 3.162278,
>     3.872983, 3.162278, 3.605551, 1, 4.582576, 5.567764, 4.582576,
>     4.242641, 3.741657, 5.196152, 5.830952, 7, 8.306624, 7.416198,
>     5.291503, 7.348469, 3.741657, 4.242641, 4.472136, 4.582576,
>     2, 4.690416, 5.477226, 6, 3.605551, 3.605551, 2.828427, 0,
>     0, 3.162278, 1, 2.236068, 3, 2, 2.236068), GROUP = 
> structure(as.integer(c(4,
>     4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 5, 5, 5,
>     5, 5, 5, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 8, 8, 8, 7, 7, 7,
>     7, 7, 7)), .Label = c("AutumnHigh", "AutumnLow", "SpringHigh",
>     "SpringLow", "SummerHigh", "SummerLow", "WinterHigh", "WinterLow"
>     ), class = "factor")), .Names = c("SEASON", "DENSITY", "RECRUITS",
> "SQRTRECRUITS", "GROUP"), row.names = c("1", "2", "3", "4", "5",
> "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
> "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27",
> "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38",
> "39", "40", "41", "42"), class = "data.frame")
> 
> I realise that Anova (car package) calculated Type III SS (given the 
> correct contrasts), however, this does not permit mixed models.  
> Conversely, if I was to specify a aov model such as;
> summary(aov(SQRTRECRUITS ~ SEASON+DENSITY+Error(DENSITY:SEASON), 
> data=quinn))
> purely to obtain a test for DENSITY (ignoring the test for SEASON), the 
> SS are Type I.
> 
> Although it is possible to calculate out the F-ratio (and p-value) 
> calculations manually and substitute them into the anova tables, I cant 
> help think that there must be a better solution.
> 
> Is there any expectation that there will be a summary routine that 
> provides Type II and Type II SS, and or is aov ever likely to 
> accommodate non-hierarchical mixed models?
> 
> Regards
> 
> Murray
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From patrick.x.zhang at gmail.com  Wed Aug  9 16:46:56 2006
From: patrick.x.zhang at gmail.com (Patrick Zhang)
Date: Wed, 9 Aug 2006 15:46:56 +0100
Subject: [R] GARCH(1,1) optimization with R
Message-ID: <1a350e7b0608090746o4b25be0ckabc993c1ebd5a356@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/487136e7/attachment.pl 

From f.harrell at vanderbilt.edu  Wed Aug  9 16:54:08 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 09 Aug 2006 09:54:08 -0500
Subject: [R] Joint confidence intervals for GLS models?
In-Reply-To: <5BBCB098FE36644C90E7836C296362880244FCAA@MCLNEXVS03.resource.ds.bah.com>
References: <5BBCB098FE36644C90E7836C296362880244FCAA@MCLNEXVS03.resource.ds.bah.com>
Message-ID: <44D9F710.7030800@vanderbilt.edu>

Kennedy David wrote:
> Dear All,
>  
> I would like to be able to estimate confidence intervals for a linear
> combination of coefficients for a GLS model.  I am familiar with John
> Foxton's helpful paper on Time Series Regression and Generalised Least
> Squares (GLS) and have learnt a bit about the gls function.
>  
> I have downloaded the gmodels package so I can use the estimable
> function.  The estimable function is very useful because it allows me to
> calculate confidence intervals for a linear combination of coefficients,
> but only for OLS models.  For example, the code below calculates the
> confidence interval for the sum of the coefficient of petrol_A and the
> coefficient of petrol_B:
>  
>> results <- lm(all_rural_count_capita ~ petrol_A + petrol_B +
> gdp_capita)
>> estimable(results,cm=c(0,1,1,0),conf.int=0.95)
> 
> However, the estimable function does not seem to work for GLS objects,
> as shown below.  The estimable documentation confirm that the object
> must be one of the following: lm, glm, lme, lmer.
> 
>> results.gls <- gls(all_rural_count_capita ~ petrol_A + petrol_B +
> gdp_capita, correlation=corARMA(p=1),method='ML')
>> estimable(results.gls,cm=c(0,1,1,0),conf.int=0.95)
> Error in estimable(results.gls, cm = c(0, 1, 1, 0), conf.int = 0.95) : 
>         obj must be of class 'lm', 'glm', 'aov', 'lme', 'lmer', 'gee',
> 'geese' or 'nlme'
> 
> 
> Therefore, I am looking for a solution to this problem.  I think that
> the solution (if it exists) may be down one of the following paths:
>  
> 1) An alternative command which allows me to generate joint confidence
> intervals for the objects generated by the gls function.  p.s. I note
> that the intervals function only appears to produce confidence intervals
> for each coeffcient (not for a linear combination of coeffcients).
>  
> 2) An alternative means of generating GLS estimates as lm, glm, lme or
> lmer objects, so they can be inputed into the estimable function.
>  
> Regards,
> David

I'm glad you are using gls because I think it's underused. When you 
don't need random effects but want to handle serial correlation, things 
are much easier with gls.  The Design package (which requires the Hmisc 
package) has a function glsD that allows easy anova( ) and contrast( ) 
usage.  Contrasts are simple because they are done in terms of 
differences in predicted values for any user settings:

contrast(fit, list(sex='male', times=1:5), list(sex='female', times=1:5))

Confidence intervals from contrast( ) (actually contrast.Design) are not 
simultaneous in the sense of providing simultaneous confidence bands 
over the whole time axis.  I would welcome code for that using a 
chi-square multiple d.f. approximation or other approach.

A case study using glsD will be in a 2nd edition of my book Regression 
Modeling Strategies which is still some time away.

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From martin.chlond at btinternet.com  Wed Aug  9 17:14:33 2006
From: martin.chlond at btinternet.com (Martin)
Date: Wed, 9 Aug 2006 16:14:33 +0100
Subject: [R] Combinations question
Message-ID: <000601c6bbc6$85dc64d0$0200a8c0@MCHLOND>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/b3cc8fad/attachment.pl 

From dusa.adrian at gmail.com  Wed Aug  9 17:21:12 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 9 Aug 2006 18:21:12 +0300
Subject: [R] objects and environments
Message-ID: <200608091821.12907.dusa.adrian@gmail.com>

Dear list,

I have two functions created in the same environment, fun1 and fun2.
fun2 is called by fun1, but fun2 should use an object which is created in fun1

fun1 <- function(x) {
    ifelse(somecondition, bb <- "o", bb <- "*")
    ## mymatrix is created, then
    myresult <- apply(mymatrix, 1, fun2)
}

fun2 <- function(idx) {
   if (bb == "o) {
      # do something with idx
   } else {
      # do something else with idx
   }
}

What should I do to have "bb" available in fun2?
I tried everything I could with sys.parent(), sys.frame(), parent.env() but it 
just doesn't want to work.

I have three solutions but none of them satisfactory; inside fun1:
1. assign("bb", aa, .GlobalEnv)  # don't want to do that, do I?
2. assign("bb", aa, 1) # for some reason "aa" appears in the .GlobalEnv anyway
3. pass "bb" as an argument to fun2, but this would require:
   apply(mymatrix, 1, function(idx) fun2(idx, bb)) # which is not elegant


I played further with assign and get, but there's something I'm missing:

fun1 <- function() {
    e2 <- new.env()
    assign("bb", 4, e2)
    fun2()
}

fun2 <- function(idx) {
    get("bb", e2)
}

> fun1()
Error in get("bb", e2) : object "e2" not found

Any hint would be highly appreciated,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From s-walker at ti.com  Wed Aug  9 17:19:36 2006
From: s-walker at ti.com (Walker, Sam)
Date: Wed, 9 Aug 2006 10:19:36 -0500
Subject: [R] ggplot facet label font size
Message-ID: <47AE778BBF209547AF19DC8810F5FAF3113CF7@dlee08.ent.ti.com>

Thanks for the lattice help Gabor and Deepayan.

These snippets work well.  I've only been using R a few months and it
and the user community have exceeded my expectations.

Best Regards,
-Sam

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Sunday, August 06, 2006 1:18 AM
To: Walker, Sam
Cc: r-help at stat.math.ethz.ch; Gabor Grothendieck
Subject: Re: [R] ggplot facet label font size

On 8/3/06, Walker, Sam <s-walker at ti.com> wrote:
>
> This works OK, but there is some extra spacing between the panels, the
> top axis and the strip on the top, and the left labels and panel.
>
> How can I remove these extra spaces?

This turns out to be surprisingly easy (surprising to me at least):


#########

my.strip <-
    function(which.given, which.panel, var.name, ...)
{
    if (which.given == 1 && which.panel[2] == 2)
        strip.default(1, which.panel[1],
                      var.name = var.name[1],
                      ...)
}


my.strip.left <-
    function(which.given, which.panel, var.name, ..., horizontal)
{
    if (which.given == 2 && which.panel[1] == 1)
        strip.default(1, which.panel[2],
                      var.name = var.name[2],
                      horizontal = FALSE, ...)
}


histogram(~ tip/total_bill | sex + smoker, tips,
          strip = my.strip,
          strip.left = my.strip.left,
          par.strip.text = list(lines = 0.6),
          par.settings =
          list(layout.heights = list(strip = c(0, 1)),
               layout.widths = list(strip.left = c(1, 0)),
               add.text = list(cex = 0.7)))


#########

The trick of changing two-line strips to one-line strips is not
obvious from the documentation (such as it is), it depends on the
implementation of strips.

HTH,

Deepayan

> I've tried changing various layout.widths settings with no luck.  It
> seems the spaces are calculated based on the number of conditioning
> variables, in this case 2 (sex+smoker).
>
>
> Thanks in advance...
> -Sam
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Wednesday, August 02, 2006 6:04 PM
> To: Walker, Sam
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ggplot facet label font size
>
> On 8/2/06, Walker, Sam <s-walker at ti.com> wrote:
> > How do I change the font size in the facet labels along the edges of
> the
> > plot?
> >
> > For example (from the ggplot help file):
> >     p<-ggplot(tips, sex ~ smoker, aesthetics=list(x=tip/total_bill))
> >     gghistogram(p)
> >
> > In this plot, the facet labels are "smoker: No", "smoker: Yes",
"sex:
> > Female", "sex: Male".  What command can I use to reduce the font
size
> of
> > these labels?
> >
> > In lattice terminology, cex is used to scale these strip labels.
But
> I
> > couldn't find the equivalent in ggplot.
> >
> > The reason I'm asking is I have a 9x7 array of plots which I've been
> > plotting with lattice.  I wanted to use ggplot because I like having
> the
> > labels on the edge of the plots
>
> Note that lattice can do that by using custom strip functions:
>
> library(ggplot) # data resides here
> library(lattice)
>
> my.strip <- function(which.given, which.panel, ...)
>    if (which.given == 1 && which.panel[2] == 2)
>       strip.default(which.given, which.panel, ...)
>
> my.strip.left <- function(which.given, which.panel, ..., horizontal)
>    if (which.given == 2 && which.panel[1] == 1)
>       strip.default(which.given, which.panel, horizontal = FALSE, ...)
>
> histogram(~ tip/total_bill | sex + smoker, tips, strip = my.strip,
>      strip.left = my.strip.left, par.settings = list(add.text =
> list(cex = 0.7)))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://www.stat.wisc.edu/~deepayan/


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug  9 17:31:05 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 9 Aug 2006 17:31:05 +0200
Subject: [R] objects and environments
References: <200608091821.12907.dusa.adrian@gmail.com>
Message-ID: <007b01c6bbc8$d46fd530$0540210a@www.domain>

try this:

fun1 <- function(x) {
    environment(fun2) <- environment()
    ifelse(somecondition, bb <- "o", bb <- "*")
    ## mymatrix is created, then
    myresult <- apply(mymatrix, 1, fun2)
}


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Adrian Dusa" <dusa.adrian at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 09, 2006 5:21 PM
Subject: [R] objects and environments


> Dear list,
>
> I have two functions created in the same environment, fun1 and fun2.
> fun2 is called by fun1, but fun2 should use an object which is 
> created in fun1
>
> fun1 <- function(x) {
>    ifelse(somecondition, bb <- "o", bb <- "*")
>    ## mymatrix is created, then
>    myresult <- apply(mymatrix, 1, fun2)
> }
>
> fun2 <- function(idx) {
>   if (bb == "o) {
>      # do something with idx
>   } else {
>      # do something else with idx
>   }
> }
>
> What should I do to have "bb" available in fun2?
> I tried everything I could with sys.parent(), sys.frame(), 
> parent.env() but it
> just doesn't want to work.
>
> I have three solutions but none of them satisfactory; inside fun1:
> 1. assign("bb", aa, .GlobalEnv)  # don't want to do that, do I?
> 2. assign("bb", aa, 1) # for some reason "aa" appears in the 
> .GlobalEnv anyway
> 3. pass "bb" as an argument to fun2, but this would require:
>   apply(mymatrix, 1, function(idx) fun2(idx, bb)) # which is not 
> elegant
>
>
> I played further with assign and get, but there's something I'm 
> missing:
>
> fun1 <- function() {
>    e2 <- new.env()
>    assign("bb", 4, e2)
>    fun2()
> }
>
> fun2 <- function(idx) {
>    get("bb", e2)
> }
>
>> fun1()
> Error in get("bb", e2) : object "e2" not found
>
> Any hint would be highly appreciated,
> Adrian
>
> -- 
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dusa.adrian at gmail.com  Wed Aug  9 17:36:37 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 9 Aug 2006 18:36:37 +0300
Subject: [R] objects and environments
In-Reply-To: <007b01c6bbc8$d46fd530$0540210a@www.domain>
References: <200608091821.12907.dusa.adrian@gmail.com>
	<007b01c6bbc8$d46fd530$0540210a@www.domain>
Message-ID: <200608091836.37236.dusa.adrian@gmail.com>

On Wednesday 09 August 2006 18:31, Dimitris Rizopoulos wrote:
> try this:
>
> fun1 <- function(x) {
>     environment(fun2) <- environment()
>     ifelse(somecondition, bb <- "o", bb <- "*")
>     ## mymatrix is created, then
>     myresult <- apply(mymatrix, 1, fun2)
> }

Beautiful :)
Thanks very much Dimitris, I was out of energy after several hours of 
struggling with this.
Best,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From jacques.veslot at good.ibl.fr  Wed Aug  9 17:33:05 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 09 Aug 2006 17:33:05 +0200
Subject: [R] Combinations question
In-Reply-To: <000601c6bbc6$85dc64d0$0200a8c0@MCHLOND>
References: <000601c6bbc6$85dc64d0$0200a8c0@MCHLOND>
Message-ID: <44DA0031.8090906@good.ibl.fr>

library(gtools)
cb <- function(n,r) t(apply(combinations(n, r), 1, function(x) ifelse(1:n %in% x, 1, 0)))
cb(6,3)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Martin a ?crit :
> I need to generate a {0,1} matrix wifht nCr rows and n columns. The rows of the matrix will consist of all possible combinations containing r ones.
> 
> My clumsy attempt for n = 6 and r = 3 is
> 
> X <- expand.grid(c(1,0),c(1,0),c(1,0),c(1,0),c(1,0),c(1,0))
> Y <- X[rowSums(X)==3,]
> 
> I can genralize this in a function but the result is quite ugly. Any suggestions?
> 
> Thank you in advance.
> 
> Martin
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Yingfu.Xie at sekon.slu.se  Wed Aug  9 17:33:54 2006
From: Yingfu.Xie at sekon.slu.se (Yingfu Xie)
Date: Wed, 9 Aug 2006 17:33:54 +0200
Subject: [R] minimization a quadratic form with some coef fixed and some
	constrained
Message-ID: <CA871298CD1882459F7859BD08DC06E4C53147@slumail.ad.slu.se>

Hello, all,

I had problems with an extension to a classic optimization problem.

The target is to minimize a quadratic form a'Ma with respect to vector
b, where vector a=(b',-1)', i.e., a is the expand of b, and M is a
symmetric matrix (positive definite if needed). One more constrain on b
is b'b=1. I want to solve b given M.

I tried but it seems impossible to find an analytic solution for b. Any
objection?

Now, come to the numerical.  Does anybody have any idea on how to
parameterize this to use, e.g. optim() or constrOptim()? 

Any help are appreciated very much!

Regards,
Yingfu
###########################################

This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}


From eklypse at gmail.com  Wed Aug  9 17:38:29 2006
From: eklypse at gmail.com (Alessandro Gagliardi)
Date: Wed, 9 Aug 2006 11:38:29 -0400
Subject: [R] cannot plot too much (using Mac interface)
Message-ID: <af2c8600608090838h66d8f1b9u8de95da2c83d64c9@mail.gmail.com>

Thanks.  And sorry for abusing the word crash.  I understand the
difference.  However, now that you mention it, there is a case where I
think R is crashing.  I'm not sure if it's R or if it's the Mac OS
interface, but if I try to plot something complicated (or a series of
complicated things) while rendering them at the same time, R will
freeze and eventually fail.  I've been trying to avoid doing that,
creating an object separately and then ploting that object, for
example, or plotting one thing at a time instead of using a for loop.
But sometimes I forget and end up loosing my work because of it.  Next
time it happens, I will copy down what the command was that does it,
though I'm not sure how useful that will be because it usually depends
upon some of my own functions which are, in turn, built on the
tseriesChaos package.  I figure it's R running out of memory or
something.  Any ideas?

-Alessandro

On 8/9/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Hmm.  'time' is in package stats, and you seem to have saved an object
> that is pulling in the namespace 'tseries'.
>
> Loading the workspace is done before loading the standard set of packages.
> This appears to be a bug in tseries, for if its namespace needs 'time', it
> should import it from 'stats', and it is not even depending on 'stats'.
>
> I can confirm that:
>
> gannet% env R_DEFAULT_PACKAGES=NULL R
> ...
> > loadNamespace("tseries")
> Error: object 'time' not found whilst loading namespace 'tseries'
>
> That is a matter for the 'tseries' maintainer (Cc:ed here).  If you
> try library(tseries) at that point you find
>
> > library(tseries)
> Loading required package: zoo
> Error: object 'aggregate' not found whilst loading namespace 'zoo'
> Error: package 'zoo' could not be loaded
>
> so package zoo has a similar problem (maintainer Cc:ed).
>
>
> What can you do?  The simplest is to rename the workspace and load()
> afterwards as you have done.  But before saving, remove any objects which
> have a dependence on tseries.
>
>
> On Tue, 8 Aug 2006, Alessandro Gagliardi wrote:
>
> > Lately, when I try to open R I get the following error message:
> >
> > Error: object 'time' not found whilst loading namespace 'tseries'
> > Fatal error: unable to restore saved data in .RData
> >
> > If I rename .RData to RData.RData and then try opening R again it
> > works.  Then I can load("RData.RData") without a problem.  But if I
> > try saving my workspace (as the default, ".RData") and reload R it
> > crashes all over again.
>
> R does NOT crash.  It is objecting (correctly) to your saved workspace.
> Please see the posting guide, which specifically asked you not to abuse
> that word.
>
> > I don't know how to get this 'time' object back.  (I must have removed
> > it by accident at some point.)  Any ideas?
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


-- 
Alessandro Gagliardi
Integrative Neuroscience Program
Rutgers University Mind Brain Analysis
alessandro at gagliardi.name

"The opposite of a correct statement is a false statement.
But the opposite of a profound truth may well be another profound truth."
-Niels Bohr


From oswald at dhlaw.de  Wed Aug  9 18:07:00 2006
From: oswald at dhlaw.de (Christian Oswald)
Date: Wed, 09 Aug 2006 18:07:00 +0200
Subject: [R] categorical data
Message-ID: <44DA0824.70109@dhlaw.de>

Dear List,

I neeed a grouped list with two sort of categorical data. I have a data
.frame like this.
	year	cat.	b	c
1	2006	a1	125	212
2	2006	a2	256	212	
3	2005	a1	14	12
4	2004	a3	565	123
5	2004	a2	156	789	
6	2005	a1	1	456
7	2003	a2	786	123
8	2003	a1	421	569
9  	2002	a2	425	245

I need a list with the sum of b and c for every year and every cat (a1,
a2 or a3) in this year. I had used the tapply function to build the sum
for every year or every cat. How can I combine the two grouping values?

Thanks,

Christian


From rolf at erdos.math.unb.ca  Wed Aug  9 18:08:25 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Wed, 9 Aug 2006 13:08:25 -0300 (ADT)
Subject: [R] minimization a quadratic form with some coef fixed and some
	constrained
Message-ID: <200608091608.k79G8P3t009025@erdos.math.unb.ca>

Yingfu Xie wrote:

> I had problems with an extension to a classic optimization problem.
> 
> The target is to minimize a quadratic form a'Ma with respect to vector
> b, where vector a=(b',-1)', i.e., a is the expand of b, and M is a
> symmetric matrix (positive definite if needed). One more constrain on b
> is b'b=1. I want to solve b given M.
> 
> I tried but it seems impossible to find an analytic solution for b. Any
> objection?
> 
> Now, come to the numerical.  Does anybody have any idea on how to
> parameterize this to use, e.g. optim() or constrOptim()? 
> 
> Any help are appreciated very much!

	The analytic solution is trivial.  Write M as

		| M_11 c |
		| c'   m |

	Then given that M_11 is positive definite, the
	minimizer is

		b = (M_11)^{-1}c

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From ggrothendieck at gmail.com  Wed Aug  9 18:14:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Aug 2006 12:14:31 -0400
Subject: [R] objects and environments
In-Reply-To: <200608091821.12907.dusa.adrian@gmail.com>
References: <200608091821.12907.dusa.adrian@gmail.com>
Message-ID: <971536df0608090914y734fdcf7p72cbf9c5fa9cdd42@mail.gmail.com>

Dmitris has already provided the solution but just throught I would'
mention that your third alternative can be written:

apply(mymatrix, 1, fun2, bb = bb)

(assuming fun2 has arguments idx and bb) which is not
nearly so ugly so you might reconsider whether its ok
for you to just pass bb.


On 8/9/06, Adrian Dusa <dusa.adrian at gmail.com> wrote:
> Dear list,
>
> I have two functions created in the same environment, fun1 and fun2.
> fun2 is called by fun1, but fun2 should use an object which is created in fun1
>
> fun1 <- function(x) {
>    ifelse(somecondition, bb <- "o", bb <- "*")
>    ## mymatrix is created, then
>    myresult <- apply(mymatrix, 1, fun2)
> }
>
> fun2 <- function(idx) {
>   if (bb == "o) {
>      # do something with idx
>   } else {
>      # do something else with idx
>   }
> }
>
> What should I do to have "bb" available in fun2?
> I tried everything I could with sys.parent(), sys.frame(), parent.env() but it
> just doesn't want to work.
>
> I have three solutions but none of them satisfactory; inside fun1:
> 1. assign("bb", aa, .GlobalEnv)  # don't want to do that, do I?
> 2. assign("bb", aa, 1) # for some reason "aa" appears in the .GlobalEnv anyway
> 3. pass "bb" as an argument to fun2, but this would require:
>   apply(mymatrix, 1, function(idx) fun2(idx, bb)) # which is not elegant
>
>
> I played further with assign and get, but there's something I'm missing:
>
> fun1 <- function() {
>    e2 <- new.env()
>    assign("bb", 4, e2)
>    fun2()
> }
>
> fun2 <- function(idx) {
>    get("bb", e2)
> }
>
> > fun1()
> Error in get("bb", e2) : object "e2" not found
>
> Any hint would be highly appreciated,
> Adrian
>
> --
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From chrish at stats.ucl.ac.uk  Wed Aug  9 18:21:18 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 9 Aug 2006 17:21:18 +0100 (BST)
Subject: [R] R CMD check error
Message-ID: <Pine.LNX.4.64.0608091716370.28888@egon.stats.ucl.ac.uk>

Dear list,

R CMD check on my updated package now generated the following error:

"LaTeX errors when creating DVI version.
This typically indicates Rd problems."

But the Rd files (and everything else) were checked as "OK" (I 
removed the problem about which I asked the list some hours ago, but
answers are still appreciated because I rather created a rough 
workaround than solved it).

What can I do to get better information about what caused the problem in 
my Rd files?

Best,
Christian


*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From dusa.adrian at gmail.com  Wed Aug  9 18:28:58 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 9 Aug 2006 19:28:58 +0300
Subject: [R] objects and environments
In-Reply-To: <971536df0608090914y734fdcf7p72cbf9c5fa9cdd42@mail.gmail.com>
References: <200608091821.12907.dusa.adrian@gmail.com>
	<971536df0608090914y734fdcf7p72cbf9c5fa9cdd42@mail.gmail.com>
Message-ID: <200608091928.58934.dusa.adrian@gmail.com>

On Wednesday 09 August 2006 19:14, Gabor Grothendieck wrote:
> Dmitris has already provided the solution but just throught I would'
> mention that your third alternative can be written:
>
> apply(mymatrix, 1, fun2, bb = bb)
>
> (assuming fun2 has arguments idx and bb) which is not
> nearly so ugly so you might reconsider whether its ok
> for you to just pass bb.

Aah-aaaaaaa....!! :)
So that's the way to do it...
I don't know how many times I read the help from apply and I missed it every 
time.
Well, I learned many things today, I feel much better now :)

Thank you,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From quesada at gmail.com  Wed Aug  9 18:25:36 2006
From: quesada at gmail.com (Jose Quesada)
Date: Wed, 9 Aug 2006 16:25:36 +0000 (UTC)
Subject: [R] Can one use correlations (or R^2) as data for an ANOVA?
Message-ID: <loom.20060809T182509-304@post.gmane.org>

Dear all,

This may seem obvious for some of you, but it got me thinking.
Can one use correlations (or R^2) as data for an ANOVA?

The case in hand: I have several models fitting the same data (individual fits
per subject). The different models fitting the same guy will produce different
R^2, I was wondering if these could be used as dependent variable in a repeated
measures design where the repeated factors are the different models.

For some reason, this "analysis of variance on 'variance explained' as data
rings a bell as something not methodologically correct. On the other hand, R^2
may be distributed in a way that fulfill of the ANOVA assumptions.

Is it an aberration?

Thanks a lot in advance,
-Jose


From maechler at stat.math.ethz.ch  Wed Aug  9 18:41:22 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 9 Aug 2006 18:41:22 +0200
Subject: [R] R CMD check error
In-Reply-To: <Pine.LNX.4.64.0608091716370.28888@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.64.0608091716370.28888@egon.stats.ucl.ac.uk>
Message-ID: <17626.4146.853253.738576@stat.math.ethz.ch>

>>>>> "Christian" == Christian Hennig <chrish at stats.ucl.ac.uk>
>>>>>     on Wed, 9 Aug 2006 17:21:18 +0100 (BST) writes:

    Christian> Dear list, R CMD check on my updated package now
    Christian> generated the following error:

    Christian> "LaTeX errors when creating DVI version.  This
    Christian> typically indicates Rd problems."

    Christian> But the Rd files (and everything else) were
    Christian> checked as "OK" (I removed the problem about
    Christian> which I asked the list some hours ago, but
    Christian> answers are still appreciated because I rather
    Christian> created a rough workaround than solved it).

    Christian> What can I do to get better information about
    Christian> what caused the problem in my Rd files?

Run
	R CMD Rd2dvi

directly on the relevant Rd file.

Mit liebem Gruss: Martin


From elvis at xlsolutions-corp.com  Wed Aug  9 18:42:48 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Wed, 09 Aug 2006 09:42:48 -0700
Subject: [R] (3) Courses & R/Splus Advanced Programming in New York City
	***September 11-12 ***by the R Development Core Tean Guru
Message-ID: <20060809094248.9f08cc34deb45d78e54b3b5664e21546.be1226c06d.wbe@email.secureserver.net>


From m_nurza at yahoo.com  Wed Aug  9 18:48:55 2006
From: m_nurza at yahoo.com (nurza m)
Date: Wed, 9 Aug 2006 17:48:55 +0100 (BST)
Subject: [R] numerical differentiation
Message-ID: <20060809164855.51017.qmail@web33609.mail.mud.yahoo.com>

Hi list,
How to do numerical differentiation in R. is it using
genD in numDeriv package

thanks in advance


From maechler at stat.math.ethz.ch  Wed Aug  9 18:49:40 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 9 Aug 2006 18:49:40 +0200
Subject: [R] Combinations question
In-Reply-To: <44DA0031.8090906@good.ibl.fr>
References: <000601c6bbc6$85dc64d0$0200a8c0@MCHLOND>
	<44DA0031.8090906@good.ibl.fr>
Message-ID: <17626.4644.278600.544617@stat.math.ethz.ch>

>>>>> "Jacques" == Jacques VESLOT <jacques.veslot at good.ibl.fr>
>>>>>     on Wed, 09 Aug 2006 17:33:05 +0200 writes:

    Jacques> library(gtools)
    Jacques> cb <- function(n,r) t(apply(combinations(n, r), 1, function(x) ifelse(1:n %in% x, 1, 0)))

    Jacques> cb(6,3)

Several months ago, when this came up as well,
I had summarized the diverse 'combinations' approaches and
found that really the one from package
"combinat" (rather than "gtools") was best. 
The function there is called combn().

Also, the next version of R (R-2.4.0) will contain an (even
slightly faster) version of that, i.e., if you dare to use
"R-devel", you have  combn() available from ``standard R''...
and that's another reason why I'd recommend using the
"combinat" version rather than the "gtools" one.

Martin Maechler, ETH Zurich


    Jacques> Martin Chlond  a ?crit :

    >> I need to generate a {0,1} matrix wifht nCr rows and n
    >> columns. The rows of the matrix will consist of all
    >> possible combinations containing r ones.

    >> My clumsy attempt for n = 6 and r = 3 is
    >> 
    >> X <- expand.grid(c(1,0),c(1,0),c(1,0),c(1,0),c(1,0),c(1,0))
    >> Y <- X[rowSums(X)==3,]

    >> I can genralize this in a function but the result is
    >> quite ugly. Any suggestions?

    >> Thank you in advance.
    >> 
    >> Martin


From chrish at stats.ucl.ac.uk  Wed Aug  9 19:08:29 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 9 Aug 2006 18:08:29 +0100 (BST)
Subject: [R] R CMD check error (solved)
In-Reply-To: <17626.4146.853253.738576@stat.math.ethz.ch>
References: <Pine.LNX.4.64.0608091716370.28888@egon.stats.ucl.ac.uk>
	<17626.4146.853253.738576@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0608091805450.28888@egon.stats.ucl.ac.uk>

Thank you Martin,

>    Christian> Dear list, R CMD check on my updated package now
>    Christian> generated the following error:
>
>    Christian> "LaTeX errors when creating DVI version.  This
>    Christian> typically indicates Rd problems."

(...)
>
> Run
> 	R CMD Rd2dvi
>
> directly on the relevant Rd file.

This helped me to find the problem (a mathematical formula not using 
\eqn{})!

Lieber Gruss zurueck,
Christian


*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From pgilbert at bank-banque-canada.ca  Wed Aug  9 19:32:38 2006
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 09 Aug 2006 13:32:38 -0400
Subject: [R] numerical differentiation
In-Reply-To: <20060809164855.51017.qmail@web33609.mail.mud.yahoo.com>
References: <20060809164855.51017.qmail@web33609.mail.mud.yahoo.com>
Message-ID: <44DA1C36.1040602@bank-banque-canada.ca>

nurza m wrote:
> Hi list,
> How to do numerical differentiation in R. is it using
> genD in numDeriv package

This depends on why you want the derivatives. The functions in numDeriv 
use Richardson extrapolation to get fairly accurate numerical 
derivatives. If you want the derivatives for optimization, then you will 
probably want to sacrifice accuracy for something faster.

In numDeriv, genD calculates both first and second derivatives of vector 
values functions.  If you want accurate numerical derivatives, possibly 
you want something simpler from numDeriv, like grad, jacobian, or hessian.
> 
> thanks in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential inform...{{dropped}}


From kubovy at virginia.edu  Wed Aug  9 20:00:23 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 9 Aug 2006 14:00:23 -0400
Subject: [R] R2HTML: request for an extended example
In-Reply-To: <44D9E1BA.7010808@telenet.be>
References: <F061CE75-17E6-4647-8613-2DA559B49B3B@virginia.edu>
	<44D9E1BA.7010808@telenet.be>
Message-ID: <90486BF3-2B42-439C-9E63-24A6BAADFA6A@virginia.edu>

On Aug 9, 2006, at 9:23 AM, Tobias Verbeke wrote:
> Michael Kubovy wrote:
>> Dear R-helpers,
>>
>> If you know of an extended example of the use of R2HTML, in which  
>> the  various constructs are present in one place, could you please  
>> point  me to it or send it to me?
>>
> There was an article in the R News of December 2003
>
> http://cran.r-project.org/doc/Rnews/

Thanks, Tobias. That's just what I needed.

I ran into a small problem: when I run the following, the HTML is  
missing some kind of icon at the top. Any ideas how to fix that?

 > HTMLStart(outdir = file.path("/Users", "mk", "Documents", "teach",  
'2006.3.PSYC712'), echo = T, HTMLframe = F, Title = 'John Doe HW 1')

*** Output redirected to directory:  /Users/mk/Documents/teach/ 
2006.3.PSYC712
*** Use HTMLStop() to end redirection.[1] TRUE
HTML> as.title("This is my first title")
[1] "This is my first title"
attr(,"class")
[1] "title"
HTML> x <- 1
HTML> y<- 2
HTML> x+y
[1] 3
HTML> HTMLStop()
[1] "/Users/mk/Documents/teach/2006.3.PSYC712/index.html"
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From bates at stat.wisc.edu  Wed Aug  9 20:16:28 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 9 Aug 2006 13:16:28 -0500
Subject: [R] lme4 and lmeSplines
In-Reply-To: <adf71a630608021225u73a2a547i9ba43fb930980a04@mail.gmail.com>
References: <adf71a630608021225u73a2a547i9ba43fb930980a04@mail.gmail.com>
Message-ID: <40e66e0b0608091116x60caadccx9c67e2f29cee525a@mail.gmail.com>

On 8/2/06, Kevin Wright <kwright68 at gmail.com> wrote:
> I'm trying to use the lmeSplines package together with lme4.
>
> Below is (1) an example of lmeSplines together with nlme (2) an
> attempt to use lmeSplines with lme4 (3) then a comparison of the
> random effects from the two different methods.
>
> (1)
>
> require(lmeSplines)
> data(smSplineEx1)
> dat <- smSplineEx1
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> dat$all <- rep(1,nrow(dat))
> times20 <- seq(1,100,length=20)
> Zt20 <- smspline(times20)
> dat$Zt20 <- approx.Z(Zt20, times20, dat$time)
> fit1.20 <- lme(y~time, data=dat, random=list(all=pdIdent(~Zt20-1)))
> # Loess model
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> # Spline model
> with(dat, lines(fitted(fit1.20)~time, col="red"))
> # Save random effects for later
> ranef.nlme <- unlist(ranef(fit1.20))
>
> (2) Now an attempt to use lme4:
>
> library(lmeSplines)
> detach(package:nlme)
> library(lme4)
> data(smSplineEx1)
> # Use 20 spline in lme4
> dat <- smSplineEx1
> times20 <- seq(1,100,length=20)
> Zt20 <- smspline(times20)
> dat <- cbind(dat, approx.Z(Zt20, times20, dat$time))
> names(dat)[4:21] <- paste("Zt",names(dat)[4:21],sep="")
> dat$all <- rep(1, nrow(dat))
> fit1.20 <- lmer(y~time
>              +(-1+Zt1|all)+(-1+Zt2|all)+(-1+Zt3|all)+(-1+Zt4|all)+(-1+Zt5|all)+(-1+Zt6|all)
>              +(-1+Zt7|all)+(-1+Zt8|all)+(-1+Zt9|all)+(-1+Zt10|all)+(-1+Zt11|all)+(-1+Zt12|all)
>              +(-1+Zt13|all)+(-1+Zt14|all)+(-1+Zt15|all)+(-1+Zt16|all)+(-1+Zt17|all)+(-1+Zt18|all),
>              data=dat)
> #summary(fit1)
> # Plot the data and loess fit
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> # Fitting with splines
> with(dat, lines(fitted(fit1.20)~time, col="red"))
> ranef.lme4 <- unlist(ranef(fit1.20))
>
> (3) Compare nlme lme4 random effects
>
> plot(ranef.nlme~ranef.lme4)
>
> The plot of fitted values from lme4 is visually appealing, but the
> random effects from lme4 are peculiar--three are non-zero and the rest
> are essentially zero.
>
> Any help in getting lme4 + lmeSplines working would be appreciated.
> It is not unlikely that I have the lmer syntax wrong.

It is not surprising that you get different answers from lme and lme4
because you are fitting different models.  The variances of the random
effects for the spline basis in the lme fit are constrained to be
equal.  In the lmer fit they are not constrained to be equal.  It is
interesting that you get all but three of the variances essentially
zero.  That means that there are only three active components in your
spline basis, out of 20, for the fit.

I exchanged some mail off-list with Rod Ball about the definition of
the random effects needed for lmeSplines and we concluded that the
current capabilities in lmer are not sufficiently flexible to use it
for lmeSplines.  However, the sources for lmer are freely available
and any enterprising programmer who would like to use the components
for a more flexible model is welcome to do so.

The point is that the tools are available in lmer to represent a
mixed-effects model and to evaluate the log-likelihood or restricted
log-likelihood from such a model very efficiently.  To optimize a
model such as that being used in the lme fit one needs to go from the
parameters to the model representation.  This is the part that would
need to be written and after that you could hook into existing code.

> Kevin Wright
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lanre.okusanya at gmail.com  Wed Aug  9 20:37:56 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Wed, 9 Aug 2006 14:37:56 -0400
Subject: [R] Unique rows
Message-ID: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>

hello all,

I have a dataset where the subjects are duplicated. How do I subset
such that I can get only I row/subject.

aa<-c(1,1,2,2,3,3,4,4,5,5,6,6)
bb<-c(56,56,33,33,53,53,20,20,63,63,9,9)
cc<-data.frame(aa,bb)

I would like to subset df(cc) such that I can get
aa bb
1 56
2 33
3 53
4 20
5 63
6 9

I know this should be fairly easy but I can't figure how to do it in a
dataframe and keep all my columns

Thanks


From Dimitris.Rizopoulos at med.kuleuven.be  Wed Aug  9 20:48:14 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Wed, 09 Aug 2006 20:48:14 +0200
Subject: [R] Unique rows
In-Reply-To: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>
References: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>
Message-ID: <20060809204814.up6dkqsjqkn40k8k@webmail3.kuleuven.be>

if you want the first row for the unique 'aa' entries, try the following:

cc[!duplicated(cc$aa), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm



Quoting Lanre Okusanya <lanre.okusanya at gmail.com>:

> hello all,
>
> I have a dataset where the subjects are duplicated. How do I subset
> such that I can get only I row/subject.
>
> aa<-c(1,1,2,2,3,3,4,4,5,5,6,6)
> bb<-c(56,56,33,33,53,53,20,20,63,63,9,9)
> cc<-data.frame(aa,bb)
>
> I would like to subset df(cc) such that I can get
> aa bb
> 1 56
> 2 33
> 3 53
> 4 20
> 5 63
> 6 9
>
> I know this should be fairly easy but I can't figure how to do it in a
> dataframe and keep all my columns
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jholtman at gmail.com  Wed Aug  9 20:49:57 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 9 Aug 2006 14:49:57 -0400
Subject: [R] Unique rows
In-Reply-To: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>
References: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>
Message-ID: <644e1f320608091149n41ececffo271daa182574ef3c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/a0ee59d0/attachment.pl 

From lanre.okusanya at gmail.com  Wed Aug  9 20:50:33 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Wed, 9 Aug 2006 14:50:33 -0400
Subject: [R] Unique rows
In-Reply-To: <dd6040c90608091141n2a5966casf8450a3b44797e69@mail.gmail.com>
References: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>
	<dd6040c90608091141n2a5966casf8450a3b44797e69@mail.gmail.com>
Message-ID: <6e25bb420608091150l7737136dm93363b073784c018@mail.gmail.com>

Thanks. I tried that, however for some reason, it still left some duplicates

On 8/9/06, Gary Collins <collins.gs at gmail.com> wrote:
> try
>
> > unique(cc)
>    aa bb
> 1   1 56
> 3   2 33
> 5   3 53
> 7   4 20
> 9   5 63
> 11  6  9
>
> HTH
>
> Gary
>
> On 09/08/06, Lanre Okusanya <lanre.okusanya at gmail.com> wrote:
> > hello all,
> >
> > I have a dataset where the subjects are duplicated. How do I subset
> > such that I can get only I row/subject.
> >
> > aa<-c(1,1,2,2,3,3,4,4,5,5,6,6)
> > bb<-c(56,56,33,33,53,53,20,20,63,63,9,9)
> > cc<-data.frame(aa,bb)
> >
> > I would like to subset df(cc) such that I can get
> > aa bb
> > 1 56
> > 2 33
> > 3 53
> > 4 20
> > 5 63
> > 6 9
> >
> > I know this should be fairly easy but I can't figure how to do it in a
> > dataframe and keep all my columns
> >
> > Thanks
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From lanre.okusanya at gmail.com  Wed Aug  9 20:53:53 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Wed, 9 Aug 2006 14:53:53 -0400
Subject: [R] Unique rows
In-Reply-To: <20060809204814.up6dkqsjqkn40k8k@webmail3.kuleuven.be>
References: <6e25bb420608091137v12fe8886rbba5a30ea21fcc5e@mail.gmail.com>
	<20060809204814.up6dkqsjqkn40k8k@webmail3.kuleuven.be>
Message-ID: <6e25bb420608091153l23962b51p69a42339c390c6f7@mail.gmail.com>

Thanks! That worked well

On 8/9/06, Dimitrios Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
> if you want the first row for the unique 'aa' entries, try the following:
>
> cc[!duplicated(cc$aa), ]
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>       http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
>
> Quoting Lanre Okusanya <lanre.okusanya at gmail.com>:
>
> > hello all,
> >
> > I have a dataset where the subjects are duplicated. How do I subset
> > such that I can get only I row/subject.
> >
> > aa<-c(1,1,2,2,3,3,4,4,5,5,6,6)
> > bb<-c(56,56,33,33,53,53,20,20,63,63,9,9)
> > cc<-data.frame(aa,bb)
> >
> > I would like to subset df(cc) such that I can get
> > aa bb
> > 1 56
> > 2 33
> > 3 53
> > 4 20
> > 5 63
> > 6 9
> >
> > I know this should be fairly easy but I can't figure how to do it in a
> > dataframe and keep all my columns
> >
> > Thanks
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


From mschwartz at mn.rr.com  Wed Aug  9 21:04:18 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 09 Aug 2006 14:04:18 -0500
Subject: [R] categorical data
In-Reply-To: <44DA0824.70109@dhlaw.de>
References: <44DA0824.70109@dhlaw.de>
Message-ID: <1155150258.3790.242.camel@localhost.localdomain>

On Wed, 2006-08-09 at 18:07 +0200, Christian Oswald wrote:
> Dear List,
> 
> I neeed a grouped list with two sort of categorical data. I have a data
> .frame like this.
> 	year	cat.	b	c
> 1	2006	a1	125	212
> 2	2006	a2	256	212	
> 3	2005	a1	14	12
> 4	2004	a3	565	123
> 5	2004	a2	156	789	
> 6	2005	a1	1	456
> 7	2003	a2	786	123
> 8	2003	a1	421	569
> 9  	2002	a2	425	245
> 
> I need a list with the sum of b and c for every year and every cat (a1,
> a2 or a3) in this year. I had used the tapply function to build the sum
> for every year or every cat. How can I combine the two grouping values?

Christian,

Is that what you want (using DF as your data.frame):

> aggregate(DF[, c("b", "c")], 
            by = list(Year = DF$year, Cat = DF$cat.),
            sum)
  Year Cat   b   c
1 2003  a1 421 569
2 2005  a1  15 468
3 2006  a1 125 212
4 2002  a2 425 245
5 2003  a2 786 123
6 2004  a2 156 789
7 2006  a2 256 212
8 2004  a3 565 123

You can also reorder the results by Year and Cat:

> DF.result <- aggregate(DF[, c("b", "c")], 
                         by = list(Year = DFyear, Cat = DF$cat.), 
                         sum)

> DF.result[order(DF.result$Year, DF.result$Cat), ]
  Year Cat   b   c
4 2002  a2 425 245
1 2003  a1 421 569
5 2003  a2 786 123
6 2004  a2 156 789
8 2004  a3 565 123
2 2005  a1  15 468
3 2006  a1 125 212
7 2006  a2 256 212



Note that tapply() can only handle one 'X' vector at a time, whereas
aggregate can handle multiple 'X' columns in one call. For example:

> tapply(DF$b, list(DF$year, DF$cat.), sum)
      a1  a2  a3
2002  NA 425  NA
2003 421 786  NA
2004  NA 156 565
2005  15  NA  NA
2006 125 256  NA

will give you the sum of 'b' for each combination of Year and Cat within
the 2d table, but I suspect this is not the output format you want. You
also get NA's in the cells where there was not the given combination
present in your data.

HTH,

Marc Schwartz


From Robert.McGehee at geodecapital.com  Wed Aug  9 21:23:11 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 9 Aug 2006 15:23:11 -0400
Subject: [R] Weighted Mean Confidence Interval
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C9468CC@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,
I'm looking to calculate a 95% confidence interval about my estimate for
a sample's weighted mean, where the calculated confidence interval would
equal the t-test confidence interval of the sample in the case when all
of the weights are equal.

My initial thought was to simply implement a modified version of the
t-test function but substituting the weighted variance and mean for the
unweighted variance and mean.

For example:

weighted.ttest.ci <- function(x, weights, conf.level = 0.95) {
    require(Hmisc)
    nx <- length(x)
    df <- nx - 1
    vx <- wtd.var(x, weights, normwt = TRUE) ## From Hmisc
    mx <- weighted.mean(x, weights)
    stderr <- sqrt(vx/nx)
    tstat <- mx/stderr ## not mx - mu
    alpha <- 1 - conf.level
    cint <- qt(1 - alpha/2, df)
    cint <- tstat + c(-cint, cint)
    cint * stderr
}

However, in the below extreme case where the weights are 0 for 8 of the
samples and 1 otherwise we get a much tighter confidence interval rather
than a wider one that I would expect. So it seems that the above
implementation is not correct, and perhaps lacks from some kind of
degrees of freedom adjustment (?).

> weighted.ttest(1:10, c(1, 1, rep(0, 8)))
[1] 1.122974 1.877026

Has anyone implemented a function for calculating confidence intervals
for means about weighted samples, or can anyone provide guidance for
implementing one? I realize I could always bootstrap the sample, but an
analytic (read: fast) solution is desirable if available.

Thanks in advance,
Robert

Robert McGehee
Quantitative Analyst
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}


From cspark at ces.clemson.edu  Wed Aug  9 21:31:48 2006
From: cspark at ces.clemson.edu (C. Park)
Date: Wed, 9 Aug 2006 15:31:48 -0400
Subject: [R] Plot with Julian dates.
Message-ID: <20060809192024.M95997@ces.clemson.edu>

Dear Sir/Madam:

I simply want to draw x-y plot with Julian dates (x) and numbers (y).
Please see below for my program.

In the older version of R, the plot(jdat, miles) worked without any problem.
But, with the new version of R, plot(jdat, miles) does not work any more. 
So, I added log="" option (as far as I know, it is a default setting, so
log="" should not be needed). 
Now, it works with log="" option. But it is weird to me. 

I also check
plot( jdat, miles, log="", type="p")

The option type="p" is also a default. So it should be the same as 
plot( jdat, miles, log=""). The figures are the same, but the one with 
type="p" option yields a warning message. 

Do you think it is a R bug?
Thanks for your valuable time.
C. Park

Note: I used the following R version                         
platform       i686-redhat-linux-gnu     
arch           i686                      
os             linux-gnu                 
system         i686, linux-gnu           
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)

##=========================================
## Here is my program
##=========================================

library(survival)

dat = c( "12-5-1995",  "12-30-1995", "8-27-1996", "11-20-1996", "8-29-1997",
         "11-7-1997",  "4-4-1998",   "7-15-1998", "11-2-1998",  "1-16-1999" )
miles = c(106,   1468, 7298, 7935, 13440,  14460, 17132, 20214, 22802, 24724 )
jdat = as.date(dat)   ## Convert to Julian date form.

 plot( jdat, miles )   ## Not working
Error in plot.window(xlim, ylim, log, asp, ...) : 
        "log=" specification must be character

 plot( jdat, miles, log="" )  ## Works

 plot( jdat, miles, log="", type="p") ## Works, but warning messages
Warning message:
graphical parameter "type" is obsolete in: axis(side, at, labels, tick, line,
pos, outer, font, lty, lwd,


From rab45+ at pitt.edu  Wed Aug  9 21:43:12 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 09 Aug 2006 15:43:12 -0400
Subject: [R] Linear Trend in Residiuals From lme
Message-ID: <1155152592.3424.8.camel@localhost.localdomain>

I'm fitting a mixed effects model:

fit.1 <- lme(y~x,random=~1|id,data=df)

There are two different observations for each id for both x and y. When
I use plot(fit.1), there is a strong increasing linear trend in the
residuals versus the fitted values (with no outliers). This also happens
if I use random=~x|id. Am I specifying something incorrectly?

Rick B.


From tobias.verbeke at telenet.be  Wed Aug  9 22:04:31 2006
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Wed, 09 Aug 2006 22:04:31 +0200
Subject: [R] R2HTML: request for an extended example
In-Reply-To: <90486BF3-2B42-439C-9E63-24A6BAADFA6A@virginia.edu>
References: <F061CE75-17E6-4647-8613-2DA559B49B3B@virginia.edu>
	<44D9E1BA.7010808@telenet.be>
	<90486BF3-2B42-439C-9E63-24A6BAADFA6A@virginia.edu>
Message-ID: <44DA3FCF.6000609@telenet.be>

Michael Kubovy wrote:
> On Aug 9, 2006, at 9:23 AM, Tobias Verbeke wrote:
>> Michael Kubovy wrote:
>>> Dear R-helpers,
>>>
>>> If you know of an extended example of the use of R2HTML, in which 
>>> the  various constructs are present in one place, could you please 
>>> point  me to it or send it to me?
>>>
>> There was an article in the R News of December 2003
>>
>> http://cran.r-project.org/doc/Rnews/
>
> Thanks, Tobias. That's just what I needed.
>
> I ran into a small problem: when I run the following, the HTML is 
> missing some kind of icon at the top. Any ideas how to fix that?
>
> > HTMLStart(outdir = file.path("/Users", "mk", "Documents", "teach", 
> '2006.3.PSYC712'), echo = T, HTMLframe = F, Title = 'John Doe HW 1')
>
> *** Output redirected to directory:  
> /Users/mk/Documents/teach/2006.3.PSYC712
> *** Use HTMLStop() to end redirection.[1] TRUE
> HTML> as.title("This is my first title")
> [1] "This is my first title"
> attr(,"class")
> [1] "title"
> HTML> x <- 1
> HTML> y<- 2
> HTML> x+y
> [1] 3
> HTML> HTMLStop()
> [1] "/Users/mk/Documents/teach/2006.3.PSYC712/index.html"
I adapted your code to my situation (using R2HTML 1.57 on
R 2.3.1 under GNU/Linux and Firefox 1.5.0.4 to open the
resulting HTML file) and did not notice a missing icon.

What browser do you use ?

library(R2HTML)
HTMLStart(outdir = file.path("/home", "tobias", "R", "packages", "R2HTML"),
                   # mind the TRUE instead of T and the FALSE instead of F
                   echo = TRUE, HTMLframe = FALSE, Title = "John Doe HW 1")
as.title("This is my first title")
x <- 1
y <- 2
x + y
HTMLStop()

--Tobias

PS page source code obtained:

<html xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title> John Doe HW 1 </title>
<link rel=stylesheet href="R2HTML.css" type=text/css>
<object id="mathplayer" classid="clsid:32F66A20-7614-11D4-BD11-00104BD3F987"></object>
<?import namespace="mml" implementation="#mathplayer"?>
<script type="text/javascript" src="ASCIIMathML.js"></script>
<link href="./runtime/styles/xp/grid.css" rel="stylesheet" type="text/css" ></link>
<link href="gridR2HTML.css" rel="stylesheet" type="text/css" ></link>

<script src="./runtime/lib/grid.js"></script>

<script src="./gridR2HTML.js"></script>

<script>
   nequations=0;
</script>
</head>
<body onload="translate()" bgcolor= FFFFFF background="" >
<a name=Num2>&nbsp;</a><p><xmp class=command>> as.title("This is my first title")</xmp></p>
 <h2 > This is my first title</h2>
<a name=Num3>&nbsp;</a><p><xmp class=command>> x <- 1</xmp></p>
<a name=Num4>&nbsp;</a><p><xmp class=command>> y <- 2</xmp></p>
<a name=Num5>&nbsp;</a><p><xmp class=command>> x + y</xmp></p><p class='numeric'>3</p>

<hr size=1>
<font size=-1>
	 Generated on: <I>Wed Aug  9 21:29:45 2006</i> - <b>R2HTML</b> 
<hr size=1>
	</body>
</html


From bates at stat.wisc.edu  Wed Aug  9 22:04:51 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 9 Aug 2006 15:04:51 -0500
Subject: [R] Linear Trend in Residiuals From lme
In-Reply-To: <1155152592.3424.8.camel@localhost.localdomain>
References: <1155152592.3424.8.camel@localhost.localdomain>
Message-ID: <40e66e0b0608091304s76dadbbdnebdd9d395ace7420@mail.gmail.com>

On 8/9/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> I'm fitting a mixed effects model:
>
> fit.1 <- lme(y~x,random=~1|id,data=df)
>
> There are two different observations for each id for both x and y. When
> I use plot(fit.1), there is a strong increasing linear trend in the
> residuals versus the fitted values (with no outliers). This also happens
> if I use random=~x|id. Am I specifying something incorrectly?

Could you provide a reproducible example please?

I suspect that the problem comes from having only two observations per
level of id.  When you have very few observations per group the roles
of the random effect and the per-observation noise term in explaining
the variation become confounded.  However, I can't check if this is
the case without looking at some data and model fits.


From fb572 at hotmail.com  Wed Aug  9 22:20:15 2006
From: fb572 at hotmail.com (Frank Black)
Date: Wed, 09 Aug 2006 16:20:15 -0400
Subject: [R] optim error
Message-ID: <BAY116-F3525CEA2390A8A4B7C0096FB550@phx.gbl>

Dear all,

There have been one or two questions posted to the list regarding the optim 
error "non-finite finite-difference value [4]."  The error apparently means 
that the 4th element of the gradient is non-finite.  My question is what 
part(s) of my program should I fiddle with in an attempt to fix it?  
Starting values?  Something in the log-likelihood itself?  Perhaps the data 
(which is generated)?  Any thoughts would be greatly appreciated.

Thanks, Frank


From rab45+ at pitt.edu  Wed Aug  9 22:32:51 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 09 Aug 2006 16:32:51 -0400
Subject: [R] Linear Trend in Residiuals From lme
In-Reply-To: <40e66e0b0608091304s76dadbbdnebdd9d395ace7420@mail.gmail.com>
References: <1155152592.3424.8.camel@localhost.localdomain>
	<40e66e0b0608091304s76dadbbdnebdd9d395ace7420@mail.gmail.com>
Message-ID: <1155155572.3424.17.camel@localhost.localdomain>

On Wed, 2006-08-09 at 15:04 -0500, Douglas Bates wrote:
> On 8/9/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> > I'm fitting a mixed effects model:
> >
> > fit.1 <- lme(y~x,random=~1|id,data=df)
> >
> > There are two different observations for each id for both x and y. When
> > I use plot(fit.1), there is a strong increasing linear trend in the
> > residuals versus the fitted values (with no outliers). This also happens
> > if I use random=~x|id. Am I specifying something incorrectly?
> 
> Could you provide a reproducible example please?
> 
> I suspect that the problem comes from having only two observations per
> level of id.  When you have very few observations per group the roles
> of the random effect and the per-observation noise term in explaining
> the variation become confounded.  However, I can't check if this is
> the case without looking at some data and model fits.

Unfortunately, I can't send the actual data. I did make a simple
intercepts-only example with two observations per group but it does not
exhibit the linear trend.

library(nlme)

x <- rnorm(20,5,1)
id <- factor(rep(1:20,each=2))

y <- as.vector(sapply(x,rnorm,n=2,sd=0.2))

df <- data.frame(id,y)
df.gd <- groupedData(y~x|id,data=df)

summary(lme.1 <- lme(y~1,random=~1|id,data=df.gd))
plot(lme.1)

If I fit an intercepts-only model to the actual data, I still see the
trend in the residuals.

What other analysis would you suggest?

Rick B.


From pburns at pburns.seanet.com  Wed Aug  9 22:41:49 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 09 Aug 2006 21:41:49 +0100
Subject: [R] Speeding indexing and sub-sectioning of 3d array
In-Reply-To: <C0FF6B41.110A%swidanf@janelia.hhmi.org>
References: <C0FF6B41.110A%swidanf@janelia.hhmi.org>
Message-ID: <44DA488D.4040104@pburns.seanet.com>

First off, I hope that the function you list is just an example
since it only returns what the last iteration does -- obviously
the same answer can be arrived at much quicker.

The main principal in speeding up loops is to do as little
inside the loops as possible.  'fjj1' is essentially the same as
the listed function, but with one slight cleanup.

fjj1 <-
function(x, radius=3)
{
        dx <- dim(x)
        dx1 <- dx[1]
        dx2 <- dx[2]
        dx3 <- dx[3]
        for(i in (radius + 1):(dx1 - radius - 1)) {
                for(j in (radius + 1):(dx2 - radius - 1)) {
                        for(k in (radius + 1):(dx3 - radius -1)) {
                                ans <- mean(x[(i-radius):(i+radius),
                                        (j-radius):(j+radius), 
(k-radius):(k+radius)])
                        }
                }
        }
        ans
}

The time to run fjj1(jj, 3) on my machine where jj is a
245 by 175 by 150 array was 1222 seconds.

'fjj2'  substantially reduces the number of sequences
created.  It took 975 seconds.

fjj2 <-
function(x, radius=3)
{
        dx <- dim(x)
        dx1 <- dx[1]
        dx2 <- dx[2]
        dx3 <- dx[3]
        rseq <- -radius:radius
        for(i in (radius + 1):(dx1 - radius - 1)) {
                for(j in (radius + 1):(dx2 - radius - 1)) {
                        for(k in (radius + 1):(dx3 - radius -1)) {
                                ans <- mean(x[i + rseq, j + rseq, k + rseq])
                        }
                }
        }
        ans
}


'fjj3' reduces some of the subscripting (but possibly at the
expense of using more memory -- I'm not sure if it does or
not).  It took 936 seconds.

fjj3 <-
function(x, radius=3)
{
        dx <- dim(x)
        dx1 <- dx[1]
        dx2 <- dx[2]
        dx3 <- dx[3]
        rseq <- -radius:radius
        for(i in (radius + 1):(dx1 - radius - 1)) {
                A <- x[i + rseq, , , drop=FALSE]
                for(j in (radius + 1):(dx2 - radius - 1)) {
                        B <- A[, j + rseq, , drop=FALSE]
                        for(k in (radius + 1):(dx3 - radius -1)) {
                                ans <- mean(B[ , , k + rseq])
                        }
                }
        }
        ans
}

'fjj4' reverses the order of the loops.  Because of the
way that arrays are stored, it makes sense that subscripting
a sequence in the first dimension would be faster than
subscripting subsequent dimensions.  This does seem to be
the case.  'fjj4' took 839 seconds.

fjj4 <-
function(x, radius=3)
{
        dx <- dim(x)
        dx1 <- dx[1]
        dx2 <- dx[2]
        dx3 <- dx[3]
        rseq <- -radius:radius
        for(i in (radius + 1):(dx3 - radius - 1)) {
                A <- x[, ,i + rseq, drop=FALSE]
                for(j in (radius + 1):(dx2 - radius - 1)) {
                        B <- A[, j + rseq, , drop=FALSE]
                        for(k in (radius + 1):(dx1 - radius -1)) {
                                ans <- mean(B[k + rseq, , ])
                        }
                }
        }
        ans
}


Another change that would make a marginal difference
would be to generate the sequences controlling the inner
loops once at the outset.

If the computation at the heart of the function really is a
mean or something similar, then it is possible that there
will be tricks to update that value more efficiently.

Finally, if this will be used enough that the speed is an
issue, then rewriting it in C would be a good approach.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Swidan, Firas wrote:

>Hi,
>
>I am having a problem with a very slow indexing and sub-sectioning of a 3d
>array:
>
>  
>
>>dim(arr)
>>    
>>
>[1] 245 175 150
>
>For each point in the array, I am trying to calculate the mean of the values
>in its surrounding:
>
>
>mean( arr[ (i - radius):(i + radius),
>                                (j - radius):(j + radius),
>                                (k - radius):(k + radius)] )
>
>Putting that code in 3 for loops
>
>calculateKMedian <- function( arr, radius){
>
>  for( i in (radius + 1):(dim(arr)[1] - radius - 1) ){
>    for( j in (radius + 1):(dim(arr)[2] - radius - 1) )
>      for( k in (radius + 1):(dim(arr)[3] - radius - 1) ){
>
>
>        mediansArr <- mean( arr[ (i - radius):(i + radius),
>                                (j - radius):(j + radius),
>                                (k - radius):(k + radius)] )
>
>      }
>  }
>  return(mediansArr)
>}
>
>Results in a very slow run:
>
>  
>
>>system.time( calculateKMedian( a, 3))
>>    
>>
>[1] 423.468   0.096 423.631   0.000   0.000
>
>If I replace 
>
>        mediansArr <- mean( arr[ (i - radius):(i + radius),
>                                (j - radius):(j + radius),
>                                (k - radius):(k + radius)] )
>
>With an access to the (I,j,k) cell's value
>
> mediansArr <- arr[i,j,k]
>
>The running time decreases to
>
>  
>
>>system.time( calculateKMedian( a, 3))
>>    
>>
>[1] 14.821  0.005 14.829  0.000  0.000
>
>
>
>But 14 seconds are still pretty expensive for just scanning the array.
>
>Is there anything I can do to speed the indexing and the sub-sectioning of
>the 3d array in this case?
>
>Thanks for the help,
>Firas.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From rab45+ at pitt.edu  Wed Aug  9 22:55:39 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 09 Aug 2006 16:55:39 -0400
Subject: [R] Linear Trend in Residiuals From lme
In-Reply-To: <40e66e0b0608091304s76dadbbdnebdd9d395ace7420@mail.gmail.com>
References: <1155152592.3424.8.camel@localhost.localdomain>
	<40e66e0b0608091304s76dadbbdnebdd9d395ace7420@mail.gmail.com>
Message-ID: <1155156939.3424.21.camel@localhost.localdomain>

On Wed, 2006-08-09 at 15:04 -0500, Douglas Bates wrote:
> On 8/9/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> > I'm fitting a mixed effects model:
> >
> > fit.1 <- lme(y~x,random=~1|id,data=df)
> >
> > There are two different observations for each id for both x and y. When
> > I use plot(fit.1), there is a strong increasing linear trend in the
> > residuals versus the fitted values (with no outliers). This also happens
> > if I use random=~x|id. Am I specifying something incorrectly?
> 
> Could you provide a reproducible example please?
> 
> I suspect that the problem comes from having only two observations per
> level of id.  When you have very few observations per group the roles
> of the random effect and the per-observation noise term in explaining
> the variation become confounded.  However, I can't check if this is
> the case without looking at some data and model fits.

I tried using geeglm from geepack to fit a marginal model. I understand
this is not the same as a mixed effects model but the residuals don't
have the linear trend. Should I avoid using lme in this case?

Rick B.


From JeeBee at troefpunt.nl  Wed Aug  9 23:19:25 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 09 Aug 2006 23:19:25 +0200
Subject: [R] tk combobox question
References: <44D9F4C0.3080106@gmx.de>
Message-ID: <pan.2006.08.09.21.19.23.432510@troefpunt.nl>


I think you want something like this:

  require(tcltk) || stop("Package tcltk is not available.")
  version.BWidget <- tclvalue(tclRequire("BWidget"))

  modify_command <- function() {
    print("Hey, I'm modified...")
  }

  tt <- tktoplevel()

  values = c("foo", "bar", "jeebee")
  combo <- tkwidget(tt, "ComboBox", "-modifycmd", 
                    modify_command, values=values)
  tkgrid(combo)

> How can I get the index of the selected item in an Iwidgets combobox? Till now I 
> am only able to get the contents of the selected combobox item.

For BWidget:
  as.numeric(tclvalue(tcl(combo,"getvalue")))+1

I haven't used Iwdigets yet...

JeeBee.


From Gregor.Gorjanc at bfro.uni-lj.si  Wed Aug  9 23:26:36 2006
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 9 Aug 2006 23:26:36 +0200
Subject: [R] R CMD check and RUnit
References: <7FFEE688B57D7346BC6241C55900E730F31C3B@pollux.bfro.uni-lj.si>
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31C4F@pollux.bfro.uni-lj.si>

I have created page [1] in R Wiki on this topic.

http://wiki.r-project.org/rwiki/doku.php?id=developers:runit

Seth, I have also managed to tweak doRUnit.R so that R CMD check gives an error 
if the tests fail. I will leave windows port for someone else ;)

"Gorjanc Gregor" <Gregor.Gorjanc at ...> writes:
> In case you are interested in fusing RUnit with R CMD check under unix
> alike OS, here is one way of doing/hacking this.
>
> My aim was to perform unit tests:
> (1) during R CMD check
> (2) at any other time

As you know, we've been using the RUnit package in our group here in
Seattle to help with the development of Biobase and graph.  I've found
that writing unit tests improve my coding by making me focus on the
API instead of the implementation and by making bug fixing easier.

I've been meaning to improve the scripts I use to run the tests and
what you've posted is a start in that direction.

I have two additional aims, however, that aren't yet covered:

(3) R CMD check should give an error if the tests fail.
(4) Works on Windows.

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From bates at stat.wisc.edu  Wed Aug  9 23:43:32 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 9 Aug 2006 16:43:32 -0500
Subject: [R] Linear Trend in Residiuals From lme
In-Reply-To: <1155156939.3424.21.camel@localhost.localdomain>
References: <1155152592.3424.8.camel@localhost.localdomain>
	<40e66e0b0608091304s76dadbbdnebdd9d395ace7420@mail.gmail.com>
	<1155156939.3424.21.camel@localhost.localdomain>
Message-ID: <40e66e0b0608091443o1d095940x9f745744fb5e2317@mail.gmail.com>

On 8/9/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> On Wed, 2006-08-09 at 15:04 -0500, Douglas Bates wrote:
> > On 8/9/06, Rick Bilonick <rab45+ at pitt.edu> wrote:
> > > I'm fitting a mixed effects model:
> > >
> > > fit.1 <- lme(y~x,random=~1|id,data=df)
> > >
> > > There are two different observations for each id for both x and y. When
> > > I use plot(fit.1), there is a strong increasing linear trend in the
> > > residuals versus the fitted values (with no outliers). This also happens
> > > if I use random=~x|id. Am I specifying something incorrectly?
> >
> > Could you provide a reproducible example please?
> >
> > I suspect that the problem comes from having only two observations per
> > level of id.  When you have very few observations per group the roles
> > of the random effect and the per-observation noise term in explaining
> > the variation become confounded.  However, I can't check if this is
> > the case without looking at some data and model fits.
>
> I tried using geeglm from geepack to fit a marginal model. I understand
> this is not the same as a mixed effects model but the residuals don't
> have the linear trend. Should I avoid using lme in this case?

Probably.


From amstat2006 at gmail.com  Wed Aug  9 23:45:08 2006
From: amstat2006 at gmail.com (Am Stat)
Date: Wed, 9 Aug 2006 17:45:08 -0400
Subject: [R] How to draw the decision boundaries for LDA and Rpart object
References: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>
	<Pine.LNX.4.64.0608090800030.7250@gannet.stats.ox.ac.uk>
Message-ID: <005101c6bbfd$1599aa70$0400a8c0@LeonE1405>

Dr. Ripley,

Thanks very much for your help. I have used your partition tree and it works 
well.  I am not familiar with the 'tree' package but I found that the 
threshold to make a cut  returned by tree and rpart is almost the same 
value.

Does that mean the decision boundaries for Rpart and Tree are the same for a 
same data when using the default  value of parameters, no matter what the 
structure of data  is?

Thanks very much!

Leon




----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Am Stat" <amstat2006 at gmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 09, 2006 3:04 AM
Subject: Re: [R] How to draw the decision boundaries for LDA and Rpart 
object


> On Wed, 9 Aug 2006, Am Stat wrote:
>
>> Hello useR,
>>
>> Could you please tell me how to draw the decision boundaries in a
>> scatterplot of the original data for a LDA or Rpart object.
>
> There are examples in MASS (the book).
>
>> For example:
>> > library(rpart)
>> >fit.rpart <- rpart(as.factor(group.id)~., data=data.frame(Data) )
>>
>>
>> How can I draw the cutting lines on the orignial Data?
>>
>> Or is there any built in functions that can read the rpart object
>> 'fit.rpart' to do that?
>
> See partition.tree in package tree.
>
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pbarata at infolink.com.br  Thu Aug 10 00:16:45 2006
From: pbarata at infolink.com.br (Paulo Barata)
Date: Wed, 09 Aug 2006 19:16:45 -0300
Subject: [R] parameter yaxs / function hist (graphics)
In-Reply-To: <1155134376.3790.18.camel@localhost.localdomain>
References: <mailman.9.1154685603.13659.r-help@stat.math.ethz.ch>	
	<44D7ECEA.10009@infolink.com.br>	
	<Pine.LNX.4.63.0608072304570.7707@est.ufpr.br>	
	<44D90707.1060002@infolink.com.br>
	<1155134376.3790.18.camel@localhost.localdomain>
Message-ID: <44DA5ECD.8000701@infolink.com.br>


Mr. Schwartz,

Thank you very much for the proper solution.

Regards,

Paulo Barata
(Rio de Janeiro, Brasil)
----------------------------------------------------------------

Marc Schwartz (via MN) wrote:
> Paulo,
> 
> Try the following:
> 
> x <- rnorm(100)
> 
> par(xaxs = "i")
> par(yaxs = "i")
> 
> hist(x, breaks = seq(-4, 4, 0.5), ylim = c(0, 40),
>      xlim = c(-4, 4))
> 
> box()
> 
> 
> The problem is that graphics:::plot.histogram() is coded in such a way
> that the use of 'xaxs' and 'yaxs' are ineffectual, as they are not
> passed to the internal call to plot.window(), which does not provide for
> '...' args in this case. So they cannot be passed 'inline' as you
> attempted.
> 
> Thus, you need to set these prior to the plotting of the histogram.
> 
> BTW, a better option for the call to axis() relative to using the 'line'
> argument, is to use the 'pos' argument and set it to 0:
> 
> x <- rnorm(100)
> 
> par(xaxs = "i")
> par(yaxs = "i")
> 
> hist(x, breaks = seq(-4, 4, 0.5), ylim = c(0, 40),
>      xlim = c(-4, 4), xaxt = "n")
> 
> axis(1, pos = 0)
> 
> box()
> 
> HTH,
> 
> Marc Schwartz
> 
> On Tue, 2006-08-08 at 18:49 -0300, Paulo Barata wrote:
>> Dear Paulo,
>>
>> Thank you for your reply. But I doubt yours is a proper
>> solution to my request, for some reasons:
>>
>> 1. The position of the axis graphed with the command axis(1, line=-1)
>> depends on the size of the graphics window.
>>
>> 2. After your graph is on the screen, in case one may want a boxed
>> graph, a box() command will produce a histogram "floating in the air",
>> not "touching" the horizontal axis.
>>
>> Of course, one could build a proper box (with labels, etc.) by means
>> of more primitive graphics functions like lines (package graphics)
>> and others, but I think that would mean a lot of work.
>>
>> Thank you again.
> 
>> Paulo Justiniano Ribeiro Jr wrote:
>>> Paulo
>>>
>>> One possibility is to draw the histogram without axes and then add them 
>>> wherever you want.
>>>
>>> For instance with something along the lines:
>>>
>>> x <- rnorm(500)
>>> hist(x, axes=F)
>>> axis(1, line=-1)
>>>
>>> For more details: ?axis
>>>
>>> best
>>> P.J.
>>> On Mon, 7 Aug 2006, Paulo Barata wrote:
>>>
>>>> Dear R users,
>>>>
>>>> The parameters xaxs and yaxs (function par, package graphics)
>>>> seem not to work with the function hist (package graphics),
>>>> even when the parameters xlim and ylim are defined.
>>>>
>>>> Is there any way to make yaxs="i" and xaxs="i" work properly
>>>> with the function hist, mainly to produce histograms that
>>>> "touch" the horizontal axis? The R documentation and the
>>>> R mailing lists archive don't seem to be of help here.
>>>>
>>>> I am using R 2.3.1, running under Windows XP.
>>>>
>>>> ## Example:
>>>> x <- rnorm(100)
>>>> hist(x,breaks=seq(-4,4,0.5),ylim=c(0,40),yaxs="i",
>>>>   xlim=c(-4,4),xaxs="i")
>>>> box()
>>>>
>>>> Thank you very much.
>>>>
>>>> Paulo Barata
>>>>
> 
>


From gregor.gorjanc at bfro.uni-lj.si  Thu Aug 10 00:27:05 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 10 Aug 2006 00:27:05 +0200
Subject: [R] Constrain coefs. in linear model to sum to 0
In-Reply-To: <B998A44C8986644EA8029CFE6396A924547676@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924547676@exqld2-bne.qld.csiro.au>
Message-ID: <44DA6139.90501@bfro.uni-lj.si>

Hello,

Bill.Venables at csiro.au wrote:
>  
> Gorjanc Gregor asks:
> 
>> Hello!
>>
>> I would like to use constrain to sum coeficients of a factor to 0
> instead
>> of classical corner contraint i.e. I would like to fit a model like
>>
>> lm(y ~ 1 + effectA + effectB)
>>
>> and say get parameters
>>
>> intercept
>> effectA_1
>> effectA_2
>> effectB_1
>> effectB_2
>> effectB_3
>>
>> where effectA_1 represents deviation of level A_1 from intercept and 
>> sum(effectA_1, effectA_2) = 0 and the same for factor B.
>>
>> Is this possible to do?
> 
> Yes, but not quite as simply as you would like.  If you set
> 
> options(contrasts = c("contr.sum", "contr.poly"))
> 
> for example, then factor models are parametrised as you wish above,
> BUT you don't get all the effects directly
> 
> In your case above, for example, if fm is the fitted model object, then
> 
> coef(fm)
> 
> Will give you intercept, effectA_2, effectB_2, effectB_3.  The
> remaining effects*_1 you will need to calculate as the negative of the
> sum of all the others.
> 
> This gets a bit more complicated when you have crossed terms, a*b, but
> the same principle applies.

Thank you for the response. Peter Dalgaard already mentioned that I can
get missing coefs with taking negative of the sum of displayed coefs.
However, as you already noted that, things are complicated with crossed
terms. I was able to handle nested regression but did not had any luck
with interactions. For example:

### --- Picture of "the model" ---

if(FALSE) {
   | a1   | a2   |
----------------------
b1 |  8   | 14   | 11
----------------------
b2 | 13   | 17   | 15
----------------------
b3 |  2   |  8   |  5
----------------------
b4 |  4   | 10   |  7
----------------------
   |  6.8 | 12.3 |
}

### --- Setup ---

N <- 400
ab <- c( 8, 13,  2,  4,
        14, 17,  8, 10)
sigmaE <- 0.01

levA <- c("a1", "a2")
facA <- factor(sample(rep(levA, times=N / length(levA))))

levB <- c("b1", "b2", "b3", "b4")
facB <- factor(sample(rep(levB, times=N / length(levB))))

table(facA, facB)

facAB <- factor(paste(facA, facB, sep="-"))
yAB <- ab[as.integer(facAB)]

## Create design matrix and simulate y
tmp <- model.matrix(~ facAB - 1)
y <- tmp %*% as.matrix(ab) + rnorm(n=N, mean=0, sd=sigmaE)

contrasts(facA) <- contr.sum
contrasts(facB) <- contr.sum

### --- Fit the model ---

fit <- lm(y ~ facA * facB, data=tmpData)
coefs <- coef(fit)
(int <- coefs[1])
(yMean <- mean(y))

a <- coefs[2]
(a <- c(a, -a))
tapply(y, list(facA), mean)
tapply(y, list(facA), mean) - yMean
## Hmm, why is there such a big difference between mean values and
## coefs from the fit? I am doing something wrong here.

b <- coefs[3:5]
(b <- c(b, -sum(b)))
tapply(y, list(facB), mean)
tapply(y, list(facB), mean) - yMean
## Even more strange

(ab <- coefs[6:8])
## ...#@^+??
tapply(y, list(facA, facB), mean)
tapply(y, list(facA, facB), mean) - yMean
## How can I proceed here?

## Using functions proposed by Martin Maechler:
fitAov <- aov(y ~ facA * facB)
model.tables(fitAov, "means", se=TRUE)
## vauuu, this is great
model.tables(fitAov, "effects", se=TRUE)
## also nice and this fits with my raw mean test

What am I doing wrong then above with coefficients from the model?

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From Yingfu.Xie at sekon.slu.se  Thu Aug 10 00:22:09 2006
From: Yingfu.Xie at sekon.slu.se (Yingfu Xie)
Date: Thu, 10 Aug 2006 00:22:09 +0200
Subject: [R] minimization a quadratic form with some coef fixed and some
	constrained
References: <200608091608.k79G8P3t009025@erdos.math.unb.ca>
Message-ID: <CA871298CD1882459F7859BD08DC06E4C9C0BD@slumail.ad.slu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/c6970162/attachment.pl 

From dchron at essex.ac.uk  Thu Aug 10 01:46:13 2006
From: dchron at essex.ac.uk (Chronopoulos Dimitris)
Date: Thu, 10 Aug 2006 00:46:13 +0100
Subject: [R] decimal accuracy in pnorm( )
Message-ID: <000601c6bc0e$00175150$f07ff59b@patra>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/e3965a32/attachment.pl 

From h.wickham at gmail.com  Thu Aug 10 01:51:02 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 9 Aug 2006 16:51:02 -0700
Subject: [R] How to draw the decision boundaries for LDA and Rpart object
In-Reply-To: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>
References: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>
Message-ID: <f8e6ff050608091651n11bab22wf1b63b5f7673c2d9@mail.gmail.com>

> Could you please tell me how to draw the decision boundaries in a scatterplot of the original data for a LDA or Rpart object.
>
> For example:
> > library(rpart)
> >fit.rpart <- rpart(as.factor(group.id)~., data=data.frame(Data) )
>

You might want to have a look a classifly
(http://had.co.nz/classifly/) which will do this in high dimensions.
(You will need to install ggobi for it to work, however)

Hadley


From john_d_mchenry at yahoo.com  Thu Aug 10 02:30:47 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Wed, 9 Aug 2006 17:30:47 -0700 (PDT)
Subject: [R] Is there a better way than x[1:length(x)-1] ?
Message-ID: <20060810003047.1461.qmail@web35402.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060809/c6421e3b/attachment.pl 

From conanphelan at yahoo.ca  Thu Aug 10 03:04:21 2006
From: conanphelan at yahoo.ca (Conan Phelan)
Date: Wed, 9 Aug 2006 21:04:21 -0400 (EDT)
Subject: [R] graphic output file format
Message-ID: <20060810010422.17838.qmail@web32508.mail.mud.yahoo.com>


I would like to save graphics I produce in jpeg or gif
formats. The GDD package sounds like it should let me
to do this, but I cannot get it to install (error:
Can't find gd.h!). Anyone know what's up?

Thnx, C


From epistat at gmail.com  Thu Aug 10 03:06:27 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 10 Aug 2006 09:06:27 +0800
Subject: [R] how to link matrix with the variables
Message-ID: <2fc17e30608091806i7a3e4ff9m13859d90b76c28b4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/1995fae0/attachment.pl 

From kubovy at virginia.edu  Thu Aug 10 03:21:40 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 9 Aug 2006 21:21:40 -0400
Subject: [R] Capabilities and JGR
In-Reply-To: <44DA3FCF.6000609@telenet.be>
References: <F061CE75-17E6-4647-8613-2DA559B49B3B@virginia.edu>
	<44D9E1BA.7010808@telenet.be>
	<90486BF3-2B42-439C-9E63-24A6BAADFA6A@virginia.edu>
	<44DA3FCF.6000609@telenet.be>
Message-ID: <0B53EE21-62E1-4326-9412-29C6E2C6EFC0@virginia.edu>

Dear R-helpers,

Using the Mac OS X GUI,
 > capabilities()
     jpeg      png    tcltk      X11 http/ftp  sockets   libxml      
fifo   cledit    iconv      NLS
     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE      
TRUE     TRUE     TRUE     TRUE

Under JGR
 > capabilities()
     jpeg      png    tcltk      X11 http/ftp  sockets   libxml      
fifo   cledit    iconv      NLS
    FALSE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE      
TRUE     TRUE     TRUE     TRUE

Is there a way to activate jpeg and png under JGR?

I did a RSiteSearch('png JGR') and no found solution there.

Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0
JGR 1.4.2
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From berwin at maths.uwa.edu.au  Thu Aug 10 03:56:50 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 10 Aug 2006 09:56:50 +0800
Subject: [R] minimization a quadratic form with some coef fixed and
	some	constrained
In-Reply-To: <CA871298CD1882459F7859BD08DC06E4C9C0BD@slumail.ad.slu.se>
References: <200608091608.k79G8P3t009025@erdos.math.unb.ca>
	<CA871298CD1882459F7859BD08DC06E4C9C0BD@slumail.ad.slu.se>
Message-ID: <17626.37474.906761.751666@bossiaea.maths.uwa.edu.au>

>>>>> "YX" == Yingfu Xie <Yingfu.Xie at sekon.slu.se> writes:

    YX> Thanks for reply! But I think that solution is right without
    YX> the constrain b'b=1. With this constrain, the solution is not
    YX> so simple. :(
But simple enough. :)

Write down the Lagrange function for the problem.  Say, 'lam' is the
Lagrange parameter for enforcing the constraint b'b=1.  Then, using
Rolf's notation:
    RT> [...]  Write M as

    RT> | M_11 c |
    RT> | c'   m |

Then the system of equations that b and the Lagrange parameter have to
fulfill is:

        b = (M_11 + lam*I)^{-1} c   (with I being the identity matrix)
and   lam = b' M_11 b - b'c

You can either use the first equation and do a (grid) search for the
value of 'lam' that gives you b'b=1 (could be negative!), or start
with lam=0 and then alternate between the two equations until
convergence.  

At least I think that this will solve your problem. :)  Thinking a bit
about the geometry of the problem, I actually believe that if c=0, you
might have an identifiability problem, i.e. there are at least two 
solutions, or, depending on M_11, infinitely many.

Hope this helps.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From verify at indotradezone.com  Thu Aug 10 04:17:38 2006
From: verify at indotradezone.com (verify at indotradezone.com)
Date: Thu, 10 Aug 2006 09:17:38 +0700
Subject: [R] Your email requires verification
	verify#QTaQ31oVh73RQhN8fgCXtoSQaCqGXMTf
Message-ID: <E1GB078-0001N3-08@quicktrack.techscape.co.id>

The message you sent requires that you verify that you 
are a real live human being and not a spam source.

To complete this verification, simply reply to this message and leave
the subject line intact.

The headers of the message sent from your address are show below:

>From r-help at stat.math.ethz.ch Thu Aug 10 09:17:37 2006
Received: from [202.155.110.119] (helo=stat.math.ethz.ch)
 by quicktrack.techscape.co.id with esmtp (Exim 4.52)
 id 1GB06o-0001IY-PQ
 for andry at indotradezone.com; Thu, 10 Aug 2006 09:17:37 +0700
From: r-help at stat.math.ethz.ch
To: andry at indotradezone.com
Subject: Returned mail: see transcript for details
Date: Thu, 10 Aug 2006 09:26:04 +0700
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="----=_NextPart_000_0004_EA8369B9.1E350EC1"
X-Priority: 3
X-MSMail-Priority: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MIMEOLE: Produced By Microsoft MimeOLE V6.00.2600.0000


From sell_mirage_ne at hotmail.com  Thu Aug 10 04:39:57 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Wed, 09 Aug 2006 21:39:57 -0500
Subject: [R] index.cond in xyplot
Message-ID: <BAY110-F25F97DE1007BEF26271AECC74A0@phx.gbl>

Dear R-users

I have 5 dependent variables (y1 to y5) and one independent variable (x) and 
3 conditioning variables (m, n, and 0). Each of the conditioning variables 
has 2 levels.  I created 2*4 panel plots.

xyplot(y1+y2+y3+y4+y5 ~ x | m*n*o,layout = c(4,2))

I would like to reorder the 8 panels. I tried to use index.cond (e.g., 
index.cond = list(c(1,3,2,4,5,7,6,8)) but it didn't work out. I got a error 
message "Error in cond.orders(foo) : Invalid value of index.cond". Please 
let me know if I didn't use index.cond argument properly.

I looked at the example in R-help but all examples have just only one 
conditioning variable.

Is there any way I can arrange the panels in whatever order I want ?

Thanks

Taka


From ggrothendieck at gmail.com  Thu Aug 10 04:57:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Aug 2006 22:57:31 -0400
Subject: [R] index.cond in xyplot
In-Reply-To: <BAY110-F25F97DE1007BEF26271AECC74A0@phx.gbl>
References: <BAY110-F25F97DE1007BEF26271AECC74A0@phx.gbl>
Message-ID: <971536df0608091957p6c01be1dw12dc5abda5cf151f@mail.gmail.com>

That's not a valid specification.  See the description of the index.cond
argument in ?xyplot and in particular this part:

               If 'index.cond' is a list, it has to be as long as the number of
               conditioning variables, and the 'i'-th component has to
be a valid
               indexing vector for the integer vector '1:nlevels(g_i)'
               (which can, among other things, repeat some of the
               levels or drop some altogether).

Thus in your case index.cond is a list with three components and
each of those components can specify a vector of the levels of interest in
the order of interest.

For example, compare the output of these two to get the idea where
CO2 is a builtin data set:

xyplot(conc ~ uptake | Type * Treatment, CO2, index.cond = list(1:2, 1:2))
xyplot(conc ~ uptake | Type * Treatment, CO2, index.cond = list(1:2, 2:1))

On 8/9/06, Taka Matzmoto <sell_mirage_ne at hotmail.com> wrote:
> Dear R-users
>
> I have 5 dependent variables (y1 to y5) and one independent variable (x) and
> 3 conditioning variables (m, n, and 0). Each of the conditioning variables
> has 2 levels.  I created 2*4 panel plots.
>
> xyplot(y1+y2+y3+y4+y5 ~ x | m*n*o,layout = c(4,2))
>
> I would like to reorder the 8 panels. I tried to use index.cond (e.g.,
> index.cond = list(c(1,3,2,4,5,7,6,8)) but it didn't work out. I got a error
> message "Error in cond.orders(foo) : Invalid value of index.cond". Please
> let me know if I didn't use index.cond argument properly.
>
> I looked at the example in R-help but all examples have just only one
> conditioning variable.
>
> Is there any way I can arrange the panels in whatever order I want ?
>
> Thanks
>
> Taka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kubovy at virginia.edu  Thu Aug 10 05:21:11 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 9 Aug 2006 23:21:11 -0400
Subject: [R] R2HTML incomplete echo
Message-ID: <9D024DB3-2CCE-41A8-8E6D-456ED04A92E9@virginia.edu>

Dear R-helpers,

When I run
 > HTMLStart(outdir = outDir, echo = TRUE, HTMLframe = TRUE,
+     filename = 'jdHW1', Title = 'John Doe HW 2')
*** Output redirected to directory:  /Users/mk/Documents/teach/ 
2006.3.PSYC712
*** Use HTMLStop() to end redirection.[1] TRUE
HTML> as.title("This is my first title")
[1] "This is my first title"
attr(,"class")
[1] "title"
HTML> x <- 1
HTML> y<- 2
HTML> x+y
[1] 3
HTML> plot(sin, -pi, 2*pi, main = 'Sinus')
HTML> HTMLplot(Caption = 'Look at this curve!')
[1] TRUE
HTML> HTMLStop()
[1] "/Users/mk/Documents/teach/2006.3.PSYC712/jdHW1_main.html"


The output in file:///Users/mk/Documents/teach/2006.3.PSYC712/ 
jdHW1.html is correct until we get to the plot() function. The output  
is:
as.title("This is my first title")
 > x <- 1
 > y <- 2
 > x + y
3
 > x(x)
NULL

and then the figure.

What should I do to have it echo the command "plot(sin, -pi, 2*pi,  
main = 'Sinus')" instead of "x(x)"?

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From deepayan.sarkar at gmail.com  Thu Aug 10 05:24:54 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 9 Aug 2006 22:24:54 -0500
Subject: [R] index.cond in xyplot
In-Reply-To: <BAY110-F25F97DE1007BEF26271AECC74A0@phx.gbl>
References: <BAY110-F25F97DE1007BEF26271AECC74A0@phx.gbl>
Message-ID: <eb555e660608092024s411d0d93g6850e7adc4ef30f2@mail.gmail.com>

On 8/9/06, Taka Matzmoto <sell_mirage_ne at hotmail.com> wrote:
> Dear R-users
>
> I have 5 dependent variables (y1 to y5) and one independent variable (x) and
> 3 conditioning variables (m, n, and 0). Each of the conditioning variables
> has 2 levels.  I created 2*4 panel plots.
>
> xyplot(y1+y2+y3+y4+y5 ~ x | m*n*o,layout = c(4,2))
>
> I would like to reorder the 8 panels. I tried to use index.cond (e.g.,
> index.cond = list(c(1,3,2,4,5,7,6,8)) but it didn't work out. I got a error
> message "Error in cond.orders(foo) : Invalid value of index.cond". Please
> let me know if I didn't use index.cond argument properly.

Gabor has already explained why this is wrong.

> I looked at the example in R-help but all examples have just only one
> conditioning variable.
>
> Is there any way I can arrange the panels in whatever order I want ?

Think of a lattice plot as a an array, with each conditioning variable
defining a dimension. In your case, it's a 3-dimensional array. If you
assign the result of the xyplot call to a variable, you can reorder
the indices just like an array, e.g.:

p <- xyplot(y1+y2+y3+y4+y5 ~ x | m*n*o,layout = c(4,2))
p[, 2:1,  1]

(this is just a convenient way of specifying index.cond). You cannot
arrange the panels ``any way you want'', you can only rearrange
columns/rows/whatever.

If you really do want an arbitrary arrangement, then you don't want 3
different conditioning variables, you want only one. If you want a
separate panel for each combination of m, n and o, create a new factor
as their interaction, e.g.

p <- xyplot(y1+y2+y3+y4+y5 ~ x | m:n:o,layout = c(4,2))

Then, you have a one-dimensional vector-like object, which you can reorder by

p[c(1,3,2,4,5,7,6,8)]

etc.

-Deepayan


From hodgess at gator.dt.uh.edu  Thu Aug 10 06:40:24 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 9 Aug 2006 23:40:24 -0500
Subject: [R]  OT UNIX grep question
Message-ID: <200608100440.k7A4eOae003670@gator.dt.uh.edu>

Dear R People:

I want to use the "grep" command in UNIX/Linux to check
some words from the dictionary.

Let's say I use:

grep dog /usr/share/dict/words

and I get back

bulldog
dog
dogged

and so on.

How could I just get back "dog" with the grep command please?

Thanks,
Sincerely
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From ripley at stats.ox.ac.uk  Thu Aug 10 07:57:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Aug 2006 06:57:19 +0100 (BST)
Subject: [R] Error message when using optim
In-Reply-To: <BAY116-F3525CEA2390A8A4B7C0096FB550@phx.gbl>
References: <BAY116-F3525CEA2390A8A4B7C0096FB550@phx.gbl>
Message-ID: <Pine.LNX.4.64.0608100649520.29575@gannet.stats.ox.ac.uk>

(Subject changed to something less perjorative.  This is not `optim 
error'.)

On Wed, 9 Aug 2006, Frank Black wrote:

> Dear all,
> 
> There have been one or two questions posted to the list regarding the optim 
> error "non-finite finite-difference value [4]."  The error apparently means 
> that the 4th element of the gradient is non-finite.

(Without an example of the optim usage, we have little to go on.  This 
does not occur in the default method, so we don't even know which method 
was asked for.  Please do study the posting guide: we ask to information 
for good reasons.)

It means that the finite-difference approximation to the gradient is 
non-finite (as it says).  Most likely this occurs when the user-supplied 
function is returning Inf (so the finite difference is Inf - Inf) or
returning NA/NaN.

> My question is what part(s) of my program should I fiddle with in an 
> attempt to fix it?  Starting values?  Something in the log-likelihood 
> itself?  Perhaps the data (which is generated)?  Any thoughts would be 
> greatly appreciated.

If the function you are optimizing never returns Inf or NA/NaN, the 
message will not occur.  Nor will it occur if you supply a gradient 
function.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Aug 10 08:01:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Aug 2006 07:01:55 +0100 (BST)
Subject: [R] How to draw the decision boundaries for LDA and Rpart object
In-Reply-To: <005101c6bbfd$1599aa70$0400a8c0@LeonE1405>
References: <003c01c6bb72$f1881110$0400a8c0@LeonE1405>
	<Pine.LNX.4.64.0608090800030.7250@gannet.stats.ox.ac.uk>
	<005101c6bbfd$1599aa70$0400a8c0@LeonE1405>
Message-ID: <Pine.LNX.4.64.0608100658330.29575@gannet.stats.ox.ac.uk>

On Wed, 9 Aug 2006, Am Stat wrote:

> Dr. Ripley,

R-help is not the address of `Dr. Ripley', nor even that of the person who 
wrote to you.

> Thanks very much for your help. I have used your partition tree and it works 
> well.  I am not familiar with the 'tree' package but I found that the 
> threshold to make a cut  returned by tree and rpart is almost the same 
> value.
> 
> Does that mean the decision boundaries for Rpart and Tree are the same for a 
> same data when using the default  value of parameters, no matter what the 
> structure of data  is?

No, they can differ.  There are comparisons in MASS (the book).

> 
> Thanks very much!
> 
> Leon
> 
> 
> 
> 
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Am Stat" <amstat2006 at gmail.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, August 09, 2006 3:04 AM
> Subject: Re: [R] How to draw the decision boundaries for LDA and Rpart 
> object
> 
> 
> > On Wed, 9 Aug 2006, Am Stat wrote:
> >
> >> Hello useR,
> >>
> >> Could you please tell me how to draw the decision boundaries in a
> >> scatterplot of the original data for a LDA or Rpart object.
> >
> > There are examples in MASS (the book).
> >
> >> For example:
> >> > library(rpart)
> >> >fit.rpart <- rpart(as.factor(group.id)~., data=data.frame(Data) )
> >>
> >>
> >> How can I draw the cutting lines on the orignial Data?
> >>
> >> Or is there any built in functions that can read the rpart object
> >> 'fit.rpart' to do that?
> >
> > See partition.tree in package tree.
> >
> >> PLEASE do read the posting guide 
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Aug 10 08:07:44 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Aug 2006 07:07:44 +0100 (BST)
Subject: [R] decimal accuracy in pnorm( )
In-Reply-To: <000601c6bc0e$00175150$f07ff59b@patra>
References: <000601c6bc0e$00175150$f07ff59b@patra>
Message-ID: <Pine.LNX.4.64.0608100703000.29575@gannet.stats.ox.ac.uk>

On Thu, 10 Aug 2006, Chronopoulos Dimitris wrote:

> Dear R users
> 
> Is there any way to increase the decimal accuracy for the normal 
> probability distribution? When one needs an accurate p-value for 
> instance this is provided by
> 
> pnorm(10,lower.tail=F)
> [1] 7.619853e-24
> 
> However, what happens when instead of a P[X<x], a more accurate P[X>=x] 
> is the objective.

That is P[X>=x] !

If you meant that 

> pnorm(10,lower.tail=TRUE)
[1] 1

then the problem is your computer, which has no representable numbers 
between 1-1e-16 and 1, and so correctly chose the nearest representable 
number.

A basic understanding of numerical methods is necessary to do statistical 
calculations accurately: perhaps your university offers courses in the 
area?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Thu Aug 10 08:34:34 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Aug 2006 23:34:34 -0700
Subject: [R] how to use the EV AND condEV from BMA's results?
In-Reply-To: <2fc17e30608030553q4dcc0fefqaddfe363bd40db64@mail.gmail.com>
References: <2fc17e30608030553q4dcc0fefqaddfe363bd40db64@mail.gmail.com>
Message-ID: <44DAD37A.6050403@pdf.com>

<see in line>

zhijie zhang wrote:
> Dear friends,
> In R, the help of "bic.glm" tells the difference between postmean(the
> posterior mean of each coefficient from model averaging) and
> condpostmean(the posterior mean of each coefficient conditional on the
> variable being included in the model), But it's still unclear about the
> results explanations, and the artile of Rnews in 2005 on BMA still don't
> give more detail on it.
> Suppose my results of logistic regression analyzed by bic.glm (BMA) as
> follows:(dataset is birthwt(MASS) and i include the interaction)
> 
> 
> 
>                   p!=0      EV      SD     condEV  cond SD    model 1   model
> 2   model 3   model 4    model 5
> 
> Intercept         100     0.1841  1.2204   0.184    1.220        1.017
> 1.175    -0.853    -1.057     0.532
> 
> age                17.8   -0.0113  0.0285  -0.063    0.036         .
> .         .         .       -0.071
> 
> lwt               50.0   -0.0079  0.0093   -0.016   0.007       -0.017    -
> 0.017      .         .         .
> 
> smokeTRUE          9.5   0.0469  0.1798   0.496    0.345         .
>     .
>    .         .          .
> 
> ptdTRUE           99.4    1.5161  0.4751   1.526   0.461       1.407
> 1.596     1.732     1.463      1.608
> 
> htTRUE            54.4   0.9477  1.0269    1.742   0.744        1.894
> 1.930      .         .         .
> 
> uiTRUE            13.3    0.0976  0.2987   0.731    0.453         .
> .         .         .         .
> 
> ftv               12.3
> 
> 
>    .1                    -0.0257  0.5117   -0.209   2.438        .
>     .
> -0.867      .         .
> 
>    .2+                    0.7470  2.1277   6.081    3.371        .
> .        6.024      .         .
> 
> age.ftv1          33.7   -0.0136  0.0278  -0.040    0.035         .       -
> 0.036      .         .         .
> 
> age.ftv2.         15.9   -0.0340  0.0950  -0.214    0.135         .
> .       -0.271      .         .
> 
> smokeTRUE.uiTRUE   2.4   0.0103  0.1209    0.422   0.652        .
>    .
> .         .          .
> 
> 
> 
> 
> nVar                                                            3
>     4
> 3         1         2
> post prob                                                     0.117
> 0.086      0.083     0.061     0.044
> 
> 1. how should I write my final logistic model?

SG:  The "final logistic model" is actually a weighted average of the 
"model 1", "model 2", ..., "model 49", with weights given by the 
"postprob" attribute of the model fit object.  I know of no simpler form.

> 2. Which parameter estimation should be used, condEV OR EV? 

SG:  It depends on what you want to do.  For a qualitative assessment of 
the importance of different variables, you could use either or both 
together;  if they are substantially different, you know there is 
"multicollinearity", i.e., the explanatory variables are highly 
correlated.

	  It would be nice to have predict methods in BMA.  Writing one for 
'bicreg' should be fairly easy, e.g., using the following:

	  E(f) = E(over i of E(f|i))

	  var(f) = var(over i of E(f|i)) + E(over i of var(f|i)).

	  However, this won't work very well for an object of class 'bic.glm'. 
  To get predicted probability of success for this, a function like this 
could call 'predict.glm' repeatedly, once for each model considered, 
transform each answer into "probability of success" for that model, then 
average the results with weights given by 'postprob'.  There may be 
literature on how to get confidence bounds, but I'm not familiar with 
it.  One possibility might be Monte Carlo:  Select a model at random 
following "postprob", then compute a random vector of parameter 
estimates for that model using estimated mean and covariance matrix, 
then convert that either to predicted logits or "probability of success" 
at each point desired.  Do that, say, 1,000 or 10,000 times.  For each 
set of predictions, compute "quantile(..., c(.025, .975))".

	  I haven't tried this, but I believe it should work.

	  Hope this helps.
	  Spencer Graves

How should I use
> the two different parameter estimations correctly?
> Thanks for your precious time!
> 
>


From Sebastian.Leuzinger at unibas.ch  Thu Aug 10 09:14:33 2006
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Thu, 10 Aug 2006 09:14:33 +0200
Subject: [R] installing rimage
Message-ID: <200608100914.33754.Sebastian.Leuzinger@unibas.ch>

dear list
while installing the package rimage, i get the error message 

checking jpeglib.h usability... no
checking jpeglib.h presence... no
checking for jpeglib.h... no
configure: error: Sorry, can't find jpeglib header
ERROR: configuration failed for package 'rimage'

although i have installed jpeglib as required. does R not find this library? 
but why did it find the fftw library which rimage also required and i 
installed at the very same place?

OS: Linux Suse 9.3
R version 2.1.0

thanks

------------------------------------------------
Sebastian Leuzinger
University of Basel, Department of Environmental Science
Institute of Botany
Sch?nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger


From oswald at dhlaw.de  Thu Aug 10 09:19:06 2006
From: oswald at dhlaw.de (Christian Oswald)
Date: Thu, 10 Aug 2006 09:19:06 +0200
Subject: [R] categorical data
Message-ID: <44DADDEA.6040103@dhlaw.de>

Hello,

thats what I need, a list sorted first after year and then after
categorie. But I get an error message

> df
      df     cate b    c
 [1,] "2006" "a1" "1"  "1"
 [2,] "2006" "a2" "2"  "2"
 [3,] "2005" "a1" "3"  "3"
 [4,] "2004" "a3" "1"  "1"
 [5,] "2004" "a2" "2"  "2"
 [6,] "2005" "a1" "3"  "3"
 [7,] "2003" "a2" "11" "11"
 [8,] "2003" "a1" "2"  "2"
 [9,] "2006" "a2" "3"  "3"
> res<-aggregate( df[,c(3,4)], list(df$year,df$cate), sum)
Fehler in as.vector(x, mode) : Argument hat ung?ltigen 'mode'


(Error in as.vector(x,mode) :Argument has invalid mode)

I had tested the mode and receive "character". Can someone explain what
thats mean?

Christian



On Wed, 2006-08-09 at 18:07 +0200, Christian Oswald wrote:
> > Dear List,
> >
> > I neeed a grouped list with two sort of categorical data. I have a data
> > .frame like this.
> > 	year	cat.	b	c
> > 1	2006	a1	125	212
> > 2	2006	a2	256	212	
> > 3	2005	a1	14	12
> > 4	2004	a3	565	123
> > 5	2004	a2	156	789	
> > 6	2005	a1	1	456
> > 7	2003	a2	786	123
> > 8	2003	a1	421	569
> > 9  	2002	a2	425	245
> >
> > I need a list with the sum of b and c for every year and every cat (a1,
> > a2 or a3) in this year. I had used the tapply function to build the sum
> > for every year or every cat. How can I combine the two grouping values?

Christian,

Is that what you want (using DF as your data.frame):

> > aggregate(DF[, c("b", "c")],
            by = list(Year = DF$year, Cat = DF$cat.),
            sum)
  Year Cat   b   c
1 2003  a1 421 569
2 2005  a1  15 468
3 2006  a1 125 212
4 2002  a2 425 245
5 2003  a2 786 123
6 2004  a2 156 789
7 2006  a2 256 212
8 2004  a3 565 123

You can also reorder the results by Year and Cat:

> > DF.result <- aggregate(DF[, c("b", "c")],
                         by = list(Year = DFyear, Cat = DF$cat.),
                         sum)

> > DF.result[order(DF.result$Year, DF.result$Cat), ]
  Year Cat   b   c
4 2002  a2 425 245
1 2003  a1 421 569
5 2003  a2 786 123
6 2004  a2 156 789
8 2004  a3 565 123
2 2005  a1  15 468
3 2006  a1 125 212
7 2006  a2 256 212



Note that tapply() can only handle one 'X' vector at a time, whereas
aggregate can handle multiple 'X' columns in one call. For example:

> > tapply(DF$b, list(DF$year, DF$cat.), sum)
      a1  a2  a3
2002  NA 425  NA
2003 421 786  NA
2004  NA 156 565
2005  15  NA  NA
2006 125 256  NA

will give you the sum of 'b' for each combination of Year and Cat within
the 2d table, but I suspect this is not the output format you want. You
also get NA's in the cells where there was not the given combination
present in your data.

HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



aw.de


From francoisromain at free.fr  Thu Aug 10 10:04:11 2006
From: francoisromain at free.fr (francoisromain at free.fr)
Date: Thu, 10 Aug 2006 10:04:11 +0200
Subject: [R] OT UNIX grep question
In-Reply-To: <200608100440.k7A4eOae003670@gator.dt.uh.edu>
References: <200608100440.k7A4eOae003670@gator.dt.uh.edu>
Message-ID: <1155197051.44dae87b0256e@imp8-g19.free.fr>

Hi,

You have to learn about regular expressions. Then you'll come up with something
like :

grep "^dog$" /usr/share/dict/words

Cheers,

Romain


Selon Erin Hodgess <hodgess at gator.dt.uh.edu>:

> Dear R People:
>
> I want to use the "grep" command in UNIX/Linux to check
> some words from the dictionary.
>
> Let's say I use:
>
> grep dog /usr/share/dict/words
>
> and I get back
>
> bulldog
> dog
> dogged
>
> and so on.
>
> How could I just get back "dog" with the grep command please?
>
> Thanks,
> Sincerely
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Roger.Bivand at nhh.no  Thu Aug 10 09:49:34 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 10 Aug 2006 07:49:34 +0000 (UTC)
Subject: [R] extractAIC using surf.ls
References: <DE508AFC-D7C8-412F-9C1F-FA7DB49B164F@leeds.ac.uk>
Message-ID: <loom.20060810T093236-494@post.gmane.org>

Yan Wong <h.y.wong <at> leeds.ac.uk> writes:

> 
> Although the 'spatial' documentation doesn't mention that extractAIC  
> works, it does seem to give an output.

Could I suggest moving this question to the R-sig-geo list?

Please note that surf.ls() converts x and y to the [-1, +1] range to ensure that
higher powers of possibly very large absolute coordinate values do not cause
trouble, so that the surf.ls() and lm() models may differ anyway. 

I believe that there is a bug in extractAIC.trls() - which I contributed to the
spatial package some years ago, with edf <- df.residual.trls(fit) rather than n
- df.residual.trls(fit). When this is corrected, for this case, the extractAIC()
results agree.

Roger Bivand

> I may have misunderstood, but shouldn't the following give at least  
> the same d.f.?
> 
>  > library(spatial)
>  > data(topo, package="MASS")
>  > extractAIC(surf.ls(2, topo))
> [1]  46.0000 437.5059
>  > extractAIC(lm(z ~ x+I(x^2)+y+I(y^2)+x:y, topo))
> [1]   6.0000 357.5059
> 
> # and if the AIC values differ, shouldn't they do so by an additive  
> constant?
> 
>  > (extractAIC(surf.ls(2, topo))-extractAIC(lm(z ~ x+I(x^2)+y+I(y^2) 
> +x:y, topo)))[2]
> [1] 80
>  > (extractAIC(surf.ls(1, topo))-extractAIC(lm(z ~ x+y, topo)))[2]
> [1] 92
> 
> # Using R 2.3.1 (OS X), spatial version 7.2-27.1
> 
> Thanks
> 
> Yan
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From maechler at stat.math.ethz.ch  Thu Aug 10 10:21:00 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 10 Aug 2006 10:21:00 +0200
Subject: [R] installing rimage
In-Reply-To: <200608100914.33754.Sebastian.Leuzinger@unibas.ch>
References: <200608100914.33754.Sebastian.Leuzinger@unibas.ch>
Message-ID: <17626.60524.169387.162639@stat.math.ethz.ch>

>>>>> "Sebastian" == Sebastian Leuzinger <Sebastian.Leuzinger at unibas.ch>
>>>>>     on Thu, 10 Aug 2006 09:14:33 +0200 writes:

    Sebastian> dear list
    Sebastian> while installing the package rimage, i get the error message 

    Sebastian> checking jpeglib.h usability... no
    Sebastian> checking jpeglib.h presence... no
    Sebastian> checking for jpeglib.h... no
    Sebastian> configure: error: Sorry, can't find jpeglib header
    Sebastian> ERROR: configuration failed for package 'rimage'

    Sebastian> although i have installed jpeglib as required. 

  [ "as required"? ]

    Sebastian> does R not find this library? 

it would probably find the *library*, but above it tells you
that it looks for the header (*.h) files, not the library.

Typically the header files are only in the "jpeglib-dev" (or
"..-devel") package where I assume that you've installed a SuSE
package called "jpeglib".

    Sebastian> but why did it find the fftw library which rimage also required and i 
    Sebastian> installed at the very same place?

(maybe it really only wanted the *library*, not the headers,
 or you have installed the headers there ...)

    Sebastian> OS: Linux Suse 9.3
    Sebastian> R version 2.1.0

    Sebastian> thanks

Mid eme liebe Gruess vom e Zircher.. :-)

Martin Maechler, ETH Zurich


From stefan.grosse at uni-erfurt.de  Thu Aug 10 10:38:11 2006
From: stefan.grosse at uni-erfurt.de (Stefan Grosse)
Date: Thu, 10 Aug 2006 10:38:11 +0200
Subject: [R] graphic output file format
In-Reply-To: <20060810010422.17838.qmail@web32508.mail.mud.yahoo.com>
References: <20060810010422.17838.qmail@web32508.mail.mud.yahoo.com>
Message-ID: <44DAF073.9000109@uni-erfurt.de>

What speaks against using the jpeg (or png) device?

type:

?jpeg

for help

Have also a look at the wiki:

http://wiki.r-project.org/rwiki/doku.php?id=tips:graphics-base:0savegraphs

Stefan

Conan Phelan schrieb:
> I would like to save graphics I produce in jpeg or gif
> formats. The GDD package sounds like it should let me
> to do this, but I cannot get it to install (error:
> Can't find gd.h!). Anyone know what's up?
>
> Thnx, C
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From spencer.graves at pdf.com  Thu Aug 10 10:40:59 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Aug 2006 01:40:59 -0700
Subject: [R] Fitting data with optim or nls--different time scales
In-Reply-To: <Pine.LNX.4.44.0608081108390.27357-100000@ctbp0.ucsd.edu>
References: <Pine.LNX.4.44.0608081108390.27357-100000@ctbp0.ucsd.edu>
Message-ID: <44DAF11B.3040809@pdf.com>

<see in line>

Leslie Chavez wrote:
> Hi,
> 
> I have a system of ODE's I can solve with lsoda.
> 
> Model=function(t,x,parms) 
> {
>     #parameter definitions
>     lambda=parms[1]; beta=parms[2]; 
>     d = parms[3]; delta = parms[4]; 
>      p=parms[5];    c=parms[6]
>      
>       xdot[1] = lambda - (d*x[1])- (beta*x[3]*x[1])
>       xdot[2] = (beta*x[3]*x[1]) - (delta*x[2])
>       xdot[3] = (p*x[2]) - (c*x[3])
>      
>     return(list(xdot))
> }
> 
> I want to fit the output out[,4] to experimental data that is only 
> available on days 0, 7, 12, 14, 17, and 20. I don't know how to set up 
> optim or nls so that it takes out[,4] on the appropriate day, but still 
> runs lsoda on a time scale of 0.01 day.
> 
> Below is the function I've been using to run 'optim', at the 
> course-grained time scale:
> 
SG:  What about the following:

  Modelfit=function(s) {
  	parms[1:4]=s[1:4];
	times=c(0,7,12,14,17,20,25)
	lsodaTimes <- seq(min(times),max(times), by=0.01)
  	out=lsoda(x0,lsodaTimes,Model,parms)
	obsTimes <- (100*times-1)
  	mse=mean((log10(out[obsTimes,4])-log10(i(times)))^2)
  #	cat(times)
  	return(mse)
  }

	  Your example is not self contained, so obviously I haven't tried this 
with it.  However, something of this nature should work fine, I believe. 
  Something similar but different should also work, I believe, with 
'nls';  this would give you access to many helper functions (see 
"methods(class='nls')").  If 'nls' bombed on me, I'd then try 'optim' as 
it is less brittle.  Then I might use the output of 'optim' as initial 
values for 'nls' to get confidence intervals etc.

	  hope this helps.
	  Spencer Graves

> #x0=c(T0,I0,V0)
> x0=c(2249,0,1)
> #parms(lambda, beta, d, delta, p, c)
> parms[5:6]=c(1.0,23)
> 
> s0=c(49994,8456,6.16E-8,0.012) #initial values
> 
> fit=optim(s0,Modelfit)
> 
> Right now, lsoda is being run on too course-grained a time scale in the 
> function Modelfit. Most examples of optim and nls I have found compare 
> two data sets at the same times, and run lsoda on the time scale the 
> data is available at, but I would like to run lsoda at a finer scale, and 
> only compare the appropriate time points with the experiment.  I have also 
> tried using nls, but I have the same problem. Does anyone have 
> suggestions?
> 
> Thank you very much,
> 
> Leslie
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bansouvik at gmail.com  Thu Aug 10 10:54:12 2006
From: bansouvik at gmail.com (souvik banerjee)
Date: Thu, 10 Aug 2006 14:24:12 +0530
Subject: [R] How to fit bivaraite longitudinal mixed model ?
Message-ID: <7193991f0608100154m5296632bq1804439ba37b5d4d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/bb39d2be/attachment.pl 

From spencer.graves at pdf.com  Thu Aug 10 10:50:37 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Aug 2006 01:50:37 -0700
Subject: [R] Help with short time series
In-Reply-To: <20060808214338.M50933@nemo.unipr.it>
References: <20060808214338.M50933@nemo.unipr.it>
Message-ID: <44DAF35D.4060809@pdf.com>

	  Please see the examples in "?corCAR1".  If you are fitting models 
like this, I highly recommend you spend some quality time with Pinheiro 
and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).  As 
part of this, I suggest you work through the script files "ch01.R", 
"ch02.R", ..., "ch06.R", "ch08.R" in the "~\library\nlme\scripts" 
subdirectory of the R installation on your hard drive.  This will alert 
you to subtle differences between the R syntax and that of the book and 
help you avoid silly typographical errors that can ruin your whole day.

	  Enjoy,
	  Spencer Graves

Simone Vincenzi wrote:
> Thanks for the help. 
> I provide below the dataset I'm using, it's a little bit different from what 
> I was describing (sorry for that). The streams are 3 and I have an unequal 
> number of years for each stream.
> 
> Stream Density Year 
> 1     Zak    0.20 2000 
> 2     Zak    0.36 2001 
> 3     Zak    0.41 2002 
> 4     Zak    0.34 2003 
> 5     Zak    0.28 2004 
> 6     Gor    0.08 1999 
> 7     Gor    0.05 2000 
> 8     Gor    0.14 2001 
> 9     Gor    0.16 2002 
> 10    Gor    0.13 2003 
> 11    Gat    0.18 2004 
> 12    Gat    0.10 2001 
> 13    Gat    0.37 2002 
> 14    Gat    0.57 2003 
> 15    Gat    0.47 2004
> 
> I tried to follow the suggestions of Dieter, but the model does not fit. 
> Any suggestion will be appreciated
> 
> 
> 
> Dear R-list, 
>> I have a statistical problem with the comparison of two short time-series 
> of 
>> density data in an ecological framework. I have to compare two short time 
>> series (5 years, one value for each year) of species density data (it is 
> the 
>> density of fish in two different streams) to test if the two means of the 
>> five densities are significantly different, so basically if the two mean 
>> stream-specific fish densities are significantly different. 
>> I don't think I can use a straight t-test due to the problem of 
>> autocorrelation and I don't think I can use a repeated measure ANOVA as I 
>> don't have any replicates. 
>> Any help would be greatly appreciated.
> 
> try something like
> 
> library(nlme) 
> summary(lme(dens~stream+year,data=mystreamdata,random=~year|stream))
> 
> This should also give you an estimate if the slopes are different if you 
> test 
> against the simplified model
> 
> summary(lme(dens~stream+year,data=mystreamdata,random=~1|stream))
> 
> Since you did not provide a short example data set, this is only 
> approximatively 
> right.
> 
> Dieter
>


From lescroel_cebc at no-log.org  Thu Aug 10 10:59:25 2006
From: lescroel_cebc at no-log.org (Amelie LESCROEL)
Date: Thu, 10 Aug 2006 10:59:25 +0200
Subject: [R] hist() and bar spacing
Message-ID: <20060810085929.27200240008D@mwinf1002.orange.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/494ed830/attachment.pl 

From spencer.graves at pdf.com  Thu Aug 10 11:00:31 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Aug 2006 02:00:31 -0700
Subject: [R] Split-plot model
In-Reply-To: <BAY23-F135B0827D0C64DA86BACFDA0540@phx.gbl>
References: <BAY23-F135B0827D0C64DA86BACFDA0540@phx.gbl>
Message-ID: <44DAF5AF.4050705@pdf.com>

	  Have you considered the 'lme' function in library(nlme)?  The 'lme' 
function is new, much more flexible, and as far as I know at least as 
good and usually better than 'aov'.  I would try the following:

	  fit <- lme(response~CO2*NITROGEN, random=~1|ROOM)

	  Checking "methods(class='lme')" gives a list of helper functions to 
do a variety of useful things with 'fit'.  For more information, see 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer).  I suggest you use the companion R script files in the 
"~\library\nlme\scripts" subdirectory of your R installation to 
facilitate your study of the book.  There is at least one minor but 
important difference in the syntax between the book and R.  In addition, 
using these script files should save you from any problems with 
typographical errors, thereby making your study more enjoyable and 
educational.

	  Hope this helps.
	  Spencer Graves

Chreis Habeck wrote:
> How do I set up my model equation in aov to analyze a split-plot design?
> 
> I have two factors (CO2 and NITROGEN), each with two levels (high and 
> ambient).   CO2 is my whole-plot factor with three replicates for each level 
> (i.e., 6 rooms total).
> 
> Is this syntax below correct?
> 
> summary(aov(response ~ ROOM + CO2*NITROGEN + Error(ROOM/CO2)))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p_connolly at ihug.co.nz  Thu Aug 10 11:00:52 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Thu, 10 Aug 2006 21:00:52 +1200
Subject: [R] Colours in silhouette plots (cluster package)
Message-ID: <20060810090052.GP12850@ihug.co.nz>

I tried using colours in silhouette plots by specifying a vector of
colours instead of the default "gray" for the col parameter.

No bars are drawn, though the rest of the plot works as it did with
the grey bars.  On investigating cluster:::plot.silhouette, I came
across this part:

  if (do.col.sort && (lc <- length(col)) > 1) {
        if (lc == k) 
            col <- col[cli]
        else if (lc != n) 
            col <- rep(col, length = n)
       col <- rev(col[attr(x, "iOrd")])
    }

Since attr(x, "iOrd") was NULL, I would end up with an empty col
vector which explained why I didn't get any bars.

When I removed [attr(x, "iOrd")], the plot worked how I think it's
meant to.  The silhouette object I used was created from one obtained
from the pam function in the same package.  I've not tried plotting
other types, so I can't say how general the phenomenon could be.

Could it be that the "iOrd" part belongs when some other conditions
apply or is it some left over code from a previous version?  Or is it
just that I've done something silly?



platform       x86_64-unknown-linux-gnu  
arch           x86_64                    
os             linux-gnu                 
system         x86_64, linux-gnu         
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)

Along with cluster with that distribution:
Packaged:      Wed May 17 09:56:22 2006; maechler
Built:         R 2.3.1; x86_64-unknown-linux-gnu; 2006-06-05 11:57:15;
               unix

best


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From ligges at statistik.uni-dortmund.de  Thu Aug 10 11:07:34 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Aug 2006 11:07:34 +0200
Subject: [R] hist() and bar spacing
In-Reply-To: <20060810085929.27200240008D@mwinf1002.orange.fr>
References: <20060810085929.27200240008D@mwinf1002.orange.fr>
Message-ID: <44DAF756.4090709@statistik.uni-dortmund.de>



Amelie LESCROEL wrote:
> Hello,
> 
>  
> 
> Could someone explain to me why I get so different histograms, in terms of
> bar spacing, depending on the number of counts entered ? (see example below)
> 
>  
> 
>> test <- c(0,1,1,1,1,0,0,0,0,0,2,3,2,2,2,3,3,4,5,6,7,5,4,3,4,2,2)
> 
>> hist(test)
> 
>  
> 
> I obtain this kind of histogram (what I actually want), i.e. large bars with
> no space between them
> 
>  
> 
> x
> 
> xx
> 
> xxx
> 
> xxxx
> 
> xxxxx
> 
> xxxxxx
> 
> xxxxxxx
> 
> xxxxxxxxx
> 
> xxxxxxxxxx
> 
>  
> 
> Entering:
> 
>> test <-
> c(3,1,2,1,1,1,2,1,1,2,2,2,0,1,1,1,3,0,0,2,5,2,0,0,1,6,0,1,1,4,0,0,2,1,0,1,3,
> 0,0,0,3,2,1,3,6,3,1,0,1,0,1,0,1,0,0,4,1,3,2,7,5,1,0,0,1,2,0,2,0,2,0,2,1,1,3,
> 1,3,0,2,1,3,1,1,0,1,7,2,1,2,2,2,1,2,5,1,2,2,2,1,1,4,2,0,4,1,1,1,5,1,0,2,5,2,
> 1,1,1,1,2,1,1,1,1,4,2,1,3,1,1,2,4,2,1,1,1,1,1,1,1,1,1,4,1,0,2,2,5,1,4,0,1,2,
> 2,2,2,1,2,1,3,5,2,0,0,2,0,0,0,0,1,1,2,2,1,1,2,1,0,1,0,2,1,3,0,0,0,1,3,1,3,1,
> 0,3,1,2,0,1,1,1,3,4,4,2,1,1,1,2,1,2,2,5,1,1,1,1,1,2,0,1,3,1,1,2,3,1,1,2,0,0,
> 2,0,1,1,1,1,1,1,1,1,1,1,5,3,1,1,1,0,0,1,0,1,1,0,0,1,2,0,0,0,2,3,1,1,1,2,0,0,
> 0,0,0,0,0,0,0,0,0,0,0,0,1,1,2,0,1,0,0,0,1,3,0,1,3,1,2,1,1,2,1,1,3,1,1,1,0,0,
> 1,1,1,1,1,1,1,1,4,1,3,3,1,2,1,2,3,4,1,0,2,2,1,2,1,2,3,0,0,2,3,2,4,1,0,1,1,0,
> 1,3,2,3,3,1,3,1,0,2,1,1,1,1,1,1,3,0,0,1,1,1,1,1,1,3,1,3,3,0,1,2,1,2,3,1,2,1,
> 2,2,2,1,1,1,1,0,2,2,1,2,2,2,1,1,2,2,1,1,1,2,2,1,1,1)

You data follows a *discrete* distribution, hence a barplot seems to be 
more appropriate here:

barplot(table(testtest))

Uwe Ligges




>> hist(test)
> 
>  
> 
> I obtain a strange histogram with the first 2 bars grouped together and then
> thin bars (not occupying the entire bin width) separated by spaces.
> 
>  
> 
>   x
> 
>   x   x
> 
> xx   x
> 
> xx   x   x
> 
> xx   x   x   x
> 
> xx   x   x   x   x
> 
> xx   x   x   x   x
> 
> xx   x   x   x   x   x
> 
> xx   x   x   x   x   x   x
> 
> xx   x   x   x   x   x   x   x   x 
> 
>  
> 
> I suspect that it?s causing me some troubles to fit my data to a probability
> density function. Using:
> 
>  
> 
>> plot(density(test))
> 
>  
> 
> I got a ?wave-shaped? density line, with the line going down to zero between
> each bar. And I wouldn?t expect the density line to go down between each
> integer? or surely there is something that I don?t understand!
> 
>  
> 
> Thanks for your help,
> 
>  
> 
> Am?lie Lescro?l
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Gregor.Gorjanc at bfro.uni-lj.si  Thu Aug 10 10:40:00 2006
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Thu, 10 Aug 2006 10:40:00 +0200
Subject: [R] [R-pkgs] New upload of connectedness
References: <7FFEE688B57D7346BC6241C55900E730F31C50@pollux.bfro.uni-lj.si>
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31C54@pollux.bfro.uni-lj.si>

Hello!

I have uploaded update (0.2-1) for connectedness.

NEWS?

o NA's in either x or y are not removed by default, but only when
  drop=TRUE. Subset method gains dropNA=NULL argument due to
  this. Now subset arguments are properly placed after '...'. 2006-08-08

o Added unit tests. 2006-08-08

o Removed connectedness.package.Rd as it is not really needed
  here. 2006-08-08

o LGPL -> GPL. 2006-06-17

o Removed dependency on package RColorBrewer since this was not really
  needed. I needed RColorBrewer only to get a set of default distinct
  colors. 2006-06-17

o Only unique levels are printed out in Levels* columns. 2006-06-06

o Percent in form of 33 and not 0.33. 2006-06-06

o Minor documentation fixes - typos etc. 2006-05-02

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From jim at bitwrit.com.au  Fri Aug 11 01:26:29 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 10 Aug 2006 19:26:29 -0400
Subject: [R] hist() and bar spacing
In-Reply-To: <20060810085929.27200240008D@mwinf1002.orange.fr>
References: <20060810085929.27200240008D@mwinf1002.orange.fr>
Message-ID: <44DBC0A5.8040906@bitwrit.com.au>

Amelie LESCROEL wrote:
> ...
> I obtain this kind of histogram (what I actually want), i.e. large bars with
> no space between them
> 
>  ...
> I obtain a strange histogram with the first 2 bars grouped together and then
> thin bars (not occupying the entire bin width) separated by spaces.
> 
> ... 
> 
> I suspect that it?s causing me some troubles to fit my data to a probability
> density function. Using:
> 
Hi Amelie,

Them's the breaks. Try this:

hist(test,breaks=-1:7)

and you should get what you want.

Jim


From csardi at rmki.kfki.hu  Thu Aug 10 11:30:55 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 10 Aug 2006 11:30:55 +0200
Subject: [R] Is there a better way than x[1:length(x)-1] ?
In-Reply-To: <20060810003047.1461.qmail@web35402.mail.mud.yahoo.com>
References: <20060810003047.1461.qmail@web35402.mail.mud.yahoo.com>
Message-ID: <20060810093055.GA19044@localdomain>

Jack,

in R ":" is an ordinary function with two arguments, ie. 1:10 is a function
call with arguments 1 and 10. This way at the time ":" is evaluated the
variable which will be indexed is not known; it is thus impossible to know
its length. This is why it is not easy to implement "end" in R. 

Since R is very good at computing 'on the language' there might be a way (i
would call it hack) to implement 'end' but i'm not aware of it. 
Eg. you might consider writing a function like 

idx <- function(x, i) {
  i <- substitute(i)
  i <- eval(i, c(as.list(parent.frame()), end=length(x)))
  x[i]
}
      
and then you can write things like this:

x <- 1:10
idx(x, 2:end)
idx(x, 2:(end-1))

R gurus, please correct me if i'm wrong. 
Gabor

On Wed, Aug 09, 2006 at 05:30:47PM -0700, John McHenry wrote:
> Hi WizaRds,
> 
> In MATLAB you can do
> 
> x=1:10
> 
> and then specify
> 
> x(2:end)
> 
> to get 
> 
> 2 3 4 5 6 7 8 9 10
> 
> or whatever (note that in MATLAB the parenthetic index notation is used, not brackets as in R). The point is that 'end' allows you to refer to the final index point of the array.
> 
> Obviously there isn't much gain in syntax when the variable name is x, but when it's something like 
> 
> hereIsABigVariableName(j:end-i)
> 
> it makes things a lot more readable than 
> 
>  hereIsABigVariableName(j:length(hereIsABigVariableName)-i)
> 
> In R I could do:
> 
> n<- length(hereIsABigVariableName)
>  hereIsABigVariableName[j:n-i]
> 
> but I'd like to use something like 'end', if it exists.
> 
> Am I missing something obvious in R that does what 'end' does in MATLAB?
> 
> Thanks,
> 
> Jack.
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From franco.mendolia at gmx.de  Thu Aug 10 11:41:50 2006
From: franco.mendolia at gmx.de (Franco Mendolia)
Date: Thu, 10 Aug 2006 11:41:50 +0200
Subject: [R] tk combobox question
In-Reply-To: <pan.2006.08.09.21.19.23.432510@troefpunt.nl>
References: <44D9F4C0.3080106@gmx.de>
	<pan.2006.08.09.21.19.23.432510@troefpunt.nl>
Message-ID: <44DAFF5E.3000504@gmx.de>


Datum: 09.08.2006 23:19

> I think you want something like this:
> 
>   require(tcltk) || stop("Package tcltk is not available.")
>   version.BWidget <- tclvalue(tclRequire("BWidget"))
> 
>   modify_command <- function() {
>     print("Hey, I'm modified...")
>   }
> 
>   tt <- tktoplevel()
> 
>   values = c("foo", "bar", "jeebee")
>   combo <- tkwidget(tt, "ComboBox", "-modifycmd", 
>                     modify_command, values=values)
>   tkgrid(combo)

That is exactly what I was looking for! Thanks


From luc.hoegaerts at gmail.com  Thu Aug 10 12:00:28 2006
From: luc.hoegaerts at gmail.com (Luc Hoegaerts)
Date: Thu, 10 Aug 2006 12:00:28 +0200
Subject: [R] sn package - skew t - code for analytical expressions for first
	4 moments
Message-ID: <1ae2a2810608100300i2fd453a7gbaca8b5aac384461@mail.gmail.com>

hello users of the SN package,


i thought i post here some useful help on R code on the 4 moments for the skew t

sampling gives seldom good results for skewness and kurtosis, so
one really needs the analytical results,

it took me some time to get it from the article

Azzalini, A. & Capitanio, A. (2003),
 Distributions generated by perturbation of symmetry with emphasis on
a multivariate skew-t distribution.

hope it is one day useful for someone else too.

bye
luc

# computing analytical moments for skew t of azzalini
xi=location;
alpha=shape;
omega=sd(X);
delta=alpha/sqrt(1+alpha^2);
mu=delta*sqrt(df/pi)*gamma(0.5*(df-1))/gamma(0.5*df);

# the 4 first moments
moment1=xi+omega*mu;
moment2=xi^2 + 2*omega*mu*xi + omega^2 * df/(df-2);
moment3=xi^3 + 3*omega*mu*xi^2 + 3*omega^2*df/(df-2)*xi +
omega^3*mu*(3-delta^2)*df/(df-3);
moment4=xi^4 + 4*omega*mu*xi^3 + 6*omega^2*df/(df-2)*xi^2 +
4*omega^3*mu*(3-delta^2)*df/(df-3)*xi+omega^4*3*df^2/((df-2)*(df-4));

# the 4 useful measures
mean=moment1;
var=moment2;
skew=moment3/var^(3/2);
kurt=moment4/var^2 - 3;


From petr.pikal at precheza.cz  Thu Aug 10 12:01:29 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Aug 2006 12:01:29 +0200
Subject: [R] Is there a better way than x[1:length(x)-1] ?
In-Reply-To: <20060810003047.1461.qmail@web35402.mail.mud.yahoo.com>
Message-ID: <44DB2019.11677.20CC9B@localhost>

Hi

On 9 Aug 2006 at 17:30, John McHenry wrote:

Date sent:      	Wed, 9 Aug 2006 17:30:47 -0700 (PDT)
From:           	John McHenry <john_d_mchenry at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Is there a better way than x[1:length(x)-1] ?

> Hi WizaRds,
> 
> In MATLAB you can do
> 
> x=1:10
> 
> and then specify
> 
> x(2:end)
> 
> to get 
> 
> 2 3 4 5 6 7 8 9 10
> 
> or whatever (note that in MATLAB the parenthetic index notation is
> used, not brackets as in R). The point is that 'end' allows you to
> refer to the final index point of the array.
> 
> Obviously there isn't much gain in syntax when the variable name is x,
> but when it's something like 
> 
> hereIsABigVariableName(j:end-i)
> 
> it makes things a lot more readable than 
> 
>  hereIsABigVariableName(j:length(hereIsABigVariableName)-i)
> 
> In R I could do:
> 
> n<- length(hereIsABigVariableName)
>  hereIsABigVariableName[j:n-i]
> 
> but I'd like to use something like 'end', if it exists.

It probably does not exist (but in R you never know :-)
and you can easily to construct it yourself

 x <- 1:10
 end <- function(x) length(x)
 x[2:end(x)]

 [1]  2  3  4  5  6  7  8  9 10
 >

HTH
Petr



> 
> Am I missing something obvious in R that does what 'end' does in
> MATLAB?
> 
> Thanks,
> 
> Jack.
> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Thu Aug 10 12:11:39 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Aug 2006 12:11:39 +0200
Subject: [R] categorical data
In-Reply-To: <44DADDEA.6040103@dhlaw.de>
Message-ID: <44DB227B.18603.2A1B9A@localhost>

Hi

On 10 Aug 2006 at 9:19, Christian Oswald wrote:

Date sent:      	Thu, 10 Aug 2006 09:19:06 +0200
From:           	Christian Oswald <oswald at dhlaw.de>
To:             	r-help at stat.math.ethz.ch
Subject:        	Re: [R] categorical data
Send reply to:  	oswald at dhlaw.de
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hello,
> 
> thats what I need, a list sorted first after year and then after
> categorie. But I get an error message
> 
> > df
>       df     cate b    c
>  [1,] "2006" "a1" "1"  "1"
>  [2,] "2006" "a2" "2"  "2"
>  [3,] "2005" "a1" "3"  "3"
>  [4,] "2004" "a3" "1"  "1"
>  [5,] "2004" "a2" "2"  "2"
>  [6,] "2005" "a1" "3"  "3"
>  [7,] "2003" "a2" "11" "11"
>  [8,] "2003" "a1" "2"  "2"
>  [9,] "2006" "a2" "3"  "3"

This is not a data frame but character matrix
try str(df). It was probably constructed by cbind(...), try to use 
data.frame(....) instead.

Or you can try

as.data.frame(df) but then you need to change resulting factors back 
to numeric
?as.character
?as.numeric

HTH
Petr


try

> > res<-aggregate( df[,c(3,4)], list(df$year,df$cate), sum)
> Fehler in as.vector(x, mode) : Argument hat ung?ltigen 'mode'
> 
> 
> (Error in as.vector(x,mode) :Argument has invalid mode)
> 
> I had tested the mode and receive "character". Can someone explain
> what thats mean?
> 
> Christian
> 
> 
> 
> On Wed, 2006-08-09 at 18:07 +0200, Christian Oswald wrote:
> > > Dear List,
> > >
> > > I neeed a grouped list with two sort of categorical data. I have a
> > > data .frame like this. 	year	cat.	b	c 1	2006	a1	125	212
> > > 2	2006	a2	256	212	 3	2005	a1	14	12 4	2004	a3	565	123
> > > 5	2004	a2	156	789	 6	2005	a1	1	456 7	2003	a2	786	123
> > > 8	2003	a1	421	569 9  	2002	a2	425	245
> > >
> > > I need a list with the sum of b and c for every year and every cat
> > > (a1, a2 or a3) in this year. I had used the tapply function to
> > > build the sum for every year or every cat. How can I combine the
> > > two grouping values?
> 
> Christian,
> 
> Is that what you want (using DF as your data.frame):
> 
> > > aggregate(DF[, c("b", "c")],
>             by = list(Year = DF$year, Cat = DF$cat.),
>             sum)
>   Year Cat   b   c
> 1 2003  a1 421 569
> 2 2005  a1  15 468
> 3 2006  a1 125 212
> 4 2002  a2 425 245
> 5 2003  a2 786 123
> 6 2004  a2 156 789
> 7 2006  a2 256 212
> 8 2004  a3 565 123
> 
> You can also reorder the results by Year and Cat:
> 
> > > DF.result <- aggregate(DF[, c("b", "c")],
>                          by = list(Year = DFyear, Cat = DF$cat.), sum)
> 
> > > DF.result[order(DF.result$Year, DF.result$Cat), ]
>   Year Cat   b   c
> 4 2002  a2 425 245
> 1 2003  a1 421 569
> 5 2003  a2 786 123
> 6 2004  a2 156 789
> 8 2004  a3 565 123
> 2 2005  a1  15 468
> 3 2006  a1 125 212
> 7 2006  a2 256 212
> 
> 
> 
> Note that tapply() can only handle one 'X' vector at a time, whereas
> aggregate can handle multiple 'X' columns in one call. For example:
> 
> > > tapply(DF$b, list(DF$year, DF$cat.), sum)
>       a1  a2  a3
> 2002  NA 425  NA
> 2003 421 786  NA
> 2004  NA 156 565
> 2005  15  NA  NA
> 2006 125 256  NA
> 
> will give you the sum of 'b' for each combination of Year and Cat
> within the 2d table, but I suspect this is not the output format you
> want. You also get NA's in the cells where there was not the given
> combination present in your data.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
> 
> 
> 
> aw.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From arun.kumar.saha at gmail.com  Thu Aug 10 12:12:42 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 10 Aug 2006 15:42:42 +0530
Subject: [R] Geometrical Interpretation of Eigen value and Eigen vector
Message-ID: <d4c57560608100312s358a1c00ud33f257db3418783@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/e3bced32/attachment.pl 

From kbeath at efs.mq.edu.au  Thu Aug 10 13:45:25 2006
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Thu, 10 Aug 2006 21:45:25 +1000
Subject: [R] nlme iteration process, few questions
In-Reply-To: <s4da40aa.003@mail.efs.mq.edu.au>
References: <s4da40aa.003@mail.efs.mq.edu.au>
Message-ID: <571C48EE-E6EC-4CFD-8831-DB392882B212@efs.mq.edu.au>

> Recently I started using nlme intensively, and since it is all new for
> me, I have some questions. I am running nlme with
> control=list(verbose=TRUE) and during one lengthy fitting, I started
> watching the output for some clues, how to speed up the process. I
> noticed that in one case, the iteration process is alternating between
> two solutions. Here is an output of 5 iterations, after which I
> stopped the calculation:
>

I have sometimes fixed this problem by increasing the number of nlm  
and PNLS iterations. It can be due to the random effects estimates  
being too small, so not necessary in the model, or I suspect that a  
high correlation between the random effects would also produce the  
problem.

The usual method is to try fitting simpler models by removing random  
effects or with structured covariance matrix and then use the  
estimates of the fixed effects as starting values for more complex  
models.

HTH
Ken


From rolf at erdos.math.unb.ca  Thu Aug 10 13:51:36 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Thu, 10 Aug 2006 08:51:36 -0300 (ADT)
Subject: [R] OT UNIX grep question
Message-ID: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>

francoisromain at free.fr wrote:

> You have to learn about regular expressions. Then you'll come up with
> something like :
> 
> grep "^dog$" /usr/share/dict/words

*You* have to learn about shell syntax.  The foregoing doesn't
work; it gives an ``Illegal variable name.'' error.  To protect
against the shell interpretation of the dollar sign you have
to use *single* quotes.

		grep '^dog$' /usr/share/dict/words

*does* work.  (Try it!)

			cheers,

				Rolf Turner
				rolf at math.unb.ca


From crassshed at gmail.com  Thu Aug 10 14:16:00 2006
From: crassshed at gmail.com (Chris wallace)
Date: Thu, 10 Aug 2006 12:16:00 +0000
Subject: [R] OT UNIX grep question
In-Reply-To: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
References: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
Message-ID: <9bcdfad70608100516w1930b430i81e9ce7c452700d8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/643cc6e5/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 10 14:38:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Aug 2006 08:38:05 -0400
Subject: [R] Is there a better way than x[1:length(x)-1] ?
In-Reply-To: <20060810003047.1461.qmail@web35402.mail.mud.yahoo.com>
References: <20060810003047.1461.qmail@web35402.mail.mud.yahoo.com>
Message-ID: <971536df0608100538n1dad75d3lc70d60171ecfe9f1@mail.gmail.com>

On 8/9/06, John McHenry <john_d_mchenry at yahoo.com> wrote:
> Hi WizaRds,
>
> In MATLAB you can do
>
> x=1:10
>
> and then specify
>
> x(2:end)
>
> to get
>
> 2 3 4 5 6 7 8 9 10
>

In R you could do the above via:

x[-1]


> or whatever (note that in MATLAB the parenthetic index notation is used, not brackets as in R). The point is that 'end' allows you to refer to the final index point of the array.
>
> Obviously there isn't much gain in syntax when the variable name is x, but when it's something like
>
> hereIsABigVariableName(j:end-i)
>
> it makes things a lot more readable than
>
>  hereIsABigVariableName(j:length(hereIsABigVariableName)-i)
>
> In R I could do:
>
> n<- length(hereIsABigVariableName)
>  hereIsABigVariableName[j:n-i]

In "R version 2.4.0 Under development (unstable) (2006-08-08 r38825)"
available from CRAN, head and tail can have negative arguments:

   head(x, -2)

is the same as x[1:8] using your x.

>
> but I'd like to use something like 'end', if it exists.
>
> Am I missing something obvious in R that does what 'end' does in MATLAB?
>
> Thanks,
>
> Jack.
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jtk at cmp.uea.ac.uk  Thu Aug 10 14:49:51 2006
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 10 Aug 2006 13:49:51 +0100
Subject: [R] OT UNIX grep question
In-Reply-To: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
References: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
Message-ID: <20060810124951.GA31775@jtkpc.cmp.uea.ac.uk>

On Thu, Aug 10, 2006 at 08:51:36AM -0300, Rolf Turner wrote:
> francoisromain at free.fr wrote:
> 
> > You have to learn about regular expressions. Then you'll come up with
> > something like :
> > 
> > grep "^dog$" /usr/share/dict/words
> 
> *You* have to learn about shell syntax.  The foregoing doesn't
> work; it gives an ``Illegal variable name.'' error.  To protect
> against the shell interpretation of the dollar sign you have
> to use *single* quotes.
> 
> 		grep '^dog$' /usr/share/dict/words
> 
> *does* work.  (Try it!)

you're perfectly right about single quotes being the correct thing to use
here, but not all shells are insisting on linguistic correctness the way
[t]csh does. bash leaves constructs that it cannot expand as they are, so
the variant with double quotes does work as expected with bash (although
through a mechanism that might be unexpected by most).

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |             email: jtk at cmp.uea.ac.uk                               |
 |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*


From s.wood at bath.ac.uk  Thu Aug 10 14:59:48 2006
From: s.wood at bath.ac.uk (Simon Wood)
Date: Thu, 10 Aug 2006 13:59:48 +0100
Subject: [R] Geometrical Interpretation of Eigen value and Eigen vector
In-Reply-To: <d4c57560608100312s358a1c00ud33f257db3418783@mail.gmail.com>
References: <d4c57560608100312s358a1c00ud33f257db3418783@mail.gmail.com>
Message-ID: <200608101359.48588.s.wood@bath.ac.uk>

You can decompose a symmetric matrix A as 
A=UDU'
where U is a matrix of eigenvectors (in its columns), and D is a diagonal 
matrix of eigenvalues. Since A is symmetric, U is orthogonal. So what A does 
to a vector x when you form Ax has a simple geometerical interpretation:
1. x is rotated into the `eigenspace' of A, by U'
2. the elements of the rotated x are rescaled by multiplication by the   
eigenvalues  of A.
3. The reverse of the rotation from step 1 is applied to the rescaled rotated 
x, by U.    

Any use?

> Dear all,
>
> It is not a R related problem rather than statistical/mathematical. However
> I am posting this query hoping that anyone can help me on this matter. My
> problem is to get the Geometrical Interpretation of Eigen value and Eigen
> vector of any square matrix. Can anyone give me a light on it?
>
> Thanks and regards,
> Arun
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From YCH at softcomputing.com  Thu Aug 10 15:09:35 2006
From: YCH at softcomputing.com (Yohan CHOUKROUN)
Date: Thu, 10 Aug 2006 15:09:35 +0200
Subject: [R] tcltk library on linux
Message-ID: <C8F48FD780E12D4DB197507B897D00DD072F5B1A@ntexch.softcomputing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/6749fdd9/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 10 15:18:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Aug 2006 09:18:08 -0400
Subject: [R] Geometrical Interpretation of Eigen value and Eigen vector
In-Reply-To: <d4c57560608100312s358a1c00ud33f257db3418783@mail.gmail.com>
References: <d4c57560608100312s358a1c00ud33f257db3418783@mail.gmail.com>
Message-ID: <971536df0608100618j7548bd90n69bbe3e177f72043@mail.gmail.com>

A matrix M can be thought of as a linear transformation which maps
input vector x to output vector y:

     y = Mx

The eigenvectors are those "directions" that this mapping preserves.
That is if x is an eigenvector then y = ax for some scalar a.  i.e.
y lies in the same one dimensional space as x.  The only difference
is that y is dilated or contracted and possibly reversed and the scale factor
defining this dilation/contraction/reversal which corresponds to a particular
eigenvector x is its eigenvalue:  i.e. y = ax (where a is a scalar,
the eigenvalue, corresponding to eigenvector x).

In matrix terms, the eigenvectors form that basis in which the
linear transformation M has a diagonal matrix and the diagonal
values are the eigenvalues.

On 8/10/06, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> Dear all,
>
> It is not a R related problem rather than statistical/mathematical. However
> I am posting this query hoping that anyone can help me on this matter. My
> problem is to get the Geometrical Interpretation of Eigen value and Eigen
> vector of any square matrix. Can anyone give me a light on it?
>
> Thanks and regards,
> Arun
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kubovy at virginia.edu  Thu Aug 10 15:33:28 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 10 Aug 2006 09:33:28 -0400
Subject: [R] jpeg() and JGR
Message-ID: <E4B8B78D-2F1E-4378-B159-4FF5CBC81236@virginia.edu>

Dear R-helpers,

In JGR:
jpeg()
Error in X11(paste("jpeg::", quality, ":", filename, sep = ""),  
width,  :
	unable to start device JPEG
In addition: Warning message:
unable to open connection to X11 display ''

 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "grid"      "datasets"  "methods"   "stats"     "graphics"   
"grDevices" "utils"     "base"

other attached packages:
     R2HTML      Hmisc    effects    foreign    lattice        
MASS        JGR     JavaGD      rJava
     "1.57"   "3.0-12"    "1.0-8"   "0.8-15"  "0.13-10" "7.2-27.1"     
"1.4-2"    "0.3-3"    "0.4-3"


But in the R Os X GUI, the command works, and produces Rplot001.jpeg

 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "methods"   "stats"     "graphics"
[4] "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
foreign   R2HTML
"0.8-15"   "1.57"
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From S.Pickett at exeter.ac.uk  Thu Aug 10 16:00:56 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Thu, 10 Aug 2006 15:00:56 +0100 (BST)
Subject: [R] help with structuring random factors using lmer()
Message-ID: <1182.144.173.76.117.1155218456.squirrel@www.webmail.ex.ac.uk>

Hi,
I am an R beginner and having problems structuring my REML models. I have
a model with
y=weight
x1=time
x2=timesquared
id=individual identity
I need to structure the model such that in the random effects there is a
constant intercept for all individuals but a separate individual slope for
both x1 and x2 (a coefficient score for every individual).
m1<-lmer(weight~time+timesq+(1|id)+(timesq-1|id)+(time-1|id), data=dataset)
coef(m1)
gives me nearly what I want except there isnt an individual coefficient
score for each individual for x2.
Any suggestions very much appreciated.
Simon Pickett
s.pickett at exeter.ac.uk



Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From sharon.kuhlmann at smi.ki.se  Thu Aug 10 16:01:17 2006
From: sharon.kuhlmann at smi.ki.se (Sharon Kuhlmann-B)
Date: Thu, 10 Aug 2006 16:01:17 +0200
Subject: [R] Convergence in geese/gee
Message-ID: <37D69F7F7458FA4DABF83457837E0E07D1CAE7@smiexch1.smi.local>

We are currently analyzing data on children clustered in day care-centers (DCC). We have tried to use geepack and gee libraries to estimate an overall incidence rate for absences (=number of absences/risk time) by specifying

geese(number.absences ~ offset(log(risktime)), id=day.care.id, 
                 family=poisson("log"), data=dcc, corstr="exch",
                 sca.link="log", cor.link="fisherz")

gee(number.absences ~ offset(log(risktime)), id=day.care.id, 
                    family=poisson, data=dcc, corstr="exchangeable")


However it returns a value error of 1 ,in some cases it returnes NaN estimates, andin the case or gee, it hangs. We intend eventually to add other covariates we are interested in.

Our clusters (day-care centers) include about 50 children each, and in one case over 100. By taking a smaller number of children in each day care center, we managed to obtain convergence, but as long as the cluster size was under 25 (i.e. no day care center larger than 25 children). 

Is the geese and gee functions limited by the size of the cluster? And if so, are there any suggestion how to go around the problem?

Thank you for your help.

Sincerely,

Sharon

============================================
Sharon K?hlmann Berenzon, Ph.D.
Statistician
Dept. Epidemiology
Swedish Institute for Infectious Disease Control (SMI) 
 
Sharon.Kuhlmann at smi.ki.se
tel. +46-8-457 2376; fax. +46-8-30 06 26


From mpiktas at gmail.com  Thu Aug 10 16:07:17 2006
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Thu, 10 Aug 2006 17:07:17 +0300
Subject: [R] optim error
In-Reply-To: <BAY116-F3525CEA2390A8A4B7C0096FB550@phx.gbl>
References: <BAY116-F3525CEA2390A8A4B7C0096FB550@phx.gbl>
Message-ID: <e47808320608100707t3e432c9ao776edf28e499827c@mail.gmail.com>

Hi,

On 8/9/06, Frank Black <fb572 at hotmail.com> wrote:
> Dear all,
>
> There have been one or two questions posted to the list regarding the optim
> error "non-finite finite-difference value [4]."  The error apparently means
> that the 4th element of the gradient is non-finite.  My question is what
> part(s) of my program should I fiddle with in an attempt to fix it?
> Starting values?  Something in the log-likelihood itself?  Perhaps the data
> (which is generated)?  Any thoughts would be greatly appreciated.
>

Use Nelder-Mead algorithm for finding apropriate starting values. This
algorithm does not use gradients, so you will not aforementioned
error. After Nelder-Mead you can try again with gradient methods, like
BFGS. If that does not help, try scaling your data. Optim behaves
better (IMHO) when all parameters are of the same order. If you do not
need hessian, and BFGS fails, use only Nelder-Mead, it will at least
give you something.

Vaidotas Zemlys
--
Doctorate student, Vilnius University
http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php


From Yingfu.Xie at sekon.slu.se  Thu Aug 10 16:08:15 2006
From: Yingfu.Xie at sekon.slu.se (Yingfu Xie)
Date: Thu, 10 Aug 2006 16:08:15 +0200
Subject: [R] Solve non-linear equatuion,
	grid search... [Was] RE: minimization of a quadratic form with some
	coef fixed and someconstrained
In-Reply-To: <17626.37474.906761.751666@bossiaea.maths.uwa.edu.au>
Message-ID: <CA871298CD1882459F7859BD08DC06E4C53149@slumail.ad.slu.se>



Hello, Berwin,

Thanks a lot for answering! I prefer the first method, which seems
easier to carry out. So, what I need now is some method to solve the
non-linear equation
c'(M_11-lam*I)^(-1) (M_11-lam*I)^(-1)c=1
for lam, where c is a vector, M_11 is a symmetric matrix and I is the
identity matrix.
I searched the R archive somehow, but didn't find anything valuable. Is
there any function in R available for this problem? I am grateful of any
hints.

When c=0, my problem reduces to the typical minimization of b'M_11b
w.r.t b'b=1. The solution is the normalized eigenvector associated with
the minimum eigenvalue of M_11, right?

Thanks,
Yingfu

PS: There is a type error in the first condition for b: the '+' should
write to '-'.

-----Original Message-----
From: Berwin A Turlach [mailto:berwin at bossiaea.maths.uwa.edu.au] On
Behalf Of Berwin A Turlach
Sent: den 10 augusti 2006 03:57
To: Yingfu Xie
Cc: Rolf Turner; r-help at stat.math.ethz.ch
Subject: Re: [R] minimization a quadratic form with some coef fixed and
someconstrained

>>>>> "YX" == Yingfu Xie <Yingfu.Xie at sekon.slu.se> writes:

    YX> Thanks for reply! But I think that solution is right without
    YX> the constrain b'b=1. With this constrain, the solution is not
    YX> so simple. :(
But simple enough. :)

Write down the Lagrange function for the problem.  Say, 'lam' is the
Lagrange parameter for enforcing the constraint b'b=1.  Then, using
Rolf's notation:
    RT> [...]  Write M as

    RT> | M_11 c |
    RT> | c'   m |

Then the system of equations that b and the Lagrange parameter have to
fulfill is:

        b = (M_11 + lam*I)^{-1} c   (with I being the identity matrix)
and   lam = b' M_11 b - b'c

You can either use the first equation and do a (grid) search for the
value of 'lam' that gives you b'b=1 (could be negative!), or start
with lam=0 and then alternate between the two equations until
convergence.  

At least I think that this will solve your problem. :)  Thinking a bit
about the geometry of the problem, I actually believe that if c=0, you
might have an identifiability problem, i.e. there are at least two 
solutions, or, depending on M_11, infinitely many.

Hope this helps.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)

The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin

###########################################

This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}


From dimitris.rizopoulos at med.kuleuven.be  Thu Aug 10 16:24:21 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 10 Aug 2006 16:24:21 +0200
Subject: [R] help with structuring random factors using lmer()
References: <1182.144.173.76.117.1155218456.squirrel@www.webmail.ex.ac.uk>
Message-ID: <002c01c6bc88$ac2afb70$0540210a@www.domain>

probably you're looking for something like:

m1 <- lmer(weight ~ time + I(time^2) + (time + I(time^2) - 1 | id), 
data = dataset)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Simon Pickett" <S.Pickett at exeter.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 10, 2006 4:00 PM
Subject: [R] help with structuring random factors using lmer()


> Hi,
> I am an R beginner and having problems structuring my REML models. I 
> have
> a model with
> y=weight
> x1=time
> x2=timesquared
> id=individual identity
> I need to structure the model such that in the random effects there 
> is a
> constant intercept for all individuals but a separate individual 
> slope for
> both x1 and x2 (a coefficient score for every individual).
> m1<-lmer(weight~time+timesq+(1|id)+(timesq-1|id)+(time-1|id), 
> data=dataset)
> coef(m1)
> gives me nearly what I want except there isnt an individual 
> coefficient
> score for each individual for x2.
> Any suggestions very much appreciated.
> Simon Pickett
> s.pickett at exeter.ac.uk
>
>
>
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From petr.pikal at precheza.cz  Thu Aug 10 16:32:54 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Aug 2006 16:32:54 +0200
Subject: [R] bug in interaction order when using drop?
Message-ID: <44DB5FB6.651.1194A21@localhost>

Hallo all

> version
               _                                     
platform       i386-pc-mingw32                       
arch           i386                                  
os             mingw32                               
system         i386, mingw32                         
status         beta                                  
major          2                                     
minor          3.1                                   
year           2006                                  
month          05                                    
day            23                                    
svn rev        38179                                 
language       R                                     
version.string Version 2.3.1 beta (2006-05-23 r38179)
>

When I use interaction(....) without drop=T parameters I will get 
neatly organized factor with "protiproud" and "souproud" aligned.

> levels(interaction(vykon, teplota, proudeni))
 [1] "3.750.protiproud"  "12.750.protiproud" "3.775.protiproud"  
"12.775.protiproud" "3.800.protiproud"  "12.800.protiproud"
 [7] "3.825.protiproud"  "12.825.protiproud" "3.850.protiproud"  
"12.850.protiproud" "3.750.souproud"    "12.750.souproud"  
[13] "3.775.souproud"    "12.775.souproud"   "3.800.souproud"    
"12.800.souproud"   "3.825.souproud"    "12.825.souproud"  
[19] "3.850.souproud"    "12.850.souproud"  

However when I use 

> levels(interaction(vykon, teplota, proudeni, drop=T))
[1] "3.775.protiproud"  "3.800.souproud"    "3.750.souproud"    
"12.850.souproud"   "12.825.protiproud"

everything is out of order. I know I can reorder any factor according 
to my wish but it would be good to have it ordered same way as 
without using drop.

Everything comes from unique in

if (drop) {
        f <- unique(ans[!is.na(ans)])
        ans <- match(ans, f)
        lvs <- lvs[f]
}

maybe it can be modified.

if (drop) {
        f <- unique(ans[!is.na(ans)])
        ord <- order(f)
        ans <- match(ans, f)
        lvs <- lvs[f[ord]]
        }

which seems to work but I am not sure if it does not makes problems 
having NA in data.

Here is my data frame.
Thank you 

Petr Pikal

> dump("df", file=stdout()) 
df <-
structure(list(proudeni = structure(as.integer(c(1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
1, 1, 1)), .Label = c("protiproud", "souproud"), class = "factor"), 
    vykon = as.integer(c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 
    12, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 
    12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 
    12, 12, 12, 12, 12, 12)), teplota = as.integer(c(775, 775, 
    775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
    775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 800, 800, 
    800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 750, 850, 
    850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 825, 775, 
    775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
    775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 800, 
    800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 750, 
    850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 825, 
    775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
    775, 775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 
    800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
    750, 850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 
    825, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
    775, 775, 775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 
    800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
    800, 750, 850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 
    825, 825))), .Names = c("proudeni", "vykon", "teplota"), 
row.names = c("1", 
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", 
"25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", 
"36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", 
"47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", 
"58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", 
"69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", 
"80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90", 
"91", "92", "93", "94", "95", "96", "97", "98", "99", "100", 
"101", "102", "103", "104", "105", "106", "107", "108", "109", 
"110", "111", "112", "113", "114", "115", "116", "117", "118", 
"119", "120", "121", "122", "123", "124", "125", "126", "127", 
"128", "129", "130", "131", "132", "133", "134", "135", "136", 
"137", "138", "139", "140", "141", "142", "143", "144", "145", 
"146", "147", "148", "149", "150", "151", "152", "153", "154", 
"155", "156", "157", "158", "159", "160", "161", "162", "163", 
"164", "165", "166", "167", "168", "169", "170", "171", "172", 
"173", "174", "175", "176", "177", "178", "179", "180", "181", 
"182", "183", "184", "185", "186", "187", "188", "189", "190", 
"191", "192", "193", "194", "195", "196"), class = "data.frame")
> Petr Pikal
petr.pikal at precheza.cz


From c.oswald at matsci.uni-sb.de  Thu Aug 10 16:39:04 2006
From: c.oswald at matsci.uni-sb.de (Christian Oswald)
Date: Thu, 10 Aug 2006 16:39:04 +0200
Subject: [R] categorical data
Message-ID: <44DB4508.4050009@matsci.uni-sb.de>

Hello,

thank you very much, it works super!

Christian

Hi

On 10 Aug 2006 at 9:19, Christian Oswald wrote:

Date sent:      	Thu, 10 Aug 2006 09:19:06 +0200
From:           	Christian Oswald <oswald at dhlaw.de>
To:             	r-help at stat.math.ethz.ch
Subject:        	Re: [R] categorical data
Send reply to:  	oswald at dhlaw.de
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hello,
> 
> thats what I need, a list sorted first after year and then after
> categorie. But I get an error message
> 
> > df
>       df     cate b    c
>  [1,] "2006" "a1" "1"  "1"
>  [2,] "2006" "a2" "2"  "2"
>  [3,] "2005" "a1" "3"  "3"
>  [4,] "2004" "a3" "1"  "1"
>  [5,] "2004" "a2" "2"  "2"
>  [6,] "2005" "a1" "3"  "3"
>  [7,] "2003" "a2" "11" "11"
>  [8,] "2003" "a1" "2"  "2"
>  [9,] "2006" "a2" "3"  "3"

This is not a data frame but character matrix
try str(df). It was probably constructed by cbind(...), try to use
data.frame(....) instead.

Or you can try

as.data.frame(df) but then you need to change resulting factors back
to numeric
?as.character
?as.numeric

HTH
Petr


try

> > res<-aggregate( df[,c(3,4)], list(df$year,df$cate), sum)
> Fehler in as.vector(x, mode) : Argument hat ung?ltigen 'mode'
> 
> 
> (Error in as.vector(x,mode) :Argument has invalid mode)
> 
> I had tested the mode and receive "character". Can someone explain
> what thats mean?
> 
> Christian
> 
> 
> 
> On Wed, 2006-08-09 at 18:07 +0200, Christian Oswald wrote:
> > > Dear List,
> > >
> > > I neeed a grouped list with two sort of categorical data. I have a
> > > data .frame like this. 	year	cat.	b	c 1	2006	a1	125	212
> > > 2	2006	a2	256	212	 3	2005	a1	14	12 4	2004	a3	565	123
> > > 5	2004	a2	156	789	 6	2005	a1	1	456 7	2003	a2	786	123
> > > 8	2003	a1	421	569 9  	2002	a2	425	245
> > >
> > > I need a list with the sum of b and c for every year and every cat
> > > (a1, a2 or a3) in this year. I had used the tapply function to
> > > build the sum for every year or every cat. How can I combine the
> > > two grouping values?
> 
> Christian,
> 
> Is that what you want (using DF as your data.frame):
> 
> > > aggregate(DF[, c("b", "c")],
>             by = list(Year = DF$year, Cat = DF$cat.),
>             sum)
>   Year Cat   b   c
> 1 2003  a1 421 569
> 2 2005  a1  15 468
> 3 2006  a1 125 212
> 4 2002  a2 425 245
> 5 2003  a2 786 123
> 6 2004  a2 156 789
> 7 2006  a2 256 212
> 8 2004  a3 565 123
> 
> You can also reorder the results by Year and Cat:
> 
> > > DF.result <- aggregate(DF[, c("b", "c")],
>                          by = list(Year = DFyear, Cat = DF$cat.), sum)
> 
> > > DF.result[order(DF.result$Year, DF.result$Cat), ]
>   Year Cat   b   c
> 4 2002  a2 425 245
> 1 2003  a1 421 569
> 5 2003  a2 786 123
> 6 2004  a2 156 789
> 8 2004  a3 565 123
> 2 2005  a1  15 468
> 3 2006  a1 125 212
> 7 2006  a2 256 212
> 
> 
> 
> Note that tapply() can only handle one 'X' vector at a time, whereas
> aggregate can handle multiple 'X' columns in one call. For example:
> 
> > > tapply(DF$b, list(DF$year, DF$cat.), sum)
>       a1  a2  a3
> 2002  NA 425  NA
> 2003 421 786  NA
> 2004  NA 156 565
> 2005  15  NA  NA
> 2006 125 256  NA
> 
> will give you the sum of 'b' for each combination of Year and Cat
> within the 2d table, but I suspect this is not the output format you
> want. You also get NA's in the cells where there was not the given
> combination present in your data.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
> 
> 
> 
> aw.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




-- 
Diplom-Ingenieur Christian Oswald
Universitaet des Saarlandes
Lehrstuhl fuer Pulvertechnologie von Glas und Keramik
Gebaude D2 2
D-66123 Saarbruecken

Tel.: (+49) 0681/302-5249
Fax.: (+49) 0681/302-5227


From dgerlanc at gmail.com  Thu Aug 10 17:07:53 2006
From: dgerlanc at gmail.com (Daniel Gerlanc)
Date: Thu, 10 Aug 2006 11:07:53 -0400
Subject: [R] Sampling from a Matrix
In-Reply-To: <1155133358.3790.5.camel@localhost.localdomain>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAB685@usctmx1106.merck.com>
	<1155133358.3790.5.camel@localhost.localdomain>
Message-ID: <84c9e3cb0608100807x2bdd37c4w22f9609e548eaf99@mail.gmail.com>

Once again, thanks for your help.

I did not state the problem correctly, though the code is correct for
what I want to do.

A better description of the problem would be that there is a matrix of
probabilities:

> set.seed(1)
> probs <- array(abs(rnorm(25, sd = 0.33)), dim = c(5,5), dimnames = list(1:5, letters[1:5]))
> probs
       a      b       c         d        e
1 0.21 0.27 0.50 0.0148 0.303
2 0.06 0.16 0.13 0.0053 0.258
3 0.28 0.24 0.21 0.3115 0.025
4 0.53 0.19 0.73 0.2710 0.656
5 0.11 0.10 0.37 0.1960 0.205

The column names, dimnames(probs)[[2]], are the names of units to be
sampled.    Each row is a trial.  For each row (trial), I want to
sample 3 of the units such that for each row I get a vector like the
following:

[1] "a", "b", "a"

The samples are to be done with replacement, and these vectors could
be combined to form a matrix of the samples.

The purpose of the "probs" matrix is to give each unit a probability
that it will be sampled.

One way to do this follows:

index <- 1:ncol(probs)

res <- matrix(0,
                      nrow = dim(probs)[1],
                      ncol = 3
)

for(i in 1:nrow(probs)){

## gets the indexes of the values chosen

res[, i] <- sample(index, size = 3, replace = TRUE, prob = probs[i, ])

}

Using "apply" as Andy described would accomplish the intended result.

-- Dan

> > Hmm... If I read Daniel's code (which is different from his description)
> > correctly, that doesn't seem to be what he wanted.  Perhaps something like
> > this:
> >
> > apply(probs, 1, function(p) sample(1:ncol(probs), 3, replace=TRUE, prob=p))
> >
> > Andy
>
> Andy,
>
> You are of course correct. I had focused on the description of the
> problem, rather than the code provided, presuming that the code was not
> correct, including the use of 'replace' and 'prob' in sample().
>
> I suppose it would be up to Daniel for clarification.
>
> Regards,
>
> Marc
>
>
>


-- 
Daniel Gerlanc
Williams College '07


From paber at tower-research.com  Thu Aug 10 17:10:35 2006
From: paber at tower-research.com (Patrick Aber)
Date: Thu, 10 Aug 2006 11:10:35 -0400
Subject: [R] summary statistics on an entire data frame
Message-ID: <000901c6bc8f$216c9ff0$1916000a@PABERPC>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/eeff98f8/attachment.pl 

From petr.pikal at precheza.cz  Thu Aug 10 17:12:34 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Aug 2006 17:12:34 +0200
Subject: [R] bug in interaction order when using drop?
In-Reply-To: <44DB5FB6.651.1194A21@localhost>
Message-ID: <44DB6902.31178.13D97A6@localhost>

Ooops, my first suggestion reorders factor itself but

if (drop) factor(ans) else ans

instead of whole drop construction shall preserve levels order 
without changing order of factor

Petr

On 10 Aug 2006 at 16:32, Petr Pikal wrote:

From:           	"Petr Pikal" <petr.pikal at precheza.cz>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Thu, 10 Aug 2006 16:32:54 +0200
Priority:       	normal
Subject:        	[R] bug in interaction order when using drop?

> Hallo all
> 
> > version
>                _                                   
> platform       i386-pc-mingw32                       
> arch           i386                                  
> os             mingw32                               
> system         i386, mingw32                         
> status         beta                                  
> major          2                                   
> minor          3.1                                   
> year           2006                                  
> month          05                                   
> day            23                                   
> svn rev        38179                                 
> language       R                                   
> version.string Version 2.3.1 beta (2006-05-23 r38179)
> >
> 
> When I use interaction(....) without drop=T parameters I will get
> neatly organized factor with "protiproud" and "souproud" aligned.
> 
> > levels(interaction(vykon, teplota, proudeni))
>  [1] "3.750.protiproud"  "12.750.protiproud" "3.775.protiproud" 
> "12.775.protiproud" "3.800.protiproud"  "12.800.protiproud"
>  [7] "3.825.protiproud"  "12.825.protiproud" "3.850.protiproud" 
> "12.850.protiproud" "3.750.souproud"    "12.750.souproud"  [13]
> "3.775.souproud"    "12.775.souproud"   "3.800.souproud"   
> "12.800.souproud"   "3.825.souproud"    "12.825.souproud"  [19]
> "3.850.souproud"    "12.850.souproud"  
> 
> However when I use 
> 
> > levels(interaction(vykon, teplota, proudeni, drop=T))
> [1] "3.775.protiproud"  "3.800.souproud"    "3.750.souproud"    
> "12.850.souproud"   "12.825.protiproud"
> 
> everything is out of order. I know I can reorder any factor according
> to my wish but it would be good to have it ordered same way as without
> using drop.
> 
> Everything comes from unique in
> 
> if (drop) {
>         f <- unique(ans[!is.na(ans)])
>         ans <- match(ans, f)
>         lvs <- lvs[f]
> }
> 
> maybe it can be modified.
> 
> if (drop) {
>         f <- unique(ans[!is.na(ans)])
>         ord <- order(f)
>         ans <- match(ans, f)
>         lvs <- lvs[f[ord]]
>         }
> 
> which seems to work but I am not sure if it does not makes problems
> having NA in data.
> 
> Here is my data frame.
> Thank you 
> 
> Petr Pikal
> 
> > dump("df", file=stdout()) 
> df <-
> structure(list(proudeni = structure(as.integer(c(1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 
> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
> 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 
> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
> 1, 1, 1)), .Label = c("protiproud", "souproud"), class = "factor"), 
>     vykon = as.integer(c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 
>     12, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 
>     12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
>     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 
>     12, 12, 12, 12, 12, 12)), teplota = as.integer(c(775, 775, 
>     775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
>     775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 800, 800, 
>     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 750, 850, 
>     850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 825, 775, 
>     775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
>     775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 800, 
>     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 750, 
>     850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 825, 
>     775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
>     775, 775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 
>     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
>     750, 850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 
>     825, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
>     775, 775, 775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 
>     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
>     800, 750, 850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 
>     825, 825))), .Names = c("proudeni", "vykon", "teplota"), 
> row.names = c("1", 
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
> "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
> "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
> "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
> "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
> "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
> "91", "92", "93", "94", "95", "96", "97", "98", "99", "100", "101",
> "102", "103", "104", "105", "106", "107", "108", "109", "110", "111",
> "112", "113", "114", "115", "116", "117", "118", "119", "120", "121",
> "122", "123", "124", "125", "126", "127", "128", "129", "130", "131",
> "132", "133", "134", "135", "136", "137", "138", "139", "140", "141",
> "142", "143", "144", "145", "146", "147", "148", "149", "150", "151",
> "152", "153", "154", "155", "156", "157", "158", "159", "160", "161",
> "162", "163", "164", "165", "166", "167", "168", "169", "170", "171",
> "172", "173", "174", "175", "176", "177", "178", "179", "180", "181",
> "182", "183", "184", "185", "186", "187", "188", "189", "190", "191",
> "192", "193", "194", "195", "196"), class = "data.frame") > Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From berwin at maths.uwa.edu.au  Thu Aug 10 17:17:46 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 10 Aug 2006 23:17:46 +0800
Subject: [R] Solve non-linear equatuion,
	grid search... [Was] RE: minimization of a quadratic form with some
	coef fixed and someconstrained
In-Reply-To: <CA871298CD1882459F7859BD08DC06E4C53149@slumail.ad.slu.se>
References: <17626.37474.906761.751666@bossiaea.maths.uwa.edu.au>
	<CA871298CD1882459F7859BD08DC06E4C53149@slumail.ad.slu.se>
Message-ID: <17627.19994.827679.418186@bossiaea.maths.uwa.edu.au>

G'day Yingfu,

>>>>> "YX" == Yingfu Xie <Yingfu.Xie at sekon.slu.se> writes:

    YX> Thanks a lot for answering! I prefer the first method, which
    YX> seems easier to carry out. So, what I need now is some method
    YX> to solve the non-linear equation [...]  I searched the R
    YX> archive somehow, but didn't find anything valuable. Is there
    YX> any function in R available for this problem? I am grateful of
    YX> any hints.
You have to code the function
        f(lam) = c'(M_11-lam*I)^(-2)c-1
in R.  Note that lam is univariate.

If you could easily find lam1 and lam2 with f(lam1)<0 and f(lam2)>0,
then you could use uniroot().  But optim() could be easier to use for
finding lam s.t. f(lam)=0, just try to minimise f(lam)^2.

    YX> When c=0, my problem reduces to the typical minimization of
    YX> b'M_11b w.r.t b'b=1. The solution is the normalized
    YX> eigenvector associated with the minimum eigenvalue of M_11,
    YX> right?
Depends on your M_11, if M_11 is the identity matrix, then you have
infinitely many solutions.  Intuitively, I would have thought that the
solution is the normalised eigenvector (and its negation, so there are
at least two solutions) of the maximum eigenvalue of M_11, but I might
be drawing the wrong picture in my mind.  It should be either the
maximum or minimum.  Note, if that eigenvalue has geometric
multiplicity > 1, then there will be infinitely many solutions.

    YX> PS: There is a type error in the first condition for b: the
    YX> '+' should write to '-'.
I believe both versions are correct.

We are enforcing an equality constraint, not an inequality
constraint.  So it doesn't matter whether we write the Lagrangian as
        objective function + lam *(b'b-1)
or
        objective function - lam *(b'b-1)
In the KKT conditions, we will only have the complimentary condition
that lam*(b'b-1)=0, no condition that lam>=0.

When you enforce inequality constraints, then you have to take care
with your signs and how you write the Lagrangian.

Cheers,

        Berwin


From petr.pikal at precheza.cz  Thu Aug 10 17:37:59 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Aug 2006 17:37:59 +0200
Subject: [R] summary statistics on an entire data frame
In-Reply-To: <000901c6bc8f$216c9ff0$1916000a@PABERPC>
Message-ID: <44DB6EF7.19469.154DD1B@localhost>

Hi
data frame is list so

mean(unlist(DF)) or sd(unlist(df))

shall do what you want if I understood correctly.

HTH
Petr


On 10 Aug 2006 at 11:10, Patrick Aber wrote:

From:           	"Patrick Aber" <paber at tower-research.com>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Thu, 10 Aug 2006 11:10:35 -0400
Subject:        	[R] summary statistics on an entire data frame

> Hello,
> 
> I am wondering how to compute functions like mean(), sd(), etc. on a
> data frame, but instead of getting a vector with the summary stat
> calculated individually for each column, get one number for the whole
> data set.  I have noticed that coercing the frame to a time series
> (ts) gives the desired result for certain functions, but not others. 
> mean() gives a single numeric result, but sd() still gives standard
> deviations per column.  I have noticed that when my data is coerced to
> a ts, it actually gets the class c("mts", ts") instead of just "ts",
> which may or may not have something to do with the problem.
> 
> Thanks for any help you can give!
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From m-krutky at northwestern.edu  Thu Aug 10 17:40:06 2006
From: m-krutky at northwestern.edu (m-krutky at northwestern.edu)
Date: Thu, 10 Aug 2006 16:40:06 +0100
Subject: [R] glmmPQL question!
Message-ID: <20060810154007.02D3929@lulu.it.northwestern.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/cb399b83/attachment.pl 

From dsohal at gmail.com  Thu Aug 10 18:07:21 2006
From: dsohal at gmail.com (Davendra Sohal)
Date: Thu, 10 Aug 2006 12:07:21 -0400
Subject: [R] Multiple density curves
Message-ID: <c2f237040608100907t44dba1dax3c34a0178b73ddee@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/1ebb9144/attachment.pl 

From mmanfrin at ulb.ac.be  Thu Aug 10 18:13:46 2006
From: mmanfrin at ulb.ac.be (Max Manfrin)
Date: Thu, 10 Aug 2006 18:13:46 +0200
Subject: [R] How to speed up nested for loop computations
Message-ID: <A07FC905-D0C3-4B76-B22F-258035727FA6@ulb.ac.be>

Dear all,
	here is the result of R.Version():

 > R.Version()
$platform
[1] "powerpc-apple-darwin8.6.0"

$arch
[1] "powerpc"

$os
[1] "darwin8.6.0"

$system
[1] "powerpc, darwin8.6.0"

$status
[1] ""

$major
[1] "2"

$minor
[1] "3.1"

$year
[1] "2006"

$month
[1] "06"

$day
[1] "01"

$`svn rev`
[1] "38247"

$language
[1] "R"

$version.string
[1] "Version 2.3.1 (2006-06-01)"

 >

	I have the following code to produce boxplots of some experimental  
data, but it's really slow (I'm a newbie of R and the quality of my  
code is not really high-level!). Could you give me some guidelines  
(or examples) on how to remove those nested for loops (maybe using  
the "apply" function) so to speed-up the computation?

---BEGIN CODE---

optimal_values<-read.table("optimal_values_80.txt",header=TRUE)
resPIR2OPT<-read.table("parallel_independent_2- 
opt_80_100.txt",header=TRUE)
resSEQ2OPT<-read.table("sequential_2-opt_80_800.txt",header=TRUE)
resSEQ22OPT<-read.table("sequential2_2-opt_80_100.txt",header=TRUE)
resFC1x102OPT<-read.table("fc.1.x.10_2-opt_80_100.txt",header=TRUE)
resFC26102OPT<-read.table("fc.2.6.10_2-opt_80_100.txt",header=TRUE)
resFC27102OPT<-read.table("fc.2.7.10_2-opt_80_100.txt",header=TRUE)
resFC28102OPT<-read.table("fc.2.8.10_2-opt_80_100.txt",header=TRUE)
resFC29102OPT<-read.table("fc.2.9.10_2-opt_80_100.txt",header=TRUE)
resFC36102OPT<-read.table("fc.3.6.10_2-opt_80_100.txt",header=TRUE)
resFC37102OPT<-read.table("fc.3.7.10_2-opt_80_100.txt",header=TRUE)
resFC38102OPT<-read.table("fc.3.8.10_2-opt_80_100.txt",header=TRUE)
resFC39102OPT<-read.table("fc.3.9.10_2-opt_80_100.txt",header=TRUE)
resHC1x102OPT<-read.table("hc.1.x.10_2-opt_80_100.txt",header=TRUE)
resHC26102OPT<-read.table("hc.2.6.10_2-opt_80_100.txt",header=TRUE)
resHC27102OPT<-read.table("hc.2.7.10_2-opt_80_100.txt",header=TRUE)
resHC28102OPT<-read.table("hc.2.8.10_2-opt_80_100.txt",header=TRUE)
resHC29102OPT<-read.table("hc.2.9.10_2-opt_80_100.txt",header=TRUE)
resHC36102OPT<-read.table("hc.3.6.10_2-opt_80_100.txt",header=TRUE)
resHC37102OPT<-read.table("hc.3.7.10_2-opt_80_100.txt",header=TRUE)
resHC38102OPT<-read.table("hc.3.8.10_2-opt_80_100.txt",header=TRUE)
resHC39102OPT<-read.table("hc.3.9.10_2-opt_80_100.txt",header=TRUE)
resRW1x102OPT<-read.table("rw.1.x.10_2-opt_80_100.txt",header=TRUE)
resRW26102OPT<-read.table("rw.2.6.10_2-opt_80_100.txt",header=TRUE)
resRW27102OPT<-read.table("rw.2.7.10_2-opt_80_100.txt",header=TRUE)
resRW28102OPT<-read.table("rw.2.8.10_2-opt_80_100.txt",header=TRUE)
resRW29102OPT<-read.table("rw.2.9.10_2-opt_80_100.txt",header=TRUE)
resRW36102OPT<-read.table("rw.3.6.10_2-opt_80_100.txt",header=TRUE)
resRW37102OPT<-read.table("rw.3.7.10_2-opt_80_100.txt",header=TRUE)
resRW38102OPT<-read.table("rw.3.8.10_2-opt_80_100.txt",header=TRUE)
resRW39102OPT<-read.table("rw.3.9.10_2-opt_80_100.txt",header=TRUE)
resUR1x102OPT<-read.table("ur.1.x.10_2-opt_80_100.txt",header=TRUE)
resUR26102OPT<-read.table("ur.2.6.10_2-opt_80_100.txt",header=TRUE)
resUR27102OPT<-read.table("ur.2.7.10_2-opt_80_100.txt",header=TRUE)
resUR28102OPT<-read.table("ur.2.8.10_2-opt_80_100.txt",header=TRUE)
resUR29102OPT<-read.table("ur.2.9.10_2-opt_80_100.txt",header=TRUE)
resUR36102OPT<-read.table("ur.3.6.10_2-opt_80_100.txt",header=TRUE)
resUR37102OPT<-read.table("ur.3.7.10_2-opt_80_100.txt",header=TRUE)
resUR38102OPT<-read.table("ur.3.8.10_2-opt_80_100.txt",header=TRUE)
resUR39102OPT<-read.table("ur.3.9.10_2-opt_80_100.txt",header=TRUE)

res<-rbind 
(resFC1x102OPT,resFC26102OPT,resFC27102OPT,resFC28102OPT,resFC29102OPT,r 
esFC36102OPT,resFC37102OPT,resFC38102OPT,resFC39102OPT,resRW1x102OPT,res 
RW26102OPT,resRW27102OPT,resRW28102OPT,resRW29102OPT,resRW36102OPT,resRW 
37102OPT,resRW38102OPT,resRW39102OPT,resHC1x102OPT,resUR1x102OPT,resUR26 
102OPT,resUR27102OPT,resUR28102OPT,resUR29102OPT,resUR36102OPT,resUR3710 
2OPT,resUR38102OPT,resUR39102OPT,resPIR2OPT,resSEQ2OPT,resSEQ22OPT)


attach(res)
lalgo<-levels(idalgo)
linstance<-levels(instance)
ltry<-unique(try)
lcpu<-unique(cpu_id)

for (i in (1:length(linstance)))
{
	current_instance<-linstance[i]

	bestalgo<-list()
	for (j in (1:length(ltry)))
	{
		current_try<-ltry[j]	

		for (k in (1:length(lalgo)))
		{
			current_algo<-lalgo[k]
			
			res2<-res[res$instance==current_instance & res$try==current_try &  
res$idalgo==current_algo,]
# res2 contains for a given instance, a given try, and a given algo,  
all results
			res3<-res2[res2$best==min(res2$best),]
			res4<-res3[res3$time==min(res3$time),]
			if (nrow(res4)>1)
			{
				res4<-res4[1,]	
			}
			if (nrow(res4)==1)
			{
				res4$best<-(res4$best*100/optimal_values[optimal_values 
$instance==linstance[i],]$optimum)-100
     			print(res4)
				bestalgo<-rbind(bestalgo,res4)
			}
		}
	}	
	epsfile=paste(linstance[i],"_100_lim.eps",sep="")
	postscript(file=epsfile,onefile=TRUE,horizontal=TRUE)
	l<-split(bestalgo$best,list(bestalgo$idalgo))
	par(mar=c(5,5,5,3),cex.axis=0.7,las=2,mgp=c(4, 1, 0))
	title_plot=paste("100 iterations - instance ",linstance[i],sep="")
	boxplot(l,xlab="",ylab="% distance from best known solution",names=c 
(levels(bestalgo$idalgo)),main=title_plot,ylim=c(0,0.5))

	dev.off()
	epsfile=paste(linstance[i],"_100_nolim.eps",sep="")
	postscript(file=epsfile,onefile=TRUE,horizontal=TRUE)
	l<-split(bestalgo$best,list(bestalgo$idalgo))
	par(mar=c(5,5,5,3),cex.axis=0.7,las=2,mgp=c(4, 1, 0))
	title_plot=paste("100 iterations - instance ",linstance[i],sep="")
	boxplot(l,xlab="",ylab="% distance from best known solution",names=c 
(levels(bestalgo$idalgo)),main=title_plot)

	dev.off()			
	
}
detach(res)

---END CODE ---


	This is the output of the command str(res):

 > str(res)
`data.frame':	230200 obs. of  11 variables:
$ idalgo   : Factor w/ 31 levels "FC.1.x.10-2opt",..: 1 1 1 1 1 1 1 1  
1 1 ...
$ topo     : Factor w/ 7 levels "FC","RW","HC",..: 1 1 1 1 1 1 1 1 1  
1 ...
$ schema   : Factor w/ 12 levels "1.x.10","2.6.10",..: 1 1 1 1 1 1 1  
1 1 1 ...
$ ls       : int  2 2 2 2 2 2 2 2 2 2 ...
$ type     : Factor w/ 2 levels "Par","Seq": 1 1 1 1 1 1 1 1 1 1 ...
$ cpu_id   : int  0 0 0 0 0 0 0 0 0 0 ...
$ instance : Factor w/ 2 levels "lipa80a","tai80a": 1 1 1 1 1 1 1 1 1  
1 ...
$ try      : int  1 1 1 1 1 1 1 1 1 1 ...
$ best     : int  255434 255321 255296 255224 255181 255030 254985  
254961 254927 254897 ...
$ time     : num  0.09 0.09 0.09 0.18 0.27 0.46 1 1.37 1.42 1.66 ...
$ iteration: int  1 1 1 2 3 5 11 17 18 20 ...
 >

	Hoping that somebody could help me, accept my best regards.
----
Max MANFRIN
http://iridia.ulb.ac.be/~mmanfrin/


From jholtman at gmail.com  Thu Aug 10 18:46:15 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 10 Aug 2006 12:46:15 -0400
Subject: [R] How to speed up nested for loop computations
In-Reply-To: <A07FC905-D0C3-4B76-B22F-258035727FA6@ulb.ac.be>
References: <A07FC905-D0C3-4B76-B22F-258035727FA6@ulb.ac.be>
Message-ID: <644e1f320608100946r2dc8277dv38f2f4645eca7d18@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/872cb644/attachment.pl 

From edd at debian.org  Thu Aug 10 18:49:54 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 10 Aug 2006 11:49:54 -0500
Subject: [R] tcltk library on linux
In-Reply-To: <C8F48FD780E12D4DB197507B897D00DD072F5B1A@ntexch.softcomputing.com>
References: <C8F48FD780E12D4DB197507B897D00DD072F5B1A@ntexch.softcomputing.com>
Message-ID: <17627.25522.432802.768722@basebud.nulle.part>


On 10 August 2006 at 15:09, Yohan CHOUKROUN wrote:
| I want to use the Rcmdr package which depends on the tcltk library.
| 
| I'm on Linux Ubuntu.

$ sudo apt-get install r-cran-rcmdr

should be all you need thanks to all the work done on the Debian side.

Amicalement,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From stefan.grosse at uni-erfurt.de  Thu Aug 10 19:00:08 2006
From: stefan.grosse at uni-erfurt.de (Stefan Grosse)
Date: Thu, 10 Aug 2006 19:00:08 +0200
Subject: [R] graphic output file format
In-Reply-To: <20060810164524.45102.qmail@web32515.mail.mud.yahoo.com>
References: <20060810164524.45102.qmail@web32515.mail.mud.yahoo.com>
Message-ID: <44DB6618.2050109@uni-erfurt.de>

Please mail always also to the list, since it may be that there is
someone who could reply better and/or faster.

Especially in this case since I am only experienced in M$ Windows and
Linux so I am unable to guess whats wrong with OSX. does not even the
postscript device work? With postscript I get the best quality for printing.

e.g.:
postscript("test.eps", width = 8.0, height = 6.0, horizontal = FALSE,
onefile =FALSE, paper = "special")
curve(x^2)
dev.off()


Conan Phelan schrieb:
> Thanks for the feedback and link Stefan,
>
> Perhaps if I provide a little more background. (Pls
> forgive both my ignorance and my long windedness.)
>
>  I am on Mac OSX and am only able to load the Quartz
> graphic device (is that normal?). From quartz, I've
> only been able to generate pdf files, which do not
> match the quartz display to my satisfaction. I've been
> attempting to find a way to generate other output
> files for comparison. (I've managed to generate a bmp
> file that looked awful, after acquiring Gostscript). 
>
> Anyway, I figure R must able to produce high quality
> graphic outputs, so I'm trying different things. To
> this end I was hoping that the GDD package would allow
> me to load the x11 module but it seems to be a no-go
> (is there some system requirement I'm missing?).
>
> Conan
>
>


From matteucci at stat.unibo.it  Thu Aug 10 19:10:37 2006
From: matteucci at stat.unibo.it (Mariagiulia Matteucci)
Date: Thu, 10 Aug 2006 19:10:37 +0200 (CEST)
Subject: [R] mcmc pack
Message-ID: <6932247.1155229837565.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Hello, 
I would like to see the C++ code for the MCMC package, how can I do
that?
Thanks,

Mariagiulia


From swidanf at janelia.hhmi.org  Thu Aug 10 20:34:27 2006
From: swidanf at janelia.hhmi.org (Swidan, Firas)
Date: Thu, 10 Aug 2006 14:34:27 -0400
Subject: [R] Speeding indexing and sub-sectioning of 3d array
In-Reply-To: <44DA488D.4040104@pburns.seanet.com>
Message-ID: <C100F473.1122%swidanf@janelia.hhmi.org>

Hi Patrick,

Thanks for the help. The function I listed is just an example. I isolated
and kept only the problematic part in my code for clarity sake. I ended up
implementing the functionality in C and now it takes 22 seconds to calculate
the objective.

Best regards,
Firas.

On 8/9/06 4:41 PM, "Patrick Burns" <pburns at pburns.seanet.com> wrote:

> First off, I hope that the function you list is just an example
> since it only returns what the last iteration does -- obviously
> the same answer can be arrived at much quicker.
> 
> The main principal in speeding up loops is to do as little
> inside the loops as possible.  'fjj1' is essentially the same as
> the listed function, but with one slight cleanup.
> 
> fjj1 <-
> function(x, radius=3)
> {
>         dx <- dim(x)
>         dx1 <- dx[1]
>         dx2 <- dx[2]
>         dx3 <- dx[3]
>         for(i in (radius + 1):(dx1 - radius - 1)) {
>                 for(j in (radius + 1):(dx2 - radius - 1)) {
>                         for(k in (radius + 1):(dx3 - radius -1)) {
>                                 ans <- mean(x[(i-radius):(i+radius),
>                                         (j-radius):(j+radius),
> (k-radius):(k+radius)])
>                         }
>                 }
>         }
>         ans
> }
> 
> The time to run fjj1(jj, 3) on my machine where jj is a
> 245 by 175 by 150 array was 1222 seconds.
> 
> 'fjj2'  substantially reduces the number of sequences
> created.  It took 975 seconds.
> 
> fjj2 <-
> function(x, radius=3)
> {
>         dx <- dim(x)
>         dx1 <- dx[1]
>         dx2 <- dx[2]
>         dx3 <- dx[3]
>         rseq <- -radius:radius
>         for(i in (radius + 1):(dx1 - radius - 1)) {
>                 for(j in (radius + 1):(dx2 - radius - 1)) {
>                         for(k in (radius + 1):(dx3 - radius -1)) {
>                                 ans <- mean(x[i + rseq, j + rseq, k + rseq])
>                         }
>                 }
>         }
>         ans
> }
> 
> 
> 'fjj3' reduces some of the subscripting (but possibly at the
> expense of using more memory -- I'm not sure if it does or
> not).  It took 936 seconds.
> 
> fjj3 <-
> function(x, radius=3)
> {
>         dx <- dim(x)
>         dx1 <- dx[1]
>         dx2 <- dx[2]
>         dx3 <- dx[3]
>         rseq <- -radius:radius
>         for(i in (radius + 1):(dx1 - radius - 1)) {
>                 A <- x[i + rseq, , , drop=FALSE]
>                 for(j in (radius + 1):(dx2 - radius - 1)) {
>                         B <- A[, j + rseq, , drop=FALSE]
>                         for(k in (radius + 1):(dx3 - radius -1)) {
>                                 ans <- mean(B[ , , k + rseq])
>                         }
>                 }
>         }
>         ans
> }
> 
> 'fjj4' reverses the order of the loops.  Because of the
> way that arrays are stored, it makes sense that subscripting
> a sequence in the first dimension would be faster than
> subscripting subsequent dimensions.  This does seem to be
> the case.  'fjj4' took 839 seconds.
> 
> fjj4 <-
> function(x, radius=3)
> {
>         dx <- dim(x)
>         dx1 <- dx[1]
>         dx2 <- dx[2]
>         dx3 <- dx[3]
>         rseq <- -radius:radius
>         for(i in (radius + 1):(dx3 - radius - 1)) {
>                 A <- x[, ,i + rseq, drop=FALSE]
>                 for(j in (radius + 1):(dx2 - radius - 1)) {
>                         B <- A[, j + rseq, , drop=FALSE]
>                         for(k in (radius + 1):(dx1 - radius -1)) {
>                                 ans <- mean(B[k + rseq, , ])
>                         }
>                 }
>         }
>         ans
> }
> 
> 
> Another change that would make a marginal difference
> would be to generate the sequences controlling the inner
> loops once at the outset.
> 
> If the computation at the heart of the function really is a
> mean or something similar, then it is possible that there
> will be tricks to update that value more efficiently.
> 
> Finally, if this will be used enough that the speed is an
> issue, then rewriting it in C would be a good approach.
> 
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Swidan, Firas wrote:
> 
>> Hi,
>> 
>> I am having a problem with a very slow indexing and sub-sectioning of a 3d
>> array:
>> 
>>  
>> 
>>> dim(arr)
>>>    
>>> 
>> [1] 245 175 150
>> 
>> For each point in the array, I am trying to calculate the mean of the values
>> in its surrounding:
>> 
>> 
>> mean( arr[ (i - radius):(i + radius),
>>                                (j - radius):(j + radius),
>>                                (k - radius):(k + radius)] )
>> 
>> Putting that code in 3 for loops
>> 
>> calculateKMedian <- function( arr, radius){
>> 
>>  for( i in (radius + 1):(dim(arr)[1] - radius - 1) ){
>>    for( j in (radius + 1):(dim(arr)[2] - radius - 1) )
>>      for( k in (radius + 1):(dim(arr)[3] - radius - 1) ){
>> 
>> 
>>        mediansArr <- mean( arr[ (i - radius):(i + radius),
>>                                (j - radius):(j + radius),
>>                                (k - radius):(k + radius)] )
>> 
>>      }
>>  }
>>  return(mediansArr)
>> }
>> 
>> Results in a very slow run:
>> 
>>  
>> 
>>> system.time( calculateKMedian( a, 3))
>>>    
>>> 
>> [1] 423.468   0.096 423.631   0.000   0.000
>> 
>> If I replace 
>> 
>>        mediansArr <- mean( arr[ (i - radius):(i + radius),
>>                                (j - radius):(j + radius),
>>                                (k - radius):(k + radius)] )
>> 
>> With an access to the (I,j,k) cell's value
>> 
>> mediansArr <- arr[i,j,k]
>> 
>> The running time decreases to
>> 
>>  
>> 
>>> system.time( calculateKMedian( a, 3))
>>>    
>>> 
>> [1] 14.821  0.005 14.829  0.000  0.000
>> 
>> 
>> 
>> But 14 seconds are still pretty expensive for just scanning the array.
>> 
>> Is there anything I can do to speed the indexing and the sub-sectioning of
>> the 3d array in this case?
>> 
>> Thanks for the help,
>> Firas.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>>  
>>


From erick at nospammail.net  Thu Aug 10 21:08:42 2006
From: erick at nospammail.net (erick at nospammail.net)
Date: Thu, 10 Aug 2006 12:08:42 -0700
Subject: [R] graphic output file format
In-Reply-To: <44DB6618.2050109@uni-erfurt.de>
References: <20060810164524.45102.qmail@web32515.mail.mud.yahoo.com>
	<44DB6618.2050109@uni-erfurt.de>
Message-ID: <1155236922.24212.268104251@webmail.messagingengine.com>


I received the following communication from the package maintainer
 for GDD (Simon Urbanek) when I was having compile problems on linux...

> > gd.x86_64                                2.0.28-4.4E.1          
> > installed       Matched from:
>
> ^^
>  +--- your GD is too old. You'll need at least gd-2.0.29 (see README  
> in GDD) and 2.0.33 is recommended.

> You may want to try Cairo (see the nightly page) instead of GDD,  
> because cairographics has some features that GD is lacking and it is  
> actively maintained (unfortunately libgd is pretty much dead).
> 
> Cheers,
> Simon 

Disclaimer: I actually cannot get the Cairo to work either!
Eric



On Thu, 10 Aug 2006 19:00:08 +0200, "Stefan Grosse"
<stefan.grosse at uni-erfurt.de> said:
> Please mail always also to the list, since it may be that there is
> someone who could reply better and/or faster.

[Case in point.]
> 
> Especially in this case since I am only experienced in M$ Windows and
> Linux so I am unable to guess whats wrong with OSX. does not even the
> postscript device work? With postscript I get the best quality for
> printing.
> 
> e.g.:
> postscript("test.eps", width = 8.0, height = 6.0, horizontal = FALSE,
> onefile =FALSE, paper = "special")
> curve(x^2)
> dev.off()
> 
> 
> Conan Phelan schrieb:
> > Thanks for the feedback and link Stefan,
> >
> > Perhaps if I provide a little more background. (Pls
> > forgive both my ignorance and my long windedness.)
> >
> >  I am on Mac OSX and am only able to load the Quartz
> > graphic device (is that normal?). From quartz, I've
> > only been able to generate pdf files, which do not
> > match the quartz display to my satisfaction. I've been
> > attempting to find a way to generate other output
> > files for comparison. (I've managed to generate a bmp
> > file that looked awful, after acquiring Gostscript). 
> >
> > Anyway, I figure R must able to produce high quality
> > graphic outputs, so I'm trying different things. To
> > this end I was hoping that the GDD package would allow
> > me to load the x11 module but it seems to be a no-go
> > (is there some system requirement I'm missing?).
> >
> > Conan
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From laksh004 at tc.umn.edu  Thu Aug 10 21:10:08 2006
From: laksh004 at tc.umn.edu (Kamakshi)
Date: Thu, 10 Aug 2006 14:10:08 -0500
Subject: [R] QUestion on prediction of class from rpart
Message-ID: <44DB8490.2010509@tc.umn.edu>

Hello,

I am trying to predict the classes of  a test data set after training an 
rpart tree.
When I run:
predict(rpart_object_based_on_training_data, newdata = "testdata", type 
= "class", na.action = na.pass)
I get an error message saying that a variable that is present in both 
training and test data sets has new
levels in the test set. This is true that there are new levels for some 
of the variables in the test set, although,
the variables themselves are identical in both. My understanding from 
reading the documentation on
predict.rpart is that if one of the facor-variables does have new levels 
in the test set, it is passed through the tree and
is left at the deepest possible node. I tried to run predict.rpart 
directly but it says "function not found". Does this have
to be installed separately? I have loaded the rpart library to run the 
training data. I have not
found this exact situation in the Archives.

Thankyou
Kamakshi

--
laksh004 at umn.edu


From luizrodrigotozzi at gmail.com  Thu Aug 10 21:21:56 2006
From: luizrodrigotozzi at gmail.com (Luiz Rodrigo Tozzi)
Date: Thu, 10 Aug 2006 16:21:56 -0300
Subject: [R] graphic output file format
In-Reply-To: <1155236922.24212.268104251@webmail.messagingengine.com>
References: <20060810164524.45102.qmail@web32515.mail.mud.yahoo.com>
	<44DB6618.2050109@uni-erfurt.de>
	<1155236922.24212.268104251@webmail.messagingengine.com>
Message-ID: <22175d30608101221t56c2aa99p78b5c1af82e851b5@mail.gmail.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/cf922603/attachment.pl 

From iuri at ufrgs.br  Thu Aug 10 21:29:48 2006
From: iuri at ufrgs.br (Iuri Gavronski)
Date: Thu, 10 Aug 2006 16:29:48 -0300
Subject: [R] Variance Components in R
Message-ID: <20060810162948.k9t7ffyqpy5ssgos@webmail.ufrgs.br>

Hi,

I'm trying to fit a model using variance components in R, but if very  
new on it, so I'm asking for your help.

I have imported the SPSS database onto R, but I don't know how to  
convert the commands... the SPSS commands I'm trying to convert are:
VARCOMP
   RATING BY CHAIN SECTOR RESP ASPECT ITEM
   /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
   /METHOD = MINQUE (1)
   /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
               SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP  
CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
               SECTOR*RESP*ASPECT SECTOR*RESP*ITEM CHAIN*RESP*ASPECT
   /INTERCEPT = INCLUDE.

VARCOMP
   RATING BY CHAIN SECTOR RESP ASPECT ITEM
   /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
   /METHOD = REML
   /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
               SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP  
CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
               SECTOR*RESP*ASPECT SECTOR*RESP*ITEM CHAIN*RESP*ASPECT
   /INTERCEPT = INCLUDE.

Thank you for your help.

Best regards,

Iuri.

_______________________________________
Iuri Gavronski - iuri at ufrgs.br
doutorando
UFRGS/PPGA/NITEC - www.ppga.ufrgs.br
Brazil


From ssj1364 at gmail.com  Thu Aug 10 21:35:10 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Thu, 10 Aug 2006 13:35:10 -0600
Subject: [R] Negatie Binomial Regression: "Warning while fitting theta:
	alternation limit reached"
Message-ID: <1c6126db0608101235s2fbf7ffdva1279bc5b370199@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/d149909b/attachment.pl 

From mmanfrin at ulb.ac.be  Thu Aug 10 22:10:53 2006
From: mmanfrin at ulb.ac.be (Max Manfrin)
Date: Thu, 10 Aug 2006 22:10:53 +0200
Subject: [R] How to speed up nested for loop computations
In-Reply-To: <644e1f320608100946r2dc8277dv38f2f4645eca7d18@mail.gmail.com>
References: <A07FC905-D0C3-4B76-B22F-258035727FA6@ulb.ac.be>
	<644e1f320608100946r2dc8277dv38f2f4645eca7d18@mail.gmail.com>
Message-ID: <14A0D8CF-9AFA-42EB-8AE6-A87BC06FA6D2@ulb.ac.be>

On 10 Aug 2006, at 18:46, jim holtman wrote:

> It appears that you are trying to partition the dataframe and then  
> do some operations.  It is probably better to use 'split' to  
> generate the set of indices of the partitions and then do the  
> operations on the subset.  Here is an example that calculate the  
> 'mean' of each partition:
>
> > n <- 20
> > x <- data.frame(id=sample(1:3,n,TRUE), type=sample(1:3,n,TRUE),  
> value=runif(n))
> > x.split <- split(1:nrow(x), list(x$id, x$type), drop=TRUE)
> > x.split
> $`3.1`
> [1]  1 15 19
>
> $`1.1`
> [1] 2
... cut ...

> > # calculate the number of values in the partition and their mean
>
> > lapply(x.split, function(z) c(length(z),mean(x$value[z])))
> $`3.1`
> [1] 3.0000000 0.3120459
>
> $`1.1`
> [1] 1.0000000 0.5642638
... cut ...
> You should be able to extend this approach to your data.

I tried to follow your suggestion. I indeed have to partition the  
data frame: my complete set of data contains for each problem  
instance ("instance") of a given size (the number of instances of a  
given size in the example is 2), for each search algorithm ("idalgo")  
(the number of algorithm I'm testing is 78), for each trial ("try")  
(I test each algorithm on each instance 30 times) all the best-so-far  
solutions value ("best") found by every CPU (my parallel algorithm  
runs on 8 CPU) during the duration of the search.

I therefore applied to the res data frame the command
 >res.split <- split(res, list(res$instance, res$try, res$idalgo),  
drop=TRUE)

For every partition (and I have 4680 partition of the type  
instance.try.idalgo) I need to identify the best solution found (so,  
among the 8 CPU I need to identify the one with the lowest value of  
"best"). Unluckly the split command doesn't give me back the indexes  
of the row of res data frame like in your example, but gives me a  
"subset" of the res, so I don't know how to write the lapply function  
to return the indexes of the rows in res containing the minimum value  
of best for the partitions.


I here give an example with a subset of the data:

 > optimal_values<-read.table("optimal_values_80.txt",header=TRUE)
 > resPIR2OPT<-read.table("parallel_independent_2- 
opt_80_800.txt",header=TRUE)
 > resSEQ2OPT<-read.table("sequential_2-opt_80_6400.txt",header=TRUE)
 > resSEQ22OPT<-read.table("sequential2_2-opt_80_800.txt",header=TRUE)
 >
 > res<-rbind(resPIR2OPT,resSEQ2OPT,resSEQ22OPT)
 > str(res)
`data.frame':	14774 obs. of  11 variables:
$ idalgo   : Factor w/ 3 levels "PIR-2opt","SEQ-2opt",..: 1 1 1 1 1 1  
1 1 1 1 ...
$ topo     : Factor w/ 3 levels "PIR","SEQ","SEQ2": 1 1 1 1 1 1 1 1 1  
1 ...
$ schema   : Factor w/ 3 levels "PIR","SEQ","SEQ2": 1 1 1 1 1 1 1 1 1  
1 ...
$ ls       : int  2 2 2 2 2 2 2 2 2 2 ...
$ type     : Factor w/ 2 levels "Par","Seq": 1 1 1 1 1 1 1 1 1 1 ...
$ cpu_id   : int  0 0 0 0 0 0 0 0 0 0 ...
$ instance : Factor w/ 2 levels "lipa80a","tai80a": 1 1 1 1 1 1 1 1 1  
1 ...
$ try      : int  1 1 1 1 1 1 1 1 1 1 ...
$ best     : int  255289 255250 255209 255112 254991 254971 254969  
254897 254893 254892 ...
$ time     : num  0.09 0.09 0.09 0.19 1.16 1.49 1.55 1.72 1.78 1.93 ...
$ iteration: int  1 1 1 2 13 18 19 22 23 26 ...
 > res.split <- split(res, list(res$instance, res$try, res$idalgo),  
drop=TRUE)
 > str(res.split)
List of 180
$ lipa80a.1.PIR-2opt  :`data.frame':	184 obs. of  11 variables:
   ..$ idalgo   : Factor w/ 3 levels "PIR-2opt","SEQ-2opt",..: 1 1 1  
1 1 1 1 1 1 1 ...
   ..$ topo     : Factor w/ 3 levels "PIR","SEQ","SEQ2": 1 1 1 1 1 1  
1 1 1 1 ...
   ..$ schema   : Factor w/ 3 levels "PIR","SEQ","SEQ2": 1 1 1 1 1 1  
1 1 1 1 ...
   ..$ ls       : int [1:184] 2 2 2 2 2 2 2 2 2 2 ...
   ..$ type     : Factor w/ 2 levels "Par","Seq": 1 1 1 1 1 1 1 1 1  
1 ...
   ..$ cpu_id   : int [1:184] 0 0 0 0 0 0 0 0 0 0 ...
   ..$ instance : Factor w/ 2 levels "lipa80a","tai80a": 1 1 1 1 1 1  
1 1 1 1 ...
   ..$ try      : int [1:184] 1 1 1 1 1 1 1 1 1 1 ...
   ..$ best     : int [1:184] 255289 255250 255209 255112 254991  
254971 254969 254897 254893 254892 ...
   ..$ time     : num [1:184] 0.09 0.09 0.09 0.19 1.16 1.49 1.55 1.72  
1.78 1.93 ...
   ..$ iteration: int [1:184] 1 1 1 2 13 18 19 22 23 26 ...
$ lipa80a.2.PIR-2opt  :`data.frame':	230 obs. of  11 variables:
   ..$ idalgo   : Factor w/ 3 levels "PIR-2opt","SEQ-2opt",..: 1 1 1  
1 1 1 1 1 1 1 ...
   ..$ topo     : Factor w/ 3 levels "PIR","SEQ","SEQ2": 1 1 1 1 1 1  
1 1 1 1 ...
   ..$ schema   : Factor w/ 3 levels "PIR","SEQ","SEQ2": 1 1 1 1 1 1  
1 1 1 1 ...
   ..$ ls       : int [1:230] 2 2 2 2 2 2 2 2 2 2 ...
   ..$ type     : Factor w/ 2 levels "Par","Seq": 1 1 1 1 1 1 1 1 1  
1 ...
   ..$ cpu_id   : int [1:230] 0 0 0 0 0 0 0 0 0 0 ...
   ..$ instance : Factor w/ 2 levels "lipa80a","tai80a": 1 1 1 1 1 1  
1 1 1 1 ...
   ..$ try      : int [1:230] 2 2 2 2 2 2 2 2 2 2 ...
   ..$ best     : int [1:230] 255557 255264 255235 255201 255193  
255192 255186 255103 254990 254971 ...
   ..$ time     : num [1:230] 0.09 0.09 0.19 0.19 0.37 1.29 1.36 1.36  
1.58 1.89 ...
   ..$ iteration: int [1:230] 1 1 2 2 4 15 16 16 19 24 ...


My question now is: how do I extract from each partition the row with  
the minimal best value? I need to boxplot them.

Thanks again in advance for any help anybody could give.

----
Max MANFRIN
http://iridia.ulb.ac.be/~mmanfrin/


-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 194 bytes
Desc: This is a digitally signed message part
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060810/40afd2e7/attachment.bin 

From francoisromain at free.fr  Thu Aug 10 22:25:55 2006
From: francoisromain at free.fr (francoisromain at free.fr)
Date: Thu, 10 Aug 2006 22:25:55 +0200
Subject: [R] OT UNIX grep question
In-Reply-To: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
References: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
Message-ID: <1155241555.44db96532d44a@imp2-g19.free.fr>

Selon Rolf Turner <rolf at erdos.math.unb.ca>:

> francoisromain at free.fr wrote:
>
> > You have to learn about regular expressions. Then you'll come up with
> > something like :
> >
> > grep "^dog$" /usr/share/dict/words
>
> *You* have to learn about shell syntax.  The foregoing doesn't
> work; it gives an ``Illegal variable name.'' error.  To protect
> against the shell interpretation of the dollar sign you have
> to use *single* quotes.
>
> 		grep '^dog$' /usr/share/dict/words
>
> *does* work.  (Try it!)
>

Hi,

Sorry for inconvenience, both are working on my fedora gnu/linux bash, Jan gave
the explanation (Thanks!).

BTW, did you really think I didn't try the double quote call ?
But do not hesitate if **you** see something **I** should learn.

Cheers,

Romain


From francoisromain at free.fr  Thu Aug 10 22:32:05 2006
From: francoisromain at free.fr (francoisromain at free.fr)
Date: Thu, 10 Aug 2006 22:32:05 +0200
Subject: [R] OT UNIX grep question
In-Reply-To: <9bcdfad70608100516w1930b430i81e9ce7c452700d8@mail.gmail.com>
References: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
	<9bcdfad70608100516w1930b430i81e9ce7c452700d8@mail.gmail.com>
Message-ID: <1155241925.44db97c5eef66@imp2-g19.free.fr>

Selon Chris wallace <crassshed at gmail.com>:

> On 10/08/06, Rolf Turner <rolf at erdos.math.unb.ca> wrote:
> >
> > francoisromain at free.fr wrote:
> >
> >                 grep '^dog$' /usr/share/dict/words
> >
> >
> or (simpler, in my view)
>
>                    grep -w dog /usr/share/dict/words
>
> Chris.
>


Well, for the record it's does not work with my settings.
Maybe *Mr Turner* can give you a lesson as well. Sorry I'm just in the mood for
a joke ...

Romain

$ grep -w dog /usr/share/dict/words
bird-dog
bull-dog
cat-and-dog
dog
dog-banner
....
water-dog
wolf-dog
yellow-dog


From andy_liaw at merck.com  Thu Aug 10 22:34:19 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Aug 2006 16:34:19 -0400
Subject: [R] Multiple density curves
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2C5E@usctmx1106.merck.com>

Assuming "dat" is the name of the data frame, this should get you started:

library(lattice)
histogram(~ unlist(dat[-1]) | factor(rep(c("A1", "A2"), each=nrow(dat))),
          panel=function(...) {
              panel.histogram(...)
              panel.densityplot(...)
          }, type="density")

Andy

From: Davendra Sohal
> 
> Hi,
> 
> I am new to R...a recent convert from SAS.
> I have a dataset that looks like this:
> 
> SEQ    A1    A2
> A    532.5    554.5
> B    25.5    35.5
> C    265.2    522.2
> D    245.55    521.56
> E    546.52    141.52
> F    243.25    32.56
> G    452.55    635.56
> H    15.14    16.54
> I    543.4    646.56
> J    54.4    654.5
> K    646.5    64.54
> L    645.4    614.46
> M    646.54    634.46
> 
> I want to make a histogram each for A1 and A2, with density 
> curves, on the same plot so that I can see how they overlap.
> 
> Please let me know some simple code for this.
> 
> I looked at ldahist but it was complicated. Anything simpler?
> 
> Thanks a lot,
> -DS.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From crassshed at gmail.com  Thu Aug 10 22:37:20 2006
From: crassshed at gmail.com (Chris wallace)
Date: Thu, 10 Aug 2006 20:37:20 +0000
Subject: [R] OT UNIX grep question
In-Reply-To: <1155241925.44db97c5eef66@imp2-g19.free.fr>
References: <200608101151.k7ABpaYe004374@erdos.math.unb.ca>
	<9bcdfad70608100516w1930b430i81e9ce7c452700d8@mail.gmail.com>
	<1155241925.44db97c5eef66@imp2-g19.free.fr>
Message-ID: <9bcdfad70608101337i4e3af872neb04d27cb62f9097@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/84a09e49/attachment.pl 

From ruser2006 at yahoo.com  Fri Aug 11 00:17:18 2006
From: ruser2006 at yahoo.com (r user)
Date: Thu, 10 Aug 2006 15:17:18 -0700 (PDT)
Subject: [R] basic question re lm()
Message-ID: <20060810221718.17610.qmail@web37008.mail.mud.yahoo.com>

I am using R in a Windows environment.

I have a basic question regarding lm().

I have a dataframe ?data1? with ncol=w.

I know that my dependent variable is in column1.

Is there a way to write the regression formula so that
I can use columns 2 thru w as my independent
variables?



e.g. something like:  ? lm(data1[,1] ~ data1[,2:w] ) ?

Thanks


From bruno.giordano at music.mcgill.ca  Fri Aug 11 00:18:22 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Thu, 10 Aug 2006 18:18:22 -0400
Subject: [R]  logistic discrimination: which chance performance??
Message-ID: <001601c6bcca$edda4c00$6400a8c0@brungio>

Hello,
I am using logistic discriminant analysis to check whether a known 
classification Yobs can be predicted by few continuous variables X.

What I do is to predict class probabilities with multinom() in nnet(), 
obtaining a predicted classification Ypred and then compute the percentage 
P(obs) of objects classified the same in Yobs and Ypred.

My problem now is to figure out whether P(obs) is significantly higher than 
chance.

I opted for a crude permutation approach: compute P(perm) over 10000 random 
permutations of Yobs (i.e., refit the multinom() model 10000 times randomly 
permuting Yobs) and consider P(obs) as significantly higher than chance if 
higher than the 95th percentile of the P(perm) distribution.

Now, the problem is that the mode of P(perm) is always really close to 
P(obs), e.g., if P(obs)=1 (perfect discrimination) also the most likely 
P(perm) value is 1!!!

I figured out that this is due to the fact that, with my data, randomly 
permuted classifications are highly likely to strongly agree with the 
observed classification Yobs, but, probably since my machine learning 
background is almost 0, I am kind of lost about how to proceed at this 
point.

I would greatly appreciate a comment on this.

Thanks
    Bruno

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Bruno L. Giordano, Ph.D.
CIRMMT
Schulich School of Music, McGill University
555 Sherbrooke Street West
Montr?al, QC H3A 1E3
Canada
http://www.music.mcgill.ca/~bruno/


From ggrothendieck at gmail.com  Fri Aug 11 00:33:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Aug 2006 18:33:52 -0400
Subject: [R] basic question re lm()
In-Reply-To: <20060810221718.17610.qmail@web37008.mail.mud.yahoo.com>
References: <20060810221718.17610.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <971536df0608101533o2530e5a6o287ee7bb7802cad2@mail.gmail.com>

Try:

lm(Sepal.Length ~., iris)

On 8/10/06, r user <ruser2006 at yahoo.com> wrote:
> I am using R in a Windows environment.
>
> I have a basic question regarding lm().
>
> I have a dataframe "data1" with ncol=w.
>
> I know that my dependent variable is in column1.
>
> Is there a way to write the regression formula so that
> I can use columns 2 thru w as my independent
> variables?
>
>
>
> e.g. something like:  " lm(data1[,1] ~ data1[,2:w] ) "
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Fri Aug 11 00:40:15 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Aug 2006 18:40:15 -0400
Subject: [R] basic question re lm()  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2C85@usctmx1106.merck.com>

lm(data1) should work just fine.  E.g.,

R> data1 <- data.frame(v1=rnorm(10), v2=rnorm(10), v3=rnorm(10))
R> lm(data1)

Call:
lm(formula = data1)

Coefficients:
(Intercept)           v2           v3  
     0.5746       0.3363      -0.5549  

Andy

From: r user
> 
> I am using R in a Windows environment.
> 
> I have a basic question regarding lm().
> 
> I have a dataframe "data1" with ncol=w.
> 
> I know that my dependent variable is in column1.
> 
> Is there a way to write the regression formula so that I can 
> use columns 2 thru w as my independent variables?
> 
> 
> 
> e.g. something like:  " lm(data1[,1] ~ data1[,2:w] ) "
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From Horace.Tso at pgn.com  Fri Aug 11 00:54:11 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Thu, 10 Aug 2006 15:54:11 -0700
Subject: [R] day, month, year functions
Message-ID: <s4db56b1.019@pgn.com>

Hi list, 

I'm trying to turn a date into something productive. (Not what you may be thinking....) 

I want three functions so I could take a "date" object and get the day of week, month, and year from it.

xx <- as.Date("2006-01-05") 

month(xx) equal 1
day(xx) equal 5
year(xx) equal 2006

I'm aware of the weekdays() and months() functions in the base package. But they return a character object which requires some coding to convert into a numeric value.

I've also tried the sday.of.week() in fCalendar but it doesn't like my date,

> sday.of.week(xx)
Error in Ops.Date(sdates, 10000) : %/% not defined for Date objects

Do these functions exist in some package I'm not aware of?

Thanks in adv.

Horace Tso


From ggrothendieck at gmail.com  Fri Aug 11 01:22:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Aug 2006 19:22:14 -0400
Subject: [R] day, month, year functions
In-Reply-To: <s4db56b1.019@pgn.com>
References: <s4db56b1.019@pgn.com>
Message-ID: <971536df0608101622r42321728w69aa8efedcb3701d@mail.gmail.com>

Here are three ways:

xx <- as.Date("2006-01-05")

# 1. use as.POSIXlt
as.POSIXlt(xx)$mday
as.POSIXlt(xx)$mon + 1
as.POSIXlt(xx)$year + 1900

# 2. use format
as.numeric(format(xx, "%d"))
as.numeric(format(xx, "%m"))
as.numeric(format(xx, "%Y"))

# 3. use month.day.year in chron package
library(chron)
month.day.year(unclass(xx))$day
month.day.year(unclass(xx))$month
month.day.year(unclass(xx))$year

Also see the help desk article in R News 4/1.


On 8/10/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> Hi list,
>
> I'm trying to turn a date into something productive. (Not what you may be thinking....)
>
> I want three functions so I could take a "date" object and get the day of week, month, and year from it.
>
> xx <- as.Date("2006-01-05")
>
> month(xx) equal 1
> day(xx) equal 5
> year(xx) equal 2006
>
> I'm aware of the weekdays() and months() functions in the base package. But they return a character object which requires some coding to convert into a numeric value.
>
> I've also tried the sday.of.week() in fCalendar but it doesn't like my date,
>
> > sday.of.week(xx)
> Error in Ops.Date(sdates, 10000) : %/% not defined for Date objects
>
> Do these functions exist in some package I'm not aware of?
>
> Thanks in adv.
>
> Horace Tso


From blomsp at ozemail.com.au  Fri Aug 11 02:05:58 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Fri, 11 Aug 2006 10:05:58 +1000
Subject: [R] basic question re lm()
In-Reply-To: <20060810221718.17610.qmail@web37008.mail.mud.yahoo.com>
References: <20060810221718.17610.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <44DBC9E6.8010603@ozemail.com.au>

You could look at using lm.fit instead of lm.

Alternatively, you can paste the names of the variables together using 
the following approach. It's a bit baroque, but it works:

form.fn <- function (dframe) {
nms <- names(dframe)
formula(paste(nms[1], "~", paste(nms[2:length(nms)], collapse="+")))
}

Then do:

form <- form.fn(data1)
lm(form, data=data1)

HTH,

Simon.


r user wrote:
> I am using R in a Windows environment.
>
> I have a basic question regarding lm().
>
> I have a dataframe ?data1? with ncol=w.
>
> I know that my dependent variable is in column1.
>
> Is there a way to write the regression formula so that
> I can use columns 2 thru w as my independent
> variables?
>
>
>
> e.g. something like:  ? lm(data1[,1] ~ data1[,2:w] ) ?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From gregor.gorjanc at bfro.uni-lj.si  Fri Aug 11 02:22:31 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 11 Aug 2006 00:22:31 +0000 (UTC)
Subject: [R] mcmc pack
References: <6932247.1155229837565.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
Message-ID: <loom.20060811T021927-139@post.gmane.org>

Mariagiulia Matteucci <matteucci <at> stat.unibo.it> writes:

> 
> Hello, 
> I would like to see the C++ code for the MCMC package, how can I do
> that?
> Thanks,

Hi,

- go to CRAN page i.e. http://cran.r-project.org/ or one of its mirros
- click on packages
- find MCMCpack and click on its link
- on MCMCpack page you will notice several links and one of them is *.tar.gz,
which is a source for the package -> download and unpack that file and you are
in the bussines

Gregor


From gregor.gorjanc at bfro.uni-lj.si  Fri Aug 11 02:27:27 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 11 Aug 2006 00:27:27 +0000 (UTC)
Subject: [R] day, month, year functions
References: <s4db56b1.019@pgn.com>
	<971536df0608101622r42321728w69aa8efedcb3701d@mail.gmail.com>
Message-ID: <loom.20060811T022520-438@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
> 
> Here are three ways:
> 
> xx <- as.Date("2006-01-05")
> 
> # 1. use as.POSIXlt
> as.POSIXlt(xx)$mday
> as.POSIXlt(xx)$mon + 1
> as.POSIXlt(xx)$year + 1900
> 
> # 2. use format
> as.numeric(format(xx, "%d"))
> as.numeric(format(xx, "%m"))
> as.numeric(format(xx, "%Y"))
> 
> # 3. use month.day.year in chron package
> library(chron)
> month.day.year(unclass(xx))$day
> month.day.year(unclass(xx))$month
> month.day.year(unclass(xx))$year

Hi,

it would really be great if there would be

sec(), min(), hour() day(), month(), year()

generic functions that would work on all "date" classes. Where
applicable of course. I imagine that argument to get out integer
or character would alse be nice.

Gregor


From bruno.giordano at music.mcgill.ca  Fri Aug 11 05:07:41 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Thu, 10 Aug 2006 23:07:41 -0400
Subject: [R] logistic discrimination: which chance performance??
References: <001601c6bcca$edda4c00$6400a8c0@brungio>
Message-ID: <006401c6bcf3$5f07cf60$6400a8c0@brungio>

Well,
If posting a possible solution to one's own problem is not part of the
netiquette of this list please correct me.

Following Titus et al. (1984) one might use Cohen's kappa to have a
chance-corrected measure of agreement between the original and reproduced
classification:

Kappa() in library vcd
kappa2() in library irr
ckappa() in library psy
cohen.kappa() in library concord......

    Bruno

Kimberly Titus; James A. Mosher; Byron K. Williams (1984), Chance-corrected 
Classification for Use in Discriminant Analysis: Ecological Applications, 
American Midland Naturalist, 111(1),1-7.


----- Original Message ----- 
From: "Bruno L. Giordano" <bruno.giordano at music.mcgill.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 10, 2006 6:18 PM
Subject: [R] logistic discrimination: which chance performance??


> Hello,
> I am using logistic discriminant analysis to check whether a known
> classification Yobs can be predicted by few continuous variables X.
>
> What I do is to predict class probabilities with multinom() in nnet(),
> obtaining a predicted classification Ypred and then compute the percentage
> P(obs) of objects classified the same in Yobs and Ypred.
>
> My problem now is to figure out whether P(obs) is significantly higher
> than
> chance.
>
> I opted for a crude permutation approach: compute P(perm) over 10000
> random
> permutations of Yobs (i.e., refit the multinom() model 10000 times
> randomly
> permuting Yobs) and consider P(obs) as significantly higher than chance if
> higher than the 95th percentile of the P(perm) distribution.
>
> Now, the problem is that the mode of P(perm) is always really close to
> P(obs), e.g., if P(obs)=1 (perfect discrimination) also the most likely
> P(perm) value is 1!!!
>
> I figured out that this is due to the fact that, with my data, randomly
> permuted classifications are highly likely to strongly agree with the
> observed classification Yobs, but, probably since my machine learning
> background is almost 0, I am kind of lost about how to proceed at this
> point.
>
> I would greatly appreciate a comment on this.
>
> Thanks
>    Bruno
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Bruno L. Giordano, Ph.D.
> CIRMMT
> Schulich School of Music, McGill University
> 555 Sherbrooke Street West
> Montr?al, QC H3A 1E3
> Canada
> http://www.music.mcgill.ca/~bruno/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zhiliang.ma at gmail.com  Fri Aug 11 05:27:02 2006
From: zhiliang.ma at gmail.com (Zhiliang Ma)
Date: Thu, 10 Aug 2006 23:27:02 -0400
Subject: [R] Read in vtk data
Message-ID: <b39377d10608102027k36cd48a3l2917344de0a71ac6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060810/505198f2/attachment.pl 

From adik at ilovebacon.org  Fri Aug 11 08:02:02 2006
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Thu, 10 Aug 2006 23:02:02 -0700 (PDT)
Subject: [R] Pairwise n for large correlation tables?
In-Reply-To: <mailman.9.1155031203.15896.r-help@stat.math.ethz.ch>
References: <mailman.9.1155031203.15896.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0608102255350.4867@parser.ilovebacon.org>


On Tue, 8 Aug 2006, ggrothendieck at gmail.com wrote:

> Try this:
>
> # mat is test matrix
> mat <- matrix(1:25, 5)
> mat[2,2] <- mat[3,4] <- NA
> crossprod(!is.na(mat))

Exactly what I was looking for! Thanks.

--Adam

>
>
> On 8/7/06, Adam D. I. Kramer <adik at ilovebacon.org> wrote:
>> Hello,
>>
>> I'm using a very large data set (n > 100,000 for 7 columns), for which I'm
>> pretty happy dealing with pairwise-deleted correlations to populate my
>> correlation table. E.g.,
>>
>> a <- cor(cbind(col1, col2, col3),use="pairwise.complete.obs")
>>
>> ...however, I am interested in the number of cases used to compute each
>> cell of the correlation table. I am unable to find such a function via
>> google searches, so I wrote one of my own. This turns out to be highly
>> inefficient (e.g., it takes much, MUCH longer than the correlations do). Any
>> hints, regarding other functions to use or ways to maket his speedier, would
>> be much appreciated!
>>
>> pairwise.n <- function(df=stop("Must provide data frame!")) {
>>   if (!is.data.frame(df)) {
>>     df <- as.data.frame(df)
>>   }
>>   colNum <- ncol(df)
>>   result <- matrix(data=NA,nrow=colNum,ncol=ncolNum,dimnames=list(colnames(df),colnames(df)))
>>   for(i in 1:colNum) {
>>     for (j in i:colNum) {
>>       result[i,j] <- length(df[!is.na(df[i])&!is.na(df[j])])/colNum
>>     }
>>   }
>>   result
>> }
>>
>> --
>> Adam D. I. Kramer
>> University of Oregon


From christian.montel at eligo.de  Fri Aug 11 08:35:27 2006
From: christian.montel at eligo.de (Christian Montel)
Date: Fri, 11 Aug 2006 08:35:27 +0200
Subject: [R]  - factanal scores correlated?
Message-ID: <44DC252F.9040507@eligo.de>

Hi,

I wonder why factor scores produced by factanal are correlated, and I'd 
appreciate any hints from people that may help me to get a deeper 
understanding why that's the case. By the way: I'm a psychologist used 
to SPSS, so that question my sound a little silly to your ears.

Here's my minimal example:

***********************************************
      v1 <- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
      v2 <- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
      v3 <- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
      v4 <- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
      v5 <- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
      v6 <- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)
      m1 <- cbind(v1,v2,v3,v4,v5,v6)
      myfac <- factanal(m1, factors=3, scores="regression")#
      cor(myfac$scores)
***********************************************

Tells me
             Factor1     Factor2     Factor3
Factor1 1.000000000 0.001624383 0.002862785
Factor2 0.001624383 1.000000000 0.001956953
Factor3 0.002862785 0.001956953 1.000000000

which means that factor correlations are indeed quite low with regard to 
interpretation issues, but an analysis of a larger dataset yielded 
factor intercorrelations up to .10.

I guess this is an optimization issue because a lower setting of "lower" 
tends to lower factor intercorrelations, but I'm still confused because 
I (misleadingly?) thought that factor scores are (completely) 
independent by definition?

Any hints would be greatly appreciated,

best regards,

Christian


-- 
--------------------------
Dr. Christian Montel
eligo GmbH -- B?ro Berlin
Arndtstr. 34
10965 Berlin
Tel. 030 -- 69 00 11 42
Fax  030 -- 69 00 47 61
christian.montel at eligo.de
http://www.eligo.de/


From ripley at stats.ox.ac.uk  Fri Aug 11 09:17:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Aug 2006 08:17:03 +0100 (BST)
Subject: [R] extractAIC using surf.ls
In-Reply-To: <loom.20060810T093236-494@post.gmane.org>
References: <DE508AFC-D7C8-412F-9C1F-FA7DB49B164F@leeds.ac.uk>
	<loom.20060810T093236-494@post.gmane.org>
Message-ID: <Pine.LNX.4.64.0608110811320.12748@gannet.stats.ox.ac.uk>

Roger, thank you for looking into this.

However, the posting guide asked the poster to contact the maintainer. Had 
(s)he done so, I would have pointed out that spatial 7.28-2 (the current 
version for R-devel) has this corrected (in a slightly simpler way).

On Thu, 10 Aug 2006, Roger Bivand wrote:

> Yan Wong <h.y.wong <at> leeds.ac.uk> writes:
> 
> > 
> > Although the 'spatial' documentation doesn't mention that extractAIC  
> > works, it does seem to give an output.
> 
> Could I suggest moving this question to the R-sig-geo list?
> 
> Please note that surf.ls() converts x and y to the [-1, +1] range to ensure that
> higher powers of possibly very large absolute coordinate values do not cause
> trouble, so that the surf.ls() and lm() models may differ anyway. 
> 
> I believe that there is a bug in extractAIC.trls() - which I contributed to the
> spatial package some years ago, with edf <- df.residual.trls(fit) rather than n
> - df.residual.trls(fit). When this is corrected, for this case, the extractAIC()
> results agree.
> 
> Roger Bivand
> 
> > I may have misunderstood, but shouldn't the following give at least  
> > the same d.f.?
> > 
> >  > library(spatial)
> >  > data(topo, package="MASS")
> >  > extractAIC(surf.ls(2, topo))
> > [1]  46.0000 437.5059
> >  > extractAIC(lm(z ~ x+I(x^2)+y+I(y^2)+x:y, topo))
> > [1]   6.0000 357.5059
> > 
> > # and if the AIC values differ, shouldn't they do so by an additive  
> > constant?
> > 
> >  > (extractAIC(surf.ls(2, topo))-extractAIC(lm(z ~ x+I(x^2)+y+I(y^2) 
> > +x:y, topo)))[2]
> > [1] 80
> >  > (extractAIC(surf.ls(1, topo))-extractAIC(lm(z ~ x+y, topo)))[2]
> > [1] 92
> > 
> > # Using R 2.3.1 (OS X), spatial version 7.2-27.1
> > 
> > Thanks
> > 
> > Yan

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From YCH at softcomputing.com  Fri Aug 11 09:31:48 2006
From: YCH at softcomputing.com (Yohan CHOUKROUN)
Date: Fri, 11 Aug 2006 09:31:48 +0200
Subject: [R] RE :  tcltk library on linux
Message-ID: <C8F48FD780E12D4DB197507B897D00DD072F5B20@ntexch.softcomputing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/987474e5/attachment.pl 

From matteucci at stat.unibo.it  Fri Aug 11 10:32:41 2006
From: matteucci at stat.unibo.it (Mariagiulia Matteucci)
Date: Fri, 11 Aug 2006 10:32:41 +0200 (CEST)
Subject: [R] about MCMC pack again...
Message-ID: <7283970.1155285161788.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Hello, thank you very much for your previous answers about the C++ code.
I am interested in the application of the Gibbs Sampler in the IRT
models, so in the function MCMCirt1d and MCMCirtkd. I've found the C++
source codes, as you suggested, but I cannot find anything about the
Gibbs Sampler. All the files are for the Metropolis algorithm.
Maybe I am not able to read them very well, by the way my problem is
principally about missing data. Do you know how they deal with them? I
cannot find any explanation, only that I can have some missing data. Do
they skip the missing data or do they code them by "0"? And what about 2
types of missing data: not presented items and not answered items?

Thank you for reply,

Mariagiulia 


Mariagiulia Matteucci
Dipartimento di Scienze Statistiche ?Paolo Fortunati?
Universit? di Bologna
Via Belle Arti, 41 
40126 Bologna (ITALY)
e-mail: matteucci at stat.unibo.it
TEL: +39 051 264182
FAX: +39 051 232153


From ripley at stats.ox.ac.uk  Fri Aug 11 10:42:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Aug 2006 09:42:01 +0100 (BST)
Subject: [R] - factanal scores correlated?
In-Reply-To: <44DC252F.9040507@eligo.de>
References: <44DC252F.9040507@eligo.de>
Message-ID: <Pine.LNX.4.64.0608110939470.18181@gannet.stats.ox.ac.uk>

This is a Heywood case, and you don't have a valid fit:

> myfac

Call:
factanal(x = m1, factors = 3, scores = "regression")

Uniquenesses:
   v1    v2    v3    v4    v5    v6 
0.005 0.101 0.005 0.224 0.084 0.005 

notice no less than 3 very small uniquenesses.


On Fri, 11 Aug 2006, Christian Montel wrote:

> Hi,
> 
> I wonder why factor scores produced by factanal are correlated, and I'd 
> appreciate any hints from people that may help me to get a deeper 
> understanding why that's the case. By the way: I'm a psychologist used 
> to SPSS, so that question my sound a little silly to your ears.
> 
> Here's my minimal example:
> 
> ***********************************************
>       v1 <- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
>       v2 <- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
>       v3 <- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
>       v4 <- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
>       v5 <- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
>       v6 <- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)
>       m1 <- cbind(v1,v2,v3,v4,v5,v6)
>       myfac <- factanal(m1, factors=3, scores="regression")#
>       cor(myfac$scores)
> ***********************************************
> 
> Tells me
>              Factor1     Factor2     Factor3
> Factor1 1.000000000 0.001624383 0.002862785
> Factor2 0.001624383 1.000000000 0.001956953
> Factor3 0.002862785 0.001956953 1.000000000
> 
> which means that factor correlations are indeed quite low with regard to 
> interpretation issues, but an analysis of a larger dataset yielded 
> factor intercorrelations up to .10.
> 
> I guess this is an optimization issue because a lower setting of "lower" 
> tends to lower factor intercorrelations, but I'm still confused because 
> I (misleadingly?) thought that factor scores are (completely) 
> independent by definition?
> 
> Any hints would be greatly appreciated,
> 
> best regards,
> 
> Christian
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From B.Rowlingson at lancaster.ac.uk  Fri Aug 11 10:51:12 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 11 Aug 2006 09:51:12 +0100
Subject: [R] about MCMC pack again...
In-Reply-To: <7283970.1155285161788.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
References: <7283970.1155285161788.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
Message-ID: <44DC4500.4070403@lancaster.ac.uk>

Mariagiulia Matteucci wrote:
> Hello, thank you very much for your previous answers about the C++ code.
> I am interested in the application of the Gibbs Sampler in the IRT
> models, so in the function MCMCirt1d and MCMCirtkd. I've found the C++
> source codes, as you suggested, but I cannot find anything about the
> Gibbs Sampler. All the files are for the Metropolis algorithm.

  $ cd MCMCpack/
  $ grep -ir gibbs .

produces loads of output, including:

./src/MCMCfactanal.cc:    } // end Gibbs loop
./src/MCMChierEI.cc:// and slice sampling and Gibbs sampling to sample 
from the posterior
./src/MCMCirt1d.cc:    } // end Gibbs loop
./src/MCMCmixfactanal.cc:  // Gibbs Sampler //
./src/MCMCmixfactanal.cc:  } // end Gibbs loop
./src/MCMCoprobit.cc:    // Gibbs loop
./src/MCMCordfactanal.cc:  // Gibbs Sampler //
./src/MCMCpanel.cc:// simulate from posterior density and return a Gibbs 
by parameters matrix
./src/MCMCpanel.cc:         const int* burnin, const int* gibbs,  const 
int* thin,
./src/MCMCpanel.cc:   int Mgibbs = gibbs[0];
./src/MCMCpanel.cc:   int Mtotiter = Mburnin + Mgibbs;
./src/MCMCpanel.cc:   Matrix<double> beta_holder(Mgibbs/Mthin,Mp);
./src/MCMCpanel.cc:   Matrix<double> D_holder(Mgibbs/Mthin,Mq*Mq);
./src/MCMCpanel.cc:   Matrix<double> sigma2_holder(Mgibbs/Mthin, 1);
./src/MCMCpanel.cc:   // gibbs loop
./src/MCMCregress.cc:     // Gibbs sampler
./src/MCMCregress.cc:       // second set of Gibbs scans
./src/MCMCSVDreg.cc:    /////////////////// Gibbs sampler 
///////////////////

  Perhaps some of these are useful?

  For your info, I know nothing about MCMCpack, I just know how to use 
grep to search for things. If you are on Windows, you can probably use 
the Windows File Explorer Search option to look for it. But give me grep 
anyday...

Barry


From h.y.wong at leeds.ac.uk  Fri Aug 11 10:55:35 2006
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 11 Aug 2006 09:55:35 +0100
Subject: [R] extractAIC using surf.ls
In-Reply-To: <Pine.LNX.4.64.0608110811320.12748@gannet.stats.ox.ac.uk>
References: <DE508AFC-D7C8-412F-9C1F-FA7DB49B164F@leeds.ac.uk>
	<loom.20060810T093236-494@post.gmane.org>
	<Pine.LNX.4.64.0608110811320.12748@gannet.stats.ox.ac.uk>
Message-ID: <BD7AB6A1-096F-44A6-A515-6539C912B407@leeds.ac.uk>


On 11 Aug 2006, at 08:17, Prof Brian Ripley wrote:

> Roger, thank you for looking into this.

Yes, and thanks to both of you for the corrections.

> However, the posting guide asked the poster to contact the  
> maintainer. Had
> (s)he done so, I would have pointed out that spatial 7.28-2 (the  
> current
> version for R-devel) has this corrected (in a slightly simpler way).

Thanks. There have been a few times when what I thought was a bug was  
due to a misunderstanding on my part. It seems better to me to check  
on the R-help list that it really is a bug before bothering the  
maintainers.

On that note, I see that Prof. Ripley is the author of the loess  
package. When trying to adjust some of the control parameters for a  
loess fit, I tried the following (wrong) incantation.

loess(dist ~ speed, cars, control = list(statistics = "exact"))

Although it is wrong (should be control = loess.control(statistics =  
"exact")), entering this as a command actually crashes R on the 2  
systems I have tried (OS X, linux, both R-2.3.1), with the error  
below. I'm not sure if you consider this a bug, as the command I  
typed was invalid.

-------------------

  *** caught segfault ***
address (nil), cause 'memory not mapped'

Traceback:
1: .C(R_loess_raw, as.double(y), as.double(x), as.double 
(weights),     as.double(robust), as.integer(D), as.integer(N),  
as.double(span),     as.integer(degree), as.integer(nonparametric),  
as.integer(order.drop.sqr),     as.integer(sum.drop.sqr), as.double 
(span * cell), as.character(surf.stat),     fitted.values = double 
(N), parameter = integer(7), a = integer(max.kd),     xi = double 
(max.kd), vert = double(2 * D), vval = double((D +         1) *  
max.kd), diagonal = double(N), trL = double(1),     delta1 = double 
(1), delta2 = double(1), as.integer(surf.stat ==         "interpolate/ 
exact"))
2: simpleLoess(y, x, w, span, degree, parametric, drop.square,  
normalize,     control$statistics, control$surface, control$cell,  
iterations,     control$trace.hat)
3: loess(dist ~ speed, cars, control = list(statistics = "exact"))

Possible actions:
1: abort (with core dump)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace


From S.Pickett at exeter.ac.uk  Fri Aug 11 11:33:46 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Fri, 11 Aug 2006 10:33:46 +0100 (BST)
Subject: [R] help: convert lmer.coef to matrix
Message-ID: <1075.144.173.76.117.1155288826.squirrel@www.webmail.ex.ac.uk>

Hi all,
I am trying to coerce the coeficients from a REML using lmer() to a matrix
of numbers which I can then write into excel. I have looked in the archive
and read around in the (Matrix) documentation but havent found anything of
use.
Any suggestions much appreciated,
Thankyou, S.



Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From i.m.s.white at ed.ac.uk  Fri Aug 11 10:43:04 2006
From: i.m.s.white at ed.ac.uk (i.m.s.white)
Date: Fri, 11 Aug 2006 09:43:04 +0100
Subject: [R] Linear Trend in Residuals From lme
In-Reply-To: <1155152592.3424.8.camel@localhost.localdomain>
References: <1155152592.3424.8.camel@localhost.localdomain>
Message-ID: <20060811084303.GB10743@trotter.cap.ed.ac.uk>

As an extreme example of this sort of thing, consider

fit <- lme(y ~ 1, random = ~ 1 | group)

where there is exactly one observation per group, so that it is not possible to
get separate estimates of group and residual variances.  Despite this, lme
often (always?) provides a solution consistent with the data. Because of the
singularity, the plot of residuals against fitted values for this solution
shows a straight line.  This is easily recognized as an aberration, but I can
imagine configurations of data (e.g. with most groups having just one
observation and a few with two or more) where the residual vs fitted value plot
might show an apparent trend.



On Wed, Aug 09, 2006 at 03:43:12PM -0400, Rick Bilonick wrote:
> I'm fitting a mixed effects model:
> 
> fit.1 <- lme(y~x,random=~1|id,data=df)
> 
> There are two different observations for each id for both x and y. When
> I use plot(fit.1), there is a strong increasing linear trend in the
> residuals versus the fitted values (with no outliers). This also happens
> if I use random=~x|id. Am I specifying something incorrectly?
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
************************************************
*    I.White                                   *
*    University of Edinburgh                   *
*    Ashworth Laboratories, West Mains Road    *
*    Edinburgh EH9 3JT                         *
*    Fax: 0131 650 6564   Tel: 0131 650 5490   *
*    E-mail: i.m.s.white at ed.ac.uk              *


From matteucci at stat.unibo.it  Fri Aug 11 11:54:39 2006
From: matteucci at stat.unibo.it (Mariagiulia Matteucci)
Date: Fri, 11 Aug 2006 11:54:39 +0200 (CEST)
Subject: [R] about MCMC pack again...
In-Reply-To: <44DC4500.4070403@lancaster.ac.uk>
References: <7283970.1155285161788.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
	<44DC4500.4070403@lancaster.ac.uk>
Message-ID: <4856447.1155290079809.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Hello, I am using Windows, I tried to use th File Search and also the
Windows Grep but I cannot find any file! In the list you showed me there
are some useful , I really don't know how can I find them! I tried in
the R folder, src folder, MCMC pack folder and I dowloaded the .tar file
about MCMC pack where there are the codes, I really don't know what to
do!

Mariagiulia


On Aug 11, 2006 10:51 AM, Barry Rowlingson
<B.Rowlingson at lancaster.ac.uk> wrote:

> Mariagiulia Matteucci wrote:
> > Hello, thank you very much for your previous answers about the C++
> > code.
> > I am interested in the application of the Gibbs Sampler in the IRT
> > models, so in the function MCMCirt1d and MCMCirtkd. I've found the
> > C++
> > source codes, as you suggested, but I cannot find anything about the
> > Gibbs Sampler. All the files are for the Metropolis algorithm.
> 
>   $ cd MCMCpack/
>   $ grep -ir gibbs .
> 
> produces loads of output, including:
> 
> ./src/MCMCfactanal.cc:    } // end Gibbs loop
> ./src/MCMChierEI.cc:// and slice sampling and Gibbs sampling to sample
> from the posterior
> ./src/MCMCirt1d.cc:    } // end Gibbs loop
> ./src/MCMCmixfactanal.cc:  // Gibbs Sampler //
> ./src/MCMCmixfactanal.cc:  } // end Gibbs loop
> ./src/MCMCoprobit.cc:    // Gibbs loop
> ./src/MCMCordfactanal.cc:  // Gibbs Sampler //
> ./src/MCMCpanel.cc:// simulate from posterior density and return a
> Gibbs
> by parameters matrix
> ./src/MCMCpanel.cc: const int* burnin, const int* gibbs, const
> int* thin,
> ./src/MCMCpanel.cc:   int Mgibbs = gibbs[0];
> ./src/MCMCpanel.cc:   int Mtotiter = Mburnin + Mgibbs;
> ./src/MCMCpanel.cc:   Matrix<double> beta_holder(Mgibbs/Mthin,Mp);
> ./src/MCMCpanel.cc:   Matrix<double> D_holder(Mgibbs/Mthin,Mq*Mq);
> ./src/MCMCpanel.cc:   Matrix<double> sigma2_holder(Mgibbs/Mthin, 1);
> ./src/MCMCpanel.cc:   // gibbs loop
> ./src/MCMCregress.cc:     // Gibbs sampler
> ./src/MCMCregress.cc:       // second set of Gibbs scans
> ./src/MCMCSVDreg.cc:    /////////////////// Gibbs sampler 
> ///////////////////
> 
>   Perhaps some of these are useful?
> 
> For your info, I know nothing about MCMCpack, I just know how to use
> grep to search for things. If you are on Windows, you can probably use
> the Windows File Explorer Search option to look for it. But give me
> grep
> anyday...
> 
> Barry
>


From bernd.weiss at uni-koeln.de  Fri Aug 11 12:39:14 2006
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Fri, 11 Aug 2006 12:39:14 +0200
Subject: [R] help: convert lmer.coef to matrix
In-Reply-To: <1075.144.173.76.117.1155288826.squirrel@www.webmail.ex.ac.uk>
References: <1075.144.173.76.117.1155288826.squirrel@www.webmail.ex.ac.uk>
Message-ID: <44DC7A72.5977.12D7026@bernd.weiss.uni-koeln.de>

On 11 Aug 2006 at 10:33, Simon Pickett wrote:

Date sent:      	Fri, 11 Aug 2006 10:33:46 +0100 (BST)
From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] help: convert lmer.coef to matrix

> Hi all,
> I am trying to coerce the coeficients from a REML using lmer() to a
> matrix
> of numbers which I can then write into excel. I have looked in the
> archive
> and read around in the (Matrix) documentation but havent found
> anything of
> use.
> Any suggestions much appreciated,
> Thankyou, S.
> 

I would suggest something like this:

library(lme4)
data(sleepstudy)
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
lmer.coef <- fixef(fm1)

# save coefficients columnwise
# keep in mind that dec="," is needed to fulfil the requirements of 
my German version of MS-Excel
lmer.out.mat<-matrix(lmer.coef,ncol=length(lmer.coef))
write.table(lmer.out.mat,file="d:/lmer_out.csv",sep=";",dec=",",col.na
mes=names(lmer.coef),row.names=F)

# save coefficients rowwise
lmer.out.df<-as.data.frame(lmer.coef)
write.table(lmer.out.df,file="d:/lmer_out.csv",sep=";",dec=",",row.nam
es=T,col.names=F)

Is that what you are looking for?	

Bernd


From jholtman at gmail.com  Fri Aug 11 12:40:50 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 11 Aug 2006 06:40:50 -0400
Subject: [R] How to speed up nested for loop computations
In-Reply-To: <14A0D8CF-9AFA-42EB-8AE6-A87BC06FA6D2@ulb.ac.be>
References: <A07FC905-D0C3-4B76-B22F-258035727FA6@ulb.ac.be>
	<644e1f320608100946r2dc8277dv38f2f4645eca7d18@mail.gmail.com>
	<14A0D8CF-9AFA-42EB-8AE6-A87BC06FA6D2@ulb.ac.be>
Message-ID: <644e1f320608110340x180e5790g209cfdbb8d4b2b4c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/1b63e146/attachment.pl 

From yxiaomail at gmail.com  Fri Aug 11 12:42:43 2006
From: yxiaomail at gmail.com (Yong Xiao)
Date: Fri, 11 Aug 2006 18:42:43 +0800
Subject: [R] garch results is different other soft
Message-ID: <1aa19fc00608110342r76026c45h853fb76503ad0fcd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/02cb3efa/attachment.pl 

From wade.wall at gmail.com  Fri Aug 11 13:05:43 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Fri, 11 Aug 2006 07:05:43 -0400
Subject: [R] Box's M test
Message-ID: <e23082be0608110405w2f859dafl43f65cdd758a254@mail.gmail.com>

Hi all,

Is there a box's M test for R? I have looked around, but have been
unable to find it.

Thanks
Wade


From S.Pickett at exeter.ac.uk  Fri Aug 11 13:20:08 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Fri, 11 Aug 2006 12:20:08 +0100 (BST)
Subject: [R] help:coerce lmer.coef to matrix
Message-ID: <1171.144.173.76.117.1155295208.squirrel@www.webmail.ex.ac.uk>

Hi,
Thanks for your response, it nearly worked! But it only wrote one coloumn
of data and not the three columns I need.
Using fixef(m1) doesnt give the same results as coef(m1) when you are
using more than one random effect. I need the coefficients for each
individual so I use coef(m1) to get this which results in an object of
class lmer.coef, 3 columns by 700 rows.
as.data.frame() wont work on this and I cant seem to specify that I want
three columns when I tried <-matrix(lmer.coef,ncol=length(lmer.coef))....
Thanks very much,
S



Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From h.y.wong at leeds.ac.uk  Fri Aug 11 13:21:58 2006
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 11 Aug 2006 12:21:58 +0100
Subject: [R] Suggestions for help & weighted.mean
Message-ID: <012543A2-5D7A-464D-93EF-9BADC8C77EB5@leeds.ac.uk>

Hi, just a quick question:

Should weighted.mean be able to cope with the specific case where one  
weight is Inf? I came across this when trying to implement a simple  
weighted moving average algorithm for surface smoothing: these  
algorithms often result in a single infinite weight when predicting  
the surface value at known data points.

e.g.

 > weighted.mean(c(77,88,99), c(Inf, 1, 2)) #should this return 77?
[1] NaN

Cheers

Yan

p.s. a while ago I suggested using '??xxx' as a shortcut for  
help.search("xxx"), much like '?xxx' is a shortcut for help("xxx"). I  
was just wondering if anyone had any more thoughts on the matter?


From andy_liaw at merck.com  Fri Aug 11 13:21:34 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Aug 2006 07:21:34 -0400
Subject: [R] Box's M test  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2C93@usctmx1106.merck.com>

Depends on how you look.  Try RSiteSearch("Box's M Test") at the R prompt.

Andy 

From: Wade Wall
> 
> Hi all,
> 
> Is there a box's M test for R? I have looked around, but have 
> been unable to find it.
> 
> Thanks
> Wade
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From HDoran at air.org  Fri Aug 11 13:27:43 2006
From: HDoran at air.org (Doran, Harold)
Date: Fri, 11 Aug 2006 07:27:43 -0400
Subject: [R] about MCMC pack again...
Message-ID: <2323A6D37908A847A7C32F1E3662C80E276E49@dc1ex01.air.org>

Let's maybe back up a bit on this. You said you are interested in
learning about the application of the Gibbs sampler for IRT models. I
don't think opening the C++ code would be the best approach for this.

Let me recommend the following article

    Patz, R. J., and Junker, B. W.  (1999). A straightforward approach 
    to Markov chain Monte Carlo for item response models.  Journal of 
    Educational and Behavioral Statistics, 24, 146-178.

This will give you what you need to know. Richard Patz also developed a
program written in S that follows the models presented in the article.
You can find this somewhere on the statlib cmu website. Also, I don't
know how mcmcirt works under the hood exactly, but Gibbs sampler is a
special case of the MH algorithm when the acceptance rate is 1.

Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Mariagiulia Matteucci
> Sent: Friday, August 11, 2006 5:55 AM
> To: Barry Rowlingson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] about MCMC pack again...
> 
> Hello, I am using Windows, I tried to use th File Search and 
> also the Windows Grep but I cannot find any file! In the list 
> you showed me there are some useful , I really don't know how 
> can I find them! I tried in the R folder, src folder, MCMC 
> pack folder and I dowloaded the .tar file about MCMC pack 
> where there are the codes, I really don't know what to do!
> 
> Mariagiulia
> 
> 
> On Aug 11, 2006 10:51 AM, Barry Rowlingson 
> <B.Rowlingson at lancaster.ac.uk> wrote:
> 
> > Mariagiulia Matteucci wrote:
> > > Hello, thank you very much for your previous answers 
> about the C++ 
> > > code.
> > > I am interested in the application of the Gibbs Sampler 
> in the IRT 
> > > models, so in the function MCMCirt1d and MCMCirtkd. I've found the
> > > C++
> > > source codes, as you suggested, but I cannot find 
> anything about the 
> > > Gibbs Sampler. All the files are for the Metropolis algorithm.
> > 
> >   $ cd MCMCpack/
> >   $ grep -ir gibbs .
> > 
> > produces loads of output, including:
> > 
> > ./src/MCMCfactanal.cc:    } // end Gibbs loop
> > ./src/MCMChierEI.cc:// and slice sampling and Gibbs 
> sampling to sample 
> > from the posterior
> > ./src/MCMCirt1d.cc:    } // end Gibbs loop
> > ./src/MCMCmixfactanal.cc:  // Gibbs Sampler //
> > ./src/MCMCmixfactanal.cc:  } // end Gibbs loop
> > ./src/MCMCoprobit.cc:    // Gibbs loop
> > ./src/MCMCordfactanal.cc:  // Gibbs Sampler // 
> ./src/MCMCpanel.cc:// 
> > simulate from posterior density and return a Gibbs by parameters 
> > matrix
> > ./src/MCMCpanel.cc: const int* burnin, const int* gibbs, const
> > int* thin,
> > ./src/MCMCpanel.cc:   int Mgibbs = gibbs[0];
> > ./src/MCMCpanel.cc:   int Mtotiter = Mburnin + Mgibbs;
> > ./src/MCMCpanel.cc:   Matrix<double> beta_holder(Mgibbs/Mthin,Mp);
> > ./src/MCMCpanel.cc:   Matrix<double> D_holder(Mgibbs/Mthin,Mq*Mq);
> > ./src/MCMCpanel.cc:   Matrix<double> sigma2_holder(Mgibbs/Mthin, 1);
> > ./src/MCMCpanel.cc:   // gibbs loop
> > ./src/MCMCregress.cc:     // Gibbs sampler
> > ./src/MCMCregress.cc:       // second set of Gibbs scans
> > ./src/MCMCSVDreg.cc:    /////////////////// Gibbs sampler 
> > ///////////////////
> > 
> >   Perhaps some of these are useful?
> > 
> > For your info, I know nothing about MCMCpack, I just know 
> how to use 
> > grep to search for things. If you are on Windows, you can 
> probably use 
> > the Windows File Explorer Search option to look for it. But give me 
> > grep anyday...
> > 
> > Barry
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Fri Aug 11 13:31:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Aug 2006 12:31:55 +0100 (BST)
Subject: [R] bug in interaction order when using drop?
In-Reply-To: <44DB6902.31178.13D97A6@localhost>
References: <44DB6902.31178.13D97A6@localhost>
Message-ID: <Pine.LNX.4.64.0608111202450.32270@gannet.stats.ox.ac.uk>

On Thu, 10 Aug 2006, Petr Pikal wrote:

> Ooops, my first suggestion reorders factor itself but
> 
> if (drop) factor(ans) else ans
> 
> instead of whole drop construction shall preserve levels order 
> without changing order of factor

Even easier would be to return ans[,drop=drop].  It seems to me that there 
is an argument for expecting interaction(..., drop=TRUE) to give the same 
result as interaction(...)[,drop=TRUE], but little argument that any 
ordering is a *bug*.

The order of the levels of a factor are arbitrary, and in fact they seem 
to me to be in a strange order, with the levels of the first factor 
varying fastest (reverse lexiographic order).

> levels(interaction(c("A", "A", "B"), letters[1:3]))
[1] "A.a" "B.a" "A.b" "B.b" "A.c" "B.c"

so the existing

> levels(interaction(c("A", "A", "B"), letters[1:3], drop=T))
[1] "A.a" "A.b" "B.c"

looks more sensible in this case.

> 
> Petr
> 
> On 10 Aug 2006 at 16:32, Petr Pikal wrote:
> 
> From:           	"Petr Pikal" <petr.pikal at precheza.cz>
> To:             	r-help at stat.math.ethz.ch
> Date sent:      	Thu, 10 Aug 2006 16:32:54 +0200
> Priority:       	normal
> Subject:        	[R] bug in interaction order when using drop?
> 
> > Hallo all
> > 
> > > version
> >                _                                   
> > platform       i386-pc-mingw32                       
> > arch           i386                                  
> > os             mingw32                               
> > system         i386, mingw32                         
> > status         beta                                  
> > major          2                                   
> > minor          3.1                                   
> > year           2006                                  
> > month          05                                   
> > day            23                                   
> > svn rev        38179                                 
> > language       R                                   
> > version.string Version 2.3.1 beta (2006-05-23 r38179)
> > >
> > 
> > When I use interaction(....) without drop=T parameters I will get
> > neatly organized factor with "protiproud" and "souproud" aligned.
> > 
> > > levels(interaction(vykon, teplota, proudeni))
> >  [1] "3.750.protiproud"  "12.750.protiproud" "3.775.protiproud" 
> > "12.775.protiproud" "3.800.protiproud"  "12.800.protiproud"
> >  [7] "3.825.protiproud"  "12.825.protiproud" "3.850.protiproud" 
> > "12.850.protiproud" "3.750.souproud"    "12.750.souproud"  [13]
> > "3.775.souproud"    "12.775.souproud"   "3.800.souproud"   
> > "12.800.souproud"   "3.825.souproud"    "12.825.souproud"  [19]
> > "3.850.souproud"    "12.850.souproud"  
> > 
> > However when I use 
> > 
> > > levels(interaction(vykon, teplota, proudeni, drop=T))
> > [1] "3.775.protiproud"  "3.800.souproud"    "3.750.souproud"    
> > "12.850.souproud"   "12.825.protiproud"
> > 
> > everything is out of order. I know I can reorder any factor according
> > to my wish but it would be good to have it ordered same way as without
> > using drop.
> > 
> > Everything comes from unique in
> > 
> > if (drop) {
> >         f <- unique(ans[!is.na(ans)])
> >         ans <- match(ans, f)
> >         lvs <- lvs[f]
> > }
> > 
> > maybe it can be modified.
> > 
> > if (drop) {
> >         f <- unique(ans[!is.na(ans)])
> >         ord <- order(f)
> >         ans <- match(ans, f)
> >         lvs <- lvs[f[ord]]
> >         }
> > 
> > which seems to work but I am not sure if it does not makes problems
> > having NA in data.
> > 
> > Here is my data frame.
> > Thank you 
> > 
> > Petr Pikal
> > 
> > > dump("df", file=stdout()) 
> > df <-
> > structure(list(proudeni = structure(as.integer(c(1, 1, 1, 1, 
> > 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 
> > 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
> > 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> > 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
> > 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> > 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
> > 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
> > 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 
> > 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
> > 1, 1, 1)), .Label = c("protiproud", "souproud"), class = "factor"), 
> >     vykon = as.integer(c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 
> >     12, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 
> >     12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
> >     3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 12, 12, 12, 12, 12, 
> >     12, 12, 12, 12, 12, 12)), teplota = as.integer(c(775, 775, 
> >     775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
> >     775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 800, 800, 
> >     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 750, 850, 
> >     850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 825, 775, 
> >     775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
> >     775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 800, 
> >     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 750, 
> >     850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 825, 
> >     775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
> >     775, 775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 800, 
> >     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
> >     750, 850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 825, 
> >     825, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 775, 
> >     775, 775, 775, 775, 775, 775, 775, 800, 800, 800, 800, 800, 
> >     800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
> >     800, 750, 850, 850, 850, 850, 850, 850, 825, 825, 825, 825, 
> >     825, 825))), .Names = c("proudeni", "vykon", "teplota"), 
> > row.names = c("1", 
> > "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
> > "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> > "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
> > "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
> > "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57",
> > "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68",
> > "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79",
> > "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90",
> > "91", "92", "93", "94", "95", "96", "97", "98", "99", "100", "101",
> > "102", "103", "104", "105", "106", "107", "108", "109", "110", "111",
> > "112", "113", "114", "115", "116", "117", "118", "119", "120", "121",
> > "122", "123", "124", "125", "126", "127", "128", "129", "130", "131",
> > "132", "133", "134", "135", "136", "137", "138", "139", "140", "141",
> > "142", "143", "144", "145", "146", "147", "148", "149", "150", "151",
> > "152", "153", "154", "155", "156", "157", "158", "159", "160", "161",
> > "162", "163", "164", "165", "166", "167", "168", "169", "170", "171",
> > "172", "173", "174", "175", "176", "177", "178", "179", "180", "181",
> > "182", "183", "184", "185", "186", "187", "188", "189", "190", "191",
> > "192", "193", "194", "195", "196"), class = "data.frame") > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Fri Aug 11 13:48:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Aug 2006 07:48:54 -0400
Subject: [R] Multiple density curves
In-Reply-To: <c2f237040608100907t44dba1dax3c34a0178b73ddee@mail.gmail.com>
References: <c2f237040608100907t44dba1dax3c34a0178b73ddee@mail.gmail.com>
Message-ID: <971536df0608110448r2ec8576bt3b145fd228020cb2@mail.gmail.com>

>From your description I assume you want both histograms
and the densities all on the same chart.  With existing R
graphics I am not sure that there really is a simple way to
do that.

That aside, note that the hist function returns a list of
components that includes

- breaks, defining the breakpoints of the histogram
- intensities defining the heights of the histogram bars

We can use these two to determine the breaks and y limits
of the combined plot and then use the breaks= and ylim=
arguments of hist to specify them so that both histograms
can be drawn on the same chart.  We also use freq=FALSE
in the hist calls to draw intensities rather than counts.  On
the second hist call we use add=TRUE to cause it to be drawn
on the existing plot.

The other problem is to distinguish the superimposition of
the bars and that can be handled by using shading lines of
different colors and angles using the col= and angle= and
density= arguments of hist.


# data
DF <- structure(list(SEQ = structure(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11, 12, 13), .Label = c("A", "B", "C", "D", "E", "F", "G", "H",
"I", "J", "K", "L", "M"), class = "factor"), A1 = c(532.5, 25.5,
265.2, 245.55, 546.52, 243.25, 452.55, 15.14, 543.4, 54.4, 646.5,
645.4, 646.54), A2 = c(554.5, 35.5, 522.2, 521.56, 141.52, 32.56,
635.56, 16.54, 646.56, 654.5, 64.54, 614.46, 634.46)), .Names = c("SEQ",
"A1", "A2"), class = "data.frame", row.names = c("1", "2", "3",
"4", "5", "6", "7", "8", "9", "10", "11", "12", "13"))

# determine breaks and y limits of the combined plot
breaks <- hist(c(DF$A1, DF$A2), plot = FALSE)$breaks
ymax1 <- max(hist(DF$A1, breaks = breaks, plot = FALSE)$intensities)
ymax2 <- max(hist(DF$A2, breaks = breaks, plot = FALSE)$intensities)
ylim <- c(0, max(ymax1, ymax2))

# draw the two histograms and two densities
hist(DF$A1, ang = 45, col = "red", ylim = ylim, freq = FALSE, density = 10)
lines(density(DF$A1), col = "red")
hist(DF$A2, ang = -45, col = "blue", add = TRUE, freq = FALSE, density = 10)
lines(density(DF$A2), col = "blue")

On 8/10/06, Davendra Sohal <dsohal at gmail.com> wrote:
> Hi,
>
> I am new to R...a recent convert from SAS.
> I have a dataset that looks like this:
>
> SEQ    A1    A2
> A    532.5    554.5
> B    25.5    35.5
> C    265.2    522.2
> D    245.55    521.56
> E    546.52    141.52
> F    243.25    32.56
> G    452.55    635.56
> H    15.14    16.54
> I    543.4    646.56
> J    54.4    654.5
> K    646.5    64.54
> L    645.4    614.46
> M    646.54    634.46
>
> I want to make a histogram each for A1 and A2, with density curves, on the
> same plot so that I can see how they overlap.
>
> Please let me know some simple code for this.
>
> I looked at ldahist but it was complicated. Anything simpler?
>
> Thanks a lot,
> -DS.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Fri Aug 11 13:49:11 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 11 Aug 2006 07:49:11 -0400
Subject: [R] Suggestions for help & weighted.mean
In-Reply-To: <012543A2-5D7A-464D-93EF-9BADC8C77EB5@leeds.ac.uk>
References: <012543A2-5D7A-464D-93EF-9BADC8C77EB5@leeds.ac.uk>
Message-ID: <44DC6EB7.9060004@stats.uwo.ca>

Yan Wong wrote:
> Hi, just a quick question:
>
> Should weighted.mean be able to cope with the specific case where one  
> weight is Inf? I came across this when trying to implement a simple  
> weighted moving average algorithm for surface smoothing: these  
> algorithms often result in a single infinite weight when predicting  
> the surface value at known data points.
>
> e.g.
>
>  > weighted.mean(c(77,88,99), c(Inf, 1, 2)) #should this return 77?
> [1] NaN
>
>   
It makes sense in this case, but in the case where there is more than 
one infinite weight, the result has to be NaN. 

Right now the bit of weighted.mean that does the calculations is

 sum(x * w)/sum(w)

and this would be a lot more complicated if it were to handle this very 
special case.

On the other hand, if you know that in your situation there is at most 
one infinite weight, then you could use

if (any(inf <- is.infinite(w))) x[inf]
else weighted.mean(x, w)

in your own code.

Duncan Murdoch


> Cheers
>
> Yan
>
> p.s. a while ago I suggested using '??xxx' as a shortcut for  
> help.search("xxx"), much like '?xxx' is a shortcut for help("xxx"). I  
> was just wondering if anyone had any more thoughts on the matter?
Suggestions like this (and probably the one above) belong more in the 
R-devel list than here.  I think your ?? suggestion is reasonable; why 
don't you write up the necessary patch to implement it, and see if it's 
feasible?   Include that in your post to R-devel, and it will be easier 
for others to see the pitfalls (if there are any).

Duncan Murdoch


From ggrothendieck at gmail.com  Fri Aug 11 14:01:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Aug 2006 08:01:25 -0400
Subject: [R] Multiple density curves
In-Reply-To: <971536df0608110448r2ec8576bt3b145fd228020cb2@mail.gmail.com>
References: <c2f237040608100907t44dba1dax3c34a0178b73ddee@mail.gmail.com>
	<971536df0608110448r2ec8576bt3b145fd228020cb2@mail.gmail.com>
Message-ID: <971536df0608110501u6cf62035hd8eb73957b42f1d1@mail.gmail.com>

The code below was missing the breaks= argument to hist.
I had not noticed because coincidentally both give the same
breaks anways thus the following corrected version gives the
same plot in this case but might not in other cases.

# data
DF <- structure(list(SEQ = structure(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11, 12, 13), .Label = c("A", "B", "C", "D", "E", "F", "G", "H",
"I", "J", "K", "L", "M"), class = "factor"), A1 = c(532.5, 25.5,
265.2, 245.55, 546.52, 243.25, 452.55, 15.14, 543.4, 54.4, 646.5,
645.4, 646.54), A2 = c(554.5, 35.5, 522.2, 521.56, 141.52, 32.56,
635.56, 16.54, 646.56, 654.5, 64.54, 614.46, 634.46)), .Names = c("SEQ",
"A1", "A2"), class = "data.frame", row.names = c("1", "2", "3",
"4", "5", "6", "7", "8", "9", "10", "11", "12", "13"))

# determine breaks and y limits of the combined plot
breaks <- hist(c(DF$A1, DF$A2), plot = FALSE)$breaks
ymax1 <- max(hist(DF$A1, breaks = breaks, plot = FALSE)$intensities)
ymax2 <- max(hist(DF$A2, breaks = breaks, plot = FALSE)$intensities)
ylim <- c(0, max(ymax1, ymax2))

# draw the two histograms and two densities
hist(DF$A1, ang = 45, col = "red", ylim = ylim,
	breaks = breaks, freq = FALSE, density = 10)
lines(density(DF$A1), col = "red")
hist(DF$A2, ang = -45, col = "blue", add = TRUE,
	breaks = breaks, freq = FALSE, density = 10)
lines(density(DF$A2), col = "blue")



On 8/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> From your description I assume you want both histograms
> and the densities all on the same chart.  With existing R
> graphics I am not sure that there really is a simple way to
> do that.
>
> That aside, note that the hist function returns a list of
> components that includes
>
> - breaks, defining the breakpoints of the histogram
> - intensities defining the heights of the histogram bars
>
> We can use these two to determine the breaks and y limits
> of the combined plot and then use the breaks= and ylim=
> arguments of hist to specify them so that both histograms
> can be drawn on the same chart.  We also use freq=FALSE
> in the hist calls to draw intensities rather than counts.  On
> the second hist call we use add=TRUE to cause it to be drawn
> on the existing plot.
>
> The other problem is to distinguish the superimposition of
> the bars and that can be handled by using shading lines of
> different colors and angles using the col= and angle= and
> density= arguments of hist.
>
>
> # data
> DF <- structure(list(SEQ = structure(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> 11, 12, 13), .Label = c("A", "B", "C", "D", "E", "F", "G", "H",
> "I", "J", "K", "L", "M"), class = "factor"), A1 = c(532.5, 25.5,
> 265.2, 245.55, 546.52, 243.25, 452.55, 15.14, 543.4, 54.4, 646.5,
> 645.4, 646.54), A2 = c(554.5, 35.5, 522.2, 521.56, 141.52, 32.56,
> 635.56, 16.54, 646.56, 654.5, 64.54, 614.46, 634.46)), .Names = c("SEQ",
> "A1", "A2"), class = "data.frame", row.names = c("1", "2", "3",
> "4", "5", "6", "7", "8", "9", "10", "11", "12", "13"))
>
> # determine breaks and y limits of the combined plot
> breaks <- hist(c(DF$A1, DF$A2), plot = FALSE)$breaks
> ymax1 <- max(hist(DF$A1, breaks = breaks, plot = FALSE)$intensities)
> ymax2 <- max(hist(DF$A2, breaks = breaks, plot = FALSE)$intensities)
> ylim <- c(0, max(ymax1, ymax2))
>
> # draw the two histograms and two densities
> hist(DF$A1, ang = 45, col = "red", ylim = ylim, freq = FALSE, density = 10)
> lines(density(DF$A1), col = "red")
> hist(DF$A2, ang = -45, col = "blue", add = TRUE, freq = FALSE, density = 10)
> lines(density(DF$A2), col = "blue")
>
> On 8/10/06, Davendra Sohal <dsohal at gmail.com> wrote:
> > Hi,
> >
> > I am new to R...a recent convert from SAS.
> > I have a dataset that looks like this:
> >
> > SEQ    A1    A2
> > A    532.5    554.5
> > B    25.5    35.5
> > C    265.2    522.2
> > D    245.55    521.56
> > E    546.52    141.52
> > F    243.25    32.56
> > G    452.55    635.56
> > H    15.14    16.54
> > I    543.4    646.56
> > J    54.4    654.5
> > K    646.5    64.54
> > L    645.4    614.46
> > M    646.54    634.46
> >
> > I want to make a histogram each for A1 and A2, with density curves, on the
> > same plot so that I can see how they overlap.
> >
> > Please let me know some simple code for this.
> >
> > I looked at ldahist but it was complicated. Anything simpler?
> >
> > Thanks a lot,
> > -DS.
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From matteucci at stat.unibo.it  Fri Aug 11 14:05:51 2006
From: matteucci at stat.unibo.it (Mariagiulia Matteucci)
Date: Fri, 11 Aug 2006 14:05:51 +0200 (CEST)
Subject: [R] about MCMC pack again...
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E276E49@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E276E49@dc1ex01.air.org>
Message-ID: <7591219.1155297951392.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear Harold, 
thank you very much. By the way, I know the article but my interest is
more on the normal ogive model, and on the Inversion (probability
integral transformation) method applied to the Gibbs sampler.
If somebody knows something about it and about missing data in MCMC pack
in R, please let me know

Mariagiulia

On Aug 11, 2006 01:27 PM, "Doran, Harold" <HDoran at air.org> wrote:

> Let's maybe back up a bit on this. You said you are interested in
> learning about the application of the Gibbs sampler for IRT models. I
> don't think opening the C++ code would be the best approach for this.
> 
> Let me recommend the following article
> 
> Patz, R. J., and Junker, B. W. (1999). A straightforward approach
>     to Markov chain Monte Carlo for item response models.  Journal of 
>     Educational and Behavioral Statistics, 24, 146-178.
> 
> This will give you what you need to know. Richard Patz also developed
> a
> program written in S that follows the models presented in the article.
> You can find this somewhere on the statlib cmu website. Also, I don't
> know how mcmcirt works under the hood exactly, but Gibbs sampler is a
> special case of the MH algorithm when the acceptance rate is 1.
> 
> Harold
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > Mariagiulia Matteucci
> > Sent: Friday, August 11, 2006 5:55 AM
> > To: Barry Rowlingson
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] about MCMC pack again...
> > 
> > Hello, I am using Windows, I tried to use th File Search and 
> > also the Windows Grep but I cannot find any file! In the list 
> > you showed me there are some useful , I really don't know how 
> > can I find them! I tried in the R folder, src folder, MCMC 
> > pack folder and I dowloaded the .tar file about MCMC pack 
> > where there are the codes, I really don't know what to do!
> > 
> > Mariagiulia
> > 
> > 
> > On Aug 11, 2006 10:51 AM, Barry Rowlingson 
> > <B.Rowlingson at lancaster.ac.uk> wrote:
> > 
> > > Mariagiulia Matteucci wrote:
> > > > Hello, thank you very much for your previous answers 
> > about the C++ 
> > > > code.
> > > > I am interested in the application of the Gibbs Sampler 
> > in the IRT 
> > > > models, so in the function MCMCirt1d and MCMCirtkd. I've found
> > > > the
> > > > C++
> > > > source codes, as you suggested, but I cannot find 
> > anything about the 
> > > > Gibbs Sampler. All the files are for the Metropolis algorithm.
> > > 
> > >   $ cd MCMCpack/
> > >   $ grep -ir gibbs .
> > > 
> > > produces loads of output, including:
> > > 
> > > ./src/MCMCfactanal.cc:    } // end Gibbs loop
> > > ./src/MCMChierEI.cc:// and slice sampling and Gibbs 
> > sampling to sample 
> > > from the posterior
> > > ./src/MCMCirt1d.cc:    } // end Gibbs loop
> > > ./src/MCMCmixfactanal.cc:  // Gibbs Sampler //
> > > ./src/MCMCmixfactanal.cc:  } // end Gibbs loop
> > > ./src/MCMCoprobit.cc:    // Gibbs loop
> > > ./src/MCMCordfactanal.cc:  // Gibbs Sampler // 
> > ./src/MCMCpanel.cc:// 
> > > simulate from posterior density and return a Gibbs by parameters 
> > > matrix
> > > ./src/MCMCpanel.cc: const int* burnin, const int* gibbs, const
> > > int* thin,
> > > ./src/MCMCpanel.cc:   int Mgibbs = gibbs[0];
> > > ./src/MCMCpanel.cc:   int Mtotiter = Mburnin + Mgibbs;
> > > ./src/MCMCpanel.cc:   Matrix<double> beta_holder(Mgibbs/Mthin,Mp);
> > > ./src/MCMCpanel.cc:   Matrix<double> D_holder(Mgibbs/Mthin,Mq*Mq);
> > > ./src/MCMCpanel.cc: Matrix<double> sigma2_holder(Mgibbs/Mthin, 1);
> > > ./src/MCMCpanel.cc:   // gibbs loop
> > > ./src/MCMCregress.cc:     // Gibbs sampler
> > > ./src/MCMCregress.cc:       // second set of Gibbs scans
> > > ./src/MCMCSVDreg.cc:    /////////////////// Gibbs sampler 
> > > ///////////////////
> > > 
> > >   Perhaps some of these are useful?
> > > 
> > > For your info, I know nothing about MCMCpack, I just know 
> > how to use 
> > > grep to search for things. If you are on Windows, you can 
> > probably use 
> > > the Windows File Explorer Search option to look for it. But give
> > > me
> > > grep anyday...
> > > 
> > > Barry
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
>


From petr.pikal at precheza.cz  Fri Aug 11 14:07:44 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 11 Aug 2006 14:07:44 +0200
Subject: [R] bug in interaction order when using drop?
In-Reply-To: <Pine.LNX.4.64.0608111202450.32270@gannet.stats.ox.ac.uk>
References: <44DB6902.31178.13D97A6@localhost>
Message-ID: <44DC8F30.10402.150B217@localhost>

On 11 Aug 2006 at 12:31, Prof Brian Ripley wrote:

Date sent:      	Fri, 11 Aug 2006 12:31:55 +0100 (BST)
From:           	Prof Brian Ripley <ripley at stats.ox.ac.uk>
To:             	Petr Pikal <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] bug in interaction order when using drop?

> On Thu, 10 Aug 2006, Petr Pikal wrote:
> 
> > Ooops, my first suggestion reorders factor itself but
> > 
> > if (drop) factor(ans) else ans
> > 
> > instead of whole drop construction shall preserve levels order
> > without changing order of factor
> 
> Even easier would be to return ans[,drop=drop].  It seems to me that
> there is an argument for expecting interaction(..., drop=TRUE) to give
> the same result as interaction(...)[,drop=TRUE], but little argument
> that any ordering is a *bug*.

Maybe bug was an *exaggeration*, but what surprised me was different 
order in using interaction with and without drop. Well, I would call 
it not consistent behaviour as omitting unused levels silently change 
an order of factor levels.

> set.seed(1)
> DF<-data.frame(x=sample(LETTERS[1:3],10, replace=T), 
y=sample(letters[1:3],10, replace=T))

> interaction(DF$x,DF$y)
 [1] A.a B.a B.c C.b A.c C.b C.c B.c B.b A.c
Levels: A.a B.a C.a A.b B.b C.b A.c B.c C.c

Here is neat ordering, however as you said first level varying 
fastest.

> interaction(DF$x,DF$y, drop=T)
 [1] A.a B.a B.c C.b A.c C.b C.c B.c B.b A.c
Levels: A.a B.a B.c C.b A.c C.c B.b

This seems to me chaotic, but I will be glad if you explain to me 
some rational pattern in it.

> my.int(DF$x,DF$y,drop=T) # changed as suggested
 [1] A.a B.a B.c C.b A.c C.b C.c B.c B.b A.c
Levels: A.a B.a B.b C.b A.c B.c C.c
>

Same ordering as without drop, with unused levels omitted.
Best regards.
Petr Pikal

> 
> The order of the levels of a factor are arbitrary, and in fact they
> seem to me to be in a strange order, with the levels of the first
> factor varying fastest (reverse lexiographic order).
> 
> > levels(interaction(c("A", "A", "B"), letters[1:3]))
> [1] "A.a" "B.a" "A.b" "B.b" "A.c" "B.c"
> 
> so the existing
> 
> > levels(interaction(c("A", "A", "B"), letters[1:3], drop=T))
> [1] "A.a" "A.b" "B.c"
> 
> looks more sensible in this case.
> 
> > 
> > Petr
> > 
> > On 10 Aug 2006 at 16:32, Petr Pikal wrote:
> > 
> > From:           	"Petr Pikal" <petr.pikal at precheza.cz>
> > To:             	r-help at stat.math.ethz.ch
> > Date sent:      	Thu, 10 Aug 2006 16:32:54 +0200
> > Priority:       	normal
> > Subject:        	[R] bug in interaction order when using drop?

<snip>

> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595

Petr Pikal
petr.pikal at precheza.cz


From i_shi_i2005 at yahoo.co.jp  Fri Aug 11 15:09:31 2006
From: i_shi_i2005 at yahoo.co.jp (asako Ishii)
Date: Fri, 11 Aug 2006 22:09:31 +0900
Subject: [R] Is the PC1 in PCA always a "size effect"?
Message-ID: <20060811220929.4E5F.I_SHI_I2005@yahoo.co.jp>

Hi

Im a beginner to R or statistics in general.

I am comparing our products(6 in all) and competitive products(2). These products have a similar chemical formulation, but slightly different to each other. I chose the quantity of each chemicals as the variables and did a PCA to extract the characteristics of the products. The result was a PC1 which could be interpreted as the difference between us and the competitive. In my previous experience, PCA always gave me a PC1 that is a linear combination of the same sign, which is called more or less a "size effect" as is explained in statistics textbooks. Is there anything wrong in my PCA or in my interpretation of the result? Does the same thing often happen in other field too?

Thanks in advance!
-- 
Yukihiro Ishii <yukiasais at ybb.ne.jp>


From i_shi_i2005 at yahoo.co.jp  Fri Aug 11 15:18:26 2006
From: i_shi_i2005 at yahoo.co.jp (asako Ishii)
Date: Fri, 11 Aug 2006 22:18:26 +0900
Subject: [R]  Is the PC1 in PCA always a "size effect"?
Message-ID: <20060811221707.9C6C.I_SHI_I2005@yahoo.co.jp>

Hi

Im a beginner to R or statistics in general.

I am comparing our products(6 in all) and competitive products(2). These products have a similar chemical formulation, but slightly different to each other. I chose the quantity of each chemicals as the variables and did a PCA to extract the characteristics of the products. The result was a PC1 which could be interpreted as the difference between us and the competitive. In my previous experience, PCA always gave me a PC1 that is a linear combination of the same sign, which is called more or less a "size effect" as is explained in statistics textbooks. Is there anything wrong in my PCA or in my interpretation of the result? Does the same thing often happen in other field too?

Thanks in advance!

Asako Ishii<i_shi_i2005 at yahoo.co.jp>


From sfalcon at fhcrc.org  Fri Aug 11 16:29:09 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 11 Aug 2006 07:29:09 -0700
Subject: [R] Running out of memory when using lapply
In-Reply-To: <44D759A6.1000600@childrens.harvard.edu> (Kamila Naxerova's
	message of "Mon, 07 Aug 2006 11:17:58 -0400")
References: <mailman.7.1154944803.26285.r-help@stat.math.ethz.ch>
	<44D759A6.1000600@childrens.harvard.edu>
Message-ID: <m2d5b7ibwq.fsf@ziti.fhcrc.org>

Hi Kamila,

"Kamila Naxerova" <kamila.naxerova at childrens.harvard.edu> writes:

> Hi all!
>
> I'm afraid I programmed something totally non-sensical and inefficient, 
> but I can't figure out how to do it better.
>
> I have a list of ~ 40 000 characters. I want to take each element at a 
> time, map it to a large data frame with 
> hit=which(data.frame$column==elementFromList), then compute some 
> statistic on data.frame[hit,] and return a result that consists of 
> either 1) a list of integers or 2) a character.
>
> res=lapply(listof40000,myfunction,dataframeToSearchIn)
>
> On a small scale, this works and returns something like
>
> str(res)
> [[1]]
> [1] "UNIQUE"
> [[2]]
> [1]   405   406   407 16351
> [[3]]
> [1] "REMOVE"
> [[4]]
> [1] "REMOVE"
>
> If I try this with the entire 40 000 character list, though, I get the 
> "Reached total allocation of 1022Mb: see help(memory.size)" error message.
>
> Can someone please give me a hint how to solve this problem correctly? 
> THANKS!

One thing you might try is not running the entire 40K list at once.
Perhaps try breaking it into 4 10K lists, running each, and combining
the results.  This may get you around the allocation problem.

Another thing would be to find a system with more RAM (also read the
FAQ regarding ways to make the most amount of RAM available to R on
Windows, is that where you are?).

+ seth


From bates at stat.wisc.edu  Fri Aug 11 16:59:18 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Aug 2006 09:59:18 -0500
Subject: [R] help:coerce lmer.coef to matrix
In-Reply-To: <1171.144.173.76.117.1155295208.squirrel@www.webmail.ex.ac.uk>
References: <1171.144.173.76.117.1155295208.squirrel@www.webmail.ex.ac.uk>
Message-ID: <40e66e0b0608110759p785dc787me943cb630e88c102@mail.gmail.com>

On 8/11/06, Simon Pickett <S.Pickett at exeter.ac.uk> wrote:
> Hi,
> Thanks for your response, it nearly worked! But it only wrote one coloumn
> of data and not the three columns I need.
> Using fixef(m1) doesnt give the same results as coef(m1) when you are
> using more than one random effect. I need the coefficients for each
> individual so I use coef(m1) to get this which results in an object of
> class lmer.coef, 3 columns by 700 rows.
> as.data.frame() wont work on this and I cant seem to specify that I want
> three columns when I tried <-matrix(lmer.coef,ncol=length(lmer.coef))....
> Thanks very much,

Situations like this are when Martin Maechler's str function comes in
so handy.  It displays the structure of the object from which you will
see that coef(lmerModel) returns a list of data frames, not a data
frame.  You can create a matrix from an element of thiis list easily.

> library(Matrix)
Loading required package: lattice
> data(sleepstudy)
> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
   Data: sleepstudy
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.628 1769.593 -871.8141   1751.986     1743.628
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.090  24.7405
          Days         35.072   5.9221  0.066
 Residual             654.941  25.5918
number of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept) 251.4051     6.8246  36.838
Days         10.4673     1.5458   6.771

Correlation of Fixed Effects:
     (Intr)
Days -0.138
> str(coef(fm1))
Formal class 'coef.lmer' [package "Matrix"] with 1 slots
  ..@ .Data:List of 1
  .. ..$ :`data.frame':	18 obs. of  2 variables:
  .. .. ..$ (Intercept): num [1:18] 254 211 212 275 274 ...
  .. .. ..$ Days       : num [1:18] 19.67  1.85  5.02  5.65  7.40 ...
> as.matrix(coef(fm1)[[1]])
    (Intercept)       Days
308    253.6637 19.6662580
309    211.0065  1.8475846
310    212.4448  5.0184079
330    275.0956  5.6529533
331    273.6653  7.3973901
332    260.4446 10.1951148
333    268.2455 10.2436606
334    244.1725 11.5418624
335    251.0714 -0.2848734
337    286.2955 19.0955683
349    226.1950 11.6407015
350    238.3351 17.0814918
351    255.9829  7.4520285
352    272.2687 14.0032983
369    254.6806 11.3395024
370    225.7922 15.2897520
371    252.2121  9.4791308
372    263.7196 11.7513151


From h.y.wong at leeds.ac.uk  Fri Aug 11 17:01:43 2006
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 11 Aug 2006 16:01:43 +0100
Subject: [R] Suggestions for help & weighted.mean
In-Reply-To: <44DC6EB7.9060004@stats.uwo.ca>
References: <012543A2-5D7A-464D-93EF-9BADC8C77EB5@leeds.ac.uk>
	<44DC6EB7.9060004@stats.uwo.ca>
Message-ID: <67A8B2A2-C74B-4D61-BD19-7ACCBF6A7AC2@leeds.ac.uk>


On 11 Aug 2006, at 12:49, Duncan Murdoch wrote:

> It makes sense in this case, but in the case where there is more  
> than one infinite weight, the result has to be NaN.
> ... it would be a lot more complicated if it were to handle this  
> very special case.

Yes - I see that it may not be worth slowing down the code to cope  
with this one particular case. I suppose it really comes down to a  
question of completeness versus speed.

> On the other hand, if you know that in your situation there is at  
> most one infinite weight, then you could use
>
> if (any(inf <- is.infinite(w))) x[inf]
> else weighted.mean(x, w)

Thanks. I think I do something like that already, but your code is  
cleaner than mine!

>> p.s. a while ago I suggested using '??xxx' as a shortcut for   
>> help.search("xxx"), much like '?xxx' is a shortcut for help 
>> ("xxx"). I  was just wondering if anyone had any more thoughts on  
>> the matter?
> Suggestions like this (and probably the one above) belong more in  
> the R-devel list than here.

OK. Thanks.

> I think your ?? suggestion is reasonable; why don't you write up  
> the necessary patch to implement it, and see if it's feasible?    
> Include that in your post to R-devel, and it will be easier for  
> others to see the pitfalls (if there are any).

Great. I'll do that when I have time (and if I can work out how the  
codebase works), then try posting it to R-devel.

Cheers

Yan


From jeff.horner at vanderbilt.edu  Fri Aug 11 17:10:21 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Fri, 11 Aug 2006 10:10:21 -0500
Subject: [R] graphic output file format
In-Reply-To: <22175d30608101221t56c2aa99p78b5c1af82e851b5@mail.gmail.com>
References: <20060810164524.45102.qmail@web32515.mail.mud.yahoo.com>	<44DB6618.2050109@uni-erfurt.de>	<1155236922.24212.268104251@webmail.messagingengine.com>
	<22175d30608101221t56c2aa99p78b5c1af82e851b5@mail.gmail.com>
Message-ID: <44DC9DDD.90701@vanderbilt.edu>

Luiz Rodrigo Tozzi wrote:
> Hi
> 
> I had some problems using GDD, especially with colors and with some advanced
> plottings.
> 
> In my case I didnt try the Cairo, I just got back to the png() using Xvfb
> (its almost 3 or 4 times slower than my first tries qith GDD, but using GDD
> would cause me to rewirte some parts of my script)
> 
> Since my Xvfb "came back to normal" in cron mode, I didnt use the Cairo, but
> mr urbanek really suggested it to me.

I've had a little experience with both GDD and Cairo:

GDD has served my needs well when drawing thin lines and pixels, 
although, the current implementation has problems with the background 
color and the line width (lwd) not being honored. I've hacked around a 
bit, and I can now set the background correctly, but I need to clean up 
the patch to submit to Simon. Also, libgdd rasterizes pngs very well. If 
you're interested in examples, check out the png images here:

http://biostat.mc.vanderbilt.edu/DiffGraphics

For the same needs, Cairo does very poorly rasterizing for fine pixel 
detail; it's not so much libcairo's fault as much as the package Cairo 
not being able to handle this, if it can at all. A better explanation of 
this issue is here: http://cairographics.org/FAQ. Read the first 
question: "Why does my 1-pixel wide horizontal/vertical line come out 
fat and blurry (eg. 2 pixels wide and half-intensity)?"

I'm not sure why you're having trouble with colors in GDD; I've never 
run into this issue.


> I dont know if you have something similar to Xvfb in Mac OSX
> 
> Tozzi
> 
> 2006/8/10, erick at nospammail.net <erick at nospammail.net>:
>>
>> I received the following communication from the package maintainer
>> for GDD (Simon Urbanek) when I was having compile problems on linux...
>>
>>>> gd.x86_64                                2.0.28-4.4E.1
>>>> installed       Matched from:
>>> ^^
>>>  +--- your GD is too old. You'll need at least gd-2.0.29 (see README
>>> in GDD) and 2.0.33 is recommended.
>>> You may want to try Cairo (see the nightly page) instead of GDD,
>>> because cairographics has some features that GD is lacking and it is
>>> actively maintained (unfortunately libgd is pretty much dead).
>>>
>>> Cheers,
>>> Simon
>> Disclaimer: I actually cannot get the Cairo to work either!
>> Eric
>>
>>
>>
>> On Thu, 10 Aug 2006 19:00:08 +0200, "Stefan Grosse"
>> <stefan.grosse at uni-erfurt.de> said:
>>> Please mail always also to the list, since it may be that there is
>>> someone who could reply better and/or faster.
>> [Case in point.]
>>> Especially in this case since I am only experienced in M$ Windows and
>>> Linux so I am unable to guess whats wrong with OSX. does not even the
>>> postscript device work? With postscript I get the best quality for
>>> printing.
>>>
>>> e.g.:
>>> postscript("test.eps", width = 8.0, height = 6.0, horizontal = FALSE,
>>> onefile =FALSE, paper = "special")
>>> curve(x^2)
>>> dev.off()
>>>
>>>
>>> Conan Phelan schrieb:
>>>> Thanks for the feedback and link Stefan,
>>>>
>>>> Perhaps if I provide a little more background. (Pls
>>>> forgive both my ignorance and my long windedness.)
>>>>
>>>>  I am on Mac OSX and am only able to load the Quartz
>>>> graphic device (is that normal?). From quartz, I've
>>>> only been able to generate pdf files, which do not
>>>> match the quartz display to my satisfaction. I've been
>>>> attempting to find a way to generate other output
>>>> files for comparison. (I've managed to generate a bmp
>>>> file that looked awful, after acquiring Gostscript).
>>>>
>>>> Anyway, I figure R must able to produce high quality
>>>> graphic outputs, so I'm trying different things. To
>>>> this end I was hoping that the GDD package would allow
>>>> me to load the x11 module but it seems to be a no-go
>>>> (is there some system requirement I'm missing?).
>>>>
>>>> Conan

-- 
Jeffrey Horner       Computer Systems Analyst         School of Medicine
615-322-8606         Department of Biostatistics   Vanderbilt University


From claude.josse at wanadoo.fr  Fri Aug 11 17:14:42 2006
From: claude.josse at wanadoo.fr (claude.josse)
Date: Fri, 11 Aug 2006 17:14:42 +0200
Subject: [R] tkinsert
Message-ID: <000801c6bd58$e2440c80$6cf8d656@Claude>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/009090a7/attachment.pl 

From f.harrell at vanderbilt.edu  Fri Aug 11 17:33:36 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 11 Aug 2006 10:33:36 -0500
Subject: [R] logistic discrimination: which chance performance??
In-Reply-To: <006401c6bcf3$5f07cf60$6400a8c0@brungio>
References: <001601c6bcca$edda4c00$6400a8c0@brungio>
	<006401c6bcf3$5f07cf60$6400a8c0@brungio>
Message-ID: <44DCA350.1090108@vanderbilt.edu>

Bruno L. Giordano wrote:
> Well,
> If posting a possible solution to one's own problem is not part of the
> netiquette of this list please correct me.
> 
> Following Titus et al. (1984) one might use Cohen's kappa to have a
> chance-corrected measure of agreement between the original and reproduced
> classification:
> 
> Kappa() in library vcd
> kappa2() in library irr
> ckappa() in library psy
> cohen.kappa() in library concord......
> 
>     Bruno
> 
> Kimberly Titus; James A. Mosher; Byron K. Williams (1984), Chance-corrected 
> Classification for Use in Discriminant Analysis: Ecological Applications, 
> American Midland Naturalist, 111(1),1-7.
> 
> 
> ----- Original Message ----- 
> From: "Bruno L. Giordano" <bruno.giordano at music.mcgill.ca>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, August 10, 2006 6:18 PM
> Subject: [R] logistic discrimination: which chance performance??
> 
> 
>> Hello,
>> I am using logistic discriminant analysis to check whether a known
>> classification Yobs can be predicted by few continuous variables X.
>>
>> What I do is to predict class probabilities with multinom() in nnet(),
>> obtaining a predicted classification Ypred and then compute the percentage
>> P(obs) of objects classified the same in Yobs and Ypred.
>>
>> My problem now is to figure out whether P(obs) is significantly higher
>> than
>> chance.

The most powerful approach, and one that is automatically corrected for 
chance, is to use the likelihood ratio test for the global null 
hypothesis for the whole model.

With classification proportions you not only lose power and have trouble 
correcting for chance, but you have arbitrariness in what constitutes a 
positive prediction.

Frank Harrell

>>
>> I opted for a crude permutation approach: compute P(perm) over 10000
>> random
>> permutations of Yobs (i.e., refit the multinom() model 10000 times
>> randomly
>> permuting Yobs) and consider P(obs) as significantly higher than chance if
>> higher than the 95th percentile of the P(perm) distribution.
>>
>> Now, the problem is that the mode of P(perm) is always really close to
>> P(obs), e.g., if P(obs)=1 (perfect discrimination) also the most likely
>> P(perm) value is 1!!!
>>
>> I figured out that this is due to the fact that, with my data, randomly
>> permuted classifications are highly likely to strongly agree with the
>> observed classification Yobs, but, probably since my machine learning
>> background is almost 0, I am kind of lost about how to proceed at this
>> point.
>>
>> I would greatly appreciate a comment on this.
>>
>> Thanks
>>    Bruno
>>
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Bruno L. Giordano, Ph.D.
>> CIRMMT
>> Schulich School of Music, McGill University
>> 555 Sherbrooke Street West
>> Montr?al, QC H3A 1E3
>> Canada
>> http://www.music.mcgill.ca/~bruno/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From drnevich at uiuc.edu  Fri Aug 11 17:46:41 2006
From: drnevich at uiuc.edu (Jenny Drnevich)
Date: Fri, 11 Aug 2006 10:46:41 -0500
Subject: [R] [BioC] problem loading affycoretools (more details)
In-Reply-To: <6.2.1.2.2.20060810122323.03661e98@express.cites.uiuc.edu>
References: <6.2.1.2.2.20060810122323.03661e98@express.cites.uiuc.edu>
Message-ID: <6.2.1.2.2.20060810150725.03662e80@express.cites.uiuc.edu>

Hi again,

I have been playing around with the order of loading packages, and as far 
as I can tell, there's nothing specific with affycoretools that's causing 
my Rgui to crash (i.e., shuts down and the Microsoft 'please send error 
report' box pops up). Instead, it has something to do with the order & type 
of packages that are loaded that add items to the menu bar by themselves, 
or adding more options under 'Vignettes'. Specific combinations of any 3 
of: affycoretools, RWinEdt, affyPLM or affylmGUI can cause the crash. So 
far, I haven't been able to get it to crash without affycoretools, even if 
I load all the dependencies by themselves, but that may just be because I 
don't know of another package that adds to the menu bar.  After playing 
around with this for a while, even combinations of two packages (that 
loaded fine before) started to crash the Rgui until I rebooted my PC. I 
remember having similar sorts of trouble a while back when manually 
downloading zipped packages and installing them via the menu - after doing 
a few, clicking on the menu again in the same R session would cause the 
Rgui to crash. As this may not be specific to Bioconductor (except that you 
may not put so many items in the menu unless you're using BioC packages), 
I'm cross posting to the R mailing list.

Cheers,
Jenny


At 12:37 PM 8/10/2006, Jenny Drnevich wrote:
>Hi Jim & all,
>
>I'm having trouble getting affycoretools to work. I apparently had
>downloaded & installed affycoretools before, but had never used it. When I
>tried to load it today, I got an error message saying I was missing a
>dependent package, GOstats. So I used biocLite() to download & install
>GOstats & it's dependencies. Then when I tried to load affycoretools, I
>crashed my Rgui. I restarted the Rgui a few times in different ways, and
>each time I tried to load affycoretools, it crashed the Rgui. So then I
>went and deleted R_HOME/library/affycoretools and tried using biocLite() to
>get it again. This time it didn't crash, but it gave a warning message
>about it not containing a vignette (see below for output & sessionInfo). I
>checked on bioconductor.org, and there is a vignette for affycoretools. I
>haven't tried any of the functions yet, but I'm worried that something's
>amiss. Any suggestions on what to check or reload?
>
>Thanks,
>Jenny
>
>P.S. It was great to finally meet many of you at the BioC2006 conference!
>
>
>R : Copyright 2006, The R Foundation for Statistical Computing
>Version 2.3.1 (2006-06-01)
>ISBN 3-900051-07-0
>
>R is free software and comes with ABSOLUTELY NO WARRANTY.
>You are welcome to redistribute it under certain conditions.
>Type 'license()' or 'licence()' for distribution details.
>
>    Natural language support but running in an English locale
>
>R is a collaborative project with many contributors.
>Type 'contributors()' for more information and
>'citation()' on how to cite R or R packages in publications.
>
>Type 'demo()' for some demos, 'help()' for on-line help, or
>'help.start()' for an HTML browser interface to help.
>Type 'q()' to quit R.
>
>Loading required package: affy
>Loading required package: Biobase
>Loading required package: tools
>
>Welcome to Bioconductor
>
>
>      Vignettes contain introductory material.
>
>      To view, simply type 'openVignette()' or start with 'help(Biobase)'.
>
>      For details on reading vignettes, see the openVignette help page.
>
>
>Loading required package: affyio
>Loading required package: affydata
>Loading required package: Biobase
>Loading required package: gcrma
>Loading required package: matchprobes
>Loading required package: reposTools
>Loading required package: simpleaffy
>Loading required package: genefilter
>Loading required package: survival
>Loading required package: splines
>Welcome to 'affyQCReport' V 1.6-1
>Further information available at: www.bifix.org
>        mailto: craig.parman at bifix.org
>Loading required package: ade4
>Loading required package: scatterplot3d
>
>  > source("http://bioconductor.org/biocLite.R")
>  > biocLite("affycoretools")
>
>Running getBioC version 0.1.6 with R version 2.3.1
>Running biocinstall version 1.8.4 with R version 2.3.1
>Your version of R requires version 1.8 of Bioconductor.
>trying URL
>'http://bioconductor.org/packages/1.8/bioc/bin/windows/contrib/2.3/affycoretools_1.4.0.zip'
>Content type 'application/zip' length 10626563 bytes
>opened URL
>downloaded 10377Kb
>
>package 'affycoretools' successfully unpacked and MD5 sums checked
>
>The downloaded packages are in
>          C:\Documents and Settings\drnevich\Local
>Settings\Temp\RtmpfRnlqb\downloaded_packages
>updating HTML package descriptions
>
>  > library(affycoretools)
>
>Loading required package: GOstats
>Loading required package: graph
>Loading required package: Ruuid
>Loading required package: GO
>Loading required package: annotate
>Loading required package: RBGL
>Loading required package: xtable
>
>Attaching package: 'xtable'
>
>
>          The following object(s) are masked from package:graph :
>
>           label
>
>Loading required package: multtest
>Loading required package: Category
>Loading required package: KEGG
>Loading required package: hgu95av2
>Warning message:
>affycoretools contains no vignette, nothing is added to the menu bar in:
>addVigs2WinMenu("affycoretools")
>
>  > sessionInfo()
>Version 2.3.1 (2006-06-01)
>i386-pc-mingw32
>
>attached base packages:
>[1] "splines"   "tools"     "methods"   "stats"     "graphics"  "grDevices"
>[7] "utils"     "datasets"  "base"
>
>other attached packages:
>affycoretools       GOstats      Category      hgu95av2          KEGG
>        "1.4.0"       "1.6.0"       "1.4.1"      "1.12.0"      "1.12.0"
>       multtest        xtable          RBGL      annotate            GO
>       "1.10.2"       "1.3-2"       "1.8.1"      "1.10.0"      "1.12.0"
>          graph         Ruuid         made4 scatterplot3d          ade4
>       "1.10.4"      "1.10.0"       "1.6.0"      "0.3-24"       "1.4-1"
>   affyQCReport    simpleaffy    genefilter      survival    reposTools
>       "1.10.0"       "2.6.0"      "1.10.1"        "2.26"      "1.10.0"
>        affyPLM         gcrma   matchprobes      affydata          affy
>        "1.8.0"       "2.4.1"       "1.4.0"       "1.8.0"      "1.10.0"
>         affyio       Biobase         limma       RWinEdt
>        "1.0.0"      "1.10.0"       "2.7.2"       "1.7-4"
>  >
>
>
>Jenny Drnevich, Ph.D.
>
>Functional Genomics Bioinformatics Specialist
>W.M. Keck Center for Comparative and Functional Genomics
>Roy J. Carver Biotechnology Center
>University of Illinois, Urbana-Champaign
>
>330 ERML
>1201 W. Gregory Dr.
>Urbana, IL 61801
>USA
>
>ph: 217-244-7355
>fax: 217-265-5066
>e-mail: drnevich at uiuc.edu
>
>_______________________________________________
>Bioconductor mailing list
>Bioconductor at stat.math.ethz.ch
>https://stat.ethz.ch/mailman/listinfo/bioconductor
>Search the archives: 
>http://news.gmane.org/gmane.science.biology.informatics.conductor

Jenny Drnevich, Ph.D.

Functional Genomics Bioinformatics Specialist
W.M. Keck Center for Comparative and Functional Genomics
Roy J. Carver Biotechnology Center
University of Illinois, Urbana-Champaign

330 ERML
1201 W. Gregory Dr.
Urbana, IL 61801
USA

ph: 217-244-7355
fax: 217-265-5066
e-mail: drnevich at uiuc.edu


From SND at bas.ac.uk  Fri Aug 11 17:56:06 2006
From: SND at bas.ac.uk (Sara-Jane Dunn)
Date: Fri, 11 Aug 2006 16:56:06 +0100
Subject: [R] Colour-coding intervals on a line
Message-ID: <s4dcb6c0.058@bsnw.nerc-bas.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/b1d71986/attachment.pl 

From rich at u.washington.edu  Fri Aug 11 18:05:58 2006
From: rich at u.washington.edu (Rich Townsend)
Date: Fri, 11 Aug 2006 09:05:58 -0700
Subject: [R] optim error
In-Reply-To: <e47808320608100707t3e432c9ao776edf28e499827c@mail.gmail.com>
Message-ID: <004701c6bd60$08f26050$099f8e8c@MERGANSER>

If you have an idea of the ranges of your parameters, I've found that the
DEoptim package has been excellent for finding good starting values for very
large likelihood equations, then finishing it up with the optim() to get the
final estimates. 

Rich 

School of Aquatic & Fishery Sciences
University of Washington
www.cbr.washington.edu


> -----Original Message-----
> From: Vaidotas Zemlys [mailto:mpiktas at gmail.com] 
> Sent: Thursday, August 10, 2006 7:07 AM
> To: Frank Black
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] optim error
> 
> 
> Hi,
> 
> On 8/9/06, Frank Black <fb572 at hotmail.com> wrote:
> > Dear all,
> >
> > There have been one or two questions posted to the list 
> regarding the 
> > optim error "non-finite finite-difference value [4]."  The error 
> > apparently means that the 4th element of the gradient is 
> non-finite.  
> > My question is what
> > part(s) of my program should I fiddle with in an attempt to fix it?
> > Starting values?  Something in the log-likelihood itself?  
> Perhaps the data
> > (which is generated)?  Any thoughts would be greatly appreciated.
> >
> 
> Use Nelder-Mead algorithm for finding apropriate starting 
> values. This algorithm does not use gradients, so you will 
> not aforementioned error. After Nelder-Mead you can try again 
> with gradient methods, like BFGS. If that does not help, try 
> scaling your data. Optim behaves better (IMHO) when all 
> parameters are of the same order. If you do not need hessian, 
> and BFGS fails, use only Nelder-Mead, it will at least give 
> you something.
> 
> Vaidotas Zemlys
> --
> Doctorate student, Vilnius University 
> http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
> 
> 
>


From ypeng at math.mun.ca  Fri Aug 11 18:12:19 2006
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Fri, 11 Aug 2006 13:42:19 -0230
Subject: [R] rpvm/snow packages on a cluster with dual-processor machines
Message-ID: <44DCAC63.9040301@math.mun.ca>

Hi,

does anybody know how to use the dual processors in the machines of a cluster? I am using R with rpvm and snow packages. I usually start pvm daemon and add host machines first, and then run R to start my computing work. But I find that only one processor in each machine is used in this way and the other one always stays idle. Is there any simple way to tell pvm to use the two processors at the same time? In other words, I would like to see two copies of R running on each machine's two processors when using pvm. Any hints/help are greatly appreciated.

Paul.


From ruser2006 at yahoo.com  Fri Aug 11 18:43:01 2006
From: ruser2006 at yahoo.com (r user)
Date: Fri, 11 Aug 2006 09:43:01 -0700 (PDT)
Subject: [R] Getting summary.lm to include data for coefficients that are
	NAs?
Message-ID: <20060811164301.83324.qmail@web37013.mail.mud.yahoo.com>

Is there a way to get the following code to include
liens where the coefficients are ?NA??

((summary(reg))$coefficients)

explanation:

Using a loop, I am running regressions on several
?subsets? of ?data1?.

?reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) )?

My regression has 10 independent variables, and I
therefore expect 11 coefficients.
After each regression, I wish to save the coefficients
and standard errors of the coefficients in a table
with 22 columns.

I successfully extract the coefficients using the
following code:
?reg$coefficients?

I attempt to extract the standard erros using :

aperm((summary(reg))$coefficients)[2,]

((summary(reg))$coefficients)

My problem:
For some of my subsets, I am missing data for one or
more of the independent variables.  This of course
causes the coefficients and standard erros for this
variable to be ?NA?.

Is there a way to include the NA standard errors, so
that I have the same number of standard erros and
coefficients for each regression, and can then store
the coefficients and standard erros in my table of 22
columns?


From rdiaz at cnio.es  Fri Aug 11 18:54:04 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 11 Aug 2006 18:54:04 +0200
Subject: [R] rpvm/snow packages on a cluster with dual-processor machines
In-Reply-To: <44DCAC63.9040301@math.mun.ca>
References: <44DCAC63.9040301@math.mun.ca>
Message-ID: <200608111854.04425.rdiaz@cnio.es>

Dear Paul,

I have no direct experience with rpvm, but doing it with rmpi is a piece of 
cake. I could provide you with some hints if you want. (I am tempted to ask 
why you are using PVM instead of MPI, but this might be the wrong question). 

Best,

R.

On Friday 11 August 2006 18:12, Paul Y. Peng wrote:
> Hi,
>
> does anybody know how to use the dual processors in the machines of a
> cluster? I am using R with rpvm and snow packages. I usually start pvm
> daemon and add host machines first, and then run R to start my computing
> work. But I find that only one processor in each machine is used in this
> way and the other one always stays idle. Is there any simple way to tell
> pvm to use the two processors at the same time? In other words, I would
> like to see two copies of R running on each machine's two processors when
> using pvm. Any hints/help are greatly appreciated.
>
> Paul.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From ypeng at math.mun.ca  Fri Aug 11 19:21:02 2006
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Fri, 11 Aug 2006 14:51:02 -0230
Subject: [R] rpvm/snow packages on a cluster with dual-processor machines
In-Reply-To: <200608111854.04425.rdiaz@cnio.es>
References: <44DCAC63.9040301@math.mun.ca> <200608111854.04425.rdiaz@cnio.es>
Message-ID: <44DCBC7E.7030902@math.mun.ca>

Hi Ramon,

please let me know how you achieve this with rmpi. I use PVM simply because I picked it up first and it worked well for me. If MPI is the only way to make use the two processors, I will find out whether it is available or works in our cluster. Thanks a lot for your response.

Regards,
Paul.


Ramon Diaz-Uriarte wrote:
> Dear Paul,
> 
> I have no direct experience with rpvm, but doing it with rmpi is a piece of 
> cake. I could provide you with some hints if you want. (I am tempted to ask 
> why you are using PVM instead of MPI, but this might be the wrong question). 
> 
> Best,
> 
> R.
> 
> On Friday 11 August 2006 18:12, Paul Y. Peng wrote:
>> Hi,
>>
>> does anybody know how to use the dual processors in the machines of a
>> cluster? I am using R with rpvm and snow packages. I usually start pvm
>> daemon and add host machines first, and then run R to start my computing
>> work. But I find that only one processor in each machine is used in this
>> way and the other one always stays idle. Is there any simple way to tell
>> pvm to use the two processors at the same time? In other words, I would
>> like to see two copies of R running on each machine's two processors when
>> using pvm. Any hints/help are greatly appreciated.
>>
>> Paul.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented, minimal,
>> self-contained, reproducible code.
>


From mmanfrin at ulb.ac.be  Fri Aug 11 19:25:01 2006
From: mmanfrin at ulb.ac.be (Max Manfrin)
Date: Fri, 11 Aug 2006 19:25:01 +0200
Subject: [R] How to speed up nested for loop computations
In-Reply-To: <644e1f320608110340x180e5790g209cfdbb8d4b2b4c@mail.gmail.com>
References: <A07FC905-D0C3-4B76-B22F-258035727FA6@ulb.ac.be>
	<644e1f320608100946r2dc8277dv38f2f4645eca7d18@mail.gmail.com>
	<14A0D8CF-9AFA-42EB-8AE6-A87BC06FA6D2@ulb.ac.be>
	<644e1f320608110340x180e5790g209cfdbb8d4b2b4c@mail.gmail.com>
Message-ID: <9282D8BB-9337-4DA7-A314-BC514E56D9CF@ulb.ac.be>

On 11 Aug 2006, at 12:40, jim holtman wrote:

> In your split, you used the dataframe.  What you want to do is to  
> split the row numbers to give the list of indices.  Once you have  
> dont this, you can 'lapply' this list of indices to a function.

Once again I have to thank you, because I was able to finally code  
something that works (except for an error that I'll explain below)  
and is very fast.

The new R code looks like this:

--- BEGIN CODE ---

optimal_values<-read.table("optimal_values_100.txt",header=TRUE)
resPIR2OPT<-read.table("parallel_independent_2- 
opt_100_100.txt",header=TRUE)
resSEQ2OPT<-read.table("sequential_2-opt_100_800.txt",header=TRUE)
resSEQ22OPT<-read.table("sequential2_2-opt_100_100.txt",header=TRUE)
resFC1x102OPT<-read.table("fc.1.x.10_2-opt_100_100.txt",header=TRUE)
resFC26102OPT<-read.table("fc.2.6.10_2-opt_100_100.txt",header=TRUE)
resFC27102OPT<-read.table("fc.2.7.10_2-opt_100_100.txt",header=TRUE)
resFC28102OPT<-read.table("fc.2.8.10_2-opt_100_100.txt",header=TRUE)
resFC29102OPT<-read.table("fc.2.9.10_2-opt_100_100.txt",header=TRUE)
resFC36102OPT<-read.table("fc.3.6.10_2-opt_100_100.txt",header=TRUE)
resFC37102OPT<-read.table("fc.3.7.10_2-opt_100_100.txt",header=TRUE)
resFC38102OPT<-read.table("fc.3.8.10_2-opt_100_100.txt",header=TRUE)
resFC39102OPT<-read.table("fc.3.9.10_2-opt_100_100.txt",header=TRUE)
resHC1x102OPT<-read.table("hc.1.x.10_2-opt_100_100.txt",header=TRUE)
resHC26102OPT<-read.table("hc.2.6.10_2-opt_100_100.txt",header=TRUE)
resHC27102OPT<-read.table("hc.2.7.10_2-opt_100_100.txt",header=TRUE)
resHC28102OPT<-read.table("hc.2.8.10_2-opt_100_100.txt",header=TRUE)
resHC29102OPT<-read.table("hc.2.9.10_2-opt_100_100.txt",header=TRUE)
resHC36102OPT<-read.table("hc.3.6.10_2-opt_100_100.txt",header=TRUE)
resHC37102OPT<-read.table("hc.3.7.10_2-opt_100_100.txt",header=TRUE)
resHC38102OPT<-read.table("hc.3.8.10_2-opt_100_100.txt",header=TRUE)
resHC39102OPT<-read.table("hc.3.9.10_2-opt_100_100.txt",header=TRUE)
resRW1x102OPT<-read.table("rw.1.x.10_2-opt_100_100.txt",header=TRUE)
resRW26102OPT<-read.table("rw.2.6.10_2-opt_100_100.txt",header=TRUE)
resRW27102OPT<-read.table("rw.2.7.10_2-opt_100_100.txt",header=TRUE)
resRW28102OPT<-read.table("rw.2.8.10_2-opt_100_100.txt",header=TRUE)
resRW29102OPT<-read.table("rw.2.9.10_2-opt_100_100.txt",header=TRUE)
resRW36102OPT<-read.table("rw.3.6.10_2-opt_100_100.txt",header=TRUE)
resRW37102OPT<-read.table("rw.3.7.10_2-opt_100_100.txt",header=TRUE)
resRW38102OPT<-read.table("rw.3.8.10_2-opt_100_100.txt",header=TRUE)
resRW39102OPT<-read.table("rw.3.9.10_2-opt_100_100.txt",header=TRUE)
resUR1x102OPT<-read.table("ur.1.x.10_2-opt_100_100.txt",header=TRUE)
resUR26102OPT<-read.table("ur.2.6.10_2-opt_100_100.txt",header=TRUE)
resUR27102OPT<-read.table("ur.2.7.10_2-opt_100_100.txt",header=TRUE)
resUR28102OPT<-read.table("ur.2.8.10_2-opt_100_100.txt",header=TRUE)
resUR29102OPT<-read.table("ur.2.9.10_2-opt_100_100.txt",header=TRUE)
resUR36102OPT<-read.table("ur.3.6.10_2-opt_100_100.txt",header=TRUE)
resUR37102OPT<-read.table("ur.3.7.10_2-opt_100_100.txt",header=TRUE)
resUR38102OPT<-read.table("ur.3.8.10_2-opt_100_100.txt",header=TRUE)
resUR39102OPT<-read.table("ur.3.9.10_2-opt_100_100.txt",header=TRUE)

res<-rbind 
(resFC1x102OPT,resFC26102OPT,resFC27102OPT,resFC28102OPT,resFC29102OPT,r 
esFC36102OPT,resFC37102OPT,resFC38102OPT,resFC39102OPT,resRW1x102OPT,res 
RW26102OPT,resRW27102OPT,resRW28102OPT,resRW29102OPT,resRW36102OPT,resRW 
37102OPT,resRW38102OPT,resRW39102OPT,resHC1x102OPT,resHC26102OPT,resHC27 
102OPT,resHC28102OPT,resHC29102OPT,resHC36102OPT,resHC37102OPT,resHC3810 
2OPT,resHC39102OPT,resUR1x102OPT,resUR26102OPT,resUR27102OPT,resUR28102O 
PT,resUR29102OPT,resUR36102OPT,resUR37102OPT,resUR38102OPT,resUR39102OPT 
,resPIR2OPT,resSEQ2OPT,resSEQ22OPT)

linstance<-levels(res$instance)

res.split<-split(1:nrow(res), list(res$instance, res$try, res 
$idalgo), drop=TRUE)

min.list <- lapply(res.split, function(x){
         x[match(min(res$best[x]), res$best[x])]
         })

# matches return the first among all the values with min best!!!
# so is not the one with minimal time

min.vector <- unlist(min.list)

bestalgo<-res[min.vector,]

bestalgo.split <- split(1:nrow(bestalgo), bestalgo$instance, drop=TRUE)

for (i in (1:length(bestalgo.split)))
{
         bestalgo.vector <- unlist(bestalgo.split[i])
         bestalgo.temp <- bestalgo[bestalgo.vector,]
         l<-split(bestalgo.temp$best,bestalgo.temp$idalgo)

         epsfile=paste(linstance[i],"_100_nolim.eps",sep="")
         postscript(file=epsfile,onefile=TRUE,horizontal=TRUE)
         par(mar=c(5,5,5,3),cex.axis=0.7,las=2,mgp=c(4, 1, 0))
         title_plot=paste("100 iterations - instance ",linstance 
[i],sep="")
         boxplot(l,xlab="",ylab="solution value",names=c(levels 
(bestalgo$idalgo)),main=title_plot,yaxt="n",ylim=c(optimal_values 
[optimal_values$instance==linstance[i],]$optimum,max(bestalgo.temp 
$best)))
         axis(2, seq(from=optimal_values[optimal_values 
$instance==linstance[i],]$optimum,to=max(bestalgo.temp 
$best),length.out=10))
         abline(h=optimal_values[optimal_values$instance==linstance 
[i],]$optimum)
         grid(nx=0, ny=55,col="gray5")
         dev.off()
}

--- END CODE ---

res contains the results of the experiments on 4 problem instances.  
The code produces the boxplots for the first 3 of them and give an  
errore on instance number 4 when it should execute the boxplot  
command. The error is the following:

 > boxplot(l,xlab="",ylab="solution value",names=c(levels(bestalgo 
$idalgo)),main="title_plot",ylim=c(optimal_values[optimal_values 
$instance==linstance[4],]$optimum,max(bestalgo.temp$best)))
Error in if (any(out[nna])) stats[c(1, 5)] <- range(x[!out], na.rm =  
TRUE) :
	missing value where TRUE/FALSE needed
In addition: Warning message:
NAs produced by integer overflow in: x[floor(d)] + x[ceiling(d)]



I removed the ylim to see if something changed, but the error is  
still there...



 > boxplot(l,xlab="",ylab="solution value",names=c(levels(bestalgo 
$idalgo)),main="title_plot")
Error in if (any(out[nna])) stats[c(1, 5)] <- range(x[!out], na.rm =  
TRUE) :
	missing value where TRUE/FALSE needed
In addition: Warning message:
NAs produced by integer overflow in: x[floor(d)] + x[ceiling(d)]

 > str(l)
List of 39
$ FC.1.x.10-2opt: int [1:30] 1185996137 1186007112 1186410641  
1187064761 1186282173 1186282173 1185996137 1187148446 1188080133  
1186282173 ...
$ FC.2.6.10-2opt: int [1:30] 1186282173 1185996137 1187179912  
1186282173 1186282173 1186369404 1185996137 1185996137 1186369404  
1186282173 ...
$ FC.2.7.10-2opt: int [1:30] 1186282173 1185996137 1185996137  
1187864667 1187717114 1186282173 1185996137 1186369404 1186336586  
1185996137 ...
$ FC.2.8.10-2opt: int [1:30] 1185996137 1186282173 1185996137  
1186282173 1185996137 1185996137 1186282173 1185996137 1187713774  
1186007112 ...
$ FC.2.9.10-2opt: int [1:30] 1186282173 1187071224 1186369404  
1186007112 1187252903 1186282173 1186282173 1185996137 1186007112  
1186412178 ...
$ FC.3.6.10-2opt: int [1:30] 1187064761 1186007112 1186812662  
1186231612 1186369404 1185996137 1186441298 1186886951 1186207793  
1185996137 ...
$ FC.3.7.10-2opt: int [1:30] 1186462444 1186027320 1186526792  
1186007112 1186455716 1186336586 1186123369 1185996137 1186733089  
1187033739 ...
$ FC.3.8.10-2opt: int [1:30] 1186052259 1187302984 1186202613  
1186282173 1186052259 1186648860 1186579169 1187252903 1186819027  
1186459107 ...
$ FC.3.9.10-2opt: int [1:30] 1186420877 1186475071 1186808113  
1187187542 1186007112 1186733089 1186373891 1186007112 1186007112  
1186305438 ...
$ RW.1.x.10-2opt: int [1:30] 1186369404 1186007112 1185996137  
1185996137 1185996137 1185996137 1186282173 1186055449 1187148446  
1186007112 ...
$ RW.2.6.10-2opt: int [1:30] 1186353513 1185996137 1187252903  
1186369404 1185996137 1186282173 1186282173 1186373891 1186282173  
1187695581 ...
$ RW.2.7.10-2opt: int [1:30] 1185996137 1185996137 1186282173  
1186007112 1186336586 1185996137 1186282173 1185996137 1187066018  
1186282173 ...
$ RW.2.8.10-2opt: int [1:30] 1186336586 1185996137 1186369404  
1185996137 1186129090 1186378509 1186266654 1186336586 1185996137  
1187189825 ...
$ RW.2.9.10-2opt: int [1:30] 1186420877 1185996137 1186007112  
1186113036 1186336586 1185996137 1185996137 1186407072 1186052259  
1186407072 ...
$ RW.3.6.10-2opt: int [1:30] 1186571192 1186415725 1186291434  
1185996137 1186007112 1186474741 1187084030 1186202995 1186341458  
1186687874 ...
$ RW.3.7.10-2opt: int [1:30] 1186318813 1186007112 1186282173  
1186282173 1185996137 1186398695 1187252982 1186377917 1186407550  
1186570096 ...
$ RW.3.8.10-2opt: int [1:30] 1186207793 1186007112 1186282173  
1186535035 1186205881 1186007112 1186572299 1187314416 1186373891  
1186957225 ...
$ RW.3.9.10-2opt: int [1:30] 1186347814 1186548317 1185996137  
1186369404 1187009587 1187479550 1186730064 1186282173 1185996137  
1186007112 ...
$ HC.1.x.10-2opt: int [1:30] 1185996137 1185996137 1186282173  
1185996137 1186282173 1186369404 1186282173 1185996137 1186369404  
1186336586 ...
$ HC.2.6.10-2opt: int [1:30] 1185996137 1187552951 1185996137  
1185996137 1185996137 1185996137 1187385192 1186369404 1185996137  
1187021722 ...
$ HC.2.7.10-2opt: int [1:30] 1186282173 1187183908 1185996137  
1186373891 1185996137 1186720189 1185996137 1185996137 1187702896  
1186027320 ...
$ HC.2.8.10-2opt: int [1:30] 1186027320 1186282173 1186282173  
1186369404 1186282173 1186282173 1186373891 1187148446 1185996137  
1186336586 ...
$ HC.2.9.10-2opt: int [1:30] 1185996137 1186007112 1186369404  
1186726691 1186282173 1185996137 1186369404 1186369404 1185996137  
1186282173 ...
$ HC.3.6.10-2opt: int [1:30] 1186007112 1186701767 1186007112  
1186757911 1186639768 1185996137 1186380214 1186007112 1187003657  
1186556661 ...
$ HC.3.7.10-2opt: int [1:30] 1185996137 1186007112 1186027320  
1186373891 1186369404 1186702245 1186373891 1185996137 1186402320  
1185996137 ...
$ HC.3.8.10-2opt: int [1:30] 1186386501 1186282173 1185996137  
1185996137 1186096431 1186007112 1186336586 1186213980 1185996137  
1186369404 ...
$ HC.3.9.10-2opt: int [1:30] 1187080500 1186052259 1186420877  
1186658757 1187268873 1186503010 1186052259 1186417092 1187252982  
1186336586 ...
$ UR.1.x.10-2opt: int [1:30] 1185996137 1186121052 1185996137  
1186282173 1185996137 1186282173 1186282173 1185996137 1186282173  
1186078911 ...
$ UR.2.6.10-2opt: int [1:30] 1187693812 1186282173 1186282173  
1185996137 1185996137 1185996137 1186048686 1185996137 1187686384  
1187064761 ...
$ UR.2.7.10-2opt: int [1:30] 1186007112 1186407072 1185996137  
1186282173 1186291836 1186282173 1185996137 1186282173 1186997336  
1186282173 ...
$ UR.2.8.10-2opt: int [1:30] 1186378509 1186903795 1186336586  
1188401386 1186282173 1186007112 1187385192 1185996137 1186282173  
1185996137 ...
$ UR.2.9.10-2opt: int [1:30] 1186007112 1185996137 1186336586  
1186498687 1185996137 1186336586 1186007112 1186202995 1186144595  
1186410641 ...
$ UR.3.6.10-2opt: int [1:30] 1186750319 1185996137 1186736815  
1186336586 1185996137 1186243800 1186346937 1185996137 1187506525  
1187056082 ...
$ UR.3.7.10-2opt: int [1:30] 1186410205 1186007112 1186762512  
1186638875 1186718248 1186235407 1186052259 1185996137 1186407072  
1185996137 ...
$ UR.3.8.10-2opt: int [1:30] 1186336586 1186007112 1186613948  
1185996137 1186282173 1186452282 1186535035 1186380560 1186061771  
1186373891 ...
$ UR.3.9.10-2opt: int [1:30] 1186558839 1186007112 1187209137  
1186923983 1186953819 1186774611 1186963867 1186642445 1186007112  
1186407072 ...
$ PIR-2opt      : int [1:30] 1186007112 1185996137 1186608743  
1185996137 1186585807 1186606396 1188127153 1186407072 1186373891  
1186007112 ...
$ SEQ-2opt      : int [1:30] 1186129090 1186201301 1185996137  
1185996137 1187064761 1186369404 1185996137 1186007112 1185996137  
1187252903 ...
$ SEQ2-2opt     : int [1:30] 1185996137 1187142110 1188212339  
1186919119 1187252903 1188978682 1187581451 1187127822 1188706301  
1188649642 ...


l contains data that "looks" ok: each of the 39 algorithms has his  
own 30 points for the boxplot.

I don't understand the error message. Could anybody help me  
understand what's going on?

----
Max MANFRIN
http://iridia.ulb.ac.be/~mmanfrin/


-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 194 bytes
Desc: This is a digitally signed message part
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060811/c6456a31/attachment.bin 

From rdiaz at cnio.es  Fri Aug 11 19:41:35 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 11 Aug 2006 19:41:35 +0200
Subject: [R] rpvm/snow packages on a cluster with dual-processor machines
In-Reply-To: <44DCBC7E.7030902@math.mun.ca>
References: <44DCAC63.9040301@math.mun.ca> <200608111854.04425.rdiaz@cnio.es>
	<44DCBC7E.7030902@math.mun.ca>
Message-ID: <200608111941.35238.rdiaz@cnio.es>

Dear Paul,

I am leaving right now. I'll send you the info over the weekend. But note that 
I do think it is quite possible to use pvm for your setup. I just have no 
experience with it.

R.

On Friday 11 August 2006 19:21, Paul Y. Peng wrote:
> Hi Ramon,
>
> please let me know how you achieve this with rmpi. I use PVM simply because
> I picked it up first and it worked well for me. If MPI is the only way to
> make use the two processors, I will find out whether it is available or
> works in our cluster. Thanks a lot for your response.
>
> Regards,
> Paul.
>
> Ramon Diaz-Uriarte wrote:
> > Dear Paul,
> >
> > I have no direct experience with rpvm, but doing it with rmpi is a piece
> > of cake. I could provide you with some hints if you want. (I am tempted
> > to ask why you are using PVM instead of MPI, but this might be the wrong
> > question).
> >
> > Best,
> >
> > R.
> >
> > On Friday 11 August 2006 18:12, Paul Y. Peng wrote:
> >> Hi,
> >>
> >> does anybody know how to use the dual processors in the machines of a
> >> cluster? I am using R with rpvm and snow packages. I usually start pvm
> >> daemon and add host machines first, and then run R to start my computing
> >> work. But I find that only one processor in each machine is used in this
> >> way and the other one always stays idle. Is there any simple way to tell
> >> pvm to use the two processors at the same time? In other words, I would
> >> like to see two copies of R running on each machine's two processors
> >> when using pvm. Any hints/help are greatly appreciated.
> >>
> >> Paul.
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html and provide commented,
> >> minimal, self-contained, reproducible code.

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From billk at metrumrg.com  Fri Aug 11 20:15:42 2006
From: billk at metrumrg.com (Bill Knebel)
Date: Fri, 11 Aug 2006 14:15:42 -0400
Subject: [R] Creating SAS transport files
Message-ID: <44DCC94E.7050207@metrumrg.com>

Is there any package in R to create a SAS transport file?  I checked the 
help archive and did not find any references to creating SAS transport 
files in R, only reading them.

Bill

-- 
Bill Knebel, PharmD, Ph.D.
Principal Scientist
Metrum Research Group
2 Tunxis Road
Suite 112
Tariffville, CT 06081
email: billk at metrumrg.com
tel: (860) 930-1370


From kevin.thorpe at utoronto.ca  Fri Aug 11 20:23:13 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 11 Aug 2006 14:23:13 -0400
Subject: [R] Creating SAS transport files
In-Reply-To: <44DCC94E.7050207@metrumrg.com>
References: <44DCC94E.7050207@metrumrg.com>
Message-ID: <44DCCB11.1080209@utoronto.ca>

Bill Knebel wrote:
> Is there any package in R to create a SAS transport file?  I checked the 
> help archive and did not find any references to creating SAS transport 
> files in R, only reading them.
> 
> Bill
> 

I have used write.foreign in the foreign package.  This
produces a text data file and a SAS program to read the
data into SAS.

It has worked well for my uses.  As for exporting to a
transport file directly, I do not know.

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297


From haynesm at cfr.nichd.nih.gov  Fri Aug 11 20:29:34 2006
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD) [E])
Date: Fri, 11 Aug 2006 14:29:34 -0400
Subject: [R] Creating SAS transport files
In-Reply-To: <44DCC94E.7050207@metrumrg.com>
Message-ID: <DC6119F74942094BBBB71024B83B995249D514@NIHCESMLBX7.nih.gov>

See the argument package="SAS" to the function write.foreign() in the
package foreign.

HTH 

Maurice Haynes


-----Original Message-----
From: Bill Knebel [mailto:billk at metrumrg.com] 
Sent: Friday, August 11, 2006 2:16 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Creating SAS transport files


Is there any package in R to create a SAS transport file?  I checked the

help archive and did not find any references to creating SAS transport 
files in R, only reading them.

Bill

-- 
Bill Knebel, PharmD, Ph.D.
Principal Scientist
Metrum Research Group
2 Tunxis Road
Suite 112
Tariffville, CT 06081
email: billk at metrumrg.com
tel: (860) 930-1370

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Aug 11 20:40:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Aug 2006 14:40:00 -0400
Subject: [R] Multiple density curves
In-Reply-To: <971536df0608110501u6cf62035hd8eb73957b42f1d1@mail.gmail.com>
References: <c2f237040608100907t44dba1dax3c34a0178b73ddee@mail.gmail.com>
	<971536df0608110448r2ec8576bt3b145fd228020cb2@mail.gmail.com>
	<971536df0608110501u6cf62035hd8eb73957b42f1d1@mail.gmail.com>
Message-ID: <971536df0608111140l5d3a75can11cb0743d1aa5e8c@mail.gmail.com>

Here is one more solution.  This one uses lattice.  Its a bit shorter
than the classic graphics solution.  In the classic graphics version we used
shading and color to distinguish the bars; however, grid, and therefore
lattice, do not easily support shading (its possible to simulate it using low
level vector graphics but that's beyond the scope of this) so we use
width (lwd), style (lty) and colour (col) to distinguish them.  Also note
that the for loop iterates over the groups since the lattice histogram function
does not use the groups= argument of lattice's xyplot.

library(lattice)
# data
DF <- structure(list(SEQ = structure(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11, 12, 13), .Label = c("A", "B", "C", "D", "E", "F", "G", "H",
"I", "J", "K", "L", "M"), class = "factor"), A1 = c(532.5, 25.5,
265.2, 245.55, 546.52, 243.25, 452.55, 15.14, 543.4, 54.4, 646.5,
645.4, 646.54), A2 = c(554.5, 35.5, 522.2, 521.56, 141.52, 32.56,
635.56, 16.54, 646.56, 654.5, 64.54, 614.46, 634.46)), .Names = c("SEQ",
"A1", "A2"), class = "data.frame", row.names = c("1", "2", "3",
"4", "5", "6", "7", "8", "9", "10", "11", "12", "13"))

histogram(~ unlist(DF[,-1]), type = "density",
   panel = function(x, breaks, ...)
     for(j in 2:ncol(DF)) {
        panel.histogram(DF[,j], border = j, lwd = j, lty = j,
		breaks = breaks, col = "transparent", ...)
        panel.densityplot(DF[,j], col = j, ...)
   })

On 8/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The code below was missing the breaks= argument to hist.
> I had not noticed because coincidentally both give the same
> breaks anways thus the following corrected version gives the
> same plot in this case but might not in other cases.
>
> # data
> DF <- structure(list(SEQ = structure(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> 11, 12, 13), .Label = c("A", "B", "C", "D", "E", "F", "G", "H",
> "I", "J", "K", "L", "M"), class = "factor"), A1 = c(532.5, 25.5,
> 265.2, 245.55, 546.52, 243.25, 452.55, 15.14, 543.4, 54.4, 646.5,
> 645.4, 646.54), A2 = c(554.5, 35.5, 522.2, 521.56, 141.52, 32.56,
> 635.56, 16.54, 646.56, 654.5, 64.54, 614.46, 634.46)), .Names = c("SEQ",
> "A1", "A2"), class = "data.frame", row.names = c("1", "2", "3",
> "4", "5", "6", "7", "8", "9", "10", "11", "12", "13"))
>
> # determine breaks and y limits of the combined plot
> breaks <- hist(c(DF$A1, DF$A2), plot = FALSE)$breaks
> ymax1 <- max(hist(DF$A1, breaks = breaks, plot = FALSE)$intensities)
> ymax2 <- max(hist(DF$A2, breaks = breaks, plot = FALSE)$intensities)
> ylim <- c(0, max(ymax1, ymax2))
>
> # draw the two histograms and two densities
> hist(DF$A1, ang = 45, col = "red", ylim = ylim,
>        breaks = breaks, freq = FALSE, density = 10)
> lines(density(DF$A1), col = "red")
> hist(DF$A2, ang = -45, col = "blue", add = TRUE,
>        breaks = breaks, freq = FALSE, density = 10)
> lines(density(DF$A2), col = "blue")
>
>
>
> On 8/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > From your description I assume you want both histograms
> > and the densities all on the same chart.  With existing R
> > graphics I am not sure that there really is a simple way to
> > do that.
> >
> > That aside, note that the hist function returns a list of
> > components that includes
> >
> > - breaks, defining the breakpoints of the histogram
> > - intensities defining the heights of the histogram bars
> >
> > We can use these two to determine the breaks and y limits
> > of the combined plot and then use the breaks= and ylim=
> > arguments of hist to specify them so that both histograms
> > can be drawn on the same chart.  We also use freq=FALSE
> > in the hist calls to draw intensities rather than counts.  On
> > the second hist call we use add=TRUE to cause it to be drawn
> > on the existing plot.
> >
> > The other problem is to distinguish the superimposition of
> > the bars and that can be handled by using shading lines of
> > different colors and angles using the col= and angle= and
> > density= arguments of hist.
> >
> >
> > # data
> > DF <- structure(list(SEQ = structure(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> > 11, 12, 13), .Label = c("A", "B", "C", "D", "E", "F", "G", "H",
> > "I", "J", "K", "L", "M"), class = "factor"), A1 = c(532.5, 25.5,
> > 265.2, 245.55, 546.52, 243.25, 452.55, 15.14, 543.4, 54.4, 646.5,
> > 645.4, 646.54), A2 = c(554.5, 35.5, 522.2, 521.56, 141.52, 32.56,
> > 635.56, 16.54, 646.56, 654.5, 64.54, 614.46, 634.46)), .Names = c("SEQ",
> > "A1", "A2"), class = "data.frame", row.names = c("1", "2", "3",
> > "4", "5", "6", "7", "8", "9", "10", "11", "12", "13"))
> >
> > # determine breaks and y limits of the combined plot
> > breaks <- hist(c(DF$A1, DF$A2), plot = FALSE)$breaks
> > ymax1 <- max(hist(DF$A1, breaks = breaks, plot = FALSE)$intensities)
> > ymax2 <- max(hist(DF$A2, breaks = breaks, plot = FALSE)$intensities)
> > ylim <- c(0, max(ymax1, ymax2))
> >
> > # draw the two histograms and two densities
> > hist(DF$A1, ang = 45, col = "red", ylim = ylim, freq = FALSE, density = 10)
> > lines(density(DF$A1), col = "red")
> > hist(DF$A2, ang = -45, col = "blue", add = TRUE, freq = FALSE, density = 10)
> > lines(density(DF$A2), col = "blue")
> >
> > On 8/10/06, Davendra Sohal <dsohal at gmail.com> wrote:
> > > Hi,
> > >
> > > I am new to R...a recent convert from SAS.
> > > I have a dataset that looks like this:
> > >
> > > SEQ    A1    A2
> > > A    532.5    554.5
> > > B    25.5    35.5
> > > C    265.2    522.2
> > > D    245.55    521.56
> > > E    546.52    141.52
> > > F    243.25    32.56
> > > G    452.55    635.56
> > > H    15.14    16.54
> > > I    543.4    646.56
> > > J    54.4    654.5
> > > K    646.5    64.54
> > > L    645.4    614.46
> > > M    646.54    634.46
> > >
> > > I want to make a histogram each for A1 and A2, with density curves, on the
> > > same plot so that I can see how they overlap.
> > >
> > > Please let me know some simple code for this.
> > >
> > > I looked at ldahist but it was complicated. Anything simpler?
> > >
> > > Thanks a lot,
> > > -DS.
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From Horace.Tso at pgn.com  Fri Aug 11 20:48:06 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 11 Aug 2006 11:48:06 -0700
Subject: [R] An apply and rep question
Message-ID: <s4dc6e80.021@pgn.com>

Hi list,

I'm sure the explanation must be laughably simple to the experts out
there, but I just could figure it out. I have a simple data frame that
looks like,

>head(da.off)
       DDate  OffP
1 2005-01-01 41.23
2 2005-01-02 44.86
3 2005-01-03 44.86
4 2005-01-04 43.01
5 2005-01-05 45.47
6 2005-01-06 48.62

where the first column DDate currently is character, and OffP is
numeric. 

I want to duplicate every row 2 times, so I thought I use apply(),

x <- apply(da.off, 2, rep, each=2) 

The result is a matrix of all character,

 head(x)
  DDate        OffP    
1 "2005-01-01" " 41.23"
1 "2005-01-01" " 41.23"
2 "2005-01-02" " 44.86"
2 "2005-01-02" " 44.86"
3 "2005-01-03" " 44.86"
3 "2005-01-03" " 44.86"

To convert it back to numeric, I did

x <- as.data.frame(x)
x$OffP <- as.numeric(x$OffP)

However, the OffP column didn't convert correctly, a mystery since they
"look" quite alright above. (I know, I know, there seems to be a space
there. But why?)

head(x)
         DDate OffP
1   2005-01-01  150
1.1 2005-01-01  150
2   2005-01-02  202
2.1 2005-01-02  202
3   2005-01-03  202
3.1 2005-01-03  202

Is this the wrong way to use apply or rep? 

Horace


From drnevich at uiuc.edu  Fri Aug 11 20:59:59 2006
From: drnevich at uiuc.edu (Jenny Drnevich)
Date: Fri, 11 Aug 2006 13:59:59 -0500
Subject: [R] [BioC] problem loading affycoretools - solved!
In-Reply-To: <6phodurup6i.fsf@gopher4.fhcrc.org>
References: <6.2.1.2.2.20060810122323.03661e98@express.cites.uiuc.edu>
	<6.2.1.2.2.20060810150725.03662e80@express.cites.uiuc.edu>
	<6phodurup6i.fsf@gopher4.fhcrc.org>
Message-ID: <6.2.1.2.2.20060811135442.0373af48@express.cites.uiuc.edu>

Hi Martin,

The patched version works fine - thanks for pointing it out! As for 
problems installing RWinEdt, do you have WinEdt downloaded and 
installed?  www.winedt.com   I've found it to be a useful editor for R - 
well worth the nominal fee after the trial period has ended.

Cheers,
Jenny

At 01:01 PM 8/11/2006, Martin Morgan wrote:
>Hi Jenny --
>
>This sounds like a problem that is addressed in the 'patched' version
>of 2.3.1, from the CHANGES file:
>
>R 2.3.1 patched
>===============
>
>winMenuAdd() could crash when too many menus were added.  (PR#8961)
>
>The patched version is at CRAN:
>
>R Binaries --> windows --> Base and then hunt fort the link 'r-patched
>snapshot build'.
>
>I haven't been able to test this explicitly with RWinEdt, which does
>not seem to want to install easily on my computer. If this is the
>solution, then perhaps you could forward a 'solved!' note to the list?
>
>Martin
>--
>Bioconductor
>
>Jenny Drnevich <drnevich at uiuc.edu> writes:
>
> > Hi again,
> >
> > I have been playing around with the order of loading packages, and as far
> > as I can tell, there's nothing specific with affycoretools that's causing
> > my Rgui to crash (i.e., shuts down and the Microsoft 'please send error
> > report' box pops up). Instead, it has something to do with the order & 
> type
> > of packages that are loaded that add items to the menu bar by themselves,
> > or adding more options under 'Vignettes'. Specific combinations of any 3
> > of: affycoretools, RWinEdt, affyPLM or affylmGUI can cause the crash. So
> > far, I haven't been able to get it to crash without affycoretools, even if
> > I load all the dependencies by themselves, but that may just be because I
> > don't know of another package that adds to the menu bar.  After playing
> > around with this for a while, even combinations of two packages (that
> > loaded fine before) started to crash the Rgui until I rebooted my PC. I
> > remember having similar sorts of trouble a while back when manually
> > downloading zipped packages and installing them via the menu - after doing
> > a few, clicking on the menu again in the same R session would cause the
> > Rgui to crash. As this may not be specific to Bioconductor (except that 
> you
> > may not put so many items in the menu unless you're using BioC packages),
> > I'm cross posting to the R mailing list.
> >
> > Cheers,
> > Jenny
> >
> >
> > At 12:37 PM 8/10/2006, Jenny Drnevich wrote:
> >>Hi Jim & all,
> >>
> >>I'm having trouble getting affycoretools to work. I apparently had
> >>downloaded & installed affycoretools before, but had never used it. When I
> >>tried to load it today, I got an error message saying I was missing a
> >>dependent package, GOstats. So I used biocLite() to download & install
> >>GOstats & it's dependencies. Then when I tried to load affycoretools, I
> >>crashed my Rgui. I restarted the Rgui a few times in different ways, and
> >>each time I tried to load affycoretools, it crashed the Rgui. So then I
> >>went and deleted R_HOME/library/affycoretools and tried using biocLite() to
> >>get it again. This time it didn't crash, but it gave a warning message
> >>about it not containing a vignette (see below for output & sessionInfo). I
> >>checked on bioconductor.org, and there is a vignette for affycoretools. I
> >>haven't tried any of the functions yet, but I'm worried that something's
> >>amiss. Any suggestions on what to check or reload?
> >>
> >>Thanks,
> >>Jenny
> >>
> >>P.S. It was great to finally meet many of you at the BioC2006 conference!
> >>
> >>
> >>R : Copyright 2006, The R Foundation for Statistical Computing
> >>Version 2.3.1 (2006-06-01)
> >>ISBN 3-900051-07-0
> >>
> >>R is free software and comes with ABSOLUTELY NO WARRANTY.
> >>You are welcome to redistribute it under certain conditions.
> >>Type 'license()' or 'licence()' for distribution details.
> >>
> >>    Natural language support but running in an English locale
> >>
> >>R is a collaborative project with many contributors.
> >>Type 'contributors()' for more information and
> >>'citation()' on how to cite R or R packages in publications.
> >>
> >>Type 'demo()' for some demos, 'help()' for on-line help, or
> >>'help.start()' for an HTML browser interface to help.
> >>Type 'q()' to quit R.
> >>
> >>Loading required package: affy
> >>Loading required package: Biobase
> >>Loading required package: tools
> >>
> >>Welcome to Bioconductor
> >>
> >>
> >>      Vignettes contain introductory material.
> >>
> >>      To view, simply type 'openVignette()' or start with 'help(Biobase)'.
> >>
> >>      For details on reading vignettes, see the openVignette help page.
> >>
> >>
> >>Loading required package: affyio
> >>Loading required package: affydata
> >>Loading required package: Biobase
> >>Loading required package: gcrma
> >>Loading required package: matchprobes
> >>Loading required package: reposTools
> >>Loading required package: simpleaffy
> >>Loading required package: genefilter
> >>Loading required package: survival
> >>Loading required package: splines
> >>Welcome to 'affyQCReport' V 1.6-1
> >>Further information available at: www.bifix.org
> >>        mailto: craig.parman at bifix.org
> >>Loading required package: ade4
> >>Loading required package: scatterplot3d
> >>
> >>  > source("http://bioconductor.org/biocLite.R")
> >>  > biocLite("affycoretools")
> >>
> >>Running getBioC version 0.1.6 with R version 2.3.1
> >>Running biocinstall version 1.8.4 with R version 2.3.1
> >>Your version of R requires version 1.8 of Bioconductor.
> >>trying URL
> >>'http://bioconductor.org/packages/1.8/bioc/bin/windows/contrib/2.3/affyc 
> oretools_1.4.0.zip'
> >>Content type 'application/zip' length 10626563 bytes
> >>opened URL
> >>downloaded 10377Kb
> >>
> >>package 'affycoretools' successfully unpacked and MD5 sums checked
> >>
> >>The downloaded packages are in
> >>          C:\Documents and Settings\drnevich\Local
> >>Settings\Temp\RtmpfRnlqb\downloaded_packages
> >>updating HTML package descriptions
> >>
> >>  > library(affycoretools)
> >>
> >>Loading required package: GOstats
> >>Loading required package: graph
> >>Loading required package: Ruuid
> >>Loading required package: GO
> >>Loading required package: annotate
> >>Loading required package: RBGL
> >>Loading required package: xtable
> >>
> >>Attaching package: 'xtable'
> >>
> >>
> >>          The following object(s) are masked from package:graph :
> >>
> >>           label
> >>
> >>Loading required package: multtest
> >>Loading required package: Category
> >>Loading required package: KEGG
> >>Loading required package: hgu95av2
> >>Warning message:
> >>affycoretools contains no vignette, nothing is added to the menu bar in:
> >>addVigs2WinMenu("affycoretools")
> >>
> >>  > sessionInfo()
> >>Version 2.3.1 (2006-06-01)
> >>i386-pc-mingw32
> >>
> >>attached base packages:
> >>[1] "splines"   "tools"     "methods"   "stats"     "graphics"  "grDevices"
> >>[7] "utils"     "datasets"  "base"
> >>
> >>other attached packages:
> >>affycoretools       GOstats      Category      hgu95av2          KEGG
> >>        "1.4.0"       "1.6.0"       "1.4.1"      "1.12.0"      "1.12.0"
> >>       multtest        xtable          RBGL      annotate            GO
> >>       "1.10.2"       "1.3-2"       "1.8.1"      "1.10.0"      "1.12.0"
> >>          graph         Ruuid         made4 scatterplot3d          ade4
> >>       "1.10.4"      "1.10.0"       "1.6.0"      "0.3-24"       "1.4-1"
> >>   affyQCReport    simpleaffy    genefilter      survival    reposTools
> >>       "1.10.0"       "2.6.0"      "1.10.1"        "2.26"      "1.10.0"
> >>        affyPLM         gcrma   matchprobes      affydata          affy
> >>        "1.8.0"       "2.4.1"       "1.4.0"       "1.8.0"      "1.10.0"
> >>         affyio       Biobase         limma       RWinEdt
> >>        "1.0.0"      "1.10.0"       "2.7.2"       "1.7-4"
> >>  >
> >>
> >>
> >>Jenny Drnevich, Ph.D.
> >>
> >>Functional Genomics Bioinformatics Specialist
> >>W.M. Keck Center for Comparative and Functional Genomics
> >>Roy J. Carver Biotechnology Center
> >>University of Illinois, Urbana-Champaign
> >>
> >>330 ERML
> >>1201 W. Gregory Dr.
> >>Urbana, IL 61801
> >>USA
> >>
> >>ph: 217-244-7355
> >>fax: 217-265-5066
> >>e-mail: drnevich at uiuc.edu
> >>
> >>_______________________________________________
> >>Bioconductor mailing list
> >>Bioconductor at stat.math.ethz.ch
> >>https://stat.ethz.ch/mailman/listinfo/bioconductor
> >>Search the archives:
> >>http://news.gmane.org/gmane.science.biology.informatics.conductor
> >
> > Jenny Drnevich, Ph.D.
> >
> > Functional Genomics Bioinformatics Specialist
> > W.M. Keck Center for Comparative and Functional Genomics
> > Roy J. Carver Biotechnology Center
> > University of Illinois, Urbana-Champaign
> >
> > 330 ERML
> > 1201 W. Gregory Dr.
> > Urbana, IL 61801
> > USA
> >
> > ph: 217-244-7355
> > fax: 217-265-5066
> > e-mail: drnevich at uiuc.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Aug 11 21:01:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Aug 2006 15:01:52 -0400
Subject: [R] An apply and rep question
In-Reply-To: <s4dc6e80.021@pgn.com>
References: <s4dc6e80.021@pgn.com>
Message-ID: <971536df0608111201u4f50b06do7be6d3326cf85c8@mail.gmail.com>

The approach here is to perform the repetition on the indices (or rownames)
rather than on the data frame directly.  Using the builtin data frame BOD
any of the following would work:

BOD[gl(nrow(BOD), 2),]
BOD[rep(1:nrow(BOD), each = 2),]
BOD[rep(rownames(BOD), each = 2),]

On 8/11/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> Hi list,
>
> I'm sure the explanation must be laughably simple to the experts out
> there, but I just could figure it out. I have a simple data frame that
> looks like,
>
> >head(da.off)
>       DDate  OffP
> 1 2005-01-01 41.23
> 2 2005-01-02 44.86
> 3 2005-01-03 44.86
> 4 2005-01-04 43.01
> 5 2005-01-05 45.47
> 6 2005-01-06 48.62
>
> where the first column DDate currently is character, and OffP is
> numeric.
>
> I want to duplicate every row 2 times, so I thought I use apply(),
>
> x <- apply(da.off, 2, rep, each=2)
>
> The result is a matrix of all character,
>
>  head(x)
>  DDate        OffP
> 1 "2005-01-01" " 41.23"
> 1 "2005-01-01" " 41.23"
> 2 "2005-01-02" " 44.86"
> 2 "2005-01-02" " 44.86"
> 3 "2005-01-03" " 44.86"
> 3 "2005-01-03" " 44.86"
>
> To convert it back to numeric, I did
>
> x <- as.data.frame(x)
> x$OffP <- as.numeric(x$OffP)
>
> However, the OffP column didn't convert correctly, a mystery since they
> "look" quite alright above. (I know, I know, there seems to be a space
> there. But why?)
>
> head(x)
>         DDate OffP
> 1   2005-01-01  150
> 1.1 2005-01-01  150
> 2   2005-01-02  202
> 2.1 2005-01-02  202
> 3   2005-01-03  202
> 3.1 2005-01-03  202
>
> Is this the wrong way to use apply or rep?
>
> Horace
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Fri Aug 11 21:05:04 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Aug 2006 15:05:04 -0400
Subject: [R] An apply and rep question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2D97@usctmx1106.merck.com>

The trick is to rep() the index, not the data:

R> dat2 <- dat[rep(1:nrow(dat), each=2), ]
R> dat2
         DDate  OffP
1   2005-01-01 41.23
1.1 2005-01-01 41.23
2   2005-01-02 44.86
2.1 2005-01-02 44.86
3   2005-01-03 44.86
3.1 2005-01-03 44.86
4   2005-01-04 43.01
4.1 2005-01-04 43.01
5   2005-01-05 45.47
5.1 2005-01-05 45.47
6   2005-01-06 48.62
6.1 2005-01-06 48.62

Andy

From: Horace Tso
> 
> Hi list,
> 
> I'm sure the explanation must be laughably simple to the 
> experts out there, but I just could figure it out. I have a 
> simple data frame that looks like,
> 
> >head(da.off)
>        DDate  OffP
> 1 2005-01-01 41.23
> 2 2005-01-02 44.86
> 3 2005-01-03 44.86
> 4 2005-01-04 43.01
> 5 2005-01-05 45.47
> 6 2005-01-06 48.62
> 
> where the first column DDate currently is character, and OffP 
> is numeric. 
> 
> I want to duplicate every row 2 times, so I thought I use apply(),
> 
> x <- apply(da.off, 2, rep, each=2) 
> 
> The result is a matrix of all character,
> 
>  head(x)
>   DDate        OffP    
> 1 "2005-01-01" " 41.23"
> 1 "2005-01-01" " 41.23"
> 2 "2005-01-02" " 44.86"
> 2 "2005-01-02" " 44.86"
> 3 "2005-01-03" " 44.86"
> 3 "2005-01-03" " 44.86"
> 
> To convert it back to numeric, I did
> 
> x <- as.data.frame(x)
> x$OffP <- as.numeric(x$OffP)
> 
> However, the OffP column didn't convert correctly, a mystery 
> since they "look" quite alright above. (I know, I know, there 
> seems to be a space there. But why?)
> 
> head(x)
>          DDate OffP
> 1   2005-01-01  150
> 1.1 2005-01-01  150
> 2   2005-01-02  202
> 2.1 2005-01-02  202
> 3   2005-01-03  202
> 3.1 2005-01-03  202
> 
> Is this the wrong way to use apply or rep? 
> 
> Horace
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From Horace.Tso at pgn.com  Fri Aug 11 21:20:58 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 11 Aug 2006 12:20:58 -0700
Subject: [R] An apply and rep question
Message-ID: <s4dc7636.057@pgn.com>

Thanks Gabor, Andy, and Phil. I learn new trick, particularly the use of
gl().

H.

>>> "Gabor Grothendieck" <ggrothendieck at gmail.com> 8/11/2006 12:01 PM
>>>
The approach here is to perform the repetition on the indices (or
rownames)
rather than on the data frame directly.  Using the builtin data frame
BOD
any of the following would work:

BOD[gl(nrow(BOD), 2),]
BOD[rep(1:nrow(BOD), each = 2),]
BOD[rep(rownames(BOD), each = 2),]

On 8/11/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> Hi list,
>
> I'm sure the explanation must be laughably simple to the experts out
> there, but I just could figure it out. I have a simple data frame
that
> looks like,
>
> >head(da.off)
>       DDate  OffP
> 1 2005-01-01 41.23
> 2 2005-01-02 44.86
> 3 2005-01-03 44.86
> 4 2005-01-04 43.01
> 5 2005-01-05 45.47
> 6 2005-01-06 48.62
>
> where the first column DDate currently is character, and OffP is
> numeric.
>
> I want to duplicate every row 2 times, so I thought I use apply(),
>
> x <- apply(da.off, 2, rep, each=2)
>
> The result is a matrix of all character,
>
>  head(x)
>  DDate        OffP
> 1 "2005-01-01" " 41.23"
> 1 "2005-01-01" " 41.23"
> 2 "2005-01-02" " 44.86"
> 2 "2005-01-02" " 44.86"
> 3 "2005-01-03" " 44.86"
> 3 "2005-01-03" " 44.86"
>
> To convert it back to numeric, I did
>
> x <- as.data.frame(x)
> x$OffP <- as.numeric(x$OffP)
>
> However, the OffP column didn't convert correctly, a mystery since
they
> "look" quite alright above. (I know, I know, there seems to be a
space
> there. But why?)
>
> head(x)
>         DDate OffP
> 1   2005-01-01  150
> 1.1 2005-01-01  150
> 2   2005-01-02  202
> 2.1 2005-01-02  202
> 3   2005-01-03  202
> 3.1 2005-01-03  202
>
> Is this the wrong way to use apply or rep?
>
> Horace
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From andy_liaw at merck.com  Fri Aug 11 21:24:13 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Aug 2006 15:24:13 -0400
Subject: [R] rpvm/snow packages on a cluster with dual-processor machi
 nes
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2D9C@usctmx1106.merck.com>

Caveat: I've only played with this a couple of years ago... 

I believe you can just add each host _twice_ (or as many times as the number
of CPUs at that host) to get both CPUs to work.

Andy

From: Paul Y. Peng
> 
> Hi,
> 
> does anybody know how to use the dual processors in the 
> machines of a cluster? I am using R with rpvm and snow 
> packages. I usually start pvm daemon and add host machines 
> first, and then run R to start my computing work. But I find 
> that only one processor in each machine is used in this way 
> and the other one always stays idle. Is there any simple way 
> to tell pvm to use the two processors at the same time? In 
> other words, I would like to see two copies of R running on 
> each machine's two processors when using pvm. Any hints/help 
> are greatly appreciated.
> 
> Paul.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From austin at botany.utoronto.ca  Fri Aug 11 22:00:00 2006
From: austin at botany.utoronto.ca (Ryan Austin)
Date: Fri, 11 Aug 2006 16:00:00 -0400
Subject: [R] rpvm/snow packages on a cluster with dual-processor machi
 nes
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2D9C@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2D9C@usctmx1106.merck.com>
Message-ID: <44DCE1C0.4020505@botany.utoronto.ca>

Hi,

Adding a node twice gives a duplicate node error.
However, adding the parameter sp=2000 to your pvm hostfile should enable 
dual processors.

Ryan

Liaw, Andy wrote:

>Caveat: I've only played with this a couple of years ago... 
>
>I believe you can just add each host _twice_ (or as many times as the number
>of CPUs at that host) to get both CPUs to work.
>
>Andy
>
>From: Paul Y. Peng
>  
>
>>Hi,
>>
>>does anybody know how to use the dual processors in the 
>>machines of a cluster? I am using R with rpvm and snow 
>>packages. I usually start pvm daemon and add host machines 
>>first, and then run R to start my computing work. But I find 
>>that only one processor in each machine is used in this way 
>>and the other one always stays idle. Is there any simple way 
>>to tell pvm to use the two processors at the same time? In 
>>other words, I would like to see two copies of R running on 
>>each machine's two processors when using pvm. Any hints/help 
>>are greatly appreciated.
>>
>>Paul.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>  
>


From john at emiliem.com  Fri Aug 11 22:05:21 2006
From: john at emiliem.com (John Morrow)
Date: Fri, 11 Aug 2006 13:05:21 -0700
Subject: [R] Auto-save possible in R?
Message-ID: <000001c6bd81$7a190a10$6e4b1e30$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/172383ea/attachment.pl 

From Achim.Zeileis at R-project.org  Fri Aug 11 22:25:12 2006
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Fri, 11 Aug 2006 22:25:12 +0200
Subject: [R] problem with tseries (was unable to restore saved data)
In-Reply-To: <Pine.LNX.4.64.0608090742520.7250@gannet.stats.ox.ac.uk>
References: <af2c8600608081738o4c8c8821t7c976d5f314ad088@mail.gmail.com>
	<Pine.LNX.4.64.0608090742520.7250@gannet.stats.ox.ac.uk>
Message-ID: <20060811222512.469d6393.Achim.Zeileis@R-project.org>

On Wed, 9 Aug 2006 07:58:45 +0100 (BST) Prof Brian Ripley wrote:

> Loading the workspace is done before loading the standard set of
> packages. This appears to be a bug in tseries, for if its namespace
> needs 'time', it should import it from 'stats', and it is not even
> depending on 'stats'.

[...]
 
> > loadNamespace("tseries")
> Error: object 'time' not found whilst loading namespace 'tseries'
> 
> That is a matter for the 'tseries' maintainer (Cc:ed here).  If you
> try library(tseries) at that point you find
> 
> > library(tseries)
> Loading required package: zoo
> Error: object 'aggregate' not found whilst loading namespace 'zoo'
> Error: package 'zoo' could not be loaded
> 
> so package zoo has a similar problem (maintainer Cc:ed).

Thanks for the pointer: I started revising my packages as to properly
declare dependencies, and also did the same for tseries. I'll try to
get them out on CRAN asap.

thx
Z


From matt.burger at web.de  Fri Aug 11 22:44:32 2006
From: matt.burger at web.de (Matthias Burger)
Date: Fri, 11 Aug 2006 22:44:32 +0200
Subject: [R] invisible() - does not return immediately as return() does
Message-ID: <44DCEC30.8090709@web.de>


Hi,

I stumbled across the following (unexpected for me) behavior after
replacing a return() statement in the middle of a function by invisible().

Example:
foo <- function() { cat("before\n"); return(); cat("after\n")}
>foo()
before
NULL

foo2 <- function() { cat("before\n"); invisible(TRUE); cat("after\n")}
>foo2()
before
after

I expected invisible to have the same behavior as return, namely
immediately return execution to the calling environment.

I rechecked ?invisible and ?return
and here I read in section 'See Also'
[...]
'invisible' for 'return(.)'ing _invisibly_.

Do I just misunderstand what this implies?
Put another way what is the intention behind invisible() continuing
until the last statement before returning? ?invisible does not hint at
this.


Regards,

  Matthias


>R.version.string
[1] "Version 2.3.1 (2006-06-01)"

same behavior in R 2.2.1 or
R.version.string
[1] "R version 2.4.0 Under development (unstable) (2006-07-29 r38715)"


From christos at nuverabio.com  Fri Aug 11 22:59:29 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 11 Aug 2006 16:59:29 -0400
Subject: [R] invisible() - does not return immediately as return() does
In-Reply-To: <44DCEC30.8090709@web.de>
Message-ID: <004c01c6bd89$09daa0d0$0e010a0a@headquarters.silicoinsights>

Hi,

The difference is in the _return_ value of the function.

E.g.
> foo <- function() { cat("before\n"); cat("after\n"); return("done")}
> foo()
before
after
[1] "done"

i.e. returns the return value "done".

However
> foo2 <- function() { cat("before\n"); cat("after\n"); invisible("done")}
> foo2()
before
after 

does not show the return value (invisible), but it actually returns it
invisibly:

> x <- foo2()
before
after
> x
[1] "done"


HTH.

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Matthias Burger
Sent: Friday, August 11, 2006 4:45 PM
To: r-help at stat.math.ethz.ch
Subject: [R] invisible() - does not return immediately as return() does


Hi,

I stumbled across the following (unexpected for me) behavior after replacing
a return() statement in the middle of a function by invisible().

Example:
foo <- function() { cat("before\n"); return(); cat("after\n")}
>foo()
before
NULL

foo2 <- function() { cat("before\n"); invisible(TRUE); cat("after\n")}
>foo2()
before
after

I expected invisible to have the same behavior as return, namely immediately
return execution to the calling environment.

I rechecked ?invisible and ?return
and here I read in section 'See Also'
[...]
'invisible' for 'return(.)'ing _invisibly_.

Do I just misunderstand what this implies?
Put another way what is the intention behind invisible() continuing until
the last statement before returning? ?invisible does not hint at this.


Regards,

  Matthias


>R.version.string
[1] "Version 2.3.1 (2006-06-01)"

same behavior in R 2.2.1 or
R.version.string
[1] "R version 2.4.0 Under development (unstable) (2006-07-29 r38715)"

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ypeng at math.mun.ca  Fri Aug 11 23:03:34 2006
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Fri, 11 Aug 2006 18:33:34 -0230
Subject: [R] rpvm/snow packages on a cluster with dual-processor machi
 nes
In-Reply-To: <44DCE1C0.4020505@botany.utoronto.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2D9C@usctmx1106.merck.com>
	<44DCE1C0.4020505@botany.utoronto.ca>
Message-ID: <44DCF0A6.5040900@math.mun.ca>

Luke Tierney just reminded me that makeCluster() can take a number greater than the number of machines in a cluster. It seems to be a solution to this problem. But I haven't tested it yet.

Paul.


Ryan Austin wrote:
> Hi,
> 
> Adding a node twice gives a duplicate node error.
> However, adding the parameter sp=2000 to your pvm hostfile should enable 
> dual processors.
> 
> Ryan
> 
> Liaw, Andy wrote:
> 
>> Caveat: I've only played with this a couple of years ago... 
>>
>> I believe you can just add each host _twice_ (or as many times as the number
>> of CPUs at that host) to get both CPUs to work.
>>
>> Andy
>>
>> From: Paul Y. Peng
>>  
>>
>>> Hi,
>>>
>>> does anybody know how to use the dual processors in the 
>>> machines of a cluster? I am using R with rpvm and snow 
>>> packages. I usually start pvm daemon and add host machines 
>>> first, and then run R to start my computing work. But I find 
>>> that only one processor in each machine is used in this way 
>>> and the other one always stays idle. Is there any simple way 
>>> to tell pvm to use the two processors at the same time? In 
>>> other words, I would like to see two copies of R running on 
>>> each machine's two processors when using pvm. Any hints/help 
>>> are greatly appreciated.
>>>
>>> Paul.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>    
>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sfalcon at fhcrc.org  Sat Aug 12 00:11:02 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 11 Aug 2006 15:11:02 -0700
Subject: [R] invisible() - does not return immediately as return() does
In-Reply-To: <44DCEC30.8090709@web.de> (Matthias Burger's message of "Fri,
	11 Aug 2006 22:44:32 +0200")
References: <44DCEC30.8090709@web.de>
Message-ID: <m264gyhqix.fsf@ziti.fhcrc.org>

Matthias Burger <matt.burger at web.de> writes:

> Hi,
>
> I stumbled across the following (unexpected for me) behavior after
> replacing a return() statement in the middle of a function by invisible().
>
> Example:
> foo <- function() { cat("before\n"); return(); cat("after\n")}
>>foo()
> before
> NULL
>
> foo2 <- function() { cat("before\n"); invisible(TRUE); cat("after\n")}
>>foo2()
> before
> after
>
> I expected invisible to have the same behavior as return, namely
> immediately return execution to the calling environment.
>
> I rechecked ?invisible and ?return
> and here I read in section 'See Also'
> [...]
> 'invisible' for 'return(.)'ing _invisibly_.
>
> Do I just misunderstand what this implies?
> Put another way what is the intention behind invisible() continuing
> until the last statement before returning? ?invisible does not hint at
> this.

I can understand the confusion, but I think invisible is intended to
return its argument "invisibly" and actually has nothing to do with
returning from a function (except that is where you are going to use
it almost always ;-).  

So you want return(invisible(foo)) in the middle of a function.


The man page for invisible says:
  
   Return a (temporarily) invisible copy of an object.

But the man page for return has the return(.)'ing _invisibly_
statement which I think is confusing.

Cheers,

+ seth


From sourceforge at metrak.com  Sat Aug 12 01:25:10 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Sat, 12 Aug 2006 09:25:10 +1000
Subject: [R] more on date conversion differences in 2.2.1 vs 2.3.1
Message-ID: <44DD11D6.1050602@metrak.com>

With dates I get different results with 2.2.1 and 2.3.1.  From my 
somewhat naive point point of view, the 2.2.1 behaviour seems more sensible.

Running the code below in 2.2.1:
V1
2006-08-01 2006-08-01
               1             1

With 2.3.1 I get:
V1
1154354400 1154440800
         1          1

# testdate.R
t <- read.csv2('testdate.csv', header=FALSE)
t$V1 <- as.POSIXct(t$V1)
print(t)
x <- xtabs(V2 ~ V1, data=t)
print(x)

# testdate.csv
2006-8-1;0;1
2006-8-1;1;1
2006-8-2;0;1
2006-8-2;0;0
2006-8-2;1;1


From darrenleeweber at gmail.com  Sat Aug 12 01:41:16 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Fri, 11 Aug 2006 16:41:16 -0700
Subject: [R] x tick labels - sparse?
Message-ID: <d2095b8c0608111641h1b056405l224029a544b3b202@mail.gmail.com>

Hi,

I'm stuck on creating a plot with x tick labels for every Nth tick
mark - how is that done?  I don't see a simple solution to this in
help(plot) or help(par) and what I've tried is not working, eg, the
following does not work, although it seems intuitive to me that it
should work:

x <- seq(-100,1000,25)
y <- x * x
% find all the x values that are multiples of 100
tmp <- x / 100
tmp <- tmp %% 1
tmp <- tmp > 0
% set all other values to null strings
xtickLabels <- as.character( x )
xtickLabels[tmp] <- ""
plot(x, y, xlab=xtickLabels)


These commands look like this (the plot is not right):

> x <- seq(-100,1000,25)
> x
 [1] -100  -75  -50  -25    0   25   50   75  100  125  150  175  200  225  250
[16]  275  300  325  350  375  400  425  450  475  500  525  550  575  600  625
[31]  650  675  700  725  750  775  800  825  850  875  900  925  950  975 1000
>
> tmp <- x / 100
> tmp
 [1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00  1.25  1.50  1.75
[13]  2.00  2.25  2.50  2.75  3.00  3.25  3.50  3.75  4.00  4.25  4.50  4.75
[25]  5.00  5.25  5.50  5.75  6.00  6.25  6.50  6.75  7.00  7.25  7.50  7.75
[37]  8.00  8.25  8.50  8.75  9.00  9.25  9.50  9.75 10.00
>
> tmp <- tmp %% 1
> tmp
 [1] 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50
[16] 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25
[31] 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00
>
> tmp <- tmp > 0
> tmp
 [1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[13] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[25] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[37] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE
>
> xtickLabels <- as.character( x )
> xtickLabels
 [1] "-100" "-75"  "-50"  "-25"  "0"    "25"   "50"   "75"   "100"  "125"
[11] "150"  "175"  "200"  "225"  "250"  "275"  "300"  "325"  "350"  "375"
[21] "400"  "425"  "450"  "475"  "500"  "525"  "550"  "575"  "600"  "625"
[31] "650"  "675"  "700"  "725"  "750"  "775"  "800"  "825"  "850"  "875"
[41] "900"  "925"  "950"  "975"  "1000"
>
> xtickLabels[tmp] <- ""
> xtickLabels
 [1] "-100" ""     ""     ""     "0"    ""     ""     ""     "100"  ""
[11] ""     ""     "200"  ""     ""     ""     "300"  ""     ""     ""
[21] "400"  ""     ""     ""     "500"  ""     ""     ""     "600"  ""
[31] ""     ""     "700"  ""     ""     ""     "800"  ""     ""     ""
[41] "900"  ""     ""     ""     "1000"
>
> y <- x * x
> y
 [1]   10000    5625    2500     625       0     625    2500    5625   10000
[10]   15625   22500   30625   40000   50625   62500   75625   90000  105625
[19]  122500  140625  160000  180625  202500  225625  250000  275625  302500
[28]  330625  360000  390625  422500  455625  490000  525625  562500  600625
[37]  640000  680625  722500  765625  810000  855625  902500  950625 1000000
>
> plot(x, y, xlab=xtickLabels)
>


Thanks in advance.

Best, Darren


From murdoch at stats.uwo.ca  Sat Aug 12 01:41:34 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 11 Aug 2006 19:41:34 -0400
Subject: [R] Auto-save possible in R?
In-Reply-To: <000001c6bd81$7a190a10$6e4b1e30$@com>
References: <000001c6bd81$7a190a10$6e4b1e30$@com>
Message-ID: <44DD15AE.8050009@stats.uwo.ca>

On 8/11/2006 4:05 PM, John Morrow wrote:
> Hello fellow R'ers, I have a simple calculation with a very large data set
> being generated (34.9 million values) on a somewhat unreliable XP box that
> will likely take ~ 74hrs.  I wanted to know if there is a way to have my
> script automatically "save.image()" throughout the calculation in case of a
> crash.  This could be on the basis of output generated or time elapsed.  I
> checked the archive, and only got a hint of it from:
> https://stat.ethz.ch/pipermail/r-help/1997-May/001611.html
> 
>  
> 
> Any quick suggestions would be greatly appreciated,

I don't know of anything exactly as you describe.  I'd recommend 
thinking about suitable restart points in the computation, and manually 
using save() or save.image() in your script.

Duncan Murdoch


From Achim.Zeileis at wu-wien.ac.at  Sat Aug 12 02:22:55 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 12 Aug 2006 02:22:55 +0200
Subject: [R] sort() for time/date objects (was: more on date conversion
 differences in 2.2.1 vs 2.3.1)
In-Reply-To: <44DD11D6.1050602@metrak.com>
References: <44DD11D6.1050602@metrak.com>
Message-ID: <20060812022255.d2735744.Achim.Zeileis@wu-wien.ac.at>

On Sat, 12 Aug 2006 09:25:10 +1000 paul sorenson wrote:

> With dates I get different results with 2.2.1 and 2.3.1.  From my 
> somewhat naive point point of view, the 2.2.1 behaviour seems more
> sensible.

Your example below does not really make the source of the problem
obvious. I tried to see where the problem comes from and the following
seems to happen:
  xtabs() calls factor() calls sort(unique()) for computing the levels
And in the last step sort() seems to strip off the "class" attribute,
leaving only the underlying numeric vector.

Of course, the functions never claimed to work for "POSIXct" objects,
hence you should call xtabs() appropriately, e.g. via
  xtabs(V2 ~ as.character(V1), data = t)

However, it might be desirable to have sort() working for time/date
objects (such as POSIXct, Date and date). sort() would just have to
preserve the class as it did in R 2.2.1.
Z

> Running the code below in 2.2.1:
> V1
> 2006-08-01 2006-08-01
>                1             1
> 
> With 2.3.1 I get:
> V1
> 1154354400 1154440800
>          1          1
> 
> # testdate.R
> t <- read.csv2('testdate.csv', header=FALSE)
> t$V1 <- as.POSIXct(t$V1)
> print(t)
> x <- xtabs(V2 ~ V1, data=t)
> print(x)
> 
> # testdate.csv
> 2006-8-1;0;1
> 2006-8-1;1;1
> 2006-8-2;0;1
> 2006-8-2;0;0
> 2006-8-2;1;1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sat Aug 12 02:34:59 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 11 Aug 2006 20:34:59 -0400
Subject: [R] x tick labels - sparse?
In-Reply-To: <d2095b8c0608111641h1b056405l224029a544b3b202@mail.gmail.com>
References: <d2095b8c0608111641h1b056405l224029a544b3b202@mail.gmail.com>
Message-ID: <971536df0608111734v2e5a4664gc3d3214a31ca8ae9@mail.gmail.com>

Try this:

x <- seq(-100,1000,25)
y <- x * x
plot(x, y, xaxt = "n")
axis(1, x[x %% 100 == 0])



On 8/11/06, Darren Weber <darrenleeweber at gmail.com> wrote:
> Hi,
>
> I'm stuck on creating a plot with x tick labels for every Nth tick
> mark - how is that done?  I don't see a simple solution to this in
> help(plot) or help(par) and what I've tried is not working, eg, the
> following does not work, although it seems intuitive to me that it
> should work:
>
> x <- seq(-100,1000,25)
> y <- x * x
> % find all the x values that are multiples of 100
> tmp <- x / 100
> tmp <- tmp %% 1
> tmp <- tmp > 0
> % set all other values to null strings
> xtickLabels <- as.character( x )
> xtickLabels[tmp] <- ""
> plot(x, y, xlab=xtickLabels)
>
>
> These commands look like this (the plot is not right):
>
> > x <- seq(-100,1000,25)
> > x
>  [1] -100  -75  -50  -25    0   25   50   75  100  125  150  175  200  225  250
> [16]  275  300  325  350  375  400  425  450  475  500  525  550  575  600  625
> [31]  650  675  700  725  750  775  800  825  850  875  900  925  950  975 1000
> >
> > tmp <- x / 100
> > tmp
>  [1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00  1.25  1.50  1.75
> [13]  2.00  2.25  2.50  2.75  3.00  3.25  3.50  3.75  4.00  4.25  4.50  4.75
> [25]  5.00  5.25  5.50  5.75  6.00  6.25  6.50  6.75  7.00  7.25  7.50  7.75
> [37]  8.00  8.25  8.50  8.75  9.00  9.25  9.50  9.75 10.00
> >
> > tmp <- tmp %% 1
> > tmp
>  [1] 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50
> [16] 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25
> [31] 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00
> >
> > tmp <- tmp > 0
> > tmp
>  [1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [13] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [25] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> [37] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE
> >
> > xtickLabels <- as.character( x )
> > xtickLabels
>  [1] "-100" "-75"  "-50"  "-25"  "0"    "25"   "50"   "75"   "100"  "125"
> [11] "150"  "175"  "200"  "225"  "250"  "275"  "300"  "325"  "350"  "375"
> [21] "400"  "425"  "450"  "475"  "500"  "525"  "550"  "575"  "600"  "625"
> [31] "650"  "675"  "700"  "725"  "750"  "775"  "800"  "825"  "850"  "875"
> [41] "900"  "925"  "950"  "975"  "1000"
> >
> > xtickLabels[tmp] <- ""
> > xtickLabels
>  [1] "-100" ""     ""     ""     "0"    ""     ""     ""     "100"  ""
> [11] ""     ""     "200"  ""     ""     ""     "300"  ""     ""     ""
> [21] "400"  ""     ""     ""     "500"  ""     ""     ""     "600"  ""
> [31] ""     ""     "700"  ""     ""     ""     "800"  ""     ""     ""
> [41] "900"  ""     ""     ""     "1000"
> >
> > y <- x * x
> > y
>  [1]   10000    5625    2500     625       0     625    2500    5625   10000
> [10]   15625   22500   30625   40000   50625   62500   75625   90000  105625
> [19]  122500  140625  160000  180625  202500  225625  250000  275625  302500
> [28]  330625  360000  390625  422500  455625  490000  525625  562500  600625
> [37]  640000  680625  722500  765625  810000  855625  902500  950625 1000000
> >
> > plot(x, y, xlab=xtickLabels)
> >
>
>
> Thanks in advance.
>
> Best, Darren
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From muster at gmail.com  Sat Aug 12 02:50:48 2006
From: muster at gmail.com (T Mu)
Date: Fri, 11 Aug 2006 20:50:48 -0400
Subject: [R] problem in reading large files
Message-ID: <b68812e70608111750g58e3e68dsd577f87384092521@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/0111dc85/attachment.pl 

From jholtman at gmail.com  Sat Aug 12 03:32:54 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 11 Aug 2006 21:32:54 -0400
Subject: [R] problem in reading large files
In-Reply-To: <b68812e70608111750g58e3e68dsd577f87384092521@mail.gmail.com>
References: <b68812e70608111750g58e3e68dsd577f87384092521@mail.gmail.com>
Message-ID: <644e1f320608111832y40dbff09ub23af720b476ef0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060811/828534b4/attachment.pl 

From murdoch at stats.uwo.ca  Sat Aug 12 04:08:08 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 11 Aug 2006 22:08:08 -0400
Subject: [R] problem in reading large files
In-Reply-To: <b68812e70608111750g58e3e68dsd577f87384092521@mail.gmail.com>
References: <b68812e70608111750g58e3e68dsd577f87384092521@mail.gmail.com>
Message-ID: <44DD3808.4080008@stats.uwo.ca>

On 8/11/2006 8:50 PM, T Mu wrote:
> I was trying to read a large .csv file (80 colums, 400,000 rows, size of
> about 200MB). I used scan(), R 2.3.1 on Windows XP. My computer is AMD 2000+
> and has 512MB ram.

You should get R-patched; there were some bugs with low memory handling 
fixed recently:

 From CHANGES:

R could crash when very low on memory. (PR#8981)

You should also get more physical memory.  512MB is not much for 
handling a 200MB of data.  You can fairly easily benefit from increasing 
up to 2 GB, and will benefit (with some work) if you have even more, up 
to 4 GB.

Duncan Murdoch

> 
> It sometimes freezes my PC, sometimes just shuts down R quitely.
> 
> Is there a way (option, function) to better handle large files?
> 
> Seemingly SAS can deal with it with no problem, but I just persuaded my
> professor transfering to R, so it is quite disappointing.
> 
> Please help, thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vincent at 7d4.com  Sat Aug 12 08:15:16 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Sat, 12 Aug 2006 08:15:16 +0200
Subject: [R] Auto-save possible in R?
In-Reply-To: <000001c6bd81$7a190a10$6e4b1e30$@com>
References: <000001c6bd81$7a190a10$6e4b1e30$@com>
Message-ID: <44DD71F4.8030905@7d4.com>

John Morrow a ?crit :

> Hello fellow R'ers, I have a simple calculation with a very large data set
> being generated (34.9 million values) on a somewhat unreliable XP box that
> will likely take ~ 74hrs.  I wanted to know if there is a way to have my
> script automatically "save.image()" throughout the calculation in case of a
> crash.  This could be on the basis of output generated or time elapsed.  I
> checked the archive, and only got a hint of it from:
> https://stat.ethz.ch/pipermail/r-help/1997-May/001611.html
> Any quick suggestions would be greatly appreciated,
> John Morrow

# make a save every hour
minutes = substr(date(),...);
if (minutes=='00') save_my_stuff;
hih


From vincent at 7d4.com  Sat Aug 12 08:32:45 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Sat, 12 Aug 2006 08:32:45 +0200
Subject: [R] Colour-coding intervals on a line
In-Reply-To: <s4dcb6c0.058@bsnw.nerc-bas.ac.uk>
References: <s4dcb6c0.058@bsnw.nerc-bas.ac.uk>
Message-ID: <44DD760D.3040403@7d4.com>

Sara-Jane Dunn a ?crit :

> I need to know if there is any way of using different colours for
> different intervals of a line on a graph. Eg. If I plot the line y=x for
> x=1:10, and split this line into 106 intervals (i.e. not a 'nice' number
> of intervals) how could I colour different intervals different colours
> without having to use segments and specify coordinates to join?
> Thanks in advance for any help!
> Sara-Jane

I think you'll have to use segments (?lines())
but it shouldn't be too complicated.

Put your line begin, line end, line color in a matrix(, ncol=106)
and call lines(...) in a for loop.

I would be happy to hear about another way to do it ?


From ripley at stats.ox.ac.uk  Sat Aug 12 08:59:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Aug 2006 07:59:22 +0100 (BST)
Subject: [R] anova.mlm for single model (one-way repeated measured anova)
In-Reply-To: <3948d9e50608112136x6d2c313ar2683cb236ee23a35@mail.gmail.com>
References: <3948d9e50608112136x6d2c313ar2683cb236ee23a35@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608120756260.24964@gannet.stats.ox.ac.uk>

On Sat, 12 Aug 2006, takahashi kohske wrote:

> Dear list members:
> 
> I'd like to one-way repeated measured anova by using mlm.
> I'm using R-2.3.1 and my code is:
> 
> dat<-matrix( c(9,7,8,8,12,11,8,13,     6,5,6,3,6,7,10,9,
>                10,13,8,13,12,14,14,16, 9,11,13,14,16,12,15,14),
>             ncol=4, dimname=list(s=1:8, c=1:4))
> mlmfit<-lm(dat~1)
> anova(mlmfit, X=~1)
> Error: ceiling(length.out) : Non-numeric argument to mathematical function
> 
> this error occurs in anova.mlm
> 
>         if (rk > 0) {
>             p1 <- 1:rk
>             comp <- object$effects[p1, ]
>             asgn <- object$assign[object$qr$pivot][p1]
>             nmeffects <- c("(Intercept)", attr(object$terms,
>                 "term.labels"))
>             tlabels <- nmeffects[1 + unique(asgn)]
>              ix <- split(seq(length = nrow(comp)), asgn)  #HERE
>             ss <- lapply(ix, function(i) crossprod(comp[i, ,
>                 drop = FALSE]))
>             df <- sapply(split(asgn, asgn), length)
>         }
> 
> because nrow(comp) returns NULL.
> 
> in my memory, R-2.2.* ( or may be R-2.3.0) can correctly handle this code.
> so, I think this is a kind of side-effect of fixing PR#8679.
> 
> currently, i can workaround as follows:
> 
> anova(mlmfit, update(mlmfit, ~0), X=~1)
> 
> this code returns correct answer.
> 
> 
> I don't know whether this behavior is correct or bug.


Yes, it is a bug.  The line

            comp <- object$effects[p1, ]

should be

            comp <- object$effects[p1, , drop=FALSE]

I am changing this in 2.3.1 patched and R-devel.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jim at bitwrit.com.au  Sun Aug 13 01:16:51 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 12 Aug 2006 19:16:51 -0400
Subject: [R] Colour-coding intervals on a line
In-Reply-To: <s4dcb6c0.058@bsnw.nerc-bas.ac.uk>
References: <s4dcb6c0.058@bsnw.nerc-bas.ac.uk>
Message-ID: <44DE6163.2060905@bitwrit.com.au>

Sara-Jane Dunn wrote:
> Hi,
>  
> This is a simple version of something that I am trying to do. If I can
> sort the problem basically, I figure I should be able to sort it for the
> program I'm writing (which would take longer to explain).
>  
> I need to know if there is any way of using different colours for
> different intervals of a line on a graph. Eg. If I plot the line y=x for
> x=1:10, and split this line into 106 intervals (i.e. not a 'nice' number
> of intervals) how could I colour different intervals different colours
> without having to use segments and specify coordinates to join?
>  
Hi Sara-Jane,
This is a problem that I have been facing myself. I wanted to display 
color-coded curves that indicated something depending upon the values 
plotted. This might be useful to you, although you will need the 
functions color.scale and rescale from the plotrix package to make it work.

color.scale.lines<-function(x,y,redrange,greenrange,bluerange,
  scale.to="y",...) {

  lenx<-length(x)
  leny<-length(y)
  nseg<-lenx-1
  if(lenx != leny) {
   warning("x and y are different lengths - some values will be ignored")
   if(lenx>leny) nseg<-leny-1
  }
  if(scale.to=="y")
   lcol<-color.scale(y[1:nseg],redrange,greenrange,bluerange)
  else lcol<-color.scale(x[1:nseg],redrange,greenrange,bluerange)
  segments(x[1:nseg],y[1:nseg],x[2:(nseg+1)],y[2:(nseg+1)],col=lcol,...)
}

plot(0,xlim=c(0,pi),ylim=c(0,1),type="n")
x<-seq(0,pi,length=106)
color.scale.lines(x,sin(x)*10,c(0,1),c(1,0),0,lwd=2)

Jim


From javalkon at hytti.uku.fi  Sat Aug 12 12:07:19 2006
From: javalkon at hytti.uku.fi (Jarimatti Valkonen)
Date: Sat, 12 Aug 2006 13:07:19 +0300
Subject: [R] tkinsert
In-Reply-To: <000801c6bd58$e2440c80$6cf8d656@Claude>
References: <000801c6bd58$e2440c80$6cf8d656@Claude>
Message-ID: <44DDA857.9090106@hytti.uku.fi>

claude.josse wrote:
 > But in my text window I have :
 >
 > x=2a=mean(c(1,2,3))
 >
 > I d'like to have something like:
 > x=2
 > a=mean(c(1,2,3))

AFAIK you need to separate lines with a newline character ('\n').
Put a 'tkinsert(txt, "end", "\n")' between the insert commands. Or
append each line with '\n'.

 >
 > 2)My second problem,

In function run:
 >    print(eval(e))

Make the evaluation in the workspace environment:
print(eval(e, envir=.GlobalEnv))

Then the results are available in R console.

HTH

-- 
Jarimatti Valkonen


From spencer.graves at pdf.com  Sat Aug 12 12:28:26 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Aug 2006 03:28:26 -0700
Subject: [R] between-within anova: aov and lme
In-Reply-To: <33271.131.136.242.1.1154606478.squirrel@webmail.utsc.utoronto.ca>
References: <33271.131.136.242.1.1154606478.squirrel@webmail.utsc.utoronto.ca>
Message-ID: <44DDAD4A.1060604@pdf.com>

	  To understand why this works, you need to understand the math in a 
more general formulation.  Ordinary least squares can be written in 
matrix / vector notation as follows:

	  y = X %*% b + e,

where y and e are N x 1 vectors, X is an N x k matrix, and b is a k x 1 
vector.  In this formulation, e follows a multivariate normal 
distribution with mean 0 and covariance = s.e^2 times the N x N identity 
matrix.

	  For mixed effects, e is assumed to follow a multivariate normal 
distribution with a more general variance-covariance structure, 
specified in various ways as discussed in Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer).  If e ~ N(0, W), then 
the maximum likelihood estimates for "b" in the above model can be 
written as follows:

	  b = inv(t(X) %*% solve(W, X)) %*% y.

	  As explained by Pinheiro and Bates, we estimate the fixed effects, 
"b", using maximum likelihood (ML) and parameters in "W" using 
"restricted maximum likelihood (REML)".

	  The standard analysis of variance is then obtained from the 
"likelihood ratio" for nested models.  In certain special cases, a 
monotonic transformation of a likelihood ratio follows an F distribution 
with degrees of freedom computed from the ranks of various matrices. 
The approach provides a unified way of analyzing data with mixed effects 
that does not care if the design is balance or not.

	  Analyses following this method may not always give the same answers 
as textbooks that discuss standard balanced designs.  However, I'm not 
prepared to discuss that.
	
	  Hope this helps.
	  Spencer Graves

##############################################
William Simpson wrote:
 > Hi Spencer
 >
 >> 	  'lme' is smart enough to figure out from the data whether a factor is
 >> 'between' or 'within' or partially one or the other.  This allows you
 >> avoid worrying about that during data analysis -- except as a check on
 >> factor coding.
 > Just to check Spencer, the following lme() statement:
 > lme(y~a*b*c,random=~1|s, data=d)
 > will work for any combination of a,b,c as between or within factors. 
At one extreme
 > a,b,c could all be between subjects, at the other extreme a,b,c could 
all be within
 > subjects, and any other combo of between/within.
 >
 > That is a bit mind-bending. So far as lme is concerned all that 
matters is that s is
 > a random effect. It will probably be difficult to convince experimental
 > psychologists who consider themselves to be experts in the 
statistical analysis of
 > experiments.
 >
 > Cheers
 > Bill
 >
#################################
	  I can't answer your question about 'aov', but have you tried the
following with 'lme':

	  lme(response~A*B*C,random=~1|subject)

	  This assumes that A, B, and C are fixed effects, either continuous
variables or factors present at only a very few levels whose effects are
not reasonably modeled as a random sample from some other distribution.
  It also assumes that the effect of each level of subject can be
reasonable modeled as a random adjustment to the intercept following a
common distribution with mean 0 and variance = 'var.subj'.

	   The function 'aov' is old and mostly obsoleted by 'nlme'.  There may
be things that can be done in 'aov' that can not be done more or less as
easily and usually better and more generally with 'lme', but I'm not
familiar with such cases.

	  Your question suggests you may not be familiar with Pinheiro and
Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).  The
standard R distribution comes with a directory "~library\nlme\scripts"
containing script files 'ch01.R', 'ch02.R', ..., 'ch06.R', and 'ch08.R'.
  These contain R script files with the R code for each chapter in the
book.  I've learned a lot from walking through the script files line by
line while reviewing the corresponding text in the book.  Doing so
protects me from problems with silly typographical errors as well as
subtle problems where the S-Plus syntax in the book gives a different
answer in R because of the few differences in the syntax between S-Plus
and R.

	  Hope this helps.
	  Spencer Graves

William Simpson wrote:
> I have 2 questions on ANOVA with 1 between subjects factor and 2 within factors.
> 
> 1. I am confused on how to do the analysis with aov because I have seen two examples
> on the web with different solutions.
> 
> a) Jon Baron (http://www.psych.upenn.edu/~baron/rpsych/rpsych.html) does
> 6.8.5 Example 5: Stevens pp. 468 - 474 (one between, two within)
> 
> between: gp
> within: drug, dose
> aov(effect ~ gp * drug * dose + Error(subj/(dose*drug)), data=Ela.uni)
> 
> b) Bill Venables answered a question on R help as follows.
> 
> - factor A between subjects
> - factors B*C within subjects.
> 
> aov(response ~ A*B*C + Error(subject), Kirk)
> "An alternative formula would be response ~ A/(B*C) + Error(subject), which
> would only change things by grouping together some of the sums of squares."
> 
> -------------------------------------------------------
> SO: which should I do????
> aov(response ~ A*B*C + Error(subject), Kirk)
> aov(response ~ A/(B*C) + Error(subject), Kirk)
> aov(response ~ A*B*C + Error(subject/(B*C)), Kirk)
> --------------------------------------------------------
> 
> 2. How would I do the analysis in lme()?
> Something like
> lme(response~A*B*C,random=~1|subject/(B*C))???
> 
> 
> Thanks very much for any help!
> Bill Simpson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dirk.enzmann at uni-hamburg.de  Sat Aug 12 13:01:38 2006
From: dirk.enzmann at uni-hamburg.de (Dirk Enzmann)
Date: Sat, 12 Aug 2006 13:01:38 +0200
Subject: [R] Geometrical Interpretation of Eigen value and Eigen vector
In-Reply-To: <mailman.11.1155290403.32412.r-help@stat.math.ethz.ch>
References: <mailman.11.1155290403.32412.r-help@stat.math.ethz.ch>
Message-ID: <44DDB512.4090002@uni-hamburg.de>

Arun,

have a look at:

http://149.170.199.144/multivar/eigen.htm

HTH,
Dirk

"Arun Kumar Saha" <arun.kumar.saha at gmail.com> wrote:

> It is not a R related problem rather than statistical/mathematical. However
> I am posting this query hoping that anyone can help me on this matter. My
> problem is to get the Geometrical Interpretation of Eigen value and Eigen
> vector of any square matrix. Can anyone give me a light on it?


From julie7.josse at laposte.net  Sat Aug 12 15:08:44 2006
From: julie7.josse at laposte.net (julie7.josse)
Date: Sat, 12 Aug 2006 15:08:44 +0200
Subject: [R] tkinsert
Message-ID: <J3VZ6K$5DCABDE8686CC9D7E34E07EECFA555D2@laposte.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060812/2ffc639b/attachment.pl 

From albert_picado at yahoo.fr  Sat Aug 12 16:09:08 2006
From: albert_picado at yahoo.fr (Albert Picado)
Date: Sat, 12 Aug 2006 16:09:08 +0200 (CEST)
Subject: [R] adding columns to a table after a loop
Message-ID: <20060812140908.17585.qmail@web26914.mail.ukl.yahoo.com>

Dear list,

I am trying to find the way to add columns to a table
(or a matrix) after a loop. The result of this loop is
an array and I would like to get all the results in a
single table once the loop is finish. I have
reproduced a simplified example below:

> a<- 1
> b <- matrix()
> while (a <= 4) {
+ b<-rnorm(10)
+ a<- a +1
+ }

# I have tried several methods but without succes so
far. 

> result <- as.data.frame(cbind(b))
> result
             b
1  -0.03250661
2  -0.59823770
3   1.58120471
4   0.41086546
5   0.78959090
6   1.23587125
7   0.83427190
8   1.09035581
9   0.11331056
10  0.25267231

the result above is not what I am looking for because
it shows only the results from the last operation in
the loop, I would like to obtain something like the
following.

> result
            b1         b2          b3         b4
1  -0.08420826 -0.0637943 -0.83246201 -1.0902384
2   0.40623009 -2.7940096  1.37664973  1.6023967
3  -1.13850505  1.1660669 -0.95962296 -0.7325098
4  -1.06183391  1.1063677  1.67948677 -1.9875475
5  -0.64431067 -0.4843952 -1.30742858  0.5064134
6   0.56729468  1.0860484  0.07651954  0.4380108
7   0.95036177 -0.5328609  1.28954934 -0.2775614
8  -0.17848223  0.5340379 -0.22613700  1.0179886
9   2.93145454 -1.8639607 -0.25478610  1.6619754
10  1.51942415 -0.2051423  0.09144450 -1.6329481

I hope someone can give me a hand

best regards

a




	
 p5.vert.ukl.yahoo.com uncompressed/chunked Sat Aug 12 13:14:00 GMT 2006


From fabletim at pasteur.fr  Fri Aug 11 16:56:48 2006
From: fabletim at pasteur.fr (Alexandre Fabrice Letimier)
Date: Fri, 11 Aug 2006 16:56:48 +0200
Subject: [R] Changing R network connections
Message-ID: <a06200708c1024989cb13@[157.99.222.25]>

Dear all,
I have some difficulties to get R to download the various packages. 
It seems that it's a problem of the network that I'm on. I would need 
to configure R so that I can put the proxy address and the port of 
the network that I'm on. Can anyone explain to me how to change these 
in R. I've looked at the preferences but couldn't find it there. 
Thanks a lot for you help.
Regards,
Fabrice Letimier
--


From dusa.adrian at gmail.com  Sat Aug 12 16:47:05 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sat, 12 Aug 2006 17:47:05 +0300
Subject: [R] RE :  tcltk library on linux
In-Reply-To: <C8F48FD780E12D4DB197507B897D00DD072F5B20@ntexch.softcomputing.com>
References: <C8F48FD780E12D4DB197507B897D00DD072F5B20@ntexch.softcomputing.com>
Message-ID: <200608121747.05611.dusa.adrian@gmail.com>

On Friday 11 August 2006 10:31, Yohan CHOUKROUN wrote:
> Thank you for your answer but I already use the .deb package.
> Also I have compiled the source code, but it is the same result...
> I have already the same error..
> I 'm going to be crazy ;-)
> Has anyone got the same problem (and found the solution!) ?
> Thanks in advance
> Yohan

Only a week ago it was a similar thread, and it was solved by installing 
the .deb package from CRAN.
Assuming you use the latest version Dapper (you didn't specified), add this 
line to your sources.list:
deb http://cran.R-project.org/bin/linux/ubuntu/ dapper/

Then:
$ sudo apt-get update
$ sudo apt-get install r-cran-rcmdr
(as Dirk Eddelbuettel advised). It _should_ work flawlessly.

HTH,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ligges at statistik.uni-dortmund.de  Sat Aug 12 16:57:16 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 12 Aug 2006 16:57:16 +0200
Subject: [R] adding columns to a table after a loop
In-Reply-To: <20060812140908.17585.qmail@web26914.mail.ukl.yahoo.com>
References: <20060812140908.17585.qmail@web26914.mail.ukl.yahoo.com>
Message-ID: <44DDEC4C.6050509@statistik.uni-dortmund.de>



Albert Picado wrote:
> Dear list,
> 
> I am trying to find the way to add columns to a table
> (or a matrix) after a loop. The result of this loop is
> an array and I would like to get all the results in a
> single table once the loop is finish. I have
> reproduced a simplified example below:
> 
>> a<- 1
>> b <- matrix()
>> while (a <= 4) {
> + b<-rnorm(10)
> + a<- a +1
> + }


Staying in your example:

a <- 1
b <- matrix(nrow=10, ncol=4)
while(a <= 4){
     b[,i] <- rnorm(10)
     a <- a + 1
}

Uwe Ligges



> # I have tried several methods but without succes so
> far. 
> 
>> result <- as.data.frame(cbind(b))
>> result
>              b
> 1  -0.03250661
> 2  -0.59823770
> 3   1.58120471
> 4   0.41086546
> 5   0.78959090
> 6   1.23587125
> 7   0.83427190
> 8   1.09035581
> 9   0.11331056
> 10  0.25267231
> 
> the result above is not what I am looking for because
> it shows only the results from the last operation in
> the loop, I would like to obtain something like the
> following.
> 
>> result
>             b1         b2          b3         b4
> 1  -0.08420826 -0.0637943 -0.83246201 -1.0902384
> 2   0.40623009 -2.7940096  1.37664973  1.6023967
> 3  -1.13850505  1.1660669 -0.95962296 -0.7325098
> 4  -1.06183391  1.1063677  1.67948677 -1.9875475
> 5  -0.64431067 -0.4843952 -1.30742858  0.5064134
> 6   0.56729468  1.0860484  0.07651954  0.4380108
> 7   0.95036177 -0.5328609  1.28954934 -0.2775614
> 8  -0.17848223  0.5340379 -0.22613700  1.0179886
> 9   2.93145454 -1.8639607 -0.25478610  1.6619754
> 10  1.51942415 -0.2051423  0.09144450 -1.6329481
> 
> I hope someone can give me a hand
> 
> best regards
> 
> a
> 
> 
> 
> 
> 	
>  p5.vert.ukl.yahoo.com uncompressed/chunked Sat Aug 12 13:14:00 GMT 2006
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Sat Aug 12 17:24:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Aug 2006 16:24:45 +0100 (BST)
Subject: [R] Changing R network connections
In-Reply-To: <a06200708c1024989cb13@[157.99.222.25]>
References: <a06200708c1024989cb13@[157.99.222.25]>
Message-ID: <Pine.LNX.4.64.0608121622200.29462@gannet.stats.ox.ac.uk>

Since you mention 'preferences', it seems you might be using Windows: the 
information you require is in the rw-FAQ.

Since the posting guide asked you to state your platform and read the 
relevant FAQ, that is the place to start: see the footer below.

On Fri, 11 Aug 2006, Alexandre Fabrice Letimier wrote:

> Dear all,
> I have some difficulties to get R to download the various packages. 
> It seems that it's a problem of the network that I'm on. I would need 
> to configure R so that I can put the proxy address and the port of 
> the network that I'm on. Can anyone explain to me how to change these 
> in R. I've looked at the preferences but couldn't find it there. 
> Thanks a lot for you help.
> Regards,
> Fabrice Letimier
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From albert_picado at yahoo.fr  Sat Aug 12 18:22:44 2006
From: albert_picado at yahoo.fr (Albert Picado)
Date: Sat, 12 Aug 2006 16:22:44 +0000 (GMT)
Subject: [R] Re :  adding columns to a table after a loop
In-Reply-To: <44DDEC4C.6050509@statistik.uni-dortmund.de>
Message-ID: <20060812162244.65919.qmail@web26902.mail.ukl.yahoo.com>

Dear Uwe,
 
Thanks for your answer. I have been trying your suggestion but I haven't been able to solve my problem as I may have simplified in excess my example. The real situation is actually much similar to the following:
 
a<-1
b<-1
i<-1
pvalue<-matrix()
while(a<=4) {
     while(b<=10){
     pvalue[i]<-rnorm(1)
     i<-i+1
     b<-b+1
     }
a<-a+1
}

To get the desired outcome I have tried two things based on your suggestion:
 
1)
 
a<-1
b<-1
i<-1
pvalue<- matrix(ncol=4, nrow=10)
result<- matrix()
while(a<=4) {
 while(b<=10){
 pvalue[i]<-rnorm(1)
 i<-i+1
 b<-b+1
 }
result[,a]<-pvalue
a<-a+1
}

here I get an error message: 
Error in "[<-"(`*tmp*`, , a, value = c(1.38661984066808, -0.641565430307799,  : 
        number of items to replace is not a multiple of replacement length

2)
 
a<-1
b<-1
i<-1
pvalue<-matrix(ncol=4, nrow=10)
while(a<=4) {
 while(b<=10){
 pvalue[i,a]<-rnorm(1)
 i<-i+1
 b<-b+1
 }
a<-a+1
}

here what I get is:
 
> pvalue
             [,1] [,2] [,3] [,4]
 [1,] -2.18379100   NA   NA   NA
 [2,] -0.90112392   NA   NA   NA
 [3,] -0.91762163   NA   NA   NA
 [4,]  0.54922398   NA   NA   NA
 [5,]  0.95511590   NA   NA   NA
 [6,]  0.08509518   NA   NA   NA
 [7,] -0.92576835   NA   NA   NA
 [8,]  1.62361191   NA   NA   NA
 [9,]  0.02644486   NA   NA   NA
[10,]  0.32406407   NA   NA   NA

 
I do not know what else to try...
 
best wishes
 
a


----- Message d'origine ----
De : Uwe Ligges <ligges at statistik.uni-dortmund.de>
? : Albert Picado <albert_picado at yahoo.fr>
Cc : r-help at stat.math.ethz.ch
Envoy? le : Samedi, 12 Ao?t 2006, 3h57mn 16s
Objet : Re: [R] adding columns to a table after a loop


Albert Picado wrote:
> Dear list,
> 
> I am trying to find the way to add columns to a table
> (or a matrix) after a loop. The result of this loop is
> an array and I would like to get all the results in a
> single table once the loop is finish. I have
> reproduced a simplified example below:
> 
>> a<- 1
>> b <- matrix()
>> while (a <= 4) {
> + b<-rnorm(10)
> + a<- a +1
> + }


Staying in your example:

a <- 1
b <- matrix(nrow=10, ncol=4)
while(a <= 4){
     b[,i] <- rnorm(10)
     a <- a + 1
}

Uwe Ligges



> # I have tried several methods but without succes so
> far. 
> 
>> result <- as.data.frame(cbind(b))
>> result
>              b
> 1  -0.03250661
> 2  -0.59823770
> 3   1.58120471
> 4   0.41086546
> 5   0.78959090
> 6   1.23587125
> 7   0.83427190
> 8   1.09035581
> 9   0.11331056
> 10  0.25267231
> 
> the result above is not what I am looking for because
> it shows only the results from the last operation in
> the loop, I would like to obtain something like the
> following.
> 
>> result
>             b1         b2          b3         b4
> 1  -0.08420826 -0.0637943 -0.83246201 -1.0902384
> 2   0.40623009 -2.7940096  1.37664973  1.6023967
> 3  -1.13850505  1.1660669 -0.95962296 -0.7325098
> 4  -1.06183391  1.1063677  1.67948677 -1.9875475
> 5  -0.64431067 -0.4843952 -1.30742858  0.5064134
> 6   0.56729468  1.0860484  0.07651954  0.4380108
> 7   0.95036177 -0.5328609  1.28954934 -0.2775614
> 8  -0.17848223  0.5340379 -0.22613700  1.0179886
> 9   2.93145454 -1.8639607 -0.25478610  1.6619754
> 10  1.51942415 -0.2051423  0.09144450 -1.6329481
> 
> I hope someone can give me a hand
> 
> best regards
> 
> a
> 
> 
> 
> 
>     
>  p5.vert.ukl.yahoo.com uncompressed/chunked Sat Aug 12 13:14:00 GMT 2006
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sat Aug 12 18:53:03 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 12 Aug 2006 18:53:03 +0200
Subject: [R] Re :  adding columns to a table after a loop
In-Reply-To: <20060812162244.65919.qmail@web26902.mail.ukl.yahoo.com>
References: <20060812162244.65919.qmail@web26902.mail.ukl.yahoo.com>
Message-ID: <44DE076F.8050803@statistik.uni-dortmund.de>



Albert Picado wrote:
> Dear Uwe,
>  
> Thanks for your answer. I have been trying your suggestion but I haven't been able to solve my problem as I may have simplified in excess my example. The real situation is actually much similar to the following:
>  
> a<-1
> b<-1
> i<-1
> pvalue<-matrix()
> while(a<=4) {
>      while(b<=10){
>      pvalue[i]<-rnorm(1)
>      i<-i+1
>      b<-b+1
>      }
> a<-a+1
> }


I think you will have to read some basic documentation such as "An 
Introduction to R" about data structures, indexing, and loops in R 
before proceeding ...

Anyway, you can do:

a <- 1
pvalue <- matrix(nrow = 10, ncol = 4)
while(a <= 4) {
     b <- 1
     while(b <= 10){
         pvalue[b, a] <- rnorm(1)
         b <- b+1
     }
     a <- a+1
}


or much shorter


pvalue <- matrix(nrow = 10, ncol = 4)
for(a in 1:4) {
     for(b in 1:10){
         pvalue[b, a] <- rnorm(1)
     }
}


Uwe Ligges




> To get the desired outcome I have tried two things based on your suggestion:
>  
> 1)
>  
> a<-1
> b<-1
> i<-1
> pvalue<- matrix(ncol=4, nrow=10)
> result<- matrix()
> while(a<=4) {
>  while(b<=10){
>  pvalue[i]<-rnorm(1)
>  i<-i+1
>  b<-b+1
>  }
> result[,a]<-pvalue
> a<-a+1
> }
> 
> here I get an error message: 
> Error in "[<-"(`*tmp*`, , a, value = c(1.38661984066808, -0.641565430307799,  : 
>         number of items to replace is not a multiple of replacement length
> 
> 2)
>  
> a<-1
> b<-1
> i<-1
> pvalue<-matrix(ncol=4, nrow=10)
> while(a<=4) {
>  while(b<=10){
>  pvalue[i,a]<-rnorm(1)
>  i<-i+1
>  b<-b+1
>  }
> a<-a+1
> }
> 
> here what I get is:
>  
>> pvalue
>              [,1] [,2] [,3] [,4]
>  [1,] -2.18379100   NA   NA   NA
>  [2,] -0.90112392   NA   NA   NA
>  [3,] -0.91762163   NA   NA   NA
>  [4,]  0.54922398   NA   NA   NA
>  [5,]  0.95511590   NA   NA   NA
>  [6,]  0.08509518   NA   NA   NA
>  [7,] -0.92576835   NA   NA   NA
>  [8,]  1.62361191   NA   NA   NA
>  [9,]  0.02644486   NA   NA   NA
> [10,]  0.32406407   NA   NA   NA
> 
>  
> I do not know what else to try...
>  
> best wishes
>  
> a
> 
> 
> ----- Message d'origine ----
> De : Uwe Ligges <ligges at statistik.uni-dortmund.de>
> ? : Albert Picado <albert_picado at yahoo.fr>
> Cc : r-help at stat.math.ethz.ch
> Envoy? le : Samedi, 12 Ao?t 2006, 3h57mn 16s
> Objet : Re: [R] adding columns to a table after a loop
> 
> 
> Albert Picado wrote:
>> Dear list,
>>
>> I am trying to find the way to add columns to a table
>> (or a matrix) after a loop. The result of this loop is
>> an array and I would like to get all the results in a
>> single table once the loop is finish. I have
>> reproduced a simplified example below:
>>
>>> a<- 1
>>> b <- matrix()
>>> while (a <= 4) {
>> + b<-rnorm(10)
>> + a<- a +1
>> + }
> 
> 
> Staying in your example:
> 
> a <- 1
> b <- matrix(nrow=10, ncol=4)
> while(a <= 4){
>      b[,i] <- rnorm(10)
>      a <- a + 1
> }
> 
> Uwe Ligges
> 
> 
> 
>> # I have tried several methods but without succes so
>> far. 
>>
>>> result <- as.data.frame(cbind(b))
>>> result
>>              b
>> 1  -0.03250661
>> 2  -0.59823770
>> 3   1.58120471
>> 4   0.41086546
>> 5   0.78959090
>> 6   1.23587125
>> 7   0.83427190
>> 8   1.09035581
>> 9   0.11331056
>> 10  0.25267231
>>
>> the result above is not what I am looking for because
>> it shows only the results from the last operation in
>> the loop, I would like to obtain something like the
>> following.
>>
>>> result
>>             b1         b2          b3         b4
>> 1  -0.08420826 -0.0637943 -0.83246201 -1.0902384
>> 2   0.40623009 -2.7940096  1.37664973  1.6023967
>> 3  -1.13850505  1.1660669 -0.95962296 -0.7325098
>> 4  -1.06183391  1.1063677  1.67948677 -1.9875475
>> 5  -0.64431067 -0.4843952 -1.30742858  0.5064134
>> 6   0.56729468  1.0860484  0.07651954  0.4380108
>> 7   0.95036177 -0.5328609  1.28954934 -0.2775614
>> 8  -0.17848223  0.5340379 -0.22613700  1.0179886
>> 9   2.93145454 -1.8639607 -0.25478610  1.6619754
>> 10  1.51942415 -0.2051423  0.09144450 -1.6329481
>>
>> I hope someone can give me a hand
>>
>> best regards
>>
>> a
>>
>>
>>
>>
>>     
>>  p5.vert.ukl.yahoo.com uncompressed/chunked Sat Aug 12 13:14:00 GMT 2006
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Sat Aug 12 19:30:33 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 12 Aug 2006 13:30:33 -0400 (EDT)
Subject: [R] Re :  adding columns to a table after a loop
In-Reply-To: <20060812162244.65919.qmail@web26902.mail.ukl.yahoo.com>
Message-ID: <20060812173033.68059.qmail@web32801.mail.mud.yahoo.com>


--- Albert Picado <albert_picado at yahoo.fr> wrote:

Albert 

> Dear Uwe,
>  
> Thanks for your answer. 

----clip

> De : Uwe Ligges <ligges at statistik.uni-dortmund.de>
> ??? : Albert Picado <albert_picado at yahoo.fr>
> Cc : r-help at stat.math.ethz.ch
> Envoy??? le : Samedi, 12 Ao???t 2006, 3h57mn 16s
> Objet : Re: [R] adding columns to a table after a
> loop
> 
> 
> Albert Picado wrote:
> > Dear list,
> > 
> > I am trying to find the way to add columns to a
> table
> > (or a matrix) after a loop. The result of this
> loop is
> > an array and I would like to get all the results
> in a
> > single table once the loop is finish. I have
> > reproduced a simplified example below:
> > 
> >> a<- 1
> >> b <- matrix()
> >> while (a <= 4) {
> > + b<-rnorm(10)
> > + a<- a +1
> > + }
> 
> 
> Staying in your example:
> 
> a <- 1
> b <- matrix(nrow=10, ncol=4)
> while(a <= 4){
>      b[,i] <- rnorm(10)
>      a <- a + 1
> }
> 
> Uwe Ligges

There is a minor typo in the above  example. b[,1]
should read b[,a].  It works very nicely with that
small correction.


From binabina at bellsouth.net  Sat Aug 12 23:23:23 2006
From: binabina at bellsouth.net (zubin)
Date: Sat, 12 Aug 2006 17:23:23 -0400
Subject: [R] lasso for variable selection
In-Reply-To: <44BDAAFB.4050306@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44BDAAFB.4050306@bellsouth.net>
Message-ID: <44DE46CB.8010304@bellsouth.net>

Attended JSM last week and Friedman mentioned the use of LASSO for 
variable selection (he uses it for rules ensembles).  I am an 
econometrician and not familiar with, i started running the examples in 
R this week and you get to the plots section of the LARS package.   
Plots of beta/max(beta)  vs standardized coefficients.  How does one 
interpret them?  u see plots of each variable converging to zero at 
different times - its pretty cool - but can i use this for variable 
importance?

for variable selection - i have a group of correlated variables that we 
need to determine importance in predicting change of a Y variable.

-zubin


From liuwensui at gmail.com  Sun Aug 13 00:04:03 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 12 Aug 2006 18:04:03 -0400
Subject: [R] lasso for variable selection
In-Reply-To: <44DE46CB.8010304@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44BDAAFB.4050306@bellsouth.net> <44DE46CB.8010304@bellsouth.net>
Message-ID: <1115a2b00608121504n24e5e997pca39c9422e4a09e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060812/89e14b5f/attachment.pl 

From binabina at bellsouth.net  Sun Aug 13 02:45:14 2006
From: binabina at bellsouth.net (zubin)
Date: Sat, 12 Aug 2006 20:45:14 -0400
Subject: [R] proc standardize & data frame x and y
In-Reply-To: <44D015C4.6080207@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44D015C4.6080207@bellsouth.net>
Message-ID: <44DE761A.4090803@bellsouth.net>

Hello!  i know these are basic but i cannot seem to find the answer thru 
my searches..

1) Can someone recommend an equivalent to SAS PROC Standardize in R?  I 
am in need to frequently standardize a data frame, with z-scores, or 
squash to 0-1 scale - is there a slick function or package someone can 
recommend?

2) Also, have data sets with a lot of predictor variables.  in the 
diabetes data frame i see that fields have been grouped to X and Y 
variables, making it very easy to identify X and Y in the regression 
techniques.  How is this done, how do you group lets say a group of 
columns into 1 matrix, within a data frame.  example: the AsIs group is 
a matrix of X variables:

 > str(diabetes)
`data.frame':   442 obs. of  3 variables:
 $ x : AsIs [1:442, 1:10] 0.038075.... -0.00188.... 0.085298.... 
-0.08906.... 0.005383.... ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : NULL
  .. ..$ : chr  "age" "sex" "bmi" "map" ...
  ..- attr(*, "class")= chr "AsIs"
 $ y : num  151 75 141 206 135 97 138 63 110 310 ...
 $ x2: AsIs [1:442, 1:64] 0.038075.... -0.00188.... 0.085298.... 
-0.08906.... 0.005383.... ...
  ..- attr(*, ".Names")= chr  "age" "age" "age" "age" ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr  "1" "2" "3" "4" ...
  .. ..$ : chr  "age" "sex" "bmi" "map" ...
  ..- attr(*, "class")= chr "AsIs"


From liuwensui at gmail.com  Sun Aug 13 02:55:03 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 12 Aug 2006 20:55:03 -0400
Subject: [R] proc standardize & data frame x and y
In-Reply-To: <44DE761A.4090803@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44D015C4.6080207@bellsouth.net> <44DE761A.4090803@bellsouth.net>
Message-ID: <1115a2b00608121755p59ab5e21q81e44cb25b3784a0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060812/6d099771/attachment.pl 

From spencer.graves at pdf.com  Sun Aug 13 03:26:17 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Aug 2006 18:26:17 -0700
Subject: [R] GARCH(1,1) optimization with R
In-Reply-To: <1a350e7b0608090746o4b25be0ckabc993c1ebd5a356@mail.gmail.com>
References: <1a350e7b0608090746o4b25be0ckabc993c1ebd5a356@mail.gmail.com>
Message-ID: <44DE7FB9.4090206@pdf.com>

	  RSiteSearch("garch", "functions") produced 21 hits, the first 10 of 
which identified 'garch'-type capabilities in packages 'tseries', 
'fSeries' and 'fOptions'.

	  Hope this helps.
	  Spencer Graves
p.s.  You might get better and quicker help from this listserve if your 
post conforms more closely to the posting guide 
"www.R-project.org/posting-guide.html", including providing commented, 
minimal, self-contained, reproducible code.  Also, you may be interested 
in the "R-sig-finance" listserve for the Special Interest Group for 'R 
in Finance'.

Patrick Zhang wrote:
>  Hello all,
> 
> Trying to implement GARCH(1,1) with ASP.NET <http://asp.net/> and
> VB.NET<http://vb.net/>.
> It involves optimization of a three-variate function with some constraints.
> Learned from Wilmott.com <http://wilmott.com/> that R might be able to do it
> but have no idea how.  Could anyone help me out please.  Thanks in advance.
> 
> Additional info:
> 1. Tried calling Excel Solver from within my web application - it works fine
> except that Excel.exe won't go away from task manager although the Quit()
> method has been used;
> 2. Also tried running (Process.Start) a separate console application that
> calls Excel Solver from the code, getting error message: The application
> failed to initialize properly (0xc0000142).
> Any thought of an alternative?
> 
> Best wishes,
> Pat
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From liuwensui at gmail.com  Sun Aug 13 07:31:21 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 13 Aug 2006 01:31:21 -0400
Subject: [R] proc standardize & data frame x and y
In-Reply-To: <44DE761A.4090803@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44D015C4.6080207@bellsouth.net> <44DE761A.4090803@bellsouth.net>
Message-ID: <1115a2b00608122231r75deef4bi4061019ed913e3f3@mail.gmail.com>

zubin,

for your second question:

supposed you have x1 and x2 and want to combine them in a matrix X in
a data frame called data, try the following code:

X<-matrix(1:10, ncol = 2, dimnames = list(NULL, c("x1", "x2")));
class(X)
class(X)<-"AsIs";
class(X)
data<-data.frame(X);
summary(data);



On 8/12/06, zubin <binabina at bellsouth.net> wrote:
> Hello!  i know these are basic but i cannot seem to find the answer thru
> my searches..
>
> 1) Can someone recommend an equivalent to SAS PROC Standardize in R?  I
> am in need to frequently standardize a data frame, with z-scores, or
> squash to 0-1 scale - is there a slick function or package someone can
> recommend?
>
> 2) Also, have data sets with a lot of predictor variables.  in the
> diabetes data frame i see that fields have been grouped to X and Y
> variables, making it very easy to identify X and Y in the regression
> techniques.  How is this done, how do you group lets say a group of
> columns into 1 matrix, within a data frame.  example: the AsIs group is
> a matrix of X variables:
>
>  > str(diabetes)
> `data.frame':   442 obs. of  3 variables:
>  $ x : AsIs [1:442, 1:10] 0.038075.... -0.00188.... 0.085298....
> -0.08906.... 0.005383.... ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : NULL
>   .. ..$ : chr  "age" "sex" "bmi" "map" ...
>   ..- attr(*, "class")= chr "AsIs"
>  $ y : num  151 75 141 206 135 97 138 63 110 310 ...
>  $ x2: AsIs [1:442, 1:64] 0.038075.... -0.00188.... 0.085298....
> -0.08906.... 0.005383.... ...
>   ..- attr(*, ".Names")= chr  "age" "age" "age" "age" ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr  "1" "2" "3" "4" ...
>   .. ..$ : chr  "age" "sex" "bmi" "map" ...
>   ..- attr(*, "class")= chr "AsIs"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From spencer.graves at pdf.com  Sun Aug 13 09:18:14 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 13 Aug 2006 00:18:14 -0700
Subject: [R] Plot with Julian dates.
In-Reply-To: <20060809192024.M95997@ces.clemson.edu>
References: <20060809192024.M95997@ces.clemson.edu>
Message-ID: <44DED236.3010909@pdf.com>

	  1.  Are you familiar with the zoo package and vignette (as described, 
e.g., in
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/60217.html)?

	  2.  If yes and if you would like further help from this listserve, 
PLEASE do read the posting guide "www.R-project.org/posting-guide.html" 
and provide commented, minimal, self-contained, reproducible code with 
future posts.  The absence of appropriately "commented, minimal, 
self-contained, reproducible code" inhibits me from commenting further 
on this issue at this time.

	  Hope this helps.
	  Spencer Graves

C. Park wrote:
> Dear Sir/Madam:
> 
> I simply want to draw x-y plot with Julian dates (x) and numbers (y).
> Please see below for my program.
> 
> In the older version of R, the plot(jdat, miles) worked without any problem.
> But, with the new version of R, plot(jdat, miles) does not work any more. 
> So, I added log="" option (as far as I know, it is a default setting, so
> log="" should not be needed). 
> Now, it works with log="" option. But it is weird to me. 
> 
> I also check
> plot( jdat, miles, log="", type="p")
> 
> The option type="p" is also a default. So it should be the same as 
> plot( jdat, miles, log=""). The figures are the same, but the one with 
> type="p" option yields a warning message. 
> 
> Do you think it is a R bug?
> Thanks for your valuable time.
> C. Park
> 
> Note: I used the following R version                         
> platform       i686-redhat-linux-gnu     
> arch           i686                      
> os             linux-gnu                 
> system         i686, linux-gnu           
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
> 
> ##=========================================
> ## Here is my program
> ##=========================================
> 
> library(survival)
> 
> dat = c( "12-5-1995",  "12-30-1995", "8-27-1996", "11-20-1996", "8-29-1997",
>          "11-7-1997",  "4-4-1998",   "7-15-1998", "11-2-1998",  "1-16-1999" )
> miles = c(106,   1468, 7298, 7935, 13440,  14460, 17132, 20214, 22802, 24724 )
> jdat = as.date(dat)   ## Convert to Julian date form.
> 
>  plot( jdat, miles )   ## Not working
> Error in plot.window(xlim, ylim, log, asp, ...) : 
>         "log=" specification must be character
> 
>  plot( jdat, miles, log="" )  ## Works
> 
>  plot( jdat, miles, log="", type="p") ## Works, but warning messages
> Warning message:
> graphical parameter "type" is obsolete in: axis(side, at, labels, tick, line,
> pos, outer, font, lty, lwd,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Sun Aug 13 13:27:59 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 13 Aug 2006 04:27:59 -0700
Subject: [R] proc standardize & data frame x and y
In-Reply-To: <44DE761A.4090803@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44D015C4.6080207@bellsouth.net> <44DE761A.4090803@bellsouth.net>
Message-ID: <f8e6ff050608130427t2534a7f1v35100f9fa06ecd45@mail.gmail.com>

> 1) Can someone recommend an equivalent to SAS PROC Standardize in R?  I
> am in need to frequently standardize a data frame, with z-scores, or
> squash to 0-1 scale - is there a slick function or package someone can
> recommend?

You could try rescaler in the reshape package.  It currently supports
five column wise rescaling/standardisation methods (common range,
common variance, robust equivalent, rank, do nothing), and I could add
more if needed.

Regards,

Hadley


From miczat at yahoo.com  Sun Aug 13 14:23:34 2006
From: miczat at yahoo.com (Michael Zatorsky)
Date: Sun, 13 Aug 2006 22:23:34 +1000 (EST)
Subject: [R] Vector Join
Message-ID: <20060813122334.54842.qmail@web51112.mail.yahoo.com>

Hi,

I'm working on producing a simple cumulative frequency
distribution.

Thanks to the help of the good people on this list I
now have four vectors that I'd like to join/relate
into a table. e.g.


v1 <- myHistogram$breaks             # classes
v2 <- myHistogram$counts             # freqs
v3 <- cumsum(v2)                     # cumulative freq
v4 <- ((v3 / length(myData)) * 100)  # cumulative %


What is the recommend approach to turning these into a
single table with four columns?  ie effectively doing
a relational join on row id? 

The goal is to ultimately have the data with one row
per class in a format I can write out to a text file
as:

  v1    v2    v3    v4
  v1    v2    v3    v4
  etc...

Any advice will be appreciated.

Regards
Michael.




		
____________________________________________________ 

Coming soon: Celebrity Survivor - 11 celebrities, 25 days, unlimited drama


From phhs80 at gmail.com  Sun Aug 13 14:34:01 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Sun, 13 Aug 2006 13:34:01 +0100
Subject: [R] Vector Join
In-Reply-To: <20060813122334.54842.qmail@web51112.mail.yahoo.com>
References: <20060813122334.54842.qmail@web51112.mail.yahoo.com>
Message-ID: <6ade6f6c0608130534m2b05003hb3fa3a8d293c08ed@mail.gmail.com>

On 8/13/06, Michael Zatorsky <miczat at yahoo.com> wrote:
> I'm working on producing a simple cumulative frequency
> distribution.
>
> Thanks to the help of the good people on this list I
> now have four vectors that I'd like to join/relate
> into a table. e.g.
>
>
> v1 <- myHistogram$breaks             # classes
> v2 <- myHistogram$counts             # freqs
> v3 <- cumsum(v2)                     # cumulative freq
> v4 <- ((v3 / length(myData)) * 100)  # cumulative %
>
>
> What is the recommend approach to turning these into a
> single table with four columns?  ie effectively doing
> a relational join on row id?
>
> The goal is to ultimately have the data with one row
> per class in a format I can write out to a text file
> as:
>
>   v1    v2    v3    v4
>   v1    v2    v3    v4
>   etc...
>
> Any advice will be appreciated.

An example follows:

> V1 <- 1:4
> V2 <- 3:6
> V3 <- 2:5
> m <- matrix(nrow=4,ncol=3)
> m
     [,1] [,2] [,3]
[1,]   NA   NA   NA
[2,]   NA   NA   NA
[3,]   NA   NA   NA
[4,]   NA   NA   NA
> m[,1] <- V1
> m[,2] <- V2
> m[,3] <- V3
> m
     [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    4    3
[3,]    3    5    4
[4,]    4    6    5
>

Paul


From dirk.enzmann at uni-hamburg.de  Sun Aug 13 15:21:51 2006
From: dirk.enzmann at uni-hamburg.de (Dirk Enzmann)
Date: Sun, 13 Aug 2006 15:21:51 +0200
Subject: [R] How to reply to a thread if receiving R-help mails in digest
	form
Message-ID: <44DF276F.3070903@uni-hamburg.de>

I receive R-help messages in digest form that makes it difficult to 
answer a post. I noticed that my answer is not added to the thread 
(instead, a new thread is started) although I use the same subject line 
(starting with "Re: ") as the original post. Is there a solution (I 
prefer the digest to separate messages for several reasons and don't 
want to change my email reader)?

The way I answer post up to now is:
1) I press the reply button of my email program (Mozilla / Thunderbird, 
Windows)
2) I delete all contents of the digest except for the post (including 
name and mail address of the posting person) I want to answer so that 
the original question will be included (cited) in my answer.
3) I add the email address to the individual sender to "cc:" to the 
automatically generated address of the R-help list.
4) I replace the automatically generated subject line (for example "Re: 
R-help Digest, Vol 42, Issue 13" by "Re: " followed by a copy of the 
original subject line of the post.
5) I write my answer and send the mail to the mailing list.

It's not that this is tedious - the problem is that the thread is 
broken. Is there a better way even if I want to keep receiving messages 
in digest form? The posting guide is silent about this.

Dirk

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-(0)40-42838.7498 (office)
        +49-(0)40-42838.4591 (Billon)
fax:   +49-(0)40-42838.2344
email: dirk.enzmann at uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html


From Ted.Harding at nessie.mcc.ac.uk  Sun Aug 13 16:21:15 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 13 Aug 2006 15:21:15 +0100 (BST)
Subject: [R] How to reply to a thread if receiving R-help mails in di
In-Reply-To: <44DF276F.3070903@uni-hamburg.de>
Message-ID: <XFMail.060813152115.Ted.Harding@nessie.mcc.ac.uk>

On 13-Aug-06 Dirk Enzmann wrote:
> I receive R-help messages in digest form that makes it
> difficult to answer a post. I noticed that my answer is
> not added to the thread (instead, a new thread is started)
> although I use the same subject line (starting with "Re: ")
> as the original post. Is there a solution (I prefer the
> digest to separate messages for several reasons and don't 
> want to change my email reader)?
> 
> The way I answer post up to now is:
> 1) I press the reply button of my email program (Mozilla /
> Thunderbird, Windows)
> 2) I delete all contents of the digest except for the post
> (including name and mail address of the posting person)
> I want to answer so that the original question will be
> included (cited) in my answer.
> 3) I add the email address to the individual sender to "cc:"
> to the automatically generated address of the R-help list.
> 4) I replace the automatically generated subject line (for
> example "Re: R-help Digest, Vol 42, Issue 13" by "Re: "
> followed by a copy of the original subject line of the post.
> 5) I write my answer and send the mail to the mailing list.
> 
> It's not that this is tedious - the problem is that the
> thread is broken. Is there a better way even if I want to
> keep receiving messages in digest form? The posting guide
> is silent about this.
> 
> Dirk

It may be that you are stuck with the "new thread" phenomenon,
though others more knowledgeable that I am may have other ideas.

It depends (at least in part) on headers associated with each
individual message in the digest, as well as the subject itself.
Again, since I don't receive R-help in digest form, I can't tell
whether these are helpful.

If you are receiving email "normally" from a list, you will
find headers like "In-Reply-To:" and "References:" in a message
which continues a thread, as well as a "Message-ID:" for the
message itself. An example from another list:

In-Reply-To: <20060811180225.GA16232 at mx0.halon.org.uk>
Message-ID: <Pine.LNX.4.64.0608121102380.27772 at seagull>
References: <20060811180225.GA16232 at mx0.halon.org.uk>

This enables mailing-list software to keep track of the messages
in a thread, since the "previous" message is identified in the
"In-Reply-To:" and "References:" headers by its own "Message-ID:",
and the "Message-ID:" for the current message can be picked up
and used in a similar way in further follow-ups.

Now I am currently receiving allstat in digest form. When I look
at the headers for each message in the digest I see the likes of
(which *is* a reply to a preceding message and therefore the
second message in a thread):

Date:    Sun, 6 Aug 2006 11:34:41 -0700
From:    Steve <steve_beaney at YAHOO.COM>
Subject: Re: Unsubscribe!
MIME-Version: 1.0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit

and so there is nothing in these headers to identify the message
that it is a reply to (the subject line not being a unique
identifier, of course).

The "Subject:" itself doesn't really count. Indeed, often people
start a new topic on a list by "replying" to a message, simply
to lazily avoid entering the list address. So they press the reply
button and preserve the list address, then delete everything else
and start "afresh" with a new subject line and whatever they want
to say, which has nothing to do with the thread they are "replying"
to.

But, as people who've experienced this will know, the effect is
that it will be handled as if it were a continuation of the thread,
albeit with a new subject. When others on turn reply to that message,
these replies (on a different topic) will get wrapped into the old
thread. This is called "hijacking the thread". When you visit the
list archives, you will find threads with a block of messages
in the middle which have "changed the subject". And all because
of the above headers.

So, unless the R digestifier is obliging enough to insert thread
referencing headers for each message in the digest ("Message-ID:"
headers at least), unlike allstat, I don't think you can avoid
starting a new thread with your reply.

==========================================

That being said, if you receive a lot of list digests it may still
be worth considering a change of email client (assuming that Mozilla
or Firefox cannot be configured to handle digests appropriately).

>From your description of how you reply, it seems that you see
every message at once in the "display message" window.

A digest is a type of MIME structure, denoted as

  Content-Type: multipart/digest; boundary="..."

and in effect each part is like an "attachment". If your email
client recognises this structure, you should see a list of the
separate parts of which the first will describe the subjects
in the remaining parts. For example, the digest I took the above
examples from has 3 parts which show as:

1 message/rfc822    8-bit    us-ascii
2 message/rfc822    8-bit    us-ascii
3 message/rfc822    8-bit    us-ascii

and if I open the first one I see

  There are 2 messages totalling 77 lines in this issue.

  Topics of the day:

    1. Unsubscribe! (2)

which tells me that there are two messages, each with the
same topic "Unsubscribe!".

Number 2 is the original "Unsubscribe!" message, and number 3
is the reply to it (cited above).

In my XFMail client (not available as far as I know for Windows),
if I double/middle click on any one of these three I read that
message directly. It also has a "Next" and "Previous" button on
any open message, so I can scroll sequentially through them,
seeing only one message at a time (and not getting them all
at once). I can reply-to/forward etc. any single message.

This has to be more convenient for reading and dealing with
digest format than the all-at-once you are experiencing in
Mozilla/Firefox!

Hoping this is useful,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Aug-06                                       Time: 15:21:09
------------------------------ XFMail ------------------------------


From jrkrideau at yahoo.ca  Sun Aug 13 16:48:38 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 13 Aug 2006 10:48:38 -0400 (EDT)
Subject: [R] How to order or sort a data.frame
Message-ID: <20060813144838.99767.qmail@web32814.mail.mud.yahoo.com>

I have a dataframe where I would like to order first
by  variable, year, and then within that variable by
month.

So far the only way that I have seen to do this is to
order by year and then subset year and sort by month
and then do an rbind to get things back together.  

Is this the right approach?  

Example:

us.state <-rep("California", 23)                      
                         
count <- c(774,283,774,283,508,283,774,283,602,283,   
                         
774,508,0,602,330,283,283,283,602,301,126, NA,301)    
                         
year <- c(2002,  2003, 2001, 2002, 2001, 2002, 2001,
2002, 2002, 2003,          
          2002, 2002, 2001,  2002, 2001, 2002, 2001,
2002, 2001, 2002,          
          2001, 2001, 2002)                           
                         
month <- c( 1, 1, 10, 10, 11, 11, 12, 12,             
                         
            2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9,
9)                        
                                                      
                         
                                                      
                         
df <- data.frame (cbind(us.state,count, year, month)) 
                         
# ordering as a factor works here                     
                         
df1 <- df[order(df$year),]                            
                         
df1                                                   
                         
                                                      
                         
df2 <- subset(df1, year==2001)                        
                         
                                                      
                         
# ordering as a factor works but not a good
appearance.                         
                                                      
                         
df3 <- df2[order(as.numeric(df2$month)),]             
                         
df3                                                   
                         


This works but  "month" is ordered as  a factor and I
would prefer to coerce it into a numeric for
presentation purposes but 
 df3 <- df2[order(as.numeric(df2$month)),] does not
seem to work,  nor has a couple of other things I've
tried.  

Any suggestions gratefully received.


From dieter.menne at menne-biomed.de  Sun Aug 13 16:50:22 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 13 Aug 2006 14:50:22 +0000 (UTC)
Subject: [R]
	=?utf-8?q?How_to_reply_to_a_thread_if_receiving_R-help_mails_?=
	=?utf-8?q?in_digest=09form?=
References: <44DF276F.3070903@uni-hamburg.de>
Message-ID: <loom.20060813T164832-306@post.gmane.org>

Dirk Enzmann <dirk.enzmann <at> uni-hamburg.de> writes:

> 
> I receive R-help messages in digest form that makes it difficult to 
> answer a post. I noticed that my answer is not added to the thread 
> (instead, a new thread is started) although I use the same subject line 
> (starting with "Re: ") as the original post. Is there a solution (I 
> prefer the digest to separate messages for several reasons and don't 
> want to change my email reader)?
> 
Dirk,

this is one of the reasons I dislike the list format, but that's a subject has
been discussed already several times, with no hope of change.

I don't use the digest, but rely on 

http://news.gmane.org/gmane.comp.lang.r.general

instead.


Dieter


From dieter.menne at menne-biomed.de  Sun Aug 13 16:56:38 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 13 Aug 2006 14:56:38 +0000 (UTC)
Subject: [R] How to order or sort a data.frame
References: <20060813144838.99767.qmail@web32814.mail.mud.yahoo.com>
Message-ID: <loom.20060813T165447-753@post.gmane.org>

John Kane <jrkrideau <at> yahoo.ca> writes:

> 
> I have a dataframe where I would like to order first
> by  variable, year, and then within that variable by
> month.

http://www.ats.ucla.edu/STAT/r/faq/sort.htm

and

http://wiki.r-project.org/rwiki/doku.php?id=misc:rtips-status

Dieter


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Aug 13 17:03:28 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 13 Aug 2006 17:03:28 +0200
Subject: [R] How to order or sort a data.frame
In-Reply-To: <20060813144838.99767.qmail@web32814.mail.mud.yahoo.com>
References: <20060813144838.99767.qmail@web32814.mail.mud.yahoo.com>
Message-ID: <20060813170328.u1yxodsmee5cgks8@webmail3.kuleuven.be>

try the following:

mdf <- data.frame(us.state, count, year, month)
mdf[order(mdf$year, mdf$month), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting John Kane <jrkrideau at yahoo.ca>:

> I have a dataframe where I would like to order first
> by  variable, year, and then within that variable by
> month.
>
> So far the only way that I have seen to do this is to
> order by year and then subset year and sort by month
> and then do an rbind to get things back together.
>
> Is this the right approach?
>
> Example:
>
> us.state <-rep("California", 23)
>
> count <- c(774,283,774,283,508,283,774,283,602,283,
>
> 774,508,0,602,330,283,283,283,602,301,126, NA,301)
>
> year <- c(2002,  2003, 2001, 2002, 2001, 2002, 2001,
> 2002, 2002, 2003,
>           2002, 2002, 2001,  2002, 2001, 2002, 2001,
> 2002, 2001, 2002,
>           2001, 2001, 2002)
>
> month <- c( 1, 1, 10, 10, 11, 11, 12, 12,
>
>             2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9,
> 9)
>
>
>
>
> df <- data.frame (cbind(us.state,count, year, month))
>
> # ordering as a factor works here
>
> df1 <- df[order(df$year),]
>
> df1
>
>
>
> df2 <- subset(df1, year==2001)
>
>
>
> # ordering as a factor works but not a good
> appearance.
>
>
> df3 <- df2[order(as.numeric(df2$month)),]
>
> df3
>
>
>
> This works but  "month" is ordered as  a factor and I
> would prefer to coerce it into a numeric for
> presentation purposes but
>  df3 <- df2[order(as.numeric(df2$month)),] does not
> seem to work,  nor has a couple of other things I've
> tried.
>
> Any suggestions gratefully received.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From spencer.graves at pdf.com  Sun Aug 13 18:40:04 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 13 Aug 2006 09:40:04 -0700
Subject: [R] How to fit bivaraite longitudinal mixed model ?
In-Reply-To: <7193991f0608100154m5296632bq1804439ba37b5d4d@mail.gmail.com>
References: <7193991f0608100154m5296632bq1804439ba37b5d4d@mail.gmail.com>
Message-ID: <44DF55E4.2060507@pdf.com>

	  Have you considered the 'nlme' package, well documented in Pinheiro 
and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer)?

	  Hope this helps.
	  Spencer Graves

souvik banerjee wrote:
> Hi
> Is there any way to fit a bivaraite longitudinal mixed model using R. I have
> a data set with col names
> 
> resp1 (Y_ij1),  resp2 (Y_ij2),  timepts (t_ij),  unit(i)
> 
>  j=1,2,..,m  and  i=1,2,..n.
> 
> I want to fit the following two models
> 
> Model 1
> 
> Y_ij1, Y_ij2 | U_i = u_i ~ N(alpha + u_i + beta1*t_ij, Sigma)
> U_i ~ iid N(0, sigu^2)
> 
> Sigma = bivariate AR structure
> 
> alpha and beta are vectors of order 2.
> 
> Model 2
> Y_ij, Y_ij2  | U_i = u_i ~ N(alpha + u_i + beta1*t_ij + U_i2*t_ij, Sigma)
> 
> U_i=(U_i1,U_i2) ~ iid N(0, Omega)
> 
> Sigma =bivariate AR structure
> 
> Here alpha and beta are vectors of order 2.
> 
> Moreover can we assume a structure for Omega.
> 
> Any help is greatly appreciated.
>  Souvik Banerjee
> Junior Research Fellow
> Dept of Statistics
> University of calcutta
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rashmim at sfu.ca  Sun Aug 13 20:48:03 2006
From: rashmim at sfu.ca (Rashmi Mathur)
Date: Sun, 13 Aug 2006 11:48:03 -0700
Subject: [R] split a y-axis to show data on different scales
Message-ID: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060813/b1ddf8e9/attachment.pl 

From h.wickham at gmail.com  Sun Aug 13 21:13:46 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 13 Aug 2006 16:13:46 -0300
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
Message-ID: <f8e6ff050608131213v7c9bd888w22ec92fd9685ccc3@mail.gmail.com>

> How do I split a y-axis to plot data on different scales?

The short answer: you shouldn't.  The whole point of plotting the data
is so that you can compare them visually on the same scale.  As soon
as you split the scales you can no longer do this, and you effectively
have two separate graphs.  This suggests how you should solve your
problem - create two graphs, one for each separate scale.

Hadley


From ff809 at ncf.ca  Sun Aug 13 22:29:28 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Sun, 13 Aug 2006 16:29:28 -0400
Subject: [R] Puzzling warning using 2.3.1...
Message-ID: <44DF8BA8.9010408@ncf.ca>

Greetings folks:

Stepped away from a win/lin dual boot system and spent the better part of the 
past week resetting a windows only arrangement.

Before the switch I was running 2.3.1 without any strange behaviours. When I 
reached the point of putting it back on the machine I would get this puzzling 
warning the first time I would try to select a cran mirror:

 > chooseCRANmirror()
Warning message:
DLL attempted to change FPU control word from 8001f to 9001f
 >

When I pulled it off and reinstalled 2.2.1 I got no such warning.

Thoughts or suggestions? Could the file have been damaged or corrupted in same 
way? The Md5sum checked out but I'm not sure how accurate an assessment that is 
of the file's integrity. Should I just download a new copy and try again with 
that? Not sure if it's important but I run a Win98se setup.

-- 
Brian Lunergan
Nepean, Ontario
Canada


---


From tlumley at u.washington.edu  Sun Aug 13 23:20:13 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 13 Aug 2006 14:20:13 -0700 (PDT)
Subject: [R] Puzzling warning using 2.3.1...
In-Reply-To: <44DF8BA8.9010408@ncf.ca>
Message-ID: <Pine.LNX.4.43.0608131420130.26951@hymn05.u.washington.edu>

On Sun, 13 Aug 2006, Brian Lunergan wrote:

> Greetings folks:
>
> Stepped away from a win/lin dual boot system and spent the better part of the
> past week resetting a windows only arrangement.
>
> Before the switch I was running 2.3.1 without any strange behaviours. When I
> reached the point of putting it back on the machine I would get this puzzling
> warning the first time I would try to select a cran mirror:
>
> > chooseCRANmirror()
> Warning message:
> DLL attempted to change FPU control word from 8001f to 9001f
> >

This means that some other program (often a video driver) rudely changed the floating point control word, but that R caught the change and stopped it.  From R's point of view the problem was prevented.  Now, it is possible that the other program thinks the floating point control word is still set the way it wants, in which case it might have problems with numerical accuracy.  R should be ok, though.



> When I pulled it off and reinstalled 2.2.1 I got no such warning.
>
> Thoughts or suggestions? Could the file have been damaged or corrupted in same
> way? The Md5sum checked out but I'm not sure how accurate an assessment thatis
> of the file's integrity.

If the MD5 hashes agree then the files are the same as when the installer was built.  There is negligible probability that a change in the file will leave the MD5 hash the same.


       -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From jrkrideau at yahoo.ca  Sun Aug 13 23:31:25 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 13 Aug 2006 17:31:25 -0400 (EDT)
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
Message-ID: <20060813213125.26891.qmail@web32804.mail.mud.yahoo.com>


--- Rashmi Mathur <rashmim at sfu.ca> wrote:

> Hello,
> 
> How do I split a y-axis to plot data on different
> scales?
> 
> Eg:
> 
> x <- 1:10
> y <-
>
c(-0.01,0.79,0.74,0.55,-0.67,0.32,-0.47,-0.05,723,759)
> plot(x,y)
> 
> I'd like to show these data on the same plot, but
> the way it's written, all
> contrast in the first 8 data points is lost.  Can R
> split a y-axis for me?
> 
> Thanks,
> Rashmi
> 
 As Hadley said: Don't. It can lead to
misinterpretations.  You are better off if you plot
two graphs.  Consider something like using
par(mfrow)and place the two graphs one above the other
on the same page. This will let you use a common
x-axis but avoids the split problem.

Quick and dirty example:

x <- c(1:10)
y <- c(1:10)
z <- c(21:30)
par(mfrow= c(2,1))
plot(x,y)
plot(x,z)

Have a look at Cleveland's book  The Elements of
Graphing Data (Revised Edition). W. S. Cleveland
(1994). Hobart Press, Summit, New Jersey for some
suggestions and the reason a split y-axis is not a
good idea.


From ripley at stats.ox.ac.uk  Sun Aug 13 23:39:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 13 Aug 2006 22:39:01 +0100 (BST)
Subject: [R] Puzzling warning using 2.3.1...
In-Reply-To: <44DF8BA8.9010408@ncf.ca>
References: <44DF8BA8.9010408@ncf.ca>
Message-ID: <Pine.LNX.4.64.0608132206030.23792@gannet.stats.ox.ac.uk>

On Sun, 13 Aug 2006, Brian Lunergan wrote:

> Greetings folks:
> 
> Stepped away from a win/lin dual boot system and spent the better part 
> of the past week resetting a windows only arrangement.
> 
> Before the switch I was running 2.3.1 without any strange behaviours. 
> When I reached the point of putting it back on the machine I would get 
> this puzzling warning the first time I would try to select a cran 
> mirror:
> 
>  > chooseCRANmirror()
> Warning message:
> DLL attempted to change FPU control word from 8001f to 9001f
>  >
> 
> When I pulled it off and reinstalled 2.2.1 I got no such warning.
> 
> Thoughts or suggestions? 

This is a warning about a problem with your OS, not with R.

> Could the file have been damaged or corrupted in same way? The Md5sum 
> checked out but I'm not sure how accurate an assessment that is of the 
> file's integrity. Should I just download a new

Did you run md5check?: that is very thorough.  See the rw-FAQ Q2.3.

> copy and try again with that? Not sure if it's important but I run a 
> Win98se setup.

This is covered by rw-FAQ Q2.21 (although that will not be obvious to 
you, but please read it now).

It indicates a problem with some *non-R* DLL on your machine, and not a 
problem with R itself.  R 2.3.1 looks up the latest list of mirrors online 
(and is much more aggressively protected against rogue DLLs), and 2.2.1 
did not, so my guess is that it is your winsock or other internet access 
DLLs.

R has corrected the problem for you.  Your OS have been end-of-lined by 
Microsoft long ago, but I do suggest you ensure it is as fully updated as 
possible (including Internet Explorer), assuming that using a current OS 
is out of the question.  (R support for that OS is likely to be withdrawn 
in the near future: it was somewhat unexpected that 2.3.x still worked 
on Win98 since it uses features that were said in some accounts not to 
be present.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rod.ball at ensisjv.com  Sun Aug 13 23:59:53 2006
From: rod.ball at ensisjv.com (Rod Ball)
Date: Mon, 14 Aug 2006 09:59:53 +1200
Subject: [R] lme4 and lmeSplines
In-Reply-To: <44D9F2A7.5070603@pdf.com>
References: <adf71a630608021225u73a2a547i9ba43fb930980a04@mail.gmail.com>
	<44D9F2A7.5070603@pdf.com>
Message-ID: <200608140959.53689.rod.ball@ensisjv.com>

The source of the problem is that lmeSplines requires a set of iid random 
effects corresponding to a transformed smoothing spline basis (`pdIdent' 
structure in nlme) i.e. pdIdent(~Zs -1),  where Zs is the transformed set of 
spline basis functions. Kevin's model does not work because each of his 
random effects has a separately estimated variance parameter, equivalent to a 
`pdDiag' structure in nlme. lmeSplines really does require nlme.

There is currently no way in lme4 to get the equivalent of a pdIdent 
structure, i.e. set of iid random effects with common variance parameter (see 
mesage from Doug Bates below), unless these correspond to a grouping factor 
as in (~1|row). There are a number of other nlme models e.g. genetics models, 
that cannot be fitted in lme4, examples are in my message to Doug Bates 
below.

Regards,
Rod Ball
-- 
Dr Roderick D. Ball, 
Statistician, 
Ensis Wood Quality - Linking Quality to Value
Ensis - The Joint Forces of CSIRO and SCION
Te Papa Tipu Innovation Park, 49 Sala Street
P.B. 3020, Rotorua, New Zealand
Phone: +64 7 343 5777
Facsimile: +64 7 348 0952
email: rod.ball at ensisjv.com
url: www.ensisjv.com

Re: pdIdent in lme4
Date: Mon, 7 Aug 2006 11:25:31 -0500
From: "Douglas Bates" <bates at stat.wisc.edu>
To: "Rod Ball" <rod.ball at ensisjv.com>
Cc: bates at wisc.edu, "Kevin Wright" <kwright68 at gmail.com>

Most of the time what would have been created using the pdIdent
structure in lme can be created from a model matrix in lmer.  However,
I don't know if this will apply to the spline basis functions.  I
imagine that what you want is to have a random effects for each of the
basis functions at each observation and then to externally impose the
structure of pdIdent on the variance-covariance matrix.  I'm afraid I
don't see a way of doing this in the current lmer structures.
However, I am documenting the structures much more extensively that
was done for the lme structures and also factoring the computation
into distinct steps.  Perhaps when I am done it will be possible for
someone to add in such a capability.

On 8/7/06, Rod Ball <rod.ball at ensisjv.com> wrote:
> Dear Doug,
>
> I have just being trying your lme4 package. The information I've read is 
that
> lme4 (lmer() function) is more general than lme. However I can't see how to
> do the equivalent of pdIdent(~Z -1) where Z is a matrix, or any pdIdent
> structure for that matter. Cf below or the help from my lmeSplines package.
> Type
> > library(lmeSplines)
> > ?smspline
> to see the example.
>
> I've had a query about how to do this (see below). The attempt below fails
> because adding multiple terms for each column of the Z-matrix gives a pdDiag
> structure, not a pdIdent structure, as needed for a smoothing spline. I 
can't
> see how to get a pdIdent structure at all, or how to use it. I did find a
> pdCompSymm class listed in lme4 on the web, but no indication how to use it,
> nor does it seem to be in my copy (R 2.3.1, just upgraded).
>
> > library(help=lme4)
>
>                 Information on package 'lme4'
>
> Description:
>
> Package:       lme4
> Version:       0.995-2
> Date:          2006-01-17
> Title:         Linear mixed-effects models using S4 classes
> Author:        Douglas Bates <bates at stat.wisc.edu> and Deepayan Sarkar
>                <deepayan at stat.wisc.edu>
> Maintainer:    Douglas Bates <bates at stat.wisc.edu>
> Description:   Fit linear and generalized linear mixed-effects models.
> Depends:       methods, R(>= 2.2.0), Matrix(>= 0.995-2), lattice
> Imports:       graphics, stats
> Suggests:      mlmRev
> SaveImage:     no
> LazyLoad:      yes
> License:       GPL version 2 or later
> Packaged:      Thu Jan 19 12:39:40 2006; bates
> Built:         R 2.3.1; ; 2006-08-02 20:15:09; unix
>
> Index:
>
> gsummary                Summarize a data frame by group
> lmList                  List of lm Objects with a Common Model
> lmList-class            Class "lmList"
> pooledSD                Extract pooled standard deviation
>
> Further information is available in the following vignettes in
> directory '/home/rod/R/library/lme4/doc':
>
> Implementation: Implementation Details (source, pdf)
>
> Another example is the heart rate data from your Rnews article. Suppose I 
want
> to examine if Drug effects and rates vary randomly with patients. How can I
> include for example Patient=pdIdent(~Drug -1)? Are there an lmer()
> equivalents for the following?
>
> fm4 <- update(fm1, random = list(Patient = ~ Time, Patient= pdIdent(~Drug
> -1)))
> or
> fm4a <- update(fm1, random=list(Patient = ~ Time, Patient= pdCompSymm(~Drug
> -1))
> or
> fm5 <- update(fm1, random=list(Patient = ~ Time, Patient= pdIdent(~Drug -1),
> Patient= pdIdent(~ Drug:Time-1)))
>
> Incidentally, I discovered also, when trying these examples, the intervals
> function does not seem to work in lme4, and the lattice package is loaded
> twice.
>
> > data("HR",package="SASmixed")
> > library(lme4)
> Loading required package: Matrix
> Loading required package: lattice
> Loading required package: lattice
> > fm1 <- lmer(HR ~ baseHR + Time*Drug + (1|Patient), data=HR)
> > intervals(fm1)
> Error: could not find function "intervals"
>
> Is lme4 intended to be a replacement for nlme, or only an alternative for
> certain models?
>
>
> Thanks,
> Rod Ball
> --
> Dr Roderick D. Ball,
> Statistician,
> Ensis Wood Quality - Linking Quality to Value
> Ensis - The Joint Forces of CSIRO and SCION
> Te Papa Tipu Innovation Park, 49 Sala Street
> P.B. 3020, Rotorua, New Zealand
> Phone: +64 7 343 5777
> Facsimile: +64 7 348 0952
> email: rod.ball at ensisjv.com
> url: www.ensisjv.com
>
>
> lme4 and lmeSplines
> Date: Wed, 2 Aug 2006 14:25:20 -0500
> From: "Kevin Wright" <kwright68 at gmail.com>
> To: r-help at stat.math.ethz.ch
>
> I'm trying to use the lmeSplines package together with lme4.
>
> Below is (1) an example of lmeSplines together with nlme (2) an
> attempt to use lmeSplines with lme4 (3) then a comparison of the
> random effects from the two different methods.
>
> (1)
>
> require(lmeSplines)
> data(smSplineEx1)
> dat <- smSplineEx1
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> dat$all <- rep(1,nrow(dat))
> times20 <- seq(1,100,length=20)
> Zt20 <- smspline(times20)
> dat$Zt20 <- approx.Z(Zt20, times20, dat$time)
> fit1.20 <- lme(y~time, data=dat, random=list(all=pdIdent(~Zt20-1)))
> # Loess model
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> # Spline model
> with(dat, lines(fitted(fit1.20)~time, col="red"))
> # Save random effects for later
> ranef.nlme <- unlist(ranef(fit1.20))
>
> (2) Now an attempt to use lme4:
>
> library(lmeSplines)
> detach(package:nlme)
> library(lme4)
> data(smSplineEx1)
> # Use 20 spline in lme4
> dat <- smSplineEx1
> times20 <- seq(1,100,length=20)
> Zt20 <- smspline(times20)
> dat <- cbind(dat, approx.Z(Zt20, times20, dat$time))
> names(dat)[4:21] <- paste("Zt",names(dat)[4:21],sep="")
> dat$all <- rep(1, nrow(dat))
> fit1.20 <- lmer(y~time
>
> 
 +(-1+Zt1|all)+(-1+Zt2|all)+(-1+Zt3|all)+(-1+Zt4|all)+(-1+Zt5|all)+(-1+Zt6|all)
>
> 
 +(-1+Zt7|all)+(-1+Zt8|all)+(-1+Zt9|all)+(-1+Zt10|all)+(-1+Zt11|all)+(-1+Zt12|all)
>
> 
 +(-1+Zt13|all)+(-1+Zt14|all)+(-1+Zt15|all)+(-1+Zt16|all)+(-1+Zt17|all)+(-1+Zt18|all),
>              data=dat)
> #summary(fit1)
> # Plot the data and loess fit
> dat.lo <- loess(y~time, data=dat)
> plot(dat.lo)
> # Fitting with splines
> with(dat, lines(fitted(fit1.20)~time, col="red"))
> ranef.lme4 <- unlist(ranef(fit1.20))
>
> (3) Compare nlme lme4 random effects
>
> plot(ranef.nlme~ranef.lme4)
>
> The plot of fitted values from lme4 is visually appealing, but the
> random effects from lme4 are peculiar--three are non-zero and the rest
> are essentially zero.
>
> Any help in getting lme4 + lmeSplines working would be appreciated.
> It is not unlikely that I have the lmer syntax wrong.
>
> Kevin Wright
>
>
>
>



On Thu, 10 Aug 2006 02:35, Spencer Graves wrote:
>    At least at one time, to use lmer after lme, you had to quit R
> before loading 'lme4' because of substantive conflicts between 'nlme'
> and 'lme4' / 'Matrix'.  That may be the source of your problem.  I can
> think of two possible ways to get around this:
>
> 	  1.  I might try quitting R and restarting after your use of
> 'lmeSplines' but before 'lmer'.
>
> 	  2.  If that failed, I might try making local copies of everything I
> needed from 'lmeSplines' and using them with 'lmer'.  If that failed,
> I'd use 'debug' to walk through the code (or local copies of whatever I
> needed, possibly with name changes) until I figured it out.
>
> 	  I know this is not what you wanted to hear, but I hope it helps.
> 	  Spencer Graves
>


From ryetimothy at gmail.com  Mon Aug 14 01:35:04 2006
From: ryetimothy at gmail.com (Timothy Rye)
Date: Sun, 13 Aug 2006 18:35:04 -0500
Subject: [R] Gower Similarity Coefficient
Message-ID: <79646f810608131635p32e0a9d1y777e0233fc3e7999@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060813/f5460ece/attachment.pl 

From izmirlig at mail.nih.gov  Mon Aug 14 04:39:22 2006
From: izmirlig at mail.nih.gov (Izmirlian, Grant (NIH/NCI) [E])
Date: Sun, 13 Aug 2006 22:39:22 -0400
Subject: [R] Geometrical Interpretation of Eigen value and Eigen vector
References: <mailman.11.1155290403.32412.r-help@stat.math.ethz.ch>
	<44DDB512.4090002@uni-hamburg.de>
Message-ID: <13F8170A4373B44286C0AAE19807E8CC69731F@NIHCESMLBX11.nih.gov>

Ok, I had a look at it. It seems like awefully far to dig for the main point which is easily 
summarized in a few sentences.

If we super-impose the pre-image and image spaces (plot the input and output in the same
picture), then in 1 dimension, a linear function, say 'a x', takes its input, x, and stretches
it by a factor |a|. If 'a' is negative, then the direction that 'x' points is reversed.

Understanding several dimensions, as is usually the case, requires us to refine our
understanding of the 1-dimensional case.  In several dimensions, a linear function, 
say 'A x' (where 'A' is an m by m matrix and 'x' is an 'm' vector) will result in the stretching
of the input, 'x', along the direction its pointing, by a factor 'a'. However, this is the case
_only_ if 'x' lies in one of the 'characteristic directions' corresponding to 'A'. Since 'A'
is an m by m matrix, there will be at most m such 'characteristic directions'.  Each of the
characteristic directions has its associated stretching factor.  The characteristic directions
are called eigenvectors and the corresponding stretching factors are called eigenvalues.

Think about what this means in 1-dimension (hint: there's only one dimension so only
one possible direction).

The number of linearly independent characteristic directions (eigenvectors) is called the
rank of the matrix, A.  If you understand the concept of 'basis' then you know that any
m vector can be expressed in terms of the basis of eigenvectors of 'A' (that is unless A is not
of 'full rank' and has less than m linearly independent eigenvectors, in which case we decomponse
'x' into two orthogonal components, one as a linear combination of the eigenvectors of A and the other
gets mapped to 0 by A.)

Thus to each input 'x' is assigned an output 'y' which is the sum of coefficients in the eigenvector
basis representation of 'x' times corresponding eigenvalues.  This can be understood as the  
diagonalization of 'A'.  By the way, the referenced page was in error because the singular value
decomposition (I think the page actually called it the single value decomposition...free translation(s).com 
anyone) is not the same thing as the diagonalization.

There, it took a little more than a few sentences, but at least by the close of the second paragraph
one gets the basic idea.

Now, in closing, Arun, please spend some time thinking about the answer to your question before
you cut and paste it into your homework assignment.


-----Original Message-----
From: Dirk Enzmann [mailto:dirk.enzmann at uni-hamburg.de]
Sent: Sat 8/12/2006 7:01 AM
To: r-help at stat.math.ethz.ch
Cc: arun.kumar.saha at gmail.com
Subject: Re: [R] Geometrical Interpretation of Eigen value and Eigen vector
 
Arun,

have a look at:

http://149.170.199.144/multivar/eigen.htm

HTH,
Dirk

"Arun Kumar Saha" <arun.kumar.saha at gmail.com> wrote:

> It is not a R related problem rather than statistical/mathematical. However
> I am posting this query hoping that anyone can help me on this matter. My
> problem is to get the Geometrical Interpretation of Eigen value and Eigen
> vector of any square matrix. Can anyone give me a light on it?


From Ray.Brownrigg at mcs.vuw.ac.nz  Mon Aug 14 06:27:02 2006
From: Ray.Brownrigg at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 14 Aug 2006 16:27:02 +1200 (NZST)
Subject: [R] Speeding indexing and sub-sectioning of 3d array
Message-ID: <200608140427.k7E4R2DI019713@tahi.mcs.vuw.ac.nz>

> Date: Thu, 10 Aug 2006 14:34:27 -0400
> From: "Swidan, Firas" <swidanf at janelia.hhmi.org>
> 
> Hi Patrick,
> 
> Thanks for the help. The function I listed is just an example. I isolated
> and kept only the problematic part in my code for clarity sake. I ended up
> implementing the functionality in C and now it takes 22 seconds to calculate
> the objective.
> 
> Best regards,
> Firas.
> 
Interestingly, I was able to develop an algorithm in R that achieves the
same order-of-magnitude speedup as your C code, but at the expense of
greater memory requirements.  However it only works if the function you
are using is really is mean() [your code labels use Median].  It does
this by making use of cumsum() and logical indexing, working with sums
of values rather than calculationg the means and then dividing by the
numbers of values in the hypercube at the end.

If you want to try coding this algorithm in C for even greater
performance improvement (or for interest only), let me know.  I suspect
it will be difficult to code in C because of the vectorisation it takes
advantage of.

In the output below, cK3d() is your algorithm (slightly adjusted to
cover the whole matrix and to return something), and cK3dme is my
equivalent, running on a Pentium IV 3.2GHz, NetBSD system with 1GB
memory.

Regards,
Ray Brownrigg
----
> x <- rnorm(245*175*150)
> dim(x) <- c(245, 175, 150)
> unix.time(yme <- cK3dme(x, 3))
[1] 13.870  1.690 15.813  0.000  0.000
> unix.time(y <- cK3d(x, 3))
[1] 500.206   0.035 505.738   0.000   0.000
> all.equal(y, yme)
[1] TRUE
> 
----


From paul at openstreet.com  Mon Aug 14 08:28:48 2006
From: paul at openstreet.com (Paul Check)
Date: Mon, 14 Aug 2006 02:28:48 -0400
Subject: [R] GAM Package: preplot.gam taking a **long** time
Message-ID: <44E01820.80009@openstreet.com>

Hi: I have a large data set that I'm testing and I'm finding that 
preplot.gam is taking a very long amount of time to compute (like, more 
than 20 minutes). My machine is 32-bit, Debian unstable, 4GB memory, 
dual Xeon 3GHz. While the data set is very large, the gam() procedure is 
able to compute the model without any trouble, in a minute or so. Is 
there any reason why preplot.gam would be so slow? I am not using 
"newdata" in the preplot.gam function, so I am assuming that the memory 
is not a problem. Or could it be?

Is it normal for preplot.gam to take so long on large data sets? I have 
not had this experience with S-Plus, on a lower quality machine, same data.

Thanks, Paul


From ubk at kogalur-shear.com  Mon Aug 14 06:16:58 2006
From: ubk at kogalur-shear.com (K. B. Udaya)
Date: Mon, 14 Aug 2006 00:16:58 -0400
Subject: [R] [R-pkgs] Random Survival Forest 1.0.0 is now available.
Message-ID: <DOEKJCLKLCEDEDHLDCJICEFOCNAA.ubk@kogalur-shear.com>

Dear useRs,

Release 1.0.0 of the new R package 'randomSurvivalForest' is now available
on CRAN and its mirrors.  The package implements Ishwaran and Kogalur's
Random Survival Forests algorithm for right censored survival data.  The
algorithm is closely patterned after Breiman's random forests, but suitably
modified for the survival setting.  Some key features are:

o An ensemble cumulative hazard is constructed
    from binary recursive survival trees grown
    under different splitting rules.

o Mortality estimates interpretable in terms
    of total number of deaths are provided.

o An out-of-bag estimate of Harrell's concordance
    index is provided for assessing prediction.

Detailed information on usage can be found in the manual.

Thank you.

Hemant Ishwaran,  Udaya B. Kogalur

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ccleland at optonline.net  Mon Aug 14 10:53:55 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 14 Aug 2006 04:53:55 -0400
Subject: [R] Gower Similarity Coefficient
In-Reply-To: <79646f810608131635p32e0a9d1y777e0233fc3e7999@mail.gmail.com>
References: <79646f810608131635p32e0a9d1y777e0233fc3e7999@mail.gmail.com>
Message-ID: <44E03A23.7050207@optonline.net>

Timothy Rye wrote:
> I'm interested in clustering my data using the Gower Similarity Coefficient,
> and I was wondering if R is capable of using that metric
> 
> Timothy Rye

RSiteSearch("Gower") points to a number of relevant messages in the
archives.  It also reveals gdist() in the mvpart package, vegdist() in
the vegan package, and dist.binary() in the ade4 package, which may do
what you want.

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jim at bitwrit.com.au  Tue Aug 15 01:46:50 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 14 Aug 2006 19:46:50 -0400
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
Message-ID: <44E10B6A.3040505@bitwrit.com.au>

Rashmi Mathur wrote:
> Hello,
> 
> How do I split a y-axis to plot data on different scales?
> 
> Eg:
> 
> x <- 1:10
> y <- c(-0.01,0.79,0.74,0.55,-0.67,0.32,-0.47,-0.05,723,759)
> plot(x,y)
> 
> I'd like to show these data on the same plot, but the way it's written, all
> contrast in the first 8 data points is lost.  Can R split a y-axis for me?
> 
Hi Rashmi,

Although Hadley's answer is relevant (displaying vastly different ranges 
of data can be dangerous) you might find that gap.plot in the plotrix 
package will do the dirty deed.

Jim


From fsajb4 at uaf.edu  Mon Aug 14 12:05:38 2006
From: fsajb4 at uaf.edu (adlai burman)
Date: Mon, 14 Aug 2006 02:05:38 -0800
Subject: [R] posting
Message-ID: <72cad6efa785d0ae67087ed74b36f82a@uaf.edu>

fsajb4 at uaf.edu


From fsajb4 at uaf.edu  Mon Aug 14 12:25:17 2006
From: fsajb4 at uaf.edu (adlai burman)
Date: Mon, 14 Aug 2006 02:25:17 -0800
Subject: [R] naive help with setting node attributes
Message-ID: <93ac66fa00da74d334adb4de8020059d@uaf.edu>

I have been trying for a LONG time to figure out how to do what I 
imagine is a fairly simple graph rendering issue. Can anyone help me 
figure out how to take two LARGE graphs G1 and G2 (not random) and set 
their node/edge attributes separately and uniquely such that when they 
are joined, G1 and G2 will retain their unique attributes under 
rendering?

Thanks,
AJ


From csardi at rmki.kfki.hu  Mon Aug 14 12:31:34 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Mon, 14 Aug 2006 12:31:34 +0200
Subject: [R] naive help with setting node attributes
In-Reply-To: <93ac66fa00da74d334adb4de8020059d@uaf.edu>
References: <93ac66fa00da74d334adb4de8020059d@uaf.edu>
Message-ID: <20060814103134.GD9964@localdomain>

AJ,

hmmm, which package are you using? This might help answering the question,
which is quite hard to decypher in this form. At least for me.

Gabor

On Mon, Aug 14, 2006 at 02:25:17AM -0800, adlai burman wrote:
> I have been trying for a LONG time to figure out how to do what I 
> imagine is a fairly simple graph rendering issue. Can anyone help me 
> figure out how to take two LARGE graphs G1 and G2 (not random) and set 
> their node/edge attributes separately and uniquely such that when they 
> are joined, G1 and G2 will retain their unique attributes under 
> rendering?
> 
> Thanks,
> AJ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From hannesalazar at gmail.com  Mon Aug 14 13:47:25 2006
From: hannesalazar at gmail.com (yohannes alazar)
Date: Mon, 14 Aug 2006 12:47:25 +0100
Subject: [R] column to row
Message-ID: <ae94396d0608140447y42f0e19duc23b5e383523ccb0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/16abfa7c/attachment.pl 

From phhs80 at gmail.com  Mon Aug 14 13:52:59 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 14 Aug 2006 12:52:59 +0100
Subject: [R] column to row
In-Reply-To: <ae94396d0608140447y42f0e19duc23b5e383523ccb0@mail.gmail.com>
References: <ae94396d0608140447y42f0e19duc23b5e383523ccb0@mail.gmail.com>
Message-ID: <6ade6f6c0608140452j48b53d73r8da762d9ae0a9f@mail.gmail.com>

On 8/14/06, yohannes alazar <hannesalazar at gmail.com> wrote:
> I have a data in two columns and how can i convert it to one row . thank you
> in advance
>
> inpute
>
> 1 2
> 3 4
> 5 6
> 7 8
> 9 1
>
>
> out put
>
> 1 2 3 4 5 6 7 8 9 1

An example follows:

> input <- matrix(1:10,5,2)
> input
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> output <- c(input[,1],input[,2])
> output
 [1]  1  2  3  4  5  6  7  8  9 10
>

Paul


From Soren.Hojsgaard at agrsci.dk  Mon Aug 14 13:58:18 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Mon, 14 Aug 2006 13:58:18 +0200
Subject: [R] Calculating trace of products
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC047E46D9@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/5a4d12a4/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Mon Aug 14 13:57:12 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 14 Aug 2006 13:57:12 +0200
Subject: [R] column to row
References: <ae94396d0608140447y42f0e19duc23b5e383523ccb0@mail.gmail.com>
Message-ID: <00ca01c6bf98$c74ec400$0540210a@www.domain>

probably something like:

mat <- matrix(c(1:9, 1), 5, byrow = TRUE)
c(t(mat))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "yohannes alazar" <hannesalazar at gmail.com>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Monday, August 14, 2006 1:47 PM
Subject: [R] column to row


> Dear mailing list
> I have a data in two columns and how can i convert it to one row . 
> thank you
> in advance
>
> inpute
>
> 1 2
> 3 4
> 5 6
> 7 8
> 9 1
>
>
> out put
>
> 1 2 3 4 5 6 7 8 9 1
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From neuro3000 at hotmail.com  Mon Aug 14 14:10:34 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Mon, 14 Aug 2006 08:10:34 -0400
Subject: [R] column to row
In-Reply-To: <ae94396d0608140447y42f0e19duc23b5e383523ccb0@mail.gmail.com>
Message-ID: <BAY112-F7060B698350A9309724A3AF4E0@phx.gbl>

>input <- matrix(1:10,5,2)
>input
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
>as.vector(input)
[1]  1  2  3  4  5  6  7  8  9 10

Neuro


>From: "yohannes alazar" <hannesalazar at gmail.com>
>To: r-help <r-help at stat.math.ethz.ch>
>Subject: [R] column to row
>Date: Mon, 14 Aug 2006 12:47:25 +0100
>
>Dear mailing list
>I have a data in two columns and how can i convert it to one row . thank 
>you
>in advance
>
>inpute
>
>1 2
>3 4
>5 6
>7 8
>9 1
>
>
>out put
>
>1 2 3 4 5 6 7 8 9 1
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rkoenker at uiuc.edu  Mon Aug 14 14:16:57 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 14 Aug 2006 07:16:57 -0500
Subject: [R] Calculating trace of products
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC047E46D9@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E46D9@DJFPOST01.djf.agrsci.dk>
Message-ID: <67128C45-9194-4F28-B84E-93A1F2074DCD@uiuc.edu>

I would suspect that something simple like

	sum(diag(crossprod(A,B)))

would be quite competitive...

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Aug 14, 2006, at 6:58 AM, S?ren H?jsgaard wrote:

> Dear all,
> I need to calculate tr(A B), tr(A B A B) and similar quantities  
> **fast** where the matrices A, B are symmetrical. I've searched for  
> built-in functions for that purpose, but without luck. Can anyone  
> help?
> Thanks in advance
> S?ren
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Mon Aug 14 14:28:18 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 14 Aug 2006 14:28:18 +0200
Subject: [R] Calculating trace of products
References: <C83C5E3DEEE97E498B74729A33F6EAEC047E46D9@DJFPOST01.djf.agrsci.dk>
Message-ID: <010b01c6bf9d$1fa203c0$0540210a@www.domain>

check the following:

A <- matrix(rnorm(100*100), 100, 100); A <- A + t(A)
B <- matrix(rnorm(100*100), 100, 100); B <- B + t(B)

sum(diag(A %*% B))
sum(A * B)

system.time(for(i in 1:10000) out <- sum(diag(A %*% B)))
system.time(for(i in 1:10000) out <- sum(A * B))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "S?ren H?jsgaard" <Soren.Hojsgaard at agrsci.dk>
To: "R-help list" <r-help at stat.math.ethz.ch>
Sent: Monday, August 14, 2006 1:58 PM
Subject: [R] Calculating trace of products


Dear all,
I need to calculate tr(A B), tr(A B A B) and similar quantities 
**fast** where the matrices A, B are symmetrical. I've searched for 
built-in functions for that purpose, but without luck. Can anyone 
help?
Thanks in advance
S?ren

[[alternative HTML version deleted]]




--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From maechler at stat.math.ethz.ch  Mon Aug 14 14:30:39 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Aug 2006 14:30:39 +0200
Subject: [R] Gower Similarity Coefficient
In-Reply-To: <44E03A23.7050207@optonline.net>
References: <79646f810608131635p32e0a9d1y777e0233fc3e7999@mail.gmail.com>
	<44E03A23.7050207@optonline.net>
Message-ID: <17632.27887.179213.825114@stat.math.ethz.ch>

>>>>> "Chuck" == Chuck Cleland <ccleland at optonline.net>
>>>>>     on Mon, 14 Aug 2006 04:53:55 -0400 writes:

    Chuck> Timothy Rye wrote:
    >> I'm interested in clustering my data using the Gower
    >> Similarity Coefficient, and I was wondering if R is
    >> capable of using that metric
    >> 
    >> Timothy Rye

    Chuck> RSiteSearch("Gower") points to a number of relevant
    Chuck> messages in the archives.  It also reveals gdist() in
    Chuck> the mvpart package, vegdist() in the vegan package,
    Chuck> and dist.binary() in the ade4 package, which may do
    Chuck> what you want.

The daisy() function from the package "cluster"
(which is *recommended* hence part of every complete R installation)
is also based on Gower's (dis)similarity coefficient.

So you don't need to install a new package
[But I need to add the word 'Gower' to a better place on
 daisy()'s help page ...]

Martin Maechler, ETH Zurich.


From maechler at stat.math.ethz.ch  Mon Aug 14 14:50:24 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Aug 2006 14:50:24 +0200
Subject: [R] Gower Similarity Coefficient
In-Reply-To: <17632.27887.179213.825114@stat.math.ethz.ch>
References: <79646f810608131635p32e0a9d1y777e0233fc3e7999@mail.gmail.com>
	<44E03A23.7050207@optonline.net>
	<17632.27887.179213.825114@stat.math.ethz.ch>
Message-ID: <17632.29072.432189.716541@stat.math.ethz.ch>

>>>>> "Martin" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 14 Aug 2006 14:30:39 +0200 writes:

>>>>> "Chuck" == Chuck Cleland <ccleland at optonline.net>
>>>>>     on Mon, 14 Aug 2006 04:53:55 -0400 writes:

    Chuck> Timothy Rye wrote:
    >>> I'm interested in clustering my data using the Gower
    >>> Similarity Coefficient, and I was wondering if R is
    >>> capable of using that metric
    >>> 
    >>> Timothy Rye

    Chuck> RSiteSearch("Gower") points to a number of relevant
    Chuck> messages in the archives.  It also reveals gdist() in
    Chuck> the mvpart package, vegdist() in the vegan package,
    Chuck> and dist.binary() in the ade4 package, which may do
    Chuck> what you want.

    Martin> The daisy() function from the package "cluster"
    Martin> (which is *recommended* hence part of every complete
    Martin> R installation) is also based on Gower's
    Martin> (dis)similarity coefficient.

    Martin> So you don't need to install a new package [But I
    Martin> need to add the word 'Gower' to a better place on
    Martin> daisy()'s help page ...]

Hmm, actually, it's already there.

And if you use help.search() smartly 
[I use lib.loc = <R system library> in order to not search in
 the more than 1000 CRAN and bioconductor packages we have installed],

> help.search("Gower", agrep = FALSE, lib.loc = tail(.libPaths(), 1))

I get exactly the correct match

  >> Help files with alias or concept or title matching Gower using
  >> regular expression matching:

  >> daisy(cluster)          Dissimilarity Matrix Calculation

  >> Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.


Martin Maechler


From andreas.svensson at bio.ntnu.no  Mon Aug 14 15:57:01 2006
From: andreas.svensson at bio.ntnu.no (Andreas Svensson)
Date: Mon, 14 Aug 2006 15:57:01 +0200
Subject: [R] mtext uses the typographical descender to align text
Message-ID: <44E0812D.9070503@bio.ntnu.no>

<>

Hello

One sometimes (quite often really ) marvel at the choice of defaults in 
R's graphic engine.
For some obscure reason, mtext uses the typographical  descender (bottom 
of letters) to align text. That is: "gG" will end up slightly higher 
that "GG". Depending on the font, "Q" might end up higher than "O".

(for explanation of baseline & descender see: 
http://www.paratype.com/help/term/terms.asp?code=88)

Example:

y1 <- rnorm(30)
y2 <- rnorm(30)
group <- as.factor(rep(1:6, each=10))
y <- c(y1,y2)
testdata <- data.frame(group, y)
plot (y ~group, axes=F, xlab="why does R align the BOTTOM of the 
letters???" )
box()
mtext(expression(italic("Normal 1"))   ,1 ,  line=1, at=1)
mtext(expression(italic("Higher 1 "))  ,1 ,  line=1, at=2)
mtext(expression(italic("Normal 3 "))  ,1 ,  line=1, at=3)
mtext(expression(italic("Higher 2 "))  ,1 ,  line=1, at=4)
mtext(expression(italic("Normal 3 "))  ,1 ,  line=1, at=5)
mtext(expression(italic("Higher 3 "))  ,1 ,  line=1, at=6)


As the word "Higher" includes a descending letter (g), this factor name 
ends up higher.
I must say I have never encountered a software that uses the 
typographical descender instead of the baseline to align text .

Does anyone know how to make R use the baseline instead? Perhaps using  
adj or padj? Or do I have to do something silly as adding
"g", col=white
to each mtext-line to trick R into aligning the names.

Cheers
Andreas


From rduarte at ipimar.pt  Mon Aug 14 16:11:53 2006
From: rduarte at ipimar.pt (Rafael Duarte)
Date: Mon, 14 Aug 2006 15:11:53 +0100
Subject: [R] Lattice barchart with different fill pattern
Message-ID: <44E084A9.60907@ipimar.pt>

Dear list,
I am new to lattice plots.
I want to make a barchart with 10 and more levels.
I need to use a grey scale for printing purposes.

The problem is that with 10 or more levels in factors it is very 
difficult to distinguish each level in the plot and legend, since the 
greys are very similar (some levels have value of zero and don't appear).

Here is an example of my problem:

df <- data.frame("year" = rep(1996:2005,10),
"spe" = c(rep("aa",10), rep("bb",10), rep("cc",10), rep("dd",10), 
rep("ee",10), rep("ff",10), rep("gg",10), 
rep("hh",10),rep("ii",10),rep("jj",10)),
"value" = sample( c(0:10),100 , replace=TRUE)
  )

require("lattice")
barchart( value ~ factor(year), groups = spe, data=df, stack = TRUE, 
main="", xlab="",
          auto.key = list(points = FALSE, rectangles = TRUE, space = 
"right" ),
          par.settings = list(superpose.polygon = list(col = 
gray.colors(10) ), scales = list(cex=0.8, rot=c(90,0,0)))
)

I was thinking that by changing the fill pattern, the different levels 
could be better distinguished.

I have read the lattice help and searched in the mailing list archives, 
but did no find any solution for lattice barchart.

Is there a way to change the fill pattern of the bar levels or any other 
approach to help in the identification of the different levels in the 
barchart?

Many thanks.

Rafael Duarte

OS: windows XP


From andy_liaw at merck.com  Mon Aug 14 16:09:13 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Aug 2006 10:09:13 -0400
Subject: [R] lasso for variable selection
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2E52@usctmx1106.merck.com>

For "importance" it's probably best to stick with absolute values of
coefficients, instead of value of the penalty parameter for which the
coefficients changed to non-zero.

Friedman skipped a lot of details on his rule ensemble in that talk, due to
time constraint.  In his implementation he was using his own algorithm,
PathSeeker, for which paper and software are available on his web page.
PathSeeker is a different generalization of LASSO than LAR.

HTH,
Andy 

From: zubin
> 
> Attended JSM last week and Friedman mentioned the use of 
> LASSO for variable selection (he uses it for rules 
> ensembles).  I am an econometrician and not familiar with, i 
> started running the examples in 
> R this week and you get to the plots section of the LARS package.   
> Plots of beta/max(beta)  vs standardized coefficients.  How 
> does one interpret them?  u see plots of each variable 
> converging to zero at different times - its pretty cool - but 
> can i use this for variable importance?
> 
> for variable selection - i have a group of correlated 
> variables that we need to determine importance in predicting 
> change of a Y variable.
> 
> -zubin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From andy_liaw at merck.com  Mon Aug 14 16:15:04 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Aug 2006 10:15:04 -0400
Subject: [R] Auto-save possible in R?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2E59@usctmx1106.merck.com>

You could try something like:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/0334.html

replacing the time-stamp with save.image() or save().  Not sure how it works
in BATCH though...

HTH,
Andy 

From: John Morrow
> 
> Hello fellow R'ers, I have a simple calculation with a very 
> large data set being generated (34.9 million values) on a 
> somewhat unreliable XP box that will likely take ~ 74hrs.  I 
> wanted to know if there is a way to have my script 
> automatically "save.image()" throughout the calculation in 
> case of a crash.  This could be on the basis of output 
> generated or time elapsed.  I checked the archive, and only 
> got a hint of it from:
> https://stat.ethz.ch/pipermail/r-help/1997-May/001611.html
> 
>  
> 
> Any quick suggestions would be greatly appreciated,
> 
>  
> 
> John Morrow
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From andy_liaw at merck.com  Mon Aug 14 16:17:52 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Aug 2006 10:17:52 -0400
Subject: [R] rpvm/snow packages on a cluster with dual-processor machi
 nes
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2E5D@usctmx1106.merck.com>

That's what I've tried before, on three dual-Xeon boxes, so I know it worked
(as documented a that time).

Andy 

From: Paul Y. Peng
> 
> Luke Tierney just reminded me that makeCluster() can take a 
> number greater than the number of machines in a cluster. It 
> seems to be a solution to this problem. But I haven't tested it yet.
> 
> Paul.
> 
> 
> Ryan Austin wrote:
> > Hi,
> > 
> > Adding a node twice gives a duplicate node error.
> > However, adding the parameter sp=2000 to your pvm hostfile should 
> > enable dual processors.
> > 
> > Ryan
> > 
> > Liaw, Andy wrote:
> > 
> >> Caveat: I've only played with this a couple of years ago... 
> >>
> >> I believe you can just add each host _twice_ (or as many 
> times as the 
> >> number of CPUs at that host) to get both CPUs to work.
> >>
> >> Andy
> >>
> >> From: Paul Y. Peng
> >>  
> >>
> >>> Hi,
> >>>
> >>> does anybody know how to use the dual processors in the 
> machines of 
> >>> a cluster? I am using R with rpvm and snow packages. I 
> usually start 
> >>> pvm daemon and add host machines first, and then run R to 
> start my 
> >>> computing work. But I find that only one processor in 
> each machine 
> >>> is used in this way and the other one always stays idle. Is there 
> >>> any simple way to tell pvm to use the two processors at the same 
> >>> time? In other words, I would like to see two copies of R 
> running on 
> >>> each machine's two processors when using pvm. Any hints/help are 
> >>> greatly appreciated.
> >>>
> >>> Paul.
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list 
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>>    
> >>>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide 
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>  
> >>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From roger.bos at gmail.com  Mon Aug 14 16:22:28 2006
From: roger.bos at gmail.com (roger bos)
Date: Mon, 14 Aug 2006 10:22:28 -0400
Subject: [R] left-justified fixed-width format
Message-ID: <1db726800608140722w5c54737fub33094b2dc7d3f47@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/f5cc68aa/attachment.pl 

From wsimpson at utsc.utoronto.ca  Mon Aug 14 16:27:19 2006
From: wsimpson at utsc.utoronto.ca (William Simpson)
Date: Mon, 14 Aug 2006 10:27:19 -0400 (EDT)
Subject: [R] lme() F-values disagree with aov()
Message-ID: <36882.131.136.242.1.1155565639.squirrel@webmail.utsc.utoronto.ca>

I have used lme() on data from a between-within subjects experiment. The correct
ANOVA table is known because this is a textbook example (Experimental Design by
Roger Kirk Chapter 12: Split-Plot Factorial Design). The lme() F-values differ from
the known results. Please help me understand why.

d<-read.table("kirkspf2.dat",header=TRUE)
for(j in 1:4) d[,j] <- factor(d[,j])  ### Make vars into type "factor"

##lme() results
library(nlme)
fit<-lme(y~a*b*c,random=~1|s, data=d)
anova(fit)

##correct anova table
##subjects are nested within a; a between, b & c within
fit2<-aov(y ~ a*b*c + Error(s/(c*b)), data=d)
summary(fit2)

I suspect I need a different random=... statement in lme().
Thanks very much for any help
Bill

The data file is attached -- kirkspf2.dat
Here it is again:

s a c b y
1 1 1 1 3
1 1 1 2 7
1 1 2 1 4
1 1 2 2 7
2 1 1 1 6
2 1 1 2 8
2 1 2 1 5
2 1 2 2 8
3 1 1 1 3
3 1 1 2 7
3 1 2 1 4
3 1 2 2 9
4 1 1 1 3
4 1 1 2 6
4 1 2 1 3
4 1 2 2 8
5 2 1 1 1
5 2 1 2 5
5 2 2 1 2
5 2 2 2 10
6 2 1 1 2
6 2 1 2 6
6 2 2 1 3
6 2 2 2 10
7 2 1 1 2
7 2 1 2 5
7 2 2 1 4
7 2 2 2 9
8 2 1 1 2
8 2 1 2 6
8 2 2 1 3
8 2 2 2 11

From p.dalgaard at biostat.ku.dk  Mon Aug 14 16:33:15 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Aug 2006 16:33:15 +0200
Subject: [R] mtext uses the typographical descender to align text
In-Reply-To: <44E0812D.9070503@bio.ntnu.no>
References: <44E0812D.9070503@bio.ntnu.no>
Message-ID: <x264gvidzo.fsf@viggo.kubism.ku.dk>

Andreas Svensson <andreas.svensson at bio.ntnu.no> writes:

> <>
> 
> Hello
> 
> One sometimes (quite often really ) marvel at the choice of defaults in 
> R's graphic engine.
> For some obscure reason, mtext uses the typographical  descender (bottom 
> of letters) to align text. That is: "gG" will end up slightly higher 
> that "GG". Depending on the font, "Q" might end up higher than "O".
> 
> (for explanation of baseline & descender see: 
> http://www.paratype.com/help/term/terms.asp?code=88)
<snip>
> mtext(expression(italic("Normal 1"))   ,1 ,  line=1, at=1)
<snap>
> As the word "Higher" includes a descending letter (g), this factor name 
> ends up higher.
> I must say I have never encountered a software that uses the 
> typographical descender instead of the baseline to align text .
> 
> Does anyone know how to make R use the baseline instead? Perhaps using  
> adj or padj? Or do I have to do something silly as adding
> "g", col=white
> to each mtext-line to trick R into aligning the names.

Notice that it only happens when plotting math expressions. Using 

 mtext("Normal 1"   ,1 ,  line=1, at=1,font=3)
 mtext("Higher 1 "  ,1 ,  line=1, at=2, font=3)
etc.

does not exhibit the same effect. 

I suppose the idea is that math expressions can grow arbitrarily tall
or deep, and that the design decision (wise or not) has been to align
on the bounding box of the whole enchilada. For a workaround, if you
do need to do it with expressions, notice the phantom() construction.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Mon Aug 14 16:34:57 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 14 Aug 2006 09:34:57 -0500
Subject: [R] Lattice barchart with different fill pattern
In-Reply-To: <44E084A9.60907@ipimar.pt>
References: <44E084A9.60907@ipimar.pt>
Message-ID: <eb555e660608140734i6f46f4bdwa97efcc6e57b8e8b@mail.gmail.com>

On 8/14/06, Rafael Duarte <rduarte at ipimar.pt> wrote:
> Dear list,
> I am new to lattice plots.
> I want to make a barchart with 10 and more levels.
> I need to use a grey scale for printing purposes.
>
> The problem is that with 10 or more levels in factors it is very
> difficult to distinguish each level in the plot and legend, since the
> greys are very similar (some levels have value of zero and don't appear).
>
> Here is an example of my problem:
>
> df <- data.frame("year" = rep(1996:2005,10),
> "spe" = c(rep("aa",10), rep("bb",10), rep("cc",10), rep("dd",10),
> rep("ee",10), rep("ff",10), rep("gg",10),
> rep("hh",10),rep("ii",10),rep("jj",10)),
> "value" = sample( c(0:10),100 , replace=TRUE)
>  )

Nothing to do with your problem, but 'rep' has an argument 'each'
which would shorten the call above.

> require("lattice")
> barchart( value ~ factor(year), groups = spe, data=df, stack = TRUE,
> main="", xlab="",
>          auto.key = list(points = FALSE, rectangles = TRUE, space =
> "right" ),
>          par.settings = list(superpose.polygon = list(col =
> gray.colors(10) ), scales = list(cex=0.8, rot=c(90,0,0)))
> )
>
> I was thinking that by changing the fill pattern, the different levels
> could be better distinguished.
>
> I have read the lattice help and searched in the mailing list archives,
> but did no find any solution for lattice barchart.
>
> Is there a way to change the fill pattern of the bar levels or any other
> approach to help in the identification of the different levels in the
> barchart?

Unfortunately, no. lattice depends on the grid package for rendering,
and last time I checked, grid didn't have support for fill patterns.

Deepayan


From ripley at stats.ox.ac.uk  Mon Aug 14 16:41:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Aug 2006 15:41:09 +0100 (BST)
Subject: [R] left-justified fixed-width format
In-Reply-To: <1db726800608140722w5c54737fub33094b2dc7d3f47@mail.gmail.com>
References: <1db726800608140722w5c54737fub33094b2dc7d3f47@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608141538370.7428@gannet.stats.ox.ac.uk>

On Mon, 14 Aug 2006, roger bos wrote:

> I need to save data in fixed-width format without headers and reading the
> help archive leads me to believe that sprintf is pretty much the only way to
> do this.  My question is, is there anyway to change the output so the text
> in each column is left justified instead of right justified?  My code sample
> is below where comb is the data frame.  TIA, Roger

format can do this, easily.  And for sprintf, use the '-' flag documented 
on the help page(!):

> sprintf("%22s", "foo")
[1] "                   foo"
> sprintf("%-22s", "foo")
[1] "foo                   "

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From i.m.s.white at ed.ac.uk  Mon Aug 14 15:38:47 2006
From: i.m.s.white at ed.ac.uk (i.m.s.white)
Date: Mon, 14 Aug 2006 14:38:47 +0100
Subject: [R] lme() F-values disagree with aov()
In-Reply-To: <36882.131.136.242.1.1155565639.squirrel@webmail.utsc.utoronto.ca>
References: <36882.131.136.242.1.1155565639.squirrel@webmail.utsc.utoronto.ca>
Message-ID: <20060814133847.GB579@trotter.cap.ed.ac.uk>

Your lme statement is OK. To get the usual split-plot anova, your aov
statement should be

fit2 <- aov(y ~ a*b*c + Error(s), data = d)

This gives the same F-values as lme.


On Mon, Aug 14, 2006 at 10:27:19AM -0400, William Simpson wrote:
> I have used lme() on data from a between-within subjects experiment. The correct
> ANOVA table is known because this is a textbook example (Experimental Design by
> Roger Kirk Chapter 12: Split-Plot Factorial Design). The lme() F-values differ from
> the known results. Please help me understand why.
> 
> d<-read.table("kirkspf2.dat",header=TRUE)
> for(j in 1:4) d[,j] <- factor(d[,j])  ### Make vars into type "factor"
> 
> ##lme() results
> library(nlme)
> fit<-lme(y~a*b*c,random=~1|s, data=d)
> anova(fit)
> 
> ##correct anova table
> ##subjects are nested within a; a between, b & c within
> fit2<-aov(y ~ a*b*c + Error(s/(c*b)), data=d)
> summary(fit2)
> 
> I suspect I need a different random=... statement in lme().
> Thanks very much for any help
> Bill
> 
> The data file is attached -- kirkspf2.dat
> Here it is again:
> 
> s a c b y
> 1 1 1 1 3
> 1 1 1 2 7
> 1 1 2 1 4
> 1 1 2 2 7
> 2 1 1 1 6
> 2 1 1 2 8
> 2 1 2 1 5
> 2 1 2 2 8
> 3 1 1 1 3
> 3 1 1 2 7
> 3 1 2 1 4
> 3 1 2 2 9
> 4 1 1 1 3
> 4 1 1 2 6
> 4 1 2 1 3
> 4 1 2 2 8
> 5 2 1 1 1
> 5 2 1 2 5
> 5 2 2 1 2
> 5 2 2 2 10
> 6 2 1 1 2
> 6 2 1 2 6
> 6 2 2 1 3
> 6 2 2 2 10
> 7 2 1 1 2
> 7 2 1 2 5
> 7 2 2 1 4
> 7 2 2 2 9
> 8 2 1 1 2
> 8 2 1 2 6
> 8 2 2 1 3
> 8 2 2 2 11

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
************************************************
*    I.White                                   *
*    University of Edinburgh                   *
*    Ashworth Laboratories, West Mains Road    *
*    Edinburgh EH9 3JT                         *
*    Fax: 0131 650 6564   Tel: 0131 650 5490   *
*    E-mail: i.m.s.white at ed.ac.uk              *


From wsimpson at utsc.utoronto.ca  Mon Aug 14 16:56:16 2006
From: wsimpson at utsc.utoronto.ca (William Simpson)
Date: Mon, 14 Aug 2006 10:56:16 -0400 (EDT)
Subject: [R] lme() F-values disagree with aov()
In-Reply-To: <20060814133847.GB579@trotter.cap.ed.ac.uk>
References: <36882.131.136.242.1.1155565639.squirrel@webmail.utsc.utoronto.ca>
	<20060814133847.GB579@trotter.cap.ed.ac.uk>
Message-ID: <34331.131.136.242.1.1155567376.squirrel@webmail.utsc.utoronto.ca>

> Your lme statement is OK. To get the usual split-plot anova, your aov
> statement should be
>
> fit2 <- aov(y ~ a*b*c + Error(s), data = d)

No, this gives wrong F-values. By "wrong" I mean it does not agree with the
published table.

Table 12.10-2, page 559:
                     Number of obs =      32     R-squared     =  0.9920
                     Root MSE      = .559017     Adj R-squared =  0.9589

            Source |  Partial SS    df       MS           F     Prob > F
        -----------+----------------------------------------------------
             Model |     233.625    25       9.345      29.90     0.0002
                   |
                 a |       3.125     1       3.125       2.00     0.2070
               s|a |       9.375     6      1.5625
        -----------+----------------------------------------------------
                 b |      162.00     1      162.00     199.38     0.0000
               a*b |       6.125     1       6.125       7.54     0.0335
             b*s|a |       4.875     6       .8125
        -----------+----------------------------------------------------
                 c |       24.50     1       24.50      61.89     0.0002
               a*c |      10.125     1      10.125      25.58     0.0023
             c*s|a |       2.375     6  .395833333
        -----------+----------------------------------------------------
               b*c |        8.00     1        8.00      25.60     0.0023
             a*b*c |       3.125     1       3.125      10.00     0.0195
                   |
          Residual |       1.875     6       .3125
        -----------+----------------------------------------------------
             Total |      235.50    31  7.59677419

Bill


From jrkrideau at yahoo.ca  Mon Aug 14 17:08:49 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 14 Aug 2006 11:08:49 -0400 (EDT)
Subject: [R] How to order or sort a data.frame
In-Reply-To: <20060813170328.u1yxodsmee5cgks8@webmail3.kuleuven.be>
Message-ID: <20060814150849.84325.qmail@web32813.mail.mud.yahoo.com>


--- Dimitrios Rizopoulos
<Dimitris.Rizopoulos at med.kuleuven.be> wrote:

> try the following:
> 
> mdf <- data.frame(us.state, count, year, month)
> mdf[order(mdf$year, mdf$month), ]
> 


Thansk to Dimitris and Dieter. This has helped since
seems to have shown me a way around the problem. It
just means that I have to sort the data earlier.  

What my example did not show clearly is that when I
subset the data the variable 'month' which is numeric
in the original dataframe becomes a factor.  I was
wondering if there was a way to sort the factor so
that I would get a numeric sort.  So far I have not
been able to see how to coerce the factor "month" into
a numeric when using 'order"

Thanks for helpl.

Hi Hadley, 
I have not had time to check out the reshape but
thanks.




> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      
>
http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> Quoting John Kane <jrkrideau at yahoo.ca>:
> 
> > I have a dataframe where I would like to order
> first
> > by  variable, year, and then within that variable
> by
> > month.
> >
> > So far the only way that I have seen to do this is
> to
> > order by year and then subset year and sort by
> month
> > and then do an rbind to get things back together.
> >
> > Is this the right approach?
> >
> > Example:
> >
> > us.state <-rep("California", 23)
> >
> > count <-
> c(774,283,774,283,508,283,774,283,602,283,
> >
> > 774,508,0,602,330,283,283,283,602,301,126, NA,301)
> >
> > year <- c(2002,  2003, 2001, 2002, 2001, 2002,
> 2001,
> > 2002, 2002, 2003,
> >           2002, 2002, 2001,  2002, 2001, 2002,
> 2001,
> > 2002, 2001, 2002,
> >           2001, 2001, 2002)
> >
> > month <- c( 1, 1, 10, 10, 11, 11, 12, 12,
> >
> >             2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9,
> 9,
> > 9)
> >
> >
> >
> >
> > df <- data.frame (cbind(us.state,count, year,
> month))
> >
> > # ordering as a factor works here
> >
> > df1 <- df[order(df$year),]
> >
> > df1
> >
> >
> >
> > df2 <- subset(df1, year==2001)
> >
> >
> >
> > # ordering as a factor works but not a good
> > appearance.
> >
> >
> > df3 <- df2[order(as.numeric(df2$month)),]
> >
> > df3
> >
> >
> >
> > This works but  "month" is ordered as  a factor
> and I
> > would prefer to coerce it into a numeric for
> > presentation purposes but
> >  df3 <- df2[order(as.numeric(df2$month)),] does
> not
> > seem to work,  nor has a couple of other things
> I've
> > tried.
> >
> > Any suggestions gratefully received.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> >
> 
> 
> 
> Disclaimer:
> http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
>


From m-krutky at northwestern.edu  Mon Aug 14 17:19:14 2006
From: m-krutky at northwestern.edu (Matthew A. Krutky)
Date: Mon, 14 Aug 2006 10:19:14 -0500
Subject: [R]  help with glmmPQL
Message-ID: <20060814151918.B2F1DA1@lulu.it.northwestern.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/fc9e7d17/attachment.pl 

From demirtas at uic.edu  Mon Aug 14 17:30:02 2006
From: demirtas at uic.edu (HAKAN DEMIRTAS)
Date: Mon, 14 Aug 2006 10:30:02 -0500
Subject: [R] solving non-linear system of equations
References: <mailman.9.1155463202.18864.r-help@stat.math.ethz.ch>
Message-ID: <001201c6bfb6$9266edd0$9d3ff880@demirtasxp157>

Didn't get any useful response to the following question. Trying again.
--------------------------------------------------------------------------------
I can't seem to get computationally stable estimates for the following
system:

Y=a+bX+cX^2+dX^3, where X~N(0,1). (Y is expressed as a linear combination
of the first three powers of a standard normal variable.) Assuming that
E(Y)=0 and Var(Y)=1, one can obtain the following equations after tedious
algebraic calculations:

1) b^2+6bd+2c^2+15d^2=1
2) 2c(b^2+24bd+105d^2+2)=E(Y^3)
3) 24[bd+c^2(1+b^2+28bd)+d^2(12+48bd+141c^2+225d^2)]=E(Y^4)-3

Obviously, a=-c. Suppose that distributional form of Y is given so we know
E(Y^3) and E(Y^4). In other words, we have access to the third and fourth
raw moments. How do we solve for these four coefficients? I reduced the
number of unknowns/equations to two, and subsequently used a grid
approach. It works well when I am close to the center of the support, but
fails at the tails. Any ideas?

Hakan Demirtas


From p.dalgaard at biostat.ku.dk  Mon Aug 14 17:40:14 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Aug 2006 17:40:14 +0200
Subject: [R] lme() F-values disagree with aov()
In-Reply-To: <34331.131.136.242.1.1155567376.squirrel@webmail.utsc.utoronto.ca>
References: <36882.131.136.242.1.1155565639.squirrel@webmail.utsc.utoronto.ca>
	<20060814133847.GB579@trotter.cap.ed.ac.uk>
	<34331.131.136.242.1.1155567376.squirrel@webmail.utsc.utoronto.ca>
Message-ID: <x2wt9bgwbl.fsf@viggo.kubism.ku.dk>

"William Simpson" <wsimpson at utsc.utoronto.ca> writes:

> > Your lme statement is OK. To get the usual split-plot anova, your aov
> > statement should be
> >
> > fit2 <- aov(y ~ a*b*c + Error(s), data = d)
> 
> No, this gives wrong F-values. By "wrong" I mean it does not agree with the
> published table.

Well, it's the model that is equivalent to your lme() model....

Thing is that you want to add random effects of s:b and s:c, which are
crossed factors, so somewhat tricky to code with lme() (this sort of
thing is easier in lmer() from the lme4 packages). 

The generic way to handle this in lme() is via something like

           random=list(s=pdBlocked(list(
                     pdIdent(~1),
                     pdIdent(~b-1),
                     pdIdent(~c-1))))

You probably won't get the degrees of freedom right, though. 

 
> Table 12.10-2, page 559:
>                      Number of obs =      32     R-squared     =  0.9920
>                      Root MSE      = .559017     Adj R-squared =  0.9589
> 
>             Source |  Partial SS    df       MS           F     Prob > F
>         -----------+----------------------------------------------------
>              Model |     233.625    25       9.345      29.90     0.0002
>                    |
>                  a |       3.125     1       3.125       2.00     0.2070
>                s|a |       9.375     6      1.5625
>         -----------+----------------------------------------------------
>                  b |      162.00     1      162.00     199.38     0.0000
>                a*b |       6.125     1       6.125       7.54     0.0335
>              b*s|a |       4.875     6       .8125
>         -----------+----------------------------------------------------
>                  c |       24.50     1       24.50      61.89     0.0002
>                a*c |      10.125     1      10.125      25.58     0.0023
>              c*s|a |       2.375     6  .395833333
>         -----------+----------------------------------------------------
>                b*c |        8.00     1        8.00      25.60     0.0023
>              a*b*c |       3.125     1       3.125      10.00     0.0195
>                    |
>           Residual |       1.875     6       .3125
>         -----------+----------------------------------------------------
>              Total |      235.50    31  7.59677419
> 
> Bill
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From andreas.svensson at bio.ntnu.no  Mon Aug 14 17:48:55 2006
From: andreas.svensson at bio.ntnu.no (Andreas Svensson)
Date: Mon, 14 Aug 2006 17:48:55 +0200
Subject: [R] mtext uses the typographical descender to align text
In-Reply-To: <x264gvidzo.fsf@viggo.kubism.ku.dk>
References: <44E0812D.9070503@bio.ntnu.no> <x264gvidzo.fsf@viggo.kubism.ku.dk>
Message-ID: <44E09B67.7040707@bio.ntnu.no>

Hi

The reason I use expression is to get italics. Perhaps this is possible 
some other way?

/A

Peter Dalgaard wrote:

>Andreas Svensson <andreas.svensson at bio.ntnu.no> writes:
>
>  
>
>><>
>>
>>Hello
>>
>>One sometimes (quite often really ) marvel at the choice of defaults in 
>>R's graphic engine.
>>For some obscure reason, mtext uses the typographical  descender (bottom 
>>of letters) to align text. That is: "gG" will end up slightly higher 
>>that "GG". Depending on the font, "Q" might end up higher than "O".
>>
>>(for explanation of baseline & descender see: 
>>http://www.paratype.com/help/term/terms.asp?code=88)
>>    
>>
><snip>
>  
>
>>mtext(expression(italic("Normal 1"))   ,1 ,  line=1, at=1)
>>    
>>
><snap>
>  
>
>>As the word "Higher" includes a descending letter (g), this factor name 
>>ends up higher.
>>I must say I have never encountered a software that uses the 
>>typographical descender instead of the baseline to align text .
>>
>>Does anyone know how to make R use the baseline instead? Perhaps using  
>>adj or padj? Or do I have to do something silly as adding
>>"g", col=white
>>to each mtext-line to trick R into aligning the names.
>>    
>>
>
>Notice that it only happens when plotting math expressions. Using 
>
> mtext("Normal 1"   ,1 ,  line=1, at=1,font=3)
> mtext("Higher 1 "  ,1 ,  line=1, at=2, font=3)
>etc.
>
>does not exhibit the same effect. 
>
>I suppose the idea is that math expressions can grow arbitrarily tall
>or deep, and that the design decision (wise or not) has been to align
>on the bounding box of the whole enchilada. For a workaround, if you
>do need to do it with expressions, notice the phantom() construction.
>
>  
>


From wsimpson at utsc.utoronto.ca  Mon Aug 14 17:48:46 2006
From: wsimpson at utsc.utoronto.ca (William Simpson)
Date: Mon, 14 Aug 2006 11:48:46 -0400 (EDT)
Subject: [R] lme() F-values disagree with aov()
In-Reply-To: <x2wt9bgwbl.fsf@viggo.kubism.ku.dk>
References: <36882.131.136.242.1.1155565639.squirrel@webmail.utsc.utoronto.ca>
	<20060814133847.GB579@trotter.cap.ed.ac.uk>
	<34331.131.136.242.1.1155567376.squirrel@webmail.utsc.utoronto.ca>
	<x2wt9bgwbl.fsf@viggo.kubism.ku.dk>
Message-ID: <32229.131.136.242.1.1155570526.squirrel@webmail.utsc.utoronto.ca>

Thanks very much Peter!
>> > Your lme statement is OK. To get the usual split-plot anova, your aov
>> > statement should be
>> >
>> > fit2 <- aov(y ~ a*b*c + Error(s), data = d)
>>
>> No, this gives wrong F-values. By "wrong" I mean it does not agree with the
>> published table.
>
> Well, it's the model that is equivalent to your lme() model....
Yes, I thought my lme() model was wrong but couldn't figure out how to do it properly.

> Thing is that you want to add random effects of s:b and s:c, which are
> crossed factors, so somewhat tricky to code with lme() (this sort of
> thing is easier in lmer() from the lme4 packages).
I am happy to do it that way if you show me how...

> The generic way to handle this in lme() is via something like
>
>            random=list(s=pdBlocked(list(
>                      pdIdent(~1),
>                      pdIdent(~b-1),
>                      pdIdent(~c-1))))
>
I see. Thanks very much Peter!!

Bill


From bibiko at eva.mpg.de  Mon Aug 14 17:56:30 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Mon, 14 Aug 2006 17:56:30 +0200
Subject: [R] Question on .Options$max.print
Message-ID: <101FDEA7-0FE2-4AB2-8666-D315AF4E48EC@eva.mpg.de>

Hi,


I have a tiny question concerning .Options$max.print


I have to set up this value to a greater value than 10000 because I  
want to concatenate my output of a function to one single string (for  
connivence).

I did this via .Options$max.print <- 64000 or options(max.print=64000)

Then I call out <- paste(out, blabla) several times, but nchar(out)  
is never larger than 10000.

I read in the help about '.Options' that this is not yet used in base  
R. Could this be my problem?


Thanks for any hint

Hans

BTW I believe, there is a typo within the help page about '.Options'

...
The ?factory-fresh? default settings of some of these options are
...
max.print     1000
...


This should be
...
max.print     10000
...


From ligges at statistik.uni-dortmund.de  Mon Aug 14 17:59:53 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Aug 2006 17:59:53 +0200
Subject: [R] mtext uses the typographical descender to align text
In-Reply-To: <44E09B67.7040707@bio.ntnu.no>
References: <44E0812D.9070503@bio.ntnu.no> <x264gvidzo.fsf@viggo.kubism.ku.dk>
	<44E09B67.7040707@bio.ntnu.no>
Message-ID: <44E09DF9.2010101@statistik.uni-dortmund.de>



Andreas Svensson wrote:
> Hi
> 
> The reason I use expression is to get italics. Perhaps this is possible 
> some other way?


1. Yes, see ?mtext:

plot(1:10)
mtext("Hello World", 3, font=3)


2. expression() has to use bounding boxes for several reasons, in 
particular fractions spring to mind at once. Solutions as Peter suggested:
   plot(1:10)
   mtext(expression("Hello World" * phantom("g")), 3)

Uwe Ligges



> /A
> 
> Peter Dalgaard wrote:
> 
>> Andreas Svensson <andreas.svensson at bio.ntnu.no> writes:
>>
>>  
>>
>>> <>
>>>
>>> Hello
>>>
>>> One sometimes (quite often really ) marvel at the choice of defaults in 
>>> R's graphic engine.
>>> For some obscure reason, mtext uses the typographical  descender (bottom 
>>> of letters) to align text. That is: "gG" will end up slightly higher 
>>> that "GG". Depending on the font, "Q" might end up higher than "O".
>>>
>>> (for explanation of baseline & descender see: 
>>> http://www.paratype.com/help/term/terms.asp?code=88)
>>>    
>>>
>> <snip>
>>  
>>
>>> mtext(expression(italic("Normal 1"))   ,1 ,  line=1, at=1)
>>>    
>>>
>> <snap>
>>  
>>
>>> As the word "Higher" includes a descending letter (g), this factor name 
>>> ends up higher.
>>> I must say I have never encountered a software that uses the 
>>> typographical descender instead of the baseline to align text .
>>>
>>> Does anyone know how to make R use the baseline instead? Perhaps using  
>>> adj or padj? Or do I have to do something silly as adding
>>> "g", col=white
>>> to each mtext-line to trick R into aligning the names.
>>>    
>>>
>> Notice that it only happens when plotting math expressions. Using 
>>
>> mtext("Normal 1"   ,1 ,  line=1, at=1,font=3)
>> mtext("Higher 1 "  ,1 ,  line=1, at=2, font=3)
>> etc.
>>
>> does not exhibit the same effect. 
>>
>> I suppose the idea is that math expressions can grow arbitrarily tall
>> or deep, and that the design decision (wise or not) has been to align
>> on the bounding box of the whole enchilada. For a workaround, if you
>> do need to do it with expressions, notice the phantom() construction.
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From spencer.graves at pdf.com  Mon Aug 14 18:14:42 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Aug 2006 09:14:42 -0700
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <44E10B6A.3040505@bitwrit.com.au>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au>
Message-ID: <44E0A172.2020907@pdf.com>

	  The pro's and con's of using "scale breaks" were discussed by 
Cleveland (1985) The Elements of Graphing Data (Wadsworth, pp. 85-91, 
149).  I don't know what Cleveland said about this is the second edition 
of this book, but I believe there are times when scale breaks are 
appropriate, but the display should make this nonstandard transition 
very clear;  otherwise, it can be seriously misleading.

	  Hope this helps.
	  Spencer Graves

Jim Lemon wrote:
> Rashmi Mathur wrote:
>> Hello,
>>
>> How do I split a y-axis to plot data on different scales?
>>
>> Eg:
>>
>> x <- 1:10
>> y <- c(-0.01,0.79,0.74,0.55,-0.67,0.32,-0.47,-0.05,723,759)
>> plot(x,y)
>>
>> I'd like to show these data on the same plot, but the way it's written, all
>> contrast in the first 8 data points is lost.  Can R split a y-axis for me?
>>
> Hi Rashmi,
> 
> Although Hadley's answer is relevant (displaying vastly different ranges 
> of data can be dangerous) you might find that gap.plot in the plotrix 
> package will do the dirty deed.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Mon Aug 14 18:44:51 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Aug 2006 18:44:51 +0200
Subject: [R] Question on .Options$max.print
In-Reply-To: <101FDEA7-0FE2-4AB2-8666-D315AF4E48EC@eva.mpg.de>
References: <101FDEA7-0FE2-4AB2-8666-D315AF4E48EC@eva.mpg.de>
Message-ID: <17632.43139.631817.387814@stat.math.ethz.ch>

>>>>> "HansJB" == Hans-Joerg Bibiko <bibiko at eva.mpg.de>
>>>>>     on Mon, 14 Aug 2006 17:56:30 +0200 writes:

    HansJB> Hi, I have a tiny question concerning
    HansJB> .Options$max.print


    HansJB> I have to set up this value to a greater value than
    HansJB> 10000 because I want to concatenate my output of a
    HansJB> function to one single string (for connivence).

    HansJB> I did this via .Options$max.print <- 64000 or
    HansJB> options(max.print=64000)

    HansJB> Then I call out <- paste(out, blabla) several times,
    HansJB> but nchar(out) is never larger than 10000.

    HansJB> I read in the help about '.Options' that this is not
    HansJB> yet used in base R. Could this be my problem?

well, it at least makes clear that your assumption that 
this option would influence your paste() must be wrong.

I don't think there's any option influencing paste()
and I hope there won't every be any.
Options typically should only influence ``output formatting''
but not the result of a ``computational'' (i.e. non-printing/plotting)
function.

    HansJB> Thanks for any hint

The posting guide -- and every footer of all R-help posting asks
for a small reproducible example of R code.
So please do provide one
[and, BTW, keep this thread on R-help; do not reply privately..]

    HansJB> Hans

    HansJB> BTW I believe, there is a typo within the help page
    HansJB> about '.Options'

    HansJB> ...  The ?factory-fresh? default settings of some
    HansJB> of these options are ...  max.print 1000 ...


    HansJB> This should be ...  max.print 10000 ...

Yes, thank you.

Martin Maechler, ETH Zurich


From asaguiar at spsconsultoria.com  Mon Aug 14 18:53:27 2006
From: asaguiar at spsconsultoria.com (Alexandre Aguiar)
Date: Mon, 14 Aug 2006 13:53:27 -0300
Subject: [R] Attempt to access unmapped memory
Message-ID: <200608141353.28993.asaguiar@spsconsultoria.com>

Hi,

I am usiing R 2.3.1 under Linux kernel 2.6.11 with libreadline/libhistory 5.1. 
Bothe R and readline were compiled without quircks with gcc 3.3.3 and g77.

Every time I try to edit command line by using del key the following error 
happens:

------------8><--------------
> ipacks <- instaleld.packages()
 *** caught segfault ***
address (nil), cause 'memory not mapped'

Possible actions:
1: abort (with core dump)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
Selection: Segmentation fault
------------8><--------------

As I have readline 4.3 installed too, I adjusted the symlinks to point to 5.1. 
No problem with bash. This setup works fine in another machine with same OS 
and versions.

Any clues?

Thanks in advance.


-- 


Alexandre Aguiar
Independent consultant for medical research
SPS Consultoria
Voice: +55-11-9320-2046
Fax: +55-11-5549-8760


From info at aghmed.fsnet.co.uk  Mon Aug 14 18:56:58 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 14 Aug 2006 17:56:58 +0100
Subject: [R] How to reply to a thread if receiving R-help mails in
 digest form
In-Reply-To: <44DF276F.3070903@uni-hamburg.de>
References: <44DF276F.3070903@uni-hamburg.de>
Message-ID: <7.0.0.16.0.20060814175320.019f32f0@aghmed.fsnet.co.uk>

At 14:21 13/08/2006, Dirk Enzmann wrote:
>I receive R-help messages in digest form that makes it difficult to 
>answer a post. I noticed that my answer is not added to the thread 
>(instead, a new thread is started) although I use the same subject 
>line (starting with "Re: ") as the original post. Is there a 
>solution (I prefer the digest to separate messages for several 
>reasons and don't want to change my email reader)?

Dirk, I asked some while ago for people to tell me how they read 
digests in their news reader so I could put together a list of 
helpful hints. Unfortunately I got nowhere. In the email client I use 
(Eudora) there is an option to receive digests as attachments (as Ted 
Harding has also outlined in another email. It is rather hidden so I 
suspect if you can do it in Thunderbird it will be similarly hidden. 
Try looking for such an option and let me know if you find it.


>The way I answer post up to now is:
>1) I press the reply button of my email program (Mozilla / 
>Thunderbird, Windows)
>2) I delete all contents of the digest except for the post 
>(including name and mail address of the posting person) I want to 
>answer so that the original question will be included (cited) in my answer.
>3) I add the email address to the individual sender to "cc:" to the 
>automatically generated address of the R-help list.
>4) I replace the automatically generated subject line (for example 
>"Re: R-help Digest, Vol 42, Issue 13" by "Re: " followed by a copy 
>of the original subject line of the post.
>5) I write my answer and send the mail to the mailing list.
>
>It's not that this is tedious - the problem is that the thread is 
>broken. Is there a better way even if I want to keep receiving 
>messages in digest form? The posting guide is silent about this.
>
>Dirk
>
>*************************************************
>Dr. Dirk Enzmann
>Institute of Criminal Sciences
>Dept. of Criminology
>Edmund-Siemers-Allee 1
>D-20146 Hamburg
>Germany
>
>phone: +49-(0)40-42838.7498 (office)
>        +49-(0)40-42838.4591 (Billon)
>fax:   +49-(0)40-42838.2344
>email: dirk.enzmann at uni-hamburg.de
>www: 
>http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html
>
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From med at aghmed.fsnet.co.uk  Mon Aug 14 19:00:12 2006
From: med at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 14 Aug 2006 18:00:12 +0100
Subject: [R] Vector Join
In-Reply-To: <20060813122334.54842.qmail@web51112.mail.yahoo.com>
References: <20060813122334.54842.qmail@web51112.mail.yahoo.com>
Message-ID: <7.0.0.16.0.20060814175822.0196ed70@aghmed.fsnet.co.uk>

At 13:23 13/08/2006, Michael Zatorsky wrote:
>Hi,
>
>I'm working on producing a simple cumulative frequency
>distribution.
>
>Thanks to the help of the good people on this list I
>now have four vectors that I'd like to join/relate
>into a table. e.g.
>
>
>v1 <- myHistogram$breaks             # classes
>v2 <- myHistogram$counts             # freqs
>v3 <- cumsum(v2)                     # cumulative freq
>v4 <- ((v3 / length(myData)) * 100)  # cumulative %

data.frame(v1 = myHistogram$breaks, v2 = myHistogram$counts, and so on ...)



>What is the recommend approach to turning these into a
>single table with four columns?  ie effectively doing
>a relational join on row id?
>
>The goal is to ultimately have the data with one row
>per class in a format I can write out to a text file
>as:
>
>   v1    v2    v3    v4
>   v1    v2    v3    v4
>   etc...
>
>Any advice will be appreciated.
>
>Regards
>Michael.
>
>
>
>
>
>____________________________________________________
>
>Coming soon: Celebrity Survivor - 11 celebrities, 25 days, unlimited drama

Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html


From spencer.graves at pdf.com  Mon Aug 14 19:01:24 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Aug 2006 10:01:24 -0700
Subject: [R] solving non-linear system of equations
In-Reply-To: <001201c6bfb6$9266edd0$9d3ff880@demirtasxp157>
References: <mailman.9.1155463202.18864.r-help@stat.math.ethz.ch>
	<001201c6bfb6$9266edd0$9d3ff880@demirtasxp157>
Message-ID: <44E0AC64.5030002@pdf.com>

	  Are you saying you did not receive my reply stamped "8/10/2006 2:12 
AM" in my Sent folder, or that my reply was not useful?  In case the 
former is correct, my comments were as follows:

"Have you tried writing a function to compute SS = sum of squares 
deviations between the the left and right hand sides of your three 
equations, then using 'optim'?  See also Venables and Ripley (2002) 
Modern Applied Statistics with S, 4th ed. (Springer).

p.s.  I don't see how it's obvious that 'a=-c'."

	  If you think these comments are not useful, I would appreciate the 
courtesy of a reply.  I have solved many superficially similar problems 
in this way, and I would like to know why this would not work for your 
case.

       hope this helps.
       Spencer Graves	

HAKAN DEMIRTAS wrote:
> Didn't get any useful response to the following question. Trying again.
> --------------------------------------------------------------------------------
> I can't seem to get computationally stable estimates for the following
> system:
> 
> Y=a+bX+cX^2+dX^3, where X~N(0,1). (Y is expressed as a linear combination
> of the first three powers of a standard normal variable.) Assuming that
> E(Y)=0 and Var(Y)=1, one can obtain the following equations after tedious
> algebraic calculations:
> 
> 1) b^2+6bd+2c^2+15d^2=1
> 2) 2c(b^2+24bd+105d^2+2)=E(Y^3)
> 3) 24[bd+c^2(1+b^2+28bd)+d^2(12+48bd+141c^2+225d^2)]=E(Y^4)-3
> 
> Obviously, a=-c. Suppose that distributional form of Y is given so we know
> E(Y^3) and E(Y^4). In other words, we have access to the third and fourth
> raw moments. How do we solve for these four coefficients? I reduced the
> number of unknowns/equations to two, and subsequently used a grid
> approach. It works well when I am close to the center of the support, but
> fails at the tails. Any ideas?
> 
> Hakan Demirtas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From matteucci at stat.unibo.it  Mon Aug 14 19:07:11 2006
From: matteucci at stat.unibo.it (Mariagiulia Matteucci)
Date: Mon, 14 Aug 2006 19:07:11 +0200 (CEST)
Subject: [R] missing data treatment in MCMC pack
Message-ID: <2098519.1155575231711.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Hello, 
does anyone know something about missing data in the MCMC pack? 
thank you,
Mariagiulia



Mariagiulia Matteucci
Dipartimento di Scienze Statistiche ?Paolo Fortunati?
Universit? di Bologna
Via Belle Arti, 41 
40126 Bologna (ITALY)
e-mail: matteucci at stat.unibo.it
TEL: +39 051 264182
FAX: +39 051 232153


From chrysopa at gmail.com  Mon Aug 14 19:12:02 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 14 Aug 2006 14:12:02 -0300
Subject: [R] Making R script to run in a console
Message-ID: <200608141412.02147.chrysopa@gmail.com>

Hi,

is possible to make a R script to run under a console without open the R 
environment?

Something like this example.R

#!/usr/bin/R

function(name="Put here your name") {
print(name)
}

In a console I make
./example.R name="Ronaldo Reis J?nior"
then program print my name.

It is possible?

Thanks
Ronaldo
-- 
A jury consists of twelve persons chosen to decide who has the better lawyer.
		-- Robert Frost
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From p.dalgaard at biostat.ku.dk  Mon Aug 14 20:12:00 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Aug 2006 20:12:00 +0200
Subject: [R] solving non-linear system of equations
In-Reply-To: <44E0AC64.5030002@pdf.com>
References: <mailman.9.1155463202.18864.r-help@stat.math.ethz.ch>
	<001201c6bfb6$9266edd0$9d3ff880@demirtasxp157>
	<44E0AC64.5030002@pdf.com>
Message-ID: <x2ejvjchlb.fsf@turmalin.kubism.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> p.s.  I don't see how it's obvious that 'a=-c'."

> > Y=a+bX+cX^2+dX^3, where X~N(0,1). (Y is expressed as a linear combination
> > of the first three powers of a standard normal variable.) Assuming that
> > E(Y)=0 and Var(Y)=1, one can obtain the following equations after tedious
> > algebraic calculations:

Take means on both sides: EY = a+0+c+0 = 0


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Dimitris.Rizopoulos at med.kuleuven.be  Mon Aug 14 20:36:43 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Mon, 14 Aug 2006 20:36:43 +0200
Subject: [R] How to order or sort a data.frame
In-Reply-To: <20060814150849.84325.qmail@web32813.mail.mud.yahoo.com>
References: <20060814150849.84325.qmail@web32813.mail.mud.yahoo.com>
Message-ID: <20060814203643.6tfbwh643n4s84k0@webmail4.kuleuven.be>

Quoting John Kane <jrkrideau at yahoo.ca>:

>
> --- Dimitrios Rizopoulos
> <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
>
>> try the following:
>>
>> mdf <- data.frame(us.state, count, year, month)
>> mdf[order(mdf$year, mdf$month), ]
>>
>
>
> Thansk to Dimitris and Dieter. This has helped since
> seems to have shown me a way around the problem. It
> just means that I have to sort the data earlier.
>
> What my example did not show clearly is that when I
> subset the data the variable 'month' which is numeric
> in the original dataframe becomes a factor.  I was
> wondering if there was a way to sort the factor so
> that I would get a numeric sort.  So far I have not
> been able to see how to coerce the factor "month" into
> a numeric when using 'order"

for converting factors to numeric check R FAQ 7.10.

Best,
Dimitris



>
> Thanks for helpl.
>
> Hi Hadley,
> I have not had time to check out the reshape but
> thanks.
>
>
>
>
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/(0)16/336899
>> Fax: +32/(0)16/337015
>> Web: http://med.kuleuven.be/biostat/
>>
>>
> http://www.student.kuleuven.be/~m0390867/dimitris.htm
>>
>>
>> Quoting John Kane <jrkrideau at yahoo.ca>:
>>
>> > I have a dataframe where I would like to order
>> first
>> > by  variable, year, and then within that variable
>> by
>> > month.
>> >
>> > So far the only way that I have seen to do this is
>> to
>> > order by year and then subset year and sort by
>> month
>> > and then do an rbind to get things back together.
>> >
>> > Is this the right approach?
>> >
>> > Example:
>> >
>> > us.state <-rep("California", 23)
>> >
>> > count <-
>> c(774,283,774,283,508,283,774,283,602,283,
>> >
>> > 774,508,0,602,330,283,283,283,602,301,126, NA,301)
>> >
>> > year <- c(2002,  2003, 2001, 2002, 2001, 2002,
>> 2001,
>> > 2002, 2002, 2003,
>> >           2002, 2002, 2001,  2002, 2001, 2002,
>> 2001,
>> > 2002, 2001, 2002,
>> >           2001, 2001, 2002)
>> >
>> > month <- c( 1, 1, 10, 10, 11, 11, 12, 12,
>> >
>> >             2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9,
>> 9,
>> > 9)
>> >
>> >
>> >
>> >
>> > df <- data.frame (cbind(us.state,count, year,
>> month))
>> >
>> > # ordering as a factor works here
>> >
>> > df1 <- df[order(df$year),]
>> >
>> > df1
>> >
>> >
>> >
>> > df2 <- subset(df1, year==2001)
>> >
>> >
>> >
>> > # ordering as a factor works but not a good
>> > appearance.
>> >
>> >
>> > df3 <- df2[order(as.numeric(df2$month)),]
>> >
>> > df3
>> >
>> >
>> >
>> > This works but  "month" is ordered as  a factor
>> and I
>> > would prefer to coerce it into a numeric for
>> > presentation purposes but
>> >  df3 <- df2[order(as.numeric(df2$month)),] does
>> not
>> > seem to work,  nor has a couple of other things
>> I've
>> > tried.
>> >
>> > Any suggestions gratefully received.
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained,
>> reproducible code.
>> >
>> >
>>
>>
>>
>> Disclaimer:
>> http://www.kuleuven.be/cwis/email_disclaimer.htm
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From tom.boonen.maiden at gmail.com  Mon Aug 14 21:11:29 2006
From: tom.boonen.maiden at gmail.com (Tom Boonen)
Date: Mon, 14 Aug 2006 15:11:29 -0400
Subject: [R] ARMA(1,1) for panel data
Message-ID: <cc088e260608141211g49e647f8hde7bfd1660575b10@mail.gmail.com>

Dear List,

I am new to TS-Modeling in R. I would like to fit an ARMA(1,1) model
for a balanced panel, running Y on a full set of unit and year dummies
using an arma(1,1) for the disturbance:

y_it=unit.dummies+yeardummies+e_it

where: e_it=d*e_it-1+u_it+q*u_it-1

How can I fit this model in R? arma() does not seem to take covariates
(or I don't understand how to specify the function so that it would).
Thank you very much.

Best, Tom


From ideguise at interchange.ubc.ca  Mon Aug 14 21:18:27 2006
From: ideguise at interchange.ubc.ca (Isabelle Deguise)
Date: Mon, 14 Aug 2006 12:18:27 -0700
Subject: [R] CircStats help
Message-ID: <383A6064-21AA-42EF-8EEE-B6222A548F35@interchange.ubc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/8126a25d/attachment.pl 

From bolker at ufl.edu  Mon Aug 14 21:36:44 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 14 Aug 2006 19:36:44 +0000 (UTC)
Subject: [R] CircStats help
References: <383A6064-21AA-42EF-8EEE-B6222A548F35@interchange.ubc.ca>
Message-ID: <loom.20060814T213228-250@post.gmane.org>

Isabelle Deguise <ideguise <at> interchange.ubc.ca> writes:

> 
> Hello,
> 
> 	
>  > x <- c(5.009684,37.814266, 295.722970, 0.000000,  
> 326.463366,242.678840)
>  > radx <- rad(x)
>  > circ.mean(radx)
> [1] -0.4283351
>  > deg(-0.4283351)
> [1] -24.54179
> 
> I would just like this number to be converted to the appropriate  
> angle between 0-360 degrees. Can someone help me?
> 

  Just add 360 to the result ... when in doubt, you can also look at what
the function is actually doing by typing the name of the
function by itself, e.g.

> circ.mean

In this case it shows that the function is just adding
up the sines and cosines of the individual angles and
then taking the arc-tangent.  With this size problem
you can work through the details and understand how
it all works ...

  Ben Bolker


From ripley at stats.ox.ac.uk  Mon Aug 14 21:40:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Aug 2006 20:40:22 +0100 (BST)
Subject: [R] Attempt to access unmapped memory
In-Reply-To: <200608141353.28993.asaguiar@spsconsultoria.com>
References: <200608141353.28993.asaguiar@spsconsultoria.com>
Message-ID: <Pine.LNX.4.64.0608142038520.10511@gannet.stats.ox.ac.uk>

On Mon, 14 Aug 2006, Alexandre Aguiar wrote:

> Hi,
> 
> I am usiing R 2.3.1 under Linux kernel 2.6.11 with 
> libreadline/libhistory 5.1.

That is known to be buggy.  Either patch it or downdate to 5.0.

> Bothe R and readline were compiled without quircks with gcc 3.3.3 and g77.
> 
> Every time I try to edit command line by using del key the following error 
> happens:
> 
> ------------8><--------------
> > ipacks <- instaleld.packages()
>  *** caught segfault ***
> address (nil), cause 'memory not mapped'
> 
> Possible actions:
> 1: abort (with core dump)
> 2: normal R exit
> 3: exit R without saving workspace
> 4: exit R saving workspace
> Selection: Segmentation fault
> ------------8><--------------
> 
> As I have readline 4.3 installed too, I adjusted the symlinks to point to 5.1. 
> No problem with bash. This setup works fine in another machine with same OS 
> and versions.
> 
> Any clues?
> 
> Thanks in advance.
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Peter.Eiger at gmx.net  Mon Aug 14 22:00:08 2006
From: Peter.Eiger at gmx.net (Peter Eiger)
Date: Mon, 14 Aug 2006 22:00:08 +0200
Subject: [R] Fast way to load multiple files
Message-ID: <20060814200008.170570@gmx.net>

Hi,

Instead of having to program a loop to load several workspaces in a directory, it would be nice to store the filenames in a list "filelist" and then to apply "load" to this list
"lapply( filelist, load)"
Unfortunately, although it seems that R is loading the files, the contained objects are not available in the workspace afterwards.
Any hints what I'm doing wrong or how to circumvent the problem?
Peter
--


From jrkrideau at yahoo.ca  Mon Aug 14 22:10:30 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 14 Aug 2006 16:10:30 -0400 (EDT)
Subject: [R] How to order or sort a data.frame
In-Reply-To: <20060814203643.6tfbwh643n4s84k0@webmail4.kuleuven.be>
Message-ID: <20060814201030.71871.qmail@web32806.mail.mud.yahoo.com>


--- Dimitrios Rizopoulos
<Dimitris.Rizopoulos at med.kuleuven.be> wrote:

> Quoting John Kane <jrkrideau at yahoo.ca>:
> 
> >
> > --- Dimitrios Rizopoulos
> > <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
> >
> >> try the following:
> >>
> >> mdf <- data.frame(us.state, count, year, month)
> >> mdf[order(mdf$year, mdf$month), ]
> >>
> >
> >
> > Thansk to Dimitris and Dieter. This has helped
> since
> > seems to have shown me a way around the problem.
> It
> > just means that I have to sort the data earlier.
> >
> > What my example did not show clearly is that when
> I
> > subset the data the variable 'month' which is
> numeric
> > in the original dataframe becomes a factor.  I was
> > wondering if there was a way to sort the factor so
> > that I would get a numeric sort.  So far I have
> not
> > been able to see how to coerce the factor "month"
> into
> > a numeric when using 'order"
> 
> for converting factors to numeric check R FAQ 7.10.
> 
> Best,
> Dimitris

Argh!  Of course. I should never trust my memory!

Thanks.
 

> 
> 
> >
> > Thanks for helpl.
> >
> > Hi Hadley,
> > I have not had time to check out the reshape but
> > thanks.
> >
> >
> >
> >
> >> Ph.D. Student
> >> Biostatistical Centre
> >> School of Public Health
> >> Catholic University of Leuven
> >>
> >> Address: Kapucijnenvoer 35, Leuven, Belgium
> >> Tel: +32/(0)16/336899
> >> Fax: +32/(0)16/337015
> >> Web: http://med.kuleuven.be/biostat/
> >>
> >>
> >
>
http://www.student.kuleuven.be/~m0390867/dimitris.htm
> >>
> >>
> >> Quoting John Kane <jrkrideau at yahoo.ca>:
> >>
> >> > I have a dataframe where I would like to order
> >> first
> >> > by  variable, year, and then within that
> variable
> >> by
> >> > month.
> >> >
> >> > So far the only way that I have seen to do this
> is
> >> to
> >> > order by year and then subset year and sort by
> >> month
> >> > and then do an rbind to get things back
> together.
> >> >
> >> > Is this the right approach?
> >> >
> >> > Example:
> >> >
> >> > us.state <-rep("California", 23)
> >> >
> >> > count <-
> >> c(774,283,774,283,508,283,774,283,602,283,
> >> >
> >> > 774,508,0,602,330,283,283,283,602,301,126,
> NA,301)
> >> >
> >> > year <- c(2002,  2003, 2001, 2002, 2001, 2002,
> >> 2001,
> >> > 2002, 2002, 2003,
> >> >           2002, 2002, 2001,  2002, 2001, 2002,
> >> 2001,
> >> > 2002, 2001, 2002,
> >> >           2001, 2001, 2002)
> >> >
> >> > month <- c( 1, 1, 10, 10, 11, 11, 12, 12,
> >> >
> >> >             2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8,
> 9,
> >> 9,
> >> > 9)
> >> >
> >> >
> >> >
> >> >
> >> > df <- data.frame (cbind(us.state,count, year,
> >> month))
> >> >
> >> > # ordering as a factor works here
> >> >
> >> > df1 <- df[order(df$year),]
> >> >
> >> > df1
> >> >
> >> >
> >> >
> >> > df2 <- subset(df1, year==2001)
> >> >
> >> >
> >> >
> >> > # ordering as a factor works but not a good
> >> > appearance.
> >> >
> >> >
> >> > df3 <- df2[order(as.numeric(df2$month)),]
> >> >
> >> > df3
> >> >
> >> >
> >> >
> >> > This works but  "month" is ordered as  a factor
> >> and I
> >> > would prefer to coerce it into a numeric for
> >> > presentation purposes but
> >> >  df3 <- df2[order(as.numeric(df2$month)),] does
> >> not
> >> > seem to work,  nor has a couple of other things
> >> I've
> >> > tried.
> >> >
> >> > Any suggestions gratefully received.
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained,
> >> reproducible code.
> >> >
> >> >
> >>
> >>
> >>
> >> Disclaimer:
> >> http://www.kuleuven.be/cwis/email_disclaimer.htm
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> >
> 
> 
> 
> Disclaimer:
> http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
>


From gunter.berton at gene.com  Mon Aug 14 22:26:46 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 14 Aug 2006 13:26:46 -0700
Subject: [R] Fast way to load multiple files
In-Reply-To: <20060814200008.170570@gmx.net>
Message-ID: <004501c6bfdf$f6d08230$711f210a@gne.windows.gene.com>

A reproducible example here would help (please see posting guide). A guess:
is your filelist a list of (quoted) character strings? Correct pathnames to
the files with correct separators for your OS?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Eiger
> Sent: Monday, August 14, 2006 1:00 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Fast way to load multiple files
> 
> Hi,
> 
> Instead of having to program a loop to load several 
> workspaces in a directory, it would be nice to store the 
> filenames in a list "filelist" and then to apply "load" to this list
> "lapply( filelist, load)"
> Unfortunately, although it seems that R is loading the files, 
> the contained objects are not available in the workspace afterwards.
> Any hints what I'm doing wrong or how to circumvent the problem?
> Peter
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mschwartz at mn.rr.com  Mon Aug 14 22:58:20 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 14 Aug 2006 15:58:20 -0500
Subject: [R] Making R script to run in a console
In-Reply-To: <200608141412.02147.chrysopa@gmail.com>
References: <200608141412.02147.chrysopa@gmail.com>
Message-ID: <1155589100.4093.62.camel@localhost.localdomain>

On Mon, 2006-08-14 at 14:12 -0300, Ronaldo Reis-Jr. wrote:
> Hi,
> 
> is possible to make a R script to run under a console without open the R 
> environment?
> 
> Something like this example.R
> 
> #!/usr/bin/R
> 
> function(name="Put here your name") {
> print(name)
> }
> 
> In a console I make
> ./example.R name="Ronaldo Reis J?nior"
> then program print my name.
> 
> It is possible?
> 
> Thanks
> Ronaldo

Ronaldo,

You might want to review these web pages:

http://wiki.r-project.org/rwiki/doku.php?id=developers:rinterp

http://kavaro.fi/mediawiki/index.php/Using_R_from_the_shell


HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Mon Aug 14 23:06:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Aug 2006 22:06:47 +0100 (BST)
Subject: [R] Fast way to load multiple files
In-Reply-To: <004501c6bfdf$f6d08230$711f210a@gne.windows.gene.com>
References: <004501c6bfdf$f6d08230$711f210a@gne.windows.gene.com>
Message-ID: <Pine.LNX.4.64.0608142204200.11281@gannet.stats.ox.ac.uk>

On Mon, 14 Aug 2006, Berton Gunter wrote:

> A reproducible example here would help (please see posting guide). A guess:
> is your filelist a list of (quoted) character strings? Correct pathnames to
> the files with correct separators for your OS?

I think the issue is (from the help page)

Usage:

     load(file, envir = parent.frame())
                        ^^^^^^^^^^^^^^
Arguments:

    file: a (readable binary) connection or a character string giving
          the name of the file to load.

   envir: the environment where the data should be loaded.

and so they were not loaded into .GlobalEnv. Try

	lapply(filelist, load, envir=.GlobalEnv)

which works for me.

> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Eiger
> > Sent: Monday, August 14, 2006 1:00 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Fast way to load multiple files
> > 
> > Hi,
> > 
> > Instead of having to program a loop to load several 
> > workspaces in a directory, it would be nice to store the 
> > filenames in a list "filelist" and then to apply "load" to this list
> > "lapply( filelist, load)"
> > Unfortunately, although it seems that R is loading the files, 
> > the contained objects are not available in the workspace afterwards.
> > Any hints what I'm doing wrong or how to circumvent the problem?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From francoisromain at free.fr  Mon Aug 14 23:12:31 2006
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 14 Aug 2006 23:12:31 +0200
Subject: [R] Making R script to run in a console
In-Reply-To: <200608141412.02147.chrysopa@gmail.com>
References: <200608141412.02147.chrysopa@gmail.com>
Message-ID: <44E0E73F.5010309@free.fr>

Le 14.08.2006 19:12, Ronaldo Reis-Jr. a ?crit :
> Hi,
>
> is possible to make a R script to run under a console without open the R 
> environment?
>
> Something like this example.R
>
> #!/usr/bin/R
>
> function(name="Put here your name") {
> print(name)
> }
>
> In a console I make
> ./example.R name="Ronaldo Reis J?nior"
> then program print my name.
>
> It is possible?
>
> Thanks
> Ronaldo
>   
Hi,

take a look at that wiki page : 
http://wiki.r-project.org/rwiki/doku.php?id=developers:rinterp

Cheers,

Romain

-- 
+-------------------------------------------------------------------+
|                         Romain FRANCOIS                           |
| R Graph Gallery : http://addictedtor.free.fr/graphiques           |
`-------------------------------------------------------------------+


From tura at centroin.com.br  Mon Aug 14 23:25:29 2006
From: tura at centroin.com.br (Bernardo Rangel tura)
Date: Mon, 14 Aug 2006 18:25:29 -0300
Subject: [R] Call for Beta Testers: R+ (read R plus) for Solaris and
 Linux:
In-Reply-To: <20060808082111.61cf36fc0b73de5f0c7f07b3effe7558.54baf86559
	.wbe@email.secureserver.net>
References: <20060808082111.61cf36fc0b73de5f0c7f07b3effe7558.54baf86559.wbe@email.secureserver.net>
Message-ID: <7.0.0.16.2.20060814182132.042c9870@centroin.com.br>

At 12:21 PM 8/8/2006, jen at xlsolutions-corp.com wrote:

>We look forward to hearing your comments and inputs on R+ ... please
>feel free to suggest a final name for our commercially supported R.

I don?t understanding this mail.

Is possible exist a commercial version of R?

If R source is licensed for GPL as free software 
other people can make a commercial version?


[]s
Tura


From asaguiar at spsconsultoria.com  Tue Aug 15 00:14:52 2006
From: asaguiar at spsconsultoria.com (Alexandre Aguiar)
Date: Mon, 14 Aug 2006 19:14:52 -0300
Subject: [R] Making R script to run in a console
In-Reply-To: <1155589100.4093.62.camel@localhost.localdomain>
References: <200608141412.02147.chrysopa@gmail.com>
	<1155589100.4093.62.camel@localhost.localdomain>
Message-ID: <200608141914.54383.asaguiar@spsconsultoria.com>

Em Seg 14 Ago 2006 17:58, Marc Schwartz (via MN) escreveu:
> http://kavaro.fi/mediawiki/index.php/Using_R_from_the_shell

I made a small change to the wrapper example by implementing dynamic 
allocation of memory and sizing the command line buffer to 32768 that is the 
real maximum size (at least under bash/sh :-).

#include <stdio.h> /* file i/o functions */
#include <string.h> /* string functions */
#include <stdlib.h> /* malloc and exit codes */

/* 
 * R_HOME has to be defined. Usually it is /usr/lib/R
 * todo: copy all command line params to R
*/
int main(int argc, char **argv)
{
     /* should be enough for all */
    char *cmd, l;
    FILE *out;

    /* without this "if" a segmentation fault will happen 
    if no parameters are passed */
    if(argc == 1)
    return(EXIT_FAILURE);

    /* dynamically allocates memory for command line buffer */
    cmd=(char *)malloc(32768);
    /* assemble command line */
    strcpy(cmd,"/usr/lib/R/bin/exec/R --vanilla --slave < ");
    strcat(cmd,argv[1]);
	         
    /* do the real job */
    out=popen(cmd,"r");
    l = fgetc(out);
    printf("%c",l);
    while(l != EOF) 
    {
	l = fgetc(out);
	if(l !=EOF)
	printf("%c",l);
    }
    /* cleanup: close files and free memory */
    pclose(out);
    free((void *)cmd);
    return(EXIT_SUCCESS);
}


-- 


Alexandre Aguiar
Independent consultant for medical research
SPS Consultoria
Voice: +55-11-9320-2046
Fax: +55-11-5549-8760


From efg at stowers-institute.org  Tue Aug 15 00:44:26 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Mon, 14 Aug 2006 17:44:26 -0500
Subject: [R] Help with workaround for: Function '`[`' is not in the
	derivatives table
Message-ID: <ebqucb$hfh$1@sea.gmane.org>

# This works fine:
> a <- 1

> b <- 2

> c <- 3



> E <- expression(a * exp(b*X) + c)



> X <- c(0.5, 1.0, 2.0)



> eval(E)

[1]  5.718282 10.389056 57.598150



> D(E, "b")

a * (exp(b * X) * X)

> eval(D(E, "b"))

[1]   1.359141   7.389056 109.196300



# But if (a,b,c) are replaced with (A[1], A[2], A[3]), how can I get a 
derivative using "D"?



> A <- c(1, 2, 3)

> E <- expression(A[1] * exp(A[2]*X) + A[3])

> X <- c(0.5, 1.0, 2.0)

> eval(E)

[1]  5.718282 10.389056 57.598150



# Why doesn't this work?  Any workarounds?

> D(E, "A[2]")

Error in D(E, "A[2]") : Function '`[`' is not in the derivatives table



If I want to have a long vector of coefficients, A, (perhaps dozens) how can 
I use "D" to compute partial derivatives?



Thanks for any help with this.



efg



Earl F. Glynn

Scientific Programmer

Stowers Institute for Medical Research


From a.menicacci at fr.fournierpharma.com  Tue Aug 15 01:00:29 2006
From: a.menicacci at fr.fournierpharma.com (a.menicacci at fr.fournierpharma.com)
Date: Tue, 15 Aug 2006 01:00:29 +0200
Subject: [R] Alexandre MENICACCI/Daix/RED/GroupeFournier est absent(e).
Message-ID: <OF822B36B9.AEA87B93-ONC12571CA.007E6358-C12571CA.007E6358@fr.fournierpharma.com>





Je serai absent(e) du  14/08/2006 au 21/08/2006.

Je r?pondrai ? votre message d?s mon retour.


From andy_liaw at merck.com  Tue Aug 15 01:09:48 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Aug 2006 19:09:48 -0400
Subject: [R] Call for Beta Testers: R+ (read R plus) for Solaris and L
 inux:
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E1F@usctmx1106.merck.com>

Note that it's "commercially supported", not just "commercial".  Also,
"commercial" does not mean "closed source".  Open source (and even GPL) and
"commercial" need not be mutually exclusive.  Remember that the "free" in
"free software" is as in "freedom" rather than as in beer (as far as GPL is
concern, at least).
 
Having commercial support for R may not be a bad thing:  It may well help
adaptation in the corporate world.  (Linux probably would not have made
inroads into large corps if not for companies providing commercial support
for it.)  It would be great if those who profited from R can contribute back
to the project in some way.
 
Just my $0.02...
 
Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of Bernardo Rangel tura
Sent: Mon 8/14/2006 5:25 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Call for Beta Testers: R+ (read R plus) for Solaris and
Linux: [Broadcast]



At 12:21 PM 8/8/2006, jen at xlsolutions-corp.com wrote: 

>We look forward to hearing your comments and inputs on R+ ... please 
>feel free to suggest a final name for our commercially supported R. 

I don?t understanding this mail. 

Is possible exist a commercial version of R? 

If R source is licensed for GPL as free software 
other people can make a commercial version? 


[]s 
Tura 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>  
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Tue Aug 15 01:10:45 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 14 Aug 2006 16:10:45 -0700
Subject: [R] Help with workaround for: Function '`[`' is not in
	thederivatives table
In-Reply-To: <ebqucb$hfh$1@sea.gmane.org>
Message-ID: <005a01c6bff6$df96cd60$711f210a@gne.windows.gene.com>

I think this is the sort of problem which is most elegantly handled by
computing on the language. Here is an INelegant solution: 

>  A <- c(1, 2, 3)

> for(i in 1:3)assign(paste('A',i,sep=''),A[i])

>  E <- expression(A1 * exp(A2*X) + A3) ## could also use substitute() here,
I think
## instead of explicitly assigning the coefficients

> X <- c(0.5, 1.0, 2.0)

>  eval(E)
[1]  5.718282 10.389056 57.598150

>  D(E, "A2")
A1 * (exp(A2 * X) * X)


Bert Gunter
Genentech
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Earl F. Glynn
> Sent: Monday, August 14, 2006 3:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help with workaround for: Function '`[`' is not 
> in thederivatives table
> 
> # This works fine:
> > a <- 1
> 
> > b <- 2
> 
> > c <- 3
> 
> 
> 
> > E <- expression(a * exp(b*X) + c)
> 
> 
> 
> > X <- c(0.5, 1.0, 2.0)
> 
> 
> 
> > eval(E)
> 
> [1]  5.718282 10.389056 57.598150
> 
> 
> 
> > D(E, "b")
> 
> a * (exp(b * X) * X)
> 
> > eval(D(E, "b"))
> 
> [1]   1.359141   7.389056 109.196300
> 
> 
> 
> # But if (a,b,c) are replaced with (A[1], A[2], A[3]), how 
> can I get a 
> derivative using "D"?
> 
> 
> 
> > A <- c(1, 2, 3)
> 
> > E <- expression(A[1] * exp(A[2]*X) + A[3])
> 
> > X <- c(0.5, 1.0, 2.0)
> 
> > eval(E)
> 
> [1]  5.718282 10.389056 57.598150
> 
> 
> 
> # Why doesn't this work?  Any workarounds?
> 
> > D(E, "A[2]")
> 
> Error in D(E, "A[2]") : Function '`[`' is not in the derivatives table
> 
> 
> 
> If I want to have a long vector of coefficients, A, (perhaps 
> dozens) how can 
> I use "D" to compute partial derivatives?
> 
> 
> 
> Thanks for any help with this.
> 
> 
> 
> efg
> 
> 
> 
> Earl F. Glynn
> 
> Scientific Programmer
> 
> Stowers Institute for Medical Research
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From liuwensui at gmail.com  Tue Aug 15 04:43:52 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 14 Aug 2006 22:43:52 -0400
Subject: [R] merge 2 data frame based on more than 2 variables
Message-ID: <1115a2b00608141943l10b9b9e8sc36479a1ecb7cc0d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/9bacaf30/attachment.pl 

From blomsp at ozemail.com.au  Tue Aug 15 05:05:55 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Tue, 15 Aug 2006 13:05:55 +1000
Subject: [R] merge 2 data frame based on more than 2 variables
In-Reply-To: <1115a2b00608141943l10b9b9e8sc36479a1ecb7cc0d@mail.gmail.com>
References: <1115a2b00608141943l10b9b9e8sc36479a1ecb7cc0d@mail.gmail.com>
Message-ID: <44E13A13.4060709@ozemail.com.au>

Wensui Liu wrote:
> Dear Lister,
>
> I understand merge() can be used to join 2 data frames based on 1 variable.
> But how about merge based on more than 2 variables?
>
> Thank you so much!
>
>   
Just specify the 2 (or more) variable names in a column vector for "by")

merge(dat1, dat2, by= c("VarA", "VarB"))

assuming both data frames have columns VarA and VarB.

-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From liuwensui at gmail.com  Tue Aug 15 05:18:59 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 14 Aug 2006 23:18:59 -0400
Subject: [R] merge 2 data frame based on more than 2 variables
In-Reply-To: <44E13A13.4060709@ozemail.com.au>
References: <1115a2b00608141943l10b9b9e8sc36479a1ecb7cc0d@mail.gmail.com>
	<44E13A13.4060709@ozemail.com.au>
Message-ID: <1115a2b00608142018g43f94477ye765509f50743b05@mail.gmail.com>

what if the names are different in 2 data frames?


On 8/14/06, Simon Blomberg <blomsp at ozemail.com.au> wrote:
> Wensui Liu wrote:
> > Dear Lister,
> >
> > I understand merge() can be used to join 2 data frames based on 1 variable.
> > But how about merge based on more than 2 variables?
> >
> > Thank you so much!
> >
> >
> Just specify the 2 (or more) variable names in a column vector for "by")
>
> merge(dat1, dat2, by= c("VarA", "VarB"))
>
> assuming both data frames have columns VarA and VarB.
>
> --
> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
> Centre for Resource and Environmental Studies
> The Australian National University
> Canberra ACT 0200
> Australia
> T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
> F: +61 2 6125 0757
> CRICOS Provider # 00120C
>
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From muster at gmail.com  Tue Aug 15 05:36:25 2006
From: muster at gmail.com (T Mu)
Date: Mon, 14 Aug 2006 23:36:25 -0400
Subject: [R] help: cannot allocate vector of length 828310236
Message-ID: <b68812e70608142036i7532c7em2a6ca07624162272@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060814/09506d3b/attachment.pl 

From blomsp at ozemail.com.au  Tue Aug 15 05:54:57 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Tue, 15 Aug 2006 13:54:57 +1000
Subject: [R] merge 2 data frame based on more than 2 variables
In-Reply-To: <1115a2b00608142018g43f94477ye765509f50743b05@mail.gmail.com>
References: <1115a2b00608141943l10b9b9e8sc36479a1ecb7cc0d@mail.gmail.com>	<44E13A13.4060709@ozemail.com.au>
	<1115a2b00608142018g43f94477ye765509f50743b05@mail.gmail.com>
Message-ID: <44E14591.3020802@ozemail.com.au>

Then instead of by, use by.x and by.y to specifiy the variable names 
separately for both data frames. See ?merge, especially the examples.

Wensui Liu wrote:
> what if the names are different in 2 data frames?
>
>
> On 8/14/06, Simon Blomberg <blomsp at ozemail.com.au> wrote:
>   
>> Wensui Liu wrote:
>>     
>>> Dear Lister,
>>>
>>> I understand merge() can be used to join 2 data frames based on 1 variable.
>>> But how about merge based on more than 2 variables?
>>>
>>> Thank you so much!
>>>
>>>
>>>       
>> Just specify the 2 (or more) variable names in a column vector for "by")
>>
>> merge(dat1, dat2, by= c("VarA", "VarB"))
>>
>> assuming both data frames have columns VarA and VarB.
>>
>> --
>> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
>> Centre for Resource and Environmental Studies
>> The Australian National University
>> Canberra ACT 0200
>> Australia
>> T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
>> F: +61 2 6125 0757
>> CRICOS Provider # 00120C
>>
>>
>>     
>
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From ripley at stats.ox.ac.uk  Tue Aug 15 08:27:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Aug 2006 07:27:36 +0100 (BST)
Subject: [R] help: cannot allocate vector of length 828310236
In-Reply-To: <b68812e70608142036i7532c7em2a6ca07624162272@mail.gmail.com>
References: <b68812e70608142036i7532c7em2a6ca07624162272@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608150714280.17114@gannet.stats.ox.ac.uk>

Does it make any statistical sense to do polr or probit regression (not 
the same thing) on `really huge data'?  There are few regression-like 
problems in which model inadequacy does not swamp estimation uncertainty 
for as few as a 1000 cases.

If you want to do that sort of thing, by all means use SAS to do it.
But if you are not prepared to spend a few $$ on adequate RAM, don't 
expect free technical consultancy, especially not from those whose work 
you are using and not crediting.

- The uncredited author of polr().


On Mon, 14 Aug 2006, T Mu wrote:

> Hi all,
> 
> I was trying a probit regression using polr() and got this message,

polr is a strange choice of tool for 'probit regression' as the term is 
usually used.  It does 'ordered probit regression'.

> Error in model.matrix.default(Terms, m, contrasts) :
>         cannot allocate vector of length 828310236
> 
> The data is about 20M (a few days ago I asked a question about large file,
> thank you for responses, then I use MS Access to select those columns I
> would use).
> 
> R is 2.3.1, Windows XP, 512M Ram.
> 
> I am going to read some help on memory use in R, but hope anybody can give
> me some quick hints.

Quick hint: read and follow the posting guide BEFORE posting.

> Is it because iphysical memory runs out, or some other things could be wrong
> with data or polr()?
> Does R use virtual memory? If so, what options can I set?
> If not, can R deal with really huge data (except adding RAM according to
> data size)? If this is the case, it is too bad that I have to tell my boss
> to go back to SAS. Now it is not a speed issue yet.
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Bill.Venables at csiro.au  Tue Aug 15 08:57:40 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 15 Aug 2006 16:57:40 +1000
Subject: [R] Help with workaround for: Function '`[`' is not in
	thederivatives table
Message-ID: <B998A44C8986644EA8029CFE6396A924840070@exqld2-bne.qld.csiro.au>

Earl F. Glynn asks: 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Earl F. Glynn
> Sent: Tuesday, 15 August 2006 8:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help with workaround for: Function '`[`' is not in
thederivatives table
> 
> # This works fine:
> > a <- 1
> > b <- 2
> > c <- 3
> > E <- expression(a * exp(b*X) + c)
> > X <- c(0.5, 1.0, 2.0)
> > eval(E)
> [1]  5.718282 10.389056 57.598150
> > D(E, "b")
> a * (exp(b * X) * X)
> > eval(D(E, "b"))
> [1]   1.359141   7.389056 109.196300
> 
> # But if (a,b,c) are replaced with (A[1], A[2], A[3]), how can I get a

> derivative using "D"?

It's well to note what "D" can differentiate with respect to.  The
second argument is, to quote the help page, a "character string giving
the name of a variable..."  'A[1]' is a character string, but it is
not the name of a variable.  When parsed it becomes a call to the
function, literally, `[`.

> > A <- c(1, 2, 3)
> > E <- expression(A[1] * exp(A[2]*X) + A[3])
> > X <- c(0.5, 1.0, 2.0)
> > eval(E)
> [1]  5.718282 10.389056 57.598150

No problem, because the evaluator does know how to evaluate `[`, along
with everything else in this expr.

> 
> 
> 
> # Why doesn't this work?  Any workarounds?
> > D(E, "A[2]")
> Error in D(E, "A[2]") : Function '`[`' is not in the derivatives table

It fails because 'A[2]' is not a (character string giving the) name of
a variable.  I think the error message could be a bit more
informative, but ... all you need to do is read he help page, really.

>  If I want to have a long vector of coefficients, A, (perhaps
> dozens) how can I use "D" to compute partial derivatives?

Essentially you need to turn calls to '[' into names of variables, do
the derivative business and then turn them back again.  This is not
easy to do in complete generality, but if you are only talking about
singly subscripted arrays, you can get somewhere.  Here is an outline
of what I mean.

> Ident <- "([A-z][A-z0-9_.]*)"
> Subsc <- "([A-z][A-z0-9_.]*|[0-9]+)"
> patn <- paste(Ident, "\\[", Subsc, "\\]", sep = "")
> repl <- "\\1__\\2"
> E <- expression(A[1] * exp(A[2]*X) + A[3])
> Es <- deparse(E[[1]])
> Es
[1] "A[1] * exp(A[2] * X) + A[3]"
> Ess <- gsub(patn, repl, Es)
> Ess
[1] "A__1 * exp(A__2 * X) + A__3"
> Ex <- parse(text = Ess)[[1]]
> Ex
A__1 * exp(A__2 * X) + A__3

OK, the calls to `[` have been replaced by variables with two
underscores in the middle.  We hope this works - there is a strong
assumption here on just how complicated your indices are, for example.
We are assuming they are either identifiers (not using two successive
underscores in their names) or numbers.  If they are not, you need to
get even craftier.


> Ex1 <- D(Ex, "A__2")
> Ex1
A__1 * (exp(A__2 * X) * X)
> Ex1s <- deparse(Ex1)
> Ex1s
[1] "A__1 * (exp(A__2 * X) * X)"
> pat1 <- paste(Ident, "__", Subsc, sep = "")
> rep1 <- "\\1\\[\\2\\]"
> Ex1ss <- gsub(pat1, rep1, Ex1s)
> Ex1ss
[1] "A[1] * (exp(A[2] * X) * X)"
> Ex2 <- parse(text = Ex1ss)[[1]]
> Ex2
A[1] * (exp(A[2] * X) * X)

Which is the required result.  This is messy and gets messier if you
are looking for some kind of generality, but you need to remember, R
is not, and never will be, a replacement for Maple.

> 
> 
> Thanks for any help with this.

Best of luck in automating this, but the tools are there.

Bill Venables.


From balint.czucz at uni-corvinus.hu  Tue Aug 15 09:21:21 2006
From: balint.czucz at uni-corvinus.hu (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Tue, 15 Aug 2006 09:21:21 +0200
Subject: [R] how to call forth a class definition buried in a package
Message-ID: <fab4bcf70608150021v51c23d68k2743690936a43316@mail.gmail.com>

Dear list,

I would like to use this C call:
tmp <- .Call("R_MPinv", covariance, tol, svdmem)

but it gives me the following error:
Error in getClass("LinStatExpectCovarMPinv") :
	"LinStatExpectCovarMPinv" is not a defined class

I think the reason for this is that the C code starts with this call:

PROTECT(ans = NEW_OBJECT(MAKE_CLASS("LinStatExpectCovarMPinv")));
....

and the Class definition for "LinStatExpectCovarMPinv" hides deep
buried in package "party".

Were it not for the C call, i could avoid this error by specifying a
"where=" argument to GetClass(). But what can I do now?

Thank you!
B?lint

-- 
Cz?cz B?lint
PhD hallgat?
BCE KTK Talajtan ?s V?zgazd?lkod?s Tansz?k
1118 Budapest, Vill?nyi ?t 29-43.


From bibiko at eva.mpg.de  Tue Aug 15 10:15:21 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Tue, 15 Aug 2006 10:15:21 +0200
Subject: [R] Question on .Options$max.print - print/cat extremely long
	strings on a screen
In-Reply-To: <101FDEA7-0FE2-4AB2-8666-D315AF4E48EC@eva.mpg.de>
References: <101FDEA7-0FE2-4AB2-8666-D315AF4E48EC@eva.mpg.de>
Message-ID: <D3E591E4-28D7-458C-BB95-860F1370824E@eva.mpg.de>

Dear list members,

sorry for my incompleteness!



My problem is the following:
(R 2.3.1 on Mac OS X 10.4.7 RAM 1GByte using Mac GUI)

I have a function like this:

foo1 <- function()
{
	out <- NULL
	for(i in 1:10010) out <- paste(out, i, ". line\n", sep="")
	return(out)	
}
a <- foo1()

Now I want to display 'a' on the screen:

cat(a)

This doesn't work. 'a' is displayed only until '830. line'

print(a)

The same, 'a' is displayed only until '\n754. line\n755'.

cat(a, file='test.txt') # OK
works fine. That means, internally 'a' is fine.


Then I tried this way:

foo2 <- function()
{
	out <- NULL
	for(i in 1:10010) out <- c(out, paste(i, ". line", sep=""))
	return(out)	
}
a <- foo2()

With

cat(a,sep="\n")
I see the complete content of 'a'.

paste(a, collapse = "\n")
I only see 'a' until '\n754. line\n755'.

cat(paste(a, collapse ="\n"))
I only see 'a' until '830. line'.

cat(paste(a, collapse ="\n"),file='test.txt')
This is OK.


My question now is whether there is an option to specifiy the maximum  
size of a string which is displayed on the screen (running R in a  
GUI)? Or is this fixed?

I read the help page about '.Options' and I found a variable  
'max.print' with the comment that is not yet used in basic R.
I don't know whether this variable is responsible for that. I  
increased 'max.print' but nothing changed.

##########
If I try this code on a Windows XP machine with 756 MByte RAM R-GUI  
says after executing

foo1 <- function()
{
	out <- NULL
	for(i in 1:10010) out <- paste(out, i, ". line\n", sep="")
	return(out)	
}
a <- foo1()
cat(a)

...
7548. lineWarning message:
printing of extremely long output is truncated

At least Windows writes a warning message.

###########
If I start a R session without the R-GUI via Mac Terminal typing 'R'

a<-foo1()
cat(a)

everything works perfectly!!!


I know the issue of outputting long strings and the way to display  
long strings via foo2() would be ok for me, but I spent some time to  
figure out why my function foo1() didn't work. R, running in GUI on  
Mac, don't give you a warning. Now I know that if I print a long  
string at the Mac-GUI-console the missing final quote character is an  
indicator for a truncated output on a Mac. Maybe it would be nice to  
output a warning(?)

Cheers,

Hans


From pkoufalas at adam.com.au  Tue Aug 15 10:35:49 2006
From: pkoufalas at adam.com.au (Paul Koufalas)
Date: Tue, 15 Aug 2006 18:05:49 +0930
Subject: [R] Protection stack overflow
Message-ID: <44E18765.3000603@adam.com.au>

G'day all.

I'm a new user of R -- but an arms-length user, as I'm running it from
Octave via the ROctave interface that Duncan Temple Lang wrote some
years ago and Stephan Van Der Walt recently updated for use with Octave
2.1.71. I'm using R version 2.1.1. ROctave uses libR.so to provide the
interface. My system is Ubuntu linux 5.10 and I'm using the packages
that come with this distro.

I'm getting a protection stack overflow error when recursively
calculating AR(p) time series models using the arima() function. The
recursion is involved because I calculate a new model with each new time
series data point. (I'm trying to reproduce some results in a research
paper and this is what they're doing.)

I've tried setting the expressions parameter to a higher number using
options(expressions=500000) but I'm still getting this problem with
stack overflow. I can get about 400-odd iterations and then the overflow
error appears. I yet haven't tried running the recursion natively in R,
and I realise I should do that.

Your advice would be much appreciated.

Cheers,
Paul.


From bibiko at eva.mpg.de  Tue Aug 15 10:39:29 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Tue, 15 Aug 2006 10:39:29 +0200
Subject: [R] Query the kind of calling a funcion
In-Reply-To: <D3E591E4-28D7-458C-BB95-860F1370824E@eva.mpg.de>
References: <101FDEA7-0FE2-4AB2-8666-D315AF4E48EC@eva.mpg.de>
	<D3E591E4-28D7-458C-BB95-860F1370824E@eva.mpg.de>
Message-ID: <9844D1C8-C99F-45B6-801E-B037D150ABA1@eva.mpg.de>


Dear all,


My question is concerned to the kind how a function is called.

Example A:

 > foo(1)

Example B:

 > a <- foo(1)


Is there any way for the function foo() to recognise whether the  
returned value of foo() is stored in a variable or not, i.e. to  
distinguish between Example A and B?


Any comments are welcomed.

Many thanks in advance

Hans


From ripley at stats.ox.ac.uk  Tue Aug 15 10:55:51 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Aug 2006 09:55:51 +0100 (BST)
Subject: [R] Protection stack overflow
In-Reply-To: <44E18765.3000603@adam.com.au>
References: <44E18765.3000603@adam.com.au>
Message-ID: <Pine.LNX.4.64.0608150951560.18980@gannet.stats.ox.ac.uk>

R has a command-line option to set the ppstack size,

 --max-ppsize=N        Set max size of protect stack to N

Looks like you need to supply this (and it can be done with embedded R)
if the problem persists with current R.

You could also try arima0 or even ar to do the fitting.

On Tue, 15 Aug 2006, Paul Koufalas wrote:

> G'day all.
> 
> I'm a new user of R -- but an arms-length user, as I'm running it from
> Octave via the ROctave interface that Duncan Temple Lang wrote some
> years ago and Stephan Van Der Walt recently updated for use with Octave
> 2.1.71. I'm using R version 2.1.1. ROctave uses libR.so to provide the
> interface. My system is Ubuntu linux 5.10 and I'm using the packages
> that come with this distro.

Note that your version of R is well outdated, and the posting guide did 
ask you to update it BEFORE posting.

> I'm getting a protection stack overflow error when recursively
> calculating AR(p) time series models using the arima() function. The
> recursion is involved because I calculate a new model with each new time
> series data point. (I'm trying to reproduce some results in a research
> paper and this is what they're doing.)
> 
> I've tried setting the expressions parameter to a higher number using
> options(expressions=500000) but I'm still getting this problem with
> stack overflow. I can get about 400-odd iterations and then the overflow
> error appears. I yet haven't tried running the recursion natively in R,
> and I realise I should do that.
> 
> Your advice would be much appreciated.
> 
> Cheers,
> Paul.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Tue Aug 15 11:29:18 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Aug 2006 11:29:18 +0200
Subject: [R] How to reply to a thread if .. R-help mails .. in digest
In-Reply-To: <XFMail.060813152115.Ted.Harding@nessie.mcc.ac.uk>
References: <44DF276F.3070903@uni-hamburg.de>
	<XFMail.060813152115.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <17633.37870.312431.164327@stat.math.ethz.ch>

Thanks a lot, Ted, for the good extensive explanation.
Let me try summarize, confirm and add a bit w/o repeating too much:

- If you get regular mailing list e-mails, and reply to
  postings, any decent mail software will take the 
  'Message-Id:' header of the message you reply to, and
  produce 'In-Reply-To:' and 'References:' headers from it, 
  and will add them to your own e-mail.
  [ Most e-mail softwares keep these headers hidden, and quite a
    few pieces of e-mail crapware don't even allow you to see these headers.]

 { And as Ted mentioned; unfortunately, we still have too many
   r-help posters who **misuse** their e-mail-software's `reply'
   and then inadvertently "hijack threads"... }

- Yes, the threads are *not* built from 'Subject:'s but rather
  using the e-mail headers 'References:' and/or 'In-Reply-To:'

This is all based on international e-mail standards (RFC/..) mostly going
back into the age where most R-help readers have not ever used e-mail..

- With mailman, there are 2 ways to receive digests:
   (1) "plain text"  --- which is default, since some dumb e-mail
		programs can only deal with those
   (2) "MIME"  --- which uses the MIME standard to send the digest.
	       With a good e-mail software, this means that you
	       get ``each message as attachment'' {that's how it
	       typically looks to the user}
	   which you then can open - in your mail software(!) -
	   and it will behave like a regular e-mail; it has a
	   (typically hidden) 'Message-ID:' etc
	  ==> when replying to such a message you automatically
	  get both:
	   1) correct Subject
	   2) correct In-Reply-To + References ===> correct thread

*So* : What we (and many) recommend to all  digest  subscribers
 is to activate the "MIME" option - on their mailing list
 "membership info page" to where you get from
 https://stat.ethz.ch/mailman/listinfo/r-help 
 after entering your e-mail address at the very bottom of the
 page (and then your list password).

-- but, as Michael Dewey just mentioned,
 unfortunately there are (too many) pieces of e-mail crapware
 around which even don't support the above correctly...

Martin Maechler, 
ETH Zurich, provider of (most of) the mailing list infrastructure for R.


From hannes at ruhrau.de  Tue Aug 15 11:53:21 2006
From: hannes at ruhrau.de (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Tue, 15 Aug 2006 11:53:21 +0200 (CEST)
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <44E0A172.2020907@pdf.com>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au> <44E0A172.2020907@pdf.com>
Message-ID: <44902.129.206.90.2.1155635601.squirrel@mail.panix.com>

> 	  The pro's and con's of using "scale breaks" were discussed by
> Cleveland (1985) The Elements of Graphing Data (Wadsworth, pp. 85-91,
> 149).  I don't know what Cleveland said about this is the second edition

Spencer Graves:
> but I believe there are times when scale breaks are
> appropriate, but the display should make this nonstandard transition
> very clear;

... in which case you are close to having two graphs
sharing an x-axis and therefore saving on ink (yay!).


From amsa36060 at yahoo.com  Tue Aug 15 13:27:44 2006
From: amsa36060 at yahoo.com (Amir Safari)
Date: Tue, 15 Aug 2006 04:27:44 -0700 (PDT)
Subject: [R] A plot with a bisector
Message-ID: <20060815112744.78516.qmail@web60421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/dc63a44b/attachment.pl 

From kkthird at yahoo.com  Tue Aug 15 14:29:04 2006
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Tue, 15 Aug 2006 05:29:04 -0700 (PDT)
Subject: [R] Aliases for arguments in a function
Message-ID: <20060815122904.14281.qmail@web52511.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/580e5178/attachment.pl 

From rmh at temple.edu  Tue Aug 15 14:39:40 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 15 Aug 2006 08:39:40 -0400 (EDT)
Subject: [R] Aliases for arguments in a function
Message-ID: <20060815083940.BFY64405@po-d.temple.edu>

foo <- function(arg1, this, that) {
  if (missing(this) && !missing(that)) this <- that
  if(this < 0) stop("this must be positive")
  return(arg1/this)
}

foo(arg1=5, this=10)

foo(arg1=5, that=10)


From close2ceo at yahoo.com  Tue Aug 15 14:54:51 2006
From: close2ceo at yahoo.com (Xiaodong Jin)
Date: Tue, 15 Aug 2006 05:54:51 -0700 (PDT)
Subject: [R] nls
Message-ID: <20060815125451.12520.qmail@web31204.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/a268c330/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Aug 15 15:14:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Aug 2006 14:14:40 +0100 (BST)
Subject: [R] nls
In-Reply-To: <20060815125451.12520.qmail@web31204.mail.mud.yahoo.com>
References: <20060815125451.12520.qmail@web31204.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608151411310.25081@gannet.stats.ox.ac.uk>

You problem is x^c for x = 0.  If you intended only c > 1, try a starting 
value meeting that condition (but it seems that the optimal c is about 
0.27 is you increase x slightly).

Why have you used ~~ ?  (Maybe because despite being asked not to, you 
sent HTML mail?)

On Tue, 15 Aug 2006, Xiaodong Jin wrote:

>   Is there anyway to change any y[i] value (i=2,...6) to make following NLS workable? 
>    
>   x <- c(0,5,10,15,20,25,30)
>   y <- c(1.00000,0.82000,0.68000,0.64000,0.66667,0.68667,0.64000)
>   lm(1/y ~~ x)
>   nls(1/y ~~ a+b*x^c, start=list(a=1.16122,b=0.01565,c=1), trace=TRUE)
>    
>   #0.0920573 :  1.16122 0.01565 1.00000 
> #Error in numericDeriv(form[[3]], names(ind), env) : 
> #        Missing value or an infinity produced when evaluating the model
> 
>  			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Aug 15 15:16:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Aug 2006 14:16:48 +0100 (BST)
Subject: [R] Aliases for arguments in a function
In-Reply-To: <20060815122904.14281.qmail@web52511.mail.yahoo.com>
References: <20060815122904.14281.qmail@web52511.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608151415000.25081@gannet.stats.ox.ac.uk>

foo <- function(arg1, this=that, that)
...

works.

On Tue, 15 Aug 2006, KKThird at Yahoo.Com wrote:

> Hi all. 
> 
> I have a function that I would like to use either the argument name as originally defined or another name. Within the function (and other functions) use the argument name as originally written, so I don't want to simply remove the old argument name for the new one, but simply allow the function to treat both argument names as equivalent. 
> 
> Here is an example:
> 
> foo <- function(arg1, this)
> {
> if(this < 0) stop("this must be positive")
> return(arg1/this)
> }
> 
> foo(arg1=5, this=10)
> 
> But, I also want foo() to work equivalently with the following (where 'this' and 'that 'are treated as if they were the same):
> foo(arg1=5, that=10)
> 
> Any thoughts would be appreciated.
> 
> Thanks,
> Ken
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Avishek_Hazrachoudhury at ml.com  Tue Aug 15 15:18:20 2006
From: Avishek_Hazrachoudhury at ml.com (Hazrachoudhury, Avishek (RSCH))
Date: Tue, 15 Aug 2006 09:18:20 -0400
Subject: [R] ARCH Jump diffusion models
Message-ID: <3CB5334C6C53B74194E31D759106917F20175184@mlnyb701mb.amrs.win.ml.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/50aeae62/attachment.pl 

From p.dalgaard at biostat.ku.dk  Tue Aug 15 15:31:23 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Aug 2006 15:31:23 +0200
Subject: [R] nls
In-Reply-To: <Pine.LNX.4.64.0608151411310.25081@gannet.stats.ox.ac.uk>
References: <20060815125451.12520.qmail@web31204.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0608151411310.25081@gannet.stats.ox.ac.uk>
Message-ID: <x2r6zigm6s.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> You problem is x^c for x = 0.  If you intended only c > 1, try a starting 
> value meeting that condition (but it seems that the optimal c is about 
> 0.27 is you increase x slightly).

Surely you mean c > 0.

>  nls(1/y ~ a+b*x^exp(c), start=list(a=1.16122,b=0.01565,c=0))
Nonlinear regression model
  model:  1/y ~ a + b * x^exp(c)
   data:  parent.frame()
         a          b          c
 0.9944025  0.1953168 -1.1495206
 residual sum-of-squares:  0.03303464
>  nls(1/y ~ a+b*x^c, start=list(a=1.16122,b=0.01565,c=exp(-1.1)))
Nonlinear regression model
  model:  1/y ~ a + b * x^c
   data:  parent.frame()
        a         b         c
0.9944026 0.1953165 0.3167891
 residual sum-of-squares:  0.03303464

(but even setting c=exp(-1) triggers the error; there could be cause
for robustifying the nls algorithm)
 
> Why have you used ~~ ?  (Maybe because despite being asked not to, you 
> sent HTML mail?)
> 
> On Tue, 15 Aug 2006, Xiaodong Jin wrote:
> 
> >   Is there anyway to change any y[i] value (i=2,...6) to make following NLS workable? 
> >    
> >   x <- c(0,5,10,15,20,25,30)
> >   y <- c(1.00000,0.82000,0.68000,0.64000,0.66667,0.68667,0.64000)
> >   lm(1/y ~~ x)
> >   nls(1/y ~~ a+b*x^c, start=list(a=1.16122,b=0.01565,c=1), trace=TRUE)
> >    
> >   #0.0920573 :  1.16122 0.01565 1.00000 
> > #Error in numericDeriv(form[[3]], names(ind), env) : 
> > #        Missing value or an infinity produced when evaluating the model
> > 
> >  			
> > ---------------------------------
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From petr.pikal at precheza.cz  Tue Aug 15 15:42:18 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 15 Aug 2006 15:42:18 +0200
Subject: [R] nls
In-Reply-To: <20060815125451.12520.qmail@web31204.mail.mud.yahoo.com>
Message-ID: <44E1EB5A.29564.1AB2CC0@localhost>

Hi

Why do you want to change your variable values? It smells a rat to 
me.

If you just change your a,b,c values nls arrives to some finite 
result (e.g. c=1.5 or c=0.3) . BTW by what magic you obtained such 
precise and wrong estimates for a,b,c?

HTH
Petr





On 15 Aug 2006 at 5:54, Xiaodong Jin wrote:

Date sent:      	Tue, 15 Aug 2006 05:54:51 -0700 (PDT)
From:           	Xiaodong Jin <close2ceo at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] nls

>   Is there anyway to change any y[i] value (i=2,...6) to make
>   following NLS workable? 
> 
>   x <- c(0,5,10,15,20,25,30)
>   y <- c(1.00000,0.82000,0.68000,0.64000,0.66667,0.68667,0.64000)
>   lm(1/y ~~ x) nls(1/y ~~ a+b*x^c,
>   start=list(a=1.16122,b=0.01565,c=1), trace=TRUE)
> 
>   #0.0920573 :  1.16122 0.01565 1.00000 
> #Error in numericDeriv(form[[3]], names(ind), env) : 
> #        Missing value or an infinity produced when evaluating the
> #        model
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Tue Aug 15 15:50:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Aug 2006 14:50:26 +0100 (BST)
Subject: [R] nls
In-Reply-To: <x2r6zigm6s.fsf@viggo.kubism.ku.dk>
References: <20060815125451.12520.qmail@web31204.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0608151411310.25081@gannet.stats.ox.ac.uk>
	<x2r6zigm6s.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.64.0608151447010.25719@gannet.stats.ox.ac.uk>

On Tue, 15 Aug 2006, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > You problem is x^c for x = 0.  If you intended only c > 1, try a starting 
> > value meeting that condition (but it seems that the optimal c is about 
> > 0.27 is you increase x slightly).
> 
> Surely you mean c > 0.

I did.

> >  nls(1/y ~ a+b*x^exp(c), start=list(a=1.16122,b=0.01565,c=0))
> Nonlinear regression model
>   model:  1/y ~ a + b * x^exp(c)
>    data:  parent.frame()
>          a          b          c
>  0.9944025  0.1953168 -1.1495206
>  residual sum-of-squares:  0.03303464
> >  nls(1/y ~ a+b*x^c, start=list(a=1.16122,b=0.01565,c=exp(-1.1)))
> Nonlinear regression model
>   model:  1/y ~ a + b * x^c
>    data:  parent.frame()
>         a         b         c
> 0.9944026 0.1953165 0.3167891
>  residual sum-of-squares:  0.03303464
> 
> (but even setting c=exp(-1) triggers the error; there could be cause
> for robustifying the nls algorithm)

Well, there is an option to use a bounded-region algorithm, e.g.

x <- c(0,5,10,15,20,25,30)
y <- c(1.00000,0.82000,0.68000,0.64000,0.66667,0.68667,0.64000)
nls(1/y ~ a+b*x^c, start=list(a=1.16122,b=0.01565,c=1), trace=TRUE,
    algorithm="port", lower=c(-Inf, -Inf, 0), upper=rep(Inf, 3))

works.

>  
> > Why have you used ~~ ?  (Maybe because despite being asked not to, you 
> > sent HTML mail?)
> > 
> > On Tue, 15 Aug 2006, Xiaodong Jin wrote:
> > 
> > >   Is there anyway to change any y[i] value (i=2,...6) to make following NLS workable? 
> > >    
> > >   x <- c(0,5,10,15,20,25,30)
> > >   y <- c(1.00000,0.82000,0.68000,0.64000,0.66667,0.68667,0.64000)
> > >   lm(1/y ~~ x)
> > >   nls(1/y ~~ a+b*x^c, start=list(a=1.16122,b=0.01565,c=1), trace=TRUE)
> > >    
> > >   #0.0920573 :  1.16122 0.01565 1.00000 
> > > #Error in numericDeriv(form[[3]], names(ind), env) : 
> > > #        Missing value or an infinity produced when evaluating the model
> > > 
> > >  			
> > > ---------------------------------
> > > 
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ruser2006 at yahoo.com  Tue Aug 15 16:01:13 2006
From: ruser2006 at yahoo.com (r user)
Date: Tue, 15 Aug 2006 07:01:13 -0700 (PDT)
Subject: [R] question re: "summarry.lm" and NA values
Message-ID: <20060815140113.99778.qmail@web37012.mail.mud.yahoo.com>

Is there a way to get the following code to include
NA values where the coefficients are ?NA??

((summary(reg))$coefficients)

explanation:

Using a loop, I am running regressions on several
?subsets? of ?data1?.

?reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) )?

My regression has 10 independent variables, and I
therefore expect 11 coefficients.
After each regression, I wish to save the coefficients
and standard errors of the coefficients in a table
with 22 columns.

I successfully extract the coefficients using the
following code:
?reg$coefficients?

I attempt to extract the standard errors using :

aperm((summary(reg))$coefficients)[2,]

((summary(reg))$coefficients)

My problem:
For some of my subsets, I am missing data for one or
more of the independent variables.  This of course
causes the coefficients and standard erros for this
variable to be ?NA?.

Is there a way to include the NA standard errors, so
that I have the same number of standard erros and
coefficients for each regression, and can then store
the coefficients and standard erros in my table of 22
columns?


From ajayshah at mayin.org  Mon Aug 14 15:08:27 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Mon, 14 Aug 2006 18:38:27 +0530
Subject: [R] Presentation of multiple models in one table using xtable
Message-ID: <20060814130827.GA9396@lubyanka.local>

Consider this situation:
> x1 <- runif(100); x2 <- runif(100); y <- 2 + 3*x1 - 4*x2 + rnorm(100)
> m1 <- summary(lm(y ~ x1))
> m2 <- summary(lm(y ~ x2))
> m3 <- summary(lm(y ~ x1 + x2))

Now you have estimated 3 different "competing" models, and suppose you
want to present the set of models in one table. xtable(m1) is cool,
but doing that thrice would give us 3 different tables.

What I want is this one table:

-----------------------------------------------------------
                    M1             M2              M3
-----------------------------------------------------------
Intercept         0.0816         3.6292         2.2272
                 (0.5533)       (0.2316)***    (0.2385)***

x1                2.8151                        2.7606
                 (0.5533)***                   (0.3193)***

x2                              -4.2899        -4.2580
                                (0.401)***     (0.3031)***

$\sigma_e$        1.538          1.175          0.8873
$R^2$             0.2089         0.5385         0.7393
-----------------------------------------------------------

How would one set about doing this? I am hoping that it's possible to
write a function xtable.multi.lm where one would say
xtable.multi.lm(m1,m2,m3) and get the above table.

My sense is there are three challenges:

1. How to write a general R function which eats a unpredictable number
   of summary(lm()) objects, and fill out a matrix with results such
   as the above.

2. How to get a good xtable(), with decimal alignment and with the ***
   stuff (actually, $^{***}$). Will there be any catch in dropping
   into mathmode for $R^2$? After each pair of lines, I'd like to have
   a \\[2mm] so as to get a nice spacing in the table.

3. This style of presentation seems relevant to a whole host of models
   - whether ARCH models or survival models - not just OLS
   regressions. It would be very nice if one supported all manner of
   model objects and not just what comes out of lm().

I'm happy to take a crack at writing this function, which should
ideally go back into the xtable library. But I don't have an idea on
how to go about these two questions. If you will guide me, I am happy
to work on it. :-)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From John.Kerpel at infores.com  Tue Aug 15 16:57:37 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Tue, 15 Aug 2006 09:57:37 -0500
Subject: [R] fMultivar OLS - how to do dynamic regression?
Message-ID: <44A8B25381923D4F93B74B2676A50F6D0328E0CB@MAIL1.infores.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/1705cbd7/attachment.pl 

From petr.pikal at precheza.cz  Tue Aug 15 17:16:00 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 15 Aug 2006 17:16:00 +0200
Subject: [R] question re: "summarry.lm" and NA values
In-Reply-To: <20060815140113.99778.qmail@web37012.mail.mud.yahoo.com>
Message-ID: <44E20150.6738.200F33F@localhost>

Hi

On 15 Aug 2006 at 7:01, r user wrote:

Date sent:      	Tue, 15 Aug 2006 07:01:13 -0700 (PDT)
From:           	r user <ruser2006 at yahoo.com>
To:             	rhelp <r-help at stat.math.ethz.ch>
Subject:        	[R] question re: "summarry.lm" and NA values

> Is there a way to get the following code to include
> NA values where the coefficients are  NA ?
> 
> ((summary(reg))$coefficients)

better
coef(reg)

> 
> explanation:
> 
> Using a loop, I am running regressions on several
>  subsets  of  data1 .
> 
>  reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) ) 
> 
> My regression has 10 independent variables, and I
> therefore expect 11 coefficients.
> After each regression, I wish to save the coefficients
> and standard errors of the coefficients in a table
> with 22 columns.
> 
> I successfully extract the coefficients using the
> following code:
>  reg$coefficients 
> 
> I attempt to extract the standard errors using :
> 
> aperm((summary(reg))$coefficients)[2,]
> 
> ((summary(reg))$coefficients)
> 
> My problem:
> For some of my subsets, I am missing data for one or
> more of the independent variables.  This of course
> causes the coefficients and standard erros for this
> variable to be  NA .

??%^&*^??

What version? My lm behaves in accordance with na.action and it 
throws an error in case na.fail, computes a value in case of na.omit 
or na.exclude and again throws an error if the variable consist 
exclusively from NA values. 

The only way how to get NA in coeficient is when a variable is either 
constant or linear combination of other variable(s). Then
coef(reg) 
will give you correctly NA in the variable which appears constant and 
in this case you could use it for setting standard error also as NA 
let say by using ifelse statement and matching of names.

HTH
Petr

> 
> Is there a way to include the NA standard errors, so
> that I have the same number of standard erros and
> coefficients for each regression, and can then store
> the coefficients and standard erros in my table of 22
> columns?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Tue Aug 15 17:32:16 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 15 Aug 2006 17:32:16 +0200
Subject: [R] question re: "summarry.lm" and NA values
Message-ID: <44E20520.20165.20FD780@localhost>

Hi

just as a quick workaround you probably can use aliased value from 
summary

fff<-rep(summary(reg)$aliased,4)
dim(fff)<-c(no.of.your.variables,4)
fff[which(fff)]<-NA
fff[which(!fff)]<-coef(summary(reg))

to get coef matrix with NA values

HTH
Petr


On 15 Aug 2006 at 17:15, r user <ruser2006 at yahoo.com>, wrote:

From:           	Petr Pikal <petr.pikal at precheza.cz>
To:             	r user <ruser2006 at yahoo.com>, rhelp <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] question re: "summarry.lm" and NA values
Date sent:      	Tue, 15 Aug 2006 17:15:01 +0200

> Hi
> 
> On 15 Aug 2006 at 7:01, r user wrote:
> 
> Date sent:      	Tue, 15 Aug 2006 07:01:13 -0700 (PDT)
> From:           	r user <ruser2006 at yahoo.com>
> To:             	rhelp <r-help at stat.math.ethz.ch>
> Subject:        	[R] question re: "summarry.lm" and NA values
> 
> > Is there a way to get the following code to include
> > NA values where the coefficients are  NA ?
> > 
> > ((summary(reg))$coefficients)
> 
> better
> coef(reg)
> 
> > 
> > explanation:
> > 
> > Using a loop, I am running regressions on several
> >  subsets  of  data1 .
> > 
> >  reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) ) 
> > 
> > My regression has 10 independent variables, and I
> > therefore expect 11 coefficients.
> > After each regression, I wish to save the coefficients
> > and standard errors of the coefficients in a table
> > with 22 columns.
> > 
> > I successfully extract the coefficients using the
> > following code:
> >  reg$coefficients 
> > 
> > I attempt to extract the standard errors using :
> > 
> > aperm((summary(reg))$coefficients)[2,]
> > 
> > ((summary(reg))$coefficients)
> > 
> > My problem:
> > For some of my subsets, I am missing data for one or
> > more of the independent variables.  This of course
> > causes the coefficients and standard erros for this
> > variable to be  NA .
> 
> ??%^&*^??
> 
> What version? My lm behaves in accordance with na.action and it 
> throws an error in case na.fail, computes a value in case of na.omit
> or na.exclude and again throws an error if the variable consist
> exclusively from NA values. 
> 
> The only way how to get NA in coeficient is when a variable is either
> constant or linear combination of other variable(s). Then coef(reg)
> will give you correctly NA in the variable which appears constant and
> in this case you could use it for setting standard error also as NA
> let say by using ifelse statement and matching of names.
> 
> HTH
> Petr
> 
> > 
> > Is there a way to include the NA standard errors, so
> > that I have the same number of standard erros and
> > coefficients for each regression, and can then store
> > the coefficients and standard erros in my table of 22
> > columns?
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> 

Petr Pikal
petr.pikal at precheza.cz


From tim_b_mcdonald at yahoo.com  Tue Aug 15 17:33:50 2006
From: tim_b_mcdonald at yahoo.com (Tim McDonald)
Date: Tue, 15 Aug 2006 08:33:50 -0700 (PDT)
Subject: [R] Looking for info on the "Regression Modeling Strategies in R"
	course in DC area
Message-ID: <20060815153350.1632.qmail@web57205.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/64d823dd/attachment.pl 

From S.Pickett at exeter.ac.uk  Tue Aug 15 17:34:13 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Tue, 15 Aug 2006 16:34:13 +0100 (BST)
Subject: [R] REML with random slopes and random intercepts giving strange
 results
Message-ID: <1302.144.173.76.117.1155656053.squirrel@www.webmail.ex.ac.uk>

Hi everyone,
I have been using REML to derive intercepts and coeficients for each
individual in a growth study. So the code is
m2 <- lmer(change.wt ~ newwt+(newwt|id), data = grow)

Calling coef(model.lmer) gives a matrix with this information which is
what I want. However, as a test I looked at each individual on its own and
used a simple linear regression to obtain the same information, then I
compared the results. It looks like the REML method doesnt seem to
approximate the two parameters as well as using the simple linear
regression on each individual separately, as judged by looking at graphs.
Indeed, why do the results differ at all?
Excuse my naivety if this is a silly question.
Thanks to everyone for replying to my previous questions, very much
appreciated.
Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From ripley at stats.ox.ac.uk  Tue Aug 15 17:40:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Aug 2006 16:40:03 +0100 (BST)
Subject: [R] question re: "summarry.lm" and NA values
In-Reply-To: <44E20150.6738.200F33F@localhost>
References: <44E20150.6738.200F33F@localhost>
Message-ID: <Pine.LNX.4.64.0608151635030.26913@gannet.stats.ox.ac.uk>

On Tue, 15 Aug 2006, Petr Pikal wrote:

> Hi
> 
> On 15 Aug 2006 at 7:01, r user wrote:
> 
> Date sent:      	Tue, 15 Aug 2006 07:01:13 -0700 (PDT)
> From:           	r user <ruser2006 at yahoo.com>
> To:             	rhelp <r-help at stat.math.ethz.ch>
> Subject:        	[R] question re: "summarry.lm" and NA values
> 
> > Is there a way to get the following code to include
> > NA values where the coefficients are  NA ?
> > 
> > ((summary(reg))$coefficients)
> 
> better
> coef(reg)

coef(summary(reg)), perhaps.

> > explanation:
> > 
> > Using a loop, I am running regressions on several
> >  subsets  of  data1 .
> > 
> >  reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) ) 
> > 
> > My regression has 10 independent variables, and I
> > therefore expect 11 coefficients.
> > After each regression, I wish to save the coefficients
> > and standard errors of the coefficients in a table
> > with 22 columns.
> > 
> > I successfully extract the coefficients using the
> > following code:
> >  reg$coefficients 
> > 
> > I attempt to extract the standard errors using :
> > 
> > aperm((summary(reg))$coefficients)[2,]
> > 
> > ((summary(reg))$coefficients)
> > 
> > My problem:
> > For some of my subsets, I am missing data for one or
> > more of the independent variables.  This of course
> > causes the coefficients and standard erros for this
> > variable to be  NA .
> 
> ??%^&*^??
> 
> What version? My lm behaves in accordance with na.action and it 
> throws an error in case na.fail, computes a value in case of na.omit 
> or na.exclude and again throws an error if the variable consist 
> exclusively from NA values. 
> 
> The only way how to get NA in coeficient is when a variable is either 
> constant or linear combination of other variable(s). Then
> coef(reg) 
> will give you correctly NA in the variable which appears constant and 
> in this case you could use it for setting standard error also as NA 
> let say by using ifelse statement and matching of names.

That happens in the print method, stats:::print.summary.lm contains

        coefs <- x$coefficients
        if (!is.null(aliased <- x$aliased) && any(aliased)) {
            cn <- names(aliased)
            coefs <- matrix(NA, length(aliased), 4, dimnames = list(cn,
                colnames(coefs)))
            coefs[!aliased, ] <- x$coefficients
        }

so the code is already available

> > 
> > Is there a way to include the NA standard errors, so
> > that I have the same number of standard erros and
> > coefficients for each regression, and can then store
> > the coefficients and standard erros in my table of 22
> > columns?
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gunter.berton at gene.com  Tue Aug 15 17:49:40 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 15 Aug 2006 08:49:40 -0700
Subject: [R] question re: "summarry.lm" and NA values
In-Reply-To: <20060815140113.99778.qmail@web37012.mail.mud.yahoo.com>
Message-ID: <000c01c6c082$6b7885e0$711f210a@gne.windows.gene.com>

"Is there a way to..." always has the answer "yes" in R (or C or any
language for that matter). The question is: "Is there a GOOD way...?" where
"good" depends on the specifics of the situation. So after that polemic,
below is an effort to answer, (adding to what Petr Pikal already said):

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of r user
> Sent: Tuesday, August 15, 2006 7:01 AM
> To: rhelp
> Subject: [R] question re: "summarry.lm" and NA values
> 
> Is there a way to get the following code to include
> NA values where the coefficients are "NA"?
> 
> ((summary(reg))$coefficients)
BAAAD! Don't so this. Use the extractor on the object: coef(reg) 
This suggests that you haven't read the documentation carefully, which tends
to arouse the ire of would-be helpers.

> 
> explanation:
> 
> Using a loop, I am running regressions on several
> "subsets" of "data1".
> 
> "reg <- ( lm(lm(data1[,1] ~., data1[,2:l])) )"
??? There's an error here I think. Do you mean update()? Do you have your
subscripting correct?

> 
> My regression has 10 independent variables, and I
> therefore expect 11 coefficients.
> After each regression, I wish to save the coefficients
> and standard errors of the coefficients in a table
> with 22 columns.
> 
> I successfully extract the coefficients using the
> following code:
> "reg$coefficients"
Use the extractor, coef()

> 
> I attempt to extract the standard errors using :
> 
> aperm((summary(reg))$coefficients)[2,]

BAAAD! Use the extractor vcov(): sqrt(diag(vcov(reg)))
> 
> ((summary(reg))$coefficients)
> 
> My problem:
> For some of my subsets, I am missing data for one or
> more of the independent variables.  This of course
> causes the coefficients and standard erros for this
> variable to be "NA".
Not it doesn't, as Petr said.

One possible approach: Assuming that a variable is actually missing (all
NA's), note that coef(reg) is a named vector, so that the character string
names of the regressors actually used are available. You can thus check for
what's missing and add them as NA's at each return. Though I confess that I
see no reason to put things ina matrix rather than just using a list. But
that's a matter of personal taste I suppose.

> 
> Is there a way to include the NA standard errors, so
> that I have the same number of standard erros and
> coefficients for each regression, and can then store
> the coefficients and standard erros in my table of 22
> columns?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From HDoran at air.org  Tue Aug 15 17:53:05 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 15 Aug 2006 11:53:05 -0400
Subject: [R] REML with random slopes and random intercepts giving
	strange results
Message-ID: <2323A6D37908A847A7C32F1E3662C80E276F90@dc1ex01.air.org>

I don't this is because you are using REML. The BLUPs from a mixed model
experience some shrinkage whereas the OLS estimates would not.  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simon Pickett
> Sent: Tuesday, August 15, 2006 11:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] REML with random slopes and random intercepts 
> giving strange results
> 
> Hi everyone,
> I have been using REML to derive intercepts and coeficients 
> for each individual in a growth study. So the code is
> m2 <- lmer(change.wt ~ newwt+(newwt|id), data = grow)
> 
> Calling coef(model.lmer) gives a matrix with this information 
> which is what I want. However, as a test I looked at each 
> individual on its own and used a simple linear regression to 
> obtain the same information, then I compared the results. It 
> looks like the REML method doesnt seem to approximate the two 
> parameters as well as using the simple linear regression on 
> each individual separately, as judged by looking at graphs.
> Indeed, why do the results differ at all?
> Excuse my naivety if this is a silly question.
> Thanks to everyone for replying to my previous questions, 
> very much appreciated.
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From efg at stowers-institute.org  Tue Aug 15 18:59:56 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 15 Aug 2006 11:59:56 -0500
Subject: [R] nls convergence problem
Message-ID: <ebsuie$8t5$1@sea.gmane.org>

I'm having problems getting nls to agree that convergence has occurred in a 
toy problem.

nls.out never gets defined when there is an error in nls.  Reaching the 
maximum number of iterations is alway an error, so nls.out never gets 
defined when the maximum number of iterations is reched.

>From ?nls.control:
  tol: A positive numeric value specifying the tolerance level for
          the relative offset convergence criterion.

>From some S-Plus documentation I found online via Google:
http://www.uni-muenster.de/ZIV/Mitarbeiter/BennoSueselbeck/s-html/helpfiles/nls.control.html

tolerance:
tolerance for the relative offset convergence criterion in the algorithm. 
Default 0.001. Note that the convergence test used in nls() is strictly 
relative. Therefore if the solution to a problem turned out to be a perfect 
fit (unlikely except in artificial examples), convergence is not guaranteed 
to be recognized by the algorithm.


Here's my toy problem:
> ?nls.control
> ?nls
> # Method 2
> X <- 0:15
> Y <- 9.452 * exp(-0.109*X) + 5.111   # Toy problem
>
> nls.out <- nls(Y ~ a*exp(b*X)+c,
+                start=list(a=6,b=-0.5,c=1),
+                control=nls.control(maxiter=15, tol=0.01),  # nothing makes 
sense here
+                trace=TRUE)
1016.507 :   6.0 -0.5  1.0
143.5807 :   6.1680290 -0.1506021  4.4013020
7.306365 :   9.10093164 -0.09114858  5.44620298
0.0342703 :   9.2801070 -0.1109063  5.2795803
3.001985e-05 :   9.4506654 -0.1089749  5.1122982
1.918531e-14 :   9.452 -0.109  5.111
6.894644e-28 :   9.452 -0.109  5.111
2.208811e-29 :   9.452 -0.109  5.111
7.888609e-30 :   9.452 -0.109  5.111
7.888609e-30 :   9.452 -0.109  5.111
7.099748e-30 :   9.452 -0.109  5.111
3.155444e-30 :   9.452 -0.109  5.111
3.155444e-30 :   9.452 -0.109  5.111
3.155444e-30 :   9.452 -0.109  5.111
3.155444e-30 :   9.452 -0.109  5.111
3.155444e-30 :   9.452 -0.109  5.111
Error in nls(Y ~ a * exp(b * X) + c, start = list(a = 6, b = -0.5, c = 1), 
:
        number of iterations exceeded maximum of 15

There is near-perfect convergence after 12 iterations, but I cannot figure 
out a way for R to recognize it.

What does "relative offset convergence criterion" mean?  How can nls.control 
be used to say this problem has converged, and have nls exit without an 
error?

Should the R documentation be modified to explain what "relative offset 
convergence criterion" means?  Should the R documentation be expanded to 
include the comment from the S-Plus: "Therefore if the solution to a problem 
turned out to be a perfect fit (unlikely except in artificial examples), 
convergence is not guaranteed to be recognized by the algorithm".  If this 
is true, this seems like a suboptimal design.

Thanks for any insight about this.

efg

Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research


From muster at gmail.com  Tue Aug 15 19:10:46 2006
From: muster at gmail.com (T Mu)
Date: Tue, 15 Aug 2006 13:10:46 -0400
Subject: [R] How to show classes of all columns of a data frame?
Message-ID: <b68812e70608151010s14cb09f9i258862484b993a39@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/817101a7/attachment.pl 

From mschwartz at mn.rr.com  Tue Aug 15 19:19:35 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 15 Aug 2006 12:19:35 -0500
Subject: [R] How to show classes of all columns of a data frame?
In-Reply-To: <b68812e70608151010s14cb09f9i258862484b993a39@mail.gmail.com>
References: <b68812e70608151010s14cb09f9i258862484b993a39@mail.gmail.com>
Message-ID: <1155662375.3990.18.camel@localhost.localdomain>

On Tue, 2006-08-15 at 13:10 -0400, T Mu wrote:
> Hi all,
> 
> Suppose I have a data frame myDF, col A is factor, col B is numeric, col C
> is character. I can get their classes by
> 
> > class(myDF$A)
> 
> but is there a quick way to show what classes of all columns are? Thank you.
> 
> Tian

Depending upon the output format you desire:

> lapply(iris, class)
$Sepal.Length
[1] "numeric"

$Sepal.Width
[1] "numeric"

$Petal.Length
[1] "numeric"

$Petal.Width
[1] "numeric"

$Species
[1] "factor"


or

> sapply(iris, class)
Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species
   "numeric"    "numeric"    "numeric"    "numeric"     "factor"


See ?lapply and ?sapply


HTH,

Marc Schwartz


From knussear at mac.com  Tue Aug 15 20:12:33 2006
From: knussear at mac.com (Ken Nussear)
Date: Tue, 15 Aug 2006 11:12:33 -0700
Subject: [R] zlim not working in persp3d
Message-ID: <F199517C-5A31-4646-A462-1875913143DA@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/43aa4a5d/attachment.pl 

From ssj1364 at gmail.com  Tue Aug 15 20:14:58 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Tue, 15 Aug 2006 12:14:58 -0600
Subject: [R]  rexp question
Message-ID: <1c6126db0608151114m1487c69k612336b7286ea0ba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/a57f1c56/attachment.pl 

From knussear at mac.com  Tue Aug 15 20:29:04 2006
From: knussear at mac.com (Ken Nussear)
Date: Tue, 15 Aug 2006 11:29:04 -0700
Subject: [R] Grasper model error
Message-ID: <0F5B79FD-960B-495D-AAD5-0F5B2231DF1D@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/748b11e4/attachment.pl 

From Greg.Snow at intermountainmail.org  Tue Aug 15 20:30:47 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 15 Aug 2006 12:30:47 -0600
Subject: [R] rexp question
Message-ID: <07E228A5BE53C24CAD490193A7381BBB53DF52@LP-EXCHVS07.CO.IHC.COM>

Try:

> rexp(n=200*length(r), rate=rep(r, each=200) )

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Jones
Sent: Tuesday, August 15, 2006 12:15 PM
To: R-Help
Subject: [R] rexp question

I am using rexp to generate several exponential distributions. I am
passing rexp a vector of rates , r. I am wanting to simulate a sample of
size 200 for each rate so the code looks like:
rexp(n=200*length(r),rate=r) this gives me a vector of the random
exponential variables, but they are all disjointed b/c rexp goes through
and simulates an exponential variable for each rate and it does that 200
times, is there any way to get it to do it the oppisite way,i.e., 200
times in a row for each rate. I have already tried using a for loop but
it is slow.


thanks,

Spencer

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dieter.menne at menne-biomed.de  Tue Aug 15 20:52:36 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 15 Aug 2006 18:52:36 +0000 (UTC)
Subject: [R] nls convergence problem
References: <ebsuie$8t5$1@sea.gmane.org>
Message-ID: <loom.20060815T205133-5@post.gmane.org>

Earl F. Glynn <efg <at> stowers-institute.org> writes:

> 
> Here's my toy problem:
> > ?nls.control
> > ?nls
> > # Method 2
> > X <- 0:15
> > Y <- 9.452 * exp(-0.109*X) + 5.111   # Toy problem
> >
> > nls.out <- nls(Y ~ a*exp(b*X)+c,
> +                start=list(a=6,b=-0.5,c=1),
> +                control=nls.control(maxiter=15, tol=0.01),  # nothing makes 
> sense here
> +                trace=TRUE)

This toy problem is exactly what the warning is for:

Warning
Do not use nls on artificial "zero-residual" data. 

Add some noise and try again.

Dieter Menne


From dsohal at gmail.com  Tue Aug 15 20:55:55 2006
From: dsohal at gmail.com (Davendra Sohal)
Date: Tue, 15 Aug 2006 14:55:55 -0400
Subject: [R] Hierarchical clustering
Message-ID: <c2f237040608151155p6a917dc8x87f67576bf0e5901@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/15c69998/attachment.pl 

From muster at gmail.com  Tue Aug 15 21:29:59 2006
From: muster at gmail.com (T Mu)
Date: Tue, 15 Aug 2006 15:29:59 -0400
Subject: [R] "model = F" causing error in polr()
Message-ID: <b68812e70608151229n3808a26bl7962ab33a4c2d9b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/efced70b/attachment.pl 

From rolf at erdos.math.unb.ca  Tue Aug 15 22:09:56 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Tue, 15 Aug 2006 17:09:56 -0300 (ADT)
Subject: [R] "model = F" causing error in polr()
Message-ID: <200608152009.k7FK9uYQ022972@erdos.math.unb.ca>


Try ``model = FALSE'' rather than ``model = F'' and see if it makes a
difference.  You make have an unwanted variable named ``F'' lurking
somewhere.

(In general it is a *bad* idea to use ``F'' when ``FALSE'' is
intended.)

			cheers,

				Rolf Turner
				rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
Original message:

> Hi all,
> 
> I got an error message if I set model =F in polr(), like
> 
> > polr(y ~ x1 + x2, data1, model = F, method = "probit")
> 
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>         variable lengths differ (found for '(model)')
> but
> 
> > polr(y ~ x1 + x2, data1, method = "probit")
> 
> would work.
> 
> Why? Thank you,
> 
> Tian


From efg at stowers-institute.org  Tue Aug 15 22:45:38 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 15 Aug 2006 15:45:38 -0500
Subject: [R] nls convergence problem
References: <ebsuie$8t5$1@sea.gmane.org>
	<loom.20060815T205133-5@post.gmane.org>
Message-ID: <ebtbpe$nan$1@sea.gmane.org>

"Dieter Menne" <dieter.menne at menne-biomed.de> wrote in message 
news:loom.20060815T205133-5 at post.gmane.org...
> Earl F. Glynn <efg <at> stowers-institute.org> writes:
> This toy problem is exactly what the warning is for:
>
> Warning
> Do not use nls on artificial "zero-residual" data.
>
> Add some noise and try again.

Thank you!

I had adapted some code and must confess I had read ?nls.control thoroughly, 
but not ?nls. I had even used debug on nls, traced it through line by line 
to the .Call statement, trying to figure out why nls.out never got defined. 
The source code has no comments at all.

IMHO, the warning should be in the "Description" at the top of the ?nls 
page, not at the bottom of the page. The warning should also appear on the 
?nls.control page. But, a better way would be to have a software design that 
eliminated the warning.

It's not clear to me why this problem cannot be "fixed" somehow. You 
shouldn't need to add noise to a problem to solve it. (It's a bit like 
saying addition works, but not for integers without adding some noise.) If 
there can be arbitrary defaults of maxiter=50, and (relative) tol=1e-5 in 
nls.control, there could be another arbitrary (absolute) convergence 
criterion.  Or, maybe there's something I don't understand about the 
algorithm being used.

Just my $0.02 and minority opinion,
efg


From dsohal at gmail.com  Tue Aug 15 22:47:49 2006
From: dsohal at gmail.com (Davendra Sohal)
Date: Tue, 15 Aug 2006 16:47:49 -0400
Subject: [R] Hierarchical clustering
Message-ID: <c2f237040608151347p72149950x9d4c1728d6bf16bf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/85f0f7f1/attachment.pl 

From gunter.berton at gene.com  Tue Aug 15 23:08:42 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 15 Aug 2006 14:08:42 -0700
Subject: [R] nls convergence problem
In-Reply-To: <ebtbpe$nan$1@sea.gmane.org>
Message-ID: <007c01c6c0ae$fd260e50$711f210a@gne.windows.gene.com>

>   Or, maybe there's something I don't understand about the 
> algorithm being used.

Indeed! So before making such comments, why don't you try to learn about it?
Doug Bates is a pretty smart guy,  and I think you do him a disservice when
you assume that he somehow overlooked something that he explicitly warned
you about. I am fairly confident that if he could have made the problem go
away, he would have. So I think your vent was a bit inconsiderate and
perhaps even intemperate. The R Core folks have produced a minor miracle
IMO, and we should all be careful before assuming that they have overlooked
easily fixable problems. They're certainly not infallible -- but they're a
lot less fallible than most of the rest of us when it comes to R.

> 
> Just my $0.02 and minority opinion,
> efg
> 

... and mine.

-- Bert Gunter


From A.Robinson at ms.unimelb.edu.au  Tue Aug 15 23:39:57 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 16 Aug 2006 07:39:57 +1000
Subject: [R] A model for possibly periodic data with varying amplitude
	[repost, much edited]
Message-ID: <20060815213957.GU95817@ms.unimelb.edu.au>

Hi dear R community,

I have up to 12 measures of a protein for each of 6 patients, taken
every two or three days. The pattern of the protein looks periodic,
but the height of the peaks is highly variable.  I'm testing for
periodicity using a Monte Carlo simulation envelope approach applied
to a cumulative periodogram.  Now I want to predict the location of
the peaks in time.  Of course, the peaks might be occurring on
unmeasured days.

Sadly, an NDA prohibits me from sharing the real data. The data look
something like this:

##################################################################

patient <- data.frame(
	day = c(1, 3, 5, 8, 10, 12, 15, 17, 19, 22, 24, 26),
	protein = c(5, 3, 10, 7, 2, 8, 25, 22, 7, 10, 12, 5)
	)

plot(patient$day, patient$protein, type="b")

# This is my model:

wave.form <-
  deriv3( ~ sin(2*pi*((day-offset)/period + 0.25)) * amplitude + mean,
         c("period", "offset", "amplitude", "mean"),
         function(day, period, offset, amplitude, mean){})

curve(wave.form(x, period=7, offset=2, mean=5, amplitude=4), 
			        from=1, to=30)

require(nlme)

wave.1 <- gnls(protein ~ wave.form(day, period, offset, amplitude, mean),
               start=list(period=7, offset=0, amplitude=10, mean=5),
               weights=varPower(), data=patient)

wave.1

#################################################################

I emphasize that I don't care about the mean or amplitude, just the
offset and the period. The problem for my model is that spikes in the
data, such as at day 15, seem to make fitting the model quite
unstable, (although it is fine in this artificial case).  The best I
can think of right now is to use the "weights=varPower()" model in
gnls(), which I expect will down-weight the extreme spikes.  Often
that model fails to converge.  Then, all I can do is fall back on
modelling the log response.  If I do both, then when the weights model
converges the estimates sometimes differ, and sometimes considerably.

I know that I need more data, but more data are not available.  So,
instead I need to do the best I can :)

I have a half-baked idea of conditionally shrinking the peaks, so that
they all look like the same size - perhaps by discretizing the data
into, say, three possible values defined by quantiles of the response
variable or the estimated slope - but I'm concerned that doing so
risks creating the periodicity that I want to detect.  Also the series
is not necessarily stationary, so I wouild have to smooth it somehow
first.

On the other hand, maybe I can test for periodicity in the original
data, and then fit on some transformed data.  If I reject the null
hypothesis of no periodic oscillation, then is it reasonable to
condition subsequent analysis on the belief that periodicity exists,
and try to determine its characteristics?

I wonder if anyone can suggest an analysis technique or model that
more directly accommodates the varying heights of the peaks?  Or is
this pretty much the best I can do?

Any suggestions will be gratefully received.

Cheers,

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From efg at stowers-institute.org  Tue Aug 15 23:50:18 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 15 Aug 2006 16:50:18 -0500
Subject: [R] nls convergence problem
References: <ebtbpe$nan$1@sea.gmane.org>
	<007c01c6c0ae$fd260e50$711f210a@gne.windows.gene.com>
Message-ID: <ebtfil$3n9$1@sea.gmane.org>

"Berton Gunter" <gunter.berton at gene.com> wrote in message 
news:007c01c6c0ae$fd260e50$711f210a at gne.windows.gene.com...
>>   Or, maybe there's something I don't understand about the
>> algorithm being used.
>
> Indeed! So before making such comments, why don't you try to learn about 
> it?
> Doug Bates is a pretty smart guy,  and I think you do him a disservice 
> when
> you assume that he somehow overlooked something that he explicitly warned
> you about. I am fairly confident that if he could have made the problem go
> away, he would have. So I think your vent was a bit inconsiderate and
> perhaps even intemperate. The R Core folks have produced a minor miracle
> IMO, and we should all be careful before assuming that they have 
> overlooked
> easily fixable problems. They're certainly not infallible -- but they're a
> lot less fallible than most of the rest of us when it comes to R.

I meant no disrespect to Doug Bates or any of the R Core folks. I thought 
what I wrote had a "neutral" tone and was respectful.  I am sorry if anyone 
was offended by my comments and suggestions.  I am certainly thankful for 
all the hard work that has gone into developing R.

efg


From muster at gmail.com  Wed Aug 16 01:14:46 2006
From: muster at gmail.com (T Mu)
Date: Tue, 15 Aug 2006 19:14:46 -0400
Subject: [R] coefficients' order in polr()?
Message-ID: <b68812e70608151614k4e658756td46ee3fc511cbcef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060815/ebc0a41c/attachment.pl 

From f.harrell at vanderbilt.edu  Wed Aug 16 04:37:15 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 15 Aug 2006 21:37:15 -0500
Subject: [R] Looking for info on the "Regression Modeling Strategies in
 R"	course in DC area
In-Reply-To: <20060815153350.1632.qmail@web57205.mail.re3.yahoo.com>
References: <20060815153350.1632.qmail@web57205.mail.re3.yahoo.com>
Message-ID: <44E284DB.3080101@vanderbilt.edu>

Tim McDonald wrote:
> Hello list,
>    
>   A colleague of mine mentioned a great course on  "Regression Modeling Strategies in R". Anyone knows if this course is offered as public course in DC area?
>    
>   Thanks a bunch - TM

I'll be teaching this course in DC Sept 28-29 for XL Solutions. 
Information about the course may be obtained from 
biostat.mc.vanderbilt.edu/rms

Frank Harrell

> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ronggui.huang at gmail.com  Wed Aug 16 04:48:11 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 16 Aug 2006 10:48:11 +0800
Subject: [R] coefficients' order in polr()?
In-Reply-To: <b68812e70608151614k4e658756td46ee3fc511cbcef@mail.gmail.com>
References: <b68812e70608151614k4e658756td46ee3fc511cbcef@mail.gmail.com>
Message-ID: <38b9f0350608151948q3aa202d7sb1a857f3dc59abf8@mail.gmail.com>

you can use _relevel_ to Reorder Levels of Factor.
use ?relevel to get more information.

2006/8/16, T Mu <muster at gmail.com>:
> Hi all,
>
> I am using polr(). The resulting coefficients of first levels are always 0.
>
> What to do if I wnat to get the coefficients of the last level 0.
>
> For example, suppose x has 3 levels, 1, 2, 3
>
> probit <- plor(y ~ x, data1, method='probit')
>
> will get coefficients of level 2, 3 of x, but I want coefficients of level
> 1, 2
>
> Thank you,
> Tian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
??????
Department of Sociology
Fudan University


From rab at nauticom.net  Wed Aug 16 06:50:29 2006
From: rab at nauticom.net (Rick Bilonick)
Date: Wed, 16 Aug 2006 00:50:29 -0400
Subject: [R] Specifying Path Model in SEM for CFA
Message-ID: <1155703829.3887.9.camel@localhost.localdomain>

I'm using specify.model for the sem package. I can't figure out how to
represent the residual errors for the observed variables for a CFA
model. (Once I get this working I need to add some further constraints.)

Here is what I've tried:

model.sa <- specify.model()
  F1	 -> X1,l11, NA
  F1	 -> X2,l21, NA
  F1	 -> X3,l31, NA
  F1	 -> X4,l41, NA
  F1	 -> X5, NA, 0.20
  F2	 -> X1,l12, NA
  F2	 -> X2,l22, NA
  F2	 -> X3,l32, NA
  F2	 -> X4,l42, NA
  F2	 -> X6, NA, 0.25
  F1	<-> F2,g12, 1
  F1    <-> F1,g11, 1
  F2    <-> F2,g22, 1
  X1	<-> X1, NA, 1
  X2	<-> X2, NA, 1
  X3	<-> X3, NA, 1
  X4	<-> X4, NA, 1
  X5	<-> X5, NA, 1
  X6	<-> X6, NA, 1

This at least converges:

> summary(fit.sem)

 Model Chisquare =  2147   Df =  10 Pr(>Chisq) = 0
 Chisquare (null model) =  2934   Df =  15
 Goodness-of-fit index =  0.4822
 Adjusted goodness-of-fit index =  -0.087387
 RMSEA index =  0.66107   90 % CI: (NA, NA)
 Bentler-Bonnett NFI =  0.26823
 Tucker-Lewis NNFI =  -0.098156
 Bentler CFI =  0.26790
 BIC =  2085.1

 Normalized Residuals
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 -5.990  -0.618   0.192   0.165   1.700   3.950

 Parameter Estimates
    Estimate  Std Error z value  Pr(>|z|)
l11 -0.245981 0.21863   -1.12510 0.26054748 X1 <--- F1
l21 -0.308249 0.22573   -1.36555 0.17207875 X2 <--- F1
l31  0.202590 0.07910    2.56118 0.01043175 X3 <--- F1
l41 -0.235156 0.21980   -1.06985 0.28468885 X4 <--- F1
l12  0.839985 0.21962    3.82476 0.00013090 X1 <--- F2
l22  0.828460 0.22548    3.67418 0.00023862 X2 <--- F2
l32  0.066722 0.08369    0.79725 0.42530606 X3 <--- F2
l42  0.832037 0.21840    3.80963 0.00013917 X4 <--- F2
g12  0.936719 0.64331    1.45609 0.14536647 F2 <--> F1
g11  2.567669 1.25608    2.04418 0.04093528 F1 <--> F1
g22  1.208497 0.55040    2.19567 0.02811527 F2 <--> F2

 Iterations =  59

And it produces the following path diagram:

> path.diagram(fit.sem)
digraph "fit.sem" {
  rankdir=LR;
  size="8,8";
  node [fontname="Helvetica" fontsize=14 shape=box];
  edge [fontname="Helvetica" fontsize=10];
  center=1;
  "F2" [shape=ellipse]
  "F1" [shape=ellipse]
  "F1" -> "X1" [label="l11"];
  "F1" -> "X2" [label="l21"];
  "F1" -> "X3" [label="l31"];
  "F1" -> "X4" [label="l41"];
  "F1" -> "X5" [label=""];
  "F2" -> "X1" [label="l12"];
  "F2" -> "X2" [label="l22"];
  "F2" -> "X3" [label="l32"];
  "F2" -> "X4" [label="l42"];
  "F2" -> "X6" [label=""];
}

But I don't see the residual error terms that go into each of the
observed variables X1 - X6. I've tried:

model.sa <- specify.model()
  E1	 -> X1, e1,  1
  E2	 -> X2, e2,  1
  E3	 -> X3, e3,  1
  E4	 -> X4, e4,  1
  E5	 -> X5, e5,  1
  E6	 -> X6, e6,  1
  E1	<-> E1, s1, NA
  E2	<-> E2, s2, NA
  E3	<-> E3, s3, NA
  E4	<-> E4, s4, NA
  E5	<-> E5, s5, NA
  E6	<-> E6, s6, NA
  F1	 -> X1,l11, NA
  F1	 -> X2,l21, NA
  F1	 -> X3,l31, NA
  F1	 -> X4,l41, NA
  F1	 -> X5, NA,  1
  F2	 -> X1,l12, NA
  F2	 -> X2,l22, NA
  F2	 -> X3,l32, NA
  F2	 -> X4,l42, NA
  F2	 -> X6, NA,  1
  F1	<-> F2, NA, 1
  F1    <-> F1, NA, 1
  F2    <-> F2,g22, NA
  X1	<-> X1, NA, 1
  X2	<-> X2, NA, 1
  X3	<-> X3, NA, 1
  X4	<-> X4, NA, 1
  X5	<-> X5, NA, 1
  X6	<-> X6, NA, 1

I'm trying to use E1 - E6 as the residual error terms. But I get warning
messages about no variances for X1-X6 and it doesn't converge. Also, the
associated path diagram:

digraph "fit.sem" {
  rankdir=LR;
  size="8,8";
  node [fontname="Helvetica" fontsize=14 shape=box];
  edge [fontname="Helvetica" fontsize=10];
  center=1;
  "E1" [shape=ellipse]
  "E2" [shape=ellipse]
  "E3" [shape=ellipse]
  "E4" [shape=ellipse]
  "E5" [shape=ellipse]
  "E6" [shape=ellipse]
  "F2" [shape=ellipse]
  "F1" [shape=ellipse]
  "E1" -> "X1" [label=""];
  "E2" -> "X2" [label=""];
  "E3" -> "X3" [label=""];
  "E4" -> "X4" [label=""];
  "E5" -> "X5" [label=""];
  "E6" -> "X6" [label=""];
  "F1" -> "X1" [label="l11"];
  "F1" -> "X2" [label="l21"];
  "F1" -> "X3" [label="l31"];
  "F1" -> "X4" [label="l41"];
  "F1" -> "X5" [label=""];
  "F2" -> "X1" [label="l12"];
  "F2" -> "X2" [label="l22"];
  "F2" -> "X3" [label="l32"];
  "F2" -> "X4" [label="l42"];
  "F2" -> "X6" [label=""];
}

Has ellipses around the E1-E6 which I believe indicates they are latent
factors and not residual errors.

If anyone could point in the right direction I would appreciate it.

Rick B.


From dieter.menne at menne-biomed.de  Wed Aug 16 08:36:19 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 16 Aug 2006 06:36:19 +0000 (UTC)
Subject: [R] nls convergence problem
References: <ebsuie$8t5$1@sea.gmane.org>
	<loom.20060815T205133-5@post.gmane.org>
	<ebtbpe$nan$1@sea.gmane.org>
Message-ID: <loom.20060816T083330-951@post.gmane.org>

Earl F. Glynn <efg <at> stowers-institute.org> writes:

> 
> It's not clear to me why this problem cannot be "fixed" somehow. You 

You might try optim instead of nls, which always (well, as far I used it)
converges. However, resulting coefficients may be totally off, and you should
use profiling to check the reliability. 

Dieter


From dieter.menne at menne-biomed.de  Wed Aug 16 08:43:02 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 16 Aug 2006 06:43:02 +0000 (UTC)
Subject: [R]
	=?utf-8?q?A_model_for_possibly_periodic_data_with_varying_amp?=
	=?utf-8?q?litude=09=5Brepost=2C_much_edited=5D?=
References: <20060815213957.GU95817@ms.unimelb.edu.au>
Message-ID: <loom.20060816T083956-329@post.gmane.org>

Andrew Robinson <A.Robinson <at> ms.unimelb.edu.au> writes:

> I have up to 12 measures of a protein for each of 6 patients, taken
> every two or three days. The pattern of the protein looks periodic,
> but the height of the peaks is highly variable.  I'm testing for
> periodicity using a Monte Carlo simulation envelope approach applied
> to a cumulative periodogram.  Now I want to predict the location of
> the peaks in time.  Of course, the peaks might be occurring on
> unmeasured days.

Have you checked one of the methods in Chapter 13. of MASS to obtain a smoothed
estimate?

Dieter


From dieter.menne at menne-biomed.de  Wed Aug 16 08:50:33 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 16 Aug 2006 06:50:33 +0000 (UTC)
Subject: [R] Presentation of multiple models in one table using xtable
References: <20060814130827.GA9396@lubyanka.local>
Message-ID: <loom.20060816T084643-135@post.gmane.org>

Ajay Narottam Shah <ajayshah <at> mayin.org> writes:

> 
> Consider this situation:
> > x1 <- runif(100); x2 <- runif(100); y <- 2 + 3*x1 - 4*x2 + rnorm(100)
> > m1 <- summary(lm(y ~ x1))
> > m2 <- summary(lm(y ~ x2))
> > m3 <- summary(lm(y ~ x1 + x2))
> 
> Now you have estimated 3 different "competing" models, and suppose you
> want to present the set of models in one table. xtable(m1) is cool,
> but doing that thrice would give us 3 different tables.
> 
> What I want is this one table:
> 
> -----------------------------------------------------------
>                     M1             M2              M3
> -----------------------------------------------------------
> Intercept         0.0816         3.6292         2.2272
>                  (0.5533)       (0.2316)***    (0.2385)***
...... (my gmane newreader complains when I quote too much, and contrary to the
general believe on this list, I think he is right).

There is no standard way of doing this, so the first formatting must be done
manually. For a nice output, check the R2HTML packages and the latex() derivates
in Frank Harrell's Hmisc package.


Dieter


From stgries_lists at arcor.de  Wed Aug 16 09:17:36 2006
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Wed, 16 Aug 2006 09:17:36 +0200 (CEST)
Subject: [R] Regular expressions: retrieving matches depending on
 intervening strings
Message-ID: <33127503.1155712656185.JavaMail.ngmail@webmail14>

Dear all

I again have a regular expression question. I have this character vector a:

a<-c("<w AT0>a <w NN1>blockage <w CJC>and <w DT0>that<c PUN>.",
     "<w AT0>a <w NN1>blockage <w CJC>and <ptr target=KB2LC003><w DT0>that<c PUN>.",
     "<w AT0>a <w NN1>blockage <w CJC>and<c PUN>, <w DT0>that<c PUN>.",
     "<w AT0>a <w NN1>blockage <w CJC>and <w AJ0>hungry <w DT0>that<c PUN>.")

I would like to retrieve those elements of a in which "<w CJC>" and "<w DT0>" are

- directly adjacent, as in a[1] or
- not interrupted by "<[wc] ", as in a[2]

And, of these elements I would like to consume all characters from the "<" in "<w CJC" to the last character after "<w DT0>" that is not a "<". For example, if I was only searching a[1], I would like something like this:

matches<-gregexpr("<w CJC>[^<]+?<w DT0>[^<]+", a[1], perl=TRUE)
substr(a[1], unlist(matches), unlist(matches)+unlist(attributes(matches[[1]], "match.length"))-1)

I have been fiddling around with negative lookahead but I really can't get my head around this. Any pointers would be greatly appreciated. Thanks a lot,
STG
--
Stefan Th. Gries
-----------------------------------------------
University of California, Santa Barbara
http://www.linguistics.ucsb.edu/faculty/stgries


From attenka at utu.fi  Wed Aug 16 09:42:35 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 16 Aug 2006 10:42:35 +0300
Subject: [R] How to remove similar successive objects from a vector?
Message-ID: <1155714155.13987.31.camel@attenka>

Is there some (much) more efficient way to do this?

VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
NEWVECTOR=VECTOR[1];

for(i in 1:(length(VECTOR)-1))
{
	if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
		NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
}

> VECTOR
 [1] 3 2 4 5 5 3 3 5 1 6 6
> NEWVECTOR
[1] 3 2 4 5 3 5 1 6

_______________________________
Atte Tenkanen
University of Turku, Finland


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug 16 09:57:03 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 16 Aug 2006 09:57:03 +0200
Subject: [R] How to remove similar successive objects from a vector?
References: <1155714155.13987.31.camel@attenka>
Message-ID: <00d901c6c109$8fde3a10$0540210a@www.domain>

try something like the following:

x <- c(3,3,2,4,5,5,3,3,5,1,6,6)
#########
nx <- length(x)
ind <- c(TRUE, (x[1:(nx-1)] - x[2:nx]) != 0)
x[ind]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Atte Tenkanen" <attenka at utu.fi>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 16, 2006 9:42 AM
Subject: [R] How to remove similar successive objects from a vector?


> Is there some (much) more efficient way to do this?
>
> VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
> NEWVECTOR=VECTOR[1];
>
> for(i in 1:(length(VECTOR)-1))
> {
> if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
> NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
> }
>
>> VECTOR
> [1] 3 2 4 5 5 3 3 5 1 6 6
>> NEWVECTOR
> [1] 3 2 4 5 3 5 1 6
>
> _______________________________
> Atte Tenkanen
> University of Turku, Finland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jacques.veslot at good.ibl.fr  Wed Aug 16 10:01:04 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 16 Aug 2006 10:01:04 +0200
Subject: [R] How to remove similar successive objects from a vector?
In-Reply-To: <1155714155.13987.31.camel@attenka>
References: <1155714155.13987.31.camel@attenka>
Message-ID: <44E2D0C0.1020308@good.ibl.fr>

VECTOR=c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6)
NEWVECTOR <- ifelse(VECTOR[-length(VECTOR)]==VECTOR[-1],NA,VECTOR)
NEWVECTOR[!is.na(NEWVECTOR)]
[1] 3 2 3 4 5 3 5 1

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------

Atte Tenkanen a ?crit :
> Is there some (much) more efficient way to do this?
> 
> VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
> NEWVECTOR=VECTOR[1];
> 
> for(i in 1:(length(VECTOR)-1))
> {
> 	if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
> 		NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
> }
> 
> 
>>VECTOR
> 
>  [1] 3 2 4 5 5 3 3 5 1 6 6
> 
>>NEWVECTOR
> 
> [1] 3 2 4 5 3 5 1 6
> 
> _______________________________
> Atte Tenkanen
> University of Turku, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From javalkon at hytti.uku.fi  Wed Aug 16 10:05:03 2006
From: javalkon at hytti.uku.fi (Jarimatti Valkonen)
Date: Wed, 16 Aug 2006 11:05:03 +0300
Subject: [R] How to remove similar successive objects from a vector?
In-Reply-To: <1155714155.13987.31.camel@attenka>
References: <1155714155.13987.31.camel@attenka>
Message-ID: <20060816080503.GA73166@hytti.uku.fi>

On Wed, Aug 16, 2006 at 10:42:35AM +0300, Atte Tenkanen wrote:
> Is there some (much) more efficient way to do this?
> 
> VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
> NEWVECTOR=VECTOR[1];
> 
> for(i in 1:(length(VECTOR)-1))
> {
> 	if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
> 		NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
> }
> 
> > VECTOR
>  [1] 3 2 4 5 5 3 3 5 1 6 6
> > NEWVECTOR
> [1] 3 2 4 5 3 5 1 6

How about rle? rle(VECTOR)$values should do the same thing. Don't know
about efficiency, though.

-- 
Jarimatti Valkonen


From gavin.simpson at ucl.ac.uk  Wed Aug 16 10:05:48 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 16 Aug 2006 09:05:48 +0100
Subject: [R] How to remove similar successive objects from a vector?
In-Reply-To: <1155714155.13987.31.camel@attenka>
References: <1155714155.13987.31.camel@attenka>
Message-ID: <1155715548.2851.8.camel@dhcppc2.my.nat.localnet>

On Wed, 2006-08-16 at 10:42 +0300, Atte Tenkanen wrote:
> Is there some (much) more efficient way to do this?
> 
> VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
> NEWVECTOR=VECTOR[1];
> 
> for(i in 1:(length(VECTOR)-1))
> {
> 	if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
> 		NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
> }
> 
> > VECTOR
>  [1] 3 2 4 5 5 3 3 5 1 6 6
> > NEWVECTOR
> [1] 3 2 4 5 3 5 1 6
> 
> _______________________________
> Atte Tenkanen

Is this what you mean?

x <- c(3, 2, 4, 5, 5, 3, 3, 5, 1, 6, 6)
x.wanted <- c(3, 2, 4, 5, 3, 5, 1, 6)

X <- x[diff(x) != 0]

all.equal(x.wanted, X) # check it works

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From s.wood at bath.ac.uk  Wed Aug 16 11:07:38 2006
From: s.wood at bath.ac.uk (Simon Wood)
Date: Wed, 16 Aug 2006 10:07:38 +0100
Subject: [R] Grasper model error
In-Reply-To: <0F5B79FD-960B-495D-AAD5-0F5B2231DF1D@mac.com>
References: <0F5B79FD-960B-495D-AAD5-0F5B2231DF1D@mac.com>
Message-ID: <200608161007.38145.s.wood@bath.ac.uk>

I'm not too familiar with using grasp, but the error is generated from the 
`mgcv' package while setting up a GAM. The default smooths used for `s' terms 
by mgcv::gam are thin plate regression splines based on an `eigen 
approximation' to full thin plate splines. These have some nice properties 
but become expensive to set up for data sets with more than a few thousand 
points, in which case it is better to use a different basis for smooths of 
one variable, e.g.
s(x,bs="cr")
or for smooths of several variables use either tensor product smooths, e.g.
te(x,z)
or use the `knots' argument to `gam' in the way shown at the end of the 
examples in the `gam' help file.

Simon

On Tuesday 15 August 2006 19:29, Ken Nussear wrote:
> I tried this over a the grasp users yahoo group and got no
> response....So I wonder if anyone here knows about grasper
>
> I keep getting this error when trying to run a model.
>
>
> Error in smooth.construct.tp.smooth.spec(object, data, knots) :
> Too many knots for t.p.r.s term: see `gam.control' to increase limit,
> or use a
> different
> basis, or see large data set help for `gam'.
>
>
> I'm using R for OSX Gui Version 1.16 R-version 2.3.1, running grasper
> version
> 0.4-4
>
>
> My dataset has ~7500 rows of input, and 21 cols of input.
>
> Does anyone have an idea how to troubleshoot this?
>
> Thanks
>
>
> Ken
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From stgries_lists at arcor.de  Wed Aug 16 11:12:34 2006
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Wed, 16 Aug 2006 11:12:34 +0200 (CEST)
Subject: [R] Regular expressions: retrieving matches depending on
 intervening strings [Follow-up]
Message-ID: <683116.1155719554168.JavaMail.ngmail@webmail18>

Dear all

This is a follow-up to an earlier posting today regarding a regular expression question. In the meantime, this is the best approximation I could come up with and should give you a better idea what I am talking about.

a<-c("<w AT0>a <w NN1>blockage <w CJC>and <w DT0>that<c PUN>.",
     "<w AT0>a <w NN1>blockage <w CJC>and <ptr target=KB2LC003><w DT0>that<c PUN>.",
     "<w AT0>a <w NN1>blockage <w CJC>and<c PUN>, <w DT0>that<c PUN>.",
     "<w AT0>a <w NN1>blockage <w CJC>and <w AJ0>hungry <w DT0>that<c PUN>.")
matches<-gregexpr("<w CJC>[^<]+(?:<[^wc].*?>.*?)*<w DT0>that", a, perl=TRUE)
starts<-unlist(matches)
lengths<-unlist(sapply(matches, attributes))
stops<-starts+lengths-1
substr(a, starts, stops)

What is still missing is that the disallowed string is not just "<[wc]" but "<[wc] " and I don't know how to do that. Any ideas (preferably with lookarounds)?
Thanks a bunch,
STG
--
Stefan Th. Gries
-----------------------------------------------
University of California, Santa Barbara
http://www.linguistics.ucsb.edu/faculty/stgries
-----------------------------------------------


ORIGINAL MESSAGE
> Dear all
>
> I again have a regular expression question. I have this character vector a:
>
> a<-c("<w AT0>a <w NN1>blockage <w CJC>and <w DT0>that<c PUN>.",
> "<w AT0>a <w NN1>blockage <w CJC>and <ptr target=KB2LC003><w DT0>that<c PUN>.",
> "<w AT0>a <w NN1>blockage <w CJC>and<c PUN>, <w DT0>that<c PUN>.",
> "<w AT0>a <w NN1>blockage <w CJC>and <w AJ0>hungry <w DT0>that<c PUN>.")
>
> I would like to retrieve those elements of a in which "<w CJC>" and "<w DT0>" are
>
> - directly adjacent, as in a[1] or
> - not interrupted by "<[wc] ", as in a[2]
>
> And, of these elements I would like to consume all characters from the "<" in "<w CJC" to the last character after "<w DT0>" that is not a "<". For example, if I was only searching a[1], I would like something like this:
>
> matches<-gregexpr("<w CJC>[^<]+?<w DT0>[^<]+", a[1], perl=TRUE)
> substr(a[1], unlist(matches), unlist(matches)+unlist(attributes(matches[[1]], "match.length"))-1)
>
> I have been fiddling around with negative lookahead but I really can't get my head around this. Any pointers would be greatly appreciated. Thanks a lot,
> STG


From A.Robinson at ms.unimelb.edu.au  Wed Aug 16 11:14:51 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 16 Aug 2006 19:14:51 +1000
Subject: [R] A model for possibly periodic data with varying
	amplitude	[repost, much edited]
In-Reply-To: <loom.20060816T083956-329@post.gmane.org>
References: <20060815213957.GU95817@ms.unimelb.edu.au>
	<loom.20060816T083956-329@post.gmane.org>
Message-ID: <20060816091451.GN95817@ms.unimelb.edu.au>

Thanks for your response.

No, I haven't - I just looked and didn't see anything that looked
suitable for 12 data points.  Can you be more precise?

Cheers

Andrew

On Wed, Aug 16, 2006 at 06:43:02AM +0000, Dieter Menne wrote:
> Andrew Robinson <A.Robinson <at> ms.unimelb.edu.au> writes:
> 
> > I have up to 12 measures of a protein for each of 6 patients, taken
> > every two or three days. The pattern of the protein looks periodic,
> > but the height of the peaks is highly variable.  I'm testing for
> > periodicity using a Monte Carlo simulation envelope approach applied
> > to a cumulative periodogram.  Now I want to predict the location of
> > the peaks in time.  Of course, the peaks might be occurring on
> > unmeasured days.
> 
> Have you checked one of the methods in Chapter 13. of MASS to obtain a smoothed
> estimate?
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From attenka at utu.fi  Wed Aug 16 12:01:33 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 16 Aug 2006 13:01:33 +0300
Subject: [R] How to remove similar successive objects from a vector?
In-Reply-To: <44E2D031.8060106@pburns.seanet.com>
References: <1155714155.13987.31.camel@attenka>
	<44E2D031.8060106@pburns.seanet.com>
Message-ID: <1155722493.13987.43.camel@attenka>

Thanks for all respondents!

I wasn't precise enough, when I enclosed my example. In fact, I need a
version which works with all kinds of symbolic data, not only with
numbers. So these versions

rle(VECTOR)$values

and

VECTOR=c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6)
NEWVECTOR <- ifelse(VECTOR[-length(VECTOR)]==VECTOR[-1],NA,VECTOR)
NEWVECTOR[!is.na(NEWVECTOR)]

answered to my needs.

I made a test and the first version was 2.5x faster with my data, but
both works enough fast.

Atte

On Wed, 2006-08-16 at 08:58 +0100, Patrick Burns wrote:
> I think
> 
> rle(VECTOR)$values
> 
> will get you what you want.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Atte Tenkanen wrote:
> 
> >Is there some (much) more efficient way to do this?
> >
> >VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
> >NEWVECTOR=VECTOR[1];
> >
> >for(i in 1:(length(VECTOR)-1))
> >{
> >	if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
> >		NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
> >}
> >
> >  
> >
> >>VECTOR
> >>    
> >>
> > [1] 3 2 4 5 5 3 3 5 1 6 6
> >  
> >
> >>NEWVECTOR
> >>    
> >>
> >[1] 3 2 4 5 3 5 1 6
> >
> >_______________________________
> >Atte Tenkanen
> >University of Turku, Finland
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >  
> >


From S.Pickett at exeter.ac.uk  Wed Aug 16 13:14:11 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Wed, 16 Aug 2006 12:14:11 +0100 (BST)
Subject: [R] REML with random slopes and random intercepts giving
 strange results
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E276F90@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E276F90@dc1ex01.air.org>
Message-ID: <1132.144.173.76.117.1155726851.squirrel@www.webmail.ex.ac.uk>

Hi again,
Even stranger is the fact that the coefficeints (the slope) and the
intercepts are not independent, in fact they are directly inversely
proportional (r squared = 1).
This means that that there isnt a random slope and intercept for each
individual (which is what I wanted), but straight line that pivots in the
middle and will change from individual to individual. Is there a problem
with the way I have structured the random model or a deeper problem with
lmer()?
here is the code I used
m2 <- lmer(changewt ~ newwt+(newwt|id), data = grow)
coef(m2)
Any suggestions very much appreciated,
Simon


> I don't this is because you are using REML. The BLUPs from a mixed model
> experience some shrinkage whereas the OLS estimates would not.
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simon Pickett
>> Sent: Tuesday, August 15, 2006 11:34 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] REML with random slopes and random intercepts
>> giving strange results
>>
>> Hi everyone,
>> I have been using REML to derive intercepts and coeficients
>> for each individual in a growth study. So the code is
>> m2 <- lmer(change.wt ~ newwt+(newwt|id), data = grow)
>>
>> Calling coef(model.lmer) gives a matrix with this information
>> which is what I want. However, as a test I looked at each
>> individual on its own and used a simple linear regression to
>> obtain the same information, then I compared the results. It
>> looks like the REML method doesnt seem to approximate the two
>> parameters as well as using the simple linear regression on
>> each individual separately, as judged by looking at graphs.
>> Indeed, why do the results differ at all?
>> Excuse my naivety if this is a silly question.
>> Thanks to everyone for replying to my previous questions,
>> very much appreciated.
>> Simon Pickett
>> PhD student
>> Centre For Ecology and Conservation
>> Tremough Campus
>> University of Exeter in Cornwall
>> TR109EZ
>> Tel 01326371852
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From gavin.simpson at ucl.ac.uk  Wed Aug 16 13:21:23 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 16 Aug 2006 12:21:23 +0100
Subject: [R] How to remove similar successive objects from a vector?
In-Reply-To: <1155722493.13987.43.camel@attenka>
References: <1155714155.13987.31.camel@attenka>
	<44E2D031.8060106@pburns.seanet.com>
	<1155722493.13987.43.camel@attenka>
Message-ID: <1155727283.13440.9.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2006-08-16 at 13:01 +0300, Atte Tenkanen wrote:
> Thanks for all respondents!
> 
> I wasn't precise enough, when I enclosed my example. In fact, I need a
> version which works with all kinds of symbolic data, not only with
> numbers. So these versions
> 
> rle(VECTOR)$values
> 
> and
> 
> VECTOR=c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6)
> NEWVECTOR <- ifelse(VECTOR[-length(VECTOR)]==VECTOR[-1],NA,VECTOR)
> NEWVECTOR[!is.na(NEWVECTOR)]

Note that the above is not giving the same answer as
rle(VECTOR)$values :

> VECTOR=c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6)
> NEWVECTOR <- ifelse(VECTOR[-length(VECTOR)]==VECTOR[-1],NA,VECTOR)
> NEWVECTOR[!is.na(NEWVECTOR)]
[1] 3 2 3 4 5 3 5 1
> rle(VECTOR)$values
[1] 3 2 3 4 5 3 5 1 6
> all.equal(NEWVECTOR[!is.na(NEWVECTOR)], rle(VECTOR)$values)
[1] "Numeric: lengths (8, 9) differ"

So make sure you use the rle solution.

G

> 
> answered to my needs.
> 
> I made a test and the first version was 2.5x faster with my data, but
> both works enough fast.
> 
> Atte
> 
> On Wed, 2006-08-16 at 08:58 +0100, Patrick Burns wrote:
> > I think
> > 
> > rle(VECTOR)$values
> > 
> > will get you what you want.
> > 
> > Patrick Burns
> > patrick at burns-stat.com
> > +44 (0)20 8525 0696
> > http://www.burns-stat.com
> > (home of S Poetry and "A Guide for the Unwilling S User")
> > 
> > Atte Tenkanen wrote:
> > 
> > >Is there some (much) more efficient way to do this?
> > >
> > >VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
> > >NEWVECTOR=VECTOR[1];
> > >
> > >for(i in 1:(length(VECTOR)-1))
> > >{
> > >	if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
> > >		NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
> > >}
> > >
> > >  
> > >
> > >>VECTOR
> > >>    
> > >>
> > > [1] 3 2 4 5 5 3 3 5 1 6 6
> > >  
> > >
> > >>NEWVECTOR
> > >>    
> > >>
> > >[1] 3 2 4 5 3 5 1 6
> > >
> > >_______________________________
> > >Atte Tenkanen
> > >University of Turku, Finland
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >  
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 UK. WC1E 6BT.                 [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From HDoran at air.org  Wed Aug 16 13:35:20 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 16 Aug 2006 07:35:20 -0400
Subject: [R] [SPAM] - RE: REML with random slopes and random intercepts
	giving strange results - Bayesian Filter detected spam
Message-ID: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>

Can you provide the summary(m2) results?

> -----Original Message-----
> From: Simon Pickett [mailto:S.Pickett at exeter.ac.uk] 
> Sent: Wednesday, August 16, 2006 7:14 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: [SPAM] - RE: [R] REML with random slopes and random 
> intercepts giving strange results - Bayesian Filter detected spam
> 
> Hi again,
> Even stranger is the fact that the coefficeints (the slope) 
> and the intercepts are not independent, in fact they are 
> directly inversely proportional (r squared = 1).
> This means that that there isnt a random slope and intercept 
> for each individual (which is what I wanted), but straight 
> line that pivots in the middle and will change from 
> individual to individual. Is there a problem with the way I 
> have structured the random model or a deeper problem with lmer()?
> here is the code I used
> m2 <- lmer(changewt ~ newwt+(newwt|id), data = grow)
> coef(m2)
> Any suggestions very much appreciated,
> Simon
> 
> 
> > I don't this is because you are using REML. The BLUPs from a mixed 
> > model experience some shrinkage whereas the OLS estimates would not.
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch 
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Simon Pickett
> >> Sent: Tuesday, August 15, 2006 11:34 AM
> >> To: r-help at stat.math.ethz.ch
> >> Subject: [R] REML with random slopes and random intercepts giving 
> >> strange results
> >>
> >> Hi everyone,
> >> I have been using REML to derive intercepts and 
> coeficients for each 
> >> individual in a growth study. So the code is
> >> m2 <- lmer(change.wt ~ newwt+(newwt|id), data = grow)
> >>
> >> Calling coef(model.lmer) gives a matrix with this 
> information which 
> >> is what I want. However, as a test I looked at each 
> individual on its 
> >> own and used a simple linear regression to obtain the same 
> >> information, then I compared the results. It looks like the REML 
> >> method doesnt seem to approximate the two parameters as 
> well as using 
> >> the simple linear regression on each individual 
> separately, as judged 
> >> by looking at graphs.
> >> Indeed, why do the results differ at all?
> >> Excuse my naivety if this is a silly question.
> >> Thanks to everyone for replying to my previous questions, 
> very much 
> >> appreciated.
> >> Simon Pickett
> >> PhD student
> >> Centre For Ecology and Conservation
> >> Tremough Campus
> >> University of Exeter in Cornwall
> >> TR109EZ
> >> Tel 01326371852
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
>


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug 16 14:09:48 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 16 Aug 2006 14:09:48 +0200
Subject: [R] How to remove similar successive objects from a vector?
References: <1155714155.13987.31.camel@attenka><44E2D031.8060106@pburns.seanet.com><1155722493.13987.43.camel@attenka>
	<1155727283.13440.9.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <007901c6c12c$def7ee70$0540210a@www.domain>


----- Original Message ----- 
From: "Gavin Simpson" <gavin.simpson at ucl.ac.uk>
To: <attenka at utu.fi>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 16, 2006 1:21 PM
Subject: Re: [R] How to remove similar successive objects from a 
vector?


> On Wed, 2006-08-16 at 13:01 +0300, Atte Tenkanen wrote:
>> Thanks for all respondents!
>>
>> I wasn't precise enough, when I enclosed my example. In fact, I 
>> need a
>> version which works with all kinds of symbolic data, not only with
>> numbers. So these versions
>>
>> rle(VECTOR)$values
>>
>> and
>>
>> VECTOR=c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6)
>> NEWVECTOR <- ifelse(VECTOR[-length(VECTOR)]==VECTOR[-1],NA,VECTOR)
>> NEWVECTOR[!is.na(NEWVECTOR)]
>
> Note that the above is not giving the same answer as
> rle(VECTOR)$values :
>
>> VECTOR=c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6)
>> NEWVECTOR <- ifelse(VECTOR[-length(VECTOR)]==VECTOR[-1],NA,VECTOR)
>> NEWVECTOR[!is.na(NEWVECTOR)]
> [1] 3 2 3 4 5 3 5 1
>> rle(VECTOR)$values
> [1] 3 2 3 4 5 3 5 1 6
>> all.equal(NEWVECTOR[!is.na(NEWVECTOR)], rle(VECTOR)$values)
> [1] "Numeric: lengths (8, 9) differ"
>
> So make sure you use the rle solution.
>
> G
>


interestingly, if speed matters, then the 2nd and 3rd solutions below 
seem slightly faster than rle():

> x <- rep(c(3,2,2,3,4,4,5,5,5,3,3,3,5,1,6,6), 5000)
>
>
> system.time(for(i in 1:1000) out1 <- rle(x)$values)
[1] 55.44  2.08 57.89    NA    NA
>
>
> system.time(for(i in 1:1000) {
+     nx <- length(x)
+     ind <- c(TRUE, (x[1:(nx-1)] - x[2:nx]) != 0)
+     out2 <- x[ind]
+ })
[1] 27.69  2.28 30.36    NA    NA
>
>
> system.time(for(i in 1:1000) out3 <- x[diff(x) != 0])
[1] 22.30  2.32 24.62    NA    NA
>
>
> all.equal(out1, out2)
[1] TRUE
> all.equal(out1, out3)
[1] TRUE


Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


>> answered to my needs.
>>
>> I made a test and the first version was 2.5x faster with my data, 
>> but
>> both works enough fast.
>>
>> Atte
>>
>> On Wed, 2006-08-16 at 08:58 +0100, Patrick Burns wrote:
>> > I think
>> >
>> > rle(VECTOR)$values
>> >
>> > will get you what you want.
>> >
>> > Patrick Burns
>> > patrick at burns-stat.com
>> > +44 (0)20 8525 0696
>> > http://www.burns-stat.com
>> > (home of S Poetry and "A Guide for the Unwilling S User")
>> >
>> > Atte Tenkanen wrote:
>> >
>> > >Is there some (much) more efficient way to do this?
>> > >
>> > >VECTOR=c(3,2,4,5,5,3,3,5,1,6,6);
>> > >NEWVECTOR=VECTOR[1];
>> > >
>> > >for(i in 1:(length(VECTOR)-1))
>> > >{
>> > > if((identical(VECTOR[i], VECTOR[i+1]))==FALSE){
>> > > NEWVECTOR=c(NEWVECTOR,VECTOR[i+1])}
>> > >}
>> > >
>> > >
>> > >
>> > >>VECTOR
>> > >>
>> > >>
>> > > [1] 3 2 4 5 5 3 3 5 1 6 6
>> > >
>> > >
>> > >>NEWVECTOR
>> > >>
>> > >>
>> > >[1] 3 2 4 5 3 5 1 6
>> > >
>> > >_______________________________
>> > >Atte Tenkanen
>> > >University of Turku, Finland
>> > >
>> > >______________________________________________
>> > >R-help at stat.math.ethz.ch mailing list
>> > >https://stat.ethz.ch/mailman/listinfo/r-help
>> > >PLEASE do read the posting guide 
>> > >http://www.R-project.org/posting-guide.html
>> > >and provide commented, minimal, self-contained, reproducible 
>> > >code.
>> > >
>> > >
>> > >
>> > >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> Gavin Simpson                 [t] +44 (0)20 7679 0522
> ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
> Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
> Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
> UK. WC1E 6BT.                 [w] http://www.ucl.ac.uk/~ucfagls/
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From S.Pickett at exeter.ac.uk  Wed Aug 16 14:27:58 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Wed, 16 Aug 2006 13:27:58 +0100 (BST)
Subject: [R] [SPAM] - RE: REML with random slopes and random intercepts
 giving strange results - Bayesian Filter detected spam
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>
Message-ID: <1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>

sure, thanks again.
> summary(m2)
Linear mixed-effects model fit by REML
Formula: change.wt ~ newwt + (newwt | id)
   Data: grow
       AIC       BIC   logLik MLdeviance REMLdeviance
 -6203.178 -6164.462 3107.589  -6239.374    -6215.178
Random effects:
 Groups   Name        Variance   Std.Dev.  Corr
 id       (Intercept) 1.0868e-02 0.1042482
          newwt       4.7069e-05 0.0068606 -1.000
 Residual             1.4236e-02 0.1193136
# of obs: 4688, groups: id, 485

Fixed effects:
               Estimate  Std. Error   DF t value  Pr(>|t|)
(Intercept)  5.5692e-01  6.4189e-03 4686  86.761 < 2.2e-16 ***
newwt       -3.2382e-02  4.5962e-04 4686 -70.455 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
      (Intr)
newwt -0.954

> Can you provide the summary(m2) results?
>
>> -----Original Message-----
>> From: Simon Pickett [mailto:S.Pickett at exeter.ac.uk]
>> Sent: Wednesday, August 16, 2006 7:14 AM
>> To: Doran, Harold
>> Cc: r-help at stat.math.ethz.ch
>> Subject: [SPAM] - RE: [R] REML with random slopes and random
>> intercepts giving strange results - Bayesian Filter detected spam
>>
>> Hi again,
>> Even stranger is the fact that the coefficeints (the slope)
>> and the intercepts are not independent, in fact they are
>> directly inversely proportional (r squared = 1).
>> This means that that there isnt a random slope and intercept
>> for each individual (which is what I wanted), but straight
>> line that pivots in the middle and will change from
>> individual to individual. Is there a problem with the way I
>> have structured the random model or a deeper problem with lmer()?
>> here is the code I used
>> m2 <- lmer(changewt ~ newwt+(newwt|id), data = grow)
>> coef(m2)
>> Any suggestions very much appreciated,
>> Simon
>>
>>
>> > I don't this is because you are using REML. The BLUPs from a mixed
>> > model experience some shrinkage whereas the OLS estimates would not.
>> >
>> >> -----Original Message-----
>> >> From: r-help-bounces at stat.math.ethz.ch
>> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Simon Pickett
>> >> Sent: Tuesday, August 15, 2006 11:34 AM
>> >> To: r-help at stat.math.ethz.ch
>> >> Subject: [R] REML with random slopes and random intercepts giving
>> >> strange results
>> >>
>> >> Hi everyone,
>> >> I have been using REML to derive intercepts and
>> coeficients for each
>> >> individual in a growth study. So the code is
>> >> m2 <- lmer(change.wt ~ newwt+(newwt|id), data = grow)
>> >>
>> >> Calling coef(model.lmer) gives a matrix with this
>> information which
>> >> is what I want. However, as a test I looked at each
>> individual on its
>> >> own and used a simple linear regression to obtain the same
>> >> information, then I compared the results. It looks like the REML
>> >> method doesnt seem to approximate the two parameters as
>> well as using
>> >> the simple linear regression on each individual
>> separately, as judged
>> >> by looking at graphs.
>> >> Indeed, why do the results differ at all?
>> >> Excuse my naivety if this is a silly question.
>> >> Thanks to everyone for replying to my previous questions,
>> very much
>> >> appreciated.
>> >> Simon Pickett
>> >> PhD student
>> >> Centre For Ecology and Conservation
>> >> Tremough Campus
>> >> University of Exeter in Cornwall
>> >> TR109EZ
>> >> Tel 01326371852
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>>
>>
>> Simon Pickett
>> PhD student
>> Centre For Ecology and Conservation
>> Tremough Campus
>> University of Exeter in Cornwall
>> TR109EZ
>> Tel 01326371852
>>
>>
>


Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From p.dalgaard at biostat.ku.dk  Wed Aug 16 14:45:40 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Aug 2006 14:45:40 +0200
Subject: [R] [SPAM] - RE: REML with random slopes and random intercepts
	giving strange results - Bayesian Filter detected spam
In-Reply-To: <1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
References: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>
	<1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
Message-ID: <x2y7toetmz.fsf@viggo.kubism.ku.dk>

"Simon Pickett" <S.Pickett at exeter.ac.uk> writes:

> sure, thanks again.

What is the order of magnitude of newwt? If it is large and with a
small variance, then the -1 correlation might be an artifact of
extrapolating the individual lines to zero. Try centering newwt by
subtracting its mean.

> > summary(m2)
> Linear mixed-effects model fit by REML
> Formula: change.wt ~ newwt + (newwt | id)
>    Data: grow
>        AIC       BIC   logLik MLdeviance REMLdeviance
>  -6203.178 -6164.462 3107.589  -6239.374    -6215.178
> Random effects:
>  Groups   Name        Variance   Std.Dev.  Corr
>  id       (Intercept) 1.0868e-02 0.1042482
>           newwt       4.7069e-05 0.0068606 -1.000
>  Residual             1.4236e-02 0.1193136
> # of obs: 4688, groups: id, 485
> 
> Fixed effects:
>                Estimate  Std. Error   DF t value  Pr(>|t|)
> (Intercept)  5.5692e-01  6.4189e-03 4686  86.761 < 2.2e-16 ***
> newwt       -3.2382e-02  4.5962e-04 4686 -70.455 < 2.2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Correlation of Fixed Effects:
>       (Intr)
> newwt -0.954
> 
> > Can you provide the summary(m2) results?
> >
> >> -----Original Message-----
> >> From: Simon Pickett [mailto:S.Pickett at exeter.ac.uk]
> >> Sent: Wednesday, August 16, 2006 7:14 AM
> >> To: Doran, Harold
> >> Cc: r-help at stat.math.ethz.ch
> >> Subject: [SPAM] - RE: [R] REML with random slopes and random
> >> intercepts giving strange results - Bayesian Filter detected spam
> >>
> >> Hi again,
> >> Even stranger is the fact that the coefficeints (the slope)
> >> and the intercepts are not independent, in fact they are
> >> directly inversely proportional (r squared = 1).
> >> This means that that there isnt a random slope and intercept
> >> for each individual (which is what I wanted), but straight
> >> line that pivots in the middle and will change from
> >> individual to individual. Is there a problem with the way I
> >> have structured the random model or a deeper problem with lmer()?
> >> here is the code I used
> >> m2 <- lmer(changewt ~ newwt+(newwt|id), data = grow)
> >> coef(m2)
> >> Any suggestions very much appreciated,
> >> Simon
> >>
> >>
> >> > I don't this is because you are using REML. The BLUPs from a mixed
> >> > model experience some shrinkage whereas the OLS estimates would not.
> >> >
> >> >> -----Original Message-----
> >> >> From: r-help-bounces at stat.math.ethz.ch
> >> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> >> Simon Pickett
> >> >> Sent: Tuesday, August 15, 2006 11:34 AM
> >> >> To: r-help at stat.math.ethz.ch
> >> >> Subject: [R] REML with random slopes and random intercepts giving
> >> >> strange results
> >> >>
> >> >> Hi everyone,
> >> >> I have been using REML to derive intercepts and
> >> coeficients for each
> >> >> individual in a growth study. So the code is
> >> >> m2 <- lmer(change.wt ~ newwt+(newwt|id), data = grow)
> >> >>
> >> >> Calling coef(model.lmer) gives a matrix with this
> >> information which
> >> >> is what I want. However, as a test I looked at each
> >> individual on its
> >> >> own and used a simple linear regression to obtain the same
> >> >> information, then I compared the results. It looks like the REML
> >> >> method doesnt seem to approximate the two parameters as
> >> well as using
> >> >> the simple linear regression on each individual
> >> separately, as judged
> >> >> by looking at graphs.
> >> >> Indeed, why do the results differ at all?
> >> >> Excuse my naivety if this is a silly question.
> >> >> Thanks to everyone for replying to my previous questions,
> >> very much
> >> >> appreciated.
> >> >> Simon Pickett
> >> >> PhD student
> >> >> Centre For Ecology and Conservation
> >> >> Tremough Campus
> >> >> University of Exeter in Cornwall
> >> >> TR109EZ
> >> >> Tel 01326371852
> >> >>
> >> >> ______________________________________________
> >> >> R-help at stat.math.ethz.ch mailing list
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >>
> >> >
> >>
> >>
> >> Simon Pickett
> >> PhD student
> >> Centre For Ecology and Conservation
> >> Tremough Campus
> >> University of Exeter in Cornwall
> >> TR109EZ
> >> Tel 01326371852
> >>
> >>
> >
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ffenics2002 at yahoo.co.uk  Wed Aug 16 14:46:34 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Wed, 16 Aug 2006 13:46:34 +0100 (BST)
Subject: [R] advice on exporting a distance matrix in the correct format
	needed please
Message-ID: <20060816124634.67005.qmail@web25511.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/5a79c5cf/attachment.pl 

From jfox at mcmaster.ca  Wed Aug 16 14:47:28 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Aug 2006 08:47:28 -0400
Subject: [R] Specifying Path Model in SEM for CFA
In-Reply-To: <1155703829.3887.9.camel@localhost.localdomain>
Message-ID: <20060816124726.HMEY13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Rick,

There are a couple of problems here:

(1) You've fixed the error variance parameters for each of the observed
variables to 1 rather than defining each as a free parameter to estimate.
For example, use 

X1 <-> X1, theta1, NA

Rather than 

X1 <-> X1, NA, 1

The general principle is that if you give a parameter a name, it's a free
parameter to be estimated; if you give the name as NA, then the parameter is
given a fixed value (here, 1). (There is some more information on this and
on error-variance parameters in ?sem.)

(2) I believe that the model you're trying to specify -- in which all
variables but X6 load on F1, and all variables but X1 load on F2 -- is
underidentified.

In addition, you've set the metric of the factors by fixing one loading to
0.20 and another to 0.25. That should work but strikes me as unusual, and
makes me wonder whether this was what you really intended. It would be more
common in a CFA to fix the variance of each factor to 1, and let the factor
loadings be free parameters. Then the factor covariance would be their
correlation. 

You should not have to specify start values for free parameters (such as
g11, g22, and g12 in your model), though it is not wrong to do so. I would
not, however, specify start values that imply a singular covariance matrix
among the factors, as you've done; I'm surprised that the program was able
to get by the start values to produce a solution.

BTW, the Thurstone example in ?sem is for a confirmatory factor analysis
(albeit a slightly more complicated one with a second-order factor). There's
also an example of a one-factor CFA in the paper at
<http://socserv.socsci.mcmaster.ca/jfox/Misc/sem/SEM-paper.pdf>, though this
is for ordinal observed variables.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rick Bilonick
> Sent: Tuesday, August 15, 2006 11:50 PM
> To: R Help
> Subject: [R] Specifying Path Model in SEM for CFA
> 
> I'm using specify.model for the sem package. I can't figure 
> out how to represent the residual errors for the observed 
> variables for a CFA model. (Once I get this working I need to 
> add some further constraints.)
> 
> Here is what I've tried:
> 
> model.sa <- specify.model()
>   F1	 -> X1,l11, NA
>   F1	 -> X2,l21, NA
>   F1	 -> X3,l31, NA
>   F1	 -> X4,l41, NA
>   F1	 -> X5, NA, 0.20
>   F2	 -> X1,l12, NA
>   F2	 -> X2,l22, NA
>   F2	 -> X3,l32, NA
>   F2	 -> X4,l42, NA
>   F2	 -> X6, NA, 0.25
>   F1	<-> F2,g12, 1
>   F1    <-> F1,g11, 1
>   F2    <-> F2,g22, 1
>   X1	<-> X1, NA, 1
>   X2	<-> X2, NA, 1
>   X3	<-> X3, NA, 1
>   X4	<-> X4, NA, 1
>   X5	<-> X5, NA, 1
>   X6	<-> X6, NA, 1
> 
> This at least converges:
> 
> > summary(fit.sem)
> 
>  Model Chisquare =  2147   Df =  10 Pr(>Chisq) = 0
>  Chisquare (null model) =  2934   Df =  15
>  Goodness-of-fit index =  0.4822
>  Adjusted goodness-of-fit index =  -0.087387
>  RMSEA index =  0.66107   90 % CI: (NA, NA)
>  Bentler-Bonnett NFI =  0.26823
>  Tucker-Lewis NNFI =  -0.098156
>  Bentler CFI =  0.26790
>  BIC =  2085.1
> 
>  Normalized Residuals
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  -5.990  -0.618   0.192   0.165   1.700   3.950
> 
>  Parameter Estimates
>     Estimate  Std Error z value  Pr(>|z|)
> l11 -0.245981 0.21863   -1.12510 0.26054748 X1 <--- F1
> l21 -0.308249 0.22573   -1.36555 0.17207875 X2 <--- F1
> l31  0.202590 0.07910    2.56118 0.01043175 X3 <--- F1
> l41 -0.235156 0.21980   -1.06985 0.28468885 X4 <--- F1
> l12  0.839985 0.21962    3.82476 0.00013090 X1 <--- F2
> l22  0.828460 0.22548    3.67418 0.00023862 X2 <--- F2
> l32  0.066722 0.08369    0.79725 0.42530606 X3 <--- F2
> l42  0.832037 0.21840    3.80963 0.00013917 X4 <--- F2
> g12  0.936719 0.64331    1.45609 0.14536647 F2 <--> F1
> g11  2.567669 1.25608    2.04418 0.04093528 F1 <--> F1
> g22  1.208497 0.55040    2.19567 0.02811527 F2 <--> F2
> 
>  Iterations =  59
> 
> And it produces the following path diagram:
> 
> > path.diagram(fit.sem)
> digraph "fit.sem" {
>   rankdir=LR;
>   size="8,8";
>   node [fontname="Helvetica" fontsize=14 shape=box];
>   edge [fontname="Helvetica" fontsize=10];
>   center=1;
>   "F2" [shape=ellipse]
>   "F1" [shape=ellipse]
>   "F1" -> "X1" [label="l11"];
>   "F1" -> "X2" [label="l21"];
>   "F1" -> "X3" [label="l31"];
>   "F1" -> "X4" [label="l41"];
>   "F1" -> "X5" [label=""];
>   "F2" -> "X1" [label="l12"];
>   "F2" -> "X2" [label="l22"];
>   "F2" -> "X3" [label="l32"];
>   "F2" -> "X4" [label="l42"];
>   "F2" -> "X6" [label=""];
> }
> 
> But I don't see the residual error terms that go into each of 
> the observed variables X1 - X6. I've tried:
> 
> model.sa <- specify.model()
>   E1	 -> X1, e1,  1
>   E2	 -> X2, e2,  1
>   E3	 -> X3, e3,  1
>   E4	 -> X4, e4,  1
>   E5	 -> X5, e5,  1
>   E6	 -> X6, e6,  1
>   E1	<-> E1, s1, NA
>   E2	<-> E2, s2, NA
>   E3	<-> E3, s3, NA
>   E4	<-> E4, s4, NA
>   E5	<-> E5, s5, NA
>   E6	<-> E6, s6, NA
>   F1	 -> X1,l11, NA
>   F1	 -> X2,l21, NA
>   F1	 -> X3,l31, NA
>   F1	 -> X4,l41, NA
>   F1	 -> X5, NA,  1
>   F2	 -> X1,l12, NA
>   F2	 -> X2,l22, NA
>   F2	 -> X3,l32, NA
>   F2	 -> X4,l42, NA
>   F2	 -> X6, NA,  1
>   F1	<-> F2, NA, 1
>   F1    <-> F1, NA, 1
>   F2    <-> F2,g22, NA
>   X1	<-> X1, NA, 1
>   X2	<-> X2, NA, 1
>   X3	<-> X3, NA, 1
>   X4	<-> X4, NA, 1
>   X5	<-> X5, NA, 1
>   X6	<-> X6, NA, 1
> 
> I'm trying to use E1 - E6 as the residual error terms. But I 
> get warning messages about no variances for X1-X6 and it 
> doesn't converge. Also, the associated path diagram:
> 
> digraph "fit.sem" {
>   rankdir=LR;
>   size="8,8";
>   node [fontname="Helvetica" fontsize=14 shape=box];
>   edge [fontname="Helvetica" fontsize=10];
>   center=1;
>   "E1" [shape=ellipse]
>   "E2" [shape=ellipse]
>   "E3" [shape=ellipse]
>   "E4" [shape=ellipse]
>   "E5" [shape=ellipse]
>   "E6" [shape=ellipse]
>   "F2" [shape=ellipse]
>   "F1" [shape=ellipse]
>   "E1" -> "X1" [label=""];
>   "E2" -> "X2" [label=""];
>   "E3" -> "X3" [label=""];
>   "E4" -> "X4" [label=""];
>   "E5" -> "X5" [label=""];
>   "E6" -> "X6" [label=""];
>   "F1" -> "X1" [label="l11"];
>   "F1" -> "X2" [label="l21"];
>   "F1" -> "X3" [label="l31"];
>   "F1" -> "X4" [label="l41"];
>   "F1" -> "X5" [label=""];
>   "F2" -> "X1" [label="l12"];
>   "F2" -> "X2" [label="l22"];
>   "F2" -> "X3" [label="l32"];
>   "F2" -> "X4" [label="l42"];
>   "F2" -> "X6" [label=""];
> }
> 
> Has ellipses around the E1-E6 which I believe indicates they 
> are latent factors and not residual errors.
> 
> If anyone could point in the right direction I would appreciate it.
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bibiko at eva.mpg.de  Wed Aug 16 14:56:24 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 16 Aug 2006 14:56:24 +0200
Subject: [R] Problem with the special argument '...' within a function
In-Reply-To: <1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
References: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>
	<1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
Message-ID: <C3E2C445-30FF-4A45-8261-67A7DE7B9376@eva.mpg.de>

Dear all,

I wrote some functions using the special argument '...'. OK, it works.

But if I call such a function which also called such a function, then  
I get an error message about unused arguments.

Here's an example:

fun1 <- function(x,a=1)
{
	print(paste("x=",x))
	print(paste("a=",a))
}
fun2 <- function(y,b=2)
{
	print(paste("y=",y))
	print(paste("b=",b))
}
myfun <- function(c, ...)
{
	print(paste("c=",c))
	fun1(x=c,...)
	fun2(y=c,...)
}

This is OK.
 > myfun(c=3)
[1] "c= 3"
[1] "x= 3"
[1] "a= 1"
[1] "y= 3"
[1] "b= 2"

 > myfun(c=3,a=4)
[1] "c= 3"
[1] "x= 3"
[1] "a= 4"
Error in fun2(y = c, ...) : unused argument(s) (a ...)

I understand the error message because fun2 has no argument called 'a'.

But how can I avoid this???

I want to use this in order to be able to call myfun() with all  
arguments to control myfun(),fun1(), and fun2().

Please help!

Thanks,

Hans


From bibiko at eva.mpg.de  Wed Aug 16 15:10:51 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 16 Aug 2006 15:10:51 +0200
Subject: [R] advice on exporting a distance matrix in the correct format
	needed please
In-Reply-To: <20060816124634.67005.qmail@web25511.mail.ukl.yahoo.com>
References: <20060816124634.67005.qmail@web25511.mail.ukl.yahoo.com>
Message-ID: <97843C91-8FFC-4750-B067-731CB3E59FEE@eva.mpg.de>

In order to get a table structure out of a dist object use 'as.matrix 
()' like 'as.matrix(dist(myMatrix))' and write.table().

Hans

On 16 Aug 2006, at 14:46, Ffenics wrote:

> Hi there
> Could anyone please tell me how to export a distance matrix to  
> a .csv file please? when I tried write(matrix) I got the values but  
> not in the format of a distance matrix - just a list of numbers in  
> 4 columns - and when I try write.table(matrix, file="distance.csv")  
> I get the following
> Error in as.data.frame.default(x[[i]], optional = TRUE) :
>         cannot coerce class "dist" into a data.frame
> Any suggestions much appreciated
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From aon.912182281.tmp at aon.at  Wed Aug 16 15:48:14 2006
From: aon.912182281.tmp at aon.at (aon.912182281.tmp at aon.at)
Date: Wed, 16 Aug 2006 15:48:14 +0200 (MEST)
Subject: [R] fitting truncated normal distribution
Message-ID: <1155736094.44e3221ebc376@webmail.aon.at>

Hello,
I am a new user of R and found the function dtnorm() in the package msm.

My problem now is, that it is not possible for me to get the mean and sd out of a sample when I want a left-truncated normal distribution starting at "0".

fitdistr(x,dtnorm, start=list(mean=0, sd=1))

returns the error message 
"Fehler in "[<-"(`*tmp*`, x >= lower & x <= upper, value = numeric(0)) :    nichts zu ersetzen"

I don't know, where to enter the lower/upper value. Is there a possibility to program the dtnorm function by myself?

Thank you very much in advance for your help,
markus

-------------------------------------------
Versendet durch aonWebmail (webmail.aon.at)


From j.van_den_hoff at fz-rossendorf.de  Wed Aug 16 14:22:03 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Wed, 16 Aug 2006 14:22:03 +0200
Subject: [R] nls convergence problem
In-Reply-To: <ebtfil$3n9$1@sea.gmane.org>
References: <ebtbpe$nan$1@sea.gmane.org>	<007c01c6c0ae$fd260e50$711f210a@gne.windows.gene.com>
	<ebtfil$3n9$1@sea.gmane.org>
Message-ID: <44E30DEB.1050908@fz-rossendorf.de>

Earl F. Glynn wrote:
> "Berton Gunter" <gunter.berton at gene.com> wrote in message 
> news:007c01c6c0ae$fd260e50$711f210a at gne.windows.gene.com...
>>>   Or, maybe there's something I don't understand about the
>>> algorithm being used.
>> Indeed! So before making such comments, why don't you try to learn about 
>> it?
>> Doug Bates is a pretty smart guy,  and I think you do him a disservice 
>> when
>> you assume that he somehow overlooked something that he explicitly warned
>> you about. I am fairly confident that if he could have made the problem go
>> away, he would have. So I think your vent was a bit inconsiderate and
>> perhaps even intemperate. The R Core folks have produced a minor miracle
>> IMO, and we should all be careful before assuming that they have 
>> overlooked
>> easily fixable problems. They're certainly not infallible -- but they're a
>> lot less fallible than most of the rest of us when it comes to R.
> 
> I meant no disrespect to Doug Bates or any of the R Core folks. I thought 
> what I wrote had a "neutral" tone and was respectful.  I am sorry if anyone 
> was offended by my comments and suggestions.  I am certainly thankful for 
> all the hard work that has gone into developing R.
> 
> efg
> 

well, just a feedback to that: of course the tone of your mail was by no 
means inadequate (at least douglas bates did not object...). the 
tendency on this list to rather harshly rebuke people for some kind of 
(real or imagined) misconduct and to 'defend' R against 'attacks' is 
counterproductive and unnecessary. it goes without saying that the 
people 'behind' R can not and will not (and are not expected to) change 
the code after each mail on the help list which raises some question.

and concerning your `nls' question: sure, the noise requirement is a 
pitfall in the beginning, but afterwards it's irrelevant: you don't fit 
noise free data in real life (in the sense that real data never follow 
you model exactly). and, sure, the convergence decision could be altered 
(given enough time and knowledge). whether convergence failure on exact 
data is a bug or a feature is a matter of taste, probably.

getting better access to the trace output and especially access to 
intermediate pre-convergence values of the model parameters (this would 
  'solve' your problem, too) would really be an improvement, in my mind 
(I think this is recognized by d. bates, but simply way down his 'to do' 
list :-().


joerg


From D.GOUACHE at arvalisinstitutduvegetal.fr  Wed Aug 16 16:27:59 2006
From: D.GOUACHE at arvalisinstitutduvegetal.fr (GOUACHE David)
Date: Wed, 16 Aug 2006 16:27:59 +0200
Subject: [R] adding multiple fitted curves to xyplot graph
Message-ID: <1DF7DB4AB44EFB41A60A889186D433590259CD@srv-laminiere.arvalis-fr.com>

Hello RHelpers,

This may already have been answered, but despite days of scouring through the archives I haven't found it.
My goal is to add multiple fitted curves to a plot.
An example data set (a data frame named df in following code) is:

             x1               y1     factor1
4       1298.25       0.00000000           1
5       1393.25       0.00000000           1
6       1471.50       0.04597701           1
7       1586.70       2.56908046           1
8       1692.10      11.14080460           1
9       1832.55      45.50459770           1
10      1928.30      65.56000000           1
11      2092.40     100.00000000           1
31      1202.90       0.00000000           2
41      1298.25       0.00000000           2
51      1393.25       0.37885057           2
61      1471.50       0.76839080           2
71      1586.70       7.75206897           2
81      1692.10      50.19448276           2
91      1832.55      94.08045977           2
101     1928.30     100.00000000           2
111     2092.40     100.00000000           2
14      1028.50       0.11111111           3
22      1106.40       0.04938272           3
32      1202.90       0.03448276           3
42      1298.25       0.34482759           3
52      1393.25       1.43850575           3
62      1471.50       1.96850575           3
72      1586.70      36.80597701           3
82      1692.10      92.83390805           3
92      1832.55     100.00000000           3
15      1028.50       0.09638554           4
23      1106.40       0.39988506           4
33      1202.90       0.49321839           4
43      1298.25       1.66045977           4
53      1393.25       7.51137931           4
63      1471.50      42.02724138           4
73      1586.70      99.12068966           4
83      1692.10     100.00000000           4

I plot this with xyplot:

trellis.par.set("background","white")
trellis.par.set(list(superpose.symbol=list(pch=c(15:17,21,25))))
xyplot(y1 ~ x1, data=df, groups=factor1,
            type = "p",
            auto.key =
            list(space = "right", points = TRUE, lines = FALSE))

For each level of factor1 I fit a growth curve:

fit.curve<-function(tab)
{
	res.fit<-nls(y1 ~ 100/(1+exp(((-log(81))/a)*(x1-b))), start=list(a=min(tab$x1[tab$y1>76],na.rm=T)-max(tab$x1[tab$y1<15],na.rm=T),b=tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)][!is.na(tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)])]),data=tab) 
	coef(res.fit)
}
by(df,list(df$factor1),fit.curve)

I would like to add the 4 curves corresponding to these 4 fits to my graphic.
The elegant way would be a custom panel function I suppose, but I haven't been able to write one up...
Could someone help me out on this please?

In advance thanks very much!!!

David Gouache
Arvalis - Institut du V?g?tal
Station de La Mini?re
78280 Guyancourt
Tel: 01.30.12.96.22 / Port: 06.86.08.94.32


From ggrothendieck at gmail.com  Wed Aug 16 16:50:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Aug 2006 10:50:48 -0400
Subject: [R] adding multiple fitted curves to xyplot graph
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D433590259CD@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D433590259CD@srv-laminiere.arvalis-fr.com>
Message-ID: <971536df0608160750g57d7079fwa9f6e4f218e0b7cc@mail.gmail.com>

Try this after displaying the xyplot:

# this fit.curve returns the whole nls object, not the coefs
fit.curve<-function(tab) {
       nls(y1 ~ 100/(1+exp(((-log(81))/a)*(x1-b))),
start=list(a=min(tab$x1[tab$y1>76],na.rm=T)-max(tab$x1[tab$y1<15],na.rm=T),b=tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)][!is.na(tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)])]),data=tab)
}

trellis.focus("panel", 1, 1)
f <- function(x) panel.lines(x$x1, fitted(fit.curve(x)), col = 1)
junk <- by(df, df$factor1, f)
trellis.unfocus()



On 8/16/06, GOUACHE David <D.GOUACHE at arvalisinstitutduvegetal.fr> wrote:
> Hello RHelpers,
>
> This may already have been answered, but despite days of scouring through the archives I haven't found it.
> My goal is to add multiple fitted curves to a plot.
> An example data set (a data frame named df in following code) is:
>
>             x1               y1     factor1
> 4       1298.25       0.00000000           1
> 5       1393.25       0.00000000           1
> 6       1471.50       0.04597701           1
> 7       1586.70       2.56908046           1
> 8       1692.10      11.14080460           1
> 9       1832.55      45.50459770           1
> 10      1928.30      65.56000000           1
> 11      2092.40     100.00000000           1
> 31      1202.90       0.00000000           2
> 41      1298.25       0.00000000           2
> 51      1393.25       0.37885057           2
> 61      1471.50       0.76839080           2
> 71      1586.70       7.75206897           2
> 81      1692.10      50.19448276           2
> 91      1832.55      94.08045977           2
> 101     1928.30     100.00000000           2
> 111     2092.40     100.00000000           2
> 14      1028.50       0.11111111           3
> 22      1106.40       0.04938272           3
> 32      1202.90       0.03448276           3
> 42      1298.25       0.34482759           3
> 52      1393.25       1.43850575           3
> 62      1471.50       1.96850575           3
> 72      1586.70      36.80597701           3
> 82      1692.10      92.83390805           3
> 92      1832.55     100.00000000           3
> 15      1028.50       0.09638554           4
> 23      1106.40       0.39988506           4
> 33      1202.90       0.49321839           4
> 43      1298.25       1.66045977           4
> 53      1393.25       7.51137931           4
> 63      1471.50      42.02724138           4
> 73      1586.70      99.12068966           4
> 83      1692.10     100.00000000           4
>
> I plot this with xyplot:
>
> trellis.par.set("background","white")
> trellis.par.set(list(superpose.symbol=list(pch=c(15:17,21,25))))
> xyplot(y1 ~ x1, data=df, groups=factor1,
>            type = "p",
>            auto.key =
>            list(space = "right", points = TRUE, lines = FALSE))
>
> For each level of factor1 I fit a growth curve:
>
> fit.curve<-function(tab)
> {
>        res.fit<-nls(y1 ~ 100/(1+exp(((-log(81))/a)*(x1-b))), start=list(a=min(tab$x1[tab$y1>76],na.rm=T)-max(tab$x1[tab$y1<15],na.rm=T),b=tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)][!is.na(tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)])]),data=tab)
>        coef(res.fit)
> }
> by(df,list(df$factor1),fit.curve)
>
> I would like to add the 4 curves corresponding to these 4 fits to my graphic.
> The elegant way would be a custom panel function I suppose, but I haven't been able to write one up...
> Could someone help me out on this please?
>
> In advance thanks very much!!!
>
> David Gouache
> Arvalis - Institut du V?g?tal
> Station de La Mini?re
> 78280 Guyancourt
> Tel: 01.30.12.96.22 / Port: 06.86.08.94.32
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Setzer.Woodrow at epamail.epa.gov  Wed Aug 16 16:50:59 2006
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow at epamail.epa.gov)
Date: Wed, 16 Aug 2006 10:50:59 -0400
Subject: [R] nls convergence problem
In-Reply-To: <44E30DEB.1050908@fz-rossendorf.de>
Message-ID: <OFA84CBC25.463F1A9F-ON852571CC.004F14CD-852571CC.005192C2@epamail.epa.gov>

Joerg van den Hoff <j.van_den_hoff at fz-rossendorf.de> wrote on 08/16/2006
08:22:03 AM:

> Earl F. Glynn wrote:
[deleted]
> > efg
> >
>

[deleted]
> (I think this is recognized by d. bates, but simply way down his 'to
do'
> list :-().
>
>
> joerg

No doubt Doug Bates would gladly accept patches ... .

>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

R. Woodrow Setzer, Ph. D.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B205-01/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-1194


From sundar.dorai-raj at pdf.com  Wed Aug 16 17:12:07 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 16 Aug 2006 10:12:07 -0500
Subject: [R] fitting truncated normal distribution
In-Reply-To: <1155736094.44e3221ebc376@webmail.aon.at>
References: <1155736094.44e3221ebc376@webmail.aon.at>
Message-ID: <44E335C7.2040003@pdf.com>



aon.912182281.tmp at aon.at wrote:
> Hello,
> I am a new user of R and found the function dtnorm() in the package msm.
> 
> My problem now is, that it is not possible for me to get the mean and sd out of a sample when I want a left-truncated normal distribution starting at "0".
> 
> fitdistr(x,dtnorm, start=list(mean=0, sd=1))
> 
> returns the error message 
> "Fehler in "[<-"(`*tmp*`, x >= lower & x <= upper, value = numeric(0)) :    nichts zu ersetzen"
> 
> I don't know, where to enter the lower/upper value. Is there a possibility to program the dtnorm function by myself?
> 
> Thank you very much in advance for your help,
> markus
> 
> -------------------------------------------
> Versendet durch aonWebmail (webmail.aon.at)
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Markus,

You should always supply the package name where dtnorm is located. My 
guess is most don't know (as I didn't) it is part of the msm package. 
Also, you should supply a reproducible example so others may understand 
your particular problem. For example, when I ran your code on data 
generated from "rtnorm" (also part of msm) I got warnings related to the 
NaNs generated in pnorm and qnorm, but no error as you reported. Both of 
these suggestions are in the posting guide (see signature above).

So, to answer your problem, here's a quick example.

library(MASS) ## for fitdistr
library(msm) ## for dtnorm

dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
   dtnorm(x, mean, sd, 0, Inf, log)
}

set.seed(1) ## to others may reproduce my results exactly
x <- rtnorm(100, lower = 0)
fitdistr(x, dtnorm0, start = list(mean = 0, sd = 1))

Note, the help page ?fitdistr suggests additional parameters may be 
passed to the density function (i.e. dtnorm) or optim. However, this 
won't work here because "lower" is an argument for both functions. This 
is the reason for writing dtnorm0 which has neither a lower or an upper 
argument.

HTH,

--sundar


From ssj1364 at gmail.com  Wed Aug 16 17:22:03 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Wed, 16 Aug 2006 09:22:03 -0600
Subject: [R]  list to balanced array
Message-ID: <1c6126db0608160822ja6263dan6bf7f20d0e6a03e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/7c358a1f/attachment.pl 

From muster at gmail.com  Wed Aug 16 17:23:13 2006
From: muster at gmail.com (T Mu)
Date: Wed, 16 Aug 2006 11:23:13 -0400
Subject: [R] confusing about contrasts concept
Message-ID: <b68812e70608160823w4fa5e3b7vc3cff467d4c77d1c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/b2384f03/attachment.pl 

From g.comte at alliance-ir.net  Wed Aug 16 17:34:43 2006
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Wed, 16 Aug 2006 17:34:43 +0200
Subject: [R] Strange behavior with "hist" function filled with breaks and
	freq attribute
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3415EC77@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/92ebbb8f/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Wed Aug 16 17:34:39 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 16 Aug 2006 17:34:39 +0200
Subject: [R] list to balanced array
References: <1c6126db0608160822ja6263dan6bf7f20d0e6a03e0@mail.gmail.com>
Message-ID: <013a01c6c149$7cf9b1f0$0540210a@www.domain>

try the following:

arrivals <- matrix(sample(1:24, 100, TRUE), 10, 10)
apply(arrivals, 2, function(x) table(factor(x, levels = 1:24)))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Spencer Jones" <ssj1364 at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 16, 2006 5:22 PM
Subject: [R] list to balanced array


>I am working with a large data set of arrivals, for each day I have
> aggregated the arrivals into hrs (1-24) via: apply(x,2,table). On 
> some days
> there are zero arrivals during some hours of the day, this leaves me 
> with
> (I believe) a list of vectors of differnt lengths (see below).
>
>
> [[4]]
>
> 1  2  3  5  6  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24
> 1  3  2  3  1  1  2  3   4   4   4   3   2   6  2   5   1  2   2   2 
> 1
>
> [[5]]
>
> 2  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24
> 2  1  1  2  1   5   3   6   6  3   2   2  1   4   3   3  4   2   1
>
> I would like to be able to create an array with equal numbers of 
> rows (24)
> for each column, i.e., fill in the gaps with Zeros.
>
>
> [[5]]
>
> 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  21  22 
> 23 24
> 0  2  0  0  1  1  0  2  1   5   3   6   6  3   2   2  1   4   3   0 
> 3
> 4   2   1
>
>
> Any suggestions?
>
>
> thanks,
>
> Spencer
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From sundar.dorai-raj at pdf.com  Wed Aug 16 17:35:17 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 16 Aug 2006 10:35:17 -0500
Subject: [R] fitting truncated normal distribution
In-Reply-To: <44E335C7.2040003@pdf.com>
References: <1155736094.44e3221ebc376@webmail.aon.at>
	<44E335C7.2040003@pdf.com>
Message-ID: <44E33B35.6010702@pdf.com>

Sorry, didn't notice that you *did* mention dtnorm is part of msm. 
Ignore that part of the advice...

--sundar

Sundar Dorai-Raj wrote:
> 
> aon.912182281.tmp at aon.at wrote:
> 
>>Hello,
>>I am a new user of R and found the function dtnorm() in the package msm.
>>
>>My problem now is, that it is not possible for me to get the mean and sd out of a sample when I want a left-truncated normal distribution starting at "0".
>>
>>fitdistr(x,dtnorm, start=list(mean=0, sd=1))
>>
>>returns the error message 
>>"Fehler in "[<-"(`*tmp*`, x >= lower & x <= upper, value = numeric(0)) :    nichts zu ersetzen"
>>
>>I don't know, where to enter the lower/upper value. Is there a possibility to program the dtnorm function by myself?
>>
>>Thank you very much in advance for your help,
>>markus
>>
>>-------------------------------------------
>>Versendet durch aonWebmail (webmail.aon.at)
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
> 
> 
> Hi, Markus,
> 
> You should always supply the package name where dtnorm is located. My 
> guess is most don't know (as I didn't) it is part of the msm package. 
> Also, you should supply a reproducible example so others may understand 
> your particular problem. For example, when I ran your code on data 
> generated from "rtnorm" (also part of msm) I got warnings related to the 
> NaNs generated in pnorm and qnorm, but no error as you reported. Both of 
> these suggestions are in the posting guide (see signature above).
> 
> So, to answer your problem, here's a quick example.
> 
> library(MASS) ## for fitdistr
> library(msm) ## for dtnorm
> 
> dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>    dtnorm(x, mean, sd, 0, Inf, log)
> }
> 
> set.seed(1) ## to others may reproduce my results exactly
> x <- rtnorm(100, lower = 0)
> fitdistr(x, dtnorm0, start = list(mean = 0, sd = 1))
> 
> Note, the help page ?fitdistr suggests additional parameters may be 
> passed to the density function (i.e. dtnorm) or optim. However, this 
> won't work here because "lower" is an argument for both functions. This 
> is the reason for writing dtnorm0 which has neither a lower or an upper 
> argument.
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jacques.veslot at good.ibl.fr  Wed Aug 16 17:39:33 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 16 Aug 2006 17:39:33 +0200
Subject: [R] Trend test and test for homogeneity of odd-ratios
Message-ID: <44E33C35.5000606@good.ibl.fr>

Dear r-users,

I am looking for some R functions to do Cochran-Armitage trend test for 2*3 tables (binary phenotype 
vs. genotypes) and for testing the homogeneity of odds ratios within 2*3*k tables (binary phenotype 
vs. genotypes vs. strata).

In R-Help archives, I've found a 2003 script by Eric Lecoutre for Cochran-Armitage trend test and a 
script for Breslow-Day test for 2*2*k tables.

Could someone please tell we if there were some functions available on CRAN to do such tests ?

Thanks,

jacques


From andy_liaw at merck.com  Wed Aug 16 18:09:17 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Aug 2006 12:09:17 -0400
Subject: [R] Problem with the special argument '...' within a function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB32A2@usctmx1106.merck.com>

I'm not sure if this is what you want, but simply add ... to the list of
arguments for fun1 and fun2 would eliminate the error. 

Andy

From: Hans-Joerg Bibiko
> 
> Dear all,
> 
> I wrote some functions using the special argument '...'. OK, it works.
> 
> But if I call such a function which also called such a 
> function, then I get an error message about unused arguments.
> 
> Here's an example:
> 
> fun1 <- function(x,a=1)
> {
> 	print(paste("x=",x))
> 	print(paste("a=",a))
> }
> fun2 <- function(y,b=2)
> {
> 	print(paste("y=",y))
> 	print(paste("b=",b))
> }
> myfun <- function(c, ...)
> {
> 	print(paste("c=",c))
> 	fun1(x=c,...)
> 	fun2(y=c,...)
> }
> 
> This is OK.
>  > myfun(c=3)
> [1] "c= 3"
> [1] "x= 3"
> [1] "a= 1"
> [1] "y= 3"
> [1] "b= 2"
> 
>  > myfun(c=3,a=4)
> [1] "c= 3"
> [1] "x= 3"
> [1] "a= 4"
> Error in fun2(y = c, ...) : unused argument(s) (a ...)
> 
> I understand the error message because fun2 has no argument 
> called 'a'.
> 
> But how can I avoid this???
> 
> I want to use this in order to be able to call myfun() with 
> all arguments to control myfun(),fun1(), and fun2().
> 
> Please help!
> 
> Thanks,
> 
> Hans
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From singularitaet at gmx.net  Wed Aug 16 19:02:32 2006
From: singularitaet at gmx.net (Singu)
Date: Wed, 16 Aug 2006 19:02:32 +0200
Subject: [R] matching pairs in a Dataframe?
Message-ID: <44E34FA8.3090205@gmx.net>

Dear list,

I want to extract pairs of values out of a dataframe where one
criteria/condition does match.

I have an experiment with 3 conditions which were not always applied:

e.g.:

group   cond   x
A         1         2
A         2         4
A         3         6.5
B         1         3
B         2         4.5
C         1         2.5
C         3         4
D         2         5
D         3         6
E         1         1
E         2         4
E         3         6


Now I wanted to extract the x of those groups where condition 2 and
condition 3 do both exist.

In this example that would be groups A, D and E and the extracted pairs
e.g.:
cond2   cond3
4         6.5
5         6
4         6

(I need this for a wilcoxon test)

I would be happy if one could give me a hint, probably its very simple...

Thanks
Stefan Grosse


From ottaris at gmail.com  Wed Aug 16 19:09:50 2006
From: ottaris at gmail.com (=?ISO-8859-1?Q?=D3ttar_=CDsberg?=)
Date: Wed, 16 Aug 2006 17:09:50 +0000
Subject: [R] Autocompletion
Message-ID: <765e0a3b0608161009l55794f86t2dd841cb1dc929@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/0bcada37/attachment.pl 

From rab45+ at pitt.edu  Wed Aug 16 19:07:14 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Wed, 16 Aug 2006 13:07:14 -0400
Subject: [R] Specifying Path Model in SEM for CFA
In-Reply-To: <20060816124726.HMEY13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>
References: <20060816124726.HMEY13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1155748034.3596.11.camel@localhost.localdomain>

On Wed, 2006-08-16 at 08:47 -0400, John Fox wrote:
> Dear Rick,
> 
> There are a couple of problems here:
> 
> (1) You've fixed the error variance parameters for each of the observed
> variables to 1 rather than defining each as a free parameter to estimate.
> For example, use 
> 
> X1 <-> X1, theta1, NA
> 
> Rather than 
> 
> X1 <-> X1, NA, 1
> 
> The general principle is that if you give a parameter a name, it's a free
> parameter to be estimated; if you give the name as NA, then the parameter is
> given a fixed value (here, 1). (There is some more information on this and
> on error-variance parameters in ?sem.)
> 
> (2) I believe that the model you're trying to specify -- in which all
> variables but X6 load on F1, and all variables but X1 load on F2 -- is
> underidentified.
> 
> In addition, you've set the metric of the factors by fixing one loading to
> 0.20 and another to 0.25. That should work but strikes me as unusual, and
> makes me wonder whether this was what you really intended. It would be more
> common in a CFA to fix the variance of each factor to 1, and let the factor
> loadings be free parameters. Then the factor covariance would be their
> correlation. 
> 
> You should not have to specify start values for free parameters (such as
> g11, g22, and g12 in your model), though it is not wrong to do so. I would
> not, however, specify start values that imply a singular covariance matrix
> among the factors, as you've done; I'm surprised that the program was able
> to get by the start values to produce a solution.
> 
> BTW, the Thurstone example in ?sem is for a confirmatory factor analysis
> (albeit a slightly more complicated one with a second-order factor). There's
> also an example of a one-factor CFA in the paper at
> <http://socserv.socsci.mcmaster.ca/jfox/Misc/sem/SEM-paper.pdf>, though this
> is for ordinal observed variables.
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 

Thanks for the information. I think I understand how to handle the
residual variance after reading the sem help file more carefully. Now I
have to figure out how to constrain each column of the factor matrix to
sum to one. Maybe this will fix the problem with being under-identified.

Rick B.


From phhs80 at gmail.com  Wed Aug 16 19:21:57 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 16 Aug 2006 18:21:57 +0100
Subject: [R] Autocompletion
In-Reply-To: <765e0a3b0608161009l55794f86t2dd841cb1dc929@mail.gmail.com>
References: <765e0a3b0608161009l55794f86t2dd841cb1dc929@mail.gmail.com>
Message-ID: <6ade6f6c0608161021s48128c9bh811bc231af83c74d@mail.gmail.com>

On 8/16/06, ?ttar ?sberg <ottaris at gmail.com> wrote:
> I may be guilty of not doing my homework, but still, I've searched. I'm a
> relative newcomer to R (my forte is at present MATLAB, but for various
> reasons I'm trying to get literate in R). My question is: Is there an
> autocompletion feature buried somewhere in R?

What do you mean precisely by 'autocompletion'? Are working on MS
Windows or on Linux?

Paul


From mr.blacksheep at gmail.com  Wed Aug 16 19:26:16 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Wed, 16 Aug 2006 11:26:16 -0600
Subject: [R] Autocompletion
In-Reply-To: <765e0a3b0608161009l55794f86t2dd841cb1dc929@mail.gmail.com>
References: <765e0a3b0608161009l55794f86t2dd841cb1dc929@mail.gmail.com>
Message-ID: <46a360560608161026s301bf7f4y39799661d6a0c770@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/421fa53c/attachment.pl 

From bibiko at eva.mpg.de  Wed Aug 16 19:29:16 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 16 Aug 2006 19:29:16 +0200
Subject: [R] Problem with the special argument '...' within a function
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB32A2@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB32A2@usctmx1106.merck.com>
Message-ID: <20060816192916.csrraauvbpc0sk04@email.eva.mpg.de>

Quoting "Liaw, Andy" <andy_liaw at merck.com>:

> I'm not sure if this is what you want, but simply add ... to the list of
> arguments for fun1 and fun2 would eliminate the error.
>
> Andy
>

That's it!!

Thank you very much!!

Best,

Hans


>> Dear all,
>>
>> I wrote some functions using the special argument '...'. OK, it works.
>>
>> But if I call such a function which also called such a
>> function, then I get an error message about unused arguments.
>>
>> Here's an example:
>>
>> fun1 <- function(x,a=1)
>> {
>> 	print(paste("x=",x))
>> 	print(paste("a=",a))
>> }
>> fun2 <- function(y,b=2)
>> {
>> 	print(paste("y=",y))
>> 	print(paste("b=",b))
>> }
>> myfun <- function(c, ...)
>> {
>> 	print(paste("c=",c))
>> 	fun1(x=c,...)
>> 	fun2(y=c,...)
>> }
>>
>> This is OK.
>>  > myfun(c=3)
>> [1] "c= 3"
>> [1] "x= 3"
>> [1] "a= 1"
>> [1] "y= 3"
>> [1] "b= 2"
>>
>>  > myfun(c=3,a=4)
>> [1] "c= 3"
>> [1] "x= 3"
>> [1] "a= 4"
>> Error in fun2(y = c, ...) : unused argument(s) (a ...)
>>
>> I understand the error message because fun2 has no argument
>> called 'a'.
>>
>> But how can I avoid this???
>>
>> I want to use this in order to be able to call myfun() with
>> all arguments to control myfun(),fun1(), and fun2().
>>
>> Please help!
>>
>> Thanks,
>>
>> Hans
>>


From ottaris at gmail.com  Wed Aug 16 19:31:14 2006
From: ottaris at gmail.com (=?ISO-8859-1?Q?=D3ttar_=CDsberg?=)
Date: Wed, 16 Aug 2006 17:31:14 +0000
Subject: [R] Autocompletion
Message-ID: <765e0a3b0608161031j47c961b9ofdd611692338752b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/d06d02c4/attachment.pl 

From ssj1364 at gmail.com  Wed Aug 16 19:36:26 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Wed, 16 Aug 2006 11:36:26 -0600
Subject: [R]  separate row averages for different parts of an array
Message-ID: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/3b99cd3b/attachment.pl 

From arnau.mir at gmail.com  Wed Aug 16 19:38:27 2006
From: arnau.mir at gmail.com (Arnau Mir Torres)
Date: Wed, 16 Aug 2006 19:38:27 +0200
Subject: [R] help about agnes
Message-ID: <44E35813.4070600@mnm.uib.es>

Hello.

I have the following distance matrix between 8 points:
     
[1,] 0.000000 3.162278 7.280110 8.544004 7.071068 9.899495 6.403124 8.062258
[2,] 3.162278 0.000000 5.000000 6.403124 4.472136 8.944272 6.082763 8.062258
[3,] 7.280110 5.000000 0.000000 1.414214 1.000000 5.000000 4.242641 5.830952
[4,] 8.544004 6.403124 1.414214 0.000000 2.236068 4.123106 4.472136 5.656854
[5,] 7.071068 4.472136 1.000000 2.236068 0.000000 6.000000 5.000000 6.708204
[6,] 9.899495 8.944272 5.000000 4.123106 6.000000 0.000000 3.605551 3.000000
[7,] 6.403124 6.082763 4.242641 4.472136 5.000000 3.605551 0.000000 2.000000
[8,] 8.062258 8.062258 5.830952 5.656854 6.708204 3.000000 2.000000 0.000000

I want to apply the cluster algorithm using single linkage procedure. 
The metric is the euclidean metric.

In order to do this, I do:

aux=agnes(xMatrix, diss = 
inherits(xMatrix,"dist"),metric="euclidean",method="single")

Next, I do

plot(aux)

because I want to view the dendogram.
My question is about the graph of the dendogram.
What means the number "height" that appears on the left hand of it?
My assumption was it was the distance between clusters but I was wrong 
because
the distance matrices between the clusters are the following:

Join clusters {3} and {5} (distance=1) New matrix distance:

[1,] 0.000000 3.162278 7.071068 8.544004 9.899495 6.403124 8.062258
[2,] 3.162278 0.000000 4.472136 6.403124 8.944272 6.082763 8.062258
[3,] 7.071068 4.472136 0.000000 1.414214 5.000000 4.242641 5.830952
[4,] 8.544004 6.403124 1.414214 0.000000 4.123106 4.472136 5.656854
[5,] 9.899495 8.944272 5.000000 4.123106 0.000000 3.605551 3.000000
[6,] 6.403124 6.082763 4.242641 4.472136 3.605551 0.000000 2.000000
[7,] 8.062258 8.062258 5.830952 5.656854 3.000000 2.000000 0.000000

Join clusters {3,5} and {4} (distance=1.414214). New matrix distance:

[1,] 0.000000 3.162278 7.071068 9.899495 6.403124 8.062258
[2,] 3.162278 0.000000 4.472136 8.944272 6.082763 8.062258
[3,] 7.071068 4.472136 0.000000 4.123106 4.242641 5.656854
[4,] 9.899495 8.944272 4.123106 0.000000 3.605551 3.000000
[5,] 6.403124 6.082763 4.242641 3.605551 0.000000 2.000000
[6,] 8.062258 8.062258 5.656854 3.000000 2.000000 0.000000

Join clusters {7} and {8} (distance = 2). New matrix distance:

[1,] 0.000000 3.162278 7.071068 9.899495 6.403124
[2,] 3.162278 0.000000 4.472136 8.944272 6.082763
[3,] 7.071068 4.472136 0.000000 4.123106 4.242641
[4,] 9.899495 8.944272 4.123106 0.000000 3.000000
[5,] 6.403124 6.082763 4.242641 3.000000 0.000000

etc...
but in the graph of the dendogram, it appears the following numbers when 
it joins the clusters:

cluster {3} and {5}:  more or less 2.3
cluster {3,5} and {4}: more or less 3
cluster {7} and {8}: more or less 4.75.

As you can see, these numbers are distint from the distance between 
clusters (1, 1.414214 and 2).

So, can somebody say me what do these numbers represent?

Thanks in advance,

Arnau.


From singularitaet at gmx.net  Wed Aug 16 19:50:45 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 16 Aug 2006 19:50:45 +0200
Subject: [R] matching pairs in a Dataframe?
In-Reply-To: <44E34FA8.3090205@gmx.net>
References: <44E34FA8.3090205@gmx.net>
Message-ID: <44E35AF5.40307@gmx.net>

I found a solution:

pairs<-merge(subset(data.frame,cond==2,select=c(group,x),subset(data.frame,cond==3,select=c(group,x)),by=c("group"))

That yields something like

group   cond.x   cond.y
A      4      6.5
D      5      6
E      4      6


now I just need to figure out how I embed this into a function together
with the test, so that I don't have to write to much ....

Stefan Grosse

> Dear list,
>
> I want to extract pairs of values out of a dataframe where one
> criteria/condition does match.
>
> I have an experiment with 3 conditions which were not always applied:
>
> e.g.:
>
> group   cond   x
> A         1         2
> A         2         4
> A         3         6.5
> B         1         3
> B         2         4.5
> C         1         2.5
> C         3         4
> D         2         5
> D         3         6
> E         1         1
> E         2         4
> E         3         6
>
>
> Now I wanted to extract the x of those groups where condition 2 and
> condition 3 do both exist.
>
> In this example that would be groups A, D and E and the extracted pairs
> e.g.:
> cond2   cond3
> 4         6.5
> 5         6
> 4         6
>
>


From deepayan.sarkar at gmail.com  Wed Aug 16 19:58:42 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 16 Aug 2006 12:58:42 -0500
Subject: [R] adding multiple fitted curves to xyplot graph
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D433590259CD@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D433590259CD@srv-laminiere.arvalis-fr.com>
Message-ID: <eb555e660608161058i2fa7b9fcw5098e7285538ec19@mail.gmail.com>

On 8/16/06, GOUACHE David <D.GOUACHE at arvalisinstitutduvegetal.fr> wrote:
> Hello RHelpers,
>
> This may already have been answered, but despite days of scouring through the archives I haven't found it.
> My goal is to add multiple fitted curves to a plot.
> An example data set (a data frame named df in following code) is:
>
>              x1               y1     factor1
> 4       1298.25       0.00000000           1
> 5       1393.25       0.00000000           1
> 6       1471.50       0.04597701           1
> 7       1586.70       2.56908046           1
> 8       1692.10      11.14080460           1
> 9       1832.55      45.50459770           1
> 10      1928.30      65.56000000           1
> 11      2092.40     100.00000000           1
> 31      1202.90       0.00000000           2
> 41      1298.25       0.00000000           2
> 51      1393.25       0.37885057           2
> 61      1471.50       0.76839080           2
> 71      1586.70       7.75206897           2
> 81      1692.10      50.19448276           2
> 91      1832.55      94.08045977           2
> 101     1928.30     100.00000000           2
> 111     2092.40     100.00000000           2
> 14      1028.50       0.11111111           3
> 22      1106.40       0.04938272           3
> 32      1202.90       0.03448276           3
> 42      1298.25       0.34482759           3
> 52      1393.25       1.43850575           3
> 62      1471.50       1.96850575           3
> 72      1586.70      36.80597701           3
> 82      1692.10      92.83390805           3
> 92      1832.55     100.00000000           3
> 15      1028.50       0.09638554           4
> 23      1106.40       0.39988506           4
> 33      1202.90       0.49321839           4
> 43      1298.25       1.66045977           4
> 53      1393.25       7.51137931           4
> 63      1471.50      42.02724138           4
> 73      1586.70      99.12068966           4
> 83      1692.10     100.00000000           4
>
> I plot this with xyplot:
>
> trellis.par.set("background","white")

This doesn't do what you probably think it does.

> trellis.par.set(list(superpose.symbol=list(pch=c(15:17,21,25))))
> xyplot(y1 ~ x1, data=df, groups=factor1,
>             type = "p",
>             auto.key =
>             list(space = "right", points = TRUE, lines = FALSE))
>
> For each level of factor1 I fit a growth curve:
>
> fit.curve<-function(tab)
> {
>         res.fit<-nls(y1 ~ 100/(1+exp(((-log(81))/a)*(x1-b))), start=list(a=min(tab$x1[tab$y1>76],na.rm=T)-max(tab$x1[tab$y1<15],na.rm=T),b=tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)][!is.na(tab$x1[abs(tab$y1-50)==min(abs(tab$y1-50),na.rm=T)])]),data=tab)
>         coef(res.fit)
> }
> by(df,list(df$factor1),fit.curve)
>
> I would like to add the 4 curves corresponding to these 4 fits to my graphic.
> The elegant way would be a custom panel function I suppose, but I haven't been able to write one up...
> Could someone help me out on this please?

Here's one solution:

xyplot(y1 ~ x1, data=df, groups=factor1,
       type = "p",
       panel = panel.superpose,
       panel.groups = function(x, y, ...) {
           panel.xyplot(x, y, ...)
           fm <-
               nls(y ~ 100 / (1 + exp(((-log(81)) / a) * (x - b))),
                   start =
                   list(a = min(x[y > 76], na.rm = TRUE) -
                        max(x[y < 15], na.rm = TRUE),
                        b = x[abs(y-50) == min(abs(y - 50),
                                 na.rm = TRUE)][
                                 !is.na(x[abs(y - 50) == min(abs(y-50),
                                             na.rm = TRUE)])]))
           fit.fun <- function(x) predict(fm, newdata = list(x = x))
           panel.curve(fit.fun, ...)
       },
       auto.key =
       list(space = "right", points = TRUE, lines = FALSE))


-Deepayan


From mschwartz at mn.rr.com  Wed Aug 16 20:35:18 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 16 Aug 2006 13:35:18 -0500
Subject: [R] separate row averages for different parts of an array
In-Reply-To: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>
References: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>
Message-ID: <1155753318.3894.46.camel@localhost.localdomain>

On Wed, 2006-08-16 at 11:36 -0600, Spencer Jones wrote:
> I have an array with 44800 columns and 24 rows I would like to compute the
> row average for the array 100 columns at a time, so I would like to end up
> with an array of 24 rows x 448 columns. I have tried using apply(dataset, 1,
> function(x) mean(x[])), but I am not sure how to get it to take the average
> 100 columns at a time. Any ideas would be  welcomed.
> 
> thanks,
> 
> Spencer

Something along the lines of the following, presuming that 'mat' is your
24 * 44800 matrix:

  sapply(seq(1, 44800, 100), function(x) rowMeans(mat[, x:(x + 99)]))


The first argument in sapply() creates a sequence from 1:44800 by
increments of 100. 

sapply() then passes each value in the sequence as the starting index
value 'x' to use to subset 'mat' in 100 column sets and gets the
rowMeans() for each sub-matrix.

The returned object will be a 24 * 448 matrix.

HTH,

Marc Schwartz


From HDoran at air.org  Wed Aug 16 20:43:49 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 16 Aug 2006 14:43:49 -0400
Subject: [R] read.csv issue
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/956d4642/attachment.pl 

From andy_liaw at merck.com  Wed Aug 16 20:50:55 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Aug 2006 14:50:55 -0400
Subject: [R] Autocompletion
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB331A@usctmx1106.merck.com>

From: ?ttar ?sberg
> Hi!
> 
> That was quick, and thanks. I'm afraid I wasn't precise 
> enough. I'm using Windows XP and by autocompletion I mean 
> typing the first few letters of a command and then have the 
> system either complete the command or give you possible 

R under Emacs/ESS has autocompletion.  I believe JGR also has it.  Since you
mention Matlab and are using WinXP, JGR is probably closer to what you're
expecting.  See http://www.rosuda.org/software/JGR/.

> options, as in MATLAB or, for that matter, UNIX.

I guess you meant something like the bash shell, instead of "UNIX"?

Andy
 
> Cheers
> 
> ?ttar
> 
> 	[[alternative HTML version deleted]]
> 
>


From phhs80 at gmail.com  Wed Aug 16 20:53:38 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 16 Aug 2006 19:53:38 +0100
Subject: [R] Autocompletion
In-Reply-To: <765e0a3b0608161031j47c961b9ofdd611692338752b@mail.gmail.com>
References: <765e0a3b0608161031j47c961b9ofdd611692338752b@mail.gmail.com>
Message-ID: <6ade6f6c0608161153x2fbec533p80fe7ff1ef96a5ce@mail.gmail.com>

On 8/16/06, ?ttar ?sberg <ottaris at gmail.com> wrote:
> That was quick, and thanks. I'm afraid I wasn't precise enough. I'm using
> Windows XP and by autocompletion I mean typing the first few letters of a
> command and then have the system either complete the command or give you
> possible options, as in MATLAB or, for that matter, UNIX.

Regarding Windows XP, I do not know. However, on Linux, one presses
CTRL+R and R tries to complete the command with a command previously
used. Notwithstanding, I am sure that one can use the arrows keys on
Windows XP to invoke commands from a list of already used commands.
Try both ways.

Paul


From cgb at datanalytics.com  Wed Aug 16 21:02:37 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Wed, 16 Aug 2006 21:02:37 +0200
Subject: [R] read.csv issue
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
Message-ID: <1155754957.4359.2.camel@lenin>

Dear Harold,

One thing you can do is to read the file "plainly", even if the "\" is
lost and then, inside R, change the string value with gsub.

Sincerely,

Carlos J. Gil Bellosta

http://www.datanalytics.com
http://www.data-mining-blog.com


El mi?, 16-08-2006 a las 14:43 -0400, Doran, Harold escribi?:
> I'm trying to read in some data from a .csv format and have come across
> the following issue. Here is a simple example for replication
> 
> # A sample .csv format
> schid,sch_name
> 331-802-7081,School One
> 464-551-7357,School Two
> 388-517-7627,School Three \& Four
> 388-517-4394,School Five
> 
> Note the third line includes the \ character. However, when I read the
> data in I get
> 
> > read.csv(file.choose())
>          schid              sch_name
> 1 331-802-7081            School One
> 2 464-551-7357            School Two
> 3 388-517-7627 School Three & Four
> 4 388-517-4394           School Five
> 
> It turns out to be very important to read in this character as I have a
> program that loops through a data set and Sweave's about 30,000 files.
> The variable sch_name gets dropped into the tex file using
> \Sexpr{tmp$sch_name}. However, if there is an &, the latex file won't
> compile properly. So, what I need is for the data to be read in as
> 
>          schid              sch_name
> 1 331-802-7081            School One
> 2 464-551-7357            School Two
> 3 388-517-7627 School Three \& Four
> 4 388-517-4394           School Five
> 
> I am obligated by a client to include the & in the school name, so
> eliminating that isn't an option. I thought maybe comment.char or quote
> would be what I needed, but they didn't resolve the issue. I'm certain
> I'm missing something simple, I just can't see it.
> 
> Any thoughts?
> 
> Harold
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Wed Aug 16 21:09:33 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 16 Aug 2006 15:09:33 -0400
Subject: [R] read.csv issue
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
Message-ID: <644e1f320608161209r613049ffp264ba27218069b18@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/ec9a5b6f/attachment.pl 

From mschwartz at mn.rr.com  Wed Aug 16 21:10:14 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 16 Aug 2006 14:10:14 -0500
Subject: [R] read.csv issue
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
Message-ID: <1155755414.3894.61.camel@localhost.localdomain>

On Wed, 2006-08-16 at 14:43 -0400, Doran, Harold wrote:
> I'm trying to read in some data from a .csv format and have come across
> the following issue. Here is a simple example for replication
> 
> # A sample .csv format
> schid,sch_name
> 331-802-7081,School One
> 464-551-7357,School Two
> 388-517-7627,School Three \& Four
> 388-517-4394,School Five
> 
> Note the third line includes the \ character. However, when I read the
> data in I get
> 
> > read.csv(file.choose())
>          schid              sch_name
> 1 331-802-7081            School One
> 2 464-551-7357            School Two
> 3 388-517-7627 School Three & Four
> 4 388-517-4394           School Five
> 
> It turns out to be very important to read in this character as I have a
> program that loops through a data set and Sweave's about 30,000 files.
> The variable sch_name gets dropped into the tex file using
> \Sexpr{tmp$sch_name}. However, if there is an &, the latex file won't
> compile properly. So, what I need is for the data to be read in as
> 
>          schid              sch_name
> 1 331-802-7081            School One
> 2 464-551-7357            School Two
> 3 388-517-7627 School Three \& Four
> 4 388-517-4394           School Five
> 
> I am obligated by a client to include the & in the school name, so
> eliminating that isn't an option. I thought maybe comment.char or quote
> would be what I needed, but they didn't resolve the issue. I'm certain
> I'm missing something simple, I just can't see it.
> 
> Any thoughts?
> 
> Harold

Harold,

What version of R and OS are you running?

Under:

 Version 2.3.1 Patched (2006-08-06 r38829)

 on FC5:

> read.csv("test.csv")
         schid              sch_name
1 331-802-7081            School One
2 464-551-7357            School Two
3 388-517-7627 School Three \\& Four
4 388-517-4394           School Five

The '\' is doubled.

Take note of the impact of the 'allowEscapes' argument:

> read.csv("test.csv", allowEscapes = TRUE)
         schid            sch_name
1 331-802-7081          School One
2 464-551-7357          School Two
3 388-517-7627 School Three & Four
4 388-517-4394         School Five

The '\' is lost.

Try it with 'allowEscapes = FALSE' explicitly.

HTH,

Marc Schwartz


From HDoran at air.org  Wed Aug 16 21:15:43 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 16 Aug 2006 15:15:43 -0400
Subject: [R] read.csv issue
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2770D5@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/8e78631b/attachment.pl 

From r-help at botelho-machado.de  Wed Aug 16 21:23:33 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Wed, 16 Aug 2006 21:23:33 +0200
Subject: [R] Plots Without Displaying
Message-ID: <44E370B5.7010500@botelho-machado.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

R Help Mailing List,


I'd like to generate a plot that I could display and/or store it as e.g.
jpeg. But unfortunately always a plotting window opens. Is it possible
to prevent that?

I tried the following:
R> bp<-boxplot( sample(100), plot=FALSE)

This works somehow, but it only stores data (as discribed in the help)
in bp and it is not possible afaik to display bp later on or store them
as a jpeg.

The next:
R> p<-plot(sample(100), sample(100), plot=FALSE)
..and also a variant using jpeg() didn't work at all.

Is there a way to generally store the plots as object, without
displaying them, or perhaps directly saving them to disc as jpeg?

A "Yes" or "No" or any further help/links are appreciated!!!

TIA,
 Lothar
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFE43C1HRf7N9c+X7sRAmCqAKCN9PpAEqnQ1hJHjrDKDat49ulHPQCfRVUW
+N9AtKrFxcs/kAdSQ7iV4yk=
=Tybe
-----END PGP SIGNATURE-----


From ripley at stats.ox.ac.uk  Wed Aug 16 21:23:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Aug 2006 20:23:22 +0100 (BST)
Subject: [R] read.csv issue
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
Message-ID: <Pine.LNX.4.64.0608162020180.8277@gannet.stats.ox.ac.uk>

Set allowEscapes = FALSE when reading. See the help page for more details.

There is perhaps an argument for changing the default for allowEscapes 
under read.csv, especially as people have now changed that for 
comment.char (in R-devel).

On Wed, 16 Aug 2006, Doran, Harold wrote:

> I'm trying to read in some data from a .csv format and have come across
> the following issue. Here is a simple example for replication
> 
> # A sample .csv format
> schid,sch_name
> 331-802-7081,School One
> 464-551-7357,School Two
> 388-517-7627,School Three \& Four
> 388-517-4394,School Five
> 
> Note the third line includes the \ character. However, when I read the
> data in I get
> 
> > read.csv(file.choose())
>          schid              sch_name
> 1 331-802-7081            School One
> 2 464-551-7357            School Two
> 3 388-517-7627 School Three & Four
> 4 388-517-4394           School Five
> 
> It turns out to be very important to read in this character as I have a
> program that loops through a data set and Sweave's about 30,000 files.
> The variable sch_name gets dropped into the tex file using
> \Sexpr{tmp$sch_name}. However, if there is an &, the latex file won't
> compile properly. So, what I need is for the data to be read in as
> 
>          schid              sch_name
> 1 331-802-7081            School One
> 2 464-551-7357            School Two
> 3 388-517-7627 School Three \& Four
> 4 388-517-4394           School Five
> 
> I am obligated by a client to include the & in the school name, so
> eliminating that isn't an option. I thought maybe comment.char or quote
> would be what I needed, but they didn't resolve the issue. I'm certain
> I'm missing something simple, I just can't see it.
> 
> Any thoughts?
> 
> Harold
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Aug 16 21:25:38 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Aug 2006 20:25:38 +0100 (BST)
Subject: [R] read.csv issue
In-Reply-To: <Pine.LNX.4.64.0608162020180.8277@gannet.stats.ox.ac.uk>
References: <2323A6D37908A847A7C32F1E3662C80E2770CE@dc1ex01.air.org>
	<Pine.LNX.4.64.0608162020180.8277@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0608162024220.8277@gannet.stats.ox.ac.uk>

On Wed, 16 Aug 2006, Prof Brian Ripley wrote:

> Set allowEscapes = FALSE when reading. See the help page for more details.
> 
> There is perhaps an argument for changing the default for allowEscapes 
> under read.csv, especially as people have now changed that for 
> comment.char (in R-devel).

Oops, it was already changed in 2.2.0.  What version of R is this?

> On Wed, 16 Aug 2006, Doran, Harold wrote:
> 
> > I'm trying to read in some data from a .csv format and have come across
> > the following issue. Here is a simple example for replication
> > 
> > # A sample .csv format
> > schid,sch_name
> > 331-802-7081,School One
> > 464-551-7357,School Two
> > 388-517-7627,School Three \& Four
> > 388-517-4394,School Five
> > 
> > Note the third line includes the \ character. However, when I read the
> > data in I get
> > 
> > > read.csv(file.choose())
> >          schid              sch_name
> > 1 331-802-7081            School One
> > 2 464-551-7357            School Two
> > 3 388-517-7627 School Three & Four
> > 4 388-517-4394           School Five
> > 
> > It turns out to be very important to read in this character as I have a
> > program that loops through a data set and Sweave's about 30,000 files.
> > The variable sch_name gets dropped into the tex file using
> > \Sexpr{tmp$sch_name}. However, if there is an &, the latex file won't
> > compile properly. So, what I need is for the data to be read in as
> > 
> >          schid              sch_name
> > 1 331-802-7081            School One
> > 2 464-551-7357            School Two
> > 3 388-517-7627 School Three \& Four
> > 4 388-517-4394           School Five
> > 
> > I am obligated by a client to include the & in the school name, so
> > eliminating that isn't an option. I thought maybe comment.char or quote
> > would be what I needed, but they didn't resolve the issue. I'm certain
> > I'm missing something simple, I just can't see it.
> > 
> > Any thoughts?
> > 
> > Harold
> > 
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From HDoran at air.org  Wed Aug 16 21:27:21 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 16 Aug 2006 15:27:21 -0400
Subject: [R] read.csv issue
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2770D8@dc1ex01.air.org>

Well, for this particular program I am using 2.1.1, though I normally
use 2.3.0. Long story about why the old version is used, but I must for
this particular program. 

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Wednesday, August 16, 2006 3:26 PM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] read.csv issue
> 
> On Wed, 16 Aug 2006, Prof Brian Ripley wrote:
> 
> > Set allowEscapes = FALSE when reading. See the help page 
> for more details.
> > 
> > There is perhaps an argument for changing the default for 
> allowEscapes 
> > under read.csv, especially as people have now changed that for 
> > comment.char (in R-devel).
> 
> Oops, it was already changed in 2.2.0.  What version of R is this?
> 
> > On Wed, 16 Aug 2006, Doran, Harold wrote:
> > 
> > > I'm trying to read in some data from a .csv format and have come 
> > > across the following issue. Here is a simple example for 
> replication
> > > 
> > > # A sample .csv format
> > > schid,sch_name
> > > 331-802-7081,School One
> > > 464-551-7357,School Two
> > > 388-517-7627,School Three \& Four
> > > 388-517-4394,School Five
> > > 
> > > Note the third line includes the \ character. However, 
> when I read 
> > > the data in I get
> > > 
> > > > read.csv(file.choose())
> > >          schid              sch_name
> > > 1 331-802-7081            School One
> > > 2 464-551-7357            School Two
> > > 3 388-517-7627 School Three & Four
> > > 4 388-517-4394           School Five
> > > 
> > > It turns out to be very important to read in this character as I 
> > > have a program that loops through a data set and Sweave's 
> about 30,000 files.
> > > The variable sch_name gets dropped into the tex file using 
> > > \Sexpr{tmp$sch_name}. However, if there is an &, the latex file 
> > > won't compile properly. So, what I need is for the data 
> to be read 
> > > in as
> > > 
> > >          schid              sch_name
> > > 1 331-802-7081            School One
> > > 2 464-551-7357            School Two
> > > 3 388-517-7627 School Three \& Four
> > > 4 388-517-4394           School Five
> > > 
> > > I am obligated by a client to include the & in the school 
> name, so 
> > > eliminating that isn't an option. I thought maybe comment.char or 
> > > quote would be what I needed, but they didn't resolve the 
> issue. I'm 
> > > certain I'm missing something simple, I just can't see it.
> > > 
> > > Any thoughts?
> > > 
> > > Harold
> > > 
> > > 
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide 
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > > 
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From ripley at stats.ox.ac.uk  Wed Aug 16 21:47:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Aug 2006 20:47:26 +0100 (BST)
Subject: [R] Plots Without Displaying
In-Reply-To: <44E370B5.7010500@botelho-machado.de>
References: <44E370B5.7010500@botelho-machado.de>
Message-ID: <Pine.LNX.4.64.0608162043130.8277@gannet.stats.ox.ac.uk>

Yes, see

?jpeg
?bitmap

and as you didn't tell us your OS we don't know if these are available to 
you.

jpeg(file="test.jpg")
boxplot(sample(100))
dev.off()

may well work.

'An Introduction to R' explains about graphics devices, including these.


On Wed, 16 Aug 2006, Lothar Botelho-Machado wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> R Help Mailing List,
> 
> 
> I'd like to generate a plot that I could display and/or store it as e.g.
> jpeg. But unfortunately always a plotting window opens. Is it possible
> to prevent that?
> 
> I tried the following:
> R> bp<-boxplot( sample(100), plot=FALSE)
> 
> This works somehow, but it only stores data (as discribed in the help)
> in bp and it is not possible afaik to display bp later on or store them
> as a jpeg.
> 
> The next:
> R> p<-plot(sample(100), sample(100), plot=FALSE)
> ..and also a variant using jpeg() didn't work at all.
> 
> Is there a way to generally store the plots as object, without
> displaying them, or perhaps directly saving them to disc as jpeg?
> 
> A "Yes" or "No" or any further help/links are appreciated!!!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From PAlspach at hortresearch.co.nz  Wed Aug 16 22:01:35 2006
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 17 Aug 2006 08:01:35 +1200
Subject: [R] confusing about contrasts concept
Message-ID: <EC0F8FF776F3F74E9C63CE16641C9628016044F1@AKLEXB02.hort.net.nz>


Tian

Bill Venables wrote an excellent explanation to the S list back in 1997.
I saved it as a pdf file and attach it herewith ...

Peter Alspach
  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of T Mu
> Sent: Thursday, 17 August 2006 3:23 a.m.
> To: R-Help
> Subject: [R] confusing about contrasts concept
>
> Hi all,
>
> Where can I find a thorough explanation of Contrasts and
> Contrasts Matrices?
> I read some help but still confused.
>
> Thank you,
> Tian
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


______________________________________________________

The contents of this e-mail are privileged and/or confidential to the
named recipient and are not to be used by any other person and/or
organisation. If you have received this e-mail in error, please notify
the sender and delete all material pertaining to this e-mail.
______________________________________________________

From rmh at temple.edu  Wed Aug 16 22:20:32 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 16 Aug 2006 16:20:32 -0400 (EDT)
Subject: [R] Autocompletion
Message-ID: <20060816162032.BGB07145@po-d.temple.edu>

For autocompletion in ESS, press the TAB key.


From PAlspach at hortresearch.co.nz  Wed Aug 16 22:19:36 2006
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 17 Aug 2006 08:19:36 +1200
Subject: [R] confusing about contrasts concept [long]
Message-ID: <EC0F8FF776F3F74E9C63CE16641C9628016044FF@AKLEXB02.hort.net.nz>


Tian

It appears the attachment might not have worked so I'll embed Bill's
message at the end.

Peter Alspach


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Alspach
> Sent: Thursday, 17 August 2006 8:02 a.m.
> To: T Mu; R-Help
> Subject: Re: [R] confusing about contrasts concept
>
>
> Tian
>
> Bill Venables wrote an excellent explanation to the S list
> back in 1997.
> I saved it as a pdf file and attach it herewith ...
>
> Peter Alspach
>  
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
>
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of T Mu
> > Sent: Thursday, 17 August 2006 3:23 a.m.
> > To: R-Help
> > Subject: [R] confusing about contrasts concept
> >
>
> > Hi all,
> >
>
> > Where can I find a thorough explanation of Contrasts and
>
> > Contrasts Matrices?
> > I read some help but still confused.
> >
>
> > Thank you,
> > Tian


Date sent:	Sat, 29 Mar 1997 17:23:31 +1030
From:		Bill Venables <wvenable at attunga.stats.adelaide.edu.au>
To:		Wei Qian <wwqian at venus.monsanto.com>
Copies to:	S-news at utstat.toronto.edu
Subject:	Re: test contrasts [long]

Wei Qian writes:

I am new to Splus, so please don't be offended if my question is too
naive.

We're all friends here, Wei. It's not that kind of group...mostly.

Does anyone know how to test contrasts in Splus? To be specific, suppose
I have a one-way layout experiment with 3 treatments, and I want to test
the contrasts of treatment 1 vs. treatment 2, treatment 1 vs. treatment
3, etc. In SAS, I could use the following:

 proc gim;
   class trt;
  model y = trt;
  contrasts"! vs. 2" 1-10;
  contrasts "2 vs. 3" 10-1;
run;

 One way I can think of is that to construct my design matrix and obtain
the contrast sum of squares through a series of matrix operations, but
is there any easy way to do it or any built-in function in Splus can do
it?

The answer is 'yes' but hardly anyone seems to know how to do it. The
explanation in the 'white book' for example, seems a little incomplete
to me and not quite adequate to settle the case you raise. (The
explanation in the yellow book is also inadequate, but I hope come July
we will have fixed that.) Since this is one of the most frequent
questions people ask me in direct email, too, let me try (again) to sort
it out in some detail.

A formula such as y ~ f, where f is a factor in principle generates a
single classification model in the form

*y_{ij} == mu + phi_i + e_{ij}

Write the design matrix in the form X = [1 Xf], where, assuming f has p
levels, Xf is the n x p dummy variable (ie binary) matrix corresponding
to the phi_i's. So in matrix terms the model is written as

*y = 1 mu + Xf phi + e

(a)	If you remove the intercept term, using y ~ f -1, then Xf is the
design matrix you get and the coefficients correspond to the class
means;
(b)	If the intercept term is left in, then the design matrix X does
not have full column rank, so an adjustment has to be made to Xf to make
it so.

The redundancy comes about because the columns of Xf add to the the
1-vector, that is
Xf l_p = l_n. So let C be any p x (p -1) matrix such that [1 C] is
nonsingular. It can easily be seen that Xc = [1 (Xf C)] will have full
column rank and that fitting the model using this model matrix is
equivalent to the original redundantly specified model. The matrix C is
called the *coding matrix* for the factor f.

The linear model is actually fitted in the form

*y = 1 mu + (Xf C) alpha + e

where alpha has (p-1) components, of course. In order to make sense of
the alpha's we need to relate them back to the phi's.

For any such C there will be a vector, v, such the v'C = 0' (using ' for
transpose). (In fact v spans the orthogonal complement to the column
space of C). Clearly phi and alpha are related by

*C alpha = phi

but since v'C = 0', it follows that an identification constraint
applies, namely v'phi = 0. By multiplying both sides by (C'C)^{-1} C',
it also follows that

*alpha =(C'C)^{-1}C'phi

which provides an interpretation for the alpha's in terms of the
(constrained) phi's. For example take the Helmert contrasts.

> contr.helmert(4)
[,1]	[,2]	[,3]
1	-1	-1	-1
2	1	-1	-1
3	0	2	-1
4	0	0	3

The constraint vector is clearly v= (1,1,1,1), since the columns add to
zero. In this case the columns are also mutually orthogonal, so the
matrix (C'C^{-l} C' (the generalized inverse of C) has a similar form
apart from a few row divisors:

>fractions(ginverse(contr.helmert(4)))
[,1]	[,2]	[,3]	[,4]
[1,]	-1/2	1/2	0	0
[2,]	-1/6	-1/6	1/3	0
[3,]	-1/12	-1/12	-1/12	1/4

(ginverse() will be available in S+4.0 and fractions(), now available in
the MASS2 library, simply displays numbers in fractional form so that
patterns are more obvious).

Thus the phi's are identified by requiring that they add to zero, and

*alpha_l = (phi_2 - phi_l )/2,
*alpha_2 = [phi_3 - (phi_l + phi_2)/2] / 3

&c. When the columns of C are not mutually orthogonal the story is not
quite so obvious, though. For example take a C matrix using contr.sdif
(available in the MASS2 library)

> contr.sdif(4)
[2-1]	[3-2]	[4-3]
1	-0.75	-0.5	-0.25
2	0.25	-0.5	-0.25
3	0.25	0.5	-0.25
4	0.25	0.5	0.75

The column sums are all zero so the identification constraint is still
the same, but they are not mutually orthogonal. The coefficients do have
an easy interpretation, though:

> fractions(ginverse(contr. sdif(4)))
[,1]	[,2]	[,3]	[,4]
[l,]	-1	1	0	0
[2,]	0	-1	1	0
[3,]	0	0	-1	1

Thus alpha_i = phi_(i+l) - phi_i, the "successive differences" of phi's,
or equally of class means.

Now (at last) returning to your case. In specifying the contrasts you
want to have as the coefficients, you are specifying not C, but
(C'C^{-l} C' (say M). So what you need to do is work backwards, noting
that C is also the ginverse of M:

> M <- rbind(c(l,-l,0), c(l, 0, -1))
>M
[,1]	[,2]	[,3]
[1,]	1	-1	0
[2,]	1	0	-1
> fractions(ginverse(M))
[,1]	[,2]
[1,]	1/3	1/3
[2,]	-2/3	1/3
[3,]	1/3	-2/3

This is the C matrix you need to specify as the coding matrix. Note that
(of course) the columns add to zero so that the implied constraint on
the phi's is that they sum to zero.

Since ginverse() is not yet published code, here is a quick and dirty
version that is adequate for this kind of case (but don't push it...)

ginv <- function(M)
*if(nrow(M) < ncol(M)) t(ginv(t(M)))
*else solve(crossprod(M), t(M))

So all you need to do is as follows:

> M <- rbind(c(l, -1, 0), c(l, 0, -1))
> Cmat <- ginv(M)
> dimnames(Cmat) <- list(NULL, c('l vs 2', '1 vs 3')) # optional
> contrasts(trt) <- Cmat
> fm <- aov(y ~ trt, .....)

The easy way to see the single degrees of freedom components is:

> summary .lm(fm)$coef

and the partitioned anova table can be shown using:

> summary(fm, split = list(trt = list('l vs 2' = 1, '1 vs 3'= 2)))

Finally you can see the complete phi-style representation of the fitted
model by

> dummy.coef(fm)

(Unfortunately dummy.coef does not generate an effective variance matrix
for the coefficients, otherwise contrasts could be directly tested after
the fitting process. This could be fixed, though.)

Note that the standard contrast functions contr.helmert, contr.poly and
contr.sum (as well as out contr.sdif) all impose the same identification
constraint on the phi's, i.e. they add to 1, but contr.treatment is
different as it corresponds to phi_l = 0. So the phi parameters mean
different things in this case, but the same principle applies. If the
rows of M all add to zero (making them "conventional contrasts") so will
the columns of C = M'(MM')^{-1}, (obviously).

Bill Venables.

Bill Venables, Head, Dept of Statistics,	Tel.: +61 8 8303 5418
University of Adelaide,			Fax.: +61 8 8303 3696
South AUSTRALIA. 5005.			Email:
Bill.Venables at adelaide.edu.au


______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}


From muster at gmail.com  Wed Aug 16 22:25:47 2006
From: muster at gmail.com (T Mu)
Date: Wed, 16 Aug 2006 16:25:47 -0400
Subject: [R] confusing about contrasts concept
In-Reply-To: <EC0F8FF776F3F74E9C63CE16641C9628016044F1@AKLEXB02.hort.net.nz>
References: <AcbBSYp2f6rzM78mSimpwfDcM3W85AAJLJQg>
	<EC0F8FF776F3F74E9C63CE16641C9628016044F1@AKLEXB02.hort.net.nz>
Message-ID: <b68812e70608161325u79c86095j5690ee553f1a4fb1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/bd8721db/attachment.pl 

From r-help at botelho-machado.de  Wed Aug 16 22:49:00 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Wed, 16 Aug 2006 22:49:00 +0200
Subject: [R] Plots Without Displaying
In-Reply-To: <Pine.LNX.4.64.0608162043130.8277@gannet.stats.ox.ac.uk>
References: <44E370B5.7010500@botelho-machado.de>
	<Pine.LNX.4.64.0608162043130.8277@gannet.stats.ox.ac.uk>
Message-ID: <44E384BC.8090709@botelho-machado.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Prof Brian Ripley wrote:
> Yes, see
> 
> ?jpeg
> ?bitmap
> 
> and as you didn't tell us your OS we don't know if these are available to 
> you.
> 
> jpeg(file="test.jpg")
> boxplot(sample(100))
> dev.off()
> 
> may well work.
> 
> 'An Introduction to R' explains about graphics devices, including these.
> 
> 
> On Wed, 16 Aug 2006, Lothar Botelho-Machado wrote:
> 
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> R Help Mailing List,
>>
>>
>> I'd like to generate a plot that I could display and/or store it as e.g.
>> jpeg. But unfortunately always a plotting window opens. Is it possible
>> to prevent that?
>>
>> I tried the following:
>> R> bp<-boxplot( sample(100), plot=FALSE)
>>
>> This works somehow, but it only stores data (as discribed in the help)
>> in bp and it is not possible afaik to display bp later on or store them
>> as a jpeg.
>>
>> The next:
>> R> p<-plot(sample(100), sample(100), plot=FALSE)
>> ..and also a variant using jpeg() didn't work at all.
>>
>> Is there a way to generally store the plots as object, without
>> displaying them, or perhaps directly saving them to disc as jpeg?
>>
>> A "Yes" or "No" or any further help/links are appreciated!!!
> 
> 



Thank you for the explanation and your patience in answering me this
obviously very simple question!!

Originally I tried to store plots directly in a list. So writing them
directly to disc was just a good alternative. I knew that that jpeg()
provides functionality for that, but didn't use it correctly.

Hence, is it also possible to store a plot in a list, somehow?

Kind regards,
 Lothar
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFE44S8HRf7N9c+X7sRAgCpAKC3NhjCYwkteksOljUKWrO3166nCwCgsfLI
EPGVIoqc2dla5t6s9mmZQqE=
=h+Az
-----END PGP SIGNATURE-----


From jfox at mcmaster.ca  Wed Aug 16 23:01:53 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Aug 2006 17:01:53 -0400
Subject: [R] Specifying Path Model in SEM for CFA
In-Reply-To: <1155748034.3596.11.camel@localhost.localdomain>
Message-ID: <20060816210151.NOMD19825.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Rick,

It's unclear to me what you mean by constraining "each column of the factor
matrix to sum to one." If you intend to constrain the loadings on each
factor to sum to one, sem() won't do that, since it supports only equality
constraints, not general linear constraints on parameters of the model, but
why such a constraint would be reasonable in the first place escapes me.
More common in confirmatory factor analysis would be to constrain more of
the loadings to zero. Of course, one would do this only if it made
substantive sense in the context of the research.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Rick Bilonick [mailto:rab45+ at pitt.edu] 
> Sent: Wednesday, August 16, 2006 12:07 PM
> To: John Fox
> Cc: 'R Help'
> Subject: Re: [R] Specifying Path Model in SEM for CFA
> 
> On Wed, 2006-08-16 at 08:47 -0400, John Fox wrote:
> > Dear Rick,
> > 
> > There are a couple of problems here:
> > 
> > (1) You've fixed the error variance parameters for each of the 
> > observed variables to 1 rather than defining each as a free 
> parameter to estimate.
> > For example, use
> > 
> > X1 <-> X1, theta1, NA
> > 
> > Rather than
> > 
> > X1 <-> X1, NA, 1
> > 
> > The general principle is that if you give a parameter a 
> name, it's a 
> > free parameter to be estimated; if you give the name as NA, 
> then the 
> > parameter is given a fixed value (here, 1). (There is some more 
> > information on this and on error-variance parameters in ?sem.)
> > 
> > (2) I believe that the model you're trying to specify -- in 
> which all 
> > variables but X6 load on F1, and all variables but X1 load 
> on F2 -- is 
> > underidentified.
> > 
> > In addition, you've set the metric of the factors by fixing one 
> > loading to 0.20 and another to 0.25. That should work but 
> strikes me 
> > as unusual, and makes me wonder whether this was what you really 
> > intended. It would be more common in a CFA to fix the 
> variance of each 
> > factor to 1, and let the factor loadings be free 
> parameters. Then the 
> > factor covariance would be their correlation.
> > 
> > You should not have to specify start values for free 
> parameters (such 
> > as g11, g22, and g12 in your model), though it is not wrong 
> to do so. 
> > I would not, however, specify start values that imply a singular 
> > covariance matrix among the factors, as you've done; I'm surprised 
> > that the program was able to get by the start values to 
> produce a solution.
> > 
> > BTW, the Thurstone example in ?sem is for a confirmatory factor 
> > analysis (albeit a slightly more complicated one with a 
> second-order 
> > factor). There's also an example of a one-factor CFA in the 
> paper at 
> > <http://socserv.socsci.mcmaster.ca/jfox/Misc/sem/SEM-paper.pdf>, 
> > though this is for ordinal observed variables.
> > 
> > I hope this helps,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> 
> Thanks for the information. I think I understand how to 
> handle the residual variance after reading the sem help file 
> more carefully. Now I have to figure out how to constrain 
> each column of the factor matrix to sum to one. Maybe this 
> will fix the problem with being under-identified.
> 
> Rick B.
>


From christos at nuverabio.com  Wed Aug 16 23:10:37 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 16 Aug 2006 17:10:37 -0400
Subject: [R] Plots Without Displaying
In-Reply-To: <44E384BC.8090709@botelho-machado.de>
Message-ID: <001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>

Yes, you can do that for lattice-based plots.  The functions in the lattice
package produce objects of class "trellis" which can be stored in a list and
processed or updated at a later time:

library(lattice)
attach(barley)
plotList <- list(length=3)
plotList[[1]] <- xyplot(yield ~ site, data=barley)
plotList[[2]] <- xyplot(yield ~ variety, data=barley) 
plotList[[3]] <- xyplot(yield ~ year, data=barley)

plotList
plotList[[3]] <- update(plotList[[3]], yaxis="Yield (bushels/acre)")
print(plotList[[3]])

Obviously, you can store any lattice-based plot in the list.

HTH.

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lothar
Botelho-Machado
Sent: Wednesday, August 16, 2006 4:49 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Plots Without Displaying

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Prof Brian Ripley wrote:
> Yes, see
> 
> ?jpeg
> ?bitmap
> 
> and as you didn't tell us your OS we don't know if these are available 
> to you.
> 
> jpeg(file="test.jpg")
> boxplot(sample(100))
> dev.off()
> 
> may well work.
> 
> 'An Introduction to R' explains about graphics devices, including these.
> 
> 
> On Wed, 16 Aug 2006, Lothar Botelho-Machado wrote:
> 
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> R Help Mailing List,
>>
>>
>> I'd like to generate a plot that I could display and/or store it as e.g.
>> jpeg. But unfortunately always a plotting window opens. Is it 
>> possible to prevent that?
>>
>> I tried the following:
>> R> bp<-boxplot( sample(100), plot=FALSE)
>>
>> This works somehow, but it only stores data (as discribed in the 
>> help) in bp and it is not possible afaik to display bp later on or 
>> store them as a jpeg.
>>
>> The next:
>> R> p<-plot(sample(100), sample(100), plot=FALSE)
>> ..and also a variant using jpeg() didn't work at all.
>>
>> Is there a way to generally store the plots as object, without 
>> displaying them, or perhaps directly saving them to disc as jpeg?
>>
>> A "Yes" or "No" or any further help/links are appreciated!!!
> 
> 



Thank you for the explanation and your patience in answering me this
obviously very simple question!!

Originally I tried to store plots directly in a list. So writing them
directly to disc was just a good alternative. I knew that that jpeg()
provides functionality for that, but didn't use it correctly.

Hence, is it also possible to store a plot in a list, somehow?

Kind regards,
 Lothar
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFE44S8HRf7N9c+X7sRAgCpAKC3NhjCYwkteksOljUKWrO3166nCwCgsfLI
EPGVIoqc2dla5t6s9mmZQqE=
=h+Az
-----END PGP SIGNATURE-----

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ndesilsky at yahoo.com  Thu Aug 17 01:01:58 2006
From: ndesilsky at yahoo.com (Nick Desilsky)
Date: Wed, 16 Aug 2006 16:01:58 -0700 (PDT)
Subject: [R] bwplot in loop doesn't produce any output
Message-ID: <20060816230158.62986.qmail@web55510.mail.re4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/6c303fd0/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 17 01:04:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Aug 2006 19:04:45 -0400
Subject: [R] bwplot in loop doesn't produce any output
In-Reply-To: <20060816230158.62986.qmail@web55510.mail.re4.yahoo.com>
References: <20060816230158.62986.qmail@web55510.mail.re4.yahoo.com>
Message-ID: <971536df0608161604g73545537jf1bcb846e5d438b9@mail.gmail.com>

Its a FAQ

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f

On 8/16/06, Nick Desilsky <ndesilsky at yahoo.com> wrote:
> Hi,
>
>  running the following code by itself runs as expected.
>  ----------------------------------------------------------------------------
>  k <- 1
>  i <- 2
>  j <- 3
>  NumName <- varnames[num.cols[k]]
>  FacNames <- varnames[fac.cols[c(i,j)]]
>  tmp <- paste(FacNames[1],NumName,sep="~")
>  fml <- formula(paste(tmp,FacNames[2],sep="|"))
>  bwplot(fml, data = X)
>
>  ----------------------------------------------------------------------------
>  But when set into a loop, it doens't produce anything. I've tried sending the output to pdf(file="test.pdf"), and the pdf file stays empty.
>
>  for (i in 1:(lfc-1))
>  {
>  for (j in (i+1):lfc)
>  {
>  for (k in 1:lnc)
>  {
>  NumName <- varnames[num.cols[k]]
>  FacNames <- varnames[fac.cols[c(i,j)]]
>  tmp <- paste(FacNames[1],NumName,sep="~")
>  fml <- formula(paste(tmp,FacNames[2],sep="|"))
>  bwplot(fml, data = X)
>
>  }
>  }
>  }
>
>
>  Any thoughts ? And if you know how to unlock test.pdf for viewing while R is running, I'd appreciate this bit of info too (i've tried dev.off(), windows() and such, and test.pdf can only be viewed after shutting down R).
>
>  Thank you.
>
>  Nick.
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Thu Aug 17 01:57:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Aug 2006 16:57:01 -0700
Subject: [R] glmmPQL question!
In-Reply-To: <20060810154007.02D3929@lulu.it.northwestern.edu>
References: <20060810154007.02D3929@lulu.it.northwestern.edu>
Message-ID: <44E3B0CD.4010903@pdf.com>

  I don't think there is an easy way to do that. If it were my problem, 
I think I'd start by trying to the model using 'lmer' associated with 
the 'lme4' package. I would then try to pass the fit to 'mcmcsamp' to 
get a random sample of parameter estimates following the posterior 
distribution. Then I'd convert the simulated parameters into a 
distribution of predictions for the conditions of interest. While doing 
this, I'd apply "quantile" of the distribution of predictions for each 
set of conditions to get the desired confidence limits.

Make sense?

If you'd like further help from this listserve, please submit another 
post. When you do that, however, I strongly urge you to include 
commented, minimal, self-contained, reproducible code illustrating 
something you tried that didn't quite work to help people understand 
what you want (as suggested in the posting guide 
"www.R-project.org/posting-guide.html"). Without such a minimal, self 
contained example, your problem is rarely as clear, and people's replies 
are less likely to be relevant. Since they know that, they are less 
likely to reply.

Hope this helps.
Spencer Graves

m-krutky at northwestern.edu wrote:
> Hello Folks- 
>
> Is there a way to create confidence bands with 'glmmPQL' ??? 
>
> I am performing a stroke study for Northwestern University in Chicago, Illinois.  I am trying to
> decide a way to best plot the model which we created with the glmmPQL function in R.   I would like 
> to plot my actual averaged data points within 95 % confidence intervals from the model. Plotting
> the model is easy, but determining confidence bands is not. 
>
> Here is my model:
>
> ratiomodel<-glmmPQL(ratio~as.factor(joint)*time, random = ~ 1 | subject, family = Gamma(link =
> "identity"),alldata3)
>
> I am used to seeing confidence intervals from models that increase, ?flair out? in the y direction, 
> at the beginning and ending time points (x values) of the simulated data.  If I use 'lm' and pass
> the command 'int = "c" ' 'to create this model I can easily find and plot this type of confidence
> band for 'ratio~time'.  But I need to take into account 'as.factor(joint)', and in fact I can
> produce confidence bands with 'glm' by passing in 'se.fit = TRUE', but the problem is I need to
> make subject a random variable, and take into account my ratio with the Gamma distribution.  
>
> Is there a way to create confidence bands with 'glmmPQL' ??? ' as.factor(joint)' has 3 levels, so I 
> would like to produce this linear model with three levels and confidence bands for comparison of
> the levels of 'joint'.  
>
> Any Help at all with my problem would be greatly appreciated!!
> LJ
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From llei at bccrc.ca  Thu Aug 17 02:33:50 2006
From: llei at bccrc.ca (Linda Lei)
Date: Wed, 16 Aug 2006 17:33:50 -0700
Subject: [R] putting the mark for censored time on 1-KM curve or competing
	risk curve
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72C4F@crcmail1.BCCRC.CA>

Hi All,

 

I'm trying to figure out the cumulative incidence curve in R in some
limited time. I found in package "cmprsk", the command "plot.cuminc" can
get this curve. But I noticed that there is no mark for the censored
time there, comparing with the KM curve by "plot.survfit". Here are my
codes (attached is the data):

 

----------------

dat<-read.table("F://wendy/BMT data analysis/final
data.txt",header=TRUE,sep="\t")

 

library(cmprsk)

 

library(survival)

 

attach(dat)

 

par(mfrow=c(2,1))

 

curve<-cuminc(SURVEFP/365.25,STATREP,Ritux)

 

plot(curve,xlab="years",curvlab=c("No rituximab",
"Rituximab"),lty=1:2,col=c("red","blue"),mark.time=TRUE)

 

a<-survfit(Surv(SURVEFP/365.25,STATREP)~Ritux,type="kaplan-meier")

 

plot(a,main="OS for whole
group",conf.int=F,xlab="years",ylab="probability",lty=1:2,col=c("red","b
lue"))

-------------------

 

Could you help me with how to put the mark for censored time on
cumulative incidence curve, or maybe how to get the 1-KM curve with the
mark for censored time? This will help me a lot!

 

Thank you very much!

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: final data.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/6b2d2754/attachment.txt 

From spencer.graves at pdf.com  Thu Aug 17 02:34:58 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Aug 2006 17:34:58 -0700
Subject: [R] Variance Components in R
In-Reply-To: <20060810162948.k9t7ffyqpy5ssgos@webmail.ufrgs.br>
References: <20060810162948.k9t7ffyqpy5ssgos@webmail.ufrgs.br>
Message-ID: <44E3B9B2.9010709@pdf.com>

      I used SPSS over 25 years ago, but I don't recall ever fitting a 
variance components model with it.  Are all your random effects nested?  
If they were, I would recommend you use 'lme' in the 'nlme' package.  
However, if you have crossed random effects, I suggest you try 'lmer' 
associated with the 'lme4' package. 

      For 'lmer', documentation is available in Douglas Bates. Fitting 
linear mixed models in R. /R News/, 5(1):27-30, May 2005 
(www.r-project.org -> newsletter).  I also recommend you try the 
vignette available with the 'mlmRev' package (see, e.g., 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html). 

       Excellent documentation for both 'lme' (and indirectly for 
'lmer') is available in Pinheiro and Bates (2000) Mixed-Effects Models 
in S and S-Plus (Springer).  I have personally recommended this book so 
many times on this listserve that I just now got 234 hits for 
RSiteSearch("graves pinheiro").  Please don't hesitate to pass this 
recommendation to your university library.  This book is the primary 
documentation for the 'nlme' package, which is part of the standard R 
distribution.  A subdirectory "~library\nlme\scripts" of your R 
installation includes files named "ch01.R", "ch02.R", ..., "ch06.R", 
"ch08.R", containing the R scripts described in the book.  These R 
script files make it much easier and more enjoyable to study that book, 
because they make it much easier to try the commands described in the 
book, one line at a time, testing modifications to check you 
comprehension, etc.  In addition to avoiding problems with typographical 
errors, it also automatically overcomes a few minor but substantive 
changes in the notation between S-Plus and R. 

      Also, the "MINQUE" method has been obsolete for over 25 years.  I 
recommend you use method = "REML" except for when you want to compare 
two nested models with different fixed effects;  in that case, you 
should use method = "ML", as explained in Pinheiro and Bates (2000). 

      Hope this helps. 
      Spencer Graves

Iuri Gavronski wrote:
> Hi,
>
> I'm trying to fit a model using variance components in R, but if very  
> new on it, so I'm asking for your help.
>
> I have imported the SPSS database onto R, but I don't know how to  
> convert the commands... the SPSS commands I'm trying to convert are:
> VARCOMP
>    RATING BY CHAIN SECTOR RESP ASPECT ITEM
>    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
>    /METHOD = MINQUE (1)
>    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
>                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP  
> CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
>                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM CHAIN*RESP*ASPECT
>    /INTERCEPT = INCLUDE.
>
> VARCOMP
>    RATING BY CHAIN SECTOR RESP ASPECT ITEM
>    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
>    /METHOD = REML
>    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
>                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP  
> CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
>                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM CHAIN*RESP*ASPECT
>    /INTERCEPT = INCLUDE.
>
> Thank you for your help.
>
> Best regards,
>
> Iuri.
>
> _______________________________________
> Iuri Gavronski - iuri at ufrgs.br
> doutorando
> UFRGS/PPGA/NITEC - www.ppga.ufrgs.br
> Brazil
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From muster at gmail.com  Thu Aug 17 03:08:35 2006
From: muster at gmail.com (T Mu)
Date: Wed, 16 Aug 2006 21:08:35 -0400
Subject: [R] Setting contrasts for polr() to get same result of SAS
Message-ID: <b68812e70608161808m116a4ff5p332126e54fb676fb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060816/6be8694c/attachment.pl 

From spencer.graves at pdf.com  Thu Aug 17 05:35:02 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Aug 2006 20:35:02 -0700
Subject: [R] garch results is different other soft
In-Reply-To: <1aa19fc00608110342r76026c45h853fb76503ad0fcd@mail.gmail.com>
References: <1aa19fc00608110342r76026c45h853fb76503ad0fcd@mail.gmail.com>
Message-ID: <44E3E3E6.8000009@pdf.com>

      I do not believe there are any garch functions in the core R.  
RSiteSearch("garch", "tseries") just returned 21 hits, identifying garch 
capabilities in three different packages (tseries, fSeries and 
fOptions), plus mentions of garch in the Ecdat package. 

      I would be interested in a sensible reply to your question, and I 
believe many others would as well.  However, without more specifics, I 
don't feel I can afford the time to investigate this further.  You could 
help this by providing commented, minimal, self-contained, reproducible 
code, mentioning which 'garch' function you used in which package and 
which data readily available to all R users (as suggested in the posting 
guide "www.R-project.org/posting-guide.html").  If you do this, please 
tell us which capability of which other software you used for 
comparison, and compare the answers obtained from the other software and 
the R function(s) you used. 

      You might even be able to answer the question yourself by reading 
the code:  R is open source, and you can view the R code for any 
function just by typing the function name at a command prompt.  If part 
of the work is done in a compiled language, you will be able to see 
that, and the GNU license guarantees that you can get the source.  
Moreover, the 'debug' function in R makes it fairly easy to walk through 
a function line by line, looking at what it does at each step of the way. 

      I know this doesn't answer your question, but I hope it helps. 

      Best Wishes,
      Spencer Graves

Yong Xiao wrote:
> Hi
> I compared garch results in R with those give by other software and found
> that their coefficients are different from each other. So I wondered that a
> convention the garch funcion in R takes.
> By testing the output, I noticed it seems that garch function in R by
> default takes such a convention:
> y(t) = c + sigma(t) where c=0 and sigma(t) = a(0) + a(1)*epsilon^2 +
> b(1)*sigma(t-1)^2.
> I also checked the standard deviation series, i.e., the
> output$fitted.values, and noticed that the first element (the starting
> variance) is NA. I feel puzzled because in other software, the
> starting variance is estimated together with a(0), a(1) and b(1) by ML
> method.
> Could any clear this puzzle for me? Many thanks!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From vincent at 7d4.com  Thu Aug 17 07:23:02 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Thu, 17 Aug 2006 07:23:02 +0200
Subject: [R] separate row averages for different parts of an array
In-Reply-To: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>
References: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>
Message-ID: <44E3FD36.9080804@7d4.com>

Spencer Jones a ?crit :

> I have an array with 44800 columns and 24 rows I would like to compute the
> row average for the array 100 columns at a time, so I would like to end up
> with an array of 24 rows x 448 columns. I have tried using apply(dataset, 1,
> function(x) mean(x[])), but I am not sure how to get it to take the average
> 100 columns at a time. Any ideas would be  welcomed.
> thanks,
> Spencer

?rowSums, ?rowMeans
something like : rowMeans(my_array[,1:100])
(perhaps you'll have to use t() also)
hih


From Markus.Schweitzer at hilti.com  Thu Aug 17 07:59:11 2006
From: Markus.Schweitzer at hilti.com (Schweitzer, Markus)
Date: Thu, 17 Aug 2006 07:59:11 +0200
Subject: [R] fitting truncated normal distribution
Message-ID: <7260411F9C32E74A86AD9567E64C3301D2F4D6@LI-HAWK.hag.hilti.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/dc1d07ad/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Thu Aug 17 08:30:35 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 17 Aug 2006 06:30:35 +0000 (UTC)
Subject: [R] Setting contrasts for polr() to get same result of SAS
References: <b68812e70608161808m116a4ff5p332126e54fb676fb@mail.gmail.com>
Message-ID: <loom.20060817T082631-868@post.gmane.org>

T Mu <muster <at> gmail.com> writes:
> 
> Hi all,
> 
> I am trying to do a ordered probit regression using polr(), replicating a
> result from SAS.
> 
> >polr(y ~ x, dat, method='probit')
> 
> suppose the model is y ~ x, where y is a factor with 3 levels and x is a
> factor with 5 levels,
> 
> To get coefficients, SAS by default use the last level as reference, R by
> default use the first level (correct me if I was wrong),

Yes.

> I tried relevel, reorder, contrasts, but no success. I found what really
> matters is

I am sure those can help but you need to be carefull to "reorder" levels 
that the order is the same in SAS and R.

> >options(contrasts = c("contr.treatment", "contr.poly"))
> 
> or
> 
> >options(contrasts = c("contr.SAS", "contr.poly"))

You can also set contrasts directly to factors via

contrasts(x) <- contr.SAS

where x is your factor. You can also set different contrasts to 
different factors.

Gregor


From maechler at stat.math.ethz.ch  Thu Aug 17 09:01:13 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Aug 2006 09:01:13 +0200
Subject: [R] help about agnes
In-Reply-To: <44E35813.4070600@mnm.uib.es>
References: <44E35813.4070600@mnm.uib.es>
Message-ID: <17636.5177.397126.916861@stat.math.ethz.ch>

>>>>> "Arnau" == Arnau Mir Torres <arnau.mir at gmail.com>
>>>>>     on Wed, 16 Aug 2006 19:38:27 +0200 writes:

    Arnau> Hello.
    Arnau> I have the following distance matrix between 8 points:
     
    Arnau> [1,] 0.000000 3.162278 7.280110 8.544004 7.071068 9.899495 6.403124 8.062258
    Arnau> [2,] 3.162278 0.000000 5.000000 6.403124 4.472136 8.944272 6.082763 8.062258

    [ .................. ]


    Arnau> So, can somebody say me what do these numbers represent?

I would have helped you if you had followed the last two lines
below

    Arnau> Thanks in advance,

    Arnau> Arnau.

    > ______________________________________________
    > R-help at stat.math.ethz.ch mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.

^^^^^^^^^^^^^ these I mean ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


From dieter.menne at menne-biomed.de  Thu Aug 17 09:44:46 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 17 Aug 2006 07:44:46 +0000 (UTC)
Subject: [R] nls convergence problem
References: <44E30DEB.1050908@fz-rossendorf.de>
	<OFA84CBC25.463F1A9F-ON852571CC.004F14CD-852571CC.005192C2@epamail.epa.gov>
Message-ID: <loom.20060817T093004-77@post.gmane.org>

nls not converging for zero-noise cases

 <Setzer.Woodrow <at> epamail.epa.gov> writes:

> 
> No doubt Doug Bates would gladly accept patches ... .
> 

The zero-noise case is irrlevant in practice, but quite often I have uttered
&/(!! (vituperation filter on) when nls did not converge with real data. The
dreaded "min step reduced...". And yet, I found that nls is damned right not to
behave nicely in many cases. Recently, a colleague fitted gastric emptying
curves using GraphPad, with 100% success, and nls failed for one third of these.
When we checked GraphPads output more closely, some of the coefficients looked
like 2.1 with a confidence interval in the range  -27128 ... 314141. Nobody
forces you to look at these, though, when using GraphPad.

I only wish nls were a little bit more polite in telling me what went wrong.

Dieter


From ggrothendieck at gmail.com  Thu Aug 17 09:53:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 03:53:51 -0400
Subject: [R] separate row averages for different parts of an array
In-Reply-To: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>
References: <1c6126db0608161036o3d43f254g3f051000c3e7aaa4@mail.gmail.com>
Message-ID: <971536df0608170053v5e46324x93024ba3d5876fe5@mail.gmail.com>

The following reshapes mat so we can take the means of the columns
of the resulting 3d array and then transposes it back to the original
orientation:

   t(colMeans(array(t(mat), c(100, 448, 24))))

You might want to try it on this test set first where anscombe
is an 11x8 data set built into R.  Here are 4 solutions using
anscombe

1.   This is just the above written for the anscombe data set:

t(colMeans(array(t(anscombe), c(4,2,11))))

2. Here is a solution using apply instead of colMeans and t.
In this case anscombe is a data.frame, not an array/matrix,
and we need to turn it into one first.  The prior solution
also required a matrix but tranpose will convert a dataframe
to a matrix so we did not have to explicitly do it there.  If
your array is indeed an array as stated in your post then
you can omit the as.matrix part.  In your case the
c(11,4,2) vector would be c(24, 100, 448) :

apply(array(as.matrix(anscombe), c(11,4,2)), c(1,3), mean)

3. Here is another solution.  This one uses the zoo package
and does have the advantage of not having to specify a
bunch of dimensions.  It uses rapply from zoo (which will
be renamed rollapply in the next version of zoo so as not
to conflict with the new rapply that is appearing in R 2.4.0).
In your case both occurrences of 4 would be 100:

library(zoo)
coredata(t(rapply(zoo(t(anscombe)), 4, by = 4, mean)))

4. This is Marc's solution except we use seq instead of : at
the end in order to make use of the length= argument.
In your case c(11, 8, 4) would be c(1, 44800, 100) and length = 4
would be length = 100:

sapply(seq(1, 8, 4), function(i) rowMeans(anscombe[, seq(i, length = 4)]))





On 8/16/06, Spencer Jones <ssj1364 at gmail.com> wrote:
> I have an array with 44800 columns and 24 rows I would like to compute the
> row average for the array 100 columns at a time, so I would like to end up
> with an array of 24 rows x 448 columns. I have tried using apply(dataset, 1,
> function(x) mean(x[])), but I am not sure how to get it to take the average
> 100 columns at a time. Any ideas would be  welcomed.
>
> thanks,
>
> Spencer
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jim at bitwrit.com.au  Fri Aug 18 00:23:04 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 17 Aug 2006 18:23:04 -0400
Subject: [R] Problem with the special argument '...' within a function
In-Reply-To: <C3E2C445-30FF-4A45-8261-67A7DE7B9376@eva.mpg.de>
References: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>	<1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
	<C3E2C445-30FF-4A45-8261-67A7DE7B9376@eva.mpg.de>
Message-ID: <44E4EC48.7050609@bitwrit.com.au>

Hans-Joerg Bibiko wrote:
> Dear all,
> 
> I wrote some functions using the special argument '...'. OK, it works.
> 
> But if I call such a function which also called such a function, then  
> I get an error message about unused arguments.
> 
> Here's an example:
> 
> fun1 <- function(x,a=1)
> {
> 	print(paste("x=",x))
> 	print(paste("a=",a))
> }
> fun2 <- function(y,b=2)
> {
> 	print(paste("y=",y))
> 	print(paste("b=",b))
> }
> myfun <- function(c, ...)
> {
> 	print(paste("c=",c))
> 	fun1(x=c,...)
> 	fun2(y=c,...)
> }
> 
> This is OK.
>  > myfun(c=3)
> [1] "c= 3"
> [1] "x= 3"
> [1] "a= 1"
> [1] "y= 3"
> [1] "b= 2"
> 
>  > myfun(c=3,a=4)
> [1] "c= 3"
> [1] "x= 3"
> [1] "a= 4"
> Error in fun2(y = c, ...) : unused argument(s) (a ...)
> 
> I understand the error message because fun2 has no argument called 'a'.
> 
> But how can I avoid this???
> 
Try Ben Bolker's "clean.args" in the plotrix package
 > myfun(clean.args(list(c=3,a=4),myfun))
[1] "c= 3" "c= 4"
[1] "x= 3" "x= 4"
[1] "a= 1"
[1] "y= 3" "y= 4"
[1] "b= 2"

Jim


From maechler at stat.math.ethz.ch  Thu Aug 17 10:26:45 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Aug 2006 10:26:45 +0200
Subject: [R] day, month, year functions
In-Reply-To: <loom.20060811T022520-438@post.gmane.org>
References: <s4db56b1.019@pgn.com>
	<971536df0608101622r42321728w69aa8efedcb3701d@mail.gmail.com>
	<loom.20060811T022520-438@post.gmane.org>
Message-ID: <17636.10309.779152.236935@stat.math.ethz.ch>

>>>>> "Gregor" == Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
>>>>>     on Fri, 11 Aug 2006 00:27:27 +0000 (UTC) writes:

    Gregor> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
    >> 
    >> Here are three ways:
    >> 
    >> xx <- as.Date("2006-01-05")
    >> 
    >> # 1. use as.POSIXlt
    >> as.POSIXlt(xx)$mday
    >> as.POSIXlt(xx)$mon + 1
    >> as.POSIXlt(xx)$year + 1900
    >> 
    >> # 2. use format
    >> as.numeric(format(xx, "%d"))
    >> as.numeric(format(xx, "%m"))
    >> as.numeric(format(xx, "%Y"))
    >> 
    >> # 3. use month.day.year in chron package
    >> library(chron)
    >> month.day.year(unclass(xx))$day
    >> month.day.year(unclass(xx))$month
    >> month.day.year(unclass(xx))$year

    Gregor> Hi,

    Gregor> it would really be great if there would be

    Gregor> sec(), min(), hour() day(), month(), year()

    Gregor> generic functions that would work on all "date" classes. Where
    Gregor> applicable of course. I imagine that argument to get out integer
    Gregor> or character would alse be nice.

I disagree pretty strongly:

- We definitely don't want min() to return minutes instead of
  minimum !

- Why pollute the namespace with 6 (well, actualy 5!) new
  function names, when  as.POSIXlt()  
  *REALLY* is there exactly for this purpose ???

I rather think the authors of each of the other old-fashioned
"date" classes should provide as.POSIXlt() methods for their
classes.

Then, we'd have uniform interfaces, following's Gabor's "# 1."
above.

Martin Maechler, ETH Zurich


From gregor.gorjanc at bfro.uni-lj.si  Thu Aug 17 10:35:09 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Thu, 17 Aug 2006 10:35:09 +0200
Subject: [R] day, month, year functions
In-Reply-To: <17636.10309.779152.236935@stat.math.ethz.ch>
References: <s4db56b1.019@pgn.com>	<971536df0608101622r42321728w69aa8efedcb3701d@mail.gmail.com>	<loom.20060811T022520-438@post.gmane.org>
	<17636.10309.779152.236935@stat.math.ethz.ch>
Message-ID: <44E42A3D.3080403@bfro.uni-lj.si>

Martin Maechler wrote:
>>>>>> "Gregor" == Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
>>>>>>     on Fri, 11 Aug 2006 00:27:27 +0000 (UTC) writes:
> 
>     Gregor> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
>     >> 
>     >> Here are three ways:
>     >> 
>     >> xx <- as.Date("2006-01-05")
>     >> 
>     >> # 1. use as.POSIXlt
>     >> as.POSIXlt(xx)$mday
>     >> as.POSIXlt(xx)$mon + 1
>     >> as.POSIXlt(xx)$year + 1900
>     >> 
>     >> # 2. use format
>     >> as.numeric(format(xx, "%d"))
>     >> as.numeric(format(xx, "%m"))
>     >> as.numeric(format(xx, "%Y"))
>     >> 
>     >> # 3. use month.day.year in chron package
>     >> library(chron)
>     >> month.day.year(unclass(xx))$day
>     >> month.day.year(unclass(xx))$month
>     >> month.day.year(unclass(xx))$year
> 
>     Gregor> Hi,
> 
>     Gregor> it would really be great if there would be
> 
>     Gregor> sec(), min(), hour() day(), month(), year()
> 
>     Gregor> generic functions that would work on all "date" classes. Where
>     Gregor> applicable of course. I imagine that argument to get out integer
>     Gregor> or character would alse be nice.
> 
> I disagree pretty strongly:
> 
> - We definitely don't want min() to return minutes instead of
>   minimum !
> 

Pheu, a good catch. You are definitely right!

> - Why pollute the namespace with 6 (well, actualy 5!) new
>   function names, when  as.POSIXlt()  
>   *REALLY* is there exactly for this purpose ???
>
> I rather think the authors of each of the other old-fashioned
> "date" classes should provide as.POSIXlt() methods for their
> classes.
> 
> Then, we'd have uniform interfaces, following's Gabor's "# 1."
> above.

My proposal above was just a "direction" to a common way of dealing with
dates within R. If as.POSIXlt() methods is the way, that is perfectly
fine with me.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From rdiaz at cnio.es  Thu Aug 17 10:56:05 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Thu, 17 Aug 2006 10:56:05 +0200
Subject: [R] rpvm/snow packages on a cluster with dual-processor machi
	nes
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2E5D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB2E5D@usctmx1106.merck.com>
Message-ID: <200608171056.05524.rdiaz@cnio.es>

Dear Paul,

(I forgot to answer over the weekend). With mpi it is essentially the same. 
When using makeCluster, specify the number of slaves. If you have three 
machines, and you want each to run two slave processes, just use a 6.

Before that, though, you should tell LAM/MPI how to set up the "lam universe". 
The simplest way is to specify that in a configuration file for LAM. Put 
something like this (using appropriate IPs or host names; cpu=xx indicates 
that you want each physical node to run those many xx slaves; it might, or 
might not, be related to the actual number of CPUs) in a file called, say, 
lamb-conf1.def

192.168.2.2 cpu=2
192.168.2.3 cpu=2
192.168.2.4 cpu=2



Now do (as user, NOT root)

lamboot -v lamb-conf1.def

If that works, then start R, and use snow. 

A very good explanation on how to use mpi with R appeared in R news a while 
ago by the author of Rmpi.


HTH,

R.



On Monday 14 August 2006 16:17, Liaw, Andy wrote:
> That's what I've tried before, on three dual-Xeon boxes, so I know it
> worked (as documented a that time).
>
> Andy
>
> From: Paul Y. Peng
>
> > Luke Tierney just reminded me that makeCluster() can take a
> > number greater than the number of machines in a cluster. It
> > seems to be a solution to this problem. But I haven't tested it yet.
> >
> > Paul.
> >
> > Ryan Austin wrote:
> > > Hi,
> > >
> > > Adding a node twice gives a duplicate node error.
> > > However, adding the parameter sp=2000 to your pvm hostfile should
> > > enable dual processors.
> > >
> > > Ryan
> > >
> > > Liaw, Andy wrote:
> > >> Caveat: I've only played with this a couple of years ago...
> > >>
> > >> I believe you can just add each host _twice_ (or as many
> >
> > times as the
> >
> > >> number of CPUs at that host) to get both CPUs to work.
> > >>
> > >> Andy
> > >>
> > >> From: Paul Y. Peng
> > >>
> > >>> Hi,
> > >>>
> > >>> does anybody know how to use the dual processors in the
> >
> > machines of
> >
> > >>> a cluster? I am using R with rpvm and snow packages. I
> >
> > usually start
> >
> > >>> pvm daemon and add host machines first, and then run R to
> >
> > start my
> >
> > >>> computing work. But I find that only one processor in
> >
> > each machine
> >
> > >>> is used in this way and the other one always stays idle. Is there
> > >>> any simple way to tell pvm to use the two processors at the same
> > >>> time? In other words, I would like to see two copies of R
> >
> > running on
> >
> > >>> each machine's two processors when using pvm. Any hints/help are
> > >>> greatly appreciated.
> > >>>
> > >>> Paul.
> > >>>
> > >>> ______________________________________________
> > >>> R-help at stat.math.ethz.ch mailing list
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> > >>> http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From julie7.josse at laposte.net  Thu Aug 17 11:01:10 2006
From: julie7.josse at laposte.net (julie7.josse)
Date: Thu, 17 Aug 2006 11:01:10 +0200
Subject: [R] tkinser
Message-ID: <J44X1Y$5536F80AB031E3669F42581E52F0169F@laposte.net>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060817/217ef502/attachment.pl 

From gregd at stats.uct.ac.za  Thu Aug 17 11:01:38 2006
From: gregd at stats.uct.ac.za (Greg Distiller)
Date: Thu, 17 Aug 2006 11:01:38 +0200
Subject: [R] NLME: Limitations of using identify to interact with
	scatterplots?
Message-ID: <007f01c6c1db$c12c1c90$6f179e89@UCTPCGREGD>

I have a quick question regarding the use of identify to interact with 
points on a scatterplot. My question is essentially: can identify be used 
when one is plotting model objects to generate diagnostic plots? 
Specifically I am using NLME.
For example, I am plotting the fitted values on the x axis vs a variable 
called log2game with the following code:

plot(D2C29.nlme, log2game ~ fitted(.), abline=c(0,1))

and then I have tried to use identify as follows:

identify(D2C29.nlme$fitted[,2],Data2$log2game,row.names(Data2))

(if I leave out the [,2] on the fitted attributes then I am told that x and 
y are not the same length and it appears that this is due to the fact that 
the fitted attribute has 2 columns.)

but I get an error message that "plot.new has not been called yet".

I am not sure if this is because I am doing something wrong or if identify 
simply cannot be used in this context.

Many thanks

Greg


From s.wood at bath.ac.uk  Thu Aug 17 11:33:09 2006
From: s.wood at bath.ac.uk (Simon Wood)
Date: Thu, 17 Aug 2006 10:33:09 +0100
Subject: [R] glmmPQL question!
In-Reply-To: <44E3B0CD.4010903@pdf.com>
References: <20060810154007.02D3929@lulu.it.northwestern.edu>
	<44E3B0CD.4010903@pdf.com>
Message-ID: <200608171033.09846.s.wood@bath.ac.uk>

Will this do? best, Simon

## simulate some data...
set.seed(1)
joint <- c(rep(1,20),rep(2,20),rep(3,20))
time <- runif(60)+1
subject <- factor(rep(1:12,rep(5,12)))
mu <- time*joint
joint <- factor(joint)
y <- rgamma(mu,mu)

## fit model
b <- glmmPQL(y~joint*time,random=~1|subject,family=Gamma(link="identity"))

## extract fixed effect parameter estimates and covariance matrix
fix.b <- b$coefficients$fixed
V.b <- b$varFix

## Create a `prediction matrix'
pd <- data.frame(time = rep(seq(1,2,length=100),3),
                       joint=factor(c(rep(1,100),rep(2,100),rep(3,100))))

Xp <- model.matrix(~joint*time,pd)

## use it to get predictions and associated standard errors
mu <- Xp %*% fix.b
mu.se <- diag(Xp%*%V.b%*%t(Xp))^.5 ## inefficient for readability
## check this is done right
range(mu - predict(b,pd,level=0))

## produce plot
plot(pd$time[1:100],mu[1:100],main="joint==1",type="l")
lines(pd$time[1:100],mu[1:100]+2*mu.se[1:100],lty=2)
lines(pd$time[1:100],mu[1:100]-2*mu.se[1:100],lty=3)



> m-krutky at northwestern.edu wrote:
> > Hello Folks-
> >
> > Is there a way to create confidence bands with 'glmmPQL' ???
> >
> > I am performing a stroke study for Northwestern University in Chicago,
> > Illinois.  I am trying to decide a way to best plot the model which we
> > created with the glmmPQL function in R.   I would like to plot my actual
> > averaged data points within 95 % confidence intervals from the model.
> > Plotting the model is easy, but determining confidence bands is not.
> >
> > Here is my model:
> >
> > ratiomodel<-glmmPQL(ratio~as.factor(joint)*time, random = ~ 1 | subject,
> > family = Gamma(link = "identity"),alldata3)
> >
> > I am used to seeing confidence intervals from models that increase,
> > ?flair out? in the y direction, at the beginning and ending time points
> > (x values) of the simulated data.  If I use 'lm' and pass the command
> > 'int = "c" ' 'to create this model I can easily find and plot this type
> > of confidence band for 'ratio~time'.  But I need to take into account
> > 'as.factor(joint)', and in fact I can produce confidence bands with 'glm'
> > by passing in 'se.fit = TRUE', but the problem is I need to make subject
> > a random variable, and take into account my ratio with the Gamma
> > distribution.
> >
> > Is there a way to create confidence bands with 'glmmPQL' ??? '
> > as.factor(joint)' has 3 levels, so I would like to produce this linear
> > model with three levels and confidence bands for comparison of the levels
> > of 'joint'.
> >
> > Any Help at all with my problem would be greatly appreciated!!
> > LJ
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From rab at nauticom.net  Thu Aug 17 14:06:52 2006
From: rab at nauticom.net (Rick Bilonick)
Date: Thu, 17 Aug 2006 08:06:52 -0400
Subject: [R] Specifying Path Model in SEM for CFA
In-Reply-To: <20060816210151.NOMD19825.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20060816210151.NOMD19825.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1155816412.3329.6.camel@localhost.localdomain>

On Wed, 2006-08-16 at 17:01 -0400, John Fox wrote:
> Dear Rick,
> 
> It's unclear to me what you mean by constraining "each column of the factor
> matrix to sum to one." If you intend to constrain the loadings on each
> factor to sum to one, sem() won't do that, since it supports only equality
> constraints, not general linear constraints on parameters of the model, but
> why such a constraint would be reasonable in the first place escapes me.
> More common in confirmatory factor analysis would be to constrain more of
> the loadings to zero. Of course, one would do this only if it made
> substantive sense in the context of the research.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 

I'm trying to build a multivariate receptor model as described by
Christensen and Sain (Technometrics, vol 44 (4) pp. 328-337). The model
is

x_t = Af_t + e_t

where A is the matrix of nonnegative source compositions, x_t are the
observed pollutant concentrations at time t, and f_t are the unobserved
factors. The columns of A are supposed to sum to no more than 100%. They
say they are using a latent variable model. If sem can't handle this, do
you know of another R package that could?

Rick B.


From jfox at mcmaster.ca  Thu Aug 17 14:20:11 2006
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 17 Aug 2006 08:20:11 -0400
Subject: [R] tkinser
In-Reply-To: <J44X1Y$5536F80AB031E3669F42581E52F0169F@laposte.net>
Message-ID: <20060817122009.WQWI24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Julie,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of julie7.josse
> Sent: Thursday, August 17, 2006 4:01 AM
> To: r-help
> Subject: [R] tkinser
> Importance: High
> 
> Dear list,
> 
> I 'd like to know if it is possible to delete my text window 
> after running it??
> 
> I have add a Menu to my text window and so i can for example 
> open a script; then i run it and i have my result on my R console.
> But i'd like that after running, the code disappears 
> automatically. i d'like something that clean my text window.
>

I think that what you mean is that you'd like to delete the text in the
window rather than the window itself. If the text widget is called txt, then
you can do 

tkdelete(txt, "0.0", "end") 


> I have a second problem:
> 
> My text window and all my widgets are not fixed:
> 
> I use combo box, message box...and it always moves, it 
> appears on the right of my screen, on the bottom..or on the 
> bar tasks. I d'like my text window not move at all and after 
> if it's possible that my widgets appear on the same place on my sreen.
> 
> When they appear on the bottom on my tasks bar, i have to 
> open it each time...
> 

If the top-level Tk window is named top, then, after creating the window,
e.g., tkwm.geometry(tt, "-100+100") will position it 100 pixels 100 pixels
from the right top corner of the display.

More generally, I found it useful to read Welch, Jones, and Hobbs, Practical
Programming in Tcl and Tk, to learn these kinds of things.

I hope this helps,
 John

> 
> Thanks a lot.
> Julie.
> 
> Cet iti, pensez aux cartes postales de laposte.net !
> 
> 
> 	[[alternative HTML version deleted]]
> 
>


From jfox at mcmaster.ca  Thu Aug 17 14:57:32 2006
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 17 Aug 2006 08:57:32 -0400
Subject: [R] Specifying Path Model in SEM for CFA
In-Reply-To: <1155816412.3329.6.camel@localhost.localdomain>
Message-ID: <20060817125730.ELLX29052.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Rick,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rick Bilonick
> Sent: Thursday, August 17, 2006 7:07 AM
> To: John Fox
> Cc: 'R Help'; 'Rick Bilonick'
> Subject: Re: [R] Specifying Path Model in SEM for CFA
> 

. . .

> 
> I'm trying to build a multivariate receptor model as 
> described by Christensen and Sain (Technometrics, vol 44 (4) 
> pp. 328-337). The model is
> 
> x_t = Af_t + e_t
> 
> where A is the matrix of nonnegative source compositions, x_t 
> are the observed pollutant concentrations at time t, and f_t 
> are the unobserved factors. The columns of A are supposed to 
> sum to no more than 100%. They say they are using a latent 
> variable model. If sem can't handle this, do you know of 
> another R package that could?
> 

sem() handles only equality constraints among parameters, and this model
requires linear inequality constraints. 

I'm aware of SEM software that handles inequality constraints, but I'm not
aware of anything in R that will do it "out of the box." One possibility is
to write out the likelihood (or "fitting function") for your model and
perform a bounded optimization using optim(). It would probably be a fair
amount of work setting up the problem.

Finally, there are tricks that permit the imposition of general constraints
and inequality constraints using software, like sem(), that handles only
equality constraints. It's probably possible to do what you want using such
a trick, but it would be awkward. See the references given in Bollen,
Structural Equations with Latent Variables (Wiley, 1989), pp. 401-403.

I'm sorry that I can't be of more direct help.
 John


From r-help at botelho-machado.de  Thu Aug 17 15:55:01 2006
From: r-help at botelho-machado.de (Lothar Botelho-Machado)
Date: Thu, 17 Aug 2006 15:55:01 +0200
Subject: [R] Plots Without Displaying
In-Reply-To: <001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>
References: <001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>
Message-ID: <44E47535.90805@botelho-machado.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Thank you,

It seems that a list of plots is just possible using lattice plots. But
that's a good keyword for me to look for, I appreciate your help!

Lothar


Christos Hatzis wrote:
> Yes, you can do that for lattice-based plots.  The functions in the lattice
> package produce objects of class "trellis" which can be stored in a list and
> processed or updated at a later time:
> 
> library(lattice)
> attach(barley)
> plotList <- list(length=3)
> plotList[[1]] <- xyplot(yield ~ site, data=barley)
> plotList[[2]] <- xyplot(yield ~ variety, data=barley) 
> plotList[[3]] <- xyplot(yield ~ year, data=barley)
> 
> plotList
> plotList[[3]] <- update(plotList[[3]], yaxis="Yield (bushels/acre)")
> print(plotList[[3]])
> 
> Obviously, you can store any lattice-based plot in the list.
> 
> HTH.
> 
> -Christos
> 
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>  
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lothar
> Botelho-Machado
> Sent: Wednesday, August 16, 2006 4:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Plots Without Displaying
> 
> Prof Brian Ripley wrote:
>>> Yes, see
>>>
>>> ?jpeg
>>> ?bitmap
>>>
>>> and as you didn't tell us your OS we don't know if these are available 
>>> to you.
>>>
>>> jpeg(file="test.jpg")
>>> boxplot(sample(100))
>>> dev.off()
>>>
>>> may well work.
>>>
>>> 'An Introduction to R' explains about graphics devices, including these.
>>>
>>>
>>> On Wed, 16 Aug 2006, Lothar Botelho-Machado wrote:
>>>
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA1
>>>>
>>>> R Help Mailing List,
>>>>
>>>>
>>>> I'd like to generate a plot that I could display and/or store it as e.g.
>>>> jpeg. But unfortunately always a plotting window opens. Is it 
>>>> possible to prevent that?
>>>>
>>>> I tried the following:
>>>> R> bp<-boxplot( sample(100), plot=FALSE)
>>>>
>>>> This works somehow, but it only stores data (as discribed in the 
>>>> help) in bp and it is not possible afaik to display bp later on or 
>>>> store them as a jpeg.
>>>>
>>>> The next:
>>>> R> p<-plot(sample(100), sample(100), plot=FALSE)
>>>> ..and also a variant using jpeg() didn't work at all.
>>>>
>>>> Is there a way to generally store the plots as object, without 
>>>> displaying them, or perhaps directly saving them to disc as jpeg?
>>>>
>>>> A "Yes" or "No" or any further help/links are appreciated!!!
>>>
> 
> 
> 
> Thank you for the explanation and your patience in answering me this
> obviously very simple question!!
> 
> Originally I tried to store plots directly in a list. So writing them
> directly to disc was just a good alternative. I knew that that jpeg()
> provides functionality for that, but didn't use it correctly.
> 
> Hence, is it also possible to store a plot in a list, somehow?
> 
> Kind regards,
>  Lothar

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.





-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFE5HU1HRf7N9c+X7sRAguEAJ4855nuonJaB9VXHkGOr/SZhqow8wCfXcuB
o8oqpYoJ7MXgnVtnuGAE5Yk=
=ZWgN
-----END PGP SIGNATURE-----


From bates at stat.wisc.edu  Thu Aug 17 15:55:41 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Aug 2006 08:55:41 -0500
Subject: [R] NLME: Limitations of using identify to interact with
	scatterplots?
In-Reply-To: <007f01c6c1db$c12c1c90$6f179e89@UCTPCGREGD>
References: <007f01c6c1db$c12c1c90$6f179e89@UCTPCGREGD>
Message-ID: <40e66e0b0608170655o6e52c093hca42988db58b853b@mail.gmail.com>

Most plotting functions in the nlme package use lattice graphics
functions based on the grid package.  Identify will not work with
lattice graphics.  I'm not sure if there is a replacement.

On 8/17/06, Greg Distiller <gregd at stats.uct.ac.za> wrote:
> I have a quick question regarding the use of identify to interact with
> points on a scatterplot. My question is essentially: can identify be used
> when one is plotting model objects to generate diagnostic plots?
> Specifically I am using NLME.
> For example, I am plotting the fitted values on the x axis vs a variable
> called log2game with the following code:
>
> plot(D2C29.nlme, log2game ~ fitted(.), abline=c(0,1))
>
> and then I have tried to use identify as follows:
>
> identify(D2C29.nlme$fitted[,2],Data2$log2game,row.names(Data2))
>
> (if I leave out the [,2] on the fitted attributes then I am told that x and
> y are not the same length and it appears that this is due to the fact that
> the fitted attribute has 2 columns.)
>
> but I get an error message that "plot.new has not been called yet".
>
> I am not sure if this is because I am doing something wrong or if identify
> simply cannot be used in this context.
>
> Many thanks
>
> Greg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Aug 17 16:46:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 10:46:43 -0400
Subject: [R] Plots Without Displaying
In-Reply-To: <44E47535.90805@botelho-machado.de>
References: <001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>
	<44E47535.90805@botelho-machado.de>
Message-ID: <971536df0608170746v3a00da14g30a65582a09c9517@mail.gmail.com>

Also check out the displaylist:

http://tolstoy.newcastle.edu.au/R/help/04/05/0817.html

On 8/17/06, Lothar Botelho-Machado <r-help at botelho-machado.de> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Thank you,
>
> It seems that a list of plots is just possible using lattice plots. But
> that's a good keyword for me to look for, I appreciate your help!
>
> Lothar
>
>
> Christos Hatzis wrote:
> > Yes, you can do that for lattice-based plots.  The functions in the lattice
> > package produce objects of class "trellis" which can be stored in a list and
> > processed or updated at a later time:
> >
> > library(lattice)
> > attach(barley)
> > plotList <- list(length=3)
> > plotList[[1]] <- xyplot(yield ~ site, data=barley)
> > plotList[[2]] <- xyplot(yield ~ variety, data=barley)
> > plotList[[3]] <- xyplot(yield ~ year, data=barley)
> >
> > plotList
> > plotList[[3]] <- update(plotList[[3]], yaxis="Yield (bushels/acre)")
> > print(plotList[[3]])
> >
> > Obviously, you can store any lattice-based plot in the list.
> >
> > HTH.
> >
> > -Christos
> >
> > Christos Hatzis, Ph.D.
> > Nuvera Biosciences, Inc.
> > 400 West Cummings Park
> > Suite 5350
> > Woburn, MA 01801
> > Tel: 781-938-3830
> > www.nuverabio.com
> >
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lothar
> > Botelho-Machado
> > Sent: Wednesday, August 16, 2006 4:49 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Plots Without Displaying
> >
> > Prof Brian Ripley wrote:
> >>> Yes, see
> >>>
> >>> ?jpeg
> >>> ?bitmap
> >>>
> >>> and as you didn't tell us your OS we don't know if these are available
> >>> to you.
> >>>
> >>> jpeg(file="test.jpg")
> >>> boxplot(sample(100))
> >>> dev.off()
> >>>
> >>> may well work.
> >>>
> >>> 'An Introduction to R' explains about graphics devices, including these.
> >>>
> >>>
> >>> On Wed, 16 Aug 2006, Lothar Botelho-Machado wrote:
> >>>
> >>>> -----BEGIN PGP SIGNED MESSAGE-----
> >>>> Hash: SHA1
> >>>>
> >>>> R Help Mailing List,
> >>>>
> >>>>
> >>>> I'd like to generate a plot that I could display and/or store it as e.g.
> >>>> jpeg. But unfortunately always a plotting window opens. Is it
> >>>> possible to prevent that?
> >>>>
> >>>> I tried the following:
> >>>> R> bp<-boxplot( sample(100), plot=FALSE)
> >>>>
> >>>> This works somehow, but it only stores data (as discribed in the
> >>>> help) in bp and it is not possible afaik to display bp later on or
> >>>> store them as a jpeg.
> >>>>
> >>>> The next:
> >>>> R> p<-plot(sample(100), sample(100), plot=FALSE)
> >>>> ..and also a variant using jpeg() didn't work at all.
> >>>>
> >>>> Is there a way to generally store the plots as object, without
> >>>> displaying them, or perhaps directly saving them to disc as jpeg?
> >>>>
> >>>> A "Yes" or "No" or any further help/links are appreciated!!!
> >>>
> >
> >
> >
> > Thank you for the explanation and your patience in answering me this
> > obviously very simple question!!
> >
> > Originally I tried to store plots directly in a list. So writing them
> > directly to disc was just a good alternative. I knew that that jpeg()
> > provides functionality for that, but didn't use it correctly.
> >
> > Hence, is it also possible to store a plot in a list, somehow?
> >
> > Kind regards,
> >  Lothar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.3 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFE5HU1HRf7N9c+X7sRAguEAJ4855nuonJaB9VXHkGOr/SZhqow8wCfXcuB
> o8oqpYoJ7MXgnVtnuGAE5Yk=
> =ZWgN
> -----END PGP SIGNATURE-----
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sundar.dorai-raj at pdf.com  Thu Aug 17 16:47:18 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 17 Aug 2006 09:47:18 -0500
Subject: [R] fitting truncated normal distribution
In-Reply-To: <7260411F9C32E74A86AD9567E64C3301D2F4D6@LI-HAWK.hag.hilti.com>
References: <7260411F9C32E74A86AD9567E64C3301D2F4D6@LI-HAWK.hag.hilti.com>
Message-ID: <44E48176.70307@pdf.com>

Hi, Markus,

Are these always integers? Why do you think they should be normal or 
Weibull? Seems more like a mixture with a point mass at 0 and something 
else (e.g. Poisson, negative binomial, normal). Though it's hard to tell 
with what you have provided. If that's the case you'll have to write 
your own likelihood function or, if they are integers, use zip 
(zero-inflated Poisson) or zinb (zero-inflated negative binomial). Do an 
RSiteSearch to find many packages will do these fits.

RSiteSearch("zero-inflated")

Again, this is pure speculation based on your "x" below alone and no 
other information (I'm not sure what "demand-data" means).

HTH,

--sundar

Schweitzer, Markus wrote:
> Sorry, that I forgot an example.
> 
> I have demand-data which is either 0 or a positive value.
> 
> When I have an article which is not ordered very often, it could look
> like this:
> 
> x=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1280,0,0,0,0,640,0,0
> ,0,0,0,0,0,0,0)
> 
> 
> 
>>library(MASS) ## for fitdistr
>>library(msm) ## for dtnorm
>>
>>dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>>   dtnorm(x, mean, sd, 0, Inf, log)
>>}
>>fitdistr(x,dtnorm0,start=list(mean=0,sd=1))
> 
> 
> Unfortunately I get the same error message.
> I found a function, that works for a weibull distribution and tried to
> apply it but it didn't work neither
> 
> # truncated weibull distribution
> 
> #dweibull.trunc <-
> #function(x, shape, scale=1, trunc.=Inf, log=FALSE){
> #    ln.dens <- (dweibull(x, shape, scale, log=TRUE)
> #        -pweibull(trunc., shape, scale = 1, lower.tail = TRUE, log.p = 
> #TRUE))
> #    if(any(oops <- (x>trunc.)))
> #        ln.dens[oops] <- (-Inf)   
> #    if(log)ln.dens else exp(ln.dens)
> #}
> #
> #x <- rweibull(100, 1)
> #range(x)
> #x4 <- x[x<=4]
> #fitdistr(x4, dweibull.trunc, start=list(shape=1, scale=1), trunc=4)
> 
> ########################################################################
> ########
> # truncated normal distribution
> 
> dtnorm0 <- function(x, mean, sd, a=0, log = FALSE) {
>     ln.dens <- (dnorm(x, mean, sd)
>                 - pnorm(a, mean, sd, lower.tail=TRUE, log.p =TRUE))
>                 
>     if(any(oops <- (x<a)))
>       ln.dens[oops] <- (-Inf)
>     if(log)ln.dens else exp(ln.dens)
> }
> 
> fitdistr(x, dtnorm0, start = list(mean = 0, sd = 1))
> 
> Maybe, when I alter mean and sd, I get an answer, which is not really
> satisfactory. I hope, there is a solution possible
> And thank you in advance
> 
> markus
> 
> 
> 
> 
> 
> 
> 
> Sorry, didn't notice that you *did* mention dtnorm is part of msm. 
> Ignore that part of the advice...
> 
> --sundar
> 
> Sundar Dorai-Raj wrote:
> 
>>aon.912182281.tmp at aon.at wrote:
>>
>>
>>>Hello,
>>>I am a new user of R and found the function dtnorm() in the package
> 
> msm.
> 
>>>My problem now is, that it is not possible for me to get the mean and
> 
> sd out of a sample when I want a left-truncated normal distribution
> starting at "0".
> 
>>>fitdistr(x,dtnorm, start=list(mean=0, sd=1))
>>>
>>>returns the error message 
>>>"Fehler in "[<-"(`*tmp*`, x >= lower & x <= upper, value = numeric(0))
> 
> :    nichts zu ersetzen"
> 
>>>I don't know, where to enter the lower/upper value. Is there a
> 
> possibility to program the dtnorm function by myself?
> 
>>>Thank you very much in advance for your help, markus
>>>
>>>-------------------------------------------
>>>Versendet durch aonWebmail (webmail.aon.at)
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide 
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>Hi, Markus,
>>
>>You should always supply the package name where dtnorm is located. My 
>>guess is most don't know (as I didn't) it is part of the msm package.
>>Also, you should supply a reproducible example so others may 
>>understand your particular problem. For example, when I ran your code 
>>on data generated from "rtnorm" (also part of msm) I got warnings 
>>related to the NaNs generated in pnorm and qnorm, but no error as you 
>>reported. Both of these suggestions are in the posting guide (see
> 
> signature above).
> 
>>So, to answer your problem, here's a quick example.
>>
>>library(MASS) ## for fitdistr
>>library(msm) ## for dtnorm
>>
>>dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>>   dtnorm(x, mean, sd, 0, Inf, log)
>>}
>>
>>set.seed(1) ## to others may reproduce my results exactly x <- 
>>rtnorm(100, lower = 0) fitdistr(x, dtnorm0, start = list(mean = 0, sd 
>>= 1))
>>
>>Note, the help page ?fitdistr suggests additional parameters may be 
>>passed to the density function (i.e. dtnorm) or optim. However, this 
>>won't work here because "lower" is an argument for both functions. 
>>This is the reason for writing dtnorm0 which has neither a lower or an
> 
> 
>>upper argument.
>>
>>HTH,
>>
>>--sundar
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Aug 17 16:48:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Aug 2006 15:48:47 +0100 (BST)
Subject: [R] Plots Without Displaying
In-Reply-To: <44E47535.90805@botelho-machado.de>
References: <001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>
	<44E47535.90805@botelho-machado.de>
Message-ID: <Pine.LNX.4.64.0608171537390.17820@gannet.stats.ox.ac.uk>

On Thu, 17 Aug 2006, Lothar Botelho-Machado wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Thank you,
> 
> It seems that a list of plots is just possible using lattice plots. But
> that's a good keyword for me to look for, I appreciate your help!

Actually, that is not a list of *plots*.  The objects stored there are 
more sets of instructions to the print method of what to plot, and you can 
do that for any type of plot.

It is possible to store low-level descriptions of plots and replay them: 
see recordPlot and replayPlot.  BUT, it is preferable to run the 
expressions to create the plot on the new device.


> Christos Hatzis wrote:
> > Yes, you can do that for lattice-based plots.  The functions in the lattice
> > package produce objects of class "trellis" which can be stored in a list and
> > processed or updated at a later time:
> > 
> > library(lattice)
> > attach(barley)
> > plotList <- list(length=3)
> > plotList[[1]] <- xyplot(yield ~ site, data=barley)
> > plotList[[2]] <- xyplot(yield ~ variety, data=barley) 
> > plotList[[3]] <- xyplot(yield ~ year, data=barley)
> > 
> > plotList
> > plotList[[3]] <- update(plotList[[3]], yaxis="Yield (bushels/acre)")
> > print(plotList[[3]])
> > 
> > Obviously, you can store any lattice-based plot in the list.
> > 
> > HTH.
> > 
> > -Christos
> > 
> > Christos Hatzis, Ph.D.
> > Nuvera Biosciences, Inc.
> > 400 West Cummings Park
> > Suite 5350
> > Woburn, MA 01801
> > Tel: 781-938-3830
> > www.nuverabio.com
> >  
> > 
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lothar
> > Botelho-Machado
> > Sent: Wednesday, August 16, 2006 4:49 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Plots Without Displaying
> > 
> > Prof Brian Ripley wrote:
> >>> Yes, see
> >>>
> >>> ?jpeg
> >>> ?bitmap
> >>>
> >>> and as you didn't tell us your OS we don't know if these are available 
> >>> to you.
> >>>
> >>> jpeg(file="test.jpg")
> >>> boxplot(sample(100))
> >>> dev.off()
> >>>
> >>> may well work.
> >>>
> >>> 'An Introduction to R' explains about graphics devices, including these.
> >>>
> >>>
> >>> On Wed, 16 Aug 2006, Lothar Botelho-Machado wrote:
> >>>
> >>>> -----BEGIN PGP SIGNED MESSAGE-----
> >>>> Hash: SHA1
> >>>>
> >>>> R Help Mailing List,
> >>>>
> >>>>
> >>>> I'd like to generate a plot that I could display and/or store it as e.g.
> >>>> jpeg. But unfortunately always a plotting window opens. Is it 
> >>>> possible to prevent that?
> >>>>
> >>>> I tried the following:
> >>>> R> bp<-boxplot( sample(100), plot=FALSE)
> >>>>
> >>>> This works somehow, but it only stores data (as discribed in the 
> >>>> help) in bp and it is not possible afaik to display bp later on or 
> >>>> store them as a jpeg.
> >>>>
> >>>> The next:
> >>>> R> p<-plot(sample(100), sample(100), plot=FALSE)
> >>>> ..and also a variant using jpeg() didn't work at all.
> >>>>
> >>>> Is there a way to generally store the plots as object, without 
> >>>> displaying them, or perhaps directly saving them to disc as jpeg?
> >>>>
> >>>> A "Yes" or "No" or any further help/links are appreciated!!!
> >>>
> > 
> > 
> > 
> > Thank you for the explanation and your patience in answering me this
> > obviously very simple question!!
> > 
> > Originally I tried to store plots directly in a list. So writing them
> > directly to disc was just a good alternative. I knew that that jpeg()
> > provides functionality for that, but didn't use it correctly.
> > 
> > Hence, is it also possible to store a plot in a list, somehow?
> > 
> > Kind regards,
> >  Lothar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.3 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
> 
> iD8DBQFE5HU1HRf7N9c+X7sRAguEAJ4855nuonJaB9VXHkGOr/SZhqow8wCfXcuB
> o8oqpYoJ7MXgnVtnuGAE5Yk=
> =ZWgN
> -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From iuri at ufrgs.br  Thu Aug 17 17:08:04 2006
From: iuri at ufrgs.br (Iuri Gavronski)
Date: Thu, 17 Aug 2006 12:08:04 -0300
Subject: [R] Variance Components in R
In-Reply-To: <44E3B9B2.9010709@pdf.com>
References: <20060810162948.k9t7ffyqpy5ssgos@webmail.ufrgs.br>
	<44E3B9B2.9010709@pdf.com>
Message-ID: <60ad85c90608170808r54b255abwc17777bba0e9dd95@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/9514a880/attachment.pl 

From HDoran at air.org  Thu Aug 17 17:08:56 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 17 Aug 2006 11:08:56 -0400
Subject: [R] Variance Components in R
Message-ID: <2323A6D37908A847A7C32F1E3662C80E277154@dc1ex01.air.org>

Iuri:

The lmer function is optimal for large data with crossed random effects.
How large are your data?

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
> Sent: Thursday, August 17, 2006 11:08 AM
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Variance Components in R
> 
> Thank you for your reply.
> VARCOMP is available at SPSS advanced models, I'm not sure 
> for how long it exists... I only work with SPSS for the last 
> 4 years...
> My model only has crossed random effects, what perhaps would 
> drive me to lmer().
> However, as I have unbalanced data (why it is normally called 
> 'unbalanced design'? the data was not intended to be 
> unbalanced, only I could not get responses for all cells...), 
> I'm afraid that REML would take too much CPU, memory and time 
> to execute, and MINQUE would be faster and provide similar 
> variance estimates (please, correct me if I'm wrong on that point).
> I only found MINQUE on the maanova package, but as my study 
> is very far from genetics, I'm not sure I can use this package.
> Any comment would be appreciated.
> Iuri
> 
> On 8/16/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> >
> >       I used SPSS over 25 years ago, but I don't recall 
> ever fitting a 
> > variance components model with it.  Are all your random 
> effects nested?
> > If they were, I would recommend you use 'lme' in the 'nlme' package.
> > However, if you have crossed random effects, I suggest you 
> try 'lmer'
> > associated with the 'lme4' package.
> >
> >       For 'lmer', documentation is available in Douglas 
> Bates. Fitting 
> > linear mixed models in R. /R News/, 5(1):27-30, May 2005 
> > (www.r-project.org -> newsletter).  I also recommend you try the 
> > vignette available with the 'mlmRev' package (see, e.g., 
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html).
> >
> >        Excellent documentation for both 'lme' (and indirectly for
> > 'lmer') is available in Pinheiro and Bates (2000) 
> Mixed-Effects Models 
> > in S and S-Plus (Springer).  I have personally recommended 
> this book 
> > so many times on this listserve that I just now got 234 hits for 
> > RSiteSearch("graves pinheiro").  Please don't hesitate to pass this 
> > recommendation to your university library.  This book is 
> the primary 
> > documentation for the 'nlme' package, which is part of the 
> standard R 
> > distribution.  A subdirectory "~library\nlme\scripts" of your R 
> > installation includes files named "ch01.R", "ch02.R", ..., 
> "ch06.R", 
> > "ch08.R", containing the R scripts described in the book.  These R 
> > script files make it much easier and more enjoyable to study that 
> > book, because they make it much easier to try the commands 
> described 
> > in the book, one line at a time, testing modifications to check you 
> > comprehension, etc.  In addition to avoiding problems with 
> > typographical errors, it also automatically overcomes a few 
> minor but 
> > substantive changes in the notation between S-Plus and R.
> >
> >       Also, the "MINQUE" method has been obsolete for over 
> 25 years.  
> > I recommend you use method = "REML" except for when you want to 
> > compare two nested models with different fixed effects;  in 
> that case, 
> > you should use method = "ML", as explained in Pinheiro and 
> Bates (2000).
> >
> >       Hope this helps.
> >       Spencer Graves
> >
> > Iuri Gavronski wrote:
> > > Hi,
> > >
> > > I'm trying to fit a model using variance components in R, but if 
> > > very new on it, so I'm asking for your help.
> > >
> > > I have imported the SPSS database onto R, but I don't know how to 
> > > convert the commands... the SPSS commands I'm trying to 
> convert are:
> > > VARCOMP
> > >    RATING BY CHAIN SECTOR RESP ASPECT ITEM
> > >    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
> > >    /METHOD = MINQUE (1)
> > >    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
> > >                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP 
> > > CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
> > >                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM 
> CHAIN*RESP*ASPECT
> > >    /INTERCEPT = INCLUDE.
> > >
> > > VARCOMP
> > >    RATING BY CHAIN SECTOR RESP ASPECT ITEM
> > >    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
> > >    /METHOD = REML
> > >    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
> > >                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP 
> > > CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
> > >                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM 
> CHAIN*RESP*ASPECT
> > >    /INTERCEPT = INCLUDE.
> > >
> > > Thank you for your help.
> > >
> > > Best regards,
> > >
> > > Iuri.
> > >
> > > _______________________________________
> > > Iuri Gavronski - iuri at ufrgs.br
> > > doutorando
> > > UFRGS/PPGA/NITEC - www.ppga.ufrgs.br Brazil
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From HDoran at air.org  Thu Aug 17 17:19:42 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 17 Aug 2006 11:19:42 -0400
Subject: [R] Variance Components in R
Message-ID: <2323A6D37908A847A7C32F1E3662C80E27715B@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/951deef8/attachment.pl 

From iuri at proxima.adm.br  Thu Aug 17 17:23:06 2006
From: iuri at proxima.adm.br (Iuri Gavronski)
Date: Thu, 17 Aug 2006 12:23:06 -0300
Subject: [R] Fwd:  Variance Components in R
In-Reply-To: <60ad85c90608170815l4d45303u8dbb9979be60f123@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E277154@dc1ex01.air.org>
	<60ad85c90608170815l4d45303u8dbb9979be60f123@mail.gmail.com>
Message-ID: <60ad85c90608170823x2c7a7667h21cced4f48bc401a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/b52d63a0/attachment.pl 

From rab45+ at pitt.edu  Thu Aug 17 17:41:41 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Thu, 17 Aug 2006 11:41:41 -0400
Subject: [R] Specifying Path Model in SEM for CFA
In-Reply-To: <20060817125730.ELLX29052.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <20060817125730.ELLX29052.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1155829301.3603.22.camel@localhost.localdomain>

sem() handles only equality constraints among parameters, and this model
> requires linear inequality constraints. 
> 
> I'm aware of SEM software that handles inequality constraints, but I'm not
> aware of anything in R that will do it "out of the box." One possibility is
> to write out the likelihood (or "fitting function") for your model and
> perform a bounded optimization using optim(). It would probably be a fair
> amount of work setting up the problem.
> 
> Finally, there are tricks that permit the imposition of general constraints
> and inequality constraints using software, like sem(), that handles only
> equality constraints. It's probably possible to do what you want using such
> a trick, but it would be awkward. See the references given in Bollen,
> Structural Equations with Latent Variables (Wiley, 1989), pp. 401-403.
> 
> I'm sorry that I can't be of more direct help.
>  John


Thanks. I'll explore the options you mention. I would like to use R
because I need to couple this with block bootstrapping to handle time
dependencies.

Rick


From h.wickham at gmail.com  Thu Aug 17 17:58:48 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 17 Aug 2006 12:58:48 -0300
Subject: [R] Plots Without Displaying
In-Reply-To: <001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>
References: <44E384BC.8090709@botelho-machado.de>
	<001201c6c178$6c199510$0e010a0a@headquarters.silicoinsights>
Message-ID: <f8e6ff050608170858k1fff689eue335301fef6a1ba7@mail.gmail.com>

> Yes, you can do that for lattice-based plots.  The functions in the lattice
> package produce objects of class "trellis" which can be stored in a list and
> processed or updated at a later time:

Or for ggplot based plots:

install.packages("ggplot")
library(ggplot)

> plotList <- list(length=3)
> plotList[[1]] <- qplot(yield, site, data=barley)
> plotList[[2]] <- qplot(yield, variety, data=barley)
> plotList[[3]] <- qplot(yield, year, data=barley)

Which actually stores plot objects which are independent of their
representation as graphics.

Hadley


From spencer.graves at pdf.com  Thu Aug 17 17:59:29 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Aug 2006 08:59:29 -0700
Subject: [R] Variance Components in R
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E27715B@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E27715B@dc1ex01.air.org>
Message-ID: <44E49261.8010509@pdf.com>

Hi, Iuri: 

      If you've got an 8086 AND a huge data set, compute time might be a 
problem with 'lmer'.  However, if you a reasonably modern computer and 
only a a few thousand observations, 'lmer' should complete almost in the 
blink of an eye -- or at least in less time than it would talk for a cup 
of coffee. 

      Spencer

Doran, Harold wrote:
> This will (should) be a piece of cake for lmer. But, I don't speak SPSS.
> Can you write your model out as a linear model and give a brief
> description of the data and your problem?
>  
> In addition to what Spencer noted as help below, you should also check
> out the vignette in the mlmRev package. This will give you many
> examples.
>  
> vignette('MlmSoftRev')
>  
>  
>  
>
>
> ________________________________
>
> 	From: prof.iuri at gmail.com [mailto:prof.iuri at gmail.com] On Behalf
> Of Iuri Gavronski
> 	Sent: Thursday, August 17, 2006 11:16 AM
> 	To: Doran, Harold
> 	Subject: Re: [R] Variance Components in R
> 	
> 	
> 	9500 records. It didn`t run in SPSS or SAS on Windows machines,
> so I am trying to convert the SPSS script to R to run in a RISC station
> at the university.
> 	
> 	
> 	On 8/17/06, Doran, Harold <HDoran at air.org> wrote: 
>
> 		Iuri: 
> 		
> 		The lmer function is optimal for large data with crossed
> random effects.
> 		How large are your data?
> 		
> 		> -----Original Message-----
> 		> From: r-help-bounces at stat.math.ethz.ch 
> 		> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Iuri Gavronski
> 		> Sent: Thursday, August 17, 2006 11:08 AM
> 		> To: Spencer Graves
> 		> Cc: r-help at stat.math.ethz.ch
> 		> Subject: Re: [R] Variance Components in R
> 		>
> 		> Thank you for your reply.
> 		> VARCOMP is available at SPSS advanced models, I'm not
> sure 
> 		> for how long it exists... I only work with SPSS for
> the last
> 		> 4 years...
> 		> My model only has crossed random effects, what perhaps
> would
> 		> drive me to lmer().
> 		> However, as I have unbalanced data (why it is normally
> called 
> 		> 'unbalanced design'? the data was not intended to be
> 		> unbalanced, only I could not get responses for all
> cells...),
> 		> I'm afraid that REML would take too much CPU, memory
> and time
> 		> to execute, and MINQUE would be faster and provide
> similar 
> 		> variance estimates (please, correct me if I'm wrong on
> that point).
> 		> I only found MINQUE on the maanova package, but as my
> study
> 		> is very far from genetics, I'm not sure I can use this
> package.
> 		> Any comment would be appreciated. 
> 		> Iuri
> 		>
> 		> On 8/16/06, Spencer Graves <spencer.graves at pdf.com>
> wrote:
> 		> >
> 		> >       I used SPSS over 25 years ago, but I don't
> recall
> 		> ever fitting a
> 		> > variance components model with it.  Are all your
> random
> 		> effects nested?
> 		> > If they were, I would recommend you use 'lme' in the
> 'nlme' package.
> 		> > However, if you have crossed random effects, I
> suggest you 
> 		> try 'lmer'
> 		> > associated with the 'lme4' package.
> 		> >
> 		> >       For 'lmer', documentation is available in
> Douglas
> 		> Bates. Fitting
> 		> > linear mixed models in R. /R News/, 5(1):27-30, May
> 2005 
> 		> > (www.r-project.org -> newsletter).  I also recommend
> you try the
> 		> > vignette available with the 'mlmRev' package (see,
> e.g.,
> 		> >
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html).
> 		> >
> 		> >        Excellent documentation for both 'lme' (and
> indirectly for
> 		> > 'lmer') is available in Pinheiro and Bates (2000)
> 		> Mixed-Effects Models
> 		> > in S and S-Plus (Springer).  I have personally
> recommended
> 		> this book
> 		> > so many times on this listserve that I just now got
> 234 hits for
> 		> > RSiteSearch("graves pinheiro").  Please don't
> hesitate to pass this 
> 		> > recommendation to your university library.  This
> book is
> 		> the primary
> 		> > documentation for the 'nlme' package, which is part
> of the
> 		> standard R
> 		> > distribution.  A subdirectory
> "~library\nlme\scripts" of your R 
> 		> > installation includes files named "ch01.R",
> "ch02.R", ...,
> 		> "ch06.R",
> 		> > "ch08.R", containing the R scripts described in the
> book.  These R
> 		> > script files make it much easier and more enjoyable
> to study that 
> 		> > book, because they make it much easier to try the
> commands
> 		> described
> 		> > in the book, one line at a time, testing
> modifications to check you
> 		> > comprehension, etc.  In addition to avoiding
> problems with 
> 		> > typographical errors, it also automatically
> overcomes a few
> 		> minor but
> 		> > substantive changes in the notation between S-Plus
> and R.
> 		> >
> 		> >       Also, the "MINQUE" method has been obsolete
> for over 
> 		> 25 years.
> 		> > I recommend you use method = "REML" except for when
> you want to
> 		> > compare two nested models with different fixed
> effects;  in
> 		> that case,
> 		> > you should use method = "ML", as explained in
> Pinheiro and 
> 		> Bates (2000).
> 		> >
> 		> >       Hope this helps.
> 		> >       Spencer Graves
> 		> >
> 		> > Iuri Gavronski wrote:
> 		> > > Hi,
> 		> > >
> 		> > > I'm trying to fit a model using variance
> components in R, but if 
> 		> > > very new on it, so I'm asking for your help.
> 		> > >
> 		> > > I have imported the SPSS database onto R, but I
> don't know how to
> 		> > > convert the commands... the SPSS commands I'm
> trying to 
> 		> convert are:
> 		> > > VARCOMP
> 		> > >    RATING BY CHAIN SECTOR RESP ASPECT ITEM
> 		> > >    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
> 		> > >    /METHOD = MINQUE (1)
> 		> > >    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM 
> 		> > >                SECTOR*RESP SECTOR*ASPECT
> SECTOR*ITEM CHAIN*RESP
> 		> > > CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
> 		> > >                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM
> 		> CHAIN*RESP*ASPECT 
> 		> > >    /INTERCEPT = INCLUDE.
> 		> > >
> 		> > > VARCOMP
> 		> > >    RATING BY CHAIN SECTOR RESP ASPECT ITEM
> 		> > >    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
> 		> > >    /METHOD = REML 
> 		> > >    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
> 		> > >                SECTOR*RESP SECTOR*ASPECT
> SECTOR*ITEM CHAIN*RESP
> 		> > > CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
> 		> > >                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM
>
> 		> CHAIN*RESP*ASPECT
> 		> > >    /INTERCEPT = INCLUDE.
> 		> > >
> 		> > > Thank you for your help.
> 		> > >
> 		> > > Best regards,
> 		> > >
> 		> > > Iuri. 
> 		> > >
> 		> > > _______________________________________
> 		> > > Iuri Gavronski - iuri at ufrgs.br
> 		> > > doutorando
> 		> > > UFRGS/PPGA/NITEC - www.ppga.ufrgs.br Brazil
> 		> > >
> 		> > > ______________________________________________
> 		> > > R-help at stat.math.ethz.ch mailing list
> 		> > > https://stat.ethz.ch/mailman/listinfo/r-help
> 		> > > PLEASE do read the posting guide
> 		> > http://www.R-project.org/posting-guide.html
> 		> > > and provide commented, minimal, self-contained,
> reproducible code.
> 		> > >
> 		> >
> 		>
> 		>       [[alternative HTML version deleted]] 
> 		>
> 		> ______________________________________________
> 		> R-help at stat.math.ethz.ch mailing list
> 		> https://stat.ethz.ch/mailman/listinfo/r-help 
> 		> PLEASE do read the posting guide
> 		> http://www.R-project.org/posting-guide.html
> 		> and provide commented, minimal, self-contained,
> reproducible code. 
> 		>
> 		
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Thu Aug 17 18:06:17 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Aug 2006 09:06:17 -0700
Subject: [R] Fwd:  Variance Components in R
In-Reply-To: <60ad85c90608170823x2c7a7667h21cced4f48bc401a@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E277154@dc1ex01.air.org>	<60ad85c90608170815l4d45303u8dbb9979be60f123@mail.gmail.com>
	<60ad85c90608170823x2c7a7667h21cced4f48bc401a@mail.gmail.com>
Message-ID: <44E493F9.4070209@pdf.com>

Hi, Iuri: 

      How much RAM and how fast a microprocessor (and what version of 
Windows) do you have?  You might still try it in R under Windows.  The 
results might be comparable or dramatically better in R than in SPSS or 
SAS. 

      hope this helps.
      Spencer Graves

Iuri Gavronski wrote:
> 9500 records. It didn`t run in SPSS or SAS on Windows machines, so I am
> trying to convert the SPSS script to R to run in a RISC station at the
> university.
>
> On 8/17/06, Doran, Harold <HDoran at air.org> wrote:
>   
>> Iuri:
>>
>> The lmer function is optimal for large data with crossed random effects.
>> How large are your data?
>>
>>     
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri Gavronski
>>> Sent: Thursday, August 17, 2006 11:08 AM
>>> To: Spencer Graves
>>> Cc: r-help at stat.math.ethz.ch
>>> Subject: Re: [R] Variance Components in R
>>>
>>> Thank you for your reply.
>>> VARCOMP is available at SPSS advanced models, I'm not sure
>>> for how long it exists... I only work with SPSS for the last
>>> 4 years...
>>> My model only has crossed random effects, what perhaps would
>>> drive me to lmer().
>>> However, as I have unbalanced data (why it is normally called
>>> 'unbalanced design'? the data was not intended to be
>>> unbalanced, only I could not get responses for all cells...),
>>> I'm afraid that REML would take too much CPU, memory and time
>>> to execute, and MINQUE would be faster and provide similar
>>> variance estimates (please, correct me if I'm wrong on that point).
>>> I only found MINQUE on the maanova package, but as my study
>>> is very far from genetics, I'm not sure I can use this package.
>>> Any comment would be appreciated.
>>> Iuri
>>>
>>> On 8/16/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>>>       
>>>>       I used SPSS over 25 years ago, but I don't recall
>>>>         
>>> ever fitting a
>>>       
>>>> variance components model with it.  Are all your random
>>>>         
>>> effects nested?
>>>       
>>>> If they were, I would recommend you use 'lme' in the 'nlme' package.
>>>> However, if you have crossed random effects, I suggest you
>>>>         
>>> try 'lmer'
>>>       
>>>> associated with the 'lme4' package.
>>>>
>>>>       For 'lmer', documentation is available in Douglas
>>>>         
>>> Bates. Fitting
>>>       
>>>> linear mixed models in R. /R News/, 5(1):27-30, May 2005
>>>> (www.r-project.org -> newsletter).  I also recommend you try the
>>>> vignette available with the 'mlmRev' package (see, e.g.,
>>>> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html).
>>>>
>>>>        Excellent documentation for both 'lme' (and indirectly for
>>>> 'lmer') is available in Pinheiro and Bates (2000)
>>>>         
>>> Mixed-Effects Models
>>>       
>>>> in S and S-Plus (Springer).  I have personally recommended
>>>>         
>>> this book
>>>       
>>>> so many times on this listserve that I just now got 234 hits for
>>>> RSiteSearch("graves pinheiro").  Please don't hesitate to pass this
>>>> recommendation to your university library.  This book is
>>>>         
>>> the primary
>>>       
>>>> documentation for the 'nlme' package, which is part of the
>>>>         
>>> standard R
>>>       
>>>> distribution.  A subdirectory "~library\nlme\scripts" of your R
>>>> installation includes files named "ch01.R", "ch02.R", ...,
>>>>         
>>> "ch06.R",
>>>       
>>>> "ch08.R", containing the R scripts described in the book.  These R
>>>> script files make it much easier and more enjoyable to study that
>>>> book, because they make it much easier to try the commands
>>>>         
>>> described
>>>       
>>>> in the book, one line at a time, testing modifications to check you
>>>> comprehension, etc.  In addition to avoiding problems with
>>>> typographical errors, it also automatically overcomes a few
>>>>         
>>> minor but
>>>       
>>>> substantive changes in the notation between S-Plus and R.
>>>>
>>>>       Also, the "MINQUE" method has been obsolete for over
>>>>         
>>> 25 years.
>>>       
>>>> I recommend you use method = "REML" except for when you want to
>>>> compare two nested models with different fixed effects;  in
>>>>         
>>> that case,
>>>       
>>>> you should use method = "ML", as explained in Pinheiro and
>>>>         
>>> Bates (2000).
>>>       
>>>>       Hope this helps.
>>>>       Spencer Graves
>>>>
>>>> Iuri Gavronski wrote:
>>>>         
>>>>> Hi,
>>>>>
>>>>> I'm trying to fit a model using variance components in R, but if
>>>>> very new on it, so I'm asking for your help.
>>>>>
>>>>> I have imported the SPSS database onto R, but I don't know how to
>>>>> convert the commands... the SPSS commands I'm trying to
>>>>>           
>>> convert are:
>>>       
>>>>> VARCOMP
>>>>>    RATING BY CHAIN SECTOR RESP ASPECT ITEM
>>>>>    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
>>>>>    /METHOD = MINQUE (1)
>>>>>    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
>>>>>                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
>>>>> CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
>>>>>                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM
>>>>>           
>>> CHAIN*RESP*ASPECT
>>>       
>>>>>    /INTERCEPT = INCLUDE.
>>>>>
>>>>> VARCOMP
>>>>>    RATING BY CHAIN SECTOR RESP ASPECT ITEM
>>>>>    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
>>>>>    /METHOD = REML
>>>>>    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
>>>>>                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
>>>>> CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
>>>>>                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM
>>>>>           
>>> CHAIN*RESP*ASPECT
>>>       
>>>>>    /INTERCEPT = INCLUDE.
>>>>>
>>>>> Thank you for your help.
>>>>>
>>>>> Best regards,
>>>>>
>>>>> Iuri.
>>>>>
>>>>> _______________________________________
>>>>> Iuri Gavronski - iuri at ufrgs.br
>>>>> doutorando
>>>>> UFRGS/PPGA/NITEC - www.ppga.ufrgs.br Brazil
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>>           
>>>> http://www.R-project.org/posting-guide.html
>>>>         
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>           
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>       
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From iuri at proxima.adm.br  Thu Aug 17 19:10:00 2006
From: iuri at proxima.adm.br (Iuri Gavronski)
Date: Thu, 17 Aug 2006 14:10:00 -0300
Subject: [R] Fwd: Variance Components in R
In-Reply-To: <44E493F9.4070209@pdf.com>
References: <2323A6D37908A847A7C32F1E3662C80E277154@dc1ex01.air.org>
	<60ad85c90608170815l4d45303u8dbb9979be60f123@mail.gmail.com>
	<60ad85c90608170823x2c7a7667h21cced4f48bc401a@mail.gmail.com>
	<44E493F9.4070209@pdf.com>
Message-ID: <60ad85c90608171010v4de6761l639a8024d5e86862@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/6a973007/attachment.pl 

From iuri at ufrgs.br  Thu Aug 17 19:26:23 2006
From: iuri at ufrgs.br (Iuri Gavronski)
Date: Thu, 17 Aug 2006 14:26:23 -0300
Subject: [R] Variance Components in R
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E27715B@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E27715B@dc1ex01.air.org>
Message-ID: <60ad85c90608171026k377fc82l5d2efbe106c9b091@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/60cf18a5/attachment.pl 

From dieter.menne at menne-biomed.de  Thu Aug 17 19:38:35 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 17 Aug 2006 17:38:35 +0000 (UTC)
Subject: [R] nls convergence problem
References: <44E30DEB.1050908@fz-rossendorf.de>
	<OFA84CBC25.463F1A9F-ON852571CC.004F14CD-852571CC.005192C2@epamail.epa.gov>
	<loom.20060817T093004-77@post.gmane.org>
Message-ID: <loom.20060817T192124-52@post.gmane.org>

Yours truly <dieter.menne <at> menne-biomed.de> writes:
...
> Recently, a colleague fitted gastric emptying
> curves using GraphPad, with 100% success, and
> nls failed for one third of  these.  When we
> checked GraphPads output more closely, some of
> the coefficients looked like 2.1 with a confidence
> interval in the range  -27128 ... 314141. Nobody
> forces you to look at these, though, when using
> GraphPad.
>

Since my comment has stirred a bit of an uproar, I should add
that this is not the fault of GraphPad, but that most non-linear
fitting programs, including those in the big SXXX, give the same
results. Harvey Motulsky from Graphpad/Prism informed me that
they were going to add special tests in the new versions. Their
pdf-manual on nonlinear fitting is worth a look anyway if
Bates/Watts is over your head.

And if anyone want to see sample data:

http://www.menne-biomed.de/gastempt/gastempt1.html

> I only wish nls were a little bit more polite in telling me what went wrong.

I stand corrected:

I only wish nls were less politically correct, but rather inform me
WHY it faltered.

Dieter


From spencer.graves at pdf.com  Thu Aug 17 19:46:45 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Aug 2006 10:46:45 -0700
Subject: [R] Fwd: Variance Components in R
In-Reply-To: <60ad85c90608171010v4de6761l639a8024d5e86862@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E277154@dc1ex01.air.org>	
	<60ad85c90608170815l4d45303u8dbb9979be60f123@mail.gmail.com>	
	<60ad85c90608170823x2c7a7667h21cced4f48bc401a@mail.gmail.com>	
	<44E493F9.4070209@pdf.com>
	<60ad85c90608171010v4de6761l639a8024d5e86862@mail.gmail.com>
Message-ID: <44E4AB85.7050800@pdf.com>

      Burt Gunter just reminded me that the completion time could also 
be affected by the numbers of levels of each of the factors, especially 
random effects:  With N records, any variance components / mixed model 
software using MLE or REML will have to invert repeatedly an N x N 
matrix for the covariance structure of the random effects and noise.  If 
the software recognizes your design as having some simple structure, 
this can be quite fast;  otherwise, it could be a Herculean task.  In 
your case with N = 9500 records, just one copy of this covariance matrix 
could consume a substantial portion of 1GB RAM.  I compute 
8*9500*(9500-1)/2 = 361Mbytes. 

      However, any software that recognizes special structure in your 
design may be able to do the required computations without ever 
constructing a matrix this large.  I would say that it's still worth a 
try in R on your laptop or on the machine with 1GB RAM:  'lmer' might 
recognize special structure that neither of the other two do (and vice 
versa). 

      Hope this helps. 
      Spencer Graves    

Iuri Gavronski wrote:
> We have tried on many machines, from my laptop to a dual core Intel 
> processor with 1GB of RAM.
>
> On 8/17/06, *Spencer Graves* < spencer.graves at pdf.com 
> <mailto:spencer.graves at pdf.com>> wrote:
>
>     Hi, Iuri:
>
>           How much RAM and how fast a microprocessor (and what version of
>     Windows) do you have?  You might still try it in R under Windows.  The
>     results might be comparable or dramatically better in R than in
>     SPSS or
>     SAS.
>
>           hope this helps.
>           Spencer Graves
>
>     Iuri Gavronski wrote:
>     > 9500 records. It didn`t run in SPSS or SAS on Windows machines,
>     so I am
>     > trying to convert the SPSS script to R to run in a RISC station
>     at the
>     > university.
>     >
>     > On 8/17/06, Doran, Harold < HDoran at air.org
>     <mailto:HDoran at air.org>> wrote:
>     >
>     >> Iuri:
>     >>
>     >> The lmer function is optimal for large data with crossed random
>     effects.
>     >> How large are your data?
>     >>
>     >>
>     >>> -----Original Message-----
>     >>> From: r-help-bounces at stat.math.ethz.ch
>     <mailto:r-help-bounces at stat.math.ethz.ch>
>     >>> [mailto: r-help-bounces at stat.math.ethz.ch
>     <mailto:r-help-bounces at stat.math.ethz.ch>] On Behalf Of Iuri Gavronski
>     >>> Sent: Thursday, August 17, 2006 11:08 AM
>     >>> To: Spencer Graves
>     >>> Cc: r-help at stat.math.ethz.ch <mailto:r-help at stat.math.ethz.ch>
>     >>> Subject: Re: [R] Variance Components in R
>     >>>
>     >>> Thank you for your reply.
>     >>> VARCOMP is available at SPSS advanced models, I'm not sure
>     >>> for how long it exists... I only work with SPSS for the last
>     >>> 4 years...
>     >>> My model only has crossed random effects, what perhaps would
>     >>> drive me to lmer().
>     >>> However, as I have unbalanced data (why it is normally called
>     >>> 'unbalanced design'? the data was not intended to be
>     >>> unbalanced, only I could not get responses for all cells...),
>     >>> I'm afraid that REML would take too much CPU, memory and time
>     >>> to execute, and MINQUE would be faster and provide similar
>     >>> variance estimates (please, correct me if I'm wrong on that
>     point).
>     >>> I only found MINQUE on the maanova package, but as my study
>     >>> is very far from genetics, I'm not sure I can use this package.
>     >>> Any comment would be appreciated.
>     >>> Iuri
>     >>>
>     >>> On 8/16/06, Spencer Graves < spencer.graves at pdf.com
>     <mailto:spencer.graves at pdf.com>> wrote:
>     >>>
>     >>>>       I used SPSS over 25 years ago, but I don't recall
>     >>>>
>     >>> ever fitting a
>     >>>
>     >>>> variance components model with it.  Are all your random
>     >>>>
>     >>> effects nested?
>     >>>
>     >>>> If they were, I would recommend you use 'lme' in the 'nlme'
>     package.
>     >>>> However, if you have crossed random effects, I suggest you
>     >>>>
>     >>> try 'lmer'
>     >>>
>     >>>> associated with the 'lme4' package.
>     >>>>
>     >>>>       For 'lmer', documentation is available in Douglas
>     >>>>
>     >>> Bates. Fitting
>     >>>
>     >>>> linear mixed models in R. /R News/, 5(1):27-30, May 2005
>     >>>> (www.r-project.org <http://www.r-project.org> ->
>     newsletter).  I also recommend you try the
>     >>>> vignette available with the 'mlmRev' package (see, e.g.,
>     >>>> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html
>     <http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html>).
>     >>>>
>     >>>>        Excellent documentation for both 'lme' (and indirectly for
>     >>>> 'lmer') is available in Pinheiro and Bates (2000)
>     >>>>
>     >>> Mixed-Effects Models
>     >>>
>     >>>> in S and S-Plus (Springer).  I have personally recommended
>     >>>>
>     >>> this book
>     >>>
>     >>>> so many times on this listserve that I just now got 234 hits for
>     >>>> RSiteSearch("graves pinheiro").  Please don't hesitate to
>     pass this
>     >>>> recommendation to your university library.  This book is
>     >>>>
>     >>> the primary
>     >>>
>     >>>> documentation for the 'nlme' package, which is part of the
>     >>>>
>     >>> standard R
>     >>>
>     >>>> distribution.  A subdirectory "~library\nlme\scripts" of your R
>     >>>> installation includes files named "ch01.R", "ch02.R", ...,
>     >>>>
>     >>> "ch06.R",
>     >>>
>     >>>> "ch08.R", containing the R scripts described in the
>     book.  These R
>     >>>> script files make it much easier and more enjoyable to study that
>     >>>> book, because they make it much easier to try the commands
>     >>>>
>     >>> described
>     >>>
>     >>>> in the book, one line at a time, testing modifications to
>     check you
>     >>>> comprehension, etc.  In addition to avoiding problems with
>     >>>> typographical errors, it also automatically overcomes a few
>     >>>>
>     >>> minor but
>     >>>
>     >>>> substantive changes in the notation between S-Plus and R.
>     >>>>
>     >>>>       Also, the "MINQUE" method has been obsolete for over
>     >>>>
>     >>> 25 years.
>     >>>
>     >>>> I recommend you use method = "REML" except for when you want to
>     >>>> compare two nested models with different fixed effects;  in
>     >>>>
>     >>> that case,
>     >>>
>     >>>> you should use method = "ML", as explained in Pinheiro and
>     >>>>
>     >>> Bates (2000).
>     >>>
>     >>>>       Hope this helps.
>     >>>>       Spencer Graves
>     >>>>
>     >>>> Iuri Gavronski wrote:
>     >>>>
>     >>>>> Hi,
>     >>>>>
>     >>>>> I'm trying to fit a model using variance components in R,
>     but if
>     >>>>> very new on it, so I'm asking for your help.
>     >>>>>
>     >>>>> I have imported the SPSS database onto R, but I don't know
>     how to
>     >>>>> convert the commands... the SPSS commands I'm trying to
>     >>>>>
>     >>> convert are:
>     >>>
>     >>>>> VARCOMP
>     >>>>>    RATING BY CHAIN SECTOR RESP ASPECT ITEM
>     >>>>>    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
>     >>>>>    /METHOD = MINQUE (1)
>     >>>>>    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
>     >>>>>                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
>     >>>>> CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
>     >>>>>                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM
>     >>>>>
>     >>> CHAIN*RESP*ASPECT
>     >>>
>     >>>>>    /INTERCEPT = INCLUDE.
>     >>>>>
>     >>>>> VARCOMP
>     >>>>>    RATING BY CHAIN SECTOR RESP ASPECT ITEM
>     >>>>>    /RANDOM = CHAIN SECTOR RESP ASPECT ITEM
>     >>>>>    /METHOD = REML
>     >>>>>    /DESIGN = CHAIN SECTOR RESP ASPECT ITEM
>     >>>>>                SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
>     >>>>> CHAIN*ASPECT CHAIN*ITEM RESP*ASPECT RESP*ITEM
>     >>>>>                SECTOR*RESP*ASPECT SECTOR*RESP*ITEM
>     >>>>>
>     >>> CHAIN*RESP*ASPECT
>     >>>
>     >>>>>    /INTERCEPT = INCLUDE.
>     >>>>>
>     >>>>> Thank you for your help.
>     >>>>>
>     >>>>> Best regards,
>     >>>>>
>     >>>>> Iuri.
>     >>>>>
>     >>>>> _______________________________________
>     >>>>> Iuri Gavronski - iuri at ufrgs.br <mailto:iuri at ufrgs.br>
>     >>>>> doutorando
>     >>>>> UFRGS/PPGA/NITEC - www.ppga.ufrgs.br
>     <http://www.ppga.ufrgs.br> Brazil
>     >>>>>
>     >>>>> ______________________________________________
>     >>>>> R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>
>     mailing list
>     >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     >>>>> PLEASE do read the posting guide
>     >>>>>
>     >>>> http://www.R-project.org/posting-guide.html
>     >>>>
>     >>>>> and provide commented, minimal, self-contained, reproducible
>     code.
>     >>>>>
>     >>>>>
>     >>>       [[alternative HTML version deleted]]
>     >>>
>     >>> ______________________________________________
>     >>> R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>
>     mailing list
>     >>> https://stat.ethz.ch/mailman/listinfo/r-help
>     >>> PLEASE do read the posting guide
>     >>> http://www.R-project.org/posting-guide.html
>     >>> and provide commented, minimal, self-contained, reproducible
>     code.
>     >>>
>     >>>
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>
>     mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>     >
>
>


From vistocco at unicas.it  Thu Aug 17 19:48:38 2006
From: vistocco at unicas.it (Domenico Vistocco)
Date: Thu, 17 Aug 2006 19:48:38 +0200
Subject: [R] rbind-ing vectors inside lists
In-Reply-To: <44E4EC48.7050609@bitwrit.com.au>
References: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>
	<1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
	<C3E2C445-30FF-4A45-8261-67A7DE7B9376@eva.mpg.de>
	<44E4EC48.7050609@bitwrit.com.au>
Message-ID: <7.0.0.16.0.20060817192057.04246fd0@unicas.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/38a2065f/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 17 19:53:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 13:53:33 -0400
Subject: [R] rbind-ing vectors inside lists
In-Reply-To: <7.0.0.16.0.20060817192057.04246fd0@unicas.it>
References: <2323A6D37908A847A7C32F1E3662C80E27702E@dc1ex01.air.org>
	<1159.144.173.76.117.1155731278.squirrel@www.webmail.ex.ac.uk>
	<C3E2C445-30FF-4A45-8261-67A7DE7B9376@eva.mpg.de>
	<44E4EC48.7050609@bitwrit.com.au>
	<7.0.0.16.0.20060817192057.04246fd0@unicas.it>
Message-ID: <971536df0608171053s2bc63e03je4fdf35d614d3c6c@mail.gmail.com>

Try:

mapply(rbind, a, b, SIMPLIFY = FALSE)


On 8/17/06, Domenico Vistocco <vistocco at unicas.it> wrote:
> Dear helpeRs,
>
> suppose I have two lists as follows:
>
> a = list(1:5,5:9)
> b = lapply(a,"*",2)
>
>
> I would like to rbind-ing the two lists, that is I would like to use
> something as rbind applied
> component to component for the two list.
>
> I have used the following solution:
>
> fun.tile.wt = function(list1, list2)
> {
>         for(i in 1:length(list1))
>         {
>                 list1[[i]]=rbind(list1[[i]],list2[[i]])
>         }
>         list1
> }
> fun.tile.wt(a,b)
>
>
> Is it possible to directly obtain the result using the apply family
> (or something else)?
>
> Any suggestions is appreciated.
>
> Thanks in advance,
> domenico vistocco
>        [[alternative HTML version deleted]]
>
>
> Chiacchiera con i tuoi amici in tempo reale!
>  http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Max.Kuhn at pfizer.com  Thu Aug 17 19:56:43 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 17 Aug 2006 13:56:43 -0400
Subject: [R] unlink disables help?
Message-ID: <71257D09F114DA4A8E134DEAC70F25D305E29E9D@groamrexm03.amer.pfizer.com>

I was hoping that someone could try to reproduce an error that I am
getting. The R Site Search keeps timing out on me, so apologies of this
has already come up.

I'm using 

> R.version
               _                         
platform       i386-pc-mingw32           
arch           i386                      
os             mingw32                   
system         i386, mingw32             
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)

When I use unlink as below, the help system is disabled:

> ?print
> testPath <- tempdir()
> print(testPath)
[1] "C:\\WINDOWS\\TEMP\\Rtmpo5Wnqb"
> file.exists(testPath)
[1] TRUE
> unlink(testPath, recursive = TRUE)
> ?print
Error in int.unzip(file.path(path, zipname), topic, tmpd) : 
        'destination' does not exist

I can produce the same error with Version 2.3.0 (2006-04-24) on Windows.

I haven't been able to reproduce this with directories that are created
using other means:

> ?print
> testPath <- "c:\\tmp\\unlinkTest"
> dir.create(testPath)
> file.exists(testPath)
[1] TRUE
> unlink(testPath, recursive = TRUE)
> ?print
>

Thanks,

Max

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From rmh at temple.edu  Thu Aug 17 20:11:08 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 17 Aug 2006 14:11:08 -0400 (EDT)
Subject: [R] rbind-ing vectors inside lists
Message-ID: <20060817141108.BGE15930@po-d.temple.edu>

## initial example
a = list(1:5, 5:9)
b = lapply(a,"*",2)

library(abind)  ## you may need to download abind from CRAN
abind(data.frame(a), data.frame(b), along=.5)

## data.frames with column names
a = data.frame(first=1:5, second=5:9)
b = a^2
abind(a, b, along=.5, new.names=c("a","b"))


From dusa.adrian at gmail.com  Thu Aug 17 20:23:57 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 17 Aug 2006 21:23:57 +0300
Subject: [R] R Site Search directly from Firefox's address bar
Message-ID: <200608172123.58201.dusa.adrian@gmail.com>


Dear list,

For all those interested who use Firefox as the main browser, here is a quick 
way to make R related searches:

type "about:config" in the address bar
search for "keyword.url"
and modify it 
to "http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?idxname=functions&idxname=docs&idxname=Rhelp02a&query="

>From now on, every keyword(s) you type in the address bar will take you 
directly to the first page of hits at http://finzi.psych.upenn.edu

I found this very helpful.
Best,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From p.dalgaard at biostat.ku.dk  Thu Aug 17 20:31:13 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Aug 2006 20:31:13 +0200
Subject: [R] unlink disables help?
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D305E29E9D@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D305E29E9D@groamrexm03.amer.pfizer.com>
Message-ID: <x2odujmcy6.fsf@turmalin.kubism.ku.dk>

"Kuhn, Max" <Max.Kuhn at pfizer.com> writes:

> I was hoping that someone could try to reproduce an error that I am
> getting. The R Site Search keeps timing out on me, so apologies of this
> has already come up.
> 
> I'm using 
> 
> > R.version
<never mind>

> 
> When I use unlink as below, the help system is disabled:
> 
> > ?print
> > testPath <- tempdir()
> > print(testPath)
> [1] "C:\\WINDOWS\\TEMP\\Rtmpo5Wnqb"
> > file.exists(testPath)
> [1] TRUE
> > unlink(testPath, recursive = TRUE)
> > ?print
> Error in int.unzip(file.path(path, zipname), topic, tmpd) : 
>         'destination' does not exist
> 
> I can produce the same error with Version 2.3.0 (2006-04-24) on Windows.

Read the documentation *carefully*:

Value:

     For 'tempfile' ....

     For 'tempdir', the path of the per-session temporary directory.


And there is only one, and it is in there that the help system keeps
its stuff....

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Thu Aug 17 20:32:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 14:32:00 -0400
Subject: [R] day, month, year functions
In-Reply-To: <17636.10309.779152.236935@stat.math.ethz.ch>
References: <s4db56b1.019@pgn.com>
	<971536df0608101622r42321728w69aa8efedcb3701d@mail.gmail.com>
	<loom.20060811T022520-438@post.gmane.org>
	<17636.10309.779152.236935@stat.math.ethz.ch>
Message-ID: <971536df0608171132x4aa65562k82c2bb364c9a9f32@mail.gmail.com>

On 8/17/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Gregor" == Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
> >>>>>     on Fri, 11 Aug 2006 00:27:27 +0000 (UTC) writes:
>
>    Gregor> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
>    >>
>    >> Here are three ways:
>    >>
>    >> xx <- as.Date("2006-01-05")
>    >>
>    >> # 1. use as.POSIXlt
>    >> as.POSIXlt(xx)$mday
>    >> as.POSIXlt(xx)$mon + 1
>    >> as.POSIXlt(xx)$year + 1900
>    >>
>    >> # 2. use format
>    >> as.numeric(format(xx, "%d"))
>    >> as.numeric(format(xx, "%m"))
>    >> as.numeric(format(xx, "%Y"))
>    >>
>    >> # 3. use month.day.year in chron package
>    >> library(chron)
>    >> month.day.year(unclass(xx))$day
>    >> month.day.year(unclass(xx))$month
>    >> month.day.year(unclass(xx))$year
>
>    Gregor> Hi,
>
>    Gregor> it would really be great if there would be
>
>    Gregor> sec(), min(), hour() day(), month(), year()
>
>    Gregor> generic functions that would work on all "date" classes. Where
>    Gregor> applicable of course. I imagine that argument to get out integer
>    Gregor> or character would alse be nice.
>
> I disagree pretty strongly:
>
> - We definitely don't want min() to return minutes instead of
>  minimum !
>
> - Why pollute the namespace with 6 (well, actualy 5!) new
>  function names, when  as.POSIXlt()
>  *REALLY* is there exactly for this purpose ???
>
> I rather think the authors of each of the other old-fashioned
> "date" classes should provide as.POSIXlt() methods for their
> classes.
>
> Then, we'd have uniform interfaces, following's Gabor's "# 1."
> above.
>
> Martin Maechler, ETH Zurich


There are two problems:

1. as.POSIXlt is not generic.  (This problem may not be too important
given that as.POSIXlt does handle "Date" and chron "dates" classes
already but in terms of handling all potential classes its a limitation.)

2. in the case of as.POSIXlt converting chron "dates" objects to
POSIXlt there is a time zone consideration, as shown below, where
today, August 17th in the Eastern Daylight Time zone, is displayed
as August 16th using as.POSIXlt unless we use tz = "GMT"

> library(chron)
> # today is August 17th.
> Sys.Date()
[1] "2006-08-17"
> chron(unclass(Sys.Date()))
[1] 08/17/06
> Sys.time()
[1] "2006-08-17 14:28:19 Eastern Daylight Time"
> as.POSIXlt(Sys.Date())
[1] "2006-08-17"
> as.POSIXlt(chron(unclass(Sys.Date())))
[1] "2006-08-16 20:00:00 Eastern Daylight Time"
> as.POSIXlt(chron(unclass(Sys.Date())), tz = "GMT")
[1] "2006-08-17 GMT"
> R.version.string # Windows XP
[1] "Version 2.3.1 Patched (2006-06-04 r38279)"


From p.dalgaard at biostat.ku.dk  Thu Aug 17 20:41:45 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Aug 2006 20:41:45 +0200
Subject: [R] R Site Search directly from Firefox's address bar
In-Reply-To: <200608172123.58201.dusa.adrian@gmail.com>
References: <200608172123.58201.dusa.adrian@gmail.com>
Message-ID: <x2fyfvmcgm.fsf@turmalin.kubism.ku.dk>

Adrian Dusa <dusa.adrian at gmail.com> writes:

> Dear list,
> 
> For all those interested who use Firefox as the main browser, here is a quick 
> way to make R related searches:
> 
> type "about:config" in the address bar
> search for "keyword.url"
> and modify it 
> to "http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?idxname=functions&idxname=docs&idxname=Rhelp02a&query="
> 
> >From now on, every keyword(s) you type in the address bar will take you 
> directly to the first page of hits at http://finzi.psych.upenn.edu
> 
> I found this very helpful.

Breaks the feature that you get to www.r-project.org just by typing
"r", though...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From allen at zoology.ubc.ca  Thu Aug 17 20:46:41 2006
From: allen at zoology.ubc.ca (Pamela Allen)
Date: Thu, 17 Aug 2006 11:46:41 -0700
Subject: [R] Boxplot Help: Re-ordering the x-axis
Message-ID: <000701c6c22d$7a8d3830$5d2f678e@AERL8167G81>

I am having a problem using boxlpot with my data.  I have my data arranged
in a data table, and two of my columns are "mass" and "month".  I am trying
to plot the mass of my study animals by month, thus I would like to have it
in the order of January to December.  The problem is that R orders each
month in alphabetical order, and gives each month an integer value
corresponding to this (i.e. April is integer=1, August=2, September=12).  I
have tried many different ways to solve this but nothing is working.  If
anyone knows how to order the x-axis in boxplot, or alternatively, re-assign
integer values to each month that would be very helpful.  Thank you in
advance!  

Pamela Allen, MSc Candidate
University of British Columbia
Marine Mammal Research Unit, Fisheries Centre
Vancouver, B.C.  V6T 1Z4
 
allen at zoology.ubc.ca


From dusa.adrian at gmail.com  Thu Aug 17 20:56:47 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 17 Aug 2006 21:56:47 +0300
Subject: [R] R Site Search directly from Firefox's address bar
In-Reply-To: <x2fyfvmcgm.fsf@turmalin.kubism.ku.dk>
References: <200608172123.58201.dusa.adrian@gmail.com>
	<x2fyfvmcgm.fsf@turmalin.kubism.ku.dk>
Message-ID: <200608172156.48048.dusa.adrian@gmail.com>

On Thursday 17 August 2006 21:41, Peter Dalgaard wrote:
> [...]
> Breaks the feature that you get to www.r-project.org just by typing
> "r", though...

Oh, this is very simple to fix. I created a bookmark named "R" with the above 
location and assigned it a keyword "r".
Now, everytime I type "r" in the address bar it takes me to www.r-project.org
:)

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From mschwartz at mn.rr.com  Thu Aug 17 21:03:07 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 17 Aug 2006 14:03:07 -0500
Subject: [R] Boxplot Help: Re-ordering the x-axis
In-Reply-To: <000701c6c22d$7a8d3830$5d2f678e@AERL8167G81>
References: <000701c6c22d$7a8d3830$5d2f678e@AERL8167G81>
Message-ID: <1155841387.3893.21.camel@localhost.localdomain>

On Thu, 2006-08-17 at 11:46 -0700, Pamela Allen wrote:
> I am having a problem using boxlpot with my data.  I have my data arranged
> in a data table, and two of my columns are "mass" and "month".  I am trying
> to plot the mass of my study animals by month, thus I would like to have it
> in the order of January to December.  The problem is that R orders each
> month in alphabetical order, and gives each month an integer value
> corresponding to this (i.e. April is integer=1, August=2, September=12).  I
> have tried many different ways to solve this but nothing is working.  If
> anyone knows how to order the x-axis in boxplot, or alternatively, re-assign
> integer values to each month that would be very helpful.  Thank you in
> advance!  

Note the following in the Details section of ?boxplot:

"If multiple groups are supplied either as multiple arguments or via a
formula, parallel boxplots will be plotted, in the order of the
arguments or the order of the levels of the factor (see factor)."


If you are using a formula approach, then something like the following:

month <- factor(month, 
                levels = c("January", "February", 
                            ...,                 
                           "November", "December")

boxplot(mass ~ month)


See ?factor


For future reference, using:

  > RSiteSearch("boxplot order")

will search the r-help archive using the indicated key words, where you
will see that this has been covered previously.

HTH,

Marc Schwartz


From rmh at temple.edu  Thu Aug 17 21:06:17 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 17 Aug 2006 15:06:17 -0400 (EDT)
Subject: [R] Boxplot Help: Re-ordering the x-axis
Message-ID: <20060817150617.BGE26260@po-d.temple.edu>

month.name
class(month.name)
character"
tmp <- data.frame(m=rep(month.name, 2), y=rnorm(24))
bwplot(y ~ m, data=tmp)

tmp <- data.frame(m=ordered(rep(month.name, 2), levels=month.name), y=rnorm(24))
bwplot(y ~ m, data=tmp)


From mi2kelgrum at yahoo.com  Thu Aug 17 21:18:43 2006
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Thu, 17 Aug 2006 12:18:43 -0700 (PDT)
Subject: [R] problem with cut(as.Date("2006-08-14"), "week")
Message-ID: <20060817191843.7520.qmail@web60223.mail.yahoo.com>

When I run cut.Date or cut.POSIXt with argument breaks
= "weeks", the function gives the first day of that
week, unless the date is the first day of the week, in
which case it gives an error message as in:

> cut(as.Date("2006-08-16"), "week")
[1] 2006-08-14
Levels: 2006-08-14
> cut(as.Date("2006-08-14"), "week")
Error in 1:(1 + max(which(breaks < maxx))) : 
        result would be too long a vector
In addition: Warning message:
no non-missing arguments to max; returning -Inf 
> sessionInfo()
Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices"
"utils"     "datasets" 
[7] "base"     
> Sys.getlocale()
[1] "LC_COLLATE=English_United
States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United
States.1252"
> 

Bug or feature?

Mikkel


From p.murrell at auckland.ac.nz  Thu Aug 17 22:21:04 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 18 Aug 2006 08:21:04 +1200
Subject: [R] NLME: Limitations of using identify to interact
	with	scatterplots?
In-Reply-To: <40e66e0b0608170655o6e52c093hca42988db58b853b@mail.gmail.com>
References: <007f01c6c1db$c12c1c90$6f179e89@UCTPCGREGD>
	<40e66e0b0608170655o6e52c093hca42988db58b853b@mail.gmail.com>
Message-ID: <44E4CFB0.3060601@stat.auckland.ac.nz>

Hi

Take a look at panel.identify() (in the 'lattice' package).

I'm not sure if it will help you because I cannot run your example code.

Paul


Douglas Bates wrote:
> Most plotting functions in the nlme package use lattice graphics
> functions based on the grid package.  Identify will not work with
> lattice graphics.  I'm not sure if there is a replacement.
> 
> On 8/17/06, Greg Distiller <gregd at stats.uct.ac.za> wrote:
>> I have a quick question regarding the use of identify to interact with
>> points on a scatterplot. My question is essentially: can identify be used
>> when one is plotting model objects to generate diagnostic plots?
>> Specifically I am using NLME.
>> For example, I am plotting the fitted values on the x axis vs a variable
>> called log2game with the following code:
>>
>> plot(D2C29.nlme, log2game ~ fitted(.), abline=c(0,1))
>>
>> and then I have tried to use identify as follows:
>>
>> identify(D2C29.nlme$fitted[,2],Data2$log2game,row.names(Data2))
>>
>> (if I leave out the [,2] on the fitted attributes then I am told that x and
>> y are not the same length and it appears that this is due to the fact that
>> the fitted attribute has 2 columns.)
>>
>> but I get an error message that "plot.new has not been called yet".
>>
>> I am not sure if this is because I am doing something wrong or if identify
>> simply cannot be used in this context.
>>
>> Many thanks
>>
>> Greg
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From Manuel.A.Morales at williams.edu  Thu Aug 17 22:49:31 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Thu, 17 Aug 2006 16:49:31 -0400
Subject: [R] Simulate p-value in lme4
Message-ID: <1155847771.12391.46.camel@solidago.localdomain>

Dear list,

This is more of a stats question than an R question per se. First, I
realize there has been a lot of discussion about the problems with
estimating P-values from F-ratios for mixed-effects models in lme4.
Using mcmcsamp() seems like a great alternative for evaluating the
significance of individual coefficients, but not for groups of
coefficients as might occur in an experimental design with 3 treatment
levels. I'm wondering if the simulation approach I use below to estimate
the P-value for a 3-level factor is appropriate, or if there are any
suggestions on how else to approach this problem. The model and data in
the example are from section 10.4 of MASS.

Thanks!
Manuel

# Load req. package (see functions to generate data at end of script)
library(lme4)
library(MASS)

# Full and reduced models - pred is a factor with 3 levels
result.full <- lmer(y~pred+(1|subject), data=epil3, family="poisson")
result.base <- lmer(y~1+(1|subject), data=epil3, family="poisson")

# Naive P-value from LR for significance of "pred" factor
anova(result.base,result.full)$"Pr(>Chisq)"[[2]] # P-value
(test.stat <- anova(result.base,result.full)$Chisq[[2]]) # Chisq-stat

# P-value from simulation. Note that in the simulation, I use the
# estimated random effects for each subject rather than generating a new
# distribution of means. I'm not sure if this is appropriate or not ...
intercept <- fixef(result.base)
rand.effs <- ranef(result.base)[[1]]
mu <- exp(rep(intercept+rand.effs[[1]],2))

p.value <- function(iter, stat) {
  chi.stat <- vector()
  for(i in 1:iter) {
    resp <- rpois(length(mu), mu) # simulate values
    sim.data <- data.frame(y=resp,subject=epil3$subject,pred=epil3$pred)
    result.f <- lmer(y~pred+(1|subject), data=sim.data,
                     family="poisson")
    result.b <- lmer(y~1+(1|subject), data=sim.data, family="poisson")
    chi.stat[i] <- anova(result.b,result.f)$Chisq[[2]]
  }
  val <- sum(unlist(lapply(chi.stat, function(x) if(x>stat) 1 else
             0)))/iter
  hist(chi.stat)
  return(val)
}

p.value(10,test.stat) # Increase to >=1000 to get a reasonable P-value!

# Script to generate data, from section 10.4 of MASS
epil2 <- epil[epil$period == 1, ]
epil2["period"] <- rep(0, 59); epil2["y"] <- epil2["base"]
epil["time"] <- 1; epil2["time"] <- 4
epil2 <- rbind(epil, epil2)
epil2$pred <- unclass(epil2$trt) * (epil2$period > 0)
epil2$subject <- factor(epil2$subject)
epil3 <- aggregate(epil2, list(epil2$subject, epil2$period > 0),
                   function(x) if(is.numeric(x)) sum(x) else x[1])
epil3$pred <- factor(epil3$pred, labels = c("base", "placebo", "drug"))


From kubovy at virginia.edu  Thu Aug 17 22:53:24 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 17 Aug 2006 16:53:24 -0400
Subject: [R] Rgraphviz fails to load
Message-ID: <6BC5C62E-08D3-4694-A345-310E32B6FCD8@virginia.edu>

Dear r-helpers,

Can anyone suggest a remedy to the following failure of Rgraphviz to  
load?

 > library(Rgraphviz)
Loading required package: graph
Loading required package: Ruuid
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library '/Library/Frameworks/R.framework/ 
Resources/library/Rgraphviz/libs/ppc/Rgraphviz.so':
   dlopen(/Library/Frameworks/R.framework/Resources/library/Rgraphviz/ 
libs/ppc/Rgraphviz.so, 6): Library not loaded: /usr/local/lib/libpng. 
3.dylib
   Referenced from: /usr/local/lib/graphviz/libgvc.2.dylib
   Reason: image not found
Error: .onLoad failed in 'loadNamespace' for 'Rgraphviz'
Error: package/namespace load failed for 'Rgraphviz'

Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "utils"     "methods"   "stats"     "graphics"  "grDevices"  
"datasets"  "base"

other attached packages:
      graph      Ruuid        JGR     JavaGD      rJava       MASS     
lattice
   "1.10.6"   "1.10.0"    "1.4-4"    "0.3-4"    "0.4-5" "7.2-27.1"   
"0.13-10"


_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From HDoran at air.org  Thu Aug 17 23:33:20 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 17 Aug 2006 17:33:20 -0400
Subject: [R] Variance Components in R
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060817/2deb0ff7/attachment.pl 

From ruser2006 at yahoo.com  Fri Aug 18 00:04:06 2006
From: ruser2006 at yahoo.com (r user)
Date: Thu, 17 Aug 2006 15:04:06 -0700 (PDT)
Subject: [R] getting sapply to skip columns with non-numeric data?
Message-ID: <20060817220406.91844.qmail@web37001.mail.mud.yahoo.com>

getting s-apply to skip columns with non-numeric data?
I have a dataframe ?x? of w columns.

Some columns are numeric, some are not.

I wish to create a function to calculate the mean and
standard deviation of each numeric column, and then
?bind? the column mean and standard deviation to the
bottom of the dataframe.

e.g. 

tempmean <- apply(data.frame(x), 2, mean, na.rm = T)
xnew <- rbind(x,tempmean)

I am running into one small problem
what is the best
way to have sapply ?skip? the non-numeric data and
return NA?s?


From cberry at tajo.ucsd.edu  Fri Aug 18 00:40:20 2006
From: cberry at tajo.ucsd.edu (Charles Berry)
Date: Thu, 17 Aug 2006 22:40:20 +0000 (UTC)
Subject: [R] R Site Search directly from Firefox's address bar
References: <200608172123.58201.dusa.adrian@gmail.com>
	<x2fyfvmcgm.fsf@turmalin.kubism.ku.dk>
	<200608172156.48048.dusa.adrian@gmail.com>
Message-ID: <loom.20060818T003459-983@post.gmane.org>

Adrian Dusa <dusa.adrian <at> gmail.com> writes:

> 
> On Thursday 17 August 2006 21:41, Peter Dalgaard wrote:
> > [...]
> > Breaks the feature that you get to www.r-project.org just by typing
> > "r", though...
> 
> Oh, this is very simple to fix. I created a bookmark named "R" with the above 
> location and assigned it a keyword "r".
> Now, everytime I type "r" in the address bar it takes me to www.r-project.org
> :)
> 
 Or add a '%s' to your bookmark definition:
  
"http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?idxname=functions&idxname=docs&idxname=Rhelp02a&query=%s"

and assign it the keyword 'rsite'

then 'rsite <search terms>' will do the search

:-)


From ggrothendieck at gmail.com  Fri Aug 18 01:19:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 19:19:58 -0400
Subject: [R] getting sapply to skip columns with non-numeric data?
In-Reply-To: <20060817220406.91844.qmail@web37001.mail.mud.yahoo.com>
References: <20060817220406.91844.qmail@web37001.mail.mud.yahoo.com>
Message-ID: <971536df0608171619g3cfb7dcdi75b3f0151313878a@mail.gmail.com>

Use the first few rows of iris as test data and try this
where isnum is 1 for each numeric column and NA for
others.

irish <- head(iris)
isnum <- ifelse(sapply(iris, class) == "numeric", 1, NA)
iris.data <- data.matrix(iris)
rbind(iris, colMeans(iris.data) * isnum, sd(iris.data) * isnum)


On 8/17/06, r user <ruser2006 at yahoo.com> wrote:
> getting s-apply to skip columns with non-numeric data?
> I have a dataframe "x" of w columns.
>
> Some columns are numeric, some are not.
>
> I wish to create a function to calculate the mean and
> standard deviation of each numeric column, and then
> "bind" the column mean and standard deviation to the
> bottom of the dataframe.
>
> e.g.
>
> tempmean <- apply(data.frame(x), 2, mean, na.rm = T)
> xnew <- rbind(x,tempmean)
>
> I am running into one small problem?what is the best
> way to have sapply "skip" the non-numeric data and
> return NA's?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Fri Aug 18 01:30:01 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Aug 2006 19:30:01 -0400
Subject: [R] getting sapply to skip columns with non-numeric data?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB34FE@usctmx1106.merck.com>

There's something that either you have not thought of or neglected to tell
us:  If you have k variables in the data frame, you will need a data frame
of k variables and one row to be able to rbind() to the bottom of the
original one.  What are you going to put in place for non-numeric variables?

Perhaps this might help:

R> dat <- data.frame(f=factor(1:3), x=3:5, y=6:4)
R> rbind(dat, as.data.frame(lapply(dat, function(x) if (!is.numeric(x)) NA
else mean(x))))
      f x y
1     1 3 6
2     2 4 5
3     3 5 4
11 <NA> 4 5

Andy 

From: r user
> 
> getting s-apply to skip columns with non-numeric data?
> I have a dataframe "x" of w columns.
> 
> Some columns are numeric, some are not.
> 
> I wish to create a function to calculate the mean and 
> standard deviation of each numeric column, and then "bind" 
> the column mean and standard deviation to the bottom of the dataframe.
> 
> e.g. 
> 
> tempmean <- apply(data.frame(x), 2, mean, na.rm = T) xnew <- 
> rbind(x,tempmean)
> 
> I am running into one small problem...what is the best way to 
> have sapply "skip" the non-numeric data and return NA's?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From p.murrell at auckland.ac.nz  Fri Aug 18 01:25:05 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 18 Aug 2006 11:25:05 +1200
Subject: [R] DSC 2007
Message-ID: <44E4FAD1.1070102@stat.auckland.ac.nz>

Hi

This is a second call for abstracts.
Please forward and circulate to other interested parties.
[apologies for any cross-posting]

DSC 2007, a conference on systems and environments for statistical
computing, will take place in Auckland, New Zealand on February 15 & 16,
2007.

We invite abstracts on the development of software systems and computing
environments for interactive statistics.  The workshop will focus on,
but is not limited to, open source statistical computing.

The deadline for submitting abstracts is 2006-10-15 (October 15th).
Please visit the conference web page at
http://www.stat.auckland.ac.nz/dsc-2007/
and send abstracts (one page) to dsc2007 at stat.auckland.ac.nz

Paul
(on behalf of the Organising Committee)
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From toby.m at mail.utexas.edu  Fri Aug 18 01:46:14 2006
From: toby.m at mail.utexas.edu (Toby Muhlhofer)
Date: Thu, 17 Aug 2006 18:46:14 -0500
Subject: [R] Font-path error when starting X11 device in Gentoo
Message-ID: <44E4FFC6.9030507@mail.utexas.edu>

Dear R listers,

If I try to start the X11 device in Gentoo, I get the following 
complaint from R:

-------------------
 > X11()
Error in X11() : could not find any X11 fonts
Check that the Font Path is correct.
-------------------

xset -q produces the following output:

-------------------
Keyboard Control:
   auto repeat:  on    key click percent:  0    LED mask:  00000002
   auto repeat delay:  500    repeat rate:  30
   auto repeating keys:  00ffffffdffffbbf
                         fadfffdfffdfe5ef
                         ffffffffffffffff
                         ffffffffffffffff
   bell percent:  50    bell pitch:  400    bell duration:  100
Pointer Control:
   acceleration:  2/1    threshold:  4
Screen Saver:
   prefer blanking:  yes    allow exposures:  yes
   timeout:  0    cycle:  0
Colors:
   default colormap:  0x20    BlackPixel:  0    WhitePixel:  16777215
Font Path:
 
/usr/share/fonts/misc/,/usr/share/fonts/TTF/,/usr/share/fonts/Type1/,/usr/share/fonts/75dpi/,/usr/share/fonts/100dpi/
Bug Mode: compatibility mode is disabled
DPMS (Energy Star):
   Standby: 3600    Suspend: 3600    Off: 7200
   DPMS is Enabled
   Monitor is On
File paths:
   Config file:  /etc/X11/xorg.conf
   Modules path: /usr/lib/xorg/modules
   Log file:     /var/log/Xorg.0.log
-----------------------

Furthermore, no other graphics applications are complaining about not 
finding fonts.

By the way, this happens both with the Gentoo ebuilds (2.2.1 and 2.3.1) 
of R, as well as with the source tarball from the R website, compiled 
manually (with X support, of course).

Any suggestions?

Thanks,
	Toby


From xprt.wannabe at gmail.com  Fri Aug 18 04:00:00 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Thu, 17 Aug 2006 21:00:00 -0500
Subject: [R] Fitting Truncated Lognormal to a truncated data set (was:
	fitting truncated normal distribution)
Message-ID: <a4fecdd70608171900r5bf089aas9e43d57fe9f404ec@mail.gmail.com>

Dear List,

I am trying to fit Truncated Lognormal to a data set that is
'truncated' from above a certain value, say, 0.01.  Below is what I
was able to come up with.  I would appreciate it if you could review
and make any necessary changes.

# This is modified off the code for 'dtnorm' of library(msm).
dtlnorm <- function (n, mean = 0, sd = 1, lower = -Inf, upper = Inf)
{
   ret <- numeric()
   if (length(n) > 1)
       n <- length(n)
   while (length(ret) < n) {
       y <- rlnorm(n - length(ret), mean, sd)
       y <- y[y >= lower & y <= upper]
       ret <- c(ret, y)
   }
   stopifnot(length(ret) == n)
   ret
}

# This is modified off the code for 'rtnorm' of the library(msm).
rtlnorm <- function (n, mean = 0, sd = 1, lower = -Inf, upper = Inf)
{
   ret <- numeric()
   if (length(n) > 1)
       n <- length(n)
   while (length(ret) < n) {
       y <- rlnorm(n - length(ret), mean, sd)
       y <- y[y >= lower & y <= upper]
       ret <- c(ret, y)
   }
   stopifnot(length(ret) == n)
   ret
}

x <- rtlnorm(100, mean=-11.64857, sd = 3.422795, 0.01)

fitting truncated normal distribution" on 8/16/2006.
dtlnorm0 <- function(x, mean = 0, sd = 1)
{
dtlnorm(x, mean, sd, 0.01, Inf)
}

fitdistr(x, dtlnorm0, start = list(mean = -11, sd = 1))

Thank you.

platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R


From A.Robinson at ms.unimelb.edu.au  Fri Aug 18 05:04:35 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 18 Aug 2006 13:04:35 +1000
Subject: [R] NLME: Limitations of using identify to interact with
	scatterplots?
In-Reply-To: <40e66e0b0608170655o6e52c093hca42988db58b853b@mail.gmail.com>
References: <007f01c6c1db$c12c1c90$6f179e89@UCTPCGREGD>
	<40e66e0b0608170655o6e52c093hca42988db58b853b@mail.gmail.com>
Message-ID: <20060818030435.GP95817@ms.unimelb.edu.au>

Many useful diagnostic plots can be recreated in the usual plot()
framework, with only a little coding effort. In this case, I would
imagine that

plot(dframe$log2game, fitted(D2C29.nlme))
abline(0,1)

should get pretty close, if the name of the dataframe containing the
variable is 'dframe'.

Andrew

On Thu, Aug 17, 2006 at 08:55:41AM -0500, Douglas Bates wrote:
> Most plotting functions in the nlme package use lattice graphics
> functions based on the grid package.  Identify will not work with
> lattice graphics.  I'm not sure if there is a replacement.
> 
> On 8/17/06, Greg Distiller <gregd at stats.uct.ac.za> wrote:
> > I have a quick question regarding the use of identify to interact with
> > points on a scatterplot. My question is essentially: can identify be used
> > when one is plotting model objects to generate diagnostic plots?
> > Specifically I am using NLME.
> > For example, I am plotting the fitted values on the x axis vs a variable
> > called log2game with the following code:
> >
> > plot(D2C29.nlme, log2game ~ fitted(.), abline=c(0,1))
> >
> > and then I have tried to use identify as follows:
> >
> > identify(D2C29.nlme$fitted[,2],Data2$log2game,row.names(Data2))
> >
> > (if I leave out the [,2] on the fitted attributes then I am told that x and
> > y are not the same length and it appears that this is due to the fact that
> > the fitted attribute has 2 columns.)
> >
> > but I get an error message that "plot.new has not been called yet".
> >
> > I am not sure if this is because I am doing something wrong or if identify
> > simply cannot be used in this context.
> >
> > Many thanks
> >
> > Greg
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From ghos0033 at umn.edu  Fri Aug 18 05:23:03 2006
From: ghos0033 at umn.edu (Debarchana Ghosh)
Date: Thu, 17 Aug 2006 23:23:03 -0400
Subject: [R] Lattice package par.settings/trellis.par.settings questions
Message-ID: <44E53297.7070408@umn.edu>

Hi All,

I'm trying to modify some of the default graphic parameters in a 
conditional histogram. While I was able to change the default grey 
background to white, I couldn't change the axis.font or the xlab font.

I used the following code:

/histogram(~V751|V013+V025, finalbase, xlab="Heard of HIV/AIDS 
(No/Yes)", col=c("cyan","magenta"), par.settings=list(background="white"))

/The arguments for example  like /axis.font=2/, or /cex=2/ are not 
working in the /par.settings(). /I also tried to read the manual of 
/trellis.par.settings()/ but didn't understand how to use it and where 
exactly to put it.

Any help with this will be appreciated.

Thanks,
Debarchana.

-- 
Debarchana Ghosh
Research Assistant
Department of Geography
University of Minnesota
PH: 8143607580
email to: ghos0033 at umn.edu
www.tc.umn.edu/~ghos0033


From ggrothendieck at gmail.com  Fri Aug 18 06:35:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Aug 2006 00:35:53 -0400
Subject: [R] Lattice package par.settings/trellis.par.settings questions
In-Reply-To: <44E53297.7070408@umn.edu>
References: <44E53297.7070408@umn.edu>
Message-ID: <971536df0608172135v7abb6f88wbfeb5976e8d4ee9e@mail.gmail.com>

The parameter names are axis.text$font and axis.text$cex .
Try issuing the command:
  trellis.par.get()
to get a complete list.

Here is an example:

histogram(1:10, par.settings = list(axis.text = list(font = 2, cex = 0.5)))


On 8/17/06, Debarchana Ghosh <ghos0033 at umn.edu> wrote:
> Hi All,
>
> I'm trying to modify some of the default graphic parameters in a
> conditional histogram. While I was able to change the default grey
> background to white, I couldn't change the axis.font or the xlab font.
>
> I used the following code:
>
> /histogram(~V751|V013+V025, finalbase, xlab="Heard of HIV/AIDS
> (No/Yes)", col=c("cyan","magenta"), par.settings=list(background="white"))
>
> /The arguments for example  like /axis.font=2/, or /cex=2/ are not
> working in the /par.settings(). /I also tried to read the manual of
> /trellis.par.settings()/ but didn't understand how to use it and where
> exactly to put it.
>
> Any help with this will be appreciated.
>
> Thanks,
> Debarchana.
>
> --
> Debarchana Ghosh
> Research Assistant
> Department of Geography
> University of Minnesota
> PH: 8143607580
> email to: ghos0033 at umn.edu
> www.tc.umn.edu/~ghos0033
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From AnupTyagi at yahoo.com  Fri Aug 18 06:53:35 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Fri, 18 Aug 2006 04:53:35 +0000 (UTC)
Subject: [R] Lattice package par.settings/trellis.par.settings questions
References: <44E53297.7070408@umn.edu>
Message-ID: <loom.20060818T065050-416@post.gmane.org>

Please read about lattice.par.settings, and not trellis.par.settings. Trellis is
in S/S-plus. Anupam.


From gregor.gorjanc at bfro.uni-lj.si  Fri Aug 18 08:42:47 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 18 Aug 2006 08:42:47 +0200
Subject: [R] day, month, year functions
In-Reply-To: <971536df0608171132x4aa65562k82c2bb364c9a9f32@mail.gmail.com>
References: <s4db56b1.019@pgn.com>	
	<971536df0608101622r42321728w69aa8efedcb3701d@mail.gmail.com>	
	<loom.20060811T022520-438@post.gmane.org>	
	<17636.10309.779152.236935@stat.math.ethz.ch>
	<971536df0608171132x4aa65562k82c2bb364c9a9f32@mail.gmail.com>
Message-ID: <44E56167.7060906@bfro.uni-lj.si>

Gabor Grothendieck wrote:
> On 8/17/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>> >>>>> "Gregor" == Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si>
>> >>>>>     on Fri, 11 Aug 2006 00:27:27 +0000 (UTC) writes:

[snip]

> 
> There are two problems:
> 
> 1. as.POSIXlt is not generic.  (This problem may not be too important
> given that as.POSIXlt does handle "Date" and chron "dates" classes
> already but in terms of handling all potential classes its a limitation.)

But as.POSIXlt can be implemented as generic. Given the variatey of
potential classes this would be a reasonable move.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From francoisromain at free.fr  Fri Aug 18 09:08:02 2006
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 18 Aug 2006 09:08:02 +0200
Subject: [R] R Site Search directly from Firefox's address bar
In-Reply-To: <200608172156.48048.dusa.adrian@gmail.com>
References: <200608172123.58201.dusa.adrian@gmail.com>	<x2fyfvmcgm.fsf@turmalin.kubism.ku.dk>
	<200608172156.48048.dusa.adrian@gmail.com>
Message-ID: <44E56752.20908@free.fr>

Le 17.08.2006 20:56, Adrian Dusa a ?crit :
> On Thursday 17 August 2006 21:41, Peter Dalgaard wrote:
>   
>> [...]
>> Breaks the feature that you get to www.r-project.org just by typing
>> "r", though...
>>     

It breaks also every usage of the google feeling lucky default behaviour 
which is really useful I think.
There are R related firefox search plugins on mycroft. Find more info on 
that on the wiki :
http://wiki.r-project.org/rwiki/doku.php?id=tips:misc:firefox-search-plugins

You just have to type (*) [Ctrl]+[K] to go to the search textbox (and 
maybe [Alt]+[Up])
instead of [Ctrl]+[L] that leads to the location textbox
and if you do it with the mouse, it's not too far away neither

Cheers,

Romain


(*) the keyboard shortcuts may not be these one on your configuration

> Oh, this is very simple to fix. I created a bookmark named "R" with the above 
> location and assigned it a keyword "r".
> Now, everytime I type "r" in the address bar it takes me to www.r-project.org
> :)
- 
+-------------------------------------------------------------------+
|                         Romain FRANCOIS                           |
| R Graph Gallery : http://addictedtor.free.fr/graphiques           |
`-------------------------------------------------------------------+


From dusa.adrian at gmail.com  Fri Aug 18 09:40:56 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Fri, 18 Aug 2006 10:40:56 +0300
Subject: [R] R Site Search directly from Firefox's address bar
In-Reply-To: <44E56752.20908@free.fr>
References: <200608172123.58201.dusa.adrian@gmail.com>
	<200608172156.48048.dusa.adrian@gmail.com> <44E56752.20908@free.fr>
Message-ID: <200608181040.57259.dusa.adrian@gmail.com>

On Friday 18 August 2006 10:08, Romain Francois wrote:
> Le 17.08.2006 20:56, Adrian Dusa a ?crit :
> [...]
>
> It breaks also every usage of the google feeling lucky default behaviour
> which is really useful I think.
> There are R related firefox search plugins on mycroft. Find more info on
> that on the wiki :
> http://wiki.r-project.org/rwiki/doku.php?id=tips:misc:firefox-search-plugins

Thanks, very nice! Already added a couple of search plugins. 
Well, it's just a matter of taste, what one use Firefox's address bar for: 
Google Feeling Lucky vs. R Site Search (I prefer the later).

All the best,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From h_m_ at po.harenet.ne.jp  Fri Aug 18 10:03:52 2006
From: h_m_ at po.harenet.ne.jp (Hiroto Miyoshi)
Date: Fri, 18 Aug 2006 17:03:52 +0900
Subject: [R] Odd behaviour of R
Message-ID: <005f01c6c29c$dacfb550$0f01a8c0@HP31522725682>

Dear R-users

I need your help. I am agonised by the odd behaviour of R
described below

In the following program, data.frame "work" contains no rows,
when Pb set to 0.3 and 0.7

data<-read.csv("output.csv",header=TRUE)
for(N in c(20,40,80,160,320,640,1280)){
   for(Pb in seq(0.1,0.9,0.1)){
     work<-data[((data$N==N) & (data$Pb==Pb)),]
     print(work)    
   }
 }

That is, when Pb set to 0.3 and 0.7,
print(work) prints out the message "[1] N        Pb       a        
b        c        d    outcomeA outcomeB <0 rows> (or 0-length row.names)"

However, data does exist when Pb is 0.3 or 0.7. And
when line "work<-data[((data$N==N) & (data$Pb==Pb)),]"
is placed outside the "for" loops after Pb is manually set to 0.3,
print(work) successfully prints out the appropriate data.
Namely,

> N<-20
> Pb<-0.3
>      work<-data[((data$N==N) & (data$Pb==Pb)),]
> work
    N  Pb a b c  d outcomeA outcomeB
22 20 0.3 6 0 0 14    FALSE    FALSE
23 20 0.3 5 1 0 14    FALSE    FALSE
24 20 0.3 5 1 1 13    FALSE    FALSE
...................................................................

Why is this happen?
I am using R version 2.3.0.
Any help is greatly appreciated.

Thank you.


------------------------
Hiroto Miyoshi
h_m_ at po.harenet.ne.jp


From bgreen at dyson.brisnet.org.au  Fri Aug 18 10:13:15 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Fri, 18 Aug 2006 18:13:15 +1000
Subject: [R] using R to perform a word count - syntax refinement and
 incorrect number of dimensions error
In-Reply-To: <mailman.11.1155808803.966.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060818180555.00c07a00@pop3.brisnet.org.au>

Hello,

I am hoping someone can advise me regarding an error message I received and 
if needed, refine some syntax. I am wanting to calculate the word count for 
each row of a dataframe. Below, I have 3 variables ("V3.PD", "V3.HD", 
"V3.LP") which I want to obtain a word count for, by each row which 
contains these variables.

Any assistance is much appreciated,

Bob Green

 > wordcount1 <- read.csv("c:\\newstext.csv",header=T)
 > attach(wordcount1)
 > names(wordcount1)
[1] "X"     "i"     "V3.PD" "V3.PG" "V3.HD" "V3.BY" "V3.SN" "V3.LP" "V3.TD"
 > wc1 <-c("V3.PD", "V3.HD", "V3.LP")
 > dim(wordcount1)
[1] 178   9
 > nwords <- function(x){
+ res <- strsplit(as.character(x), "[ \t]+")
+ res <- lapply(res, length)
+ unlist(res)
+ }
 > sapply(wc1, nwords)
V3.PD V3.HD V3.LP
     1     1     1
 > sapply(paste(wc1[,1], wc1[,2]), nwords)
Error in wc1[, 1] : incorrect number of dimensions
 > dim (wc1)
NULL


From e.fegraus at conservation.org  Fri Aug 18 10:28:20 2006
From: e.fegraus at conservation.org (Eric Fegraus)
Date: Fri, 18 Aug 2006 04:28:20 -0400
Subject: [R] Maximum length of R GUI input line?
Message-ID: <488277104DB8354F9DEA1657289873470CE0B5@ci-xmail1.CI.conservation.org>

Hello,


I'm using R 2.3.1 on Windows.

I'm generating some very long SQL statements. I do this by using paste() which will contain many strings and variables.  I'm getting an error when the the total line length is longer than about 1013 characters.  For example, it works with the line containing 1013 characters and not when it is 1059.

I've looked into adjusting the options(width) and a handful of other settings.   I have a feeling there is some other setting i'm missing that i can adjust.

Any ideas?

Thanks!


From Markus.Schweitzer at hilti.com  Fri Aug 18 10:47:31 2006
From: Markus.Schweitzer at hilti.com (Schweitzer, Markus)
Date: Fri, 18 Aug 2006 10:47:31 +0200
Subject: [R] fitting truncated normal distribution
Message-ID: <7260411F9C32E74A86AD9567E64C3301D2F614@LI-HAWK.hag.hilti.com>


 Thank you Sundar,

Yes, always integers. By demand data I meant the amount of ordered
products in a certain period. Therefore, x is a vector of periods (i.e.
Weeks in a year)

In my example we could see an article, that has only been ordered in two
weeks within one year.
All the zeros show, that nobody has ordered the items in these periods.
(First half of the year/first 24 weeks)

Since the orders cannot be negative, some literature recommended to use
a truncated normal distribution (Poisson and negative binomial are also
recommended).

My "x" is just a sample out of the dataset. There might be other time
series with better attributes for a truncated normal distribution.

My problem is simply, that I only get an error message when I use
fitdistr.

>>>"Error in "[<-"(`*tmp*`, x >= lower & x <= upper, value = numeric(0))
nothing to replace.

I hope, there is a way fitdistr can also compute "difficult" data.

Best regards, markus



-----Original Message-----
From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com] 
Sent: Donnerstag, 17. August 2006 16:47
To: Schweitzer, Markus
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] fitting truncated normal distribution

Hi, Markus,

Are these always integers? Why do you think they should be normal or
Weibull? Seems more like a mixture with a point mass at 0 and something
else (e.g. Poisson, negative binomial, normal). Though it's hard to tell
with what you have provided. If that's the case you'll have to write
your own likelihood function or, if they are integers, use zip
(zero-inflated Poisson) or zinb (zero-inflated negative binomial). Do an
RSiteSearch to find many packages will do these fits.

RSiteSearch("zero-inflated")

Again, this is pure speculation based on your "x" below alone and no
other information (I'm not sure what "demand-data" means).

HTH,

--sundar

Schweitzer, Markus wrote:
> Sorry, that I forgot an example.
> 
> I have demand-data which is either 0 or a positive value.
> 
> When I have an article which is not ordered very often, it could look 
> like this:
> 
> x=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1280,0,0,0,0,640,0
> ,0
> ,0,0,0,0,0,0,0)
> 
> 
> 
>>library(MASS) ## for fitdistr
>>library(msm) ## for dtnorm
>>
>>dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>>   dtnorm(x, mean, sd, 0, Inf, log)
>>}
>>fitdistr(x,dtnorm0,start=list(mean=0,sd=1))
> 
> 
> Unfortunately I get the same error message.
> I found a function, that works for a weibull distribution and tried to

> apply it but it didn't work neither
> 
> # truncated weibull distribution
> 
> #dweibull.trunc <-
> #function(x, shape, scale=1, trunc.=Inf, log=FALSE){
> #    ln.dens <- (dweibull(x, shape, scale, log=TRUE)
> #        -pweibull(trunc., shape, scale = 1, lower.tail = TRUE, log.p
= 
> #TRUE))
> #    if(any(oops <- (x>trunc.)))
> #        ln.dens[oops] <- (-Inf)   
> #    if(log)ln.dens else exp(ln.dens)
> #}
> #
> #x <- rweibull(100, 1)
> #range(x)
> #x4 <- x[x<=4]
> #fitdistr(x4, dweibull.trunc, start=list(shape=1, scale=1), trunc=4)
> 
> ######################################################################
> ##
> ########
> # truncated normal distribution
> 
> dtnorm0 <- function(x, mean, sd, a=0, log = FALSE) {
>     ln.dens <- (dnorm(x, mean, sd)
>                 - pnorm(a, mean, sd, lower.tail=TRUE, log.p =TRUE))
>                 
>     if(any(oops <- (x<a)))
>       ln.dens[oops] <- (-Inf)
>     if(log)ln.dens else exp(ln.dens)
> }
> 
> fitdistr(x, dtnorm0, start = list(mean = 0, sd = 1))
> 
> Maybe, when I alter mean and sd, I get an answer, which is not really 
> satisfactory. I hope, there is a solution possible And thank you in 
> advance
> 
> markus
> 
> 
> 
> 
> 
> 
> 
> Sorry, didn't notice that you *did* mention dtnorm is part of msm. 
> Ignore that part of the advice...
> 
> --sundar
> 
> Sundar Dorai-Raj wrote:
> 
>>aon.912182281.tmp at aon.at wrote:
>>
>>
>>>Hello,
>>>I am a new user of R and found the function dtnorm() in the package
> 
> msm.
> 
>>>My problem now is, that it is not possible for me to get the mean and
> 
> sd out of a sample when I want a left-truncated normal distribution 
> starting at "0".
> 
>>>fitdistr(x,dtnorm, start=list(mean=0, sd=1))
>>>
>>>returns the error message
>>>"Fehler in "[<-"(`*tmp*`, x >= lower & x <= upper, value = 
>>>numeric(0))
> 
> :    nichts zu ersetzen"
> 
>>>I don't know, where to enter the lower/upper value. Is there a
> 
> possibility to program the dtnorm function by myself?
> 
>>>Thank you very much in advance for your help, markus
>>>
>>>-------------------------------------------
>>>Versendet durch aonWebmail (webmail.aon.at)
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list 
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>Hi, Markus,
>>
>>You should always supply the package name where dtnorm is located. My 
>>guess is most don't know (as I didn't) it is part of the msm package.
>>Also, you should supply a reproducible example so others may 
>>understand your particular problem. For example, when I ran your code 
>>on data generated from "rtnorm" (also part of msm) I got warnings 
>>related to the NaNs generated in pnorm and qnorm, but no error as you 
>>reported. Both of these suggestions are in the posting guide (see
> 
> signature above).
> 
>>So, to answer your problem, here's a quick example.
>>
>>library(MASS) ## for fitdistr
>>library(msm) ## for dtnorm
>>
>>dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>>   dtnorm(x, mean, sd, 0, Inf, log)
>>}
>>
>>set.seed(1) ## to others may reproduce my results exactly x <- 
>>rtnorm(100, lower = 0) fitdistr(x, dtnorm0, start = list(mean = 0, sd 
>>= 1))
>>
>>Note, the help page ?fitdistr suggests additional parameters may be 
>>passed to the density function (i.e. dtnorm) or optim. However, this 
>>won't work here because "lower" is an argument for both functions.
>>This is the reason for writing dtnorm0 which has neither a lower or an
> 
> 
>>upper argument.
>>
>>HTH,
>>
>>--sundar
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Roger.Bivand at nhh.no  Fri Aug 18 11:02:57 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 18 Aug 2006 11:02:57 +0200 (CEST)
Subject: [R] Odd behaviour of R
In-Reply-To: <005f01c6c29c$dacfb550$0f01a8c0@HP31522725682>
Message-ID: <Pine.LNX.4.44.0608181056170.9484-100000@reclus.nhh.no>

On Fri, 18 Aug 2006, Hiroto Miyoshi wrote:

> Dear R-users
> 
> I need your help. I am agonised by the odd behaviour of R
> described below
> 
> In the following program, data.frame "work" contains no rows,
> when Pb set to 0.3 and 0.7
> 
> data<-read.csv("output.csv",header=TRUE)
> for(N in c(20,40,80,160,320,640,1280)){
>    for(Pb in seq(0.1,0.9,0.1)){
>      work<-data[((data$N==N) & (data$Pb==Pb)),]
>      print(work)    
>    }
>  }
> 
> That is, when Pb set to 0.3 and 0.7,
> print(work) prints out the message "[1] N        Pb       a        
> b        c        d    outcomeA outcomeB <0 rows> (or 0-length row.names)"
> 
> However, data does exist when Pb is 0.3 or 0.7. 

This looks suspiciously like FAQ 7.31:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

and suggests that all.equal() may be a better test than "==" for an 
appropriate value of tolerance:

print(seq(0.1,0.9,0.1), 20)
seq(0.1,0.9,0.1)[7]==0.7
all.equal(seq(0.1,0.9,0.1)[7], 0.7)

> And
> when line "work<-data[((data$N==N) & (data$Pb==Pb)),]"
> is placed outside the "for" loops after Pb is manually set to 0.3,
> print(work) successfully prints out the appropriate data.
> Namely,
> 
> > N<-20
> > Pb<-0.3
> >      work<-data[((data$N==N) & (data$Pb==Pb)),]
> > work
>     N  Pb a b c  d outcomeA outcomeB
> 22 20 0.3 6 0 0 14    FALSE    FALSE
> 23 20 0.3 5 1 0 14    FALSE    FALSE
> 24 20 0.3 5 1 1 13    FALSE    FALSE
> ...................................................................
> 
> Why is this happen?
> I am using R version 2.3.0.
> Any help is greatly appreciated.
> 
> Thank you.
> 
> 
> ------------------------
> Hiroto Miyoshi
> h_m_ at po.harenet.ne.jp
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From phgrosjean at sciviews.org  Fri Aug 18 10:59:55 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 18 Aug 2006 10:59:55 +0200
Subject: [R] Maximum length of R GUI input line?
In-Reply-To: <488277104DB8354F9DEA1657289873470CE0B5@ci-xmail1.CI.conservation.org>
References: <488277104DB8354F9DEA1657289873470CE0B5@ci-xmail1.CI.conservation.org>
Message-ID: <44E5818B.1030402@sciviews.org>

You should put your SQL query in a variable, and use this variable in 
your call. Something like:

MyQuery <- "bla bla bla"
MyQuery <- paste(MyQuery, "more bla bla")
# ....
doMyRequest(MyQuery)

Best,

Philippe Grosjean



Eric Fegraus wrote:
> Hello,
> 
> 
> I'm using R 2.3.1 on Windows.
> 
> I'm generating some very long SQL statements. I do this by using paste() which will contain many strings and variables.  I'm getting an error when the the total line length is longer than about 1013 characters.  For example, it works with the line containing 1013 characters and not when it is 1059.
> 
> I've looked into adjusting the options(width) and a handful of other settings.   I have a feeling there is some other setting i'm missing that i can adjust.
> 
> Any ideas?
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From ripley at stats.ox.ac.uk  Fri Aug 18 11:15:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Aug 2006 10:15:36 +0100 (BST)
Subject: [R] Maximum length of R GUI input line?
In-Reply-To: <44E5818B.1030402@sciviews.org>
References: <488277104DB8354F9DEA1657289873470CE0B5@ci-xmail1.CI.conservation.org>
	<44E5818B.1030402@sciviews.org>
Message-ID: <Pine.LNX.4.64.0608181013130.23491@gannet.stats.ox.ac.uk>

On Fri, 18 Aug 2006, Philippe Grosjean wrote:

> You should put your SQL query in a variable, and use this variable in 
> your call. Something like:
> 
> MyQuery <- "bla bla bla"
> MyQuery <- paste(MyQuery, "more bla bla")
> # ....
> doMyRequest(MyQuery)

Indeed.  For the record, R has an limit of 1024 chars on input lines (from 
way back) and it cannot be altered.

> 
> Best,
> 
> Philippe Grosjean
> 
> 
> 
> Eric Fegraus wrote:
> > Hello,
> > 
> > 
> > I'm using R 2.3.1 on Windows.
> > 
> > I'm generating some very long SQL statements. I do this by using 
> > paste() which will contain many strings and variables.  I'm getting an 
> > error when the the total line length is longer than about 1013 
> > characters.  For example, it works with the line containing 1013 
> > characters and not when it is 1059.
> > 
> > I've looked into adjusting the options(width) and a handful of other 
> > settings.  I have a feeling there is some other setting i'm missing 
> > that i can adjust.
> > 
> > Any ideas?
> > 
> > Thanks!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From karin.lagesen at medisin.uio.no  Fri Aug 18 11:16:03 2006
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Fri, 18 Aug 2006 11:16:03 +0200
Subject: [R] rotating axis labels in plot with multiple plots
Message-ID: <ypx6oduiieuk.fsf@uracil.uio.no>


I have a plot like this, consisting of 18 different vioplots:

(monospaced font)

    ----- ----- ----- ----- ----- -----
lab |16 | |13 | |10 | | 7 | | 4 | | 1 |
    ----- ----- ----- ----- ----- -----
lab |17 | |14 | |11 | | 8 | | 5 | | 2 |
    ----- ----- ----- ----- ----- -----
lab |18 | |15 | |12 | | 9 | | 6 | | 3 |
    ----- ----- ----- ----- ----- -----
      a     b     c     d     e     f

This I create in such a way that in the graph itself plot 1 appears in
the top left corner, and plot 18 in the bottom right corner. I then
flip it around when I show it in my paper (it was simpler to do it
that way, this might change:)).

I am now trying to make labels a to f, which are a set of numbers
which are _local_ to each graph group. Now I would like these numbers
to be either vertical to the axis (so that they are in normal text
direction when the paper is rotated so that graphs 18-16 are on top),
or possibly 45 degrees to the plot group.

This is the way I create labels and such:

        plot.window(xlim = xlim, ylim = ylim)
        axis(2, at = at, labels=toptitle, tick=FALSE)
        if(!bottom) {
          if(!left){ # nothing > ok
            axis(1, at = at, labels = label, tick=FALSE)
          }
          else{ # not bottom, but left
            axis(1, at = at, labels = label, las=2)
          }
        }
        else{
          text(lognumbers, par("usr")[3] - 0.25, srt = 135, adj = 1,labels = loglabels, xpd = TRUE)
          if(!left){ # bottom, but not left
            # want to replace this one with the text
            # axis(4, at = lognumbers, labels=loglabels, las=2)
          }
          else{ # bottom and left > not ok
            axis(1, at = at, labels = label, las=2)
            # want to replace this one with the text
            # axis(4, at = lognumbers, labels=loglabels, las=2)
          }
        }
      }

In this sceme graphs 18-16 are defined as being left, whereas graphs
3, 6, 9, 12, 15, 18 are defined as bottom.

My question relates to my attemt to set nice-looking axis labels using
text. When I put them on as they are shown here, they show up on
(relative to image above) the left side of the "bottom" graphs. How do
I get them to show up on the side I would like them to?

Hope this was somewhat understandable...:)

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://www.cmbn.no/rognes/


From spencer.graves at pdf.com  Fri Aug 18 11:42:10 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Aug 2006 02:42:10 -0700
Subject: [R] ARMA(1,1) for panel data
In-Reply-To: <cc088e260608141211g49e647f8hde7bfd1660575b10@mail.gmail.com>
References: <cc088e260608141211g49e647f8hde7bfd1660575b10@mail.gmail.com>
Message-ID: <44E58B72.4080209@pdf.com>

      For "normal" panel data, the standard R tool is the nlme package, 
documented in Pinheiro and Bates (2000) Mixed-Effects Models for S and 
S-Plus (Springer).  See the examples in ?corARMA. 

      I recommend you spend some quality time with Pinheiro and Bates 
(2000).  If you do that, I suggest you start by looking at "ch01.R", 
"ch02.R", ..., "ch06.R", "ch08.R" in "~library\nlme\scripts" in your R 
installation directory.  These files contain the R commands used in Ch. 
1, Ch. 2, etc., of that book.  Using them makes studying that book 
easier, more pleasant and productive for several reasons.  First, they 
save you the work of typing in the commands yourself.  Second, they save 
you the agony of wondering why you didn't get their answer when you have 
a typographical error.  Third, there are a very few subtle syntax 
changes between the book and R that generate different answers for the 
unwary.  Fourth, you can experiment with different alternatives to test 
your understanding, etc. 

      If your data required non-normal models, I would recommend 'lmer' 
associated with the 'lme4' package.  However, that's newer and does not 
have the same level of documentation, helper functions, etc. 

      Hope this helps. 
      Spencer Graves

Tom Boonen wrote:
> Dear List,
>
> I am new to TS-Modeling in R. I would like to fit an ARMA(1,1) model
> for a balanced panel, running Y on a full set of unit and year dummies
> using an arma(1,1) for the disturbance:
>
> y_it=unit.dummies+yeardummies+e_it
>
> where: e_it=d*e_it-1+u_it+q*u_it-1
>
> How can I fit this model in R? arma() does not seem to take covariates
> (or I don't understand how to specify the function so that it would).
> Thank you very much.
>
> Best, Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Fri Aug 18 12:06:44 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Aug 2006 03:06:44 -0700
Subject: [R] NLS and IV
In-Reply-To: <1155116124.44d9ac5ca4f33@webmail.acadiau.ca>
References: <1155116124.44d9ac5ca4f33@webmail.acadiau.ca>
Message-ID: <44E59134.3060908@pdf.com>

      RSiteSearch("grogger") produced nothing, which suggests that the 
paper you cite is NOT cited in a help page in any package contributed to 
CRAN.  However, RSiteSearch("instrumental variables") just produced 31 
hits for me, among which #11 was for "systemfit{systemfit}", which 
mentions "instrumental variables" and "probits".  I also got 6 hits for 
RSiteSearch("exogeneity") and 6 for RSiteSearch("endogeneity").  You 
might also check the 'sem' package. 

      If you don't find anything that does exactly what you want, I 
doubt if it would be too difficult to program it from scratch using 
something like "optim" if you couldn't find anything better using 
RSiteSearch or similar tools. 

      If you would like further help from this listserve, please submit 
another post.  In doing so, I suggest you include commented, minimal, 
self-contained, reproducible code illustrating something you've tried, 
as suggested in the final two lines of this email.  Substantial 
anecdotal evidence suggests that posts that conform more closely to the 
style describe in the posting guide 
"www.R-project.org/posting-guide.html" (which recommends including 
commented, minimal, self-contained, reproducible code)  tend to get 
better answers to their questions quicker than other posts. 

      Hope this helps. 
      Spencer Graves

John Janmaat wrote:
> Hello All,
>
> I'm looking to test a variable in a logit model (glm(...,
> binomial(link="logit"))) for exogeneity (endogeneity).  At this point I am
> planning to try implementing Jeffery Grogger's "A Simple Test for Exogeneity in
> Probit, Logit, and Poisson Regression Models", Economic Letters, 1990.  To do
> this, I need to be able to do an instrumental variables NLS regression.  Is
> there a simple way to do this?
>
> Alternatively, is there any other test for exogeneity (endogeneity) of a
> regressor that has been implemented in R, for binary models?
>
> Thanks,
>
> John.
>
> ============================================================================
> Dr. John Janmaat                       Tel: 902-585-1461
> Department of Economics                Fax: 902-585-1461
> Acadia University,                     Email: jjanmaat at acadiau.ca
> Wolfville, Nova Scotia, Canada.        Web: ace.acadiau.ca/~jjanmaat/
> B4P 1H5
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lcorreia at sun.ac.za  Fri Aug 18 12:17:47 2006
From: lcorreia at sun.ac.za (Correia, L, Mr <lcorreia@sun.ac.za>)
Date: Fri, 18 Aug 2006 12:17:47 +0200
Subject: [R] 4^2 factorial help
Message-ID: <161F3C874F1FC44A84426FEB91B3AF1A6E1BE7@STBEVS03.stb.sun.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/d255e819/attachment.pl 

From jholtman at gmail.com  Fri Aug 18 13:08:16 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 18 Aug 2006 07:08:16 -0400
Subject: [R] using R to perform a word count - syntax refinement and
	incorrect number of dimensions error
In-Reply-To: <5.1.0.14.0.20060818180555.00c07a00@pop3.brisnet.org.au>
References: <mailman.11.1155808803.966.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060818180555.00c07a00@pop3.brisnet.org.au>
Message-ID: <644e1f320608180408l4feb1625wa3259a4dbba5014a@mail.gmail.com>

'wc1' is a vector and not a matrix.  The reference 'wc1[,1]' is not legal:

> wc1 <-c("V3.PD", "V3.HD", "V3.LP")
> dim(wc1)
NULL
> wc1[,1]
Error in wc1[, 1] : incorrect number of dimensions
>

What is it that you are trying to do?

On 8/18/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> Hello,
>
> I am hoping someone can advise me regarding an error message I received and
> if needed, refine some syntax. I am wanting to calculate the word count for
> each row of a dataframe. Below, I have 3 variables ("V3.PD", "V3.HD",
> "V3.LP") which I want to obtain a word count for, by each row which
> contains these variables.
>
> Any assistance is much appreciated,
>
> Bob Green
>
>  > wordcount1 <- read.csv("c:\\newstext.csv",header=T)
>  > attach(wordcount1)
>  > names(wordcount1)
> [1] "X"     "i"     "V3.PD" "V3.PG" "V3.HD" "V3.BY" "V3.SN" "V3.LP" "V3.TD"
>  > wc1 <-c("V3.PD", "V3.HD", "V3.LP")
>  > dim(wordcount1)
> [1] 178   9
>  > nwords <- function(x){
> + res <- strsplit(as.character(x), "[ \t]+")
> + res <- lapply(res, length)
> + unlist(res)
> + }
>  > sapply(wc1, nwords)
> V3.PD V3.HD V3.LP
>     1     1     1
>  > sapply(paste(wc1[,1], wc1[,2]), nwords)
> Error in wc1[, 1] : incorrect number of dimensions
>  > dim (wc1)
> NULL
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From phhs80 at gmail.com  Fri Aug 18 13:23:29 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Fri, 18 Aug 2006 12:23:29 +0100
Subject: [R] 4^2 factorial help
In-Reply-To: <161F3C874F1FC44A84426FEB91B3AF1A6E1BE7@STBEVS03.stb.sun.ac.za>
References: <161F3C874F1FC44A84426FEB91B3AF1A6E1BE7@STBEVS03.stb.sun.ac.za>
Message-ID: <6ade6f6c0608180423k10ce0984ub244eceaae99abaf@mail.gmail.com>

On 8/18/06, Correia, L, Mr <lcorreia at sun.ac.za> <lcorreia at sun.ac.za> wrote:
> To whom it may concern:
>
> I am trying a factorial design a system of mine that has two factors.
> Each factor was set at four different levels, with one replication for
> each of the combinations. My data is as follows:
>
>
>            A           B                     Response
>
> 1        600        2.5                   0.0257
>
> 2        600        2.5                   0.0254
>
> 3        600        5                      0.0217
>
> 4        600        5                      0.0204
>
> 5        600        10                    0.0191
>
> 6        600        10                    0.0210
>
> 7        600        20                    0.0133
>
> 8        600        20                    0.0139
>
> 9        800        2.5                   0.0312
>
> 10       800       2.5                   0.0317
>
> 11       800       5                      0.0307
>
> 12       800      5                      0.0309
>
> 13       800       10                    0.0330
>
> 14       800       10                    0.0318
>
> 15       800       20                    0.0225
>
> 16       800       20                    0.0234
>
> 17      1000      2.5                   0.0350
>
> 18      1000      2.5                   0.0352
>
> 19      1000      5                      0.0373
>
> 20      1000      5                      0.0361
>
> 21      1000     10                    0.0432
>
> 22      1000     10                    0.0402
>
> 23      1000     20                    0.0297
>
> 24      1000     20                    0.0306
>
> 25      1200      2.5                   0.0324
>
> 26      1200      2.5                   0.0326
>
> 27      1200      5                      0.0353
>
> 28      1200      5                      0.0353
>
> 29      1200     10                    0.0453
>
> 30      1200     10                    0.0436
>
> 31      1200     20                    0.0348
>
> 32      1200     20                    0.0357
>
>
>
> I am able to enter my data into R and obtain an ANOVA table (which I
> have been able to verify as correct using an excel spreadsheet), using
> the following syntax:
>
>
>
> >Factorial<-data.frame(A=c(rep(c("600", "600", "600", "600", "800",
> "800", "800", "800", "1000", "1000", "1000", "1000", "1200", "1200",
> "1200", "1200"), each=2)), B=c(rep(c("2.5", "5", "10", "20", "2.5", "5",
> "10", "20", "2.5", "5", "10", "20", "2.5", "5", "10", "20"), each=2)),
> Response = c(0.0257, 0.0254, 0.0217, 0.0204, 0.0191, 0.021, 0.0133,
> 0.0139, 0.0312, 0.0317, 0.0307, 0.0309, 0.033, 0.0318, 0.0225, 0.0234,
> 0.035, 0.0352, 0.0373, 0.0361, 0.0432, 0.0402, 0.0297, 0.0306, 0.0324,
> 0.0326, 0.0353, 0.0353, 0.0453, 0.0436, 0.0348, 0.0357))
>
>
>
> > anova(aov(Response~A*B, data=Factorial))
>
>
>
> However, this is as far as I am able to go. I would like to obtain the
> coefficients of my model, but am unable. I would also like to use other
> non-linear models as these factors are not linear. Also would like to
> add A^2 and B^2 into the ANOVA and modeling.

Try:

model <- lm(Response~A*B, data=Factorial)
anova(model)

Paul


From sundar.dorai-raj at pdf.com  Fri Aug 18 14:13:11 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 18 Aug 2006 07:13:11 -0500
Subject: [R] fitting truncated normal distribution
In-Reply-To: <7260411F9C32E74A86AD9567E64C3301D2F614@LI-HAWK.hag.hilti.com>
References: <7260411F9C32E74A86AD9567E64C3301D2F614@LI-HAWK.hag.hilti.com>
Message-ID: <44E5AED7.7010905@pdf.com>

Hi, Markus,

One other suggestion is to add the "lower" argument to fitdistr:

fitdistr(x, dtnorm0, start = list(mean = 0, sd = 1), lower = 0)

where dtnorm0 is defined as before. This indicates to fitdistr that the 
optimization should be constrained. See ?optim for details.

--sundar

Schweitzer, Markus wrote:
>  Thank you Sundar,
> 
> Yes, always integers. By demand data I meant the amount of ordered
> products in a certain period. Therefore, x is a vector of periods (i.e.
> Weeks in a year)
> 
> In my example we could see an article, that has only been ordered in two
> weeks within one year.
> All the zeros show, that nobody has ordered the items in these periods.
> (First half of the year/first 24 weeks)
> 
> Since the orders cannot be negative, some literature recommended to use
> a truncated normal distribution (Poisson and negative binomial are also
> recommended).
> 
> My "x" is just a sample out of the dataset. There might be other time
> series with better attributes for a truncated normal distribution.
> 
> My problem is simply, that I only get an error message when I use
> fitdistr.
> 
> 
>>>>"Error in "[<-"(`*tmp*`, x >= lower & x <= upper, value = numeric(0))
> 
> nothing to replace.
> 
> I hope, there is a way fitdistr can also compute "difficult" data.
> 
> Best regards, markus
> 
> 
> 
> -----Original Message-----
> From: Sundar Dorai-Raj [mailto:sundar.dorai-raj at pdf.com] 
> Sent: Donnerstag, 17. August 2006 16:47
> To: Schweitzer, Markus
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] fitting truncated normal distribution
> 
> Hi, Markus,
> 
> Are these always integers? Why do you think they should be normal or
> Weibull? Seems more like a mixture with a point mass at 0 and something
> else (e.g. Poisson, negative binomial, normal). Though it's hard to tell
> with what you have provided. If that's the case you'll have to write
> your own likelihood function or, if they are integers, use zip
> (zero-inflated Poisson) or zinb (zero-inflated negative binomial). Do an
> RSiteSearch to find many packages will do these fits.
> 
> RSiteSearch("zero-inflated")
> 
> Again, this is pure speculation based on your "x" below alone and no
> other information (I'm not sure what "demand-data" means).
> 
> HTH,
> 
> --sundar
> 
> Schweitzer, Markus wrote:
> 
>>Sorry, that I forgot an example.
>>
>>I have demand-data which is either 0 or a positive value.
>>
>>When I have an article which is not ordered very often, it could look 
>>like this:
>>
>>x=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1280,0,0,0,0,640,0
>>,0
>>,0,0,0,0,0,0,0)
>>
>>
>>
>>
>>>library(MASS) ## for fitdistr
>>>library(msm) ## for dtnorm
>>>
>>>dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>>>  dtnorm(x, mean, sd, 0, Inf, log)
>>>}
>>>fitdistr(x,dtnorm0,start=list(mean=0,sd=1))
>>
>>
>>Unfortunately I get the same error message.
>>I found a function, that works for a weibull distribution and tried to
> 
> 
>>apply it but it didn't work neither
>>
>># truncated weibull distribution
>>
>>#dweibull.trunc <-
>>#function(x, shape, scale=1, trunc.=Inf, log=FALSE){
>>#    ln.dens <- (dweibull(x, shape, scale, log=TRUE)
>>#        -pweibull(trunc., shape, scale = 1, lower.tail = TRUE, log.p
> 
> = 
> 
>>#TRUE))
>>#    if(any(oops <- (x>trunc.)))
>>#        ln.dens[oops] <- (-Inf)   
>>#    if(log)ln.dens else exp(ln.dens)
>>#}
>>#
>>#x <- rweibull(100, 1)
>>#range(x)
>>#x4 <- x[x<=4]
>>#fitdistr(x4, dweibull.trunc, start=list(shape=1, scale=1), trunc=4)
>>
>>######################################################################
>>##
>>########
>># truncated normal distribution
>>
>>dtnorm0 <- function(x, mean, sd, a=0, log = FALSE) {
>>    ln.dens <- (dnorm(x, mean, sd)
>>                - pnorm(a, mean, sd, lower.tail=TRUE, log.p =TRUE))
>>                
>>    if(any(oops <- (x<a)))
>>      ln.dens[oops] <- (-Inf)
>>    if(log)ln.dens else exp(ln.dens)
>>}
>>
>>fitdistr(x, dtnorm0, start = list(mean = 0, sd = 1))
>>
>>Maybe, when I alter mean and sd, I get an answer, which is not really 
>>satisfactory. I hope, there is a solution possible And thank you in 
>>advance
>>
>>markus
>>
>>
>>
>>
>>
>>
>>
>>Sorry, didn't notice that you *did* mention dtnorm is part of msm. 
>>Ignore that part of the advice...
>>
>>--sundar
>>
>>Sundar Dorai-Raj wrote:
>>
>>
>>>aon.912182281.tmp at aon.at wrote:
>>>
>>>
>>>
>>>>Hello,
>>>>I am a new user of R and found the function dtnorm() in the package
>>
>>msm.
>>
>>
>>>>My problem now is, that it is not possible for me to get the mean and
>>
>>sd out of a sample when I want a left-truncated normal distribution 
>>starting at "0".
>>
>>
>>>>fitdistr(x,dtnorm, start=list(mean=0, sd=1))
>>>>
>>>>returns the error message
>>>>"Fehler in "[<-"(`*tmp*`, x >= lower & x <= upper, value = 
>>>>numeric(0))
>>
>>:    nichts zu ersetzen"
>>
>>
>>>>I don't know, where to enter the lower/upper value. Is there a
>>
>>possibility to program the dtnorm function by myself?
>>
>>
>>>>Thank you very much in advance for your help, markus
>>>>
>>>>-------------------------------------------
>>>>Versendet durch aonWebmail (webmail.aon.at)
>>>>
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list 
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide
>>>>http://www.R-project.org/posting-guide.html
>>>>and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>Hi, Markus,
>>>
>>>You should always supply the package name where dtnorm is located. My 
>>>guess is most don't know (as I didn't) it is part of the msm package.
>>>Also, you should supply a reproducible example so others may 
>>>understand your particular problem. For example, when I ran your code 
>>>on data generated from "rtnorm" (also part of msm) I got warnings 
>>>related to the NaNs generated in pnorm and qnorm, but no error as you 
>>>reported. Both of these suggestions are in the posting guide (see
>>
>>signature above).
>>
>>
>>>So, to answer your problem, here's a quick example.
>>>
>>>library(MASS) ## for fitdistr
>>>library(msm) ## for dtnorm
>>>
>>>dtnorm0 <- function(x, mean = 0, sd = 1, log = FALSE) {
>>>  dtnorm(x, mean, sd, 0, Inf, log)
>>>}
>>>
>>>set.seed(1) ## to others may reproduce my results exactly x <- 
>>>rtnorm(100, lower = 0) fitdistr(x, dtnorm0, start = list(mean = 0, sd 
>>>= 1))
>>>
>>>Note, the help page ?fitdistr suggests additional parameters may be 
>>>passed to the density function (i.e. dtnorm) or optim. However, this 
>>>won't work here because "lower" is an argument for both functions.
>>>This is the reason for writing dtnorm0 which has neither a lower or an
>>
>>
>>>upper argument.
>>>
>>>HTH,
>>>
>>>--sundar
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sachinj.2006 at yahoo.com  Fri Aug 18 14:40:16 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 18 Aug 2006 05:40:16 -0700 (PDT)
Subject: [R] Insert rows - how can I accomplish this in R
Message-ID: <20060818124016.88918.qmail@web37607.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/6df95774/attachment.pl 

From andy_liaw at merck.com  Fri Aug 18 14:45:11 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 18 Aug 2006 08:45:11 -0400
Subject: [R] 4^2 factorial help
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB3514@usctmx1106.merck.com>

If you really want the quadratic terms, you need to keep those variables as
numeric, instead of factors.  (You might also want to look into something
like the central composite designs.)

summary() and coef() on the resulting fitted object should give you want you
need.  Things like these are covered in the "An Introduction to R" manual...

Andy 

From: lcorreia at sun.ac.za
> 
> To whom it may concern:
>  
> I am trying a factorial design a system of mine that has two factors.
> Each factor was set at four different levels, with one 
> replication for each of the combinations. My data is as follows:
>  
> 
>            A           B                     Response
> 
> 1        600        2.5                   0.0257
> 
> 2        600        2.5                   0.0254
> 
> 3        600        5                      0.0217
> 
> 4        600        5                      0.0204
> 
> 5        600        10                    0.0191
> 
> 6        600        10                    0.0210
> 
> 7        600        20                    0.0133
> 
> 8        600        20                    0.0139
> 
> 9        800        2.5                   0.0312
> 
> 10       800       2.5                   0.0317
> 
> 11       800       5                      0.0307
> 
> 12       800      5                      0.0309
> 
> 13       800       10                    0.0330
> 
> 14       800       10                    0.0318
> 
> 15       800       20                    0.0225
> 
> 16       800       20                    0.0234
> 
> 17      1000      2.5                   0.0350
> 
> 18      1000      2.5                   0.0352
> 
> 19      1000      5                      0.0373
> 
> 20      1000      5                      0.0361
> 
> 21      1000     10                    0.0432
> 
> 22      1000     10                    0.0402
> 
> 23      1000     20                    0.0297
> 
> 24      1000     20                    0.0306
> 
> 25      1200      2.5                   0.0324
> 
> 26      1200      2.5                   0.0326
> 
> 27      1200      5                      0.0353
> 
> 28      1200      5                      0.0353
> 
> 29      1200     10                    0.0453
> 
> 30      1200     10                    0.0436
> 
> 31      1200     20                    0.0348
> 
> 32      1200     20                    0.0357
> 
>  
> 
> I am able to enter my data into R and obtain an ANOVA table 
> (which I have been able to verify as correct using an excel 
> spreadsheet), using the following syntax:
> 
>  
> 
> >Factorial<-data.frame(A=c(rep(c("600", "600", "600", "600", "800",
> "800", "800", "800", "1000", "1000", "1000", "1000", "1200", 
> "1200", "1200", "1200"), each=2)), B=c(rep(c("2.5", "5", 
> "10", "20", "2.5", "5", "10", "20", "2.5", "5", "10", "20", 
> "2.5", "5", "10", "20"), each=2)), Response = c(0.0257, 
> 0.0254, 0.0217, 0.0204, 0.0191, 0.021, 0.0133, 0.0139, 
> 0.0312, 0.0317, 0.0307, 0.0309, 0.033, 0.0318, 0.0225, 
> 0.0234, 0.035, 0.0352, 0.0373, 0.0361, 0.0432, 0.0402, 
> 0.0297, 0.0306, 0.0324, 0.0326, 0.0353, 0.0353, 0.0453, 
> 0.0436, 0.0348, 0.0357))
> 
>  
> 
> > anova(aov(Response~A*B, data=Factorial))
> 
>  
> 
> However, this is as far as I am able to go. I would like to 
> obtain the coefficients of my model, but am unable. I would 
> also like to use other non-linear models as these factors are 
> not linear. Also would like to add A^2 and B^2 into the ANOVA 
> and modeling. 
> 
>  
> 
> Please can you help with regard and offer some advice. Your 
> help is much appreciated.
> 
>  
> 
> Yours sincerely,
> 
> Leslie Correia
> 
> ------------------------------------------------
> 
> Department of Process Engineering
> 
> University of Stellenbosch
> 
> Private Bag X1
> 
> Matieland, 7602
> 
> Stellenbosch
> 
> Tel:   0837012017
> 
> E-mail: lcorreia at sun.ac.za <mailto:lcorreia at sun.ac.za> 
> 
> ------------------------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From haynesm at cfr.nichd.nih.gov  Fri Aug 18 14:51:46 2006
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD) [E])
Date: Fri, 18 Aug 2006 08:51:46 -0400
Subject: [R] Lattice package par.settings/trellis.par.settings questions
In-Reply-To: <971536df0608172135v7abb6f88wbfeb5976e8d4ee9e@mail.gmail.com>
Message-ID: <DC6119F74942094BBBB71024B83B995249D52A@NIHCESMLBX7.nih.gov>

I am also trying to learn about lattice plots.

To get a succinct listing of the names of the lists of default parameter
settings, try:
  names(trellis.par.get())

To get a succinct listing of all the default parameter settings, try:
  str(trellis.par.get())

HTH,

Maurice Haynes


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Friday, August 18, 2006 12:36 AM
To: Debarchana Ghosh
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Lattice package par.settings/trellis.par.settings
questions


The parameter names are axis.text$font and axis.text$cex .
Try issuing the command:
  trellis.par.get()
to get a complete list.

Here is an example:

histogram(1:10, par.settings = list(axis.text = list(font = 2, cex =
0.5)))


On 8/17/06, Debarchana Ghosh <ghos0033 at umn.edu> wrote:
> Hi All,
>
> I'm trying to modify some of the default graphic parameters in a 
> conditional histogram. While I was able to change the default grey 
> background to white, I couldn't change the axis.font or the xlab font.
>
> I used the following code:
>
> /histogram(~V751|V013+V025, finalbase, xlab="Heard of HIV/AIDS 
> (No/Yes)", col=c("cyan","magenta"), 
> par.settings=list(background="white"))
>
> /The arguments for example  like /axis.font=2/, or /cex=2/ are not 
> working in the /par.settings(). /I also tried to read the manual of 
> /trellis.par.settings()/ but didn't understand how to use it and where

> exactly to put it.
>
> Any help with this will be appreciated.
>
> Thanks,
> Debarchana.
>
> --
> Debarchana Ghosh
> Research Assistant
> Department of Geography
> University of Minnesota
> PH: 8143607580
> email to: ghos0033 at umn.edu
> www.tc.umn.edu/~ghos0033
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From iuri at ufrgs.br  Fri Aug 18 15:01:41 2006
From: iuri at ufrgs.br (Iuri Gavronski)
Date: Fri, 18 Aug 2006 10:01:41 -0300
Subject: [R] Variance Components in R
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
Message-ID: <60ad85c90608180601i22ef0e71y8664d961e789a835@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/727b181b/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug 18 15:38:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Aug 2006 09:38:54 -0400
Subject: [R] Insert rows - how can I accomplish this in R
In-Reply-To: <20060818124016.88918.qmail@web37607.mail.mud.yahoo.com>
References: <20060818124016.88918.qmail@web37607.mail.mud.yahoo.com>
Message-ID: <971536df0608180638s17c3cf6fm55e70ef10231f5b@mail.gmail.com>

Here are two solutions.  In both we break up DF into rows
which start with 1.

In solution #1 we create a new data frame with the required sequence
for A and zeros for B and then we fill it in.

In solution #2 we convert each set of rows to a zoo object z
where column A is the times and B is the data.  We convert
that zoo object to a ts object (which has the effect of
filling in the missing times) and then create a zoo object
with no data from its times merging that zoo object with z
using a fill of 0.

Finally in both solutions we reconstruct the rows from that by
rbind'ing everything together.


# 1
f <- function(x) {
   DF <- data.frame(A = 1:max(x$A), B = 0)
   DF[x$A,"B"] <- x$B
   DF
}
do.call(rbind, by(DF, cumsum(DF$A == 1), f))

# 2
library(zoo)
f <- function(x) {
   z <- zoo(x$B, x$A)
   ser <- merge(zoo(,time(as.ts(z)), z, fill = 0)
   data.frame(A = time(ser), B = coredata(ser))
}
do.call(rbind, by(DF, cumsum(DF$A == 1), f)




On 8/18/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
> Hi,
>
>  I have following dataframe. Column A indicates months.
>
>  DF <- structure(list(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1,
> 2, 3, 4, 5, 7, 8, 11, 12, 1, 2, 3, 4, 5, 8), B = c(0, 0, 0, 8,
> 0, 19, 5, 19, 0, 0, 0, 11, 0, 8, 5, 11, 19, 8, 11, 10, 0, 8,
> 36, 10, 16, 10, 22)), .Names = c("A", "B"), class = "data.frame", row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27"))
>
>  There is some discontinuity in the data. For example month 6, 9,10 data (2nd year) and month 6 data (3rd year) are absent. I want to insert the rows in place of these missing months and set the corresponding B column to zero. i.e., the result should look like:
>
>  DFNEW <- structure(list(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1,
> 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8),
>    B = c(0, 0, 0, 8, 0, 19, 5, 19, 0, 0, 0, 11, 0, 8, 5, 11,
>    19, 0, 8, 11, 0, 0, 10, 0, 8, 36, 10, 16, 10, 0, 0, 22)), .Names = c("A",
> "B"), class = "data.frame", row.names = c("1", "2", "3", "4",
> "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15",
> "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
> "27", "28", "29", "30", "31", "32"))
>
>   Thanks in advance.
>
>  Sachin
>
>
> ---------------------------------
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sachinj.2006 at yahoo.com  Fri Aug 18 16:21:02 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 18 Aug 2006 07:21:02 -0700 (PDT)
Subject: [R] Insert rows - how can I accomplish this in R
In-Reply-To: <971536df0608180638s17c3cf6fm55e70ef10231f5b@mail.gmail.com>
Message-ID: <20060818142102.4653.qmail@web37615.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/a4f4f7f5/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug 18 16:26:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Aug 2006 10:26:15 -0400
Subject: [R] Insert rows - how can I accomplish this in R
In-Reply-To: <20060818142102.4653.qmail@web37615.mail.mud.yahoo.com>
References: <971536df0608180638s17c3cf6fm55e70ef10231f5b@mail.gmail.com>
	<20060818142102.4653.qmail@web37615.mail.mud.yahoo.com>
Message-ID: <971536df0608180726n44e8e0aei908fc319fa05d62f@mail.gmail.com>

I did run it so I am not sure how the error crept in.  Anyways,
I have fixed it and a corrected version is below.

---

Here are two solutions.  In both we break up DF into rows
which start with 1.

In #1 we create a new data frame with the required sequence
for A and zeros for B and then we fill it in.

In #2 we convert each set of rows to a zoo object z
where column A is the times and B is the data.  We convert
that zoo object to a ts object (which has the effect of
filling in the missing times) and then create a zoo object
with no data from that merging it with z using a fill of 0.

Finally in both solutions we reconstruct the rows from that by
rbind'ing everything together.


# 1
f <- function(x) {
   DF <- data.frame(A = 1:max(x$A), B = 0)
   DF[x$A,"B"] <- x$B
   DF
}
do.call(rbind, by(DF, cumsum(DF$A == 1), f))

# 2
library(zoo)
f <- function(x) {
   z <- zoo(x$B, x$A)
   ser <- merge(zoo(,time(as.ts(z))), z, fill = 0)
   data.frame(A = time(ser), B = coredata(ser))
}
do.call(rbind, by(DF, cumsum(DF$A == 1), f))


On 8/18/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
>
> Gabor,
>
> Thanks a lot for the help. The 1st method works fine. In 2nd method I am
> getting following error.
>
> > do.call(rbind, by(DF, cumsum(DF$A == 1), f))
> Error in zoo(, time(as.ts(z)), z, fill = 0) :
>         unused argument(s) (fill ...)
> Unable to figure out the cause.
>
> Thanks,
> Sachin
>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> Here are two solutions. In both we break up DF into rows
> which start with 1.
>
> In solution #1 we create a new data frame with the required sequence
> for A and zeros for B and then we fill it in.
>
> In solution #2 we convert each set of rows to a zoo object z
> where column A is the times and B is the data. We convert
> that zoo object to a ts object (which has the effect of
> filling in the missing times) and then create a zoo object
> with no data from its times merging that zoo object with z
> using a fill of 0.
>
> Finally in both solutions we reconstruct the rows from that by
> rbind'ing everything together.
>
>
> # 1
> f <- function(x) {
> DF <- data.frame(A = 1:max(x$A), B = 0)
> DF[x$A,"B"] <- x$B
> DF
> }
> do.call(rbind, by(DF, cumsum(DF$A == 1), f))
>
> # 2
> library(zoo)
> f <- function(x) {
> z <- zoo(x$B, x$A)
> ser <- merge(zoo(,time(as.ts(z)), z, fill = 0)
> data.frame(A = time(ser), B = coredata(ser))
> }
> do.call(rbind, by(DF, cumsum(DF$A == 1), f)
>
>
>
>
> On 8/18/06, Sachin J wrote:
> > Hi,
> >
> > I have following dataframe. Column A indicates months.
> >
> > DF <- structure(list(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1,
> > 2, 3, 4, 5, 7, 8, 11, 12, 1, 2, 3, 4, 5, 8), B = c(0, 0, 0, 8,
> > 0, 19, 5, 19, 0, 0, 0, 11, 0, 8, 5, 11, 19, 8, 11, 10, 0, 8,
> > 36, 10, 16, 10, 22)), .Names = c("A", "B"), class = "data.frame",
> row.names = c("1",
> > "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> > "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> > "25", "26", "27"))
> >
> > There is some discontinuity in the data. For example month 6, 9,10 data
> (2nd year) and month 6 data (3rd year) are absent. I want to insert the rows
> in place of these missing months and set the corresponding B column to zero.
> i.e., the result should look like:
> >
> > DFNEW <- structure(list(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1,
> > 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8),
> > B = c(0, 0, 0, 8, 0, 19, 5, 19, 0, 0, 0, 11, 0, 8, 5, 11,
> > 19, 0, 8, 11, 0, 0, 10, 0, 8, 36, 10, 16, 10, 0, 0, 22)), .Names = c("A",
> > "B"), class = "data.frame", row.names = c("1", "2", "3", "4",
> > "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15",
> > "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
> > "27", "28", "29", "30", "31", "32"))
> >
> > Thanks in advance.
> >
> > Sachin
> >
> >
> > ---------------------------------
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
>
> ________________________________
> Do you Yahoo!?
> Next-gen email? Have it all with the all-new Yahoo! Mail Beta.
>
>


From henrik.parn at bio.ntnu.no  Fri Aug 18 16:27:25 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Fri, 18 Aug 2006 16:27:25 +0200
Subject: [R] lmList and missing values
Message-ID: <44E5CE4D.4060508@bio.ntnu.no>

Dear all,

I have a question on handling of missing values in lmList. My data set 
have continuous predictor and response, x and y, and a grouping variable 
group.id. All these variables have NAs and the data set also has several 
other variables that also contains NAs.

To create the lmList-object seems to work fine:
y.list <- lmList(y ~ x | group.id, data=mydata, na.action=na.omit)

However, when I try to apply functions on the object such as coef or 
intervals it fails:

 > coef(y.list)
Error in !unlist(lapply(coefs, is.null)) :
        invalid argument type


If I beforehand select only the variables (still including missing 
values) used as arguments in lmList...
 
mydata2 <- mydata[ , c("x", "y", "group")]

...and use mydata2 in lmList as above, coef and intervals works fine.

In order to provide a reproducible example, I made a small test data 
set. This data set contained the same pattern of NAs as in my real data 
set, i.e. NAs in both used and unused variables. The example turned out 
to be not very illustrative though, because strange enough 
'na.action=na.omit' works just fine on the small test data set so I 
don't bother to include it here...

I have not encountered any problems to apply other functions, such as lm 
or lme, to my data set.

Any idea what causes the error?

A related problem seems to have occured before, although it is not clear 
if different na.action options was tried in this case:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/14293.html


Thanks in advance for any help!

Best regards,

Henrik

-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)


From tom.boonen.maiden at gmail.com  Fri Aug 18 16:41:31 2006
From: tom.boonen.maiden at gmail.com (Tom Boonen)
Date: Fri, 18 Aug 2006 10:41:31 -0400
Subject: [R] as.data.frame(cbind()) transforming numeric to factor?
Message-ID: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>

Dear List,

why does as.data.frame(cbind()) transform numeric variables to
factors, once one of the other variablesused is a character vector?

#
x.1 <- rnorm(10)
x.2 <- c(rep("Test",10))
Foo <- as.data.frame(cbind(x.1))
is.factor(Foo$x.1)

Foo <- as.data.frame(cbind(x.1,x.2))
is.factor(Foo$x.1)
#

I assume there is a good reason for this, can somebody explain? Thanks.

Best,
Tom


From stefano.sofia at regione.marche.it  Fri Aug 18 16:50:01 2006
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Fri, 18 Aug 2006 16:50:01 +0200
Subject: [R] Query: how to modify the plot of acf
Message-ID: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/830a61a7/attachment.pl 

From chrish at stats.ucl.ac.uk  Fri Aug 18 16:50:26 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Fri, 18 Aug 2006 15:50:26 +0100 (BST)
Subject: [R] R-update - what about packages and ESS?
Message-ID: <Pine.LNX.4.64.0608181545090.21027@egon.stats.ucl.ac.uk>

Hi there,

it seems that if I update R, it doesn't find previously installed packages 
anymore and is also not found by ESS.
Actually the update has been done by our system administrator who assumed 
that there would be no problems with these things (I don't have root 
access to this system) and will perhaps not be too keen on installing
everything else again.
Is there any simple way how ESS and the packages can be connected to the 
new R?

I remember that whenever I updated R on my private computers, I installed 
everything else again as well - but that's certainly not everybodies 
taste...

Best,
Christian


*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From deepayan.sarkar at gmail.com  Fri Aug 18 16:52:05 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 18 Aug 2006 09:52:05 -0500
Subject: [R] Lattice package par.settings/trellis.par.settings questions
In-Reply-To: <44E53297.7070408@umn.edu>
References: <44E53297.7070408@umn.edu>
Message-ID: <eb555e660608180752q6f0194d3udd7b3702cded4b7e@mail.gmail.com>

On 8/17/06, Debarchana Ghosh <ghos0033 at umn.edu> wrote:
> Hi All,
>
> I'm trying to modify some of the default graphic parameters in a
> conditional histogram. While I was able to change the default grey
> background to white, I couldn't change the axis.font or the xlab font.

The default background is no longer grey in the latest release.

> I used the following code:
>
> /histogram(~V751|V013+V025, finalbase, xlab="Heard of HIV/AIDS
> (No/Yes)", col=c("cyan","magenta"), par.settings=list(background="white"))
>
> /The arguments for example  like /axis.font=2/, or /cex=2/ are not
> working in the /par.settings(). /I also tried to read the manual of
> /trellis.par.settings()/ but didn't understand how to use it and where
> exactly to put it.

The documentation is certainly lacking in this area (I hope to improve
it in the coming months). Gabor mentioned the settings for axis
labels. The xlab text is controlled by "par.xlab.text", try:

str(trellis.par.get("par.xlab.text"))

Both can be controlled in the call directly, e.g.

histogram(1:10,
          scales = list(font = 2, cex = 0.5),
          xlab = list(cex = 2, col = 'red'))

Such use /is/ fairly well documented.

Deepayan


From deepayan.sarkar at gmail.com  Fri Aug 18 16:56:38 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 18 Aug 2006 09:56:38 -0500
Subject: [R] Lattice package par.settings/trellis.par.settings questions
In-Reply-To: <loom.20060818T065050-416@post.gmane.org>
References: <44E53297.7070408@umn.edu>
	<loom.20060818T065050-416@post.gmane.org>
Message-ID: <eb555e660608180756x4477b7d2i5cc88ed1e7726031@mail.gmail.com>

On 8/17/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> Please read about lattice.par.settings, and not trellis.par.settings. Trellis is
> in S/S-plus.

As far as I know, there's no such thing as lattice.par.settings.
``Trellis''-compatible things in the lattice package have the same
names as the original.

Deepayan


From mschwartz at mn.rr.com  Fri Aug 18 16:55:44 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 18 Aug 2006 09:55:44 -0500
Subject: [R] as.data.frame(cbind()) transforming numeric to factor?
In-Reply-To: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
References: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
Message-ID: <1155912944.4405.7.camel@localhost.localdomain>

On Fri, 2006-08-18 at 10:41 -0400, Tom Boonen wrote:
> Dear List,
> 
> why does as.data.frame(cbind()) transform numeric variables to
> factors, once one of the other variablesused is a character vector?
> 
> #
> x.1 <- rnorm(10)
> x.2 <- c(rep("Test",10))
> Foo <- as.data.frame(cbind(x.1))
> is.factor(Foo$x.1)
> 
> Foo <- as.data.frame(cbind(x.1,x.2))
> is.factor(Foo$x.1)
> #
> 
> I assume there is a good reason for this, can somebody explain? Thanks.
> 
> Best,
> Tom

See the Note section of ?cbind, which states:

The method dispatching is not done via UseMethod(), but by C-internal
dispatching. Therefore, there is no need for, e.g., rbind.default.

The dispatch algorithm is described in the source file
(?.../src/main/bind.c?) as

     1. For each argument we get the list of possible class memberships
        from the class attribute.
     2. We inspect each class in turn to see if there is an an
        applicable method.
     3. If we find an applicable method we make sure that it is
        identical to any method determined for prior arguments. If it is
        identical, we proceed, otherwise we immediately drop through to
        the default code.

If you want to combine other objects with data frames, it may be
necessary to coerce them to data frames first. (Note that this algorithm
can result in calling the data frame method if the arguments are all
either data frames or vectors, and this will result in the coercion of
character vectors to factors.)


Thus, note the result of:

> str(cbind(x.1, x.2))
 chr [1:10, 1:2] "-0.265756038510064" "2.13220714034528" ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:2] "x.1" "x.2"

Since a matrix can only contain a single data type, the numeric vector
is coerced to character.

Then using as.data.frame() coerces the character matrix to factors,
which is the default behavior.

If you want to create a data frame, do it this way:

> str(data.frame(x.1, x.2))
`data.frame':   10 obs. of  2 variables:
 $ x.1: num  -0.266  2.132  2.096 -0.128 -0.466 ...
 $ x.2: Factor w/ 1 level "Test": 1 1 1 1 1 1 1 1 1 1

or if you want to retain the character vector, use I():

> str(data.frame(x.1, I(x.2)))
`data.frame':   10 obs. of  2 variables:
 $ x.1: num  -0.266  2.132  2.096 -0.128 -0.466 ...
 $ x.2:Class 'AsIs'  chr [1:10] "Test" "Test" "Test" "Test" ...


See ?data.frame for more information.

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Fri Aug 18 17:11:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Aug 2006 11:11:55 -0400
Subject: [R] as.data.frame(cbind()) transforming numeric to factor?
In-Reply-To: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
References: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
Message-ID: <971536df0608180811y43cdfb07mcf57828db8254ed7@mail.gmail.com>

In "R version 2.4.0 Under development (unstable) (2006-08-08 r38825)"
one can do this:

as.data.frame(cbind(x.1,x.2),stringsAsFactors = FALSE)


On 8/18/06, Tom Boonen <tom.boonen.maiden at gmail.com> wrote:
> Dear List,
>
> why does as.data.frame(cbind()) transform numeric variables to
> factors, once one of the other variablesused is a character vector?
>
> #
> x.1 <- rnorm(10)
> x.2 <- c(rep("Test",10))
> Foo <- as.data.frame(cbind(x.1))
> is.factor(Foo$x.1)
>
> Foo <- as.data.frame(cbind(x.1,x.2))
> is.factor(Foo$x.1)
> #
>
> I assume there is a good reason for this, can somebody explain? Thanks.
>
> Best,
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Fri Aug 18 17:13:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Aug 2006 16:13:07 +0100 (BST)
Subject: [R] as.data.frame(cbind()) transforming numeric to factor?
In-Reply-To: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
References: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608181608200.27369@gannet.stats.ox.ac.uk>

On Fri, 18 Aug 2006, Tom Boonen wrote:

> Dear List,
> 
> why does as.data.frame(cbind()) transform numeric variables to
> factors, once one of the other variablesused is a character vector?
> 
> #
> x.1 <- rnorm(10)
> x.2 <- c(rep("Test",10))
> Foo <- as.data.frame(cbind(x.1))
> is.factor(Foo$x.1)
> 
> Foo <- as.data.frame(cbind(x.1,x.2))
> is.factor(Foo$x.1)
> #
> 
> I assume there is a good reason for this, can somebody explain? Thanks.

Only if you can explain the good reason why you did not just use 
data.frame(x.1, x.2)!

cbind() makes a matrix out of vectors, here a character matrix.  And then 
as.data.frame() converts character columns to factors.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tom.boonen.maiden at gmail.com  Fri Aug 18 17:16:45 2006
From: tom.boonen.maiden at gmail.com (Tom Boonen)
Date: Fri, 18 Aug 2006 11:16:45 -0400
Subject: [R] as.data.frame(cbind()) transforming numeric to factor?
In-Reply-To: <Pine.LNX.4.64.0608181608200.27369@gannet.stats.ox.ac.uk>
References: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
	<Pine.LNX.4.64.0608181608200.27369@gannet.stats.ox.ac.uk>
Message-ID: <cc088e260608180816ve80c1bfucf4e93ccfb3ab543@mail.gmail.com>

Thanks everybody. I recognize my mistake now.

I think as.data.frame(cbind(x.1,x.2),stringsAsFactors = FALSE) would
be a good idea.

Tom

On 8/18/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Fri, 18 Aug 2006, Tom Boonen wrote:
>
> > Dear List,
> >
> > why does as.data.frame(cbind()) transform numeric variables to
> > factors, once one of the other variablesused is a character vector?
> >
> > #
> > x.1 <- rnorm(10)
> > x.2 <- c(rep("Test",10))
> > Foo <- as.data.frame(cbind(x.1))
> > is.factor(Foo$x.1)
> >
> > Foo <- as.data.frame(cbind(x.1,x.2))
> > is.factor(Foo$x.1)
> > #
> >
> > I assume there is a good reason for this, can somebody explain? Thanks.
>
> Only if you can explain the good reason why you did not just use
> data.frame(x.1, x.2)!
>
> cbind() makes a matrix out of vectors, here a character matrix.  And then
> as.data.frame() converts character columns to factors.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From mikewolfgang at gmail.com  Fri Aug 18 17:17:06 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Fri, 18 Aug 2006 11:17:06 -0400
Subject: [R] apply least angle regression to generalized linear models
Message-ID: <e668df8c0608180817m5bbf73c0v1aff562fb87314ed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/e7f4bfe1/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Aug 18 17:24:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Aug 2006 16:24:45 +0100 (BST)
Subject: [R] R-update - what about packages and ESS?
In-Reply-To: <Pine.LNX.4.64.0608181545090.21027@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.64.0608181545090.21027@egon.stats.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0608181614110.27369@gannet.stats.ox.ac.uk>

Without knowing your OS this is hard to answer (and is the wrong list for 
the ESS question).

For Windows users, the packages part is covered in the rw-FAQ.

For Unix-alikes, it all depends how the update was done, but normal 
package update mechanisms (such as RPM) will not wipe out previously 
installed R packages: nor will 'make install'.

In either case you don't need root access to install packages, as you can 
use a private library.  We have things set up so a user's R_LIBS is 
something like

	R_LIBS=~/Rlibrary:/usr/local/Rlibs:/usr/local/BioC

and then install.packages() automatically installs into the user's own 
library (if it has been created).

On Fri, 18 Aug 2006, Christian Hennig wrote:

> Hi there,
> 
> it seems that if I update R, it doesn't find previously installed packages 
> anymore and is also not found by ESS.
> Actually the update has been done by our system administrator who assumed 
> that there would be no problems with these things (I don't have root 
> access to this system) and will perhaps not be too keen on installing
> everything else again.
> Is there any simple way how ESS and the packages can be connected to the 
> new R?
> 
> I remember that whenever I updated R on my private computers, I installed 
> everything else again as well - but that's certainly not everybodies 
> taste...
> 
> Best,
> Christian
> 
> 
> *** --- ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rmh at temple.edu  Fri Aug 18 17:30:28 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 18 Aug 2006 11:30:28 -0400 (EDT)
Subject: [R] R-update - what about packages and ESS?
Message-ID: <20060818113028.BGF58273@po-d.temple.edu>

ESS finds R by one of two mechanisms.

If R is in your PATH, then ESS will find it.

Or, if you have explicitly set the emacs variable
inferior-R-program-name, in .emacs, site-start.el, or ess-site.el,
then ESS will find R.

My guess is that you have explicitly set inferior-R-program-name
to your previous version of R.

Rich


From ana.pmartins at ine.pt  Fri Aug 18 17:49:58 2006
From: ana.pmartins at ine.pt (Ana Patricia Martins)
Date: Fri, 18 Aug 2006 16:49:58 +0100
Subject: [R]  Boxplot Help
Message-ID: <E97312684A84D511BDD40002A50968D6083E116C@lxpobw01.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/49263a12/attachment.pl 

From mschwartz at mn.rr.com  Fri Aug 18 17:53:43 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 18 Aug 2006 10:53:43 -0500
Subject: [R] apply least angle regression to generalized linear models
In-Reply-To: <e668df8c0608180817m5bbf73c0v1aff562fb87314ed@mail.gmail.com>
References: <e668df8c0608180817m5bbf73c0v1aff562fb87314ed@mail.gmail.com>
Message-ID: <1155916423.4405.16.camel@localhost.localdomain>

On Fri, 2006-08-18 at 11:17 -0400, Mike Wolfgang wrote:
> Hello list,
> 
> I've been searching around trying to find whether somebody has written such
> a package of least angle regression on generalized linear models, like what
> Lasso2 package does. The extension to generalized linear models is briefly
> discussed in the comment by D. Madigan and G. Ridgeway. Is such a package
> available? Thanks,
> 
> Mike

See the aptly named 'lars' package on CRAN and the attendant paper here:

  http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf

You might also want to review Professor Hastie's presentation at useR!
2006 this past spring:

  http://www.r-project.org/useR-2006/Slides/Hastie.pdf

HTH,

Marc Schwartz


From chrish at stats.ucl.ac.uk  Fri Aug 18 18:07:06 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Fri, 18 Aug 2006 17:07:06 +0100 (BST)
Subject: [R] R-update - what about packages and ESS?
In-Reply-To: <Pine.LNX.4.64.0608181614110.27369@gannet.stats.ox.ac.uk>
References: <Pine.LNX.4.64.0608181545090.21027@egon.stats.ucl.ac.uk>
	<Pine.LNX.4.64.0608181614110.27369@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0608181655230.21027@egon.stats.ucl.ac.uk>

Thanks for the answers so far.

I have to go through this with my system administrator.

The system is slackware linux.

> For Unix-alikes, it all depends how the update was done, but normal
> package update mechanisms (such as RPM) will not wipe out previously
> installed R packages: nor will 'make install'.

I don't really understand what "does not wipe out" means. Certainly, the 
packages are not deleted from the hard disk, but...

Am I right that I should expect it as standard behaviour that all 
previously installed packages are found if R is updated in a 
straightforward manner (using "make install", say)?
Or is the opposite true that I definitely should not expect it? (And is 
there any standard solution for this else than installing all packages 
again?)

> In either case you don't need root access to install packages, as you can
> use a private library.

That's not really enough because I want to have them accessible to 
students as well. Furthermore, the sysadmin may develop kind of a "the 
whole R-thing is crap"-attitude if I can't come up with a solution 
preventing new installation of all packages, and I don't really want 
that. So I'm rather not interested in discussing solutions 
which involve installing all packages again (I know that this is possible).

Best,
Christian

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From sachinj.2006 at yahoo.com  Fri Aug 18 18:13:57 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 18 Aug 2006 09:13:57 -0700 (PDT)
Subject: [R] dataframe of unequal rows
Message-ID: <20060818161357.69156.qmail@web37607.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/8b090650/attachment.pl 

From jacques.veslot at good.ibl.fr  Fri Aug 18 18:13:20 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 18 Aug 2006 18:13:20 +0200
Subject: [R] [Fwd: Trend test and test for homogeneity of odd-ratios]
Message-ID: <44E5E720.1070704@good.ibl.fr>

I partly answered my question since independence_test() function in coin package apparently do 
Cochran-Armitage trend test just like Eric Lecoutre's function tabletrend() - slightly modified here:

 > independence_test(pheno ~ geno, data = dat2, teststat = "quad", scores = list(geno = c(0, 1, 2)))

         Asymptotic General Independence Test

data:  pheno by groups 1 < 2 < 3
chi-squared = 0.2268, df = 1, p-value = 0.6339

 > tabletrend(with(dat2, table(pheno, geno)))
[1] 0.6338308

-------- Message original --------
Sujet: Trend test and test for homogeneity of odd-ratios
Date: Wed, 16 Aug 2006 17:39:33 +0200
De: Jacques VESLOT <jacques.veslot at good.ibl.fr>
Pour: R-Help <r-help at stat.math.ethz.ch>

Dear r-users,

I am looking for some R functions to do Cochran-Armitage trend test for 2*3 tables (binary phenotype
vs. genotypes) and for testing the homogeneity of odds ratios within 2*3*k tables (binary phenotype
vs. genotypes vs. strata).

In R-Help archives, I've found a 2003 script by Eric Lecoutre for Cochran-Armitage trend test and a
script for Breslow-Day test for 2*2*k tables.

Could someone please tell we if there were some functions available on CRAN to do such tests ?

Thanks,

jacques


From gunter.berton at gene.com  Fri Aug 18 18:34:01 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 18 Aug 2006 09:34:01 -0700
Subject: [R] dataframe of unequal rows
In-Reply-To: <20060818161357.69156.qmail@web37607.mail.mud.yahoo.com>
Message-ID: <003a01c6c2e4$1c93d420$711f210a@gne.windows.gene.com>

How do you indicate which fields are present in a record with less than the
full number? Via known delimiters for all fields? Via the order of values
(fields are filled in order and only the last fields in a record can
therefore be missing)?

If the former, see the "sep" parameter in read.table() and friends.
If the latter, one way is to open the file as a connection and use
readLines()(you would check how many values were present and fill in the
NA's as needed).There may be better ways, though. ?connections will  get you
started.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sachin J
> Sent: Friday, August 18, 2006 9:14 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] dataframe of unequal rows
> 
> Hi,
>    
>   How can I read data of unequal number of observations 
> (rows) as is (i.e. without introducing NA for columns of less 
> observations than the maximum. Example:
>    
>   A    B   C   D
>   1    10  1   12
>   2    10  3   12
>   3    10  4   12
>   4    10  
>   5    10  
>    
>   Thanks in advance.
>    
>   Sachin
>    
>    
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Fri Aug 18 18:48:26 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 18 Aug 2006 12:48:26 -0400
Subject: [R] apply least angle regression to generalized linear models
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB35A4@usctmx1106.merck.com>

I believe `lars' does not currently fit glms.  For that you'll probably need
to look at `glar', at:

http://www.insightful.com/Hesterberg/glars/default.asp

HTH,
Andy 

From: Marc Schwartz
> 
> On Fri, 2006-08-18 at 11:17 -0400, Mike Wolfgang wrote:
> > Hello list,
> > 
> > I've been searching around trying to find whether somebody 
> has written 
> > such a package of least angle regression on generalized 
> linear models, 
> > like what
> > Lasso2 package does. The extension to generalized linear models is 
> > briefly discussed in the comment by D. Madigan and G. Ridgeway. Is 
> > such a package available? Thanks,
> > 
> > Mike
> 
> See the aptly named 'lars' package on CRAN and the attendant 
> paper here:
> 
>   http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf
> 
> You might also want to review Professor Hastie's presentation at useR!
> 2006 this past spring:
> 
>   http://www.r-project.org/useR-2006/Slides/Hastie.pdf
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From liuwensui at gmail.com  Fri Aug 18 18:55:16 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 18 Aug 2006 12:55:16 -0400
Subject: [R] apply least angle regression to generalized linear models
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB35A4@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB35A4@usctmx1106.merck.com>
Message-ID: <1115a2b00608180955y199ed20es7e318b55fbd050f6@mail.gmail.com>

Any is right.

I don't think current version of lars can be implemented in generalized LM.


On 8/18/06, Liaw, Andy <andy_liaw at merck.com> wrote:
> I believe `lars' does not currently fit glms.  For that you'll probably need
> to look at `glar', at:
>
> http://www.insightful.com/Hesterberg/glars/default.asp
>
> HTH,
> Andy
>
> From: Marc Schwartz
> >
> > On Fri, 2006-08-18 at 11:17 -0400, Mike Wolfgang wrote:
> > > Hello list,
> > >
> > > I've been searching around trying to find whether somebody
> > has written
> > > such a package of least angle regression on generalized
> > linear models,
> > > like what
> > > Lasso2 package does. The extension to generalized linear models is
> > > briefly discussed in the comment by D. Madigan and G. Ridgeway. Is
> > > such a package available? Thanks,
> > >
> > > Mike
> >
> > See the aptly named 'lars' package on CRAN and the attendant
> > paper here:
> >
> >   http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf
> >
> > You might also want to review Professor Hastie's presentation at useR!
> > 2006 this past spring:
> >
> >   http://www.r-project.org/useR-2006/Slides/Hastie.pdf
> >
> > HTH,
> >
> > Marc Schwartz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From mschwartz at mn.rr.com  Fri Aug 18 19:02:45 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 18 Aug 2006 12:02:45 -0500
Subject: [R] apply least angle regression to generalized linear models
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB35A4@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02BB35A4@usctmx1106.merck.com>
Message-ID: <1155920565.4405.19.camel@localhost.localdomain>

Andy,

Upon further review of the documentation for lars, you are correct.

Thanks for the pointer to the work by Tim et al.

Regards,

Marc

On Fri, 2006-08-18 at 12:48 -0400, Liaw, Andy wrote:
> I believe `lars' does not currently fit glms.  For that you'll probably need
> to look at `glar', at:
> 
> http://www.insightful.com/Hesterberg/glars/default.asp
> 
> HTH,
> Andy 
> 
> From: Marc Schwartz
> > 
> > On Fri, 2006-08-18 at 11:17 -0400, Mike Wolfgang wrote:
> > > Hello list,
> > > 
> > > I've been searching around trying to find whether somebody 
> > has written 
> > > such a package of least angle regression on generalized 
> > linear models, 
> > > like what
> > > Lasso2 package does. The extension to generalized linear models is 
> > > briefly discussed in the comment by D. Madigan and G. Ridgeway. Is 
> > > such a package available? Thanks,
> > > 
> > > Mike
> > 
> > See the aptly named 'lars' package on CRAN and the attendant 
> > paper here:
> > 
> >   http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf
> > 
> > You might also want to review Professor Hastie's presentation at useR!
> > 2006 this past spring:
> > 
> >   http://www.r-project.org/useR-2006/Slides/Hastie.pdf
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From maechler at stat.math.ethz.ch  Fri Aug 18 19:10:01 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 18 Aug 2006 19:10:01 +0200
Subject: [R] as.data.frame(cbind()) transforming numeric to factor?
In-Reply-To: <cc088e260608180816ve80c1bfucf4e93ccfb3ab543@mail.gmail.com>
References: <cc088e260608180741j70381a11rbca29e16635842dc@mail.gmail.com>
	<Pine.LNX.4.64.0608181608200.27369@gannet.stats.ox.ac.uk>
	<cc088e260608180816ve80c1bfucf4e93ccfb3ab543@mail.gmail.com>
Message-ID: <17637.62569.976265.217375@stat.math.ethz.ch>

>>>>> "Tom" == Tom Boonen <tom.boonen.maiden at gmail.com>
>>>>>     on Fri, 18 Aug 2006 11:16:45 -0400 writes:

    Tom> Thanks everybody. I recognize my mistake now.
    Tom> I think    as.data.frame(cbind(x.1,x.2),stringsAsFactors = FALSE)
    Tom> would be a good idea.

I think

	data.frame(x.1, x.2 = I(x.2))

would be a considerably better idea.

[ The use of I(.) for preventing coercion to factors 
  is a much older and "S-like" way ]

Martin



    Tom> Tom

    Tom> On 8/18/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
    >> On Fri, 18 Aug 2006, Tom Boonen wrote:
    >> 
    >> > Dear List,
    >> >
    >> > why does as.data.frame(cbind()) transform numeric variables to
    >> > factors, once one of the other variablesused is a character vector?
    >> >
    >> > #
    >> > x.1 <- rnorm(10)
    >> > x.2 <- c(rep("Test",10))
    >> > Foo <- as.data.frame(cbind(x.1))
    >> > is.factor(Foo$x.1)
    >> >
    >> > Foo <- as.data.frame(cbind(x.1,x.2))
    >> > is.factor(Foo$x.1)
    >> > #
    >> >
    >> > I assume there is a good reason for this, can somebody explain? Thanks.
    >> 
    >> Only if you can explain the good reason why you did not just use
    >> data.frame(x.1, x.2)!
    >> 
    >> cbind() makes a matrix out of vectors, here a character matrix.  And then
    >> as.data.frame() converts character columns to factors.
    >> 
    >> --
    >> Brian D. Ripley,                  ripley at stats.ox.ac.uk
    >> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
    >> University of Oxford,             Tel:  +44 1865 272861 (self)
    >> 1 South Parks Road,                     +44 1865 272866 (PA)
    >> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
    >> 

    Tom> ______________________________________________
    Tom> R-help at stat.math.ethz.ch mailing list
    Tom> https://stat.ethz.ch/mailman/listinfo/r-help
    Tom> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Tom> and provide commented, minimal, self-contained, reproducible code.


From sachinj.2006 at yahoo.com  Fri Aug 18 19:44:57 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 18 Aug 2006 10:44:57 -0700 (PDT)
Subject: [R] dataframe of unequal rows
In-Reply-To: <003a01c6c2e4$1c93d420$711f210a@gne.windows.gene.com>
Message-ID: <20060818174457.97174.qmail@web37608.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/c1bf6415/attachment.pl 

From gunter.berton at gene.com  Fri Aug 18 20:16:58 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 18 Aug 2006 11:16:58 -0700
Subject: [R] dataframe of unequal rows
Message-ID: <005101c6c2f2$7eeeef20$711f210a@gne.windows.gene.com>

 
test.txt:

"V1"	"V2"	"V3"	"V4"
1	2	3	4
5	6	7	
		8	9
10			11
12	13	14	15

The fields are delimited by tab characters ("\t")


In R:

> read.table(choose.files(),sep='\t',head=TRUE)

  V1 V2 V3 V4
1  1  2  3  4
2  5  6  7 NA
3 NA NA  8  9
4 10 NA NA 11
5 12 13 14 15

(I use choose.files() on Windows to select the file via the standard file
browser widget)

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: Sachin J [mailto:sachinj.2006 at yahoo.com] 
> Sent: Friday, August 18, 2006 10:45 AM
> To: Berton Gunter; R-help at stat.math.ethz.ch
> Subject: RE: [R] dataframe of unequal rows
> 
> Bert,
>  
> I tried readLines. It reads the data as is, but cant access 
> individual columns. Still cant figure out how to accomplish 
> this. An example would be of great help.
>  
> PS: How do you indicate which fields are present in a record 
> with less than the
> full number? - Via known delimiters for all fields. 
> 
> TIA
> Sachin
>  
> 
> Berton Gunter <gunter.berton at gene.com> wrote:
> 
> 	How do you indicate which fields are present in a 
> record with less than the
> 	full number? Via known delimiters for all fields? Via 
> the order of values
> 	(fields are filled in order and only the last fields in 
> a record can
> 	therefore be missing)?
> 	
> 	If the former, see the "sep" parameter in read.table() 
> and friends.
> 	If the latter, one way is to open the file as a 
> connection and use
> 	readLines()(you would check how many values were 
> present and fill in the
> 	NA's as needed).There may be better ways, though. 
> ?connections will get you
> 	started.
> 	
> 	-- Bert Gunter
> 	Genentech Non-Clinical Statistics
> 	South San Francisco, CA
> 	
> 	"The business of the statistician is to catalyze the 
> scientific learning
> 	process." - George E. P. Box
> 	
> 	
> 	
> 	> -----Original Message-----
> 	> From: r-help-bounces at stat.math.ethz.ch 
> 	> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf 
> Of Sachin J
> 	> Sent: Friday, August 18, 2006 9:14 AM
> 	> To: R-help at stat.math.ethz.ch
> 	> Subject: [R] dataframe of unequal rows
> 	> 
> 	> Hi,
> 	> 
> 	> How can I read data of unequal number of observations 
> 	> (rows) as is (i.e. without introducing NA for columns of less 
> 	> observations than the maximum. Example:
> 	> 
> 	> A B C D
> 	> 1 10 1 12
> 	> 2 10 3 12
> 	> 3 10 4 12
> 	> 4 10 
> 	> 5 10 
> 	> 
> 	> Thanks in advance.
> 	> 
> 	> Sachin
> 	> 
> 	> 
> 	> 
> 	> 
> 	> ---------------------------------
> 	> 
> 	> [[alternative HTML version deleted]]
> 	> 
> 	> ______________________________________________
> 	> R-help at stat.math.ethz.ch mailing list
> 	> https://stat.ethz.ch/mailman/listinfo/r-help
> 	> PLEASE do read the posting guide 
> 	> http://www.R-project.org/posting-guide.html
> 	> and provide commented, minimal, self-contained, 
> reproducible code.
> 	> 
> 	
> 	
> 
> 
> ________________________________
> 
> How low will we go? Check out Yahoo! Messenger's low 
> PC-to-Phone call rates. 
> <http://us.rd.yahoo.com/mail_us/taglines/postman8/*http://us.r
> d.yahoo.com/evt=39663/*http://voice.yahoo.com> 
>


From ggrothendieck at gmail.com  Fri Aug 18 20:17:17 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Aug 2006 14:17:17 -0400
Subject: [R] Boxplot Help
In-Reply-To: <E97312684A84D511BDD40002A50968D6083E116C@lxpobw01.ine.pt>
References: <E97312684A84D511BDD40002A50968D6083E116C@lxpobw01.ine.pt>
Message-ID: <971536df0608181117h624994b2k8274c4e191325db2@mail.gmail.com>

Try this:

result <- boxplot(Petal.Length ~ Species, iris)
if (length(result$out))
 text(result$group, result$out, match(result$out, iris$Petal.Length),
   pos = 4, col = "red")

If the outliers can be non-unique then match is not enough.
In that case assume that the nth occurrence of
any value in result$out is also the nth occurrence in the
vector boxplotted.  (Sort the data frame by group if that is
not the case.)   This assumption is sufficient to allow us to write
posof which gives the index into the data frame of any value in out.

# determine position of x in y
# assuming that if there are duplicates in x that
# they occur the same number of times and in
# the same order so that the 2nd occurrence of 37
# in x would correspond to the 2nd occurrence of 37 in y
posof <- function(x, y) {
   n <- sapply(seq(x), function(m) sum(x[m] == x[1:m]))
   mapply(function(x, n) which(y == x)[n], x, n)
}

result <- boxplot(Petal.Length ~ Species, iris)
if (length(result$out))
 text(result$group, result$out, posof(result$out, iris$Petal.Length),
   pos = 4, col = "red")



On 8/18/06, Ana Patricia Martins <ana.pmartins at ine.pt> wrote:
> Hello R-users and developers,
>
>
>
> Once again, I'm asking for your help.
>
> I can identify outliers in boxplot with this instruction
>
>
>
> result <- boxplot( Income ~ Sex,  col = "lightgray", data=dados)
>
> if (length(result$out))
>
>  text(result$group, result$out, result$out, pos = 4, col = "red")
>
>
>
> But I can not identify the outlier's id (variable names) in the boxplot.
>
> Can anyone help me?
>
> Thanks in advance,
>
>
>
> Atenciosamente,
>
> Ana Patricia Martins
>
> -------------------------------------------
>
> Servi?o M?todos Estat?sticos
>
> Departamento de Metodologia Estat?stica
>
> INE - Portugal
>
> Telef:  218 426 100 - Ext: 3210
>
> E-mail:  <mailto:ana.pmartins at ine.pt> ana.pmartins at ine.pt
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From David.Brahm at geodecapital.com  Fri Aug 18 20:55:57 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 18 Aug 2006 14:55:57 -0400
Subject: [R] Floating point imprecision in sum() under R-2.3.1?
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D806867DBA@MSGBOSCLF2WIN.DMN1.FMR.COM>

After upgrading to R-2.3.1 on Linux Redhat, I was suprised by this:

R> x <- c(721.077, 592.291, 372.208, 381.182)
R> sum(x) - 2066.758
   [1] 4.547474e-13

Now I understand that floating point arithmetic is not precise, but
1) the result is exactly 0 in R-2.2.1 (patched) on the same machine,
2) .Machine$double.eps = 2.2e-16, so the error seems quite large.

Also note I get the same result on R-2.3.1 under Windows, and that
R> (721.077 + 592.291 + 372.208 + 381.182) - 2066.758
   [1] 0

Is this related to the (2.3.0) NEWS item:
  sum(), prod(), mean(), rowSums() and friends use a long double
  accumulator where available and so may be more accurate. 
and should I be concerned?  Thanks.

-- David Brahm (brahm at alum.mit.edu)


Version:
 platform = i686-pc-linux-gnu
 arch = i686
 os = linux-gnu
 system = i686, linux-gnu
 status = 
 major = 2
 minor = 3.1
 year = 2006
 month = 06
 day = 01
 svn rev = 38247
 language = R
 version.string = Version 2.3.1 (2006-06-01)

Locale:
C

Search Path:
 .GlobalEnv, package:methods, package:stats, package:graphics,
package:grDevices, package:utils, package:datasets, Autoloads,
package:base


From mb.atelier at web.de  Fri Aug 18 20:56:36 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Fri, 18 Aug 2006 20:56:36 +0200
Subject: [R] Query: how to modify the plot of acf
In-Reply-To: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>
References: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>
Message-ID: <44E60D64.5030907@web.de>

Hi Stefano,

the manual tells us that we can access components of an acf object
directly by acf.obj[.], but assignment ]<- does not work this way.

One way of doing what you want is to assign NA to x$acf[x$lag==0] like so:

x <- acf(runif(100))
x$acf[1] <- NA
plot(x)

But I suppose what you actually want is to have a reasonable ylim to be
in effect when plotting your acf. I have struggled with this myself and
found the following solution:

In plot.acf the formula for the white noise confidence interval is
something along the lines of

acf.clim <- function(x=stop("your acf"), n=x$n.used, CL=.95)
{
        cl = qnorm((1+CL)/2)/sqrt(n)
        attr(cl,"CL") <- CL
        return( cl)
}

Using this function you can plot
x <- acf(runif(100),plot=F)
plot(x, ylim=2*acf.clim(x)*c(-1,1))	# adjust magnification as needed


As for your second question we first prevent plotting of x-axis and then
add our custom axis:

x <- acf(runif(1000),100,plot=F)
plot(x, ylim=2*acf.clim(x)*c(-1,1),xaxt='n')	# note option 'xaxt'!
axis(1,at=12*(0:8))

Hope that helps,

Matthias


NB: It would be a good idea to include the acf.clim(.) inside of the acf
object... Now it is somewhat hidden inside of plot.acf

x$clim <- acf.clim(x)
str(x)





Stefano Sofia wrote:
> I need to modify the graph of the autocorrelation. I tried to do it through plot.acf but with no success. 
> 
> 1. I would like to get rid of the lag zero
> 2. I would like to have numbers on the x-axis only at lags 12, 24, 36, 48, 60, ...
> 
> Could anybody help me in this?
> 
> Any help will be appreciated
> Thank you for your attention
> Stefano
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
Journalist: "Mr. Gandhi, what do you think of Western civilization?"
Gandhi: "I think it would be a good idea."

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~                   Matthias Braeunig, Dipl.Phys.
~                University Medical Center Freiburg
~    Institute of Environmental Medicine and Hospital Epidemiology
~     Department of Evaluation Research in Complementary Medicine
~          Hugstetter Str. 55, D - 79106 Freiburg, Germany
~              Phone: +49 (0)761 270-8306, Fax: -8343
~             Web: http://kompmed.uniklinik-freiburg.de
~           Email: matthias.braeunig at uniklinik-freiburg.de
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From rdpeng at gmail.com  Fri Aug 18 21:36:49 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Fri, 18 Aug 2006 15:36:49 -0400
Subject: [R] Floating point imprecision in sum() under R-2.3.1?
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D806867DBA@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D806867DBA@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <44E616D1.5030805@gmail.com>

I think you want to look at

sum(x)/2066.758 - 1

which on my Linux box is 2.2e-16.

-roger

Brahm, David wrote:
> After upgrading to R-2.3.1 on Linux Redhat, I was suprised by this:
> 
> R> x <- c(721.077, 592.291, 372.208, 381.182)
> R> sum(x) - 2066.758
>    [1] 4.547474e-13
> 
> Now I understand that floating point arithmetic is not precise, but
> 1) the result is exactly 0 in R-2.2.1 (patched) on the same machine,
> 2) .Machine$double.eps = 2.2e-16, so the error seems quite large.
> 
> Also note I get the same result on R-2.3.1 under Windows, and that
> R> (721.077 + 592.291 + 372.208 + 381.182) - 2066.758
>    [1] 0
> 
> Is this related to the (2.3.0) NEWS item:
>   sum(), prod(), mean(), rowSums() and friends use a long double
>   accumulator where available and so may be more accurate. 
> and should I be concerned?  Thanks.
> 
> -- David Brahm (brahm at alum.mit.edu)
> 
> 
> Version:
>  platform = i686-pc-linux-gnu
>  arch = i686
>  os = linux-gnu
>  system = i686, linux-gnu
>  status = 
>  major = 2
>  minor = 3.1
>  year = 2006
>  month = 06
>  day = 01
>  svn rev = 38247
>  language = R
>  version.string = Version 2.3.1 (2006-06-01)
> 
> Locale:
> C
> 
> Search Path:
>  .GlobalEnv, package:methods, package:stats, package:graphics,
> package:grDevices, package:utils, package:datasets, Autoloads,
> package:base
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From jesse.canchola.b at bayer.com  Fri Aug 18 22:02:13 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Fri, 18 Aug 2006 16:02:13 -0400
Subject: [R] Permutations with replacement
In-Reply-To: <17637.62569.976265.217375@stat.math.ethz.ch>
Message-ID: <OF58B9A0BF.48C8426C-ON882571CE.0065EB05-882571CE.006E0763@bayer.com>

Is there a simple function or process that will create permutations with 
replacement? 

I know that using the combinat package

###### begin R code ######
> library(combinat)
> m <- t(array(unlist(permn(3)), dim = c(3, 6)))

# we can get the permutations, for example 3!=6
# gives us

> m
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    3    2
[3,]    3    1    2
[4,]    3    2    1
[5,]    2    3    1
[6,]    2    1    3
###### end R code ##########

I'd like to include the "with replacement possibilities" such as 

1,1,3
1,1,2
2,3,3

and so on.  This will eventually be done on 8!=40,320 rather than the 
development version using 3! as above.

If no function exists (I've Googled on CRAN with no palpable luck), then 
perhaps this is more of a bootstrap type problem. 

Thanks for your help in advance,
Jesse Canchola










_______________________________________________________________________________________________

The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com


From David.Brahm at geodecapital.com  Fri Aug 18 22:04:53 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 18 Aug 2006 16:04:53 -0400
Subject: [R] Floating point imprecision in sum() under R-2.3.1?
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D80781CA17@MSGBOSCLF2WIN.DMN1.FMR.COM>

I was concerned by this result (new in R-2.3.1):
R> x <- c(721.077, 592.291, 372.208, 381.182)
R> sum(x) - 2066.758
   [1] 4.547474e-13

But after Roger Peng's <rdpeng at gmail.com> insightful comment that the
relative difference (sum(x)/2066.758 - 1) is exactly what is expected,
I'm convinced that sum() is indeed really being "more accurate" than
it was in 2.2.1, i.e. accurately preserving the numerical imprecision
of the original inputs.  Sorry for the distraction...

-- David Brahm (brahm at alum.mit.edu)


From jesse.canchola.b at bayer.com  Fri Aug 18 22:25:30 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Fri, 18 Aug 2006 16:25:30 -0400
Subject: [R] Permutations with replacement
Message-ID: <OF194ADE56.7D916602-ON882571CE.00701514-882571CE.00702C3A@bayer.com>

Is there a simple function or process that will create a matrix of 
permutations with replacement? 

I know that using the combinat package

###### begin R code ######
> library(combinat)
> m <- t(array(unlist(permn(3)), dim = c(3, 6)))

# we can get the permutations, for example 3!=6
# gives us

> m
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    3    2
[3,]    3    1    2
[4,]    3    2    1
[5,]    2    3    1
[6,]    2    1    3
###### end R code ##########

I'd like to include the "with replacement possibilities" such as 

1,1,3
1,1,2
2,3,3

and so on.  This will eventually be done on 8!=40,320 rather than the 
development version using 3! as above.

If no function exists (I've Googled on CRAN with no palpable luck), then 
perhaps this is more of a bootstrap type problem. 

Thanks for your help in advance,
Jesse Canchola










_______________________________________________________________________________________________

The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com


From h-tsai at northwestern.edu  Fri Aug 18 22:26:50 2006
From: h-tsai at northwestern.edu (Hui-Ju Tsai)
Date: Fri, 18 Aug 2006 15:26:50 -0500
Subject: [R] multivariate analysis by using lme
Message-ID: <000f01c6c304$a32f9d50$4260a8c0@childrensmemorial.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/35ce34de/attachment.pl 

From davidr at rhotrading.com  Fri Aug 18 22:33:20 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 18 Aug 2006 15:33:20 -0500
Subject: [R] Permutations with replacement
Message-ID: <F9F2A641C593D7408925574C05A7BE770D1A50@rhopost.rhotrading.com>

If you also want 1,1,1 and so on, the number of these is n^n,
(n choices for each of n slots.)
In that case, you could use hcube from combinat.

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jesse Albert
Canchola
Sent: Friday, August 18, 2006 3:26 PM
To: r-help
Subject: [R] Permutations with replacement

Is there a simple function or process that will create a matrix of 
permutations with replacement? 

I know that using the combinat package

###### begin R code ######
> library(combinat)
> m <- t(array(unlist(permn(3)), dim = c(3, 6)))

# we can get the permutations, for example 3!=6
# gives us

> m
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    3    2
[3,]    3    1    2
[4,]    3    2    1
[5,]    2    3    1
[6,]    2    1    3
###### end R code ##########

I'd like to include the "with replacement possibilities" such as 

1,1,3
1,1,2
2,3,3

and so on.  This will eventually be done on 8!=40,320 rather than the 
development version using 3! as above.

If no function exists (I've Googled on CRAN with no palpable luck), then

perhaps this is more of a bootstrap type problem. 

Thanks for your help in advance,
Jesse Canchola










________________________________________________________________________
_______________________

The information contained in this e-mail is for the exclusive use of the
intended recipient(s) and may be confidential, proprietary, and/or
legally privileged.  Inadvertent disclosure of this message does not
constitute a waiver of any privilege.  If you receive this message in
error, please do not directly or indirectly use, print, copy, forward,
or disclose any part of this message.  Please also delete this e-mail
and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tilling at gmail.com  Fri Aug 18 22:43:52 2006
From: tilling at gmail.com (John Tillinghast)
Date: Fri, 18 Aug 2006 13:43:52 -0700
Subject: [R] Affy: problems using neweS
Message-ID: <73bcebe0608181343u3f65c3ddp5412138a9c49d788@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060818/2850396a/attachment.pl 

From rguha at indiana.edu  Fri Aug 18 22:44:41 2006
From: rguha at indiana.edu (Rajarshi Guha)
Date: Fri, 18 Aug 2006 16:44:41 -0400
Subject: [R] list of lists to a data.frame
Message-ID: <1155933881.7504.64.camel@localhost>

Hi, I have a situation where I have a list of lists. Each list can
contain elements of different types (but each one will be a scalar) say
of double, integer or character.

However the elements of each list are always in the same order:

x <- list('a', 1, 2)
y <- list('b', 3, 4)
z <- list('c', 5, 6)

a <- list(x,y,z)

What I'd like to do is to convert a to a data.frame.

Currently I am doing:

b <- do.call(rbind, a)

However, when I do b[,1] I get a list returned rather than a vector of
characters and similarly for b[,2] and so on.

I am clearly missing something, but how do I convert the list of lists
to a data.frame where a column is represented as a vector rather than a
list?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All life evolves by the differential survival of replicating entities.
-- Dawkins


From gunter.berton at gene.com  Fri Aug 18 22:45:20 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 18 Aug 2006 13:45:20 -0700
Subject: [R] Floating point imprecision in sum() under R-2.3.1?
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D80781CA17@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <007b01c6c307$389df060$711f210a@gne.windows.gene.com>

> 
> But after Roger Peng's <rdpeng at gmail.com> **insightful** comment that the

... but as we are not in that <<other>> S language dialect, maybe it should
be his **peRceptive** comment. ;-)

(Sorry -- it's Friday)

-- Bert Gunter


From mschwartz at mn.rr.com  Fri Aug 18 22:56:42 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 18 Aug 2006 15:56:42 -0500
Subject: [R] list of lists to a data.frame
In-Reply-To: <1155933881.7504.64.camel@localhost>
References: <1155933881.7504.64.camel@localhost>
Message-ID: <1155934603.4405.32.camel@localhost.localdomain>

On Fri, 2006-08-18 at 16:44 -0400, Rajarshi Guha wrote:
> Hi, I have a situation where I have a list of lists. Each list can
> contain elements of different types (but each one will be a scalar) say
> of double, integer or character.
> 
> However the elements of each list are always in the same order:
> 
> x <- list('a', 1, 2)
> y <- list('b', 3, 4)
> z <- list('c', 5, 6)
> 
> a <- list(x,y,z)
> 
> What I'd like to do is to convert a to a data.frame.
> 
> Currently I am doing:
> 
> b <- do.call(rbind, a)
> 
> However, when I do b[,1] I get a list returned rather than a vector of
> characters and similarly for b[,2] and so on.
> 
> I am clearly missing something, but how do I convert the list of lists
> to a data.frame where a column is represented as a vector rather than a
> list?
> 
> Thanks,

How about:

> as.data.frame(sapply(a, rbind))
  V1 V2 V3
1  a  b  c
2  1  3  5
3  2  4  6

or:

> as.data.frame(t(sapply(a, rbind)))
  V1 V2 V3
1  a  1  2
2  b  3  4
3  c  5  6


depending upon how you wanted the columns versus rows to be structured.

HTH,

Marc Schwartz


From res90sx5 at verizon.net  Sat Aug 19 02:16:37 2006
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Fri, 18 Aug 2006 17:16:37 -0700
Subject: [R] Permutations with replacement
In-Reply-To: <OF58B9A0BF.48C8426C-ON882571CE.0065EB05-882571CE.006E0763@bayer.com>
Message-ID: <000f01c6c324$bd44cec0$6401a8c0@main>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Jesse Albert Canchola
> Sent: Friday, August 18, 2006 1:02 PM
> To: r-help
> Subject: [R] Permutations with replacement
> 
> Is there a simple function or process that will create permutations with
> replacement?
> 
> I know that using the combinat package
> 
> ###### begin R code ######
> > library(combinat)
> > m <- t(array(unlist(permn(3)), dim = c(3, 6)))
> 
> # we can get the permutations, for example 3!=6
> # gives us
> 
> > m
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    1    3    2
> [3,]    3    1    2
> [4,]    3    2    1
> [5,]    2    3    1
> [6,]    2    1    3
> ###### end R code ##########
> 
> I'd like to include the "with replacement possibilities" such as
> 
> 1,1,3
> 1,1,2
> 2,3,3
> 
Isn't what you want just sampling with replacement?

  x <- c(1,2,3)
  sample(x,3,replace=TRUE)

Hope this is helpful,

Dan

Dan Nordlund
Bothell, WA  USA


From ggrothendieck at gmail.com  Sat Aug 19 05:01:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Aug 2006 23:01:41 -0400
Subject: [R] Boxplot Help
In-Reply-To: <971536df0608181117h624994b2k8274c4e191325db2@mail.gmail.com>
References: <E97312684A84D511BDD40002A50968D6083E116C@lxpobw01.ine.pt>
	<971536df0608181117h624994b2k8274c4e191325db2@mail.gmail.com>
Message-ID: <971536df0608182001w28c2b8d4j7b0b46d98cd27dc7@mail.gmail.com>

In reviewing this I found an error in the case that there is an
outlier in one group with an equal value in another group that
is not an outlier.    Also the iris example does not have duplicate
outliers so its not a very good test.  Here is a much shorter
version that does not have the cited problem.  Also we use
more suitable test data.

For each group, g, we find the indices in x, idx, of the values
corresponding to that group in out$out and then we use text()
to display those indices.  (Note that it will overprint indices
if there are multiple outliers with the same value in a group.
One could try jittering the x or y values in text to address
this.)

x <- c(1:49, 100, 51:100, 101:148, 50, 50)
grp <- gl(3, 50)
out <- boxplot(x ~ grp)
for(g in unique(out$group)) {
   idx <- which(x %in% out$out[out$group == g] & grp == g)
   text(g, x[idx], idx, pos = 4, col = 2, cex = .5)
}


On 8/18/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> result <- boxplot(Petal.Length ~ Species, iris)
> if (length(result$out))
>  text(result$group, result$out, match(result$out, iris$Petal.Length),
>   pos = 4, col = "red")
>
> If the outliers can be non-unique then match is not enough.
> In that case assume that the nth occurrence of
> any value in result$out is also the nth occurrence in the
> vector boxplotted.  (Sort the data frame by group if that is
> not the case.)   This assumption is sufficient to allow us to write
> posof which gives the index into the data frame of any value in out.
>
> # determine position of x in y
> # assuming that if there are duplicates in x that
> # they occur the same number of times and in
> # the same order so that the 2nd occurrence of 37
> # in x would correspond to the 2nd occurrence of 37 in y
> posof <- function(x, y) {
>   n <- sapply(seq(x), function(m) sum(x[m] == x[1:m]))
>   mapply(function(x, n) which(y == x)[n], x, n)
> }
>
> result <- boxplot(Petal.Length ~ Species, iris)
> if (length(result$out))
>  text(result$group, result$out, posof(result$out, iris$Petal.Length),
>   pos = 4, col = "red")
>
>
>
> On 8/18/06, Ana Patricia Martins <ana.pmartins at ine.pt> wrote:
> > Hello R-users and developers,
> >
> >
> >
> > Once again, I'm asking for your help.
> >
> > I can identify outliers in boxplot with this instruction
> >
> >
> >
> > result <- boxplot( Income ~ Sex,  col = "lightgray", data=dados)
> >
> > if (length(result$out))
> >
> >  text(result$group, result$out, result$out, pos = 4, col = "red")
> >
> >
> >
> > But I can not identify the outlier's id (variable names) in the boxplot.
> >
> > Can anyone help me?
> >
> > Thanks in advance,
> >
> >
> >
> > Atenciosamente,
> >
> > Ana Patricia Martins
> >
> > -------------------------------------------
> >
> > Servi?o M?todos Estat?sticos
> >
> > Departamento de Metodologia Estat?stica
> >
> > INE - Portugal
> >
> > Telef:  218 426 100 - Ext: 3210
> >
> > E-mail:  <mailto:ana.pmartins at ine.pt> ana.pmartins at ine.pt
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From Charles.Annis at StatisticalEngineering.com  Sat Aug 19 13:58:30 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sat, 19 Aug 2006 07:58:30 -0400
Subject: [R] string-to-number
Message-ID: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>

Greetings, Amigos:

I have been trying without success to convert a character string,
> repeated.measures.columns
[1] "3,6,10"

into c(3,6,10) for subsequent use.

as.numeric(repeated.measures.columns) doesn't work (likely because of the
commas)
[1] NA
Warning message:
NAs introduced by coercion

I've tried many things including 
strsplit(repeated.measures.columns, split = ",")

which produces a list with only one element, viz:
[[1]]
[1] "3"  "6"  "10"

as.numeric() doesn't like that either.

Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
this WAY harder than it is.

Would some kind soul please instruct me (and perhaps subsequent searchers)
how to convert the elements of a string into numbers?

Thank you.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From MSchwartz at mn.rr.com  Sat Aug 19 14:11:25 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 19 Aug 2006 07:11:25 -0500
Subject: [R] string-to-number
In-Reply-To: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
Message-ID: <1155989485.4308.7.camel@localhost.localdomain>

On Sat, 2006-08-19 at 07:58 -0400, Charles Annis, P.E. wrote:
> Greetings, Amigos:
> 
> I have been trying without success to convert a character string,
> > repeated.measures.columns
> [1] "3,6,10"
> 
> into c(3,6,10) for subsequent use.
> 
> as.numeric(repeated.measures.columns) doesn't work (likely because of the
> commas)
> [1] NA
> Warning message:
> NAs introduced by coercion
> 
> I've tried many things including 
> strsplit(repeated.measures.columns, split = ",")
> 
> which produces a list with only one element, viz:
> [[1]]
> [1] "3"  "6"  "10"
> 
> as.numeric() doesn't like that either.
> 
> Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> this WAY harder than it is.
> 
> Would some kind soul please instruct me (and perhaps subsequent searchers)
> how to convert the elements of a string into numbers?
> 
> Thank you.

One more step:

> as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
[1]  3  6 10

Use unlist() to take the output of strsplit() and convert it to a
vector, before coercing to numeric.

HTH,

Marc Schwartz


From p.dalgaard at biostat.ku.dk  Sat Aug 19 14:12:13 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Aug 2006 14:12:13 +0200
Subject: [R] string-to-number
In-Reply-To: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
Message-ID: <x24pw86i1u.fsf@turmalin.kubism.ku.dk>

"Charles Annis, P.E." <Charles.Annis at statisticalengineering.com> writes:

> Greetings, Amigos:
> 
> I have been trying without success to convert a character string,
> > repeated.measures.columns
> [1] "3,6,10"
> 
> into c(3,6,10) for subsequent use.
> 
> as.numeric(repeated.measures.columns) doesn't work (likely because of the
> commas)
> [1] NA
> Warning message:
> NAs introduced by coercion
> 
> I've tried many things including 
> strsplit(repeated.measures.columns, split = ",")
> 
> which produces a list with only one element, viz:
> [[1]]
> [1] "3"  "6"  "10"
> 
> as.numeric() doesn't like that either.
> 
> Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> this WAY harder than it is.
> 
> Would some kind soul please instruct me (and perhaps subsequent searchers)
> how to convert the elements of a string into numbers?

3) you're almost there, just not realizing it:

> x <- "3,6,10"
> as.numeric(strsplit(x,split = ",")[[1]])
[1]  3  6 10

or for that matter

> scan(textConnection(x), sep=",")
Read 3 items
[1]  3  6 10

although that leaves you with a dangling open connection.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Roger.Bivand at nhh.no  Sat Aug 19 14:20:43 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 19 Aug 2006 14:20:43 +0200 (CEST)
Subject: [R] string-to-number
In-Reply-To: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
Message-ID: <Pine.LNX.4.44.0608191417500.13565-100000@reclus.nhh.no>

On Sat, 19 Aug 2006, Charles Annis, P.E. wrote:

> Greetings, Amigos:
> 
> I have been trying without success to convert a character string,
> > repeated.measures.columns
> [1] "3,6,10"
> 
> into c(3,6,10) for subsequent use.
> 
> as.numeric(repeated.measures.columns) doesn't work (likely because of the
> commas)
> [1] NA
> Warning message:
> NAs introduced by coercion
> 
> I've tried many things including 
> strsplit(repeated.measures.columns, split = ",")
> 
> which produces a list with only one element, viz:
> [[1]]
> [1] "3"  "6"  "10"
> 
> as.numeric() doesn't like that either.

repeated.measures.columns is a vector. Consider:

repeated.measures.columns <- c("3,6,10", "5,4,9")
lst <- strsplit(repeated.measures.columns, split = ",")
lapply(lst, as.numeric)

which is why strsplit() returns a list - one list component for each 
repeated.measures.columns element. Just pick off the one you want with 
[[]]:

as.numeric(strsplit(repeated.measures.columns, split = ",")[[1]])


> 
> Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> this WAY harder than it is.
> 
> Would some kind soul please instruct me (and perhaps subsequent searchers)
> how to convert the elements of a string into numbers?
> 
> Thank you.
> 
> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:? 614-455-3265
> http://www.StatisticalEngineering.com
> ?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From AnupTyagi at yahoo.com  Sat Aug 19 14:27:37 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sat, 19 Aug 2006 12:27:37 +0000 (UTC)
Subject: [R] split a y-axis to show data on different scales
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au> <44E0A172.2020907@pdf.com>
	<44902.129.206.90.2.1155635601.squirrel@mail.panix.com>
Message-ID: <loom.20060819T142130-896@post.gmane.org>

Johannes H?sing <hannes <at> ruhrau.de> writes:

> 
> > 	  The pro's and con's of using "scale breaks" were discussed by
> > Cleveland (1985) The Elements of Graphing Data (Wadsworth, pp. 85-91,
> > 149).  I don't know what Cleveland said about this is the second edition
> 
> Spencer Graves:
> > but I believe there are times when scale breaks are
> > appropriate, but the display should make this nonstandard transition
> > very clear;
> 
> ... in which case you are close to having two graphs
> sharing an x-axis and therefore saving on ink (yay!).
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

This is an interesting visual interpretation issue: it may be possible to shade
the y-axis (which his thick like the top bars in Lattice plots), or shade the
main graphing area from dark to light (or two shades, for two scales) to give a
visual idea about the "density" or "stretch" of the space/scale on which the
points are plotted. There is problems with this as well (interpretation of
scale), but sometimes it may provide a better and quick visual communication. Is
this possible in R?


From ripley at stats.ox.ac.uk  Sat Aug 19 14:30:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Aug 2006 13:30:04 +0100 (BST)
Subject: [R] string-to-number
In-Reply-To: <1155989485.4308.7.camel@localhost.localdomain>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
	<1155989485.4308.7.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0608191322510.12049@gannet.stats.ox.ac.uk>

On Sat, 19 Aug 2006, Marc Schwartz wrote:

> On Sat, 2006-08-19 at 07:58 -0400, Charles Annis, P.E. wrote:
> > Greetings, Amigos:
> > 
> > I have been trying without success to convert a character string,
> > > repeated.measures.columns
> > [1] "3,6,10"
> > 
> > into c(3,6,10) for subsequent use.
> > 
> > as.numeric(repeated.measures.columns) doesn't work (likely because of the
> > commas)
> > [1] NA
> > Warning message:
> > NAs introduced by coercion
> > 
> > I've tried many things including 
> > strsplit(repeated.measures.columns, split = ",")
> > 
> > which produces a list with only one element, viz:
> > [[1]]
> > [1] "3"  "6"  "10"
> > 
> > as.numeric() doesn't like that either.
> > 
> > Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> > this WAY harder than it is.
> > 
> > Would some kind soul please instruct me (and perhaps subsequent searchers)
> > how to convert the elements of a string into numbers?
> > 
> > Thank you.
> 
> One more step:
> 
> > as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
> [1]  3  6 10
> 
> Use unlist() to take the output of strsplit() and convert it to a
> vector, before coercing to numeric.

Or, more simply, use [[1]] as in

as.numeric(strsplit(repeated.measures.columns, ",")[[1]])

Also,

eval(parse(text=paste("c(", repeated.measures.columns, ")")))

looks competitive, and is quite a bit more general (e.g. allows spaces, 
works with complex numbers), or you can use scan() from an anonymous file 
or a textConnection.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From info at data-ploner.com  Sat Aug 19 14:30:10 2006
From: info at data-ploner.com (data-ploner Meinhard Ploner)
Date: Sat, 19 Aug 2006 14:30:10 +0200
Subject: [R] problem with Rcmd check and fortran95, makefile
Message-ID: <000801c6c38b$36e49ba0$0200a8c0@notebookdp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060819/63d3d2a4/attachment.pl 

From d.quicke at imperial.ac.uk  Sat Aug 19 14:54:59 2006
From: d.quicke at imperial.ac.uk (Quicke, Donald L J)
Date: Sat, 19 Aug 2006 13:54:59 +0100
Subject: [R] need to find (and distinguish types of) carriage returns in a
	file that is scanned using scan
Message-ID: <D535A03654352647924346AB905842AA01152B59@icex4.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060819/75321d95/attachment.pl 

From MSchwartz at mn.rr.com  Sat Aug 19 14:58:46 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 19 Aug 2006 07:58:46 -0500
Subject: [R] string-to-number
In-Reply-To: <Pine.LNX.4.64.0608191322510.12049@gannet.stats.ox.ac.uk>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
	<1155989485.4308.7.camel@localhost.localdomain>
	<Pine.LNX.4.64.0608191322510.12049@gannet.stats.ox.ac.uk>
Message-ID: <1155992326.4308.15.camel@localhost.localdomain>

On Sat, 2006-08-19 at 13:30 +0100, Prof Brian Ripley wrote:
> On Sat, 19 Aug 2006, Marc Schwartz wrote:
> 
> > On Sat, 2006-08-19 at 07:58 -0400, Charles Annis, P.E. wrote:
> > > Greetings, Amigos:
> > > 
> > > I have been trying without success to convert a character string,
> > > > repeated.measures.columns
> > > [1] "3,6,10"
> > > 
> > > into c(3,6,10) for subsequent use.
> > > 
> > > as.numeric(repeated.measures.columns) doesn't work (likely because of the
> > > commas)
> > > [1] NA
> > > Warning message:
> > > NAs introduced by coercion
> > > 
> > > I've tried many things including 
> > > strsplit(repeated.measures.columns, split = ",")
> > > 
> > > which produces a list with only one element, viz:
> > > [[1]]
> > > [1] "3"  "6"  "10"
> > > 
> > > as.numeric() doesn't like that either.
> > > 
> > > Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> > > this WAY harder than it is.
> > > 
> > > Would some kind soul please instruct me (and perhaps subsequent searchers)
> > > how to convert the elements of a string into numbers?
> > > 
> > > Thank you.
> > 
> > One more step:
> > 
> > > as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
> > [1]  3  6 10
> > 
> > Use unlist() to take the output of strsplit() and convert it to a
> > vector, before coercing to numeric.
> 
> Or, more simply, use [[1]] as in
> 
> as.numeric(strsplit(repeated.measures.columns, ",")[[1]])
> 
> Also,
> 
> eval(parse(text=paste("c(", repeated.measures.columns, ")")))
> 
> looks competitive, and is quite a bit more general (e.g. allows spaces, 
> works with complex numbers), or you can use scan() from an anonymous file 
> or a textConnection.

I would say more than competitive:

  repeated.measures.columns <- paste(1:100000, collapse = ",")

> str(repeated.measures.columns)
 chr
"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,4"| __truncated__


> system.time(res1 <-
as.numeric(unlist(strsplit(repeated.measures.columns, ","))))
[1] 24.238  0.192 26.200  0.000  0.000

> system.time(res2 <- as.numeric(strsplit(repeated.measures.columns,
",")[[1]]))
[1] 24.313  0.196 26.471  0.000  0.000

> system.time(res3 <- eval(parse(text=paste("c(",
repeated.measures.columns, ")"))))
[1] 0.328 0.004 0.395 0.000 0.000


> str(res1)
 num [1:100000] 1 2 3 4 5 6 7 8 9 10 ...

> str(res2)
 num [1:100000] 1 2 3 4 5 6 7 8 9 10 ...

> str(res3)
 num [1:100000] 1 2 3 4 5 6 7 8 9 10 ...


> all(res1 == res2)
[1] TRUE

> all(res1 == res3)
[1] TRUE


Best regards,

Marc


From h.wickham at gmail.com  Sat Aug 19 15:01:23 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 19 Aug 2006 10:01:23 -0300
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <44902.129.206.90.2.1155635601.squirrel@mail.panix.com>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au> <44E0A172.2020907@pdf.com>
	<44902.129.206.90.2.1155635601.squirrel@mail.panix.com>
Message-ID: <f8e6ff050608190601o429700bkce0dab1d09176fe9@mail.gmail.com>

> Spencer Graves:
> > but I believe there are times when scale breaks are
> > appropriate, but the display should make this nonstandard transition
> > very clear;
>
> ... in which case you are close to having two graphs
> sharing an x-axis and therefore saving on ink (yay!).

If your main concern is saving ink, I suggest using white points ;)

Hadley


From h.wickham at gmail.com  Sat Aug 19 15:04:42 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 19 Aug 2006 10:04:42 -0300
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <loom.20060819T142130-896@post.gmane.org>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au> <44E0A172.2020907@pdf.com>
	<44902.129.206.90.2.1155635601.squirrel@mail.panix.com>
	<loom.20060819T142130-896@post.gmane.org>
Message-ID: <f8e6ff050608190604h3a0e65a3g9ff754a40ae298a3@mail.gmail.com>

> This is an interesting visual interpretation issue: it may be possible to shade
> the y-axis (which his thick like the top bars in Lattice plots), or shade the
> main graphing area from dark to light (or two shades, for two scales) to give a
> visual idea about the "density" or "stretch" of the space/scale on which the
> points are plotted. There is problems with this as well (interpretation of
> scale), but sometimes it may provide a better and quick visual communication. Is
> this possible in R?

That only emphases that fact that there is a scale break - it does not
solve the problem that the two regions of the graph are fundamentally
non-comparable because they have different scales.  I would argue that
a key component of a statistical graphic (not a pretty picture) is
that scales are consistent throughout the plot.

Hadley


From pkoufalas at adam.com.au  Sat Aug 19 15:53:44 2006
From: pkoufalas at adam.com.au (Paul Koufalas)
Date: Sat, 19 Aug 2006 23:23:44 +0930
Subject: [R] Protection stack overflow
In-Reply-To: <Pine.LNX.4.64.0608150951560.18980@gannet.stats.ox.ac.uk>
References: <44E18765.3000603@adam.com.au>
	<Pine.LNX.4.64.0608150951560.18980@gannet.stats.ox.ac.uk>
Message-ID: <44E717E8.5030208@adam.com.au>

Thanks for your reply, Prof Ripley.

I tried the --max-ppsize option and now ROctave (R embedded in Octave)
no longer produces the protection stack overflow error I was seeing with
the default.

Specifically, in the file RinOctave.cc, in the function initialize(), I
put in the --max-ppsize option as follows:

        const char * const DefaultArgs[] =
{"ROctave","--max-ppsize=100000"};

where 100000 is the maximum I found I can use with R 2.1.1.

Cheers,
Paul.


Prof Brian Ripley wrote:

>R has a command-line option to set the ppstack size,
>
> --max-ppsize=N        Set max size of protect stack to N
>
>Looks like you need to supply this (and it can be done with embedded R)
>if the problem persists with current R.
>
>  
>


From eugenedalt at yahoo.com  Sat Aug 19 16:01:39 2006
From: eugenedalt at yahoo.com (eugene dalt)
Date: Sat, 19 Aug 2006 07:01:39 -0700 (PDT)
Subject: [R] lapply?
In-Reply-To: <7.0.1.0.2.20060818134236.033499d8@earthlink.net>
Message-ID: <20060819140139.90116.qmail@web30004.mail.mud.yahoo.com>

Hi folks,

I would like to know if the following loop can be
rewritten with lapply


list1 <- as.list(0)

testv <- c(4,6,7,8)

for( i in 1:4){

list1[[i]] <- rep(5,testv[i])

}

Thanks in Advance. - Eugene


From Charles.Annis at StatisticalEngineering.com  Sat Aug 19 16:08:49 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sat, 19 Aug 2006 10:08:49 -0400
Subject: [R] string-to-number  SUMMARY
In-Reply-To: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
Message-ID: <067001c6c398$fe4ef2f0$6600a8c0@DD4XFW31>

Much gratitude to Professor Ripley, Peter Dalgaard, Marc Schwartz, and Roger
Bivand. 
__________________

Roger Bivand wrote that ... strsplit() returns a list - one list component
for each repeated.measures.columns element. Just pick off the one you want
with
[[]]:
as.numeric(strsplit(repeated.measures.columns, split = ",")[[1]])

which had stumped me, since that syntax fails without the [[1]]
specification.
__________________
Peter Dalgaard, who also suggested the [[1]] specification, pointed out that

scan(textConnection(x), sep=",")

will work, although that leaves you with a dangling open connection.
__________________
Marc Schwartz advised to ...
Use unlist() to take the output of strsplit() and convert it to a vector,
before coercing to numeric.

as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
____________________________________
Brian D. Ripley suggested that the following looks competitive, and is quite
a bit more general (e.g. allows spaces, works with complex numbers)
 
eval(parse(text=paste("c(", repeated.measures.columns, ")")))

and Marc Schwartz showed that Professor Ripley's suggestion is much faster
than the competition with some system.time trials.
____________________________________

Many thanks to all.
 

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charles Annis, P.E.
Sent: Saturday, August 19, 2006 7:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] string-to-number

Greetings, Amigos:

I have been trying without success to convert a character string,
> repeated.measures.columns
[1] "3,6,10"

into c(3,6,10) for subsequent use.

as.numeric(repeated.measures.columns) doesn't work (likely because of the
commas)
[1] NA
Warning message:
NAs introduced by coercion

I've tried many things including 
strsplit(repeated.measures.columns, split = ",")

which produces a list with only one element, viz:
[[1]]
[1] "3"  "6"  "10"

as.numeric() doesn't like that either.

Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
this WAY harder than it is.

Would some kind soul please instruct me (and perhaps subsequent searchers)
how to convert the elements of a string into numbers?

Thank you.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Sat Aug 19 16:23:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Aug 2006 10:23:35 -0400
Subject: [R] string-to-number SUMMARY
In-Reply-To: <067001c6c398$fe4ef2f0$6600a8c0@DD4XFW31>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
	<067001c6c398$fe4ef2f0$6600a8c0@DD4XFW31>
Message-ID: <971536df0608190723m52087456uf81d6ccd353952b7@mail.gmail.com>

On 8/19/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Much gratitude to Professor Ripley, Peter Dalgaard, Marc Schwartz, and Roger
> Bivand.
> __________________
>
> Roger Bivand wrote that ... strsplit() returns a list - one list component
> for each repeated.measures.columns element. Just pick off the one you want
> with
> [[]]:
> as.numeric(strsplit(repeated.measures.columns, split = ",")[[1]])
>
> which had stumped me, since that syntax fails without the [[1]]
> specification.
> __________________
> Peter Dalgaard, who also suggested the [[1]] specification, pointed out that
>
> scan(textConnection(x), sep=",")
>
> will work, although that leaves you with a dangling open connection.

You do this:

scan(textConnection(x), sep = ",")
closeAllConnections()

Now the following shows that none are open:

showConnections()

You could alternately explicitly close it:

scan(con <- textConnection(x), sep = ",")
close(con)

> __________________
> Marc Schwartz advised to ...
> Use unlist() to take the output of strsplit() and convert it to a vector,
> before coercing to numeric.
>
> as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
> ____________________________________
> Brian D. Ripley suggested that the following looks competitive, and is quite
> a bit more general (e.g. allows spaces, works with complex numbers)
>
> eval(parse(text=paste("c(", repeated.measures.columns, ")")))
>
> and Marc Schwartz showed that Professor Ripley's suggestion is much faster
> than the competition with some system.time trials.
> ____________________________________
>
> Many thanks to all.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charles Annis, P.E.
> Sent: Saturday, August 19, 2006 7:59 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] string-to-number
>
> Greetings, Amigos:
>
> I have been trying without success to convert a character string,
> > repeated.measures.columns
> [1] "3,6,10"
>
> into c(3,6,10) for subsequent use.
>
> as.numeric(repeated.measures.columns) doesn't work (likely because of the
> commas)
> [1] NA
> Warning message:
> NAs introduced by coercion
>
> I've tried many things including
> strsplit(repeated.measures.columns, split = ",")
>
> which produces a list with only one element, viz:
> [[1]]
> [1] "3"  "6"  "10"
>
> as.numeric() doesn't like that either.
>
> Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> this WAY harder than it is.
>
> Would some kind soul please instruct me (and perhaps subsequent searchers)
> how to convert the elements of a string into numbers?
>
> Thank you.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax: 614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Sat Aug 19 16:31:25 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 19 Aug 2006 16:31:25 +0200
Subject: [R] lapply?
In-Reply-To: <20060819140139.90116.qmail@web30004.mail.mud.yahoo.com>
References: <20060819140139.90116.qmail@web30004.mail.mud.yahoo.com>
Message-ID: <44E720BD.1070309@statistik.uni-dortmund.de>



eugene dalt wrote:
> Hi folks,
> 
> I would like to know if the following loop can be
> rewritten with lapply
> 
> 
> list1 <- as.list(0)
> 
> testv <- c(4,6,7,8)
> 
> for( i in 1:4){
> 
> list1[[i]] <- rep(5,testv[i])
> 
> }


Please do not cross-post!

lapply(testv, rep, x = 5)

Uwe Ligges



> Thanks in Advance. - Eugene
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Sat Aug 19 17:05:10 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Aug 2006 16:05:10 +0100 (BST)
Subject: [R] problem with Rcmd check and fortran95, makefile
In-Reply-To: <000801c6c38b$36e49ba0$0200a8c0@notebookdp>
References: <000801c6c38b$36e49ba0$0200a8c0@notebookdp>
Message-ID: <Pine.LNX.4.64.0608191555271.21429@gannet.stats.ox.ac.uk>

First, this is off-topic for R-help (see the posting guide, including the 
non-use of HTML, please).

Second, the appropriate list needs to see exactly what the output was when 
you did this. Remember R CMD SHLIB is a *make* facility, and quite 
possibly the objects were not remade after changes.

And yes, R CMD INSTALL does more than your commands -- as you have the 
sources you can take a look, or even see what the output says.
Here is a simple F95 example (using gcc pre-4.2.0)

---------- Making package testf95 ------------
  adding build stamp to DESCRIPTION
  making DLL ...
"c:/MinGW/bin/gfortran" -O3  -c cos90.f95 -o cos90.o
windres --include-dir d:/R/svn/trunk/include  -i testf95_res.rc -o 
testf95_res.o
"c:/MinGW/bin/gfortran" -shared -s  -o testf95.dll testf95.def cos90.o 
testf95_res.o  -Ld:/R/svn/trunk/bin   -lR

Note the extras, including a .def file.

If you use R 2.3.1 patched or R-devel you just don't need a Makefile and 
you can use standard Fortran 9X.



On Sat, 19 Aug 2006, data-ploner Meinhard Ploner wrote:

> Hi all,
> 
> I have Win XP and R 2.3.1 on my notebook. I would like to write a package which includes some Fortran 95 code. Interestingly, if  I compile and link the simple file test90.f90 directly with
> 
> g95 -c test90.f90
> g95 -shared -o test90.dll test90.o
> 
> then PE Viewer ( a dll viewer) shows me the right functions in the export table, hence I can use the dll in R. But as it should become part of a package I wrote the simple src/Makefile
> 
> F95=g95
> prog: test90.f90
>  $(F95) -shared -o test90.dll test90.o
> test90.f90: test90.f90
>  $(F95) -c test90.f90
> 
> which looks totally equal to the 2 commands above. If I run now 
> Rcmd check --no-latex test90
> Rcmd install test90
> 
> then test90.dll is made but the export table is empty and therefore in R the functions cannot be loaded.
> Any idea? Can it be that Rcmd gives further flags to the compiler/linker?
> 
> Any hints appreciated
> Meinhard Ploner
> 
> South Tyrol (Italy)
> 
> 
> PS The fortran file is simply:
> 
> SUBROUTINE BLABLA(A)
> !DEC$ ATTRIBUTES DLLEXPORT :: blabla    ### without this line the prob is the same :-(
> IMPLICIT DOUBLE PRECISION (A-H,O-Z)
> A=A+1
> RETURN
> END SUBROUTINE BLABLA

You could use --export-all-symbols


> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sat Aug 19 17:16:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Aug 2006 16:16:00 +0100 (BST)
Subject: [R] need to find (and distinguish types of) carriage returns in
 a file that is scanned using scan
In-Reply-To: <D535A03654352647924346AB905842AA01152B59@icex4.ic.ac.uk>
References: <D535A03654352647924346AB905842AA01152B59@icex4.ic.ac.uk>
Message-ID: <Pine.LNX.4.64.0608191607330.21429@gannet.stats.ox.ac.uk>

On Sat, 19 Aug 2006, Quicke, Donald L J wrote:

> Hope this is not too trivial
> I am reading a large file using scan. 

Why scan?

> In one part of this file there is a chunk of text within which i need to 
> know the positions of line breaks. But scan seems only

only what?

> An example of the file is:
> "
> a 0 1 0
> bftt 020
> cftt T 1 R
> 
> a 0 1 2 1 2
> b 0 1 2 2 2
> c 0 10 00 
> "
> 
> so precisely i need in the scanned file in R to know where each carriage 
> return is in the file so that i can then identify the text strings (i.e. 
> a, bftt, cftt, a, b, c ) that immediately follow the carriage return

Sounds like a job for readLines.

> On a subsidiary matter, it would be very helpful if i could distinguish 
> between Unix, Dos, and Mac carriage returns in the data file

AFAIK there is only type of carriage return character (ASCII code Ctrl-M).  
If you mean between CRLF, LF and perhaps CR line endings, you need to read 
the files as raw bytes since R's text mode regards all three as equally a 
line ending.  But that can perfectly well be done using binary-mode 
connections.

> 
> thanks
> 
> i should note also, that the input file contains much other stuff and is 
> not in the form of a table that can be read using read.table or other 
> read version. Nor do i know beforehand how many elements there are in 
> each line

Sounds like a job for connections ...

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do as we ask.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mr.blacksheep at gmail.com  Sat Aug 19 18:25:50 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Sat, 19 Aug 2006 10:25:50 -0600
Subject: [R] string-to-number
In-Reply-To: <1155992326.4308.15.camel@localhost.localdomain>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
	<1155989485.4308.7.camel@localhost.localdomain>
	<Pine.LNX.4.64.0608191322510.12049@gannet.stats.ox.ac.uk>
	<1155992326.4308.15.camel@localhost.localdomain>
Message-ID: <46a360560608190925u432b33a8rcdda2c12276f905a@mail.gmail.com>

Wow.  New respect for parse/eval.

Do you think this is a special case of a more general principle?  I
suppose the cost is memory, but from time to time a speedup like this
would be very beneficial.

Any hints about how R programmers could recognize such cases would, I
am sure, be of value to the list in general.

Many thanks for your efforts, Marc!

Regards,

Mike

On 8/19/06, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On Sat, 2006-08-19 at 13:30 +0100, Prof Brian Ripley wrote:
> > On Sat, 19 Aug 2006, Marc Schwartz wrote:
> >
> > > On Sat, 2006-08-19 at 07:58 -0400, Charles Annis, P.E. wrote:
> > > > Greetings, Amigos:
> > > >
> > > > I have been trying without success to convert a character string,
> > > > > repeated.measures.columns
> > > > [1] "3,6,10"
> > > >
> > > > into c(3,6,10) for subsequent use.
> > > >
> > > > as.numeric(repeated.measures.columns) doesn't work (likely because of the
> > > > commas)
> > > > [1] NA
> > > > Warning message:
> > > > NAs introduced by coercion
> > > >
> > > > I've tried many things including
> > > > strsplit(repeated.measures.columns, split = ",")
> > > >
> > > > which produces a list with only one element, viz:
> > > > [[1]]
> > > > [1] "3"  "6"  "10"
> > > >
> > > > as.numeric() doesn't like that either.
> > > >
> > > > Clearly: 1) I cannot be the first person to attempt this, and 2) I've made
> > > > this WAY harder than it is.
> > > >
> > > > Would some kind soul please instruct me (and perhaps subsequent searchers)
> > > > how to convert the elements of a string into numbers?
> > > >
> > > > Thank you.
> > >
> > > One more step:
> > >
> > > > as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
> > > [1]  3  6 10
> > >
> > > Use unlist() to take the output of strsplit() and convert it to a
> > > vector, before coercing to numeric.
> >
> > Or, more simply, use [[1]] as in
> >
> > as.numeric(strsplit(repeated.measures.columns, ",")[[1]])
> >
> > Also,
> >
> > eval(parse(text=paste("c(", repeated.measures.columns, ")")))
> >
> > looks competitive, and is quite a bit more general (e.g. allows spaces,
> > works with complex numbers), or you can use scan() from an anonymous file
> > or a textConnection.
>
> I would say more than competitive:
>
>   repeated.measures.columns <- paste(1:100000, collapse = ",")
>
> > str(repeated.measures.columns)
>  chr
> "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,4"| __truncated__
>
>
> > system.time(res1 <-
> as.numeric(unlist(strsplit(repeated.measures.columns, ","))))
> [1] 24.238  0.192 26.200  0.000  0.000
>
> > system.time(res2 <- as.numeric(strsplit(repeated.measures.columns,
> ",")[[1]]))
> [1] 24.313  0.196 26.471  0.000  0.000
>
> > system.time(res3 <- eval(parse(text=paste("c(",
> repeated.measures.columns, ")"))))
> [1] 0.328 0.004 0.395 0.000 0.000
>
>
> > str(res1)
>  num [1:100000] 1 2 3 4 5 6 7 8 9 10 ...
>
> > str(res2)
>  num [1:100000] 1 2 3 4 5 6 7 8 9 10 ...
>
> > str(res3)
>  num [1:100000] 1 2 3 4 5 6 7 8 9 10 ...
>
>
> > all(res1 == res2)
> [1] TRUE
>
> > all(res1 == res3)
> [1] TRUE
>
>
> Best regards,
>
> Marc
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From hellik at web.de  Sat Aug 19 20:41:06 2006
From: hellik at web.de (Helmut Kudrnovsky)
Date: Sat, 19 Aug 2006 20:41:06 +0200
Subject: [R] centroid of manually given groups in a cca-plot
Message-ID: <667136570@web.de>

dear R-friends,

i have a set of vegetation and environmental data:

	veg <- read.table("http://www.alectoria.at/dl/rmalliveg.csv", header=TRUE, sep=";")
	env <- read.table("http://www.alectoria.at/dl/rmallienv.csv", header=TRUE, sep=";")

with the library "vegan" i did a constrained correspondence analysis(cca):

	rmcca <- cca(veg,env)

and with ordihull from "vegan" it is possible to draw a line or polygon around manually
given group(s) - (groups = Factor giving the groups for which the graphical item is drawn) - in the plot of the cca.

	rmlab <- read.table("http://www.alectoria.at/dl/rmlabel.csv")
	attach(rmlab)
	ordihull(rmcca, V1, display="sites")

my question in a (more general) way: is it may be possible to get the centroid-coordinates from such
manually given groups of a cca or is it may be possible to draw the centroid of such group(s)?

with greetings from austria
helli

> version
               _                         
platform       i386-pc-mingw32           
arch           i386                      
os             mingw32                   
system         i386, mingw32             
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)


From ggrothendieck at gmail.com  Sat Aug 19 21:00:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Aug 2006 15:00:14 -0400
Subject: [R] Query: how to modify the plot of acf
In-Reply-To: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>
References: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>
Message-ID: <971536df0608191200h368cdbe9y627b2d4298fd62f0@mail.gmail.com>

Try this.  It uses Matthias' trick for getting rid of lag 1.
The it defines a new local plot.acf
 and then
defines a new local plot.acf which


# test data
set.seed(1)
x <- rnorm(1000)

# run acf
x.acf <- acf(x)

# remove lag 0 -- see Matthias' post
x.acf$acf[1] <- NA

# plot with custom x axis
plot(x.acf, xaxt = "n")
axis(1, seq(12, length(x.acf$acf), 12))





On 8/18/06, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
> I need to modify the graph of the autocorrelation. I tried to do it through plot.acf but with no success.
>
> 1. I would like to get rid of the lag zero
> 2. I would like to have numbers on the x-axis only at lags 12, 24, 36, 48, 60, ...
>
> Could anybody help me in this?
>
> Any help will be appreciated
> Thank you for your attention
> Stefano
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Daniel_J_Celta at fpl.com  Sat Aug 19 21:10:05 2006
From: Daniel_J_Celta at fpl.com (Daniel_J_Celta at fpl.com)
Date: Sat, 19 Aug 2006 15:10:05 -0400
Subject: [R] [S] lapply?
Message-ID: <OFCFDA9B08.179474D2-ON852571CF.00690D68-852571CF.00694C78@fpl.com>

Eugene,
Try

> lapply(c(4,6,7,8), function(x) rep(5, x))

I think this would do what you are trying to do.


Daniel J Celta
Project Valuation - Special Valuations
561 691 7653


                                                                                                                                              
                      "eugene dalt"                                                                                                           
                      <eugenedalt at yahoo.com>           To:       s-news at lists.biostat.wustl.edu, r-help at stat.math.ethz.ch                     
                      Sent by:                         cc:                                                                                    
                      s-news-owner at lists.biosta        Subject:  [S] lapply?                                                                  
                      t.wustl.edu                                                                                                             
                                                                                                                                              
                                                                                                                                              
                      08/19/2006 10:01 AM                                                                                                     
                                                                                                                                              
                                                                                                                                              




Hi folks,

I would like to know if the following loop can be
rewritten with lapply


list1 <- as.list(0)

testv <- c(4,6,7,8)

for( i in 1:4){

list1[[i]] <- rep(5,testv[i])

}

Thanks in Advance. - Eugene

__________________________________________________
Do You Yahoo!?
Tired of spam?  Yahoo! Mail has the best spam protection around
http://mail.yahoo.com
--------------------------------------------------------------------
This message was distributed by s-news at lists.biostat.wustl.edu.  To
...(s-news.. clipped)...


From attenka at utu.fi  Sat Aug 19 21:38:27 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Sat, 19 Aug 2006 22:38:27 +0300
Subject: [R] A matrix problem
Message-ID: <fa0ddbfb177d.44e792e3@utu.fi>

Hi,

I have a matrix with two columns. The first column means "indexes", the second one contents of those indexes. If I have a MATRIX like this,

> MATRIX
     [,1] [,2]
[1,]    1    3
[2,]    5    1
[3,]    2    1
[4,]    1    5

I'd like to get as a result vector the sums of these indexes, something like this:

> c(8,1,0,0,1)

How to do this?

I did solved it this way, but is there some more elegant way:

RESULTVECTOR=c();
RESULTMATRIX=c();
INDEXES=as.integer(names(table(TRANSP_TABLE[,1])));

for(i in INDEXES)
{
	RESULTVECTOR=c(i,sum(MATRIX[,2][MATRIX[,1]==i]))
	RESULTMATRIX=rbind(RESULTMATRIX,RESULTVECTOR)
}
row.names(RESULTMATRIX)<-INDEXES;
RESULTMATRIX=RESULTMATRIX[,2];

> RESULTMATRIX
1 2 5 
8 1 1 


Atte Tenkanen
University of Turku, Finland


From jholtman at gmail.com  Sat Aug 19 22:00:12 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 19 Aug 2006 16:00:12 -0400
Subject: [R] A matrix problem
In-Reply-To: <fa0ddbfb177d.44e792e3@utu.fi>
References: <fa0ddbfb177d.44e792e3@utu.fi>
Message-ID: <644e1f320608191300k3ee70fdr180ba2bc64e700e6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060819/c438c423/attachment.pl 

From rmh at temple.edu  Sat Aug 19 22:10:21 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sat, 19 Aug 2006 16:10:21 -0400 (EDT)
Subject: [R] A matrix problem
Message-ID: <20060819161021.BGH30353@po-d.temple.edu>

> x <- cbind(index=c(1,5,2,1), contents=c(3,1,1,5))
> x
     index contents
[1,]     1        3
[2,]     5        1
[3,]     2        1
[4,]     1        5

## use tapply to get the values you want
> z0 <- tapply(x[,"contents"], x[,"index"], sum)  ## read ?tapply
> z0
1 2 5 
8 1 1 

## more work is needed to get them into the structure you want
> r <- range(x[,"index"])
> r
[1] 1 5
> nn <- seq(r[1], r[2])
> nn
[1] 1 2 3 4 5
> z <- nn*0
> z
[1] 0 0 0 0 0
> names(z) <- nn
> z
1 2 3 4 5 
0 0 0 0 0 
> z[names(z0)] <- z0  ## read about subscripting  ?"["
> z
1 2 3 4 5 
8 1 0 0 1 
> 


## R is a matrix and vector language.  Loops are rarely needed.
## Read "An Introduction to R".
## It is clickable from the Help menu in the Windows RGui Console.
## It is available in R-2.3.1/doc/manual/R-intro.pdf on all platforms.



This is essentially the same as jim holtman's answer.  I did some extra work
to get nice names on the result vector.


From attenka at utu.fi  Sat Aug 19 23:44:35 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 20 Aug 2006 00:44:35 +0300
Subject: [R] A matrix problem
In-Reply-To: <20060819161021.BGH30353@po-d.temple.edu>
References: <20060819161021.BGH30353@po-d.temple.edu>
Message-ID: <f704f1713389.44e7b073@utu.fi>

Thanks for both, tapply seems to be a fast and delicate solution. I had appr. 150 000 rows and it took few seconds to get the result. Till now I have been too tied by for-loops. I will make acquaintance with "An Introduction to R".

Atte

----- Original Message -----
From: "Richard M. Heiberger" <rmh at temple.edu>
Date: Saturday, August 19, 2006 11:10 pm
Subject: Re: [R] A matrix problem

> > x <- cbind(index=c(1,5,2,1), contents=c(3,1,1,5))
> > x
>     index contents
> [1,]     1        3
> [2,]     5        1
> [3,]     2        1
> [4,]     1        5
> 
> ## use tapply to get the values you want
> > z0 <- tapply(x[,"contents"], x[,"index"], sum)  ## read ?tapply
> > z0
> 1 2 5 
> 8 1 1 
> 
> ## more work is needed to get them into the structure you want
> > r <- range(x[,"index"])
> > r
> [1] 1 5
> > nn <- seq(r[1], r[2])
> > nn
> [1] 1 2 3 4 5
> > z <- nn*0
> > z
> [1] 0 0 0 0 0
> > names(z) <- nn
> > z
> 1 2 3 4 5 
> 0 0 0 0 0 
> > z[names(z0)] <- z0  ## read about subscripting  ?"["
> > z
> 1 2 3 4 5 
> 8 1 0 0 1 
> > 
> 
> 
> ## R is a matrix and vector language.  Loops are rarely needed.
> ## Read "An Introduction to R".
> ## It is clickable from the Help menu in the Windows RGui Console.
> ## It is available in R-2.3.1/doc/manual/R-intro.pdf on all platforms.
> 
> 
> 
> This is essentially the same as jim holtman's answer.  I did some 
> extra work
> to get nice names on the result vector.
>


From jz7 at duke.edu  Sun Aug 20 01:58:39 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Sat, 19 Aug 2006 19:58:39 -0400 (EDT)
Subject: [R] question about cbind()
Message-ID: <Pine.GSO.4.58.0608191951070.6653@godzilla.acpub.duke.edu>

Dear all,

I have a question about how to get a matrix by combining a large number of
columns from a data file. Suppose I read a file which have 1000 columns
by:

test = read.table("dat.txt", header=F)

I know I could use "cbind()". It's easy to do when the number of columns
is small (i.e. cbind(test$V1, test$V2)). But how about build a matrix "X"
by combine the first 500 columns. Is there an easy way to write the
expression?

Thanks so much!

Jeny


From ggrothendieck at gmail.com  Sun Aug 20 02:05:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 19 Aug 2006 20:05:49 -0400
Subject: [R] question about cbind()
In-Reply-To: <Pine.GSO.4.58.0608191951070.6653@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0608191951070.6653@godzilla.acpub.duke.edu>
Message-ID: <971536df0608191705v31e75008t1b1677e1a71ce38b@mail.gmail.com>

First 4 colums of builtin data frame anscombe as a matrix:

  as.matrix(anscombe[1:4])


On 8/19/06, jz7 at duke.edu <jz7 at duke.edu> wrote:
> Dear all,
>
> I have a question about how to get a matrix by combining a large number of
> columns from a data file. Suppose I read a file which have 1000 columns
> by:
>
> test = read.table("dat.txt", header=F)
>
> I know I could use "cbind()". It's easy to do when the number of columns
> is small (i.e. cbind(test$V1, test$V2)). But how about build a matrix "X"
> by combine the first 500 columns. Is there an easy way to write the
> expression?
>
> Thanks so much!
>
> Jeny
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From thomas.harte at yahoo.com  Sun Aug 20 02:19:41 2006
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Sat, 19 Aug 2006 17:19:41 -0700 (PDT)
Subject: [R] issues with Sweave and inclusion of graphics in a document
Message-ID: <20060820001941.2199.qmail@web30209.mail.mud.yahoo.com>

the problem is a little hard to explain; the .Rnw files (below)
probably do a better job, but here goes ...

Sweave doesn't like it when i size a graphical device in a code
chunk using either, e.g.:

	windows(width=20, height=5)

in Windows, or, e.g.

	x11(width=20, height=5)

under X, when i then plot something in said device and try to 
include this graphical output in the resulting document.

Sweave does not object to my writing code chunks in the above
manner, so long as i do not wish to include the code in a LaTeX 
figure environment.

oftentimes i want to do precisely what Sweave doesn't appear
to allow. for example, with time-series data, i want to see a 
wide window on the screen as i code, and then i want to include 
the graphical output in my document the way that i fine tuned 
it on the screen. i don't want to write two pieces of code:
the first, to view output on the sceen; the second, to save
the output to a .pdf file for inclusion in the document.

some example .Rnw files should illustrate my plight.
suggestions on a workaround (i.e. how to do what i describe in 
linux/X) welcome.


% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-windows.Rnw
\documentclass[a4paper]{article}

\begin{document}

\noindent This is an example of what I can do on Windows. Unhappily, I seem to be
able to squeeze marginally more out of \texttt{Sweave} \emph{chez\/} Bill Gates
than I can under Linux. Ho, hum.

<<echo=false,results=hide>>=
	# create a simple AR process:
	make.ar.1<- function(alpha=1,n=300) {
		Z<- rnorm(n); 
		Y<- numeric(n); 
		Y[1]<- Z[1]; 
		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
		return(Y)
	}
@

<<label=ar.1>>=
	# a long AR process is best viewed in a wide window:
	windows(width=20, height=5)
	sp<- make.ar.1(alpha=.5, n=800)
	plot(sp, type="l", col="blue")
	# WISIWIS: What I See Is What I Save ;)
	savePlot("ar",type="pdf")
@
\begin{figure}
\begin{center}
% 	imporantly, by saving the plot i have direct control over graphics in LaTeX, 
% 	and i can fine-tune the the graphics placement as much as i want:
	\includegraphics[width=14.5cm]{./ar.pdf}
\caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
is best viewed in a wide window.}
\end{center}
\end{figure}


\noindent Had I tried to do the following, \texttt{Sweave} would have blown up!
\begin{verbatim}
	<<label=ar.1>>=
		windows(width=20, height=5) 	# <- this is the offending command:
		sp<- make.ar.1(alpha=.5, n=800)
		plot(sp, type="l", col="blue")
	@
	\begin{figure}
	\begin{center}
	<<fig=true>>=
	<<ar.1>>
	@
	\caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
	is best viewed in a wide window.}
	\end{center}
	\end{figure}
\end{verbatim}


\noindent The take-home message is that \texttt{savePlot} saves the day under Windows.
As far as I know, there is no equivalent under Linux, or rather, under X.

In Windows, then,
\begin{itemize}
\item I can plot the way I want on the screen;
\item I can save that plot to a file without writing any other code;
\item I can include the saved plot in my \LaTeX\ figure, allowing me to 
	fine-tune with the \verb@\includegraphics{}@ command.
\end{itemize}
Strike one for the Evil Empire.

\end{document}
% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-windows.Rnw



% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-linux.Rnw
\documentclass[a4paper]{article}

\begin{document}

\noindent This is an example of the hapless state of my \texttt{Sweave}ing under Linux. 

<<echo=false,results=hide>>=
	# create a simple AR process:
	make.ar.1<- function(alpha=1,n=300) {
		Z<- rnorm(n); 
		Y<- numeric(n); 
		Y[1]<- Z[1]; 
		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
		return(Y)
	}
@

\noindent Because of the \verb at x11(width=20, height=5)@ command, 
I can't embed the graphical output that the following piece of code 
produces in my document, although I can view the results on screen:
<<label=first.ar.1>>=
	# a long AR process is best viewed in a wide window:
	x11(width=20, height=5)
	sp<- make.ar.1(alpha=.5, n=800)
	plot(sp, type="l", col="blue")
	# no savePlot ... can't seem to do anything with this plot
	# if i try to include this code in a figure environment then
	# Sweave blows up
	# so i have to stop here :(
@

\noindent Instead, I have to do something like the following, which has the unfortunate 
side effects of disallowing me from seeing the graphical output on the screen, and,
probably
more importantly, of duplicating the above code:
<<label=ar.1,echo=true>>=
	sp<- make.ar.1(alpha=.5, n=800)
	pdf("ar.pdf", width=20, height=5)
	plot(sp, type="l", col="blue")
	dev.off()
@
\begin{figure}
\begin{center}
% 	at least i still retain direct control over graphics in LaTeX; i can fine-tune the 
% 	the graphics placement as much as i want:
	\includegraphics[width=14.5cm]{./ar.pdf}
\caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
is best viewed in a wide window.}
\end{center}
\end{figure}

Under X, then,
\begin{itemize}
\item I have to use a device such as \texttt{pdf} and I lose the ability to first 
	see the output on screen;
\item I can still save that plot to a file without writing any other code;
\item I can still include the saved plot in my \LaTeX\ figure, allowing me to 
	fine-tune with the \verb@\includegraphics{}@ command.
\end{itemize}

\end{document}
% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-linux.Rnw


From ripley at stats.ox.ac.uk  Sun Aug 20 08:03:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Aug 2006 07:03:56 +0100 (BST)
Subject: [R] question about cbind()
In-Reply-To: <Pine.GSO.4.58.0608191951070.6653@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0608191951070.6653@godzilla.acpub.duke.edu>
Message-ID: <Pine.LNX.4.64.0608200701220.6643@gannet.stats.ox.ac.uk>

On Sat, 19 Aug 2006, jz7 at duke.edu wrote:

> Dear all,
> 
> I have a question about how to get a matrix by combining a large number of
> columns from a data file. Suppose I read a file which have 1000 columns
> by:
> 
> test = read.table("dat.txt", header=F)
> 
> I know I could use "cbind()". It's easy to do when the number of columns
> is small (i.e. cbind(test$V1, test$V2)). But how about build a matrix "X"
> by combine the first 500 columns. Is there an easy way to write the
> expression?

as.matrix(test[1:500])

Or read the data as a matrix in the first place, using scan (as 
recommended on the help page for read.table *and* the relevant manual).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sun Aug 20 08:10:52 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Aug 2006 07:10:52 +0100 (BST)
Subject: [R] issues with Sweave and inclusion of graphics in a document
In-Reply-To: <20060820001941.2199.qmail@web30209.mail.mud.yahoo.com>
References: <20060820001941.2199.qmail@web30209.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608200706380.6643@gannet.stats.ox.ac.uk>

savePlot is just an internal version of dev.copy, part of the support for 
the menus on the windows() graphics device.

It is described in `An Introduction to R' (the most basic R manual).


On Sat, 19 Aug 2006, Thomas Harte wrote:

> the problem is a little hard to explain; the .Rnw files (below)
> probably do a better job, but here goes ...
> 
> Sweave doesn't like it when i size a graphical device in a code
> chunk using either, e.g.:
> 
> 	windows(width=20, height=5)
> 
> in Windows, or, e.g.
> 
> 	x11(width=20, height=5)
> 
> under X, when i then plot something in said device and try to 
> include this graphical output in the resulting document.
> 
> Sweave does not object to my writing code chunks in the above
> manner, so long as i do not wish to include the code in a LaTeX 
> figure environment.
> 
> oftentimes i want to do precisely what Sweave doesn't appear
> to allow. for example, with time-series data, i want to see a 
> wide window on the screen as i code, and then i want to include 
> the graphical output in my document the way that i fine tuned 
> it on the screen. i don't want to write two pieces of code:
> the first, to view output on the sceen; the second, to save
> the output to a .pdf file for inclusion in the document.
> 
> some example .Rnw files should illustrate my plight.
> suggestions on a workaround (i.e. how to do what i describe in 
> linux/X) welcome.
> 
> 
> % >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-windows.Rnw
> \documentclass[a4paper]{article}
> 
> \begin{document}
> 
> \noindent This is an example of what I can do on Windows. Unhappily, I seem to be
> able to squeeze marginally more out of \texttt{Sweave} \emph{chez\/} Bill Gates
> than I can under Linux. Ho, hum.
> 
> <<echo=false,results=hide>>=
> 	# create a simple AR process:
> 	make.ar.1<- function(alpha=1,n=300) {
> 		Z<- rnorm(n); 
> 		Y<- numeric(n); 
> 		Y[1]<- Z[1]; 
> 		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
> 		return(Y)
> 	}
> @
> 
> <<label=ar.1>>=
> 	# a long AR process is best viewed in a wide window:
> 	windows(width=20, height=5)
> 	sp<- make.ar.1(alpha=.5, n=800)
> 	plot(sp, type="l", col="blue")
> 	# WISIWIS: What I See Is What I Save ;)
> 	savePlot("ar",type="pdf")
> @
> \begin{figure}
> \begin{center}
> % 	imporantly, by saving the plot i have direct control over graphics in LaTeX, 
> % 	and i can fine-tune the the graphics placement as much as i want:
> 	\includegraphics[width=14.5cm]{./ar.pdf}
> \caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
> is best viewed in a wide window.}
> \end{center}
> \end{figure}
> 
> 
> \noindent Had I tried to do the following, \texttt{Sweave} would have blown up!
> \begin{verbatim}
> 	<<label=ar.1>>=
> 		windows(width=20, height=5) 	# <- this is the offending command:
> 		sp<- make.ar.1(alpha=.5, n=800)
> 		plot(sp, type="l", col="blue")
> 	@
> 	\begin{figure}
> 	\begin{center}
> 	<<fig=true>>=
> 	<<ar.1>>
> 	@
> 	\caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
> 	is best viewed in a wide window.}
> 	\end{center}
> 	\end{figure}
> \end{verbatim}
> 
> 
> \noindent The take-home message is that \texttt{savePlot} saves the day under Windows.
> As far as I know, there is no equivalent under Linux, or rather, under X.
> 
> In Windows, then,
> \begin{itemize}
> \item I can plot the way I want on the screen;
> \item I can save that plot to a file without writing any other code;
> \item I can include the saved plot in my \LaTeX\ figure, allowing me to 
> 	fine-tune with the \verb@\includegraphics{}@ command.
> \end{itemize}
> Strike one for the Evil Empire.
> 
> \end{document}
> % <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-windows.Rnw
> 
> 
> 
> % >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-linux.Rnw
> \documentclass[a4paper]{article}
> 
> \begin{document}
> 
> \noindent This is an example of the hapless state of my \texttt{Sweave}ing under Linux. 
> 
> <<echo=false,results=hide>>=
> 	# create a simple AR process:
> 	make.ar.1<- function(alpha=1,n=300) {
> 		Z<- rnorm(n); 
> 		Y<- numeric(n); 
> 		Y[1]<- Z[1]; 
> 		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
> 		return(Y)
> 	}
> @
> 
> \noindent Because of the \verb at x11(width=20, height=5)@ command, 
> I can't embed the graphical output that the following piece of code 
> produces in my document, although I can view the results on screen:
> <<label=first.ar.1>>=
> 	# a long AR process is best viewed in a wide window:
> 	x11(width=20, height=5)
> 	sp<- make.ar.1(alpha=.5, n=800)
> 	plot(sp, type="l", col="blue")
> 	# no savePlot ... can't seem to do anything with this plot
> 	# if i try to include this code in a figure environment then
> 	# Sweave blows up
> 	# so i have to stop here :(
> @
> 
> \noindent Instead, I have to do something like the following, which has the unfortunate 
> side effects of disallowing me from seeing the graphical output on the screen, and,
> probably
> more importantly, of duplicating the above code:
> <<label=ar.1,echo=true>>=
> 	sp<- make.ar.1(alpha=.5, n=800)
> 	pdf("ar.pdf", width=20, height=5)
> 	plot(sp, type="l", col="blue")
> 	dev.off()
> @
> \begin{figure}
> \begin{center}
> % 	at least i still retain direct control over graphics in LaTeX; i can fine-tune the 
> % 	the graphics placement as much as i want:
> 	\includegraphics[width=14.5cm]{./ar.pdf}
> \caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
> is best viewed in a wide window.}
> \end{center}
> \end{figure}
> 
> Under X, then,
> \begin{itemize}
> \item I have to use a device such as \texttt{pdf} and I lose the ability to first 
> 	see the output on screen;
> \item I can still save that plot to a file without writing any other code;
> \item I can still include the saved plot in my \LaTeX\ figure, allowing me to 
> 	fine-tune with the \verb@\includegraphics{}@ command.
> \end{itemize}
> 
> \end{document}
> % <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-linux.Rnw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Sun Aug 20 08:20:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 19 Aug 2006 23:20:21 -0700
Subject: [R] fMultivar OLS - how to do dynamic regression?
In-Reply-To: <44A8B25381923D4F93B74B2676A50F6D0328E0CB@MAIL1.infores.com>
References: <44A8B25381923D4F93B74B2676A50F6D0328E0CB@MAIL1.infores.com>
Message-ID: <44E7FF25.8070507@pdf.com>

      The documentation for 'OLS' says that it is a wrapper for 'lm';  
if you type 'OLS' at a command prompt, you will see that it does little 
more than calling 'lm' and returning the output. 

      An example of 'dynamic regression' appears in the example section 
of the help page for 'arima'.  Other example can be found in Sect. 14.5 
of Venables and Ripley (2002) Modern Applied Statistics with S 
(Springer).  If you haven't already, I suggest you get a copy of this 
book and work through ch. 14, aided by 'ch14.R' in folder 
'~library\MASS\scripts' of your R installation directory.  It shows how 
to use both 'gls' in the 'nlme' package and 'arima' for this kind of 
problem. 

      If after this you would like further help from this listserve, 
please submit another post.  When you do, however, I encourage you to 
include commented, minimal, self-contained, reproducible code to help 
illustrate your question and something you've tried that did not seem 
satisfactory, as suggested in the posting guide 
"www.R-project.org/posting-guide.html".  I'm confident that people who 
include such examples with a carefully written question tend get more 
helpful replies sooner. 

      Hope this helps. 
      Spencer Graves

Kerpel, John wrote:
> Hi folks!
>
>  
>
> Does anybody know how to use the OLS function in fMultivar to do dynamic
> regression?  I've tried specifying lags in OLS using a data series
> created in fSeries and it doesn't seem to work.  I've done dynamic
> regression using dyn$lm and I was wondering how to accomplish the same
> thing using the OLS function from fMultivar.  Thanks!
>
>  
>
> John
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From epistat at gmail.com  Sun Aug 20 08:46:49 2006
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 20 Aug 2006 14:46:49 +0800
Subject: [R] how to the p-values or t-values from the lm's results
Message-ID: <2fc17e30608192346k3a5f5d6cu8d629430b33338e4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060820/7016782d/attachment.pl 

From seanpor at acm.org  Sun Aug 20 09:34:09 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Sun, 20 Aug 2006 07:34:09 +0000
Subject: [R] how to the p-values or t-values from the lm's results
In-Reply-To: <2fc17e30608192346k3a5f5d6cu8d629430b33338e4@mail.gmail.com>
References: <2fc17e30608192346k3a5f5d6cu8d629430b33338e4@mail.gmail.com>
Message-ID: <8ed68eed0608200034w31b9c8d1q4019519534c81bd7@mail.gmail.com>

Hi there Zhang,
While there might be a better way... an ugly but generic way of
accessing this type of information is to use str() and a little
experimentation... here is a little history() of what I did to find
it...

a
str(a)
str(logr)
a[[1]]
a[[2]]
a[[3]]
a[[4]]
a[[4]][[1]]
a[[4]][1,]
a[[4]][,4]

and hey presto... we have it... :-)

now if I actually understood what was going on here I'd probably be
faster... but it is pretty generic and you can almost always get at
those things using this technique... no doubt somebody with more
knowledge will explain why it works :-)

cheers,
Sean

On 20/08/06, zhijie zhang <epistat at gmail.com> wrote:
> Dear friends,
>   After running the lm() model, we can get summary resluts like the
> following:
> Coefficients:
>    Estimate  Std. Error  t value Pr(>|t|)
> x1  0.11562    0.10994   1.052   0.2957
> x2 -0.13879    0.09674  -1.435   0.1548
> x3  0.01051    0.09862   0.107   0.9153
> x4  0.14183    0.08471   1.674   0.0975 .
> x5  0.18995    0.10482   1.812   0.0732 .
> x6  0.24832    0.10059   2.469   0.0154 *
> x7 -0.04425    0.11008  -0.402   0.6886
> x8  0.05146    0.10290   0.500   0.6182
> -------------------------------------------------------------
> **the program maybe :
> data<-matrix(rnorm(900),ncol=9) #9variables,1dependent var,8independent
> data<-data.frame(data)
> names(data)<-c('y','x1','x2','x3','x4','x5','x6','x7','x8')
> logr<-lm(y~x1+x2+x3+x4+x5+x6+x7+x8-1,data)
> a<-summary(logr)
> ------------------------------------------------------------------------------------------------------------------------
> Could i extract the p-values or t-values from the a$Coefficients, i searched
> the attributes(a), but don't find the options,how to do that?
> Thanks very much!
>
> --
> Kind Regards,
> Zhi Jie,Zhang ,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From A.Robinson at ms.unimelb.edu.au  Sun Aug 20 10:17:34 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 20 Aug 2006 18:17:34 +1000
Subject: [R] Query: how to modify the plot of acf
In-Reply-To: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>
References: <163CA7BD55F0B84AAD787CE6192698D70C5958@GANDALF.regionemarche.intra>
Message-ID: <20060820081734.GY95817@ms.unimelb.edu.au>

Try constructing the acf plot using the traditional plot tools. Then
you can do what you like with it.  Eg if your model is called
model.lme, then something like this should work:


acf.resid <- ACF(model.lme, resType = "n")
my.lags <- acf.resid$lag > 0.5
plot(acf.resid$lag[my.lags], acf.resid$ACF[my.lags], 
     type="b", main="Residual Autocorrelation", 
     xlab="Lag", ylab="Correlation", axes=F)
stdv <- qnorm(1 - 0.01/2)/sqrt(attr(acf.resid, "n.used"))
lines(acf.resid$lag[my.lags], stdv[my.lags], col="darkgray")
lines(acf.resid$lag[my.lags], -stdv[my.lags], col="darkgray")
abline(0,0,col="gray")
box()
axis(1)
axis(2)


Modify the first axis to put ticks and numbers where you want them.
Double-check the image against the plot of the ACF object to be sure
that it lines up right.

Cheers

Andrew


On Fri, Aug 18, 2006 at 04:50:01PM +0200, Stefano Sofia wrote:
> I need to modify the graph of the autocorrelation. I tried to do it through plot.acf but with no success. 
> 
> 1. I would like to get rid of the lag zero
> 2. I would like to have numbers on the x-axis only at lags 12, 24, 36, 48, 60, ...
> 
> Could anybody help me in this?
> 
> Any help will be appreciated
> Thank you for your attention
> Stefano
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Aug 20 10:34:22 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 20 Aug 2006 10:34:22 +0200
Subject: [R] how to the p-values or t-values from the lm's results
In-Reply-To: <2fc17e30608192346k3a5f5d6cu8d629430b33338e4@mail.gmail.com>
References: <2fc17e30608192346k3a5f5d6cu8d629430b33338e4@mail.gmail.com>
Message-ID: <20060820103422.5gzwwlo0gfgo4g0o@webmail3.kuleuven.be>

try the following:

data <- data.frame(matrix(rnorm(900), ncol = 9))
names(data) <- c("y", paste("x", 1:8, sep = ""))
logr <- lm(y ~ . - 1, data)
a <- summary(logr)

coef(a)
coef(a)[, 3:4]
coef(a)[, "t value"]
coef(a)[, "Pr(>|t|)"]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting zhijie zhang <epistat at gmail.com>:

> Dear friends,
>   After running the lm() model, we can get summary resluts like the
> following:
> Coefficients:
>    Estimate  Std. Error  t value Pr(>|t|)
> x1  0.11562    0.10994   1.052   0.2957
> x2 -0.13879    0.09674  -1.435   0.1548
> x3  0.01051    0.09862   0.107   0.9153
> x4  0.14183    0.08471   1.674   0.0975 .
> x5  0.18995    0.10482   1.812   0.0732 .
> x6  0.24832    0.10059   2.469   0.0154 *
> x7 -0.04425    0.11008  -0.402   0.6886
> x8  0.05146    0.10290   0.500   0.6182
> -------------------------------------------------------------
> **the program maybe :
> data<-matrix(rnorm(900),ncol=9) #9variables,1dependent var,8independent
> data<-data.frame(data)
> names(data)<-c('y','x1','x2','x3','x4','x5','x6','x7','x8')
> logr<-lm(y~x1+x2+x3+x4+x5+x6+x7+x8-1,data)
> a<-summary(logr)
> ------------------------------------------------------------------------------------------------------------------------
> Could i extract the p-values or t-values from the a$Coefficients, i searched
> the attributes(a), but don't find the options,how to do that?
> Thanks very much!
>
> --
> Kind Regards,
> Zhi Jie,Zhang ,
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From maj at waikato.ac.nz  Sun Aug 20 12:49:11 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Sun, 20 Aug 2006 22:49:11 +1200
Subject: [R] unquoting
Message-ID: <44E83E27.6000903@waikato.ac.nz>

I would like a function to strip quotes off character strings. I should 
work like this:

 > A <- matrix(1:6, nrow = 2, ncol=3)
 > AF <- as.data.frame(A)
 > names(AF) <- c("First","Second","Third")
 > AF
   First Second Third
1     1      3     5
2     2      4     6
 > names(AF)[2]
[1] "Second"
 > attach(AF)
 > unquote(names(AF)[2])
[1] 3 4

Of course what I actually get is

Error: couldn't find function "unquote"

The reason that I want to do this is that I have a frame with a rather 
large number of variables and I would like to loop over the names and 
print out various descriptive summaries of the variable's distribution.

OK, OK, I could just go

 > AF[,2]
[1] 3 4

but once I thought of unquoting I have some sort of inner need to know 
how to do it!

Cheers,

Murray
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From ggrothendieck at gmail.com  Sun Aug 20 13:16:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 20 Aug 2006 07:16:46 -0400
Subject: [R] unquoting
In-Reply-To: <44E83E27.6000903@waikato.ac.nz>
References: <44E83E27.6000903@waikato.ac.nz>
Message-ID: <971536df0608200416r40465b54heb6ce4bcc37841cb@mail.gmail.com>

Try these

   get(names(AF)[2])
   AF["Second"] # this one different than the rest
   AF[["Second"]]
   AF[, "Second"]
   AF$Second


On 8/20/06, Murray Jorgensen <maj at waikato.ac.nz> wrote:
> I would like a function to strip quotes off character strings. I should
> work like this:
>
>  > A <- matrix(1:6, nrow = 2, ncol=3)
>  > AF <- as.data.frame(A)
>  > names(AF) <- c("First","Second","Third")
>  > AF
>   First Second Third
> 1     1      3     5
> 2     2      4     6
>  > names(AF)[2]
> [1] "Second"
>  > attach(AF)
>  > unquote(names(AF)[2])
> [1] 3 4
>
> Of course what I actually get is
>
> Error: couldn't find function "unquote"
>
> The reason that I want to do this is that I have a frame with a rather
> large number of variables and I would like to loop over the names and
> print out various descriptive summaries of the variable's distribution.
>
> OK, OK, I could just go
>
>  > AF[,2]
> [1] 3 4
>
> but once I thought of unquoting I have some sort of inner need to know
> how to do it!

Try:

sapply(names(AF), function(nm) mean(AF[nm]))

# same as previous but names a bit nicer

sapply(names(AF), function(nm) mean(AF[nm]), USE.NAMES = FALSE)

sapply(AF, mean)

colMeans(AF)

>
> Cheers,
>
> Murray
> --
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Sun Aug 20 13:24:24 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Aug 2006 13:24:24 +0200
Subject: [R] unquoting
In-Reply-To: <44E83E27.6000903@waikato.ac.nz>
References: <44E83E27.6000903@waikato.ac.nz>
Message-ID: <x2irknlkev.fsf@turmalin.kubism.ku.dk>

Murray Jorgensen <maj at waikato.ac.nz> writes:

> I would like a function to strip quotes off character strings. I should 
> work like this:
> 
>  > A <- matrix(1:6, nrow = 2, ncol=3)
>  > AF <- as.data.frame(A)
>  > names(AF) <- c("First","Second","Third")
>  > AF
>    First Second Third
> 1     1      3     5
> 2     2      4     6
>  > names(AF)[2]
> [1] "Second"
>  > attach(AF)
>  > unquote(names(AF)[2])
> [1] 3 4
> 
> Of course what I actually get is
> 
> Error: couldn't find function "unquote"
> 
> The reason that I want to do this is that I have a frame with a rather 
> large number of variables and I would like to loop over the names and 
> print out various descriptive summaries of the variable's distribution.
> 
> OK, OK, I could just go
> 
>  > AF[,2]
> [1] 3 4
> 
> but once I thought of unquoting I have some sort of inner need to know 
> how to do it!

Anything wrong with

AF[,names(AF)[2]]

or

AF[[names(AF)[2]]]

or for that matter

eval(as.name(names(AF)[2]), envir=AF)

??

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ripley at stats.ox.ac.uk  Sun Aug 20 13:32:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Aug 2006 12:32:11 +0100 (BST)
Subject: [R] unquoting
In-Reply-To: <44E83E27.6000903@waikato.ac.nz>
References: <44E83E27.6000903@waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0608201227340.2564@gannet.stats.ox.ac.uk>

?get

I really think this has nothing to do with `quoting', rather to do with 
evaluating variables from their names. At first I though you were looking 
for noquote(), which does unquote in the conventional sense.

> noquote(names(AF)[2])
[1] Second
> get(names(AF)[2])
[1] 3 4

On Sun, 20 Aug 2006, Murray Jorgensen wrote:

> I would like a function to strip quotes off character strings. I should 
> work like this:
> 
>  > A <- matrix(1:6, nrow = 2, ncol=3)
>  > AF <- as.data.frame(A)
>  > names(AF) <- c("First","Second","Third")
>  > AF
>    First Second Third
> 1     1      3     5
> 2     2      4     6
>  > names(AF)[2]
> [1] "Second"
>  > attach(AF)
>  > unquote(names(AF)[2])
> [1] 3 4
> 
> Of course what I actually get is
> 
> Error: couldn't find function "unquote"
> 
> The reason that I want to do this is that I have a frame with a rather 
> large number of variables and I would like to loop over the names and 
> print out various descriptive summaries of the variable's distribution.
> 
> OK, OK, I could just go
> 
>  > AF[,2]
> [1] 3 4
> 
> but once I thought of unquoting I have some sort of inner need to know 
> how to do it!
> 
> Cheers,
> 
> Murray
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From iuri at proxima.adm.br  Sun Aug 20 14:13:38 2006
From: iuri at proxima.adm.br (Iuri Gavronski)
Date: Sun, 20 Aug 2006 09:13:38 -0300
Subject: [R] Variance Components in R
In-Reply-To: <60ad85c90608180601i22ef0e71y8664d961e789a835@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
	<60ad85c90608180601i22ef0e71y8664d961e789a835@mail.gmail.com>
Message-ID: <60ad85c90608200513l1d530ad5gaa7457241e3314cf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060820/11af0abe/attachment.pl 

From AnupTyagi at yahoo.com  Sun Aug 20 14:37:30 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 20 Aug 2006 12:37:30 +0000 (UTC)
Subject: [R] Grid Points
Message-ID: <loom.20060820T142815-709@post.gmane.org>

How do I put grid points (not grid lines) as the base layer of an xyplot? 

Is there a way to vary the interval at which x and y grid points are placed?

Is it possible to start a graph so that Y axis begins at 500 and ends at 800? I
am only interested in focusing on the relative distance between the points whose
values are between 500 and 800, but not their relative distance from zero.

Is there a way in R to draw two graphs so that that "share" the same X axis, but
without the gap that mfrow() creates? The origin of the top graph should be
where the Y-axis of the bottom graph ends. It will be useful for what I am
trying to do to have the X axis of the top graph be invisible.

Anupam.


From iuri at ufrgs.br  Sun Aug 20 14:42:31 2006
From: iuri at ufrgs.br (Iuri Gavronski)
Date: Sun, 20 Aug 2006 09:42:31 -0300
Subject: [R] Variance Components in R
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
Message-ID: <60ad85c90608200542s3bdb6ce7hf1362281e480f5a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060820/27c5cc98/attachment.pl 

From AnupTyagi at yahoo.com  Sun Aug 20 14:54:37 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 20 Aug 2006 12:54:37 +0000 (UTC)
Subject: [R] split a y-axis to show data on different scales
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au> <44E0A172.2020907@pdf.com>
	<44902.129.206.90.2.1155635601.squirrel@mail.panix.com>
	<loom.20060819T142130-896@post.gmane.org>
	<f8e6ff050608190604h3a0e65a3g9ff754a40ae298a3@mail.gmail.com>
Message-ID: <loom.20060820T144206-922@post.gmane.org>

I think information can be enhanced by using different scaled graphs next to
each other. mfrow() created too much space, there may be no need to again draw
the x-axis. It can be very useful to have different scales of the same data
presented next to each other, in addition to the main graph. So I think the data
of the person who started this thread could be displayed using one graph will
all the data, and then a superimposed graph (sharing same x-axis) on any part of
the data to give an enhanced visual communication. Drawing grid lines with same
tick marks in both graphs can enhance this visual communication. This is like
"static zooming". Of course it is important to make sure that the change in
scale is evident, because it is needed for the interpretation of the
graph---using a grid with same tick marks can produce this effect visually. 

Anupam.


From iuri at proxima.adm.br  Sun Aug 20 15:21:13 2006
From: iuri at proxima.adm.br (Iuri Gavronski)
Date: Sun, 20 Aug 2006 10:21:13 -0300
Subject: [R] Variance Components in R
In-Reply-To: <60ad85c90608200542s3bdb6ce7hf1362281e480f5a6@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
	<60ad85c90608200542s3bdb6ce7hf1362281e480f5a6@mail.gmail.com>
Message-ID: <60ad85c90608200621n1b203dfegaa1452f728ada7f7@mail.gmail.com>

Harold,

I have tried the following syntax:

> fm <- lmer(RATING ~ CHAIN*SECTOR*RESP +(1|CHAIN*SECTOR*RESP), gt)
> summary(fm)
Linear mixed-effects model fit by REML
Formula: RATING ~ CHAIN * SECTOR * RESP + (1 | CHAIN * SECTOR * RESP)
   Data: gt
      AIC      BIC    logLik MLdeviance REMLdeviance
 2767.466 2807.717 -1374.733   2710.253     2749.466
Random effects:
 Groups                Name        Variance Std.Dev.
 CHAIN * SECTOR * RESP (Intercept) 5.7119   2.3900
 Residual                          2.8247   1.6807
number of obs: 647, groups: CHAIN * SECTOR * RESP, 71

Fixed effects:
                    Estimate Std. Error  t value
(Intercept)        4.5760000  2.6193950  1.74697
CHAIN             -0.2014603  0.7984752 -0.25231
SECTOR            -0.1093434  2.3516722 -0.04650
RESP               0.0184237  0.0276326  0.66674
CHAIN:SECTOR       0.1423668  0.3005919  0.47362
CHAIN:RESP         0.0024786  0.0083782  0.29584
SECTOR:RESP       -0.0046001  0.0240517 -0.19126
CHAIN:SECTOR:RESP -0.0011219  0.0030762 -0.36470

Correlation of Fixed Effects:
              (Intr) CHAIN  SECTOR RESP   CHAIN:SECTOR CHAIN:R SECTOR:
CHAIN         -0.435
SECTOR        -0.845 -0.050
RESP          -0.778  0.345  0.645
CHAIN:SECTOR   0.886 -0.732 -0.635 -0.680
CHAIN:RESP     0.351 -0.782  0.038 -0.466  0.566
SECTOR:RESP    0.666  0.038 -0.786 -0.822  0.500       -0.046
CHAIN:SECTOR: -0.709  0.586  0.500  0.879 -0.789       -0.729  -0.635
>

Again, my problem is: there are no fixed effects...
The same dataset, when running at SPSS (I have a subset with 647
records), using the syntax I showed somewhere before, gives me the
following output:

Variance Components Estimation
Variance Estimates
Component      Estimate
Var(CHAIN)     ,530
Var(SECTOR)    ,000(a)
Var(RESP)      2,734
Var(ASPECT)    ,788
Var(ITEM)      ,000(a)
Var(SECTOR *   ,061
RESP)
Var(SECTOR *   ,000(a)
ASPECT)
Var(SECTOR *   ,031
ITEM)
Var(CHAIN *    2,183
RESP)
Var(CHAIN *    ,038
ASPECT)
Var(CHAIN *    ,003
ITEM)
Var(RESP *     ,467
ASPECT)
Var(RESP *     ,279
ITEM)
Var(SECTOR *   ,000(a)
RESP * ASPECT)
Var(SECTOR *   ,077
RESP * ITEM)
Var(CHAIN *    ,773
RESP * ASPECT)
Var(Error)     ,882
Dependent Variable: RATING
 Method: Restricted Maximum Likelihood Estimation
a This estimate is set to zero because it is redundant.

That's what I would like to get from R.

Any help would be appreciated.

Best regards,

Iuri

On 8/20/06, Iuri Gavronski <iuri at ufrgs.br> wrote:
>
> Harold, I have tried to adapt your syntax and got some problems. Some responses from lmer:
>
> On this one, I have tried to use "1" as a grouping variable. As I understood from Bates (2005), grouping variables are like nested design, which is not the case.
> > fm <- lmer(RATING ~ CHAIN*SECTOR*RESP +(CHAIN*SECTOR*RESP|1), gt)
> Erro em lmer(RATING ~ CHAIN * SECTOR * RESP + (CHAIN * SECTOR * RESP |  :
>         Ztl[[1]] must have 1 columns
>
> Then I have tried to ommit the fixed effects...
> > fm <- lmer(RATING ~ (CHAIN*SECTOR*RESP|1), gt)
> Erro em x[[3]] : n?o ? poss?vel dividir o objeto em subconjuntos
> (the error message would be something like "not possible to divide the object in subsets"... I don't know the original wording of message because my R is in Portuguese...)
>
> Then... I have tried to specify RESP (the persons) as the grouping variable (which doesn't make any sense to me, but...)
> > fm <- lmer(RATING ~ CHAIN*SECTOR*RESP +(CHAIN*SECTOR|RESP), gt)
> Warning message:
> nlminb returned message false convergence (8)
>  in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance = 1.49011611938477e-08,
> >
>
> Any idea?
>
>
> Regards,
>
> Iuri.
>
>
> On 8/17/06, Doran, Harold <HDoran at air.org> wrote:
> >
> >
> >
> > Iuri:
> >
> > Here is an example of how a model would be specified using  lmer using a couple of your factors:
> >
> > fm <- lmer(response.variable ~ chain*sector*resp  +(chain*sector*resp|GroupingID), data)
> >
> > This will give you a main effect for each factor and all  possible interactions. However, do you have a grouping variable? I wonder if aov  might be the better tool for your G-study?
> >
> > As a side note, I don't see that you have a factor for  persons. Isn't this also a variance component of interest for your  study?
> >
> >
> >    ________________________________
   From: prof.iuri at gmail.com    [mailto:prof.iuri at gmail.com] On Behalf
Of Iuri    Gavronski
> > Sent: Thursday, August 17, 2006 1:26 PM
> > To:    Doran, Harold
> >
> > Cc: r-help at stat.math.ethz.ch
> >
> > Subject: Re:    [R] Variance Components in R
> >
> >
> >
> >
> > I am trying to replicate Finn and Kayand? (1997) study on G-theory    application on Marketing. The idea is to have people evaluate some aspects of    service quality for chains on different economy sectors. Then, conduct a    G-study to identify the generalizability coefficient estimates for different    D-study designs.
> > I have persons rating 3 different items on 3 different    aspects of service quality on 3 chains on 3 sectors. It is normally assumed on    G-studies that the factors are random. So I have to specify a model to    estimate the variance components of CHAIN SECTOR RESP ASPECT ITEM, and the interaction of    SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP CHAIN*ASPECT CHAIN*ITEM    RESP*ASPECT RESP*ITEM SECTOR*RESP*ASPECT SECTOR*RESP*ITEM CHAIN*RESP*ASPECT.    '*' in VARCOMP means a crossed design.
> > Evaluating only the two dimensions    interactions (x*y) ran in few minutes with the full database. Including three    interactions (x*y*z) didn't complete the execution at all. I have the data and    script sent to a professor of the department of Statistics on my university    and he could not run it on either SPSS or SAS (we don't have SAS licenses here    at the business school, only SPSS). Nobody here at the business school has any    experience with R, so I don't have anyone to ask for help.
> > ? am not    sure if I have answered you question, but feel free to ask it again, and I    will try to restate the problem.
> >
> > Best regards,
> >
> > Iuri
> >
> >
> >
> >
> > On 8/17/06, Doran,    Harold <HDoran at air.org>    wrote:
> >
> > >
> > >
> > >
> > >
> > > This      will (should) be a piece of cake for lmer. But, I don't speak SPSS. Can      you write your model out as a linear model and give a brief description of      the data and your problem?
> > >
> > > In      addition to what Spencer noted as help below, you should also check out the      vignette in the mlmRev package. This will give you many      examples.
> > >
> > > vignette('MlmSoftRev')
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >        ________________________________
       From: prof.iuri at gmail.com        [mailto:prof.iuri at gmail.com]
     On Behalf Of Iuri Gavronski
> > > Sent: Thursday, August 17,        2006 11:16 AM
> > > To: Doran, Harold
> > >
> > >
> > > Subject: Re: [R] Variance Components in        R
> > >
> > >
> > >
> > >
> >
> >
> >
> > 9500 records. It didn`t run in SPSS or SAS on Windows machines,      so I am trying to convert the SPSS script to R to run in a RISC station at      the university.
> >
> >
> >
> >
> > On 8/17/06, Doran,      Harold <HDoran at air.org>      wrote:
> >
> > >
> >
> > Iuri:
> >
> > The lmer function is optimal for large data with crossed random        effects.
> > How large are your data?
> >
> > > -----Original        Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> >
> > > [mailto: r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri        Gavronski
> >
> > > Sent: Thursday, August 17, 2006 11:08 AM
> > > To:        Spencer Graves
> > > Cc: r-help at stat.math.ethz.ch
> >
> > > Subject: Re: [R]        Variance Components in R
> > >
> > > Thank you for your reply.
> > >        VARCOMP is available at SPSS advanced models, I'm not sure
> > > for        how long it exists... I only work with SPSS for the last
> > > 4        years...
> > > My model only has crossed random effects, what perhaps        would
> > > drive me to lmer().
> > > However, as I have unbalanced        data (why it is normally called
> > > 'unbalanced design'? the data was        not intended to be
> > > unbalanced, only I could not get responses for        all cells...),
> > > I'm afraid that REML would take too much CPU,        memory and time
> > > to execute, and MINQUE would be faster and provide        similar
> > > variance estimates (please, correct me if I'm wrong on        that point).
> > > I only found MINQUE on the maanova package, but as my        study
> > > is very far from genetics, I'm not sure I can use this        package.
> > > Any comment would be appreciated.
> > >        Iuri
> > >
> >
> > > On 8/16/06, Spencer Graves <spencer.graves at pdf.com > wrote:
> > > >
> > >        >       I used SPSS over 25 years ago,        but I don't recall
> > > ever fitting a
> > > > variance components        model with it.  Are all your random
> > > effects        nested?
> > > > If they were, I would recommend you use 'lme' in the        'nlme' package.
> > > > However, if you have crossed random effects,        I suggest you
> > > try 'lmer'
> > > > associated with the 'lme4'        package.
> > > >
> > > >       For        'lmer', documentation is available in Douglas
> > > Bates.        Fitting
> > > > linear mixed models in R. /R News/, 5(1):27-30, May        2005
> >
> > > > (www.r-project.org ->        newsletter).  I also recommend you try the
> >
> > > > vignette        available with the 'mlmRev' package (see, e.g.,
> >
> > > >  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html        ).
> >
> > > >
> > >        >        Excellent        documentation for both 'lme' (and indirectly for
> > > > 'lmer') is        available in Pinheiro and Bates (2000)
> > > Mixed-Effects        Models
> > > > in S and S-Plus (Springer).  I have        personally recommended
> > > this book
> > > > so many times on        this listserve that I just now got 234 hits for
> > > >        RSiteSearch("graves pinheiro").  Please don't hesitate to pass        this
> > > > recommendation to your university        library.  This book is
> > > the primary
> > > >        documentation for the 'nlme' package, which is part of the
> > >        standard R
> > > > distribution.  A subdirectory        "~library\nlme\scripts" of your R
> > > > installation includes        files named "ch01.R", "ch02.R", ...,
> > > "ch06.R",
> > > >        "ch08.R", containing the R scripts described in the book.  These        R
> > > > script files make it much easier and more enjoyable to        study that
> > > > book, because they make it much easier to try the        commands
> > > described
> > > > in the book, one line at a time,        testing modifications to check you
> > > > comprehension,        etc.  In addition to avoiding problems with
> > > >        typographical errors, it also automatically overcomes a few
> > > minor        but
> > > > substantive changes in the notation between S-Plus and        R.
> > > >
> > > >       Also, the        "MINQUE" method has been obsolete for over
> > > 25 years.
> > > >        I recommend you use method = "REML" except for when you want to
> > >        > compare two nested models with different fixed        effects;  in
> > > that case,
> > > > you should use        method = "ML", as explained in Pinheiro and
> > > Bates (2000).
> > >        >
> > > >       Hope this        helps.
> > > >       Spencer        Graves
> > > >
> > > > Iuri Gavronski wrote:
> > > > >        Hi,
> > > > >
> > > > > I'm trying to fit a model using        variance components in R, but if
> > > > > very new on it, so I'm        asking for your help.
> > > > >
> > > > > I have imported        the SPSS database onto R, but I don't know how to
> > > > >        convert the commands... the SPSS commands I'm trying to
> > > convert        are:
> > > > > VARCOMP
> > > >        >    RATING BY CHAIN SECTOR RESP ASPECT        ITEM
> > > > >    /RANDOM = CHAIN SECTOR RESP        ASPECT ITEM
> > > > >    /METHOD = MINQUE        (1)
> > > > >    /DESIGN = CHAIN SECTOR RESP        ASPECT ITEM
> > > >        >                SECTOR*RESP        SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
> > > > > CHAIN*ASPECT        CHAIN*ITEM RESP*ASPECT RESP*ITEM
> > > >        >                SECTOR*RESP*ASPECT        SECTOR*RESP*ITEM
> > > CHAIN*RESP*ASPECT
> > > >        >    /INTERCEPT = INCLUDE.
> > > >        >
> > > > > VARCOMP
> > > >        >    RATING BY CHAIN SECTOR RESP ASPECT        ITEM
> > > > >    /RANDOM = CHAIN SECTOR RESP        ASPECT ITEM
> > > > >    /METHOD = REML
> > > > >    /DESIGN = CHAIN SECTOR RESP        ASPECT ITEM
> > > >        >                SECTOR*RESP        SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
> > > > > CHAIN*ASPECT        CHAIN*ITEM RESP*ASPECT RESP*ITEM
> > > >        >                SECTOR*RESP*ASPECT        SECTOR*RESP*ITEM
> > > CHAIN*RESP*ASPECT
> > > >        >    /INTERCEPT = INCLUDE.
> > > >        >
> > > > > Thank you for your help.
> > > > >
> > >        > > Best regards,
> > > > >
> > > > > Iuri.
> > >        > >
> > > > >        _______________________________________
> > > > > Iuri Gavronski -        iuri at ufrgs.br
> >
> > > >        > doutorando
> > > > > UFRGS/PPGA/NITEC - www.ppga.ufrgs.br        Brazil
> > > > >
> > > > >        ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> >
> > > > >        https://stat.ethz.ch/mailman/listinfo/r-help
> > >        > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > >        > > and provide commented, minimal, self-contained, reproducible        code.
> > > > >
> > >        >
> > >
> > >       [[alternative        HTML version deleted]]
> > >
> > >        ______________________________________________
> >
> > > R-help at stat.math.ethz.ch  mailing list
> >
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > >        PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and        provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> >
> >
> >
>
>


From ggrothendieck at gmail.com  Sun Aug 20 15:28:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 20 Aug 2006 09:28:30 -0400
Subject: [R] Grid Points
In-Reply-To: <loom.20060820T142815-709@post.gmane.org>
References: <loom.20060820T142815-709@post.gmane.org>
Message-ID: <971536df0608200628l6f05d4cetd284a6421fc79e14@mail.gmail.com>

Try this.  gl(2,50) is such that the first 50 points are series 1
and the second 50 points are series 2.  The scales= argument
defines the positions of the tick marks and the xlim= argument
defines the x axis limits.  The layout puts the panels on top
of each other rather than side by side.  strip = FALSE eliminates
the strip above each panel.  type= says we want lines.  The
panel function puts points at the grid locations in each panel
and then calls xyplot to plot the lines.

library(lattice)
library(grid)

x <- 601:700
at <- seq(500, 800, 50)
xyplot(x ~ x | gl(2, 50), scales = list(at = at), xlim = c(500, 700),
  layout = 1:2, strip = FALSE, type = "l",
  panel = function(...) {
    grid.points(at, unit(rep(.01, length(at)), "npc"),
      pch = 20, size = unit(.2, "char"))
    panel.xyplot(...)
})


On 8/20/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> How do I put grid points (not grid lines) as the base layer of an xyplot?
>
> Is there a way to vary the interval at which x and y grid points are placed?
>
> Is it possible to start a graph so that Y axis begins at 500 and ends at 800? I
> am only interested in focusing on the relative distance between the points whose
> values are between 500 and 800, but not their relative distance from zero.
>
> Is there a way in R to draw two graphs so that that "share" the same X axis, but
> without the gap that mfrow() creates? The origin of the top graph should be
> where the Y-axis of the bottom graph ends. It will be useful for what I am
> trying to do to have the X axis of the top graph be invisible.
>
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sun Aug 20 15:58:13 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 20 Aug 2006 09:58:13 -0400
Subject: [R] split a y-axis to show data on different scales
In-Reply-To: <loom.20060820T144206-922@post.gmane.org>
References: <200608131848.k7DIm3KL004560@rm-rstar.sfu.ca>
	<44E10B6A.3040505@bitwrit.com.au> <44E0A172.2020907@pdf.com>
	<44902.129.206.90.2.1155635601.squirrel@mail.panix.com>
	<loom.20060819T142130-896@post.gmane.org>
	<f8e6ff050608190604h3a0e65a3g9ff754a40ae298a3@mail.gmail.com>
	<loom.20060820T144206-922@post.gmane.org>
Message-ID: <971536df0608200658x398dc572hf4ad142d9a3e6e8a@mail.gmail.com>

Look at oma= and mar= parameters to par for controlling the
space when using mfrow=.  e.g.

opar <- par(oma = c(6, 0, 5, 0), mar = c(0, 5.1, 0, 2.1), mfrow = c(2,2))
for(i in 1:4) plot(1:10)
par(opar)

On 8/20/06, Anupam Tyagi <AnupTyagi at yahoo.com> wrote:
> I think information can be enhanced by using different scaled graphs next to
> each other. mfrow() created too much space, there may be no need to again draw
> the x-axis. It can be very useful to have different scales of the same data
> presented next to each other, in addition to the main graph. So I think the data
> of the person who started this thread could be displayed using one graph will
> all the data, and then a superimposed graph (sharing same x-axis) on any part of
> the data to give an enhanced visual communication. Drawing grid lines with same
> tick marks in both graphs can enhance this visual communication. This is like
> "static zooming". Of course it is important to make sure that the change in
> scale is evident, because it is needed for the interpretation of the
> graph---using a grid with same tick marks can produce this effect visually.
>
> Anupam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From epistat at gmail.com  Sun Aug 20 16:13:45 2006
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 20 Aug 2006 22:13:45 +0800
Subject: [R] fit the series data
Message-ID: <2fc17e30608200713l142af2a0kffd71a8bdb7a7377@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060820/1ffc836d/attachment.pl 

From MSchwartz at mn.rr.com  Sun Aug 20 16:23:26 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 20 Aug 2006 09:23:26 -0500
Subject: [R] string-to-number
In-Reply-To: <46a360560608190925u432b33a8rcdda2c12276f905a@mail.gmail.com>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
	<1155989485.4308.7.camel@localhost.localdomain>
	<Pine.LNX.4.64.0608191322510.12049@gannet.stats.ox.ac.uk>
	<1155992326.4308.15.camel@localhost.localdomain>
	<46a360560608190925u432b33a8rcdda2c12276f905a@mail.gmail.com>
Message-ID: <1156083806.4308.70.camel@localhost.localdomain>

On Sat, 2006-08-19 at 10:25 -0600, Mike Nielsen wrote:
> Wow.  New respect for parse/eval.
> 
> Do you think this is a special case of a more general principle?  I
> suppose the cost is memory, but from time to time a speedup like this
> would be very beneficial.
> 
> Any hints about how R programmers could recognize such cases would, I
> am sure, be of value to the list in general.
> 
> Many thanks for your efforts, Marc!

Mike,

I think that one needs to consider where the time is being spent and
then adjust accordingly. Once you understand that, you can develop some
insight into what may be a more efficient approach. R provides good
profiling tools that facilitate this process.

In this case, almost all of the time in the first two examples using
strsplit(), is in that function:

> repeated.measures.columns <- paste(1:100000, collapse = ",")

> library(utils)
> Rprof(tmp <- tempfile())
> res1 <- as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
> Rprof()

> summaryRprof(tmp)
$by.self
                    self.time self.pct total.time total.pct
"strsplit"              23.68     99.7      23.68      99.7
"as.double.default"      0.06      0.3       0.06       0.3
"as.numeric"             0.00      0.0      23.74     100.0
"unlist"                 0.00      0.0      23.68      99.7

$by.total
                    total.time total.pct self.time self.pct
"as.numeric"             23.74     100.0      0.00      0.0
"strsplit"               23.68      99.7     23.68     99.7
"unlist"                 23.68      99.7      0.00      0.0
"as.double.default"       0.06       0.3      0.06      0.3

$sampling.time
[1] 23.74


Contrast that with Prof. Ripley's approach:

> Rprof(tmp <- tempfile())
> res3 <- eval(parse(text=paste("c(", repeated.measures.columns, ")")))
> Rprof()

> summaryRprof(tmp)
$by.self
        self.time self.pct total.time total.pct
"parse"      0.42     87.5       0.42      87.5
"eval"       0.06     12.5       0.48     100.0

$by.total
        total.time total.pct self.time self.pct
"eval"        0.48     100.0      0.06     12.5
"parse"       0.42      87.5      0.42     87.5

$sampling.time
[1] 0.48


To some extent, one could argue that my initial timing examples are
contrived, in that they specifically demonstrate a worst case scenario
using strsplit().  Real world examples may or may not show such gains.

For example with Charles' initial query, the initial vector was rather
short:

  > repeated.measures.columns
  [1] "3,6,10"

So if this was a one-time conversion, we would not see such significant
gains.

However, what if we had a long list of shorter entries:

> repeated.measures.columns <- paste(1:10, collapse = ",")
> repeated.measures.columns
[1] "1,2,3,4,5,6,7,8,9,10"

> big.list <- replicate(10000, list(repeated.measures.columns))

> head(big.list)
[[1]]
[1] "1,2,3,4,5,6,7,8,9,10"

[[2]]
[1] "1,2,3,4,5,6,7,8,9,10"

[[3]]
[1] "1,2,3,4,5,6,7,8,9,10"

[[4]]
[1] "1,2,3,4,5,6,7,8,9,10"

[[5]]
[1] "1,2,3,4,5,6,7,8,9,10"

[[6]]
[1] "1,2,3,4,5,6,7,8,9,10"


> system.time(res1 <- t(sapply(big.list, function(x)
as.numeric(unlist(strsplit(x, ","))))))
[1] 1.972 0.044 2.411 0.000 0.000

> str(res1)
 num [1:10000, 1:10] 1 1 1 1 1 1 1 1 1 1 ...

> head(res1)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    2    3    4    5    6    7    8    9    10
[2,]    1    2    3    4    5    6    7    8    9    10
[3,]    1    2    3    4    5    6    7    8    9    10
[4,]    1    2    3    4    5    6    7    8    9    10
[5,]    1    2    3    4    5    6    7    8    9    10
[6,]    1    2    3    4    5    6    7    8    9    10



Now use Prof. Ripley's approach:

> system.time(res3 <- t(sapply(big.list, function(x)
eval(parse(text=paste("c(", x, ")"))))))
[1] 1.676 0.012 1.877 0.000 0.000

> str(res3)
 num [1:10000, 1:10] 1 1 1 1 1 1 1 1 1 1 ...

> head(res3)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    2    3    4    5    6    7    8    9    10
[2,]    1    2    3    4    5    6    7    8    9    10
[3,]    1    2    3    4    5    6    7    8    9    10
[4,]    1    2    3    4    5    6    7    8    9    10
[5,]    1    2    3    4    5    6    7    8    9    10
[6,]    1    2    3    4    5    6    7    8    9    10



> all(res1 == res3)
[1] TRUE


We do see a notable reduction in time with strsplit(), while a notable
increase in time using eval(parse)), even though we are converting the
same net number of values (100,000).

Much of the increase with eval(parse()) is of course due to the overhead
of sapply() and navigating the list.


Let's increase the size of the list components to 1000:

> repeated.measures.columns <- paste(1:1000, collapse = ",")
> big.list <- replicate(10000, list(repeated.measures.columns))

> system.time(res1 <- sapply(big.list, function(x)
as.numeric(unlist(strsplit(x, ",")))))
[1] 33.270  0.744 37.163  0.000  0.000

> system.time(res3 <- t(sapply(big.list, function(x)
eval(parse(text=paste("c(", x, ")"))))))
[1] 15.893  0.928 18.139  0.000  0.000


So we see here that as the size of the list components increases, there
continues to be an advantage to Prof. Ripley's approach over using
strsplit().

Again, one needs to develop an understanding of where the time is spent
in the processing by profiling and then consider how to introduce
efficiencies, which in some cases may very well require the use of
compiled C/FORTRAN as may be appropriate if times become too long.

HTH,

Marc Schwartz


From jholtman at gmail.com  Sun Aug 20 17:56:36 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 20 Aug 2006 11:56:36 -0400
Subject: [R] fit the series data
In-Reply-To: <2fc17e30608200713l142af2a0kffd71a8bdb7a7377@mail.gmail.com>
References: <2fc17e30608200713l142af2a0kffd71a8bdb7a7377@mail.gmail.com>
Message-ID: <644e1f320608200856h19233401l3df7999c76959570@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060820/29598ca9/attachment.pl 

From anuptyagi at yahoo.com  Sun Aug 20 19:30:53 2006
From: anuptyagi at yahoo.com (Anupam Tyagi)
Date: Sun, 20 Aug 2006 10:30:53 -0700 (PDT)
Subject: [R] Grid Points
In-Reply-To: <971536df0608200628l6f05d4cetd284a6421fc79e14@mail.gmail.com>
Message-ID: <20060820173053.43085.qmail@web52207.mail.yahoo.com>

Thanks. How do I retain the same scale of grid.points
from one panel to next even if the scale of the data
changes? For example: c(seq(601:700),seq(6510,7000,
by=10)) ~ seq(601:700) | gl(2,50). 


--- Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> Try this.  gl(2,50) is such that the first 50 points
> are series 1
> and the second 50 points are series 2.  The scales=
> argument
> defines the positions of the tick marks and the
> xlim= argument
> defines the x axis limits.  The layout puts the
> panels on top
> of each other rather than side by side.  strip =
> FALSE eliminates
> the strip above each panel.  type= says we want
> lines.  The
> panel function puts points at the grid locations in
> each panel
> and then calls xyplot to plot the lines.
> 
> library(lattice)
> library(grid)
> 
> x <- 601:700
> at <- seq(500, 800, 50)
> xyplot(x ~ x | gl(2, 50), scales = list(at = at),
> xlim = c(500, 700),
>   layout = 1:2, strip = FALSE, type = "l",
>   panel = function(...) {
>     grid.points(at, unit(rep(.01, length(at)),
> "npc"),
>       pch = 20, size = unit(.2, "char"))
>     panel.xyplot(...)
> })
> 
> 
> On 8/20/06, Anupam Tyagi <AnupTyagi at yahoo.com>
> wrote:
> > How do I put grid points (not grid lines) as the
> base layer of an xyplot?
> >
> > Is there a way to vary the interval at which x and
> y grid points are placed?
> >
> > Is it possible to start a graph so that Y axis
> begins at 500 and ends at 800? I
> > am only interested in focusing on the relative
> distance between the points whose
> > values are between 500 and 800, but not their
> relative distance from zero.
> >
> > Is there a way in R to draw two graphs so that
> that "share" the same X axis, but
> > without the gap that mfrow() creates? The origin
> of the top graph should be
> > where the Y-axis of the bottom graph ends. It will
> be useful for what I am
> > trying to do to have the X axis of the top graph
> be invisible.
> >
> > Anupam.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
>


From spencer.graves at pdf.com  Sun Aug 20 20:22:16 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Aug 2006 11:22:16 -0700
Subject: [R] Simulate p-value in lme4
In-Reply-To: <1155847771.12391.46.camel@solidago.localdomain>
References: <1155847771.12391.46.camel@solidago.localdomain>
Message-ID: <44E8A858.5010709@pdf.com>

      You've raised a very interesting question about testing a 
fixed-effect factor with more than 2 levels using Monte Carlo.  Like 
you, I don't know how to use 'mcmcsamp' to refine the naive 
approximation. If we are lucky, someone else might comment on this for us. 

      Beyond this, you are to be commended for providing such a simple, 
self-contained example for such a sophisticated question.  I think you 
simulation misses one important point:  It assumes the between-subject 
variance is zero.  To overcome this, I think I might try either the 
bootstrap or permutation distribution scrambling the assignment of 
subjects to treatment groups but otherwise keeping the pairs of 
observations together. 

      To this end, consider the following: 

# Build a table to translate subject into 'pred'
o <- with(epil3, order(subject, y))
epil3. <- epil3[o,]
norep <- with(epil3., subject[-1]!=subject[-dim(epil3)[1]])
subj1 <- which(c(T, norep))
subj.pred <- epil3.[subj1, c("subject", "pred")]
subj. <- as.character(subj.pred$subject)
pred. <- subj.pred$pred
names(pred.) <- subj.

iter <- 10
chisq.sim <- rep(NA, iter)

set.seed(1)
for(i in 1:iter){
  s.i <- sample(subj.)
# Randomize subject assignments to 'pred' groups
  epil3.$pred <- pred.[s.i][epil3.$subject]
  fit1 <- lmer(y ~ pred+(1 | subject),
                family = poisson, data = epil3.)
  fit0 <- lmer(y ~ 1+(1 | subject),
                family = poisson, data = epil3.)
  chisq.sim[i] <- anova(fit0, fit1)[2, "Chisq"]
}

      Hope this helps. 
      Spencer Graves

Manuel Morales wrote:
> Dear list,
>
> This is more of a stats question than an R question per se. First, I
> realize there has been a lot of discussion about the problems with
> estimating P-values from F-ratios for mixed-effects models in lme4.
> Using mcmcsamp() seems like a great alternative for evaluating the
> significance of individual coefficients, but not for groups of
> coefficients as might occur in an experimental design with 3 treatment
> levels. I'm wondering if the simulation approach I use below to estimate
> the P-value for a 3-level factor is appropriate, or if there are any
> suggestions on how else to approach this problem. The model and data in
> the example are from section 10.4 of MASS.
>
> Thanks!
> Manuel
>
> # Load req. package (see functions to generate data at end of script)
> library(lme4)
> library(MASS)
>
> # Full and reduced models - pred is a factor with 3 levels
> result.full <- lmer(y~pred+(1|subject), data=epil3, family="poisson")
> result.base <- lmer(y~1+(1|subject), data=epil3, family="poisson")
>
> # Naive P-value from LR for significance of "pred" factor
> anova(result.base,result.full)$"Pr(>Chisq)"[[2]] # P-value
> (test.stat <- anova(result.base,result.full)$Chisq[[2]]) # Chisq-stat
>
> # P-value from simulation. Note that in the simulation, I use the
> # estimated random effects for each subject rather than generating a new
> # distribution of means. I'm not sure if this is appropriate or not ...
> intercept <- fixef(result.base)
> rand.effs <- ranef(result.base)[[1]]
> mu <- exp(rep(intercept+rand.effs[[1]],2))
>
> p.value <- function(iter, stat) {
>   chi.stat <- vector()
>   for(i in 1:iter) {
>     resp <- rpois(length(mu), mu) # simulate values
>     sim.data <- data.frame(y=resp,subject=epil3$subject,pred=epil3$pred)
>     result.f <- lmer(y~pred+(1|subject), data=sim.data,
>                      family="poisson")
>     result.b <- lmer(y~1+(1|subject), data=sim.data, family="poisson")
>     chi.stat[i] <- anova(result.b,result.f)$Chisq[[2]]
>   }
>   val <- sum(unlist(lapply(chi.stat, function(x) if(x>stat) 1 else
>              0)))/iter
>   hist(chi.stat)
>   return(val)
> }
>
> p.value(10,test.stat) # Increase to >=1000 to get a reasonable P-value!
>
> # Script to generate data, from section 10.4 of MASS
> epil2 <- epil[epil$period == 1, ]
> epil2["period"] <- rep(0, 59); epil2["y"] <- epil2["base"]
> epil["time"] <- 1; epil2["time"] <- 4
> epil2 <- rbind(epil, epil2)
> epil2$pred <- unclass(epil2$trt) * (epil2$period > 0)
> epil2$subject <- factor(epil2$subject)
> epil3 <- aggregate(epil2, list(epil2$subject, epil2$period > 0),
>                    function(x) if(is.numeric(x)) sum(x) else x[1])
> epil3$pred <- factor(epil3$pred, labels = c("base", "placebo", "drug"))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Sun Aug 20 20:39:38 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 20 Aug 2006 13:39:38 -0500
Subject: [R] C compile problem on Ubuntu linux
Message-ID: <44E8AC6A.1040605@vanderbilt.edu>

Under Ubuntu dapper, after installing packages gcc and g77, under

platform i486-pc-linux-gnu
arch     i486
os       linux-gnu
system   i486, linux-gnu
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R

I get an error when trying to update.packages('Hmisc'):

gcc -I/usr/lib/R/include     -fPIC  -g -O2 -c ranksort.c -o ranksort.o
In file included from ranksort.c:1:
/usr/lib/R/include/R.h:32:20: error: stdlib.h: No such file or directory
/usr/lib/R/include/R.h:33:19: error: stdio.h: No such file or directory
In file included from 
/usr/lib/gcc/i486-linux-gnu/4.0.3/include/syslimits.h:7,
                  from 
/usr/lib/gcc/i486-linux-gnu/4.0.3/include/limits.h:11,
                  from /usr/lib/R/include/R.h:34,
                  from ranksort.c:1:
/usr/lib/gcc/i486-linux-gnu/4.0.3/include/limits.h:122:61: error: 
limits.h: No such file or directory
In file included from ranksort.c:1:
/usr/lib/R/include/R.h:36:18: error: math.h: No such file or directory
In file included from /usr/lib/R/include/R.h:49,
                  from ranksort.c:1:
/usr/lib/R/include/R_ext/RS.h:23:38: error: string.h: No such file or 
directory
make: *** [ranksort.o] Error 1
ERROR: compilation failed for package 'Hmisc'

Any help appreciated

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Sun Aug 20 22:29:02 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 20 Aug 2006 15:29:02 -0500
Subject: [R] C compile problem on Ubuntu linux
In-Reply-To: <44E8B8F6.7060401@unileon.es>
References: <44E8AC6A.1040605@vanderbilt.edu> <44E8B8F6.7060401@unileon.es>
Message-ID: <44E8C60E.9060602@vanderbilt.edu>

Manuel Castej?n Limas wrote:
> Hello,
> I've just compiled Hmisc ok under dapper.
> I think you need to further install some packages.
> Have you installed libc6-dev?
> I would start installing the build-essential package.
> Best wishes
> Manuel

Thanks Manuel, apt-get install build-essential solved the problem.
Frank

> 
> Frank E Harrell Jr escribi?:
>> Under Ubuntu dapper, after installing packages gcc and g77, under
>>
>> platform i486-pc-linux-gnu
>> arch     i486
>> os       linux-gnu
>> system   i486, linux-gnu
>> status
>> major    2
>> minor    2.1
>> year     2005
>> month    12
>> day      20
>> svn rev  36812
>> language R
>>
>> I get an error when trying to update.packages('Hmisc'):
>>
>> gcc -I/usr/lib/R/include     -fPIC  -g -O2 -c ranksort.c -o ranksort.o
>> In file included from ranksort.c:1:
>> /usr/lib/R/include/R.h:32:20: error: stdlib.h: No such file or directory
>> /usr/lib/R/include/R.h:33:19: error: stdio.h: No such file or directory
>> In file included from 
>> /usr/lib/gcc/i486-linux-gnu/4.0.3/include/syslimits.h:7,
>>                   from 
>> /usr/lib/gcc/i486-linux-gnu/4.0.3/include/limits.h:11,
>>                   from /usr/lib/R/include/R.h:34,
>>                   from ranksort.c:1:
>> /usr/lib/gcc/i486-linux-gnu/4.0.3/include/limits.h:122:61: error: 
>> limits.h: No such file or directory
>> In file included from ranksort.c:1:
>> /usr/lib/R/include/R.h:36:18: error: math.h: No such file or directory
>> In file included from /usr/lib/R/include/R.h:49,
>>                   from ranksort.c:1:
>> /usr/lib/R/include/R_ext/RS.h:23:38: error: string.h: No such file or 
>> directory
>> make: *** [ranksort.o] Error 1
>> ERROR: compilation failed for package 'Hmisc'
>>
>> Any help appreciated
>>
>> Frank
>>   
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From daniil.ivanov at gmail.com  Sun Aug 20 22:45:44 2006
From: daniil.ivanov at gmail.com (Daniil Ivanov)
Date: Sun, 20 Aug 2006 23:45:44 +0300
Subject: [R] plot problem
Message-ID: <d80e51a50608201345y4dc2ab99ld4dc7d3a4a2a1b2a@mail.gmail.com>

Hello.

I'm pretty much new to R and I'm trying to produce some figures.
It seems to me, that R has some asynchronous way of plotting figures.
When I run this code:

#constructs the semivariogram of SC1929
vgm1 <- variogram(SC1929~1,~U+V,puerto.map$att.data)

# trying to make new plot
dev.set(which=dev.next())
plot(vgm1)
title(main="Semivariogram",font.main=4)
dev.copy2eps(file="fig2.eps",horizontal=T)
dev.off()

R complains that
Error in title(main = "Semivariogram", font.main = 4) :
        plot.new has not been called yet

So that plot(..) hasn't been executed before call of title.

The most weird about that, if I add fix(vgm1) before
dev.set(...), everything is just fine. Why R can't just
perform commands one by one?
P.S. I use Linux version of R.

Thanks, Daniil.


From ripley at stats.ox.ac.uk  Sun Aug 20 23:13:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Aug 2006 22:13:22 +0100 (BST)
Subject: [R] plot problem
In-Reply-To: <d80e51a50608201345y4dc2ab99ld4dc7d3a4a2a1b2a@mail.gmail.com>
References: <d80e51a50608201345y4dc2ab99ld4dc7d3a4a2a1b2a@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608202159560.18139@gannet.stats.ox.ac.uk>

On Sun, 20 Aug 2006, Daniil Ivanov wrote:

> Hello.
> 
> I'm pretty much new to R and I'm trying to produce some figures.

What have you been reading to get the ideas below?  People new to R do not 
tend to use dev.next ... indeed experienced users very rarely use it.

> It seems to me, that R has some asynchronous way of plotting figures.
> When I run this code:

It will not run for us: please see the posting guide.  You have forgotten 
to load some package here, and there are variogram() functions in at least 
packages gstat and spatial so we can't even tell what.

> #constructs the semivariogram of SC1929
> vgm1 <- variogram(SC1929~1,~U+V,puerto.map$att.data)
> 
> # trying to make new plot
> dev.set(which=dev.next())

What do think that does?  It seems to say you want to plot on the null 
device, since no next device has been set.

> plot(vgm1)
> title(main="Semivariogram",font.main=4)
> dev.copy2eps(file="fig2.eps",horizontal=T)
> dev.off()
> 
> R complains that
> Error in title(main = "Semivariogram", font.main = 4) :
>         plot.new has not been called yet
> 
> So that plot(..) hasn't been executed before call of title.

That is _not_ what it says.  A guess is that this is a lattice plot from 
package gstat, in which case plot.new has indeed not been called.

> The most weird about that, if I add fix(vgm1) before
> dev.set(...), everything is just fine. Why R can't just
> perform commands one by one?

It does.  You cannot use title() to annotate a lattice plot, though.

> P.S. I use Linux version of R.
> 
> Thanks, Daniil.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do as it asks.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maj at stats.waikato.ac.nz  Sun Aug 20 23:26:38 2006
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 21 Aug 2006 09:26:38 +1200
Subject: [R] unquoting
In-Reply-To: <Pine.LNX.4.64.0608201227340.2564@gannet.stats.ox.ac.uk>
References: <44E83E27.6000903@waikato.ac.nz>
	<Pine.LNX.4.64.0608201227340.2564@gannet.stats.ox.ac.uk>
Message-ID: <44E8D38E.5020302@stats.waikato.ac.nz>

Thank you, Brian, Peter and Gabor

Brian has what want. My heading was a bit misleading. I was looking for 
a function that would, in logicians' terms, convert 'mention' into 
'use'. (This usually goes along with the story about the importance of 
knowing the difference between a lion and "lion".) get() is just such a 
function.

Murray Jorgensen

Prof Brian Ripley wrote:
> ?get
> 
> I really think this has nothing to do with `quoting', rather to do with 
> evaluating variables from their names. At first I though you were looking 
> for noquote(), which does unquote in the conventional sense.
> 
>> noquote(names(AF)[2])
> [1] Second
>> get(names(AF)[2])
> [1] 3 4
> 
> On Sun, 20 Aug 2006, Murray Jorgensen wrote:
> 
>> I would like a function to strip quotes off character strings. I should 
>> work like this:
>>
>>  > A <- matrix(1:6, nrow = 2, ncol=3)
>>  > AF <- as.data.frame(A)
>>  > names(AF) <- c("First","Second","Third")
>>  > AF
>>    First Second Third
>> 1     1      3     5
>> 2     2      4     6
>>  > names(AF)[2]
>> [1] "Second"
>>  > attach(AF)
>>  > unquote(names(AF)[2])
>> [1] 3 4
>>
>> Of course what I actually get is
>>
>> Error: couldn't find function "unquote"
>>
>> The reason that I want to do this is that I have a frame with a rather 
>> large number of variables and I would like to loop over the names and 
>> print out various descriptive summaries of the variable's distribution.
>>
>> OK, OK, I could just go
>>
>>  > AF[,2]
>> [1] 3 4
>>
>> but once I thought of unquoting I have some sort of inner need to know 
>> how to do it!
>>
>> Cheers,
>>
>> Murray
>>
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From daniil.ivanov at gmail.com  Mon Aug 21 00:15:37 2006
From: daniil.ivanov at gmail.com (Daniil Ivanov)
Date: Mon, 21 Aug 2006 01:15:37 +0300
Subject: [R] plot problem
In-Reply-To: <Pine.LNX.4.64.0608202159560.18139@gannet.stats.ox.ac.uk>
References: <d80e51a50608201345y4dc2ab99ld4dc7d3a4a2a1b2a@mail.gmail.com>
	<Pine.LNX.4.64.0608202159560.18139@gannet.stats.ox.ac.uk>
Message-ID: <d80e51a50608201515y2bb8b598pbeab1cc275110f76@mail.gmail.com>

Hi,

Ok, thanks to all.
Problem was with class of variogram

> class(vgm1)
[1] "gstatVariogram" "data.frame"

If I fix it manually to

class(vgm1) <- "gstatVariogram"

everything runs as it should.

Thanks, Daniil.

On 8/21/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sun, 20 Aug 2006, Daniil Ivanov wrote:
>
> > Hello.
> >
> > I'm pretty much new to R and I'm trying to produce some figures.
>
> What have you been reading to get the ideas below?  People new to R do not
> tend to use dev.next ... indeed experienced users very rarely use it.
>
> > It seems to me, that R has some asynchronous way of plotting figures.
> > When I run this code:
>
> It will not run for us: please see the posting guide.  You have forgotten
> to load some package here, and there are variogram() functions in at least
> packages gstat and spatial so we can't even tell what.
>
> > #constructs the semivariogram of SC1929
> > vgm1 <- variogram(SC1929~1,~U+V,puerto.map$att.data)
> >
> > # trying to make new plot
> > dev.set(which=dev.next())
>
> What do think that does?  It seems to say you want to plot on the null
> device, since no next device has been set.
>
> > plot(vgm1)
> > title(main="Semivariogram",font.main=4)
> > dev.copy2eps(file="fig2.eps",horizontal=T)
> > dev.off()
> >
> > R complains that
> > Error in title(main = "Semivariogram", font.main = 4) :
> >         plot.new has not been called yet
> >
> > So that plot(..) hasn't been executed before call of title.
>
> That is _not_ what it says.  A guess is that this is a lattice plot from
> package gstat, in which case plot.new has indeed not been called.
>
> > The most weird about that, if I add fix(vgm1) before
> > dev.set(...), everything is just fine. Why R can't just
> > perform commands one by one?
>
> It does.  You cannot use title() to annotate a lattice plot, though.
>
> > P.S. I use Linux version of R.
> >
> > Thanks, Daniil.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> PLEASE do as it asks.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From iuri at proxima.adm.br  Mon Aug 21 00:31:42 2006
From: iuri at proxima.adm.br (Iuri Gavronski)
Date: Sun, 20 Aug 2006 19:31:42 -0300
Subject: [R] Variance Components in R
In-Reply-To: <60ad85c90608200621n1b203dfegaa1452f728ada7f7@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E2771B5@dc1ex01.air.org>
	<60ad85c90608200542s3bdb6ce7hf1362281e480f5a6@mail.gmail.com>
	<60ad85c90608200621n1b203dfegaa1452f728ada7f7@mail.gmail.com>
Message-ID: <60ad85c90608201531y78d6521ejaabe265c25a789fb@mail.gmail.com>

Dear Harold and others,

I have changed the syntax for lmer() and used this one:
require(lme4)
gt <- read.table("gt5.txt")
sink("GT output.txt")
attach(gt)
system.time(fm <- lmer(RATING ~ 1
+(1|CHAIN)
+(1|SECTOR)
+(1|RESP)
+(1|ASPECT)
+(1|ITEM)
+(1|SECTOR*RESP)
+(1|SECTOR*ASPECT)
+(1|SECTOR*ITEM)
+(1|CHAIN*RESP)
+(1|CHAIN*ASPECT)
+(1|CHAIN*ITEM)
+(1|RESP*ASPECT)
+(1|RESP*ITEM)
+(1|SECTOR*RESP*ASPECT)
+(1|SECTOR*RESP*ITEM)
+(1|CHAIN*RESP*ASPECT),
gt)
)
options(digits = 4)
options(OutDec = ",")
summary(fm, digits = 4)
sink()

Then the output I got from summary(lm) was:
Linear mixed-effects model fit by REML
Formula: RATING ~ 1 + (1 | CHAIN) + (1 | SECTOR) + (1 | RESP) + (1 |
ASPECT) +      (1 | ITEM) + (1 | SECTOR * RESP) + (1 | SECTOR *
ASPECT) +      (1 | SECTOR * ITEM) + (1 | CHAIN * RESP) + (1 | CHAIN *
ASPECT) +      (1 | CHAIN * ITEM) + (1 | RESP * ASPECT) + (1 | RESP *
ITEM) +      (1 | SECTOR * RESP * ASPECT) + (1 | SECTOR * RESP * ITEM)
+      (1 | CHAIN * RESP * ASPECT)
   Data: gt
  AIC  BIC logLik MLdeviance REMLdeviance
 2386 2462  -1176       2353         2352
Random effects:
 Groups                 Name        Variance Std.Dev.
 CHAIN * RESP * ASPECT  (Intercept) 5,89e-01 0,7675133
 SECTOR * RESP * ITEM   (Intercept) 4,91e-02 0,2216137
 RESP * ITEM            (Intercept) 2,75e-01 0,5242572
 CHAIN * RESP           (Intercept) 1,98e+00 1,4068696
 SECTOR * RESP * ASPECT (Intercept) 5,17e-10 0,0000227
 CHAIN * ITEM           (Intercept) 5,17e-10 0,0000227
 RESP * ASPECT          (Intercept) 4,77e-01 0,6908419
 SECTOR * RESP          (Intercept) 3,42e-01 0,5848027
 CHAIN * ASPECT         (Intercept) 1,61e-02 0,1269306
 SECTOR * ITEM          (Intercept) 2,24e-02 0,1495102
 ITEM                   (Intercept) 5,17e-10 0,0000227
 CHAIN                  (Intercept) 8,88e-01 0,9424441
 RESP                   (Intercept) 2,80e+00 1,6747970
 SECTOR * ASPECT        (Intercept) 5,17e-10 0,0000227
 ASPECT                 (Intercept) 8,07e-01 0,8984151
 SECTOR                 (Intercept) 5,17e-10 0,0000227
 Residual                           1,03e+00 1,0172221
number of obs: 647, groups: CHAIN * RESP * ASPECT, 138; SECTOR * RESP
* ITEM, 138; RESP * ITEM, 70; CHAIN * RESP, 70; SECTOR * RESP *
ASPECT, 47; CHAIN * ITEM, 36; RESP * ASPECT, 24; SECTOR * RESP, 24;
CHAIN * ASPECT, 18; SECTOR * ITEM, 18; ITEM, 9; CHAIN, 9; RESP, 8;
SECTOR * ASPECT, 6; ASPECT, 3; SECTOR, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)    5,797      0,891    6,51

Comparing the output I had from R and SPSS, for the same database:

Component      Estimate	 SPSS	R
Var(CHAIN)     ,530		0,888
Var(SECTOR)    ,000(a)		0,000
Var(RESP)      2,734		2,800
Var(ASPECT)    ,788		0,807
Var(ITEM)      ,000(a)		0,000
Var(SECTOR *   ,061		0,342
RESP)		
Var(SECTOR *   ,000(a)		0,000
ASPECT)		
Var(SECTOR *   ,031		0,022
ITEM)		
Var(CHAIN *    2,183		1,980
RESP)		
Var(CHAIN *    ,038		0,016
ASPECT)		
Var(CHAIN *    ,003		0,000
ITEM)		
Var(RESP *     ,467		0,477
ASPECT)		
Var(RESP *     ,279		0,275
ITEM)		
Var(SECTOR *   ,000(a)		0,000
RESP * ASPECT)		
Var(SECTOR *   ,077		0,049
RESP * ITEM)		
Var(CHAIN *    ,773		0,589
RESP * ASPECT)		
Var(Error)     ,882		1,030

As can be seen on the previous table, the results are different. Am I
specifing a different model on R and SPSS?

Is it possible to have the output from summary(lmer()) in #,###
format, instead of scientific format?

Best regards,

Iuri.
On 8/20/06, Iuri Gavronski <iuri at proxima.adm.br> wrote:
> Harold,
>
> I have tried the following syntax:
>
> > fm <- lmer(RATING ~ CHAIN*SECTOR*RESP +(1|CHAIN*SECTOR*RESP), gt)
> > summary(fm)
> Linear mixed-effects model fit by REML
> Formula: RATING ~ CHAIN * SECTOR * RESP + (1 | CHAIN * SECTOR * RESP)
>    Data: gt
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  2767.466 2807.717 -1374.733   2710.253     2749.466
> Random effects:
>  Groups                Name        Variance Std.Dev.
>  CHAIN * SECTOR * RESP (Intercept) 5.7119   2.3900
>  Residual                          2.8247   1.6807
> number of obs: 647, groups: CHAIN * SECTOR * RESP, 71
>
> Fixed effects:
>                     Estimate Std. Error  t value
> (Intercept)        4.5760000  2.6193950  1.74697
> CHAIN             -0.2014603  0.7984752 -0.25231
> SECTOR            -0.1093434  2.3516722 -0.04650
> RESP               0.0184237  0.0276326  0.66674
> CHAIN:SECTOR       0.1423668  0.3005919  0.47362
> CHAIN:RESP         0.0024786  0.0083782  0.29584
> SECTOR:RESP       -0.0046001  0.0240517 -0.19126
> CHAIN:SECTOR:RESP -0.0011219  0.0030762 -0.36470
>
> Correlation of Fixed Effects:
>               (Intr) CHAIN  SECTOR RESP   CHAIN:SECTOR CHAIN:R SECTOR:
> CHAIN         -0.435
> SECTOR        -0.845 -0.050
> RESP          -0.778  0.345  0.645
> CHAIN:SECTOR   0.886 -0.732 -0.635 -0.680
> CHAIN:RESP     0.351 -0.782  0.038 -0.466  0.566
> SECTOR:RESP    0.666  0.038 -0.786 -0.822  0.500       -0.046
> CHAIN:SECTOR: -0.709  0.586  0.500  0.879 -0.789       -0.729  -0.635
> >
>
> Again, my problem is: there are no fixed effects...
> The same dataset, when running at SPSS (I have a subset with 647
> records), using the syntax I showed somewhere before, gives me the
> following output:
>
> Variance Components Estimation
> Variance Estimates
> Component      Estimate
> Var(CHAIN)     ,530
> Var(SECTOR)    ,000(a)
> Var(RESP)      2,734
> Var(ASPECT)    ,788
> Var(ITEM)      ,000(a)
> Var(SECTOR *   ,061
> RESP)
> Var(SECTOR *   ,000(a)
> ASPECT)
> Var(SECTOR *   ,031
> ITEM)
> Var(CHAIN *    2,183
> RESP)
> Var(CHAIN *    ,038
> ASPECT)
> Var(CHAIN *    ,003
> ITEM)
> Var(RESP *     ,467
> ASPECT)
> Var(RESP *     ,279
> ITEM)
> Var(SECTOR *   ,000(a)
> RESP * ASPECT)
> Var(SECTOR *   ,077
> RESP * ITEM)
> Var(CHAIN *    ,773
> RESP * ASPECT)
> Var(Error)     ,882
> Dependent Variable: RATING
>  Method: Restricted Maximum Likelihood Estimation
> a This estimate is set to zero because it is redundant.
>
> That's what I would like to get from R.
>
> Any help would be appreciated.
>
> Best regards,
>
> Iuri
>
> On 8/20/06, Iuri Gavronski <iuri at ufrgs.br> wrote:
> >
> > Harold, I have tried to adapt your syntax and got some problems. Some responses from lmer:
> >
> > On this one, I have tried to use "1" as a grouping variable. As I understood from Bates (2005), grouping variables are like nested design, which is not the case.
> > > fm <- lmer(RATING ~ CHAIN*SECTOR*RESP +(CHAIN*SECTOR*RESP|1), gt)
> > Erro em lmer(RATING ~ CHAIN * SECTOR * RESP + (CHAIN * SECTOR * RESP |  :
> >         Ztl[[1]] must have 1 columns
> >
> > Then I have tried to ommit the fixed effects...
> > > fm <- lmer(RATING ~ (CHAIN*SECTOR*RESP|1), gt)
> > Erro em x[[3]] : n?o ? poss?vel dividir o objeto em subconjuntos
> > (the error message would be something like "not possible to divide the object in subsets"... I don't know the original wording of message because my R is in Portuguese...)
> >
> > Then... I have tried to specify RESP (the persons) as the grouping variable (which doesn't make any sense to me, but...)
> > > fm <- lmer(RATING ~ CHAIN*SECTOR*RESP +(CHAIN*SECTOR|RESP), gt)
> > Warning message:
> > nlminb returned message false convergence (8)
> >  in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance = 1.49011611938477e-08,
> > >
> >
> > Any idea?
> >
> >
> > Regards,
> >
> > Iuri.
> >
> >
> > On 8/17/06, Doran, Harold <HDoran at air.org> wrote:
> > >
> > >
> > >
> > > Iuri:
> > >
> > > Here is an example of how a model would be specified using  lmer using a couple of your factors:
> > >
> > > fm <- lmer(response.variable ~ chain*sector*resp  +(chain*sector*resp|GroupingID), data)
> > >
> > > This will give you a main effect for each factor and all  possible interactions. However, do you have a grouping variable? I wonder if aov  might be the better tool for your G-study?
> > >
> > > As a side note, I don't see that you have a factor for  persons. Isn't this also a variance component of interest for your  study?
> > >
> > >
> > >    ________________________________
>    From: prof.iuri at gmail.com    [mailto:prof.iuri at gmail.com] On Behalf
> Of Iuri    Gavronski
> > > Sent: Thursday, August 17, 2006 1:26 PM
> > > To:    Doran, Harold
> > >
> > > Cc: r-help at stat.math.ethz.ch
> > >
> > > Subject: Re:    [R] Variance Components in R
> > >
> > >
> > >
> > >
> > > I am trying to replicate Finn and Kayand? (1997) study on G-theory    application on Marketing. The idea is to have people evaluate some aspects of    service quality for chains on different economy sectors. Then, conduct a    G-study to identify the generalizability coefficient estimates for different    D-study designs.
> > > I have persons rating 3 different items on 3 different    aspects of service quality on 3 chains on 3 sectors. It is normally assumed on    G-studies that the factors are random. So I have to specify a model to    estimate the variance components of CHAIN SECTOR RESP ASPECT ITEM, and the interaction of    SECTOR*RESP SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP CHAIN*ASPECT CHAIN*ITEM    RESP*ASPECT RESP*ITEM SECTOR*RESP*ASPECT SECTOR*RESP*ITEM CHAIN*RESP*ASPECT.    '*' in VARCOMP means a crossed design.
> > > Evaluating only the two dimensions    interactions (x*y) ran in few minutes with the full database. Including three    interactions (x*y*z) didn't complete the execution at all. I have the data and    script sent to a professor of the department of Statistics on my university    and he could not run it on either SPSS or SAS (we don't have SAS licenses here    at the business school, only SPSS). Nobody here at the business school has any    experience with R, so I don't have anyone to ask for help.
> > > ? am not    sure if I have answered you question, but feel free to ask it again, and I    will try to restate the problem.
> > >
> > > Best regards,
> > >
> > > Iuri
> > >
> > >
> > >
> > >
> > > On 8/17/06, Doran,    Harold <HDoran at air.org>    wrote:
> > >
> > > >
> > > >
> > > >
> > > >
> > > > This      will (should) be a piece of cake for lmer. But, I don't speak SPSS. Can      you write your model out as a linear model and give a brief description of      the data and your problem?
> > > >
> > > > In      addition to what Spencer noted as help below, you should also check out the      vignette in the mlmRev package. This will give you many      examples.
> > > >
> > > > vignette('MlmSoftRev')
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >        ________________________________
>        From: prof.iuri at gmail.com        [mailto:prof.iuri at gmail.com]
>      On Behalf Of Iuri Gavronski
> > > > Sent: Thursday, August 17,        2006 11:16 AM
> > > > To: Doran, Harold
> > > >
> > > >
> > > > Subject: Re: [R] Variance Components in        R
> > > >
> > > >
> > > >
> > > >
> > >
> > >
> > >
> > > 9500 records. It didn`t run in SPSS or SAS on Windows machines,      so I am trying to convert the SPSS script to R to run in a RISC station at      the university.
> > >
> > >
> > >
> > >
> > > On 8/17/06, Doran,      Harold <HDoran at air.org>      wrote:
> > >
> > > >
> > >
> > > Iuri:
> > >
> > > The lmer function is optimal for large data with crossed random        effects.
> > > How large are your data?
> > >
> > > > -----Original        Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > >
> > > > [mailto: r-help-bounces at stat.math.ethz.ch] On Behalf Of Iuri        Gavronski
> > >
> > > > Sent: Thursday, August 17, 2006 11:08 AM
> > > > To:        Spencer Graves
> > > > Cc: r-help at stat.math.ethz.ch
> > >
> > > > Subject: Re: [R]        Variance Components in R
> > > >
> > > > Thank you for your reply.
> > > >        VARCOMP is available at SPSS advanced models, I'm not sure
> > > > for        how long it exists... I only work with SPSS for the last
> > > > 4        years...
> > > > My model only has crossed random effects, what perhaps        would
> > > > drive me to lmer().
> > > > However, as I have unbalanced        data (why it is normally called
> > > > 'unbalanced design'? the data was        not intended to be
> > > > unbalanced, only I could not get responses for        all cells...),
> > > > I'm afraid that REML would take too much CPU,        memory and time
> > > > to execute, and MINQUE would be faster and provide        similar
> > > > variance estimates (please, correct me if I'm wrong on        that point).
> > > > I only found MINQUE on the maanova package, but as my        study
> > > > is very far from genetics, I'm not sure I can use this        package.
> > > > Any comment would be appreciated.
> > > >        Iuri
> > > >
> > >
> > > > On 8/16/06, Spencer Graves <spencer.graves at pdf.com > wrote:
> > > > >
> > > >        >       I used SPSS over 25 years ago,        but I don't recall
> > > > ever fitting a
> > > > > variance components        model with it.  Are all your random
> > > > effects        nested?
> > > > > If they were, I would recommend you use 'lme' in the        'nlme' package.
> > > > > However, if you have crossed random effects,        I suggest you
> > > > try 'lmer'
> > > > > associated with the 'lme4'        package.
> > > > >
> > > > >       For        'lmer', documentation is available in Douglas
> > > > Bates.        Fitting
> > > > > linear mixed models in R. /R News/, 5(1):27-30, May        2005
> > >
> > > > > (www.r-project.org ->        newsletter).  I also recommend you try the
> > >
> > > > > vignette        available with the 'mlmRev' package (see, e.g.,
> > >
> > > > >  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/81375.html        ).
> > >
> > > > >
> > > >        >        Excellent        documentation for both 'lme' (and indirectly for
> > > > > 'lmer') is        available in Pinheiro and Bates (2000)
> > > > Mixed-Effects        Models
> > > > > in S and S-Plus (Springer).  I have        personally recommended
> > > > this book
> > > > > so many times on        this listserve that I just now got 234 hits for
> > > > >        RSiteSearch("graves pinheiro").  Please don't hesitate to pass        this
> > > > > recommendation to your university        library.  This book is
> > > > the primary
> > > > >        documentation for the 'nlme' package, which is part of the
> > > >        standard R
> > > > > distribution.  A subdirectory        "~library\nlme\scripts" of your R
> > > > > installation includes        files named "ch01.R", "ch02.R", ...,
> > > > "ch06.R",
> > > > >        "ch08.R", containing the R scripts described in the book.  These        R
> > > > > script files make it much easier and more enjoyable to        study that
> > > > > book, because they make it much easier to try the        commands
> > > > described
> > > > > in the book, one line at a time,        testing modifications to check you
> > > > > comprehension,        etc.  In addition to avoiding problems with
> > > > >        typographical errors, it also automatically overcomes a few
> > > > minor        but
> > > > > substantive changes in the notation between S-Plus and        R.
> > > > >
> > > > >       Also, the        "MINQUE" method has been obsolete for over
> > > > 25 years.
> > > > >        I recommend you use method = "REML" except for when you want to
> > > >        > compare two nested models with different fixed        effects;  in
> > > > that case,
> > > > > you should use        method = "ML", as explained in Pinheiro and
> > > > Bates (2000).
> > > >        >
> > > > >       Hope this        helps.
> > > > >       Spencer        Graves
> > > > >
> > > > > Iuri Gavronski wrote:
> > > > > >        Hi,
> > > > > >
> > > > > > I'm trying to fit a model using        variance components in R, but if
> > > > > > very new on it, so I'm        asking for your help.
> > > > > >
> > > > > > I have imported        the SPSS database onto R, but I don't know how to
> > > > > >        convert the commands... the SPSS commands I'm trying to
> > > > convert        are:
> > > > > > VARCOMP
> > > > >        >    RATING BY CHAIN SECTOR RESP ASPECT        ITEM
> > > > > >    /RANDOM = CHAIN SECTOR RESP        ASPECT ITEM
> > > > > >    /METHOD = MINQUE        (1)
> > > > > >    /DESIGN = CHAIN SECTOR RESP        ASPECT ITEM
> > > > >        >                SECTOR*RESP        SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
> > > > > > CHAIN*ASPECT        CHAIN*ITEM RESP*ASPECT RESP*ITEM
> > > > >        >                SECTOR*RESP*ASPECT        SECTOR*RESP*ITEM
> > > > CHAIN*RESP*ASPECT
> > > > >        >    /INTERCEPT = INCLUDE.
> > > > >        >
> > > > > > VARCOMP
> > > > >        >    RATING BY CHAIN SECTOR RESP ASPECT        ITEM
> > > > > >    /RANDOM = CHAIN SECTOR RESP        ASPECT ITEM
> > > > > >    /METHOD = REML
> > > > > >    /DESIGN = CHAIN SECTOR RESP        ASPECT ITEM
> > > > >        >                SECTOR*RESP        SECTOR*ASPECT SECTOR*ITEM CHAIN*RESP
> > > > > > CHAIN*ASPECT        CHAIN*ITEM RESP*ASPECT RESP*ITEM
> > > > >        >                SECTOR*RESP*ASPECT        SECTOR*RESP*ITEM
> > > > CHAIN*RESP*ASPECT
> > > > >        >    /INTERCEPT = INCLUDE.
> > > > >        >
> > > > > > Thank you for your help.
> > > > > >
> > > >        > > Best regards,
> > > > > >
> > > > > > Iuri.
> > > >        > >
> > > > > >        _______________________________________
> > > > > > Iuri Gavronski -        iuri at ufrgs.br
> > >
> > > > >        > doutorando
> > > > > > UFRGS/PPGA/NITEC - www.ppga.ufrgs.br        Brazil
> > > > > >
> > > > > >        ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list
> > >
> > > > > >        https://stat.ethz.ch/mailman/listinfo/r-help
> > > >        > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html
> > > >        > > and provide commented, minimal, self-contained, reproducible        code.
> > > > > >
> > > >        >
> > > >
> > > >       [[alternative        HTML version deleted]]
> > > >
> > > >        ______________________________________________
> > >
> > > > R-help at stat.math.ethz.ch  mailing list
> > >
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > >        PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and        provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > >
> > >
> > >
> > >
> >
> >
>


From daniil.ivanov at gmail.com  Mon Aug 21 00:53:07 2006
From: daniil.ivanov at gmail.com (Daniil Ivanov)
Date: Mon, 21 Aug 2006 01:53:07 +0300
Subject: [R] plot problem
In-Reply-To: <d80e51a50608201515y2bb8b598pbeab1cc275110f76@mail.gmail.com>
References: <d80e51a50608201345y4dc2ab99ld4dc7d3a4a2a1b2a@mail.gmail.com>
	<Pine.LNX.4.64.0608202159560.18139@gannet.stats.ox.ac.uk>
	<d80e51a50608201515y2bb8b598pbeab1cc275110f76@mail.gmail.com>
Message-ID: <d80e51a50608201553p15e9df50x28ab3c822bae2eb5@mail.gmail.com>

Ok, what is wrong with a following code:

# remove all the present objects
rm(list = ls())

# load the libraries we need
library(gstat)

data(meuse)
vgm1 <- variogram(log(zinc)~1, ~x+y, meuse)
plot(vgm1)
dev.copy2eps(file="fig2.eps",horizontal=T)
dev.off()

it plots nothing
but from the R console

plot(vgm1)

gives me a plot.

Thanks, Daniil.

On 8/21/06, Daniil Ivanov <daniil.ivanov at gmail.com> wrote:
> Hi,
>
> Ok, thanks to all.
> Problem was with class of variogram
>
> > class(vgm1)
> [1] "gstatVariogram" "data.frame"
>
> If I fix it manually to
>
> class(vgm1) <- "gstatVariogram"
>
> everything runs as it should.
>
> Thanks, Daniil.


From jesse.canchola.b at bayer.com  Mon Aug 21 01:32:12 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Sun, 20 Aug 2006 19:32:12 -0400
Subject: [R] Permutations with replacement
In-Reply-To: <F9F2A641C593D7408925574C05A7BE770D1A50@rhopost.rhotrading.com>
Message-ID: <OF797249D7.B7559BC9-ON882571CE.00784D9F-882571D0.0081437A@bayer.com>

Thanks, David.  That worked fabulously! 

Here is the R code for the hypercube test example: 

########## begin R code ############
library(combinat)
x <- rep(3,3)   # for partitions of 3 units into the three classes {1,2,3} 

hcube(x, scale=1, transl=0) 
########### end R code ############

For the larger one I want (i.e., 8^8), I will take a random sample of 
10,000 from the 16,777,216 possibilities.

Regards,
Jesse Canchola




<davidr at rhotrading.com> 
Sent by: r-help-bounces at stat.math.ethz.ch
08/18/2006 01:33 PM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>, "r-help" 
<r-help at stat.math.ethz.ch>
cc

Subject
Re: [R] Permutations with replacement






If you also want 1,1,1 and so on, the number of these is n^n,
(n choices for each of n slots.)
In that case, you could use hcube from combinat.

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jesse Albert
Canchola
Sent: Friday, August 18, 2006 3:26 PM
To: r-help
Subject: [R] Permutations with replacement

Is there a simple function or process that will create a matrix of 
permutations with replacement? 

I know that using the combinat package

###### begin R code ######
> library(combinat)
> m <- t(array(unlist(permn(3)), dim = c(3, 6)))

# we can get the permutations, for example 3!=6
# gives us

> m
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    3    2
[3,]    3    1    2
[4,]    3    2    1
[5,]    2    3    1
[6,]    2    1    3
###### end R code ##########

I'd like to include the "with replacement possibilities" such as 

1,1,3
1,1,2
2,3,3

and so on.  This will eventually be done on 8!=40,320 rather than the 
development version using 3! as above.

If no function exists (I've Googled on CRAN with no palpable luck), then

perhaps this is more of a bootstrap type problem. 

Thanks for your help in advance,
Jesse Canchola










________________________________________________________________________
_______________________

The information contained in this e-mail is for the exclusive use of the
intended recipient(s) and may be confidential, proprietary, and/or
legally privileged.  Inadvertent disclosure of this message does not
constitute a waiver of any privilege.  If you receive this message in
error, please do not directly or indirectly use, print, copy, forward,
or disclose any part of this message.  Please also delete this e-mail
and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Mon Aug 21 04:06:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 20 Aug 2006 22:06:21 -0400
Subject: [R] Grid Points
In-Reply-To: <20060820173053.43085.qmail@web52207.mail.yahoo.com>
References: <971536df0608200628l6f05d4cetd284a6421fc79e14@mail.gmail.com>
	<20060820173053.43085.qmail@web52207.mail.yahoo.com>
Message-ID: <971536df0608201906r34cd227ct92587350ddf090d7@mail.gmail.com>

That's the default.  See the relation subargument to scales
if you want them different.

e.g.

library(lattice)
y <- c(601:700, seq(6510,7000, by=10))
x <- c(601:700, 601:650)
g <- rep(1:2, c(100, 50))
xyplot(y ~ x | g)


On 8/20/06, Anupam Tyagi <anuptyagi at yahoo.com> wrote:
> Thanks. How do I retain the same scale of grid.points
> from one panel to next even if the scale of the data
> changes? For example: c(seq(601:700),seq(6510,7000,
> by=10)) ~ seq(601:700) | gl(2,50).
>
>
> --- Gabor Grothendieck <ggrothendieck at gmail.com>
> wrote:
>
> > Try this.  gl(2,50) is such that the first 50 points
> > are series 1
> > and the second 50 points are series 2.  The scales=
> > argument
> > defines the positions of the tick marks and the
> > xlim= argument
> > defines the x axis limits.  The layout puts the
> > panels on top
> > of each other rather than side by side.  strip =
> > FALSE eliminates
> > the strip above each panel.  type= says we want
> > lines.  The
> > panel function puts points at the grid locations in
> > each panel
> > and then calls xyplot to plot the lines.
> >
> > library(lattice)
> > library(grid)
> >
> > x <- 601:700
> > at <- seq(500, 800, 50)
> > xyplot(x ~ x | gl(2, 50), scales = list(at = at),
> > xlim = c(500, 700),
> >   layout = 1:2, strip = FALSE, type = "l",
> >   panel = function(...) {
> >     grid.points(at, unit(rep(.01, length(at)),
> > "npc"),
> >       pch = 20, size = unit(.2, "char"))
> >     panel.xyplot(...)
> > })
> >
> >
> > On 8/20/06, Anupam Tyagi <AnupTyagi at yahoo.com>
> > wrote:
> > > How do I put grid points (not grid lines) as the
> > base layer of an xyplot?
> > >
> > > Is there a way to vary the interval at which x and
> > y grid points are placed?
> > >
> > > Is it possible to start a graph so that Y axis
> > begins at 500 and ends at 800? I
> > > am only interested in focusing on the relative
> > distance between the points whose
> > > values are between 500 and 800, but not their
> > relative distance from zero.
> > >
> > > Is there a way in R to draw two graphs so that
> > that "share" the same X axis, but
> > > without the gap that mfrow() creates? The origin
> > of the top graph should be
> > > where the Y-axis of the bottom graph ends. It will
> > be useful for what I am
> > > trying to do to have the X axis of the top graph
> > be invisible.
> > >
> > > Anupam.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code.
> > >
> >
>
>
> __________________________________________________
> Do You Yahoo!?
> Tired of spam?  Yahoo! Mail has the best spam protection around
> http://mail.yahoo.com
>


From researchjj at gmail.com  Mon Aug 21 08:18:19 2006
From: researchjj at gmail.com (j.joshua thomas)
Date: Mon, 21 Aug 2006 14:18:19 +0800
Subject: [R] Clique-Method-Package-Help
In-Reply-To: <b4485c4c0607130833j69352e61qbcb87a012409d44e@mail.gmail.com>
References: <b4485c4c0607130833j69352e61qbcb87a012409d44e@mail.gmail.com>
Message-ID: <b4485c4c0608202318g4c8cbb25oe450c9879a7bfb4c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/24abfad9/attachment.pl 

From researchjj at gmail.com  Mon Aug 21 08:19:48 2006
From: researchjj at gmail.com (j.joshua thomas)
Date: Mon, 21 Aug 2006 14:19:48 +0800
Subject: [R] Clique technique-Package
Message-ID: <b4485c4c0608202319w54601dd1g2f98491b6c0f408c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/928c4f56/attachment.pl 

From Roger.Bivand at nhh.no  Mon Aug 21 08:40:14 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 21 Aug 2006 08:40:14 +0200 (CEST)
Subject: [R] plot problem
In-Reply-To: <d80e51a50608201553p15e9df50x28ab3c822bae2eb5@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0608210836140.9031-100000@reclus.nhh.no>

On Mon, 21 Aug 2006, Daniil Ivanov wrote:

> Ok, what is wrong with a following code:
> 
> # remove all the present objects
> rm(list = ls())
> 
> # load the libraries we need
> library(gstat)
> 
> data(meuse)
> vgm1 <- variogram(log(zinc)~1, ~x+y, meuse)
> plot(vgm1)
> dev.copy2eps(file="fig2.eps",horizontal=T)
> dev.off()
> 
> it plots nothing
> but from the R console

gstat:::plot.gstatVariogram uses xyplot, a lattice graphics method, to 
make it easy to plot directional variograms. So please put print() round 
the plot(), see FAQ 7.22. 

> 
> plot(vgm1)
> 
> gives me a plot.
> 
> Thanks, Daniil.
> 
> On 8/21/06, Daniil Ivanov <daniil.ivanov at gmail.com> wrote:
> > Hi,
> >
> > Ok, thanks to all.
> > Problem was with class of variogram
> >
> > > class(vgm1)
> > [1] "gstatVariogram" "data.frame"
> >
> > If I fix it manually to
> >
> > class(vgm1) <- "gstatVariogram"
> >
> > everything runs as it should.
> >
> > Thanks, Daniil.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From daniil.ivanov at gmail.com  Mon Aug 21 09:16:07 2006
From: daniil.ivanov at gmail.com (Daniil Ivanov)
Date: Mon, 21 Aug 2006 10:16:07 +0300
Subject: [R] plot problem
In-Reply-To: <Pine.LNX.4.44.0608210836140.9031-100000@reclus.nhh.no>
References: <d80e51a50608201553p15e9df50x28ab3c822bae2eb5@mail.gmail.com>
	<Pine.LNX.4.44.0608210836140.9031-100000@reclus.nhh.no>
Message-ID: <d80e51a50608210016n432b9ef4ncb7e32a6a2a70b05@mail.gmail.com>

Hi,

 thank you very much for your help.
 In examples of gstat manual it was no
 print(). I thought it is like in Matlab, you take
 an example from the manual and it works.
 Silly me ;)

Thanks, Daniil.

On 8/21/06, Roger Bivand <Roger.Bivand at nhh.no> wrote:
> On Mon, 21 Aug 2006, Daniil Ivanov wrote:
>
> > Ok, what is wrong with a following code:
> >
> > # remove all the present objects
> > rm(list = ls())
> >
> > # load the libraries we need
> > library(gstat)
> >
> > data(meuse)
> > vgm1 <- variogram(log(zinc)~1, ~x+y, meuse)
> > plot(vgm1)
> > dev.copy2eps(file="fig2.eps",horizontal=T)
> > dev.off()
> >
> > it plots nothing
> > but from the R console
>
> gstat:::plot.gstatVariogram uses xyplot, a lattice graphics method, to
> make it easy to plot directional variograms. So please put print() round
> the plot(), see FAQ 7.22.
>
> >
> > plot(vgm1)
> >
> > gives me a plot.
> >
> > Thanks, Daniil.
> >
> > On 8/21/06, Daniil Ivanov <daniil.ivanov at gmail.com> wrote:
> > > Hi,
> > >
> > > Ok, thanks to all.
> > > Problem was with class of variogram
> > >
> > > > class(vgm1)
> > > [1] "gstatVariogram" "data.frame"
> > >
> > > If I fix it manually to
> > >
> > > class(vgm1) <- "gstatVariogram"
> > >
> > > everything runs as it should.
> > >
> > > Thanks, Daniil.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
>
>


From s.martino at tno.it  Mon Aug 21 09:23:56 2006
From: s.martino at tno.it (Sergio Martino)
Date: Mon, 21 Aug 2006 09:23:56 +0200
Subject: [R] How to share variables
References: <005d01c6b636$24f1e680$3c32428a@msi>
	<971536df0608020650vbc87b7n3daa28acfc792900@mail.gmail.com>
Message-ID: <003901c6c4f2$c63ddab0$3c32428a@msi>

Thanks for your fast replay and sorry for my late one (I was on holidays)

The structure I would like to emulate (the fortran common statement) is a
different from what you are describing.

The examples "scoping" and the links to OO programming show the use of local
variable which are tied to the object itself.

My need is to have a group of (big) variable shared among some function in a
way that they can be accessed freely.

More useful is the environment example you reported. The only problem is
that it seems I need to replace the whole environment of the function. What
I need to investigate is where the variables created inside the inner
function (myfun) go. If they belong to the enviroment (e) I will get a
mix-up of variables with side effect (variables with the same name) and it
will be a pain.

If I can use inside myfun the variable as e$dat (without changing the
enviroment (no environment(myfun) <- e statement)) than it will be ok.

I need to experiment a little bit.

Sergio




> On 8/2/06, Sergio Martino <s.martino at tno.it> wrote:
> > Hi,
> >
> > I would like to realize in R a structure like the fortran common ie a
way to
> > declare some variable that can only be accessed by all the functions
which
> > need to.
> >
> > Browsing the archive it seems that the simplest way is to declare the
> > variables and the functions in a big function which wraps all. But this
is
> > impratical when the functions are big.
>
> There is a demonstration of that found by issuing the command:
>
> demo(scoping)
>
> >
> > The environments seems to do the trick but I am not enough familiar with
> > them to make my ways out.
>
> Yes place your data in an environment as shown and then for
> each function that is to access the environment should have
> its environment set accordingly:
>
> e <- new.env()
> e$dat <- 1:3
> myfun <- function(x) sum(x + dat)
> environment(myfun) <- e
> myfun(10)  # fun can access dat
>
> Realize that what you are trying to do is to create a sort of object
> oriented structure with the data being the objects and the functions
> being the methods.  The proto package provides some functionality
> to implement that and also supports delegation (similar to
> inheritance):
>
> library(proto)
> package?proto # all sources of info on proto
>
> # example - create proto object p with some data dat and a method fun
> p <- proto(dat = 1:3, fun = function(., x) sum(x + .$dat))
>
> # invoke method
> p$fun(10)  # runs fun.  fun has access to dat
>
> # create a child q of p and run fun
> # q overrides dat with its own dat while inheriting fun
> q <- p$proto(dat = 4:6)
> q$fun(10)
>
> Another possibility would be to look at the R.oo package which is
> another object oriented infrastructure based on environments.
>
> >
> > Is there any example or pointers to easy but complete environment usage?
> >
> > Thanks in Advance
> >
> > Sergio Martino


From ggrothendieck at gmail.com  Mon Aug 21 09:59:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 03:59:07 -0400
Subject: [R] How to share variables
In-Reply-To: <003901c6c4f2$c63ddab0$3c32428a@msi>
References: <005d01c6b636$24f1e680$3c32428a@msi>
	<971536df0608020650vbc87b7n3daa28acfc792900@mail.gmail.com>
	<003901c6c4f2$c63ddab0$3c32428a@msi>
Message-ID: <971536df0608210059w1c9e85a7h640290ddbd8de997@mail.gmail.com>

On 8/21/06, Sergio Martino <s.martino at tno.it> wrote:
> Thanks for your fast replay and sorry for my late one (I was on holidays)
>
> The structure I would like to emulate (the fortran common statement) is a
> different from what you are describing.
>
> The examples "scoping" and the links to OO programming show the use of local
> variable which are tied to the object itself.
>
> My need is to have a group of (big) variable shared among some function in a
> way that they can be accessed freely.
>
> More useful is the environment example you reported. The only problem is
> that it seems I need to replace the whole environment of the function. What
> I need to investigate is where the variables created inside the inner
> function (myfun) go. If they belong to the enviroment (e) I will get a
> mix-up of variables with side effect (variables with the same name) and it
> will be a pain.

Each time myfun is run a new environment is created to hold
its local variables.  The parent of that environment is e in
this example by construction.  So e and the environment that
is temporarily created to hold myfun's variables are distinct.

Adding a couple statements to display what is in each might
help clarify this:

> e <- new.env()
> e$dat <- 1:3
> myfun <- function(x) {
+ cat("In current env:", ls(), "\n")
+ cat("In parent env:", ls(parent.env(environment())), "\n")
+ sum(x + dat)
+ }
> environment(myfun) <- e
> myfun(10)  # fun can access dat
In current env: x
In parent env: dat
[1] 36


>
> If I can use inside myfun the variable as e$dat (without changing the
> enviroment (no environment(myfun) <- e statement)) than it will be ok.

Yes you can.  You can either make sure that e is visible to myfun
via normal scoping rules or pass it explicitly:

e <- new.env()
e$dat <- 1:3
myfun <- function(x) sum(x + e$dat)
myfun(10)

# or passing e explicitly

myfun2 <- function(x, e) sum(x + e$dat)
myfun2(10, e)

# or using the proto package:to define e
# and the same myfun and myfun2

library(proto)
e <- proto(dat = 1:3)
myfun(10)
myfun2(10, p)

In the above examples we used environments but these are simple
enough that we could have used lists:

e <- list(dat = 1:3)
myfun(10)  # relying on scope rules
myfun2(10, e)  # passing explicitly

>
> I need to experiment a little bit.
>
> Sergio
>
>
>
>
> > On 8/2/06, Sergio Martino <s.martino at tno.it> wrote:
> > > Hi,
> > >
> > > I would like to realize in R a structure like the fortran common ie a
> way to
> > > declare some variable that can only be accessed by all the functions
> which
> > > need to.
> > >
> > > Browsing the archive it seems that the simplest way is to declare the
> > > variables and the functions in a big function which wraps all. But this
> is
> > > impratical when the functions are big.
> >
> > There is a demonstration of that found by issuing the command:
> >
> > demo(scoping)
> >
> > >
> > > The environments seems to do the trick but I am not enough familiar with
> > > them to make my ways out.
> >
> > Yes place your data in an environment as shown and then for
> > each function that is to access the environment should have
> > its environment set accordingly:
> >
> > e <- new.env()
> > e$dat <- 1:3
> > myfun <- function(x) sum(x + dat)
> > environment(myfun) <- e
> > myfun(10)  # fun can access dat
> >
> > Realize that what you are trying to do is to create a sort of object
> > oriented structure with the data being the objects and the functions
> > being the methods.  The proto package provides some functionality
> > to implement that and also supports delegation (similar to
> > inheritance):
> >
> > library(proto)
> > package?proto # all sources of info on proto
> >
> > # example - create proto object p with some data dat and a method fun
> > p <- proto(dat = 1:3, fun = function(., x) sum(x + .$dat))
> >
> > # invoke method
> > p$fun(10)  # runs fun.  fun has access to dat
> >
> > # create a child q of p and run fun
> > # q overrides dat with its own dat while inheriting fun
> > q <- p$proto(dat = 4:6)
> > q$fun(10)
> >
> > Another possibility would be to look at the R.oo package which is
> > another object oriented infrastructure based on environments.
> >
> > >
> > > Is there any example or pointers to easy but complete environment usage?
> > >
> > > Thanks in Advance
> > >
> > > Sergio Martino
>
>


From ggrothendieck at gmail.com  Mon Aug 21 10:02:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 04:02:08 -0400
Subject: [R] How to share variables
In-Reply-To: <971536df0608210059w1c9e85a7h640290ddbd8de997@mail.gmail.com>
References: <005d01c6b636$24f1e680$3c32428a@msi>
	<971536df0608020650vbc87b7n3daa28acfc792900@mail.gmail.com>
	<003901c6c4f2$c63ddab0$3c32428a@msi>
	<971536df0608210059w1c9e85a7h640290ddbd8de997@mail.gmail.com>
Message-ID: <971536df0608210102x27d3f997tf5a6ec544ef910a3@mail.gmail.com>

On 8/21/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 8/21/06, Sergio Martino <s.martino at tno.it> wrote:
> > Thanks for your fast replay and sorry for my late one (I was on holidays)
> >
> > The structure I would like to emulate (the fortran common statement) is a
> > different from what you are describing.
> >
> > The examples "scoping" and the links to OO programming show the use of local
> > variable which are tied to the object itself.
> >
> > My need is to have a group of (big) variable shared among some function in a
> > way that they can be accessed freely.
> >
> > More useful is the environment example you reported. The only problem is
> > that it seems I need to replace the whole environment of the function. What
> > I need to investigate is where the variables created inside the inner
> > function (myfun) go. If they belong to the enviroment (e) I will get a
> > mix-up of variables with side effect (variables with the same name) and it
> > will be a pain.
>
> Each time myfun is run a new environment is created to hold
> its local variables.  The parent of that environment is e in
> this example by construction.  So e and the environment that
> is temporarily created to hold myfun's variables are distinct.
>
> Adding a couple statements to display what is in each might
> help clarify this:
>
> > e <- new.env()
> > e$dat <- 1:3
> > myfun <- function(x) {
> + cat("In current env:", ls(), "\n")
> + cat("In parent env:", ls(parent.env(environment())), "\n")
> + sum(x + dat)
> + }
> > environment(myfun) <- e
> > myfun(10)  # fun can access dat
> In current env: x
> In parent env: dat
> [1] 36
>
>
> >
> > If I can use inside myfun the variable as e$dat (without changing the
> > enviroment (no environment(myfun) <- e statement)) than it will be ok.
>
> Yes you can.  You can either make sure that e is visible to myfun
> via normal scoping rules or pass it explicitly:
>
> e <- new.env()
> e$dat <- 1:3
> myfun <- function(x) sum(x + e$dat)
> myfun(10)
>
> # or passing e explicitly
>
> myfun2 <- function(x, e) sum(x + e$dat)
> myfun2(10, e)
>
> # or using the proto package:to define e
> # and the same myfun and myfun2
>
> library(proto)
> e <- proto(dat = 1:3)
> myfun(10)
> myfun2(10, p)

Typo. That was supposed to be myfun2(10, e)

>
> In the above examples we used environments but these are simple
> enough that we could have used lists:
>
> e <- list(dat = 1:3)
> myfun(10)  # relying on scope rules
> myfun2(10, e)  # passing explicitly
>
> >
> > I need to experiment a little bit.
> >
> > Sergio
> >
> >
> >
> >
> > > On 8/2/06, Sergio Martino <s.martino at tno.it> wrote:
> > > > Hi,
> > > >
> > > > I would like to realize in R a structure like the fortran common ie a
> > way to
> > > > declare some variable that can only be accessed by all the functions
> > which
> > > > need to.
> > > >
> > > > Browsing the archive it seems that the simplest way is to declare the
> > > > variables and the functions in a big function which wraps all. But this
> > is
> > > > impratical when the functions are big.
> > >
> > > There is a demonstration of that found by issuing the command:
> > >
> > > demo(scoping)
> > >
> > > >
> > > > The environments seems to do the trick but I am not enough familiar with
> > > > them to make my ways out.
> > >
> > > Yes place your data in an environment as shown and then for
> > > each function that is to access the environment should have
> > > its environment set accordingly:
> > >
> > > e <- new.env()
> > > e$dat <- 1:3
> > > myfun <- function(x) sum(x + dat)
> > > environment(myfun) <- e
> > > myfun(10)  # fun can access dat
> > >
> > > Realize that what you are trying to do is to create a sort of object
> > > oriented structure with the data being the objects and the functions
> > > being the methods.  The proto package provides some functionality
> > > to implement that and also supports delegation (similar to
> > > inheritance):
> > >
> > > library(proto)
> > > package?proto # all sources of info on proto
> > >
> > > # example - create proto object p with some data dat and a method fun
> > > p <- proto(dat = 1:3, fun = function(., x) sum(x + .$dat))
> > >
> > > # invoke method
> > > p$fun(10)  # runs fun.  fun has access to dat
> > >
> > > # create a child q of p and run fun
> > > # q overrides dat with its own dat while inheriting fun
> > > q <- p$proto(dat = 4:6)
> > > q$fun(10)
> > >
> > > Another possibility would be to look at the R.oo package which is
> > > another object oriented infrastructure based on environments.
> > >
> > > >
> > > > Is there any example or pointers to easy but complete environment usage?
> > > >
> > > > Thanks in Advance
> > > >
> > > > Sergio Martino
> >
> >
>


From goran.brostrom at gmail.com  Mon Aug 21 11:12:49 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Mon, 21 Aug 2006 11:12:49 +0200
Subject: [R] [R-pkgs] New version of glmmML
Message-ID: <148ed8180608210212k6a7858d9s3e01011588fec2ec@mail.gmail.com>

A new version, 0.65-1, of glmmML is now on CRAN. It is a major rewrite
of the inner structures, so frequent updates (bug fixes) may be
expected for some time.

News:

* The Laplace and adaptive Gauss-Hermite approximations to the log
likelihood function are fully implemented. The Laplace method is made
the default. It should give results you can compare to the results
from 'lmer' (for the models that glmmML can handle).

* Binomial responses can now be represented as a two-column matrix
with No. of successes and No. of failures, respectively, as in glm.

* New functions: 'ghq' for calculating the constants used in the
Gauss-Hermite quadrature. 'extractAIC.glmmML', which makes it possible
to use functions like 'dropterm' (MASS) on glmmML fits.

* There are three choices of distribution for the random effects:
'gaussian' (default), 'logistic', and 'cauchy'.

* The 'conditional' bootstrap is removed, so the only choice now is
the parametric bootstrap.

* The 'posterior.means' are no longer calculated, leaving only the
'posterior.modes' as 'predictions' of the random effects.

As usual, feedback is more than welcome!

G?ran

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From dusa.adrian at gmail.com  Mon Aug 21 14:06:09 2006
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Mon, 21 Aug 2006 15:06:09 +0300
Subject: [R] test the tcltk package
Message-ID: <200608211506.09949.dusa.adrian@gmail.com>

Dear all,

Could anybody using (K)ubuntu (or Linux in general) confirm if this is a 
general problem or it's just my box?
The problem relates to the Options window in the Rcmdr package (which it looks 
fine in John Fox's Quantian). The last option (Default font) is stubborn and 
won't be set; it behaves strangely (e.g. I type 3 and it appears 2).
This options should have a button to drag left and right, but in my 
configuration this is missing; at this link you can see how it looks like:
http://www.roda.ro/Options.png

What should I do to test if the R tcltk package works fine?

I have R 2.3.1, tcl and tk version 8.4 (dev packages installed as well).

Thanks in advance,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From carlos.riveira at yahoo.com  Mon Aug 21 14:03:50 2006
From: carlos.riveira at yahoo.com (carlos riveira)
Date: Mon, 21 Aug 2006 05:03:50 -0700 (PDT)
Subject: [R] Finney's fiducial confidence intervals of LD50
Message-ID: <20060821120350.89992.qmail@web39814.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/398df40f/attachment.pl 

From YCH at softcomputing.com  Mon Aug 21 14:17:11 2006
From: YCH at softcomputing.com (Yohan CHOUKROUN)
Date: Mon, 21 Aug 2006 14:17:11 +0200
Subject: [R] RE :  test the tcltk package
Message-ID: <C8F48FD780E12D4DB197507B897D00DD072F5B92@ntexch.softcomputing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/7fabb24c/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Aug 21 14:17:59 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 21 Aug 2006 14:17:59 +0200
Subject: [R] [R-pkgs] New version of glmmML
In-Reply-To: <148ed8180608210212k6a7858d9s3e01011588fec2ec@mail.gmail.com>
References: <148ed8180608210212k6a7858d9s3e01011588fec2ec@mail.gmail.com>
Message-ID: <17641.42103.990929.53681@stat.math.ethz.ch>

Hi G?ran,

>>>>> "GB" == G?ran Brostr?m <goran.brostrom at gmail.com>
>>>>>     on Mon, 21 Aug 2006 11:12:49 +0200 writes:

    GB> A new version, 0.65-1, of glmmML is now on CRAN. It is a major rewrite
    GB> of the inner structures, so frequent updates (bug fixes) may be
    GB> expected for some time.


    GB> News:

    [............]

Sorry for my lazy question :

   What does the package do that lmer() does not?

I do know that you are a smart person so I know I will get a non-trivial
answer.  The main reason for this e-mail is really to encourage you (and
all other 'R-packages' posters) to do the following in each
posting to R-packages:

1) [One paragraph:]
  What does the package do // why do you see a need for the
  package // what's the bigger context ?

2) [another paragraph (or two...):]
  News  etc

Since the R-packages archives are relatively prominently listed
as part of "News" on the R project website, (and also since I
encourage ``everyone'' to subscribe to R-packages,)

I think the extra information in "1)" above would be appropriate
for every posting to R-packages.

Martin Maechler, ETH Zurich


From renaud.lancelot at gmail.com  Mon Aug 21 16:02:26 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Mon, 21 Aug 2006 16:02:26 +0200
Subject: [R] Finney's fiducial confidence intervals of LD50
In-Reply-To: <20060821120350.89992.qmail@web39814.mail.mud.yahoo.com>
References: <20060821120350.89992.qmail@web39814.mail.mud.yahoo.com>
Message-ID: <c2ee56800608210702r3c8353baudce29bf9081ec4a5@mail.gmail.com>

I don't know what Finney's fiducial confidence interval is but if your
problem is to handle the output of dose.p (from MASS), you can do as
follows:

> library(MASS)
> Response <- c(0, 7, 26, 27, 0, 5, 13, 29, 0, 4, 11, 25)
> Tot <- rep(30.5, 12)
> Dose <- rep(c(10, 40, 160, 640), 3)
> fm <- glm(Response/Tot ~ log10(Dose), family = quasibinomial(link = probit))
> logD50 <- dose.p(fm, cf = 1:2, p = 0.5)
> D50 <- 10^c(logD50 + c(1, -1.96, 1.96) * attr(logD50, "SE"))
> names(D50) <- c("D50", "lower", "upper")
> D50
     D50    lower    upper
164.9506 103.3171 191.9777

Best,

Renaud

2006/8/21, carlos riveira <carlos.riveira at yahoo.com>:
>   I am working with Probit regression (I cannot switch to logit) can anybody help me in finding out how to obtain with R Finney's fiducial confidence intervals for the levels of the predictor (Dose) needed to produce a proportion of 50% of responses(LD50, ED50 etc.)?
>   If the Pearson chi-square goodness-of-fit test is significant (by default), a heterogeneity factor should be used to calculate the limits.
>
>   Response<-c(0,7,26,27,0,5,13,29,0,4,11,25)
>   Tot<-rep(30.5,12)
>   Dose<-rep(c(10,40,160,640),3)
>   probit<-glm(formula = Response/Tot~ log10(Dose), family=quasibinomial
>   (link=probit))
>   D50<- round(10^(dose.p(probit,cf=1:2,p=0.5)))
>
>   #This is what SPSS calculates. I would like to reproduce these results with R:
>   #SPSS RESULTS:
>   #PRNT50= 140,83525
>   #CI = 98,37857;205,34483
>   #Regr.coeff= 1,91676 (S.E.=0,16765)
>   #Intercept=-4,11856 (S.E.=0,36355)
>   Thanks a lot for your help.
>
>   Carlos
>
>  __________________________________________________
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From thills at indiana.edu  Mon Aug 21 16:05:26 2006
From: thills at indiana.edu (Thomas Hills)
Date: Mon, 21 Aug 2006 10:05:26 -0400
Subject: [R] interpreting coxph results
Message-ID: <BB390058-96DE-4701-A595-992EF1433614@indiana.edu>

I am having trouble understanding results I'm getting back from coxph  
doing a recurrent event analysis.  I've included the model below and  
the summary.  In some cases, with minor variations, the Robust  
variance and Wald tests are significant, but the individual  
covariates may or may not be significant.  My main question is:  If  
Wald and robust tests both take into account the clustering, then why  
are they so different and how do I make sense of them.  A second  
question is:  If Wald and Robust are both significant in the summary  
tests, but all individual covariates are insignificant (these are  
Wald, yes?), what do I make of that?  I recognize the questions are  
partly R related and partly statistical (if there is a better place  
to post this please let me know).

Call:
coxph(formula = Surv(startt, stopt, rep(1, nrow(omfi))) ~ joof1 +
     topslope1 * top1 + I(early.angle/late.angle) + spac.cov +
     ave.angle + slopef.d + cluster(id) + strata(sequence), data =  
thedofile))

   n= 174
                              coef exp(coef) se(coef) robust se       
z    p
joof1                     -0.2755  7.59e-01   0.1590    0.2998 -0.919  
0.36
topslope1                 30.9827  2.86e+13  23.2339   51.9948  0.596  
0.55
top1                       0.1165  1.12e+00   0.1901    0.3951  0.295  
0.77
I(early.angle/late.angle)  0.0449  1.05e+00   0.1165    0.1296  0.347  
0.73
spac.cov                   0.9815  2.67e+00   3.4104    5.5871  0.176  
0.86
ave.angle                  0.0396  1.04e+00   0.0156    0.0266  1.488  
0.14
slopef.d                  -0.3394  7.12e-01   0.4373    0.8891 -0.382  
0.70
topslope1:top1            -5.5673  3.82e-03   2.8198    6.7696 -0.822  
0.41


Rsquare= 0.18   (max possible= 0.898 )
Likelihood ratio test= 34.5  on 8 df,   p=3.27e-05
Wald test            = 23.5  on 8 df,   p=0.00276
Score (logrank) test = 31.8  on 8 df,   p=0.000103,   Robust = 13.5   
p=0.097

   (Note: the likelihood ratio and score tests assume independence of
      observations within a cluster, the Wald and robust score tests  
do not).

Thanks for any help,

Thomas Hills
Indiana University


From dieter.menne at menne-biomed.de  Mon Aug 21 16:14:15 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 21 Aug 2006 14:14:15 +0000 (UTC)
Subject: [R] Finney's fiducial confidence intervals of LD50
References: <20060821120350.89992.qmail@web39814.mail.mud.yahoo.com>
Message-ID: <loom.20060821T161203-293@post.gmane.org>

carlos riveira <carlos.riveira <at> yahoo.com> writes:

> 
> I am working with Probit regression (I cannot switch to logit) can anybody 
> help me in finding out how to  obtain with R Finney's fiducial confidence 
> intervals for the levels of the predictor (Dose) needed to
> produce a proportion of 50% of responses(LD50, ED50 etc.)? 

See the example in MASS on <dose.p>. Don't know if it is comes out the same as
SPSS, though.

ldose <- rep(0:5, 2)
numdead <- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex <- factor(rep(c("M", "F"), c(6, 6)))
SF <- cbind(numdead, numalive = 20 - numdead)
budworm.lg0 <- glm(SF ~ sex + ldose - 1, family = binomial)

dose.p(budworm.lg0, cf = c(1,3), p = 1:3/4)
dose.p(update(budworm.lg0, family = binomial(link=probit)),
       cf = c(1,3), p = 1:3/4)

Dieter


From rgentlem at fhcrc.org  Mon Aug 21 16:15:32 2006
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Mon, 21 Aug 2006 07:15:32 -0700
Subject: [R] Clique technique-Package
In-Reply-To: <b4485c4c0608202319w54601dd1g2f98491b6c0f408c@mail.gmail.com>
References: <b4485c4c0608202319w54601dd1g2f98491b6c0f408c@mail.gmail.com>
Message-ID: <44E9C004.5040606@fhcrc.org>

the graph, RBGL and Rgraphviz packages at www.bioconductor.org have a 
fairly substantial implementation of many different graph 
representations, algorithms (including clique finding) and layout
algorithms


j.joshua thomas wrote:
> Dear R Users,
> 
>  I am looking clique a graph technique, to identify the values from the
>    dataset.
>    I tried with help.search("graph") it show's the graphics related stuff
>    Is there any package that i can use to find Clique in a dataset?
>    Thanks in Advance
>  JJ
> 
> 
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From mr.blacksheep at gmail.com  Mon Aug 21 16:16:06 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Mon, 21 Aug 2006 08:16:06 -0600
Subject: [R] string-to-number
In-Reply-To: <1156083806.4308.70.camel@localhost.localdomain>
References: <066501c6c386$c9cd8080$6600a8c0@DD4XFW31>
	<1155989485.4308.7.camel@localhost.localdomain>
	<Pine.LNX.4.64.0608191322510.12049@gannet.stats.ox.ac.uk>
	<1155992326.4308.15.camel@localhost.localdomain>
	<46a360560608190925u432b33a8rcdda2c12276f905a@mail.gmail.com>
	<1156083806.4308.70.camel@localhost.localdomain>
Message-ID: <46a360560608210716t23d50fet351a1cae4c00ae2@mail.gmail.com>

Marc,

Thanks very much for this.  I hadn't really looked at Rprof in the
past; now I have a new toy to play with!

I have formulated an hypothesis that the reason parse/eval is quicker
lies in the pattern-matching code:  strsplit is using regular
expressions, whereas perhaps parse is using some more clever (but
possibly less general) matching algorithm.  It will be interesting to
inspect the source code to get to the bottom of it.

Thanks again for your interest and efforts in this, and for pointing out Rprof!

Regards,

Mike Nielsen

On 8/20/06, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On Sat, 2006-08-19 at 10:25 -0600, Mike Nielsen wrote:
> > Wow.  New respect for parse/eval.
> >
> > Do you think this is a special case of a more general principle?  I
> > suppose the cost is memory, but from time to time a speedup like this
> > would be very beneficial.
> >
> > Any hints about how R programmers could recognize such cases would, I
> > am sure, be of value to the list in general.
> >
> > Many thanks for your efforts, Marc!
>
> Mike,
>
> I think that one needs to consider where the time is being spent and
> then adjust accordingly. Once you understand that, you can develop some
> insight into what may be a more efficient approach. R provides good
> profiling tools that facilitate this process.
>
> In this case, almost all of the time in the first two examples using
> strsplit(), is in that function:
>
> > repeated.measures.columns <- paste(1:100000, collapse = ",")
>
> > library(utils)
> > Rprof(tmp <- tempfile())
> > res1 <- as.numeric(unlist(strsplit(repeated.measures.columns, ",")))
> > Rprof()
>
> > summaryRprof(tmp)
> $by.self
>                     self.time self.pct total.time total.pct
> "strsplit"              23.68     99.7      23.68      99.7
> "as.double.default"      0.06      0.3       0.06       0.3
> "as.numeric"             0.00      0.0      23.74     100.0
> "unlist"                 0.00      0.0      23.68      99.7
>
> $by.total
>                     total.time total.pct self.time self.pct
> "as.numeric"             23.74     100.0      0.00      0.0
> "strsplit"               23.68      99.7     23.68     99.7
> "unlist"                 23.68      99.7      0.00      0.0
> "as.double.default"       0.06       0.3      0.06      0.3
>
> $sampling.time
> [1] 23.74
>
>
> Contrast that with Prof. Ripley's approach:
>
> > Rprof(tmp <- tempfile())
> > res3 <- eval(parse(text=paste("c(", repeated.measures.columns, ")")))
> > Rprof()
>
> > summaryRprof(tmp)
> $by.self
>         self.time self.pct total.time total.pct
> "parse"      0.42     87.5       0.42      87.5
> "eval"       0.06     12.5       0.48     100.0
>
> $by.total
>         total.time total.pct self.time self.pct
> "eval"        0.48     100.0      0.06     12.5
> "parse"       0.42      87.5      0.42     87.5
>
> $sampling.time
> [1] 0.48
>
>
> To some extent, one could argue that my initial timing examples are
> contrived, in that they specifically demonstrate a worst case scenario
> using strsplit().  Real world examples may or may not show such gains.
>
> For example with Charles' initial query, the initial vector was rather
> short:
>
>   > repeated.measures.columns
>   [1] "3,6,10"
>
> So if this was a one-time conversion, we would not see such significant
> gains.
>
> However, what if we had a long list of shorter entries:
>
> > repeated.measures.columns <- paste(1:10, collapse = ",")
> > repeated.measures.columns
> [1] "1,2,3,4,5,6,7,8,9,10"
>
> > big.list <- replicate(10000, list(repeated.measures.columns))
>
> > head(big.list)
> [[1]]
> [1] "1,2,3,4,5,6,7,8,9,10"
>
> [[2]]
> [1] "1,2,3,4,5,6,7,8,9,10"
>
> [[3]]
> [1] "1,2,3,4,5,6,7,8,9,10"
>
> [[4]]
> [1] "1,2,3,4,5,6,7,8,9,10"
>
> [[5]]
> [1] "1,2,3,4,5,6,7,8,9,10"
>
> [[6]]
> [1] "1,2,3,4,5,6,7,8,9,10"
>
>
> > system.time(res1 <- t(sapply(big.list, function(x)
> as.numeric(unlist(strsplit(x, ","))))))
> [1] 1.972 0.044 2.411 0.000 0.000
>
> > str(res1)
>  num [1:10000, 1:10] 1 1 1 1 1 1 1 1 1 1 ...
>
> > head(res1)
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> [1,]    1    2    3    4    5    6    7    8    9    10
> [2,]    1    2    3    4    5    6    7    8    9    10
> [3,]    1    2    3    4    5    6    7    8    9    10
> [4,]    1    2    3    4    5    6    7    8    9    10
> [5,]    1    2    3    4    5    6    7    8    9    10
> [6,]    1    2    3    4    5    6    7    8    9    10
>
>
>
> Now use Prof. Ripley's approach:
>
> > system.time(res3 <- t(sapply(big.list, function(x)
> eval(parse(text=paste("c(", x, ")"))))))
> [1] 1.676 0.012 1.877 0.000 0.000
>
> > str(res3)
>  num [1:10000, 1:10] 1 1 1 1 1 1 1 1 1 1 ...
>
> > head(res3)
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> [1,]    1    2    3    4    5    6    7    8    9    10
> [2,]    1    2    3    4    5    6    7    8    9    10
> [3,]    1    2    3    4    5    6    7    8    9    10
> [4,]    1    2    3    4    5    6    7    8    9    10
> [5,]    1    2    3    4    5    6    7    8    9    10
> [6,]    1    2    3    4    5    6    7    8    9    10
>
>
>
> > all(res1 == res3)
> [1] TRUE
>
>
> We do see a notable reduction in time with strsplit(), while a notable
> increase in time using eval(parse)), even though we are converting the
> same net number of values (100,000).
>
> Much of the increase with eval(parse()) is of course due to the overhead
> of sapply() and navigating the list.
>
>
> Let's increase the size of the list components to 1000:
>
> > repeated.measures.columns <- paste(1:1000, collapse = ",")
> > big.list <- replicate(10000, list(repeated.measures.columns))
>
> > system.time(res1 <- sapply(big.list, function(x)
> as.numeric(unlist(strsplit(x, ",")))))
> [1] 33.270  0.744 37.163  0.000  0.000
>
> > system.time(res3 <- t(sapply(big.list, function(x)
> eval(parse(text=paste("c(", x, ")"))))))
> [1] 15.893  0.928 18.139  0.000  0.000
>
>
> So we see here that as the size of the list components increases, there
> continues to be an advantage to Prof. Ripley's approach over using
> strsplit().
>
> Again, one needs to develop an understanding of where the time is spent
> in the processing by profiling and then consider how to introduce
> efficiencies, which in some cases may very well require the use of
> compiled C/FORTRAN as may be appropriate if times become too long.
>
> HTH,
>
> Marc Schwartz
>
>
>


-- 
Regards,

Mike Nielsen


From jmb56 at leicester.ac.uk  Mon Aug 21 16:29:10 2006
From: jmb56 at leicester.ac.uk (Bowden, J.M.)
Date: Mon, 21 Aug 2006 15:29:10 +0100
Subject: [R] R2WinBugs
Message-ID: <286C9166197E0C44B94FF9762B27DAC70AFD02AC@sumac.cfs.le.ac.uk>

Hi all,


I am having problems using the R2Winbugs function

When I perform an analysis directly in Winbugs I can specify that the
first 'n' iterations are to be done using an 'adaptive' phase. After
this phase the markov chain seems to mix a lot better.


I don't seem to be able to specify R2winbugs to carry out this adaptive
phase, I can just specify the burnin length but this (to my knowledge)
is not the same thing.

As a result my models are not fitting as well as I would like, has
anyone had a similar experience?



Jack


From renaud.lancelot at gmail.com  Mon Aug 21 16:35:49 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Mon, 21 Aug 2006 16:35:49 +0200
Subject: [R] Finney's fiducial confidence intervals of LD50
In-Reply-To: <c2ee56800608210702r3c8353baudce29bf9081ec4a5@mail.gmail.com>
References: <20060821120350.89992.qmail@web39814.mail.mud.yahoo.com>
	<c2ee56800608210702r3c8353baudce29bf9081ec4a5@mail.gmail.com>
Message-ID: <c2ee56800608210735t48d3cf43v15f04940e1837650@mail.gmail.com>

Sorry there was a typo in my previous reply:

> D50 <- 10^c(logD50 + c(0, -1.96, 1.96) * attr(logD50, "SE"))
> names(D50) <- c("D50", "lower", "upper")
> D50
     D50    lower    upper
140.8353 103.3171 191.9777

Best,

Renaud

2006/8/21, Renaud Lancelot <renaud.lancelot at gmail.com>:
> I don't know what Finney's fiducial confidence interval is but if your
> problem is to handle the output of dose.p (from MASS), you can do as
> follows:
>
> > library(MASS)
> > Response <- c(0, 7, 26, 27, 0, 5, 13, 29, 0, 4, 11, 25)
> > Tot <- rep(30.5, 12)
> > Dose <- rep(c(10, 40, 160, 640), 3)
> > fm <- glm(Response/Tot ~ log10(Dose), family = quasibinomial(link = probit))
> > logD50 <- dose.p(fm, cf = 1:2, p = 0.5)
> > D50 <- 10^c(logD50 + c(1, -1.96, 1.96) * attr(logD50, "SE"))
> > names(D50) <- c("D50", "lower", "upper")
> > D50
>      D50    lower    upper
> 164.9506 103.3171 191.9777
>
> Best,
>
> Renaud
>
> 2006/8/21, carlos riveira <carlos.riveira at yahoo.com>:
> >   I am working with Probit regression (I cannot switch to logit) can anybody help me in finding out how to obtain with R Finney's fiducial confidence intervals for the levels of the predictor (Dose) needed to produce a proportion of 50% of responses(LD50, ED50 etc.)?
> >   If the Pearson chi-square goodness-of-fit test is significant (by default), a heterogeneity factor should be used to calculate the limits.
> >
> >   Response<-c(0,7,26,27,0,5,13,29,0,4,11,25)
> >   Tot<-rep(30.5,12)
> >   Dose<-rep(c(10,40,160,640),3)
> >   probit<-glm(formula = Response/Tot~ log10(Dose), family=quasibinomial
> >   (link=probit))
> >   D50<- round(10^(dose.p(probit,cf=1:2,p=0.5)))
> >
> >   #This is what SPSS calculates. I would like to reproduce these results with R:
> >   #SPSS RESULTS:
> >   #PRNT50= 140,83525
> >   #CI = 98,37857;205,34483
> >   #Regr.coeff= 1,91676 (S.E.=0,16765)
> >   #Intercept=-4,11856 (S.E.=0,36355)
> >   Thanks a lot for your help.
> >
> >   Carlos
> >
> >  __________________________________________________
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Renaud LANCELOT
> D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
> Directeur adjoint charg? des affaires scientifiques
>
> CIRAD, Animal Production and Veterinary Medicine Department
> Deputy director for scientific affairs
>
> Campus international de Baillarguet
> TA 30 / B (B?t. B, Bur. 214)
> 34398 Montpellier Cedex 5 - France
> T?l   +33 (0)4 67 59 37 17
> Secr. +33 (0)4 67 59 39 04
> Fax   +33 (0)4 67 59 37 95
>


-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From bates at stat.wisc.edu  Mon Aug 21 16:51:12 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 21 Aug 2006 09:51:12 -0500
Subject: [R] [R-pkgs] New version of glmmML
In-Reply-To: <17641.42103.990929.53681@stat.math.ethz.ch>
References: <148ed8180608210212k6a7858d9s3e01011588fec2ec@mail.gmail.com>
	<17641.42103.990929.53681@stat.math.ethz.ch>
Message-ID: <40e66e0b0608210751q244839fs286094386af69659@mail.gmail.com>

On 8/21/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> Hi G?ran,
>
> >>>>> "GB" == G?ran Brostr?m <goran.brostrom at gmail.com>
> >>>>>     on Mon, 21 Aug 2006 11:12:49 +0200 writes:
>
>     GB> A new version, 0.65-1, of glmmML is now on CRAN. It is a major rewrite
>     GB> of the inner structures, so frequent updates (bug fixes) may be
>     GB> expected for some time.
>
>
>     GB> News:
>
>     [............]
>
> Sorry for my lazy question :
>
>    What does the package do that lmer() does not?

Quite a bit, actually.   G?ran has the adaptive Gauss-Hermite
quadrature method in glmmML and it is not yet available in lmer.  I
definitely plan to make it available but it has not reached the top of
the "TO DO" list.  Also, as G?ran pointed out to me, glmmML works with
the two-column matrix form of the response for a binomial generalized
linear mixed model but the answers from lmer are spurious.  Something
about the weights that I still haven't sorted out.

> I do know that you are a smart person so I know I will get a non-trivial
> answer.  The main reason for this e-mail is really to encourage you (and
> all other 'R-packages' posters) to do the following in each
> posting to R-packages:
>
> 1) [One paragraph:]
>   What does the package do // why do you see a need for the
>   package // what's the bigger context ?
>
> 2) [another paragraph (or two...):]
>   News  etc
>
> Since the R-packages archives are relatively prominently listed
> as part of "News" on the R project website, (and also since I
> encourage ``everyone'' to subscribe to R-packages,)
>
> I think the extra information in "1)" above would be appropriate
> for every posting to R-packages.
>
> Martin Maechler, ETH Zurich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From peterstencel at yahoo.de  Mon Aug 21 16:57:13 2006
From: peterstencel at yahoo.de (P. Stencel)
Date: Mon, 21 Aug 2006 16:57:13 +0200
Subject: [R] reshape a data frame
Message-ID: <000001c6c532$1694e050$0300a8c0@PS>

Dear R-helpers, 

I am trying to reshape a data frame which consists of only one column and of
several thousand rows. My purpose is to select every second element (row) of
the data frame and to create a second column with these elements. 

The data frame looks like this: 

COL1
23.190779669
74.659456135
74.019522268
27.099553199
50.02049443
76.518205961
75.924447203
75.774890661
...

Could anybody give me a piece of advice how to proceed? Are there any
commands to solve this problem?


From xiaosu at mail.ucf.edu  Mon Aug 21 16:57:06 2006
From: xiaosu at mail.ucf.edu (Xiaogang Su)
Date: Mon, 21 Aug 2006 10:57:06 -0400
Subject: [R] Assistant Professor Position - Univ. of Central
	Florida	(Orlando, FL)
Message-ID: <s4e9919a.037@mail.ucf.edu>

ASSISTANT PROFESSOR
DEPARTMENT OF STATISTICS & ACTUARIAL SCIENCE
UNIVERSITY OF CENTRAL FLORIDA

The Department of Statistics & Actuarial Science at the University of
Central Florida (UCF) invites applications for a tenure-track position
at the assistant professor level beginning August 8th, 2007. 
Qualifications include a Ph.D. in Statistics or related area by date of
hire and demonstrated or potential for excellence in teaching and
research.  Preference will be given to those with expertise in Data
Mining (and a willingness to teach in this area) but those with
expertise in other areas of statistics are encouraged to apply.  Please
send CV, 3 letters of recommendation, and transcripts to:

Search Committee
Department of Statistics & Actuarial Science
University of Central Florida
Orlando, Florida 32816-2370

Review of applications will begin on December 31st, 2006 and continue
until the position is filled.  For more information contact the
department at 407-823-5528 or visit the department's web site at
http://www.cos.ucf.edu/statistics/ 

UCF is an equal opportunity affirmative action employer.  Women and
minorities are strongly urged to apply.  Search documents may be viewed
by the public upon request in accordance with Florida statute.  AA/EOE

================================
Xiaogang Su,  Assistant Professor
Department of Statistics and Actuarial Science
University of Central Florida
Orlando, FL 32816
(407) 823-2940 [O]
xiaosu at mail.ucf.edu
http://pegasus.cc.ucf.edu/~xsu/


From dimitris.rizopoulos at med.kuleuven.be  Mon Aug 21 17:05:03 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 21 Aug 2006 17:05:03 +0200
Subject: [R] reshape a data frame
References: <000001c6c532$1694e050$0300a8c0@PS>
Message-ID: <002c01c6c533$2dfcbd20$0540210a@www.domain>

maybe something like the following:

mdf <- data.frame(COL1 = rnorm(20))
mdf
as.data.frame(matrix(mdf$COL1, ncol = 2, byrow = TRUE))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "P. Stencel" <peterstencel at yahoo.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, August 21, 2006 4:57 PM
Subject: [R] reshape a data frame


> Dear R-helpers,
>
> I am trying to reshape a data frame which consists of only one 
> column and of
> several thousand rows. My purpose is to select every second element 
> (row) of
> the data frame and to create a second column with these elements.
>
> The data frame looks like this:
>
> COL1
> 23.190779669
> 74.659456135
> 74.019522268
> 27.099553199
> 50.02049443
> 76.518205961
> 75.924447203
> 75.774890661
> ...
>
> Could anybody give me a piece of advice how to proceed? Are there 
> any
> commands to solve this problem?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From tlumley at u.washington.edu  Mon Aug 21 17:23:48 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Aug 2006 08:23:48 -0700 (PDT)
Subject: [R] interpreting coxph results
In-Reply-To: <BB390058-96DE-4701-A595-992EF1433614@indiana.edu>
References: <BB390058-96DE-4701-A595-992EF1433614@indiana.edu>
Message-ID: <Pine.LNX.4.64.0608210820330.3032@homer21.u.washington.edu>

On Mon, 21 Aug 2006, Thomas Hills wrote:

> I am having trouble understanding results I'm getting back from coxph
> doing a recurrent event analysis.  I've included the model below and
> the summary.  In some cases, with minor variations, the Robust
> variance and Wald tests are significant, but the individual
> covariates may or may not be significant.  My main question is:  If
> Wald and robust tests both take into account the clustering, then why
> are they so different and how do I make sense of them.

In your example below the coefficient for topslope1 appears to be 
infinite, so the Wald test (which uses standard errors computed at the 
estimate) will be unreliable. The score test uses standard errors computed 
at the null and so is ok.


>							  A second
> question is:  If Wald and Robust are both significant in the summary
> tests, but all individual covariates are insignificant (these are
> Wald, yes?), what do I make of that?  I recognize the questions are
> partly R related and partly statistical (if there is a better place
> to post this please let me know).

That would mean that you have good evidence that some of the variables 
affect the hazard, but not good evidence as to which ones do.

 	-thomas


> Call:
> coxph(formula = Surv(startt, stopt, rep(1, nrow(omfi))) ~ joof1 +
>     topslope1 * top1 + I(early.angle/late.angle) + spac.cov +
>     ave.angle + slopef.d + cluster(id) + strata(sequence), data =
> thedofile))
>
>   n= 174
>                              coef exp(coef) se(coef) robust se
> z    p
> joof1                     -0.2755  7.59e-01   0.1590    0.2998 -0.919
> 0.36
> topslope1                 30.9827  2.86e+13  23.2339   51.9948  0.596
> 0.55
> top1                       0.1165  1.12e+00   0.1901    0.3951  0.295
> 0.77
> I(early.angle/late.angle)  0.0449  1.05e+00   0.1165    0.1296  0.347
> 0.73
> spac.cov                   0.9815  2.67e+00   3.4104    5.5871  0.176
> 0.86
> ave.angle                  0.0396  1.04e+00   0.0156    0.0266  1.488
> 0.14
> slopef.d                  -0.3394  7.12e-01   0.4373    0.8891 -0.382
> 0.70
> topslope1:top1            -5.5673  3.82e-03   2.8198    6.7696 -0.822
> 0.41
>
>
> Rsquare= 0.18   (max possible= 0.898 )
> Likelihood ratio test= 34.5  on 8 df,   p=3.27e-05
> Wald test            = 23.5  on 8 df,   p=0.00276
> Score (logrank) test = 31.8  on 8 df,   p=0.000103,   Robust = 13.5
> p=0.097
>
>   (Note: the likelihood ratio and score tests assume independence of
>      observations within a cluster, the Wald and robust score tests
> do not).
>
> Thanks for any help,
>
> Thomas Hills
> Indiana University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From rlawton at utk.edu  Mon Aug 21 17:25:09 2006
From: rlawton at utk.edu (rlawton)
Date: Mon, 21 Aug 2006 11:25:09 -0400
Subject: [R] Creating a pixel image
Message-ID: <4516D2E0@webmail.utk.edu>

I am trying to create a pixel image from the marks of a marked spatial point 
pattern for use with the Kinhom function in the spatstat library and am having 
some difficulty.  Is there a way to create such an object using x-y 
coordinates and values at those coordinates?

                                 Many Thanks,
                                 Michael Lawton


From ggrothendieck at gmail.com  Mon Aug 21 17:29:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 11:29:37 -0400
Subject: [R] R-packages posting guide (was: Re: [R-pkgs] New version of
	glmmML)
Message-ID: <971536df0608210829l3090f4b5o339c3f9b78da51e9@mail.gmail.com>

Maybe an R-packages posting guide with an example and
an automatic append of a one or two line summary at
the end of each article posted - as already done on r-help.

On 8/21/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> Hi G?ran,
>
> >>>>> "GB" == G?ran Brostr?m <goran.brostrom at gmail.com>
> >>>>>     on Mon, 21 Aug 2006 11:12:49 +0200 writes:
>
>    GB> A new version, 0.65-1, of glmmML is now on CRAN. It is a major rewrite
>    GB> of the inner structures, so frequent updates (bug fixes) may be
>    GB> expected for some time.
>
>
>    GB> News:
>
>    [............]
>
> Sorry for my lazy question :
>
>   What does the package do that lmer() does not?
>
> I do know that you are a smart person so I know I will get a non-trivial
> answer.  The main reason for this e-mail is really to encourage you (and
> all other 'R-packages' posters) to do the following in each
> posting to R-packages:
>
> 1) [One paragraph:]
>  What does the package do // why do you see a need for the
>  package // what's the bigger context ?
>
> 2) [another paragraph (or two...):]
>  News  etc
>
> Since the R-packages archives are relatively prominently listed
> as part of "News" on the R project website, (and also since I
> encourage ``everyone'' to subscribe to R-packages,)
>
> I think the extra information in "1)" above would be appropriate
> for every posting to R-packages.
>
> Martin Maechler, ETH Zurich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From carlos.riveira at yahoo.com  Mon Aug 21 17:46:24 2006
From: carlos.riveira at yahoo.com (carlos riveira)
Date: Mon, 21 Aug 2006 08:46:24 -0700 (PDT)
Subject: [R] Fwd: Re:  Finney's fiducial confidence intervals of LD50
Message-ID: <20060821154624.34419.qmail@web39803.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/bf13591b/attachment.pl 

From spencer.graves at pdf.com  Mon Aug 21 18:13:20 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Aug 2006 09:13:20 -0700
Subject: [R] multivariate analysis by using lme
In-Reply-To: <000f01c6c304$a32f9d50$4260a8c0@childrensmemorial.org>
References: <000f01c6c304$a32f9d50$4260a8c0@childrensmemorial.org>
Message-ID: <44E9DBA0.5030909@pdf.com>

      You get the "singularity in backsolve" because res1+res2 = 1.  If 
you replace "res1+res2" everywhere by "res1+res2-1", I believe you will 
get more sensible results.   The default model assumes a constant term, 
and when res1+res2=1, the fixed effects model is singular unless you 
explicitly remove the constant by the "-1".  Alternatively, you could 
get the same thing by deleting either res1 or res2 or by using 
"I(res1-res2)" in their place. 

       Thirty years ago, I got very excited about multivariate analysis 
models crudely like this.  Since then, I've concluded that they are 
rarely as useful as one might think.  Have you made normal probability 
plots of both variables separately?  Are they both reasonably close to 
being normally distributed?  If yes, have you first tried univariate 
variance component models (a) individually and (b) for each variable 
using the other as a potential explanatory variable?  I don't understand 
your 'inter', but there is probably a way to include that in the model. 

       Only after doing the best I could with univariate modeling would 
I then consider multivariate modeling.  And then I'd want to think very 
carefully about whether the multivariate model(s) under consideration 
seemed consistent with the univariate results -- and what else they 
might tell me that I hadn't already gotten from the univariate model. 
 If you've already done all this, I'm impressed.  In the almost 30 years 
since I realized I should try univariate models first and work up to 
multivariate whenever appropriate, I've not found one application where 
the extra effort seemed justified.  R has made this much easier, but I'm 
still looking for that special application that would actually require 
the multivariate tools. 

      Hope this helps. 
      Spencer Graves

Hui-Ju Tsai wrote:
> Dear R users,
>
>  
>
> I have a data structure as follows:
>
>  
>
> id          two       res1      res2      c1         c2         inter
>
> 1          -0.786093166     1          0          1          2          6
>
> 3          -0.308495749     1          0          0          1          2
>
> 5          -0.738033048     1          0          0          0          1
>
> 7          -0.52176252      1          0          1          0          4
>
> 9          -2.023641189     1          0          1          1          5
>
> 10         0.463469887      1          0          1          0          4
>
> 14         0.379364681      1          0          1          0          4
>
> 17         -1.422889721     1          0          0          0          1
>
> 19         -2.582340053     1          0          0          2          3
>
> 20         0.721793018      1          0          1          2          6
>
> 1          -0.867823661     0          1          1          2          6
>
> 3          -0.727418536     0          1          0          1          2
>
> 5          -0.331731368     0          1          0          0          1
>
> 7          -0.835913405     0          1          1          0          4
>
> 9          -0.612541616     0          1          1          1          5
>
> 10         -0.063685221     0          1          1          0          4
>
> 14         0.3511481         0          1          1          0          4
>
> 17         -1.3847698        0          1          0          0          1
>
> 19         -2.232306187     0          1          0          2          3
>
> 20         0.0014117         0          1          1          2          6
>
>  
>
> I would like to apply multivariate analysis and test interaction effect in
> the same model by using lme function. The 'two' variable is a bi-variate
> outcome by combining two continuous outcomes. The 'res1' and 'res2' are two
> dummy variables to flag these two outcome variables, separately. The
> interaction term for c1 and c2 is "inter" with 6 levels. What I have done
> are as follows:
>
>  
>
> # capture clustering structure
>
>  
>
> tmp <- groupedData(two~res1+res2+inter|id,data=a); tmp <- na.exclude(tmp);
>
>  
>
> # treat 'inter' as a multi-level variable
>
>  
>
> options(contrasts=c(factor="contr.treatment",ordered="contr.poly"));
> tmp$inter <- factor(tmp$inter); contrasts(tmp$inter)
>
>  
>
> # perform multivariate analysis and test interaction effect
>
>  
>
> m1 <- summary(lme(two~res1+res2+inter, random=~1|id, data=tmp))
>
>  
>
> ## Error in MEEM(object, conLin, control$niterEM) : 
>
> ##        Singularity in backsolve at level 0, block 1
>
>  
>
> Unfortunately, I have been unable to get it work. If I removed these two
> dummy variables like this:
>
>  
>
> m2 <- summary(lme(two~inter, random=~1|id, data=tmp))
>
>  
>
> I did get some outputs, but these outputs in the model m2 were wrong. Is
> there any way to perform multivariate analysis and test interaction effect
> (a multi-level factor) in the same model by lme? In addition, can lme be
> used for multivariate analyses of combining several binary outcome
> variables? Or it may be doable in some R function?
>
>  
>
> Many thanks for your suggestions.
>
>  
>
> Hui-Ju
>
>  
>
>  
>
>  
>
>
>
> The contents of this e-mail message and any attachments are private and confidential communications intended solely for the addressee(s) named in this message. If you are not the intended recipient of this message, please 1) immediately notify the sender by reply e-mail and then delete this message and its attachments and 2) do not read, use, distribute disclose or copy this message and/or any attachments. 
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Mon Aug 21 18:18:11 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 21 Aug 2006 18:18:11 +0200
Subject: [R] R2WinBugs
In-Reply-To: <286C9166197E0C44B94FF9762B27DAC70AFD02AC@sumac.cfs.le.ac.uk>
References: <286C9166197E0C44B94FF9762B27DAC70AFD02AC@sumac.cfs.le.ac.uk>
Message-ID: <44E9DCC3.5020005@statistik.uni-dortmund.de>



Bowden, J.M. wrote:
> Hi all,
> 
> 
> I am having problems using the R2Winbugs function
> 
> When I perform an analysis directly in Winbugs I can specify that the
> first 'n' iterations are to be done using an 'adaptive' phase. After
> this phase the markov chain seems to mix a lot better.
> 
> 
> I don't seem to be able to specify R2winbugs to carry out this adaptive
> phase, I can just specify the burnin length but this (to my knowledge)
> is not the same thing.
> 
> As a result my models are not fitting as well as I would like, has
> anyone had a similar experience?

I think this is a question for Andrew Gelman (CCing), who wrote the 
underlying code for automatically setting the adaptive phase.

Uwe Ligges


> Jack
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From debarchana.ghosh at gmail.com  Mon Aug 21 18:57:31 2006
From: debarchana.ghosh at gmail.com (Debarchana Ghosh)
Date: Mon, 21 Aug 2006 11:57:31 -0500
Subject: [R] "vcov" error in svyby and svytable functions
Message-ID: <d1b8ff630608210957u5ed1b48eu662e420c6bd5da02@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/e55f8e7e/attachment.pl 

From rolf at erdos.math.unb.ca  Mon Aug 21 19:21:54 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Mon, 21 Aug 2006 14:21:54 -0300 (ADT)
Subject: [R] Creating a pixel image
Message-ID: <200608211721.k7LHLsrv023578@erdos.math.unb.ca>

Michael Lawton wrote:

> I am trying to create a pixel image from the marks of a marked
> spatial point pattern for use with the Kinhom function in the
> spatstat library and am having some difficulty.  Is there a way to
           ^^^^^^^
        ***package*** !!!
        (NOT ``library'')

> create such an object using x-y coordinates and values at those
> coordinates?

	It is not clear to me what you actually want to do, but there
	are tools in spatstat which may accomplish your purpose.  In
	particular, the function as.im() (see the on-line help) will
	take a function of the form ``function(x,y,...)'' (and a
	window W) and convert that function into an image defined on W.

	So if you have or can create a function which will calculate
	the ``values at those coordinates'' then there is no problem.

	Otherwise you will need to explain more clearly what it is
	that you want.

	If your are thinking of an ``image'' where the value is, say,
	10, at points of the pattern whose marks are of type 1, and
	say -10, at points of the pattern whose marks are of type 2,
	and say 0 at ``background'' points, then this is readily
	accomplished using nearest.raster.point().  But I don't think
	you gain anything by representing the point pattern as an image
	in this way, and this is not the sort of thing that one would
	feed to Kinhom().

	Quite often an input to Kinhom() would be calculated by
	``smoothing'' the field of ``delta functions'' corresponding
	to a collection of points.  The function density.ppp() is
	a good way to accomplish this.  Choosing the ``bandwidth''
	parameter sigma for the smooth is however a bit of a mystery,
	apparently even to the cognoscenti.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

P. S.  A library, as someone --- Martin Maechler??? --- has repeatedly
tried to explain to the list, is a *collection of packages*.  One
loads a package *from a library* by using the library() function.

					R. T.


From jesse.canchola.b at bayer.com  Mon Aug 21 19:48:54 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Mon, 21 Aug 2006 10:48:54 -0700
Subject: [R] Permutations with replacement (final final final)
In-Reply-To: <000f01c6c324$bd44cec0$6401a8c0@main>
Message-ID: <OF75980349.4FC1EB42-ON882571D1.00616A2B-882571D1.0061DC37@bayer.com>

Hi Daniel,

Turns out, your code, however simple, is quite elegant for my needs 
(sometimes I overanalyze  :O).  Here is my last code to do what I need to 
do:

####### begin R code ########
# generate a matrix of ten thousand rows of 1-8 
z <- t(matrix(rep(1:8,10000),8,10000))
library(Matrix)
# use the R sample function in a loop to sample each line with replacement
zcomb=Matrix()
for (i in 1:dim(z)[1]) {
    z1 <- t(matrix(sample(z,8,replace=TRUE)))
    zcomb = rbind(zcomb,z1)
    }
zcomb
####### end R code #########

Regards,
Jesse A. Canchola





"Daniel Nordlund" <res90sx5 at verizon.net> 
Sent by: r-help-bounces at stat.math.ethz.ch
08/18/2006 05:16 PM

To
"'Jesse Albert Canchola'" <jesse.canchola.b at bayer.com>, "'r-help'" 
<r-help at stat.math.ethz.ch>
cc

Subject
Re: [R] Permutations with replacement






> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
[mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Jesse Albert Canchola
> Sent: Friday, August 18, 2006 1:02 PM
> To: r-help
> Subject: [R] Permutations with replacement
> 
> Is there a simple function or process that will create permutations with
> replacement?
> 
> I know that using the combinat package
> 
> ###### begin R code ######
> > library(combinat)
> > m <- t(array(unlist(permn(3)), dim = c(3, 6)))
> 
> # we can get the permutations, for example 3!=6
> # gives us
> 
> > m
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    1    3    2
> [3,]    3    1    2
> [4,]    3    2    1
> [5,]    2    3    1
> [6,]    2    1    3
> ###### end R code ##########
> 
> I'd like to include the "with replacement possibilities" such as
> 
> 1,1,3
> 1,1,2
> 2,3,3
> 
Isn't what you want just sampling with replacement?

  x <- c(1,2,3)
  sample(x,3,replace=TRUE)

Hope this is helpful,

Dan

Dan Nordlund
Bothell, WA  USA

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




_______________________________________________________________________________________________

The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com


From damien.moore at excite.com  Mon Aug 21 19:49:14 2006
From: damien.moore at excite.com (Damien Moore)
Date: Mon, 21 Aug 2006 13:49:14 -0400 (EDT)
Subject: [R] lean and mean lm/glm?
Message-ID: <20060821174914.186AD4AFDF@xprdmxin.myway.com>


Hi All: I'm new to R and have a few questions about getting R to run efficiently with large datasets.

I'm running R on Windows XP with 1Gb ram (so about 600mb-700mb after the usual windows overhead). I have a dataset that has 4 million observations and about 20 variables. I want to run probit regressions on this data, but can't do this with more than about 500,000 observations before I start running out of ram (you could argue that I'm getting sufficient precision with <500,000 obs but lets pretend otherwise). Loading 500,000 observations into a data frame only takes about 100Mb of ram, so that isn't the problem. Instead it seems R uses huge amount of memory when running the glm methods. I called the Fortran routines that lm and glm use directly but even they create a large number of extraneous variables in the output (e.g. the Xs, ys, residuals etc) and during processing. For instance (sample code)

x=runif(1000000)
y=3*x+rnorm(1000000) #I notice this step chews up a lot more than the 7mb of ram required to store y during processing, but cleans up ok afterwards with a gc() call
X=cbind(x)
p=ncol(X)
n=NROW(y)
ny=NCOL(y)
tol=1e-7
#this is the fortran routine called by lm - regressing y on X here
z <- .Fortran("dqrls", qr = X, n = n, p = p, y = y, ny = ny, 
tol = as.double(tol), coefficients = mat.or.vec(p, ny), 
residuals = y, effects = y, rank = integer(1), pivot = 1:p, 
qraux = double(p), work = double(2 * p), PACKAGE = "base")

This code runs very quickly - suggesting that in principle R should have no problem at all handling very large data sets, but uses >100mb during processing and z is about a 20mb object. Scaling this up to a much larger dataset with many variables its easy to see i'm going to run into problems

My questions:
1. are there any memory efficient alternatives to lm/glm in R?
2. is there any way to prevent the Fortran routine "dqrls" from producing so much output? (I suspect not since its output has to be compatible with the summary method, which seems to rely on having a copy of all variables instead of just references to the relevant variables - correct me if i'm wrong on this)
3. failing 1 & 2 how easy would it be to create new versions of lm and glm that don't use so much memory? (Not that I'm volunteering or anything ;) ). There is no need to hold individual residuals in memory or make copies of the variables (at least for my purposes). How well documented is the source code?

cheers
Damien Moore


From jesse.canchola.b at bayer.com  Mon Aug 21 19:51:39 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Mon, 21 Aug 2006 10:51:39 -0700
Subject: [R] Fw:  Permutations with replacement
Message-ID: <OFAC13BAC8.D62ED316-ON882571D1.006203D1-882571D1.00621CDE@bayer.com>

My apologies, I forgot to CC: to the list on my previous communication 
with Daniel.

Jesse

----- Forwarded by Jesse Albert Canchola/EMVL/DIAG/US/BAYER on 08/21/2006 
10:50 AM -----

Jesse Albert Canchola/EMVL/DIAG/US/BAYER
08/21/2006 09:36 AM

To
"Daniel Nordlund" <res90sx5 at verizon.net>
cc

Subject
RE: [R] Permutations with replacement





Thanks, Daniel.  I need to enumerate all possibilities of 8^8 and take a 
random sample of 10,000 from there.  Then I will use the sampled 
possibilities to do a combination of data frames/files then do some math 
on those files and develop the probability distribution from resulting 
sampling statistics (I couldn't get the available bootstrap packages to do 
what I want).  BTW, the preferred solution (however inelegant) is 
reprinted below.   I did have a memory problem with the hypercube for 8^8 
so I did an 8^7 hypercube that worked and concatenated a 1-8 to the 8^7 
matrix that resulted in 8 large matrices which I attempted rbind together 
to create the 8^8 but ran into more memory problems (on matrix number 7 of 
8- it's a Windows problem - I used the --max-mem-size=2G - to no avail). 
The final solution was to take a random sample of 10,000/8=1250 from each 
of the files (doing four of the 8 files at a time, and for which I 
permuted the rows to make it more random), removed the heavy-laden files 
then rbind 'ed the smaller sampled files together to make the 10,000.

Here is the final final code:

# IDEA:  We cannot simply do a permutation of the 8 classes/file id's 
since this will not allow/simulate repeats of numbers
#        as in a bootstrap (e.g., for 3 items - 1,2,3 - we would also want 
the possibility 1,1,1 or 2,2,2 or 3,3,3 etc.
#        The 8!=40,320 permutations with replacement would become an 
8^8=16,777,216 so we would want to take a random
#        sample of 10,000 from the posibilities 
library(combinat)
# THIS WILL NOT WORK DUE TO THE LIMITATIONS OF WINDOWS MEMORY (PROBLEMATIC 
AS IN THE FAQs) SO WE WILL 
# USE A WORKAROUND
# WORKAROUND: 1) Construct the 8 to the 7 power hypercube.  2) For these 
data, create eight additional data sets that include
#             the last position to finish the construction of an 8^8 
hypercube
#x <- rep(8,8) # for partitions of 8 units into classes {1,2,3,4,5,6,7,8} 
#hcube8 <- hcube(x, scale=1, transl=0) 
#hcube8

#step 1: 8^7 = 2,097,152
x1 <- rep(8,7)
x1
hcube87 <- hcube(x1, scale=1, transl=0)
#this will generate 2,097,152 results from 1-8 but for only 7 positions

#step 2: column bind each file with 1-8 in the 8th position 
#x1a <- cbind(hcube87,1)
#x2a <- cbind(hcube87,2)
#x3a <- cbind(hcube87,3)
#x4a <- cbind(hcube87,4)
#x5a <- cbind(hcube87,5)
#x6a <- cbind(hcube87,6)
#x7a <- cbind(hcube87,7)
#x8a <- cbind(hcube87,8)

#turns out this method also chokes with the memory limitations
# Step 2 will be modified as follows: 
# Step 2a: as before; Step2b: Sample 1/8 from each piece and after every 4 
processes, delete the objects to allow for the rest.

#Step 2a
x1a <- cbind(hcube87,1)
x1a <- x1a[sample(1:2097152),] #randomly permute the rows for more 
randomness
x2a <- cbind(hcube87,2)
x2a <- x2a[sample(1:2097152),] #randomly permute the rows for more 
randomness
x3a <- cbind(hcube87,3)
x3a <- x3a[sample(1:2097152),] #randomly permute the rows for more 
randomness
x4a <- cbind(hcube87,4)
x4a <- x4a[sample(1:2097152),] #randomly permute the rows for more 
randomness

x1b <- x1a[sample(1:1250,replace=FALSE),]
x2b <- x2a[sample(1:1250,replace=FALSE),]
x3b <- x3a[sample(1:1250,replace=FALSE),]
x4b <- x4a[sample(1:1250,replace=FALSE),]

rm(x1a,x2a,x3a,x4a) #remove the big files

x5a <- cbind(hcube87,5)
x5a <- x5a[sample(1:2097152),] #randomly permute the rows for more 
randomness
x6a <- cbind(hcube87,6)
x6a <- x6a[sample(1:2097152),] #randomly permute the rows for more 
randomness
x7a <- cbind(hcube87,7)
x7a <- x7a[sample(1:2097152),] #randomly permute the rows for more 
randomness
x8a <- cbind(hcube87,8)
x8a <- x8a[sample(1:2097152),] #randomly permute the rows for more 
randomness

x5b <- x5a[sample(1:1250,replace=FALSE),]
x6b <- x6a[sample(1:1250,replace=FALSE),]
x7b <- x7a[sample(1:1250,replace=FALSE),]
x8b <- x8a[sample(1:1250,replace=FALSE),]

rm(x5a,x6a,x7a,x8a) #remove the big files

#Step 3: combine all the randomly sampled files
m <- rbind(x1b,x2b,x3b,x4b,x5b,x6b,x7b,x8b)

# NOTE:  each number in the matrix represents a file "name" from 1-8. 
# the first pointer should be numeric and then subsequent as character
# since the first time you assign a number to a character in a matrix
# the rest of the numbers in the matrix are coerced to character
m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c' ; m[m=='4']='d'; m[m=='5']='e' ; 
m[m=='6']='f'; m[m=='7']='g' ; m[m=='8']='h'
m
########### end R code ############




Thanks, David.  That worked fabulously! 

Here is the R code for the hypercube test example: 

########## begin R code ############
library(combinat)
x <- rep(3,3)   # for partitions of 3 units into the three classes {1,2,3} 


hcube(x, scale=1, transl=0) 
########### end R code ############

For the larger one I want (i.e., 8^8), I will take a random sample of 
10,000 from the 16,777,216 possibilities.

Regards,
Jesse Canchola




<davidr at rhotrading.com> 
Sent by: r-help-bounces at stat.math.ethz.ch
08/18/2006 01:33 PM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>, "r-help" 
<r-help at stat.math.ethz.ch>
cc

Subject
Re: [R] Permutations with replacement






If you also want 1,1,1 and so on, the number of these is n^n,
(n choices for each of n slots.)
In that case, you could use hcube from combinat.

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605




"Daniel Nordlund" <res90sx5 at verizon.net> 
08/18/2006 05:16 PM

To
"'Jesse Albert Canchola'" <jesse.canchola.b at bayer.com>, "'r-help'" 
<r-help at stat.math.ethz.ch>
cc

Subject
RE: [R] Permutations with replacement






> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
[mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Jesse Albert Canchola
> Sent: Friday, August 18, 2006 1:02 PM
> To: r-help
> Subject: [R] Permutations with replacement
> 
> Is there a simple function or process that will create permutations with
> replacement?
> 
> I know that using the combinat package
> 
> ###### begin R code ######
> > library(combinat)
> > m <- t(array(unlist(permn(3)), dim = c(3, 6)))
> 
> # we can get the permutations, for example 3!=6
> # gives us
> 
> > m
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    1    3    2
> [3,]    3    1    2
> [4,]    3    2    1
> [5,]    2    3    1
> [6,]    2    1    3
> ###### end R code ##########
> 
> I'd like to include the "with replacement possibilities" such as
> 
> 1,1,3
> 1,1,2
> 2,3,3
> 
Isn't what you want just sampling with replacement?

  x <- c(1,2,3)
  sample(x,3,replace=TRUE)

Hope this is helpful,

Dan

Dan Nordlund
Bothell, WA  USA






_______________________________________________________________________________________________

The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com


From sachinj.2006 at yahoo.com  Mon Aug 21 19:56:44 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 21 Aug 2006 10:56:44 -0700 (PDT)
Subject: [R] Dataframe modification
Message-ID: <20060821175644.81719.qmail@web37604.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/baa3279a/attachment.pl 

From jrkrideau at yahoo.ca  Mon Aug 21 20:00:21 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 21 Aug 2006 14:00:21 -0400 (EDT)
Subject: [R] aggregate example : where is the state.region variable?
Message-ID: <20060821180021.40781.qmail@web32804.mail.mud.yahoo.com>

I was looking ?aggregate and ran the first example

 aggregate(state.x77, list(Region = state.region),
mean)

The variables in state.x77 appear to be :
> state.x77
Population Income Illiteracy Life Exp Murder HS Grad
Frost   Area

Where is the "state.region" variable coming from?


From Greg.Snow at intermountainmail.org  Mon Aug 21 20:01:06 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 21 Aug 2006 12:01:06 -0600
Subject: [R] lean and mean lm/glm?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB53E383@LP-EXCHVS07.CO.IHC.COM>

For very large regression problems there is the biglm package (put you
data into a database, read in 500,000 rows at a time, and keep updating
the fit).

This has not been extended to glm yet.

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damien Moore
Sent: Monday, August 21, 2006 11:49 AM
To: r-help at stat.math.ethz.ch
Subject: [R] lean and mean lm/glm?


Hi All: I'm new to R and have a few questions about getting R to run
efficiently with large datasets.

I'm running R on Windows XP with 1Gb ram (so about 600mb-700mb after the
usual windows overhead). I have a dataset that has 4 million
observations and about 20 variables. I want to run probit regressions on
this data, but can't do this with more than about 500,000 observations
before I start running out of ram (you could argue that I'm getting
sufficient precision with <500,000 obs but lets pretend otherwise).
Loading 500,000 observations into a data frame only takes about 100Mb of
ram, so that isn't the problem. Instead it seems R uses huge amount of
memory when running the glm methods. I called the Fortran routines that
lm and glm use directly but even they create a large number of
extraneous variables in the output (e.g. the Xs, ys, residuals etc) and
during processing. For instance (sample code)

x=runif(1000000)
y=3*x+rnorm(1000000) #I notice this step chews up a lot more than the
7mb of ram required to store y during processing, but cleans up ok
afterwards with a gc() call
X=cbind(x)
p=ncol(X)
n=NROW(y)
ny=NCOL(y)
tol=1e-7
#this is the fortran routine called by lm - regressing y on X here z <-
.Fortran("dqrls", qr = X, n = n, p = p, y = y, ny = ny, tol =
as.double(tol), coefficients = mat.or.vec(p, ny), residuals = y, effects
= y, rank = integer(1), pivot = 1:p, qraux = double(p), work = double(2
* p), PACKAGE = "base")

This code runs very quickly - suggesting that in principle R should have
no problem at all handling very large data sets, but uses >100mb during
processing and z is about a 20mb object. Scaling this up to a much
larger dataset with many variables its easy to see i'm going to run into
problems

My questions:
1. are there any memory efficient alternatives to lm/glm in R?
2. is there any way to prevent the Fortran routine "dqrls" from
producing so much output? (I suspect not since its output has to be
compatible with the summary method, which seems to rely on having a copy
of all variables instead of just references to the relevant variables -
correct me if i'm wrong on this) 3. failing 1 & 2 how easy would it be
to create new versions of lm and glm that don't use so much memory? (Not
that I'm volunteering or anything ;) ). There is no need to hold
individual residuals in memory or make copies of the variables (at least
for my purposes). How well documented is the source code?

cheers
Damien Moore

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Mon Aug 21 20:31:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 14:31:04 -0400
Subject: [R] Dataframe modification
In-Reply-To: <20060821175644.81719.qmail@web37604.mail.mud.yahoo.com>
References: <20060821175644.81719.qmail@web37604.mail.mud.yahoo.com>
Message-ID: <971536df0608211131o1736ef20sf605aa674a8d9484@mail.gmail.com>

Here are two solutions:


A <- 1:8
B <- c(1,2,4,7,8)
C <- c(5,3,10,12,17)

# solution 1 - assignment with subscripting
DF <- data.frame(A, B = A, C = 0)
DF[A %in% B, "C"] <- C

# solution 2 - merge
DF <- with(merge(data.frame(A), data.frame(B, C), by = 1, all = TRUE),
        data.frame(A, B = A, C = ifelse(is.na(C), 0, C)))


On 8/21/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
> Hi,
>
>  How can I accomplish this in R.
>
>  I have a Dataframe with 3 columns. Column B and C have same elements. But column A has more elements than B and C. I want to compare Column A with B and do the following:
>
>  If A is not in B then insert a new row in B and C and fill these new rows with
>  B = A and C = 0. So finally I will have balanced dataframe with equal no of rows (entries) in all the columns.
>
>  For example:
>
>  A[3] = 3 but is not in B. So insert new row  and set B[3]  = 3 (new row) and C[3] = 0. Final result would look like:
>
>  A   B   C
>  1    1    5
>  2    2    3
>  3    3    0
>  4    4   10
>  5    5    0
>  6    6    0
>  7    7    12
>  8    8    17
>
>  These are the columns of DF
>  > a <- c(1,2,3,4,5,6,7,8)
>  > b <- c(1,2,4,7,8)
>  > c(5,3,10,12,17)
>
>  Thanx in advance for the help.
>
>  Sachin
>
>  __________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Aug 21 20:34:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 14:34:57 -0400
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <20060821180021.40781.qmail@web32804.mail.mud.yahoo.com>
References: <20060821180021.40781.qmail@web32804.mail.mud.yahoo.com>
Message-ID: <971536df0608211134l506e8bc8p2e84270ecc0d423c@mail.gmail.com>

Its not part of state.x77.  Its a completely separate variable.
Try ls("package:datasets") and notice its in the list
or try ?state.region and note that its a variable in datasets.


On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
> I was looking ?aggregate and ran the first example
>
>  aggregate(state.x77, list(Region = state.region),
> mean)
>
> The variables in state.x77 appear to be :
> > state.x77
> Population Income Illiteracy Life Exp Murder HS Grad
> Frost   Area
>
> Where is the "state.region" variable coming from?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sachinj.2006 at yahoo.com  Mon Aug 21 20:41:39 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 21 Aug 2006 11:41:39 -0700 (PDT)
Subject: [R] Dataframe modification
In-Reply-To: <971536df0608211131o1736ef20sf605aa674a8d9484@mail.gmail.com>
Message-ID: <20060821184139.23113.qmail@web37614.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/637cd043/attachment.pl 

From tlumley at u.washington.edu  Mon Aug 21 20:44:46 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Aug 2006 11:44:46 -0700 (PDT)
Subject: [R] "vcov" error in svyby and svytable functions
In-Reply-To: <d1b8ff630608210957u5ed1b48eu662e420c6bd5da02@mail.gmail.com>
References: <d1b8ff630608210957u5ed1b48eu662e420c6bd5da02@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608211141070.22104@homer23.u.washington.edu>

On Mon, 21 Aug 2006, Debarchana Ghosh wrote:

> Hi,
>
> I'm trying to compute survey svytable statistic on subsets by using the
> svyby function.
>
> Here is the code:
> b<-svyby(~V024+V751, by=~V025, design=strat2, svytable, round=TRUE)
>
> The vars, V024, V751 and V025 are factors. The by var has 2 levels, and
> hence there will be two subsets. strat2 is created by the svydesign function.

> It's giving me the following error:
>> b<-svyby(~V024+V751, by=~V025, design=strat2, svytable, round=TRUE)
> Error in vcov(object, ...) : no applicable method for "vcov"
>
> I can't understand what "vcov" is and why it is giving this error because
> both svyby and svytable functions do not have "vcov" as one of their
> arguements. I tried svytable without the svyby and it works fine.

The problem is that svytable() does not produce standard errors. As 
help(svyby) says

Note:

      Asking for a design effect ('deff=TRUE') from a function that does
      not produce one will cause an error or incorrect formatting of the
      output. The same will occur with 'keep.var=TRUE' if the function
      does not compute a standard error.

Adding  keep.var=FALSE to the svyby() call, which tells svyby() not to 
extract standard errors, should fix the problem.

If you want standard errors the easiest thing is to use svytotal() rather 
than svytable:
b<-svyby(~interaction(V024,V751), by=~V025, design=strat2, svytotal)


 	-thomas


From ggrothendieck at gmail.com  Mon Aug 21 20:49:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 14:49:12 -0400
Subject: [R] Dataframe modification
In-Reply-To: <20060821184139.23113.qmail@web37614.mail.mud.yahoo.com>
References: <971536df0608211131o1736ef20sf605aa674a8d9484@mail.gmail.com>
	<20060821184139.23113.qmail@web37614.mail.mud.yahoo.com>
Message-ID: <971536df0608211149w6d42aa91gd898c38fb0496a3a@mail.gmail.com>

I think the first one still works in that case.
If reordering the rows is ok then the second one works too.

A <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7,8,9,10,11,12)
B <- c(1,2,4,7,8)
C <- c(5,3,10,12,17)

# solution 1 - assignment with subscripting
DF <- data.frame(A, B = A, C = 0)
DF[A %in% B, "C"] <- C
DF

# solution 2 - merge
DF <- with(merge(data.frame(A), data.frame(B, C), by = 1, all = TRUE,
        sort = FALSE), data.frame(A, B = A, C = ifelse(is.na(C), 0, C)))
DF



On 8/21/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
>
> Hi Gabor,
>
> Thanx for the help. I forgot to mention this. Column A is something like
> this
>
> A <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6,
> 7,8,9,10,11,12)
>
> i.e it repeats. Rest all is same. How can I modify your solution to take
> care of this issue.
>
> Thanx in advance.
>
> Sachin
>
>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> Here are two solutions:
>
>
> A <- 1:8
> B <- c(1,2,4,7,8)
> C <- c(5,3,10,12,17)
>
> # solution 1 - assignment with subscripting
> DF <- data.frame(A, B = A, C = 0)
> DF[A %in% B, "C"] <- C
>
> # solution 2 - merge
> DF <- with(merge(data.frame(A), data.frame(B, C), by = 1, all = TRUE),
> data.frame(A, B = A, C = ifelse(is.na(C), 0, C)))
>
>
> On 8/21/06, Sachin J wrote:
> > Hi,
> >
> > How can I accomplish this in R.
> >
> > I have a Dataframe with 3 columns. Column B and C have same elements. But
> column A has more elements than B and C. I want to compare Column A with B
> and do the following:
> >
> > If A is not in B then insert a new row in B and C and fill these new rows
> with
> > B = A and C = 0. So finally I will have balanced dataframe with equal no
> of rows (entries) in all the columns.
> >
> > For example:
> >
> > A[3] = 3 but is not in B. So insert new row and set B[3] = 3 (new row) and
> C[3] = 0. Final result would look like:
> >
> > A B C
> > 1 1 5
> > 2 2 3
> > 3 3 0
> > 4 4 10
> > 5 5 0
> > 6 6 0
> > 7 7 12
> > 8 8 17
> >
> > These are the columns of DF
> > > a <- c(1,2,3,4,5,6,7,8)
> > > b <- c(1,2,4,7,8)
> > > c(5,3,10,12,17)
> >
> > Thanx in advance for the help.
> >
> > Sachin
> >
> > __________________________________________________
> >
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
>
> __________________________________________________
> Do You Yahoo!?
> Tired of spam? Yahoo! Mail has the best spam protection around
> http://mail.yahoo.com


From Manuel.A.Morales at williams.edu  Mon Aug 21 21:10:42 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Mon, 21 Aug 2006 15:10:42 -0400
Subject: [R] Simulate p-value in lme4
In-Reply-To: <44E8A858.5010709@pdf.com>
References: <1155847771.12391.46.camel@solidago.localdomain>
	<44E8A858.5010709@pdf.com>
Message-ID: <1156187442.6677.7.camel@solidago.localdomain>

Spencer,

Thanks for the reply. I concluded the same wrt between group variation
soon after posting. However, the approach I ended up with was fully
parametric as opposed to the resampling approach that you use in your
reply. Interestingly, the two approaches yield different P-values, I
think because your approach retains overdispersion in the data (?). In
any case, my parametric stab at this is below.

iter <- 10
chisq.sim <- rep(NA, iter)

Zt <- slot(model1,"Zt") # see ?lmer-class
n.grps <- dim(ranef(model1)[[1]])[1]
sd.ran.effs <- sqrt(VarCorr(model1)[[1]][1])
X <- slot(model1,"X") # see ?lmer-class
fix.effs <- matrix(rep(fixef(model1),dim(X)[1]), nrow=dim(X)[1],
                   byrow=T)
model.parms <- X*fix.effs # This gives parameters for each case
# Generate predicted values
pred.vals <- as.vector(apply(model.parms, 1, sum))

for(i in 1:iter) {
  rand.new <- as.vector(rnorm(grps,0, sd.ran.effs))
  rand.vals <- as.vector(rand.new%*%Zt) # Assign random effects
  mu <- pred.vals+rand.vals # Expected mean
  resp <- rpois(length(mu), exp(mu))
  sim.data <- data.frame(slot(model2,"frame"), resp) # Make data frame
  sim.model1 <- lmer(resp~1+(1|subject), data=sim.data,
                     family="poisson")
  sim.model2 <- lmer(resp~pred+(1|subject), data=sim.data,
                     family="poisson")
  chisq.sim[i] <- anova(sim.model1,sim.model2)$Chisq[[2]]
}

Manuel

On Sun, 2006-08-20 at 11:22 -0700, Spencer Graves wrote:
>       You've raised a very interesting question about testing a 
> fixed-effect factor with more than 2 levels using Monte Carlo.  Like 
> you, I don't know how to use 'mcmcsamp' to refine the naive 
> approximation. If we are lucky, someone else might comment on this for us. 
> 
>       Beyond this, you are to be commended for providing such a simple, 
> self-contained example for such a sophisticated question.  I think you 
> simulation misses one important point:  It assumes the between-subject 
> variance is zero.  To overcome this, I think I might try either the 
> bootstrap or permutation distribution scrambling the assignment of 
> subjects to treatment groups but otherwise keeping the pairs of 
> observations together. 
> 
>       To this end, consider the following: 
> 
> # Build a table to translate subject into 'pred'
> o <- with(epil3, order(subject, y))
> epil3. <- epil3[o,]
> norep <- with(epil3., subject[-1]!=subject[-dim(epil3)[1]])
> subj1 <- which(c(T, norep))
> subj.pred <- epil3.[subj1, c("subject", "pred")]
> subj. <- as.character(subj.pred$subject)
> pred. <- subj.pred$pred
> names(pred.) <- subj.
> 
> iter <- 10
> chisq.sim <- rep(NA, iter)
> 
> set.seed(1)
> for(i in 1:iter){
> ## Parameteric version
  s.i <- sample(subj.)
> # Randomize subject assignments to 'pred' groups
>   epil3.$pred <- pred.[s.i][epil3.$subject]
>   fit1 <- lmer(y ~ pred+(1 | subject),
>                 family = poisson, data = epil3.)
>   fit0 <- lmer(y ~ 1+(1 | subject),
>                 family = poisson, data = epil3.)
>   chisq.sim[i] <- anova(fit0, fit1)[2, "Chisq"]
> }
> 
>       Hope this helps. 
>       Spencer Graves
> 
> Manuel Morales wrote:
> > Dear list,
> >
> > This is more of a stats question than an R question per se. First, I
> > realize there has been a lot of discussion about the problems with
> > estimating P-values from F-ratios for mixed-effects models in lme4.
> > Using mcmcsamp() seems like a great alternative for evaluating the
> > significance of individual coefficients, but not for groups of
> > coefficients as might occur in an experimental design with 3 treatment
> > levels. I'm wondering if the simulation approach I use below to estimate
> > the P-value for a 3-level factor is appropriate, or if there are any
> > suggestions on how else to approach this problem. The model and data in
> > the example are from section 10.4 of MASS.
> >
> > Thanks!
> > Manuel
> >
> > # Load req. package (see functions to generate data at end of script)
> > library(lme4)
> > library(MASS)
> >
> > # Full and reduced models - pred is a factor with 3 levels
> > result.full <- lmer(y~pred+(1|subject), data=epil3, family="poisson")
> > result.base <- lmer(y~1+(1|subject), data=epil3, family="poisson")
> >
> > # Naive P-value from LR for significance of "pred" factor
> > anova(result.base,result.full)$"Pr(>Chisq)"[[2]] # P-value
> > (test.stat <- anova(result.base,result.full)$Chisq[[2]]) # Chisq-stat

<snip> Wrong approach here</snip>

> > # Script to generate data, from section 10.4 of MASS
> > epil2 <- epil[epil$period == 1, ]
> > epil2["period"] <- rep(0, 59); epil2["y"] <- epil2["base"]
> > epil["time"] <- 1; epil2["time"] <- 4
> > epil2 <- rbind(epil, epil2)
> > epil2$pred <- unclass(epil2$trt) * (epil2$period > 0)
> > epil2$subject <- factor(epil2$subject)
> > epil3 <- aggregate(epil2, list(epil2$subject, epil2$period > 0),
> >                    function(x) if(is.numeric(x)) sum(x) else x[1])
> > epil3$pred <- factor(epil3$pred, labels = c("base", "placebo", "drug"))
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >


From h.wickham at gmail.com  Mon Aug 21 21:14:19 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 21 Aug 2006 14:14:19 -0500
Subject: [R] multivariate analysis by using lme
In-Reply-To: <44E9DBA0.5030909@pdf.com>
References: <000f01c6c304$a32f9d50$4260a8c0@childrensmemorial.org>
	<44E9DBA0.5030909@pdf.com>
Message-ID: <f8e6ff050608211214h5058e136i3fe2eb080bb1b5fa@mail.gmail.com>

>        Only after doing the best I could with univariate modeling would
> I then consider multivariate modeling.  And then I'd want to think very
> carefully about whether the multivariate model(s) under consideration
> seemed consistent with the univariate results -- and what else they
> might tell me that I hadn't already gotten from the univariate model.
>  If you've already done all this, I'm impressed.  In the almost 30 years
> since I realized I should try univariate models first and work up to
> multivariate whenever appropriate, I've not found one application where
> the extra effort seemed justified.  R has made this much easier, but I'm
> still looking for that special application that would actually require
> the multivariate tools.

To add to Spencer's comments, I'd strongly recommend you look at your
data before trying to model it.  The attached graph, a scatterplot of
res1 vs res2 values conditional on c1 and c2, with point shape given
by inter, reveals many interesting features of your data:

 * res1 and res2 values are highly correlated
 * inter is constant for a given c1 and c2
 * there are between 1 and 3 points for each level of inter - not very
many and I don't think enough to investigate what the effect of inter
is

The plot was created using the following code:

library(ggplot)
s <- read.table("~/Desktop/sample.txt", header=T)
s <- rename(s, c(two="value"))
s$res2 <- NULL
s <- as.data.frame(cast(s, ... ~ res1))


qplot(X0, X1, c1 ~ c2, data=s, shape=factor(inter))

(note that you will need the latest version of ggplot available from
http://had.co.nz/ggplot)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graph.pdf
Type: application/pdf
Size: 18475 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060821/c7572407/attachment.pdf 

From jeff.hamann at forestinformatics.com  Mon Aug 21 21:31:19 2006
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Mon, 21 Aug 2006 12:31:19 -0700 (PDT)
Subject: [R] removing for-loop question
Message-ID: <1742.128.193.139.146.1156188679.squirrel@www.forestinformatics.com>

R-loop-killers:

I'm looking for a hint to remove the loops from this little piece of code.

Since most of the of computing I deal with is very "loop-centric", it's
often difficult for me to translate index-based code into loop-free code.
Does anyone have any hints for speeding this up?

The actuals is a data.frame object and the resids is a simple matrix...

  cb <- cov( rbind( actuals$tvolh, resids[,5] ) )
  covb <- 0
  for( i in 1:nrow( actuals ) ) {
    for( j in 2:nrow( actuals ) ) {
      covb <- covb + actuals[i,]$ha * block.acts[j,]$ha * cb[i,j]
    }
  }


thanks,
Jeff.


-- 
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421


From ripley at stats.ox.ac.uk  Mon Aug 21 21:51:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Aug 2006 20:51:53 +0100 (BST)
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <20060821180021.40781.qmail@web32804.mail.mud.yahoo.com>
References: <20060821180021.40781.qmail@web32804.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608212050420.8662@gannet.stats.ox.ac.uk>

On Mon, 21 Aug 2006, John Kane wrote:

> I was looking ?aggregate and ran the first example
> 
>  aggregate(state.x77, list(Region = state.region),
> mean)
> 
> The variables in state.x77 appear to be :
> > state.x77
> Population Income Illiteracy Life Exp Murder HS Grad
> Frost   Area
> 
> Where is the "state.region" variable coming from?

> find("state.region")
[1] "package:datasets"

Try ?state.region for more info.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h-tsai at northwestern.edu  Mon Aug 21 22:21:50 2006
From: h-tsai at northwestern.edu (Hui-Ju Tsai)
Date: Mon, 21 Aug 2006 15:21:50 -0500
Subject: [R] multivariate analysis by using lme
In-Reply-To: <f8e6ff050608211214h5058e136i3fe2eb080bb1b5fa@mail.gmail.com>
Message-ID: <000301c6c55f$6f6542b0$4260a8c0@childrensmemorial.org>

Thanks very much for all your comments and suggestions. First, the data in
my previous mail was made up, not real data. I just used it as an example to
state out my problem.

I completely agree that one should do some plot diagnosis and univariate
models before jumping into a multivariate approach. In our real data, we
have several clinical symptoms to define the disease of interest. I have
done normality check and univariate analyses for each symptom separately. It
is common that some predictor is the same risk factor for several symptoms
leading to the disease outcome. Therefore we think that multivariate
analysis may be potential application to take into account multiple testing
issue, and provide some information for the combination of high-related
clinical measures. Except for testing multi-level factor that I had a
problem to get 'lme' work, I have got consistent results for both univariate
and multivariate approaches. However if the results go to different
directions, it will really bother me because it would be hard to explain the
outputs in terms of clinical perspective.

Thanks,
Hui-Ju Tsai



-----Original Message-----
From: hadley wickham [mailto:h.wickham at gmail.com] 
Sent: Monday, August 21, 2006 2:14 PM
To: Spencer Graves
Cc: Hui-Ju Tsai; r-help at stat.math.ethz.ch
Subject: Re: Re: [R] multivariate analysis by using lme

>        Only after doing the best I could with univariate modeling would
> I then consider multivariate modeling.  And then I'd want to think very
> carefully about whether the multivariate model(s) under consideration
> seemed consistent with the univariate results -- and what else they
> might tell me that I hadn't already gotten from the univariate model.
>  If you've already done all this, I'm impressed.  In the almost 30 years
> since I realized I should try univariate models first and work up to
> multivariate whenever appropriate, I've not found one application where
> the extra effort seemed justified.  R has made this much easier, but I'm
> still looking for that special application that would actually require
> the multivariate tools.

To add to Spencer's comments, I'd strongly recommend you look at your
data before trying to model it.  The attached graph, a scatterplot of
res1 vs res2 values conditional on c1 and c2, with point shape given
by inter, reveals many interesting features of your data:

 * res1 and res2 values are highly correlated
 * inter is constant for a given c1 and c2
 * there are between 1 and 3 points for each level of inter - not very
many and I don't think enough to investigate what the effect of inter
is

The plot was created using the following code:

library(ggplot)
s <- read.table("~/Desktop/sample.txt", header=T)
s <- rename(s, c(two="value"))
s$res2 <- NULL
s <- as.data.frame(cast(s, ... ~ res1))


qplot(X0, X1, c1 ~ c2, data=s, shape=factor(inter))

(note that you will need the latest version of ggplot available from
http://had.co.nz/ggplot)



The contents of this e-mail message and any attachments are private and confidential communications intended solely for the addressee(s) named in this message. If you are not the intended recipient of this message, please 1) immediately notify the sender by reply e-mail and then delete this message and its attachments and 2) do not read, use, distribute disclose or copy this message and/or any attachments.


From srini_iyyer_bio at yahoo.com  Mon Aug 21 22:43:54 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Mon, 21 Aug 2006 13:43:54 -0700 (PDT)
Subject: [R] Escaping " ' " character
Message-ID: <20060821204354.60557.qmail@web38110.mail.mud.yahoo.com>

Dear all:

I have a character object x with ' (single-quote)
character.

x <- c('"hydrolase activity","actin
binding","3',5'-cyclic-nucleotide phosphodiesterase
activity")

I want to write a function that will identify ' and
replaces with \' 

myf <- function(term){
if (grep("'",term))
{ sub("'","\'",term)}
 }
> myf(x)
[1] "hydrolase activity"
[2] "actin binding"
[3] "3',5'-cyclic-nucleotide phosphodiesterase
activity"


In the result "3',5'" is remains unchaned. I expected
that to "3\',5\'-cyclic-nucleotide phosphodiesterase
activity". 

Could any one help me here. 
Thank you.



The reason I am asking is, I have to run another
function to make PREPARE statements for postgresql. 

Example:
"EXECUTE fetch_count_fterm_sql('hydrolase activity');"
                                                      
                 "EXECUTE fetch_count_fterm_sql('actin
binding');"
"EXECUTE
fetch_count_fterm_sql('3',5'-cyclic-nucleotide
phosphodiesterase activity');"

Here the last query statement will raise an exception
and my program fails from there. 

for instance:

mydb=# EXECUTE
fetch_count_fterm_sql('3',5'-cyclic-nucleotide
phosphodiesterase activity');
ERROR:  syntax error at or near "'-cyclic-nucleotide
phosphodiesterase activity'" at character 36



mydb=# EXECUTE
fetch_count_fterm_sql('3\',5\'-cyclic-nucleotide
phosphodiesterase activity');
 count
-------
    17
(1 row)


From qli at math.wustl.edu  Mon Aug 21 23:00:28 2006
From: qli at math.wustl.edu (Qing Li)
Date: Mon, 21 Aug 2006 16:00:28 -0500 (CDT)
Subject: [R] Question about the varmx.pca.fd
Message-ID: <36742.128.252.148.177.1156194028.squirrel@www.math.wustl.edu>

Hi, everyone,

I got puzzled with this "varmx.pca.fd" function which is in the fda
package. According to the description, it carries out the rotation using
the "VARIMAX criterion".

But in fact, if you use this "varmx.pca.fd" on any objects of class
"pca.fd", it will devide the variance equally for each rotated principal
component. This is not the VARIMAX criterion.

As an example here, see the example given in "pca.fd" function:

     daytime   <- (1:365)-0.5
     dayrange  <- c(0,365)
     dayperiod <- 365
     nbasis     <- 65
     dayrange  <- c(0,365)
     daybasis65 <- create.fourier.basis(dayrange, nbasis, dayperiod)
     harmaccelLfd <- vec2Lfd(c(0,(2*pi/365)^2,0), dayrange)
     harmfdPar     <- fdPar(daybasis65, harmaccelLfd, 1e5)
     daytempfd <- data2fd(daily$tempav, daytime, daybasis65,
                          argnames=list("Day", "Station", "Deg C"))
     daytemppcaobj <- pca.fd(daytempfd, nharm=4, harmfdPar)
     daytemppcaobj <- varmx.pca.fd(daytemppcaobj)

If you look at the daytemppcaobj$varprop, you will see
[1] 0.2522899 0.2496471 0.2486374 0.2494257

This is different with the results in many paper and books. Moreover, try
any data and you will get the rotated principal components with the
variance  equally devided.

Can any one help me with the reason for this? Any comments will be greatly
appreciated.

Thank you very much!

Qing


From rab45+ at pitt.edu  Mon Aug 21 22:52:13 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Mon, 21 Aug 2006 16:52:13 -0400
Subject: [R] Retrieving p-values and z values from lmer output
Message-ID: <1156193533.6582.6.camel@localhost.localdomain>

I can't find a way to retrieve z values and p-values from the output
from lmer in the lme4 package. How is this done?

Rick B.


From rab45+ at pitt.edu  Mon Aug 21 23:10:42 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Mon, 21 Aug 2006 17:10:42 -0400
Subject: [R] Retrieving p-values and z values from lmer output
In-Reply-To: <1156193533.6582.6.camel@localhost.localdomain>
References: <1156193533.6582.6.camel@localhost.localdomain>
Message-ID: <1156194643.6582.12.camel@localhost.localdomain>

On Mon, 2006-08-21 at 16:52 -0400, Rick Bilonick wrote:
> I can't find a way to retrieve z values and p-values from the output
> from lmer in the lme4 package. How is this done?
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

I was able to do it this way:

> slotNames(summary(fit.lmer))
 [1] "isG"       "methTitle" "logLik"    "ngrps"     "sigma"     "coefs"
 [7] "vcov"      "REmat"     "AICtab"    "flist"     "Zt"        "X"
[13] "y"         "wts"       "wrkres"    "method"    "useScale"
"family"
[19] "call"      "cnames"    "nc"        "Gp"        "XtX"       "ZtZ"
[25] "ZtX"       "Zty"       "Xty"       "Omega"     "L"         "RZX"
[31] "RXX"       "rZy"       "rXy"       "devComp"   "deviance"  "fixef"
[37] "ranef"     "RZXinv"    "bVar"      "gradComp"  "status"

> slot(summary(fit.lmer),"coefs")
               Estimate Std. Error    z value     Pr(>|z|)
(Intercept) -0.02626325 0.03114850 -0.8431628 3.991374e-01
timeAfter    0.42322569 0.02870844 14.7422023 3.453370e-49

> slot(summary(fit.lmer),"coefs")[,4]
 (Intercept)    timeAfter
3.991374e-01 3.453370e-49

Rick B.


From jz7 at duke.edu  Mon Aug 21 23:39:35 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Mon, 21 Aug 2006 17:39:35 -0400 (EDT)
Subject: [R] question about 'coef' method and fitted_value calculation
Message-ID: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>

Dear all,

I am trying to calculate the fitted values using a ridge model
(lm.ridge(), MASS library). Since the predict() does not work for lm.ridge
object, I want to get the fitted_value from the coefficients information.
The following are the codes I use:

	fit = lm.ridge(myY~myX,lambda=lamb,scales=F,coef=T)
	coeff = fit$coef

However, it seems that "coeff" (or "fit$coef") is not really the
coefficients matrix. From the manual, "Note that these are not on the
original scale and are for use by the 'coef' method...".

Could anyone please point out what is the 'coef' method the manual
mentioned, and how should I get the fitted value? I have tried simple
multiplication of the coeff and my X matrix ("coeff%*%X"). But the results
seems to be in the wrong scale.

Thanks so much!

Sincerely,
Jeny


From gunter.berton at gene.com  Mon Aug 21 23:51:10 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Aug 2006 14:51:10 -0700
Subject: [R] question about 'coef' method and fitted_value calculation
In-Reply-To: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>
Message-ID: <005701c6c56b$ea398ad0$711f210a@gne.windows.gene.com>

I'm ignorant about this, so no answer. But as you seem interested in
coefficient shrinkage, have you tried the lars or lasso2 package?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jz7 at duke.edu
> Sent: Monday, August 21, 2006 2:40 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] question about 'coef' method and fitted_value calculation
> 
> Dear all,
> 
> I am trying to calculate the fitted values using a ridge model
> (lm.ridge(), MASS library). Since the predict() does not work 
> for lm.ridge
> object, I want to get the fitted_value from the coefficients 
> information.
> The following are the codes I use:
> 
> 	fit = lm.ridge(myY~myX,lambda=lamb,scales=F,coef=T)
> 	coeff = fit$coef
> 
> However, it seems that "coeff" (or "fit$coef") is not really the
> coefficients matrix. From the manual, "Note that these are not on the
> original scale and are for use by the 'coef' method...".
> 
> Could anyone please point out what is the 'coef' method the manual
> mentioned, and how should I get the fitted value? I have tried simple
> multiplication of the coeff and my X matrix ("coeff%*%X"). 
> But the results
> seems to be in the wrong scale.
> 
> Thanks so much!
> 
> Sincerely,
> Jeny
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mschwartz at mn.rr.com  Mon Aug 21 23:51:53 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 21 Aug 2006 16:51:53 -0500
Subject: [R] Escaping " ' " character
In-Reply-To: <20060821204354.60557.qmail@web38110.mail.mud.yahoo.com>
References: <20060821204354.60557.qmail@web38110.mail.mud.yahoo.com>
Message-ID: <1156197113.4011.27.camel@localhost.localdomain>

On Mon, 2006-08-21 at 13:43 -0700, Srinivas Iyyer wrote:
> Dear all:
> 
> I have a character object x with ' (single-quote)
> character.
> 
> x <- c('"hydrolase activity","actin
> binding","3',5'-cyclic-nucleotide phosphodiesterase
> activity")
> 
> I want to write a function that will identify ' and
> replaces with \' 
> 
> myf <- function(term){
> if (grep("'",term))
> { sub("'","\'",term)}
>  }
> > myf(x)
> [1] "hydrolase activity"
> [2] "actin binding"
> [3] "3',5'-cyclic-nucleotide phosphodiesterase
> activity"
> 
> 
> In the result "3',5'" is remains unchaned. I expected
> that to "3\',5\'-cyclic-nucleotide phosphodiesterase
> activity". 
> 
> Could any one help me here. 
> Thank you.

<snip>

Srini,

Try this:

 x <- "3',5'-cyclic-nucleotide phosphodiesterase activity"

> x
[1] "3',5'-cyclic-nucleotide phosphodiesterase activity"

> gsub("'", "\\\\'", x)
[1] "3\\',5\\'-cyclic-nucleotide phosphodiesterase activity"


Note that I use gsub() to replace both instances of the ', whereas sub()
will only replace the first.

The escape character itself needs to be escaped when used within the
replacement regex, since the "\" is a metacharacter. You end up with
four "\"s since R also treats the "\" specially.

When you cat() the output, you get:

> x.new
[1] "3\\',5\\'-cyclic-nucleotide phosphodiesterase activity"

> cat(x.new, "\n")
3\',5\'-cyclic-nucleotide phosphodiesterase activity


HTH,

Marc Schwartz


From gunter.berton at gene.com  Mon Aug 21 23:58:12 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 21 Aug 2006 14:58:12 -0700
Subject: [R] multivariate analysis by using lme
In-Reply-To: <f8e6ff050608211214h5058e136i3fe2eb080bb1b5fa@mail.gmail.com>
Message-ID: <006701c6c56c$e5ea5f80$711f210a@gne.windows.gene.com>

FWIW, a small story.

Many a moon ago, I had the great good fortune and honor of driving John
Tukey to periodic consulting sessions at Merck (talk about precious cargo!).
So I got to chat with him about stuff. Basically for the reasons already
elaborated, he also had no use for multivariate methods; but of course when
**HE** elaborated, one paid attention.

Cheers,

-- Bert Gunter

No I'm not expressing an opinion. The discussion just reminded me...
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of hadley wickham
> Sent: Monday, August 21, 2006 12:14 PM
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch; Hui-Ju Tsai
> Subject: Re: [R] multivariate analysis by using lme
> 
> >        Only after doing the best I could with univariate 
> modeling would
> > I then consider multivariate modeling.  And then I'd want 
> to think very
> > carefully about whether the multivariate model(s) under 
> consideration
> > seemed consistent with the univariate results -- and what else they
> > might tell me that I hadn't already gotten from the 
> univariate model.
> >  If you've already done all this, I'm impressed.  In the 
> almost 30 years
> > since I realized I should try univariate models first and work up to
> > multivariate whenever appropriate, I've not found one 
> application where
> > the extra effort seemed justified.  R has made this much 
> easier, but I'm
> > still looking for that special application that would 
> actually require
> > the multivariate tools.
> 
> To add to Spencer's comments, I'd strongly recommend you look at your
> data before trying to model it.  The attached graph, a scatterplot of
> res1 vs res2 values conditional on c1 and c2, with point shape given
> by inter, reveals many interesting features of your data:
> 
>  * res1 and res2 values are highly correlated
>  * inter is constant for a given c1 and c2
>  * there are between 1 and 3 points for each level of inter - not very
> many and I don't think enough to investigate what the effect of inter
> is
> 
> The plot was created using the following code:
> 
> library(ggplot)
> s <- read.table("~/Desktop/sample.txt", header=T)
> s <- rename(s, c(two="value"))
> s$res2 <- NULL
> s <- as.data.frame(cast(s, ... ~ res1))
> 
> 
> qplot(X0, X1, c1 ~ c2, data=s, shape=factor(inter))
> 
> (note that you will need the latest version of ggplot available from
> http://had.co.nz/ggplot)
>


From ggrothendieck at gmail.com  Mon Aug 21 23:59:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 17:59:07 -0400
Subject: [R] question about 'coef' method and fitted_value calculation
In-Reply-To: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>
Message-ID: <971536df0608211459y7857618ftf644a700befc56da@mail.gmail.com>

Try

library(MASS)
y.lmr <- lm.ridge(y ~., longley)
coefs <- print(y.lmr)



On 8/21/06, jz7 at duke.edu <jz7 at duke.edu> wrote:
> Dear all,
>
> I am trying to calculate the fitted values using a ridge model
> (lm.ridge(), MASS library). Since the predict() does not work for lm.ridge
> object, I want to get the fitted_value from the coefficients information.
> The following are the codes I use:
>
>        fit = lm.ridge(myY~myX,lambda=lamb,scales=F,coef=T)
>        coeff = fit$coef
>
> However, it seems that "coeff" (or "fit$coef") is not really the
> coefficients matrix. From the manual, "Note that these are not on the
> original scale and are for use by the 'coef' method...".
>
> Could anyone please point out what is the 'coef' method the manual
> mentioned, and how should I get the fitted value? I have tried simple
> multiplication of the coeff and my X matrix ("coeff%*%X"). But the results
> seems to be in the wrong scale.
>
> Thanks so much!
>
> Sincerely,
> Jeny
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From exonintron at gmail.com  Tue Aug 22 00:15:31 2006
From: exonintron at gmail.com (Sender)
Date: Mon, 21 Aug 2006 15:15:31 -0700
Subject: [R] return tree from .Call
Message-ID: <686bf0c50608211515p272eb52ewe0b658ce8201681a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060821/a20174e4/attachment.pl 

From jrkrideau at yahoo.ca  Tue Aug 22 00:59:27 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 21 Aug 2006 18:59:27 -0400 (EDT)
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <971536df0608211134l506e8bc8p2e84270ecc0d423c@mail.gmail.com>
Message-ID: <20060821225927.64278.qmail@web32811.mail.mud.yahoo.com>


--- Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> Its not part of state.x77.  Its a completely
> separate variable.
> Try ls("package:datasets") and notice its in the
> list
> or try ?state.region and note that its a variable in
> datasets.

Thanks. I was wondering if it was going something like
that.

However, it is a bloody stupid example, at least to a
newbie.  A call to another data.set in what is
supposed to be a simple example is very confusing.

When someone is apparently illustrating a function
with a simple one line command I don't expect them to
call another data set, apparently create a new
variable (Region), and use that new variable as the
grouping variable without a word of explanation of
what the example is doing. 

If I sound a bit annoyed it is because I am. It might
be nice to have an example illlustate the funtion,not
do a couple of other undocumented things as well. 
> 
> 
> On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
> > I was looking ?aggregate and ran the first example
> >
> >  aggregate(state.x77, list(Region = state.region),
> > mean)
> >
> > The variables in state.x77 appear to be :
> > > state.x77
> > Population Income Illiteracy Life Exp Murder HS
> Grad
> > Frost   Area
> >
> > Where is the "state.region" variable coming from?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
>


From jerosenb at fas.harvard.edu  Tue Aug 22 01:06:33 2006
From: jerosenb at fas.harvard.edu (Janet)
Date: Mon, 21 Aug 2006 16:06:33 -0700
Subject: [R] polychor error
Message-ID: <EE668E6E-CCDB-4134-A1E5-428F17E48CED@fas.harvard.edu>


Hi.

Does anyone know whether the following error is a result of a bug or  
a feature?
I can eliminate the error by making ML=F, but I would like to see the  
values of the cut-points and their variance.


tmp.vec<-c(0,  0,  0 , 0  ,0 , 1,  0,  2,  0 , 0,  5  ,5  ,3  ,1,   
0 , 1,  5, 10, 27, 20,  9,  0,  1,  1, 12, 29, 57, 34,  0,  0,  1,   
2, 11, 31, 32)
tmp.mat<-matrix(tmp.vec, nrow=7)
rownames(tmp.mat)<-1:7
colnames(tmp.mat)<-3:7
tmp.pcc<-polychor(tmp.mat, ML=T, std.err=T)
Error in checkmvArgs(lower = lower, upper = upper, mean = mean, corr  
= corr,  :
	at least one element of ?lower? is larger than ?upper?

Thanks,

Janet


From damien.moore at excite.com  Tue Aug 22 01:44:46 2006
From: damien.moore at excite.com (Damien Moore)
Date: Mon, 21 Aug 2006 19:44:46 -0400 (EDT)
Subject: [R] lean and mean lm/glm?
Message-ID: <20060821234446.8CA7C91FB7@xprdmxin.myway.com>


>For very large regression problems there is the biglm package (put you
>data into a database, read in 500,000 rows at a time, and keep updating
>the fit).

thanks. I took a look at biglm and it seems pretty easy to use and, looking at the source, avoids much of the redundancy of lm. Correct me if i'm wrong, but I think it would be virtually impossible to extend to glm, because of the non-linearity in glm models. 

I might hack around at the source code for glm.fit -- I think I can avoid some of the redundancy involved in that routine pretty easily, but it will mean rewriting the summary output code...

cheers
Damien


--- On Mon 08/21, Greg Snow < Greg.Snow at intermountainmail.org > wrote:From: Greg Snow [mailto: Greg.Snow at intermountainmail.org]To: damien.moore at excite.com, r-help at stat.math.ethz.chDate: Mon, 21 Aug 2006 12:01:06 -0600Subject: RE: [R] lean and mean lm/glm?

For very large regression problems there is the biglm package (put you
data into a database, read in 500,000 rows at a time, and keep updating
the fit).

This has not been extended to glm yet.

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damien Moore
Sent: Monday, August 21, 2006 11:49 AM
To: r-help at stat.math.ethz.ch
Subject: [R] lean and mean lm/glm?


Hi All: I'm new to R and have a few questions about getting R to run
efficiently with large datasets.

I'm running R on Windows XP with 1Gb ram (so about 600mb-700mb after the
usual windows overhead). I have a dataset that has 4 million
observations and about 20 variables. I want to run probit regressions on
this data, but can't do this with more than about 500,000 observations
before I start running out of ram (you could argue that I'm getting
sufficient precision with <500,000 obs but lets pretend otherwise).
Loading 500,000 observations into a data frame only takes about 100Mb of
ram, so that isn't the problem. Instead it seems R uses huge amount of
memory when running the glm methods. I called the Fortran routines that
lm and glm use directly but even they create a large number of
extraneous variables in the output (e.g. the Xs, ys, residuals etc) and
during processing. For instance (sample code)

x=runif(1000000)
y=3*x+rnorm(1000000) #I notice this step chews up a lot more than the
7mb of ram required to store y during processing, but cleans up ok
afterwards with a gc() call
X=cbind(x)
p=ncol(X)
n=NROW(y)
ny=NCOL(y)
tol=1e-7
#this is the fortran routine called by lm - regressing y on X here z <-
.Fortran("dqrls", qr = X, n = n, p = p, y = y, ny = ny, tol =
as.double(tol), coefficients = mat.or.vec(p, ny), residuals = y, effects
= y, rank = integer(1), pivot = 1:p, qraux = double(p), work = double(2
* p), PACKAGE = "base")

This code runs very quickly - suggesting that in principle R should have
no problem at all handling very large data sets, but uses >100mb during
processing and z is about a 20mb object. Scaling this up to a much
larger dataset with many variables its easy to see i'm going to run into
problems

My questions:
1. are there any memory efficient alternatives to lm/glm in R?
2. is there any way to prevent the Fortran routine "dqrls" from
producing so much output? (I suspect not since its output has to be
compatible with the summary method, which seems to rely on having a copy
of all variables instead of just references to the relevant variables -
correct me if i'm wrong on this) 3. failing 1 & 2 how easy would it be
to create new versions of lm and glm that don't use so much memory? (Not
that I'm volunteering or anything ;) ). There is no need to hold
individual residuals in memory or make copies of the variables (at least
for my purposes). How well documented is the source code?

cheers
Damien Moore

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From markleeds at verizon.net  Tue Aug 22 01:46:25 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Mon, 21 Aug 2006 19:46:25 -0400
Subject: [R] aggregate example : where is the state.region variable?
References: <20060821225927.64278.qmail@web32811.mail.mud.yahoo.com>
Message-ID: <000b01c6c57c$042ecb70$2e01a8c0@m8d4477f3de884>

these people/experts provide all these packages and documentation as a FAVOR 
and for the fact that they enjoy spreading knowledge/statistical computing 
abilities etc.  It's not their job so I think criticism of the docs and the 
fact that they use a variable from another place is kind of harsh.

                                                                             
                                                      Mark





----- Original Message ----- 
From: "John Kane" <jrkrideau at yahoo.ca>
To: "Gabor Grothendieck" <ggrothendieck at gmail.com>
Cc: "R R-help" <r-help at stat.math.ethz.ch>
Sent: Monday, August 21, 2006 6:59 PM
Subject: Re: [R] aggregate example : where is the state.region variable?


>
> --- Gabor Grothendieck <ggrothendieck at gmail.com>
> wrote:
>
>> Its not part of state.x77.  Its a completely
>> separate variable.
>> Try ls("package:datasets") and notice its in the
>> list
>> or try ?state.region and note that its a variable in
>> datasets.
>
> Thanks. I was wondering if it was going something like
> that.
>
> However, it is a bloody stupid example, at least to a
> newbie.  A call to another data.set in what is
> supposed to be a simple example is very confusing.
>
> When someone is apparently illustrating a function
> with a simple one line command I don't expect them to
> call another data set, apparently create a new
> variable (Region), and use that new variable as the
> grouping variable without a word of explanation of
> what the example is doing.
>
> If I sound a bit annoyed it is because I am. It might
> be nice to have an example illlustate the funtion,not
> do a couple of other undocumented things as well.
>>
>>
>> On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
>> > I was looking ?aggregate and ran the first example
>> >
>> >  aggregate(state.x77, list(Region = state.region),
>> > mean)
>> >
>> > The variables in state.x77 appear to be :
>> > > state.x77
>> > Population Income Illiteracy Life Exp Murder HS
>> Grad
>> > Frost   Area
>> >
>> > Where is the "state.region" variable coming from?
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained,
>> reproducible code.
>> >
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Aug 22 03:03:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Aug 2006 21:03:49 -0400
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <20060821225927.64278.qmail@web32811.mail.mud.yahoo.com>
References: <971536df0608211134l506e8bc8p2e84270ecc0d423c@mail.gmail.com>
	<20060821225927.64278.qmail@web32811.mail.mud.yahoo.com>
Message-ID: <971536df0608211803r164f6139pba6f52b7211f7efa@mail.gmail.com>

It is worthwhile to note that what is being illustrated here is aggregating a
numeric matrix by a factor using the aggregate.default method and, of course,
a factor can't be part of a numeric matrix.

Of course, that is not say that the examples could not be improved in
terms of clarity, simplicity and comprehensiveness (there is no
example of aggregate.data.frame).


On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
>
> --- Gabor Grothendieck <ggrothendieck at gmail.com>
> wrote:
>
> > Its not part of state.x77.  Its a completely
> > separate variable.
> > Try ls("package:datasets") and notice its in the
> > list
> > or try ?state.region and note that its a variable in
> > datasets.
>
> Thanks. I was wondering if it was going something like
> that.
>
> However, it is a bloody stupid example, at least to a
> newbie.  A call to another data.set in what is
> supposed to be a simple example is very confusing.
>
> When someone is apparently illustrating a function
> with a simple one line command I don't expect them to
> call another data set, apparently create a new
> variable (Region), and use that new variable as the
> grouping variable without a word of explanation of
> what the example is doing.
>
> If I sound a bit annoyed it is because I am. It might
> be nice to have an example illlustate the funtion,not
> do a couple of other undocumented things as well.
> >
> >
> > On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
> > > I was looking ?aggregate and ran the first example
> > >
> > >  aggregate(state.x77, list(Region = state.region),
> > > mean)
> > >
> > > The variables in state.x77 appear to be :
> > > > state.x77
> > > Population Income Illiteracy Life Exp Murder HS
> > Grad
> > > Frost   Area
> > >
> > > Where is the "state.region" variable coming from?
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code.
> > >
> >
>
>
> __________________________________________________
> Do You Yahoo!?
> Tired of spam?  Yahoo! Mail has the best spam protection around
> http://mail.yahoo.com
>


From r.nieuwenhuis at student.ru.nl  Tue Aug 22 04:08:44 2006
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Tue, 22 Aug 2006 04:08:44 +0200
Subject: [R] Total (un)standardized effects in SEM?
Message-ID: <D2820684-54A8-4A91-B645-4A2C32860F13@student.ru.nl>

Hi there,

as a student sociology, I'm starting to learn about SEM. The course I  
follow is based on LISREL, but I want to use the SEM-package on R  
parallel to it.

Using LISREL, I found it to be very usable to be able to see the  
total direct and total indirect effects (standardized and  
unstandardized) in the output. Can I create these effects using R? I  
know how to calculate them 'by hand', but I'd like to be able to do  
it automatically.

If one of you is able to give some information, I'd be very gratefull,

with high regards,

Rense Nieuwenhuis


From mikewolfgang at gmail.com  Tue Aug 22 06:19:18 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Tue, 22 Aug 2006 00:19:18 -0400
Subject: [R] gl1ce warning message?
Message-ID: <e668df8c0608212119m1044cda0o54e4e03572553693@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/3e79371c/attachment.pl 

From arun.kumar.saha at gmail.com  Tue Aug 22 08:41:19 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Tue, 22 Aug 2006 12:11:19 +0530
Subject: [R] Adding Grid lines
Message-ID: <d4c57560608212341q769d02c4jed72769be67b85f4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/f998d621/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Aug 22 08:44:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Aug 2006 07:44:55 +0100 (BST)
Subject: [R] return tree from .Call
In-Reply-To: <686bf0c50608211515p272eb52ewe0b658ce8201681a@mail.gmail.com>
References: <686bf0c50608211515p272eb52ewe0b658ce8201681a@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608220737190.19548@gannet.stats.ox.ac.uk>

Please see the posting guide: low-level programming questions to R-devel.

On Mon, 21 Aug 2006, Sender wrote:

> Hello:
> 
> I was hoping to get some advice about how to return a tree (basically a
> linked list -- with each node containing a parent, left, and right  node
> pointers) from a C routine back into R. Each node itself contains several
> attributes (a double, a char *, an int, and a void * )
> 
> Initially I was thinking I could just return to R a SEXP containing a
> pointer to the Root Node, but then realized that the pointer would be
> useless since C probably frees the memory after I leave the routine.

That's not true in general: it depends how you allocated them.  With 
malloc, you need to do a free.  External pointer types and finalizers are 
available for just this, and you can also use the memory of R structures.

> Since trees vary in length (or height) I need to be able to loop thru the
> tree until all Nodes have been visited and saved in a SEXP. I'm really new
> to handling R objects from within C, and converting and returning a large
> tree structure is daunting. Here is some rough code I was thinking about
> using to do this.  Any suggestions or help will be greatly appreciated!

Since elements of a protected list are protected, you can simplify the use 
of PROTECTs (and may have to in order to avoid overflows) by linking into 
a list at allocation time.  Do node first, then its elements.

> SEXP list ;
> 
> PROTECT( list = allocVector(VECSXP, tree->size) ) ;
> 
> for( i = 0; i < tree->size; ++i ){
>      SEXP node, tree_double, tree_char, tree_int, tree_voidPTR ;
> 
>      PROTECT( tree_double = NEW_NUMERIC( tree->weight ) ) ;
>      PROTECT( tree_char = NEW_CHARACTER( tree->name ) ) ;
>      PROTECT( tree_int = NEW_INTEGER( tree->exons ) ) ;
>      PROTECT( tree_voidPTR = NEW_CHARACTER( tree->data ) ) ;   ??? not sure
> about this...
> 
>      PROTECT( node = allocVector( VECSXP, 4 ) ) ;
>      SET_VECTOR_ELT( node, 0, tree_double) ;
>      SET_VECTOR_ELT( node, 1, tree_char) ;
>      SET_VECTOR_ELT( node, 2, tree_int) ;
>      SET_VECTOR_ELT( node, 3, tree_voidPTR ) ;
> 
>      SET_VECTOR_ELT( list, i, node ) ;
> }
> 
> UNPROTECT( tree->size * 4 ) ;  ??? Tricky..
> return( list ) ;
> 
> Look reasonable ?
> 
> THANKS !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Tue Aug 22 08:56:18 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Aug 2006 08:56:18 +0200
Subject: [R] Adding Grid lines
In-Reply-To: <d4c57560608212341q769d02c4jed72769be67b85f4@mail.gmail.com>
References: <d4c57560608212341q769d02c4jed72769be67b85f4@mail.gmail.com>
Message-ID: <44EAAA92.3060603@statistik.uni-dortmund.de>



Arun Kumar Saha wrote:
> Dear all R users,
> 
> Can anyone please tell me how to add grid lines in any plot in R including
> in Histogram, QQ plot etc?

Have you ever typed ?grid before posting?

Uwe Ligges


> Thanks and regards,
> Arun
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From goran.brostrom at gmail.com  Tue Aug 22 08:59:55 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Tue, 22 Aug 2006 08:59:55 +0200
Subject: [R] [R-pkgs] New version of glmmML
In-Reply-To: <17641.42103.990929.53681@stat.math.ethz.ch>
References: <148ed8180608210212k6a7858d9s3e01011588fec2ec@mail.gmail.com>
	<17641.42103.990929.53681@stat.math.ethz.ch>
Message-ID: <148ed8180608212359s919366v91ee680789124ee2@mail.gmail.com>

On 8/21/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> Hi G?ran,
>
> >>>>> "GB" == G?ran Brostr?m <goran.brostrom at gmail.com>
> >>>>>     on Mon, 21 Aug 2006 11:12:49 +0200 writes:
>
>     GB> A new version, 0.65-1, of glmmML is now on CRAN. It is a major rewrite
>     GB> of the inner structures, so frequent updates (bug fixes) may be
>     GB> expected for some time.
>
>
>     GB> News:
>
>     [............]
>
> Sorry for my lazy question :
>
>    What does the package do that lmer() does not?

Hi Martin,

most of the things I mentioned in the announcement. To appreciate it I
think you have to study 'lmer' as well (defeat your laziness!;). One
thing I didn't mention is the possibility to fit a model at a fixed
value of the standard deviation of the random effects. Utilising that,
it is easy to create a profile likelihood (as a function of sigma),
which can be very helpful in certain cases.

HTH,

G?ran


From bhs2 at mevik.net  Tue Aug 22 09:09:56 2006
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Tue, 22 Aug 2006 09:09:56 +0200
Subject: [R] Adding Grid lines
In-Reply-To: <d4c57560608212341q769d02c4jed72769be67b85f4@mail.gmail.com> (Arun
	Kumar Saha's message of "Tue, 22 Aug 2006 12:11:19 +0530")
References: <d4c57560608212341q769d02c4jed72769be67b85f4@mail.gmail.com>
Message-ID: <m08xlhtfej.fsf@bar.nemo-project.org>

Perhaps ?grid will help you.

-- 
Bj?rn-Helge Mevik


From sfalcon at fhcrc.org  Tue Aug 22 04:21:21 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 21 Aug 2006 19:21:21 -0700
Subject: [R] [R-pkgs] ANN: 'weaver' package, caching for Sweave
Message-ID: <m2u045zf1a.fsf@ziti.fhcrc.org>

Hi all,

I've added a new package 'weaver' to the BioC repository:

http://www.bioconductor.org/packages/1.9/bioc/html/weaver.html

The weaver package provides extensions to the Sweave utilities
included in R's base package.  The focus of the extensions is on
caching computationally expensive (time consuming) code chunks in
Sweave documents.

Why would you want to cache code chunks?  If your Sweave document
includes one or more code chunks that take a long time to compute, you
may find it frustrating to make small changes to the document.  Each
run requires recomputing the "expensive" code chunks.  If these chunks
aren't changing, you can benefit from the caching provided by weaver.

To install:

  source("http://bioconductor.org/biocLite.R")
  biocLite("weaver")

If you give it a try and have any feedback, drop me a note.

Best Wishes,

+ seth

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From attenka at utu.fi  Tue Aug 22 11:31:49 2006
From: attenka at utu.fi (kone)
Date: Tue, 22 Aug 2006 12:31:49 +0300
Subject: [R] Successive subsets from a vector?
Message-ID: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/1b4812e6/attachment.pl 

From s.martino at tno.it  Tue Aug 22 11:45:01 2006
From: s.martino at tno.it (Sergio Martino)
Date: Tue, 22 Aug 2006 11:45:01 +0200
Subject: [R] How to share variables
References: <005d01c6b636$24f1e680$3c32428a@msi>
	<971536df0608020650vbc87b7n3daa28acfc792900@mail.gmail.com>
	<003901c6c4f2$c63ddab0$3c32428a@msi>
	<971536df0608210059w1c9e85a7h640290ddbd8de997@mail.gmail.com>
Message-ID: <007001c6c5cf$a38f7db0$3c32428a@msi>

Hi

Thanks again. I hope not to waste to much of your time.

I delete some lines of your answer

> Each time myfun is run a new environment is created to hold
> its local variables.  The parent of that environment is e in
> this example by construction.  So e and the environment that
> is temporarily created to hold myfun's variables are distinct.

This means that the enviroment is duplicated, ie it is present twince in
memory?
I must keep some big variables and it will be a waste of memory; moreover if
I update a value it will be lost.

> > If I can use inside myfun the variable as e$dat (without changing the
> > enviroment (no environment(myfun) <- e statement)) than it will be ok.
>
> Yes you can.  You can either make sure that e is visible to myfun
> via normal scoping rules or pass it explicitly:
>
> e <- new.env()
> e$dat <- 1:3
> myfun <- function(x) sum(x + e$dat)
> myfun(10)
>

Hit!!!
It solves the problem.
A small drawback is that I need to modify the name of each occurrence of the
variable.


> # or passing e explicitly
>
> myfun2 <- function(x, e) sum(x + e$dat)
> myfun2(10, e)
>

Any overhead in passing the environment? Is it a pointer?

Sergio


From i.m.s.white at ed.ac.uk  Tue Aug 22 11:49:08 2006
From: i.m.s.white at ed.ac.uk (i.m.s.white)
Date: Tue, 22 Aug 2006 10:49:08 +0100
Subject: [R] Fwd: Re:  Finney's fiducial confidence intervals of LD50
In-Reply-To: <20060821154624.34419.qmail@web39803.mail.mud.yahoo.com>
References: <20060821154624.34419.qmail@web39803.mail.mud.yahoo.com>
Message-ID: <20060822094907.GA6029@trotter.cap.ed.ac.uk>

Finney's method for finding the confidence interval for a ratio of
parameters is quite simple and is probably described in his book
'Probit analysis'. It is also known as Fieller's method so an
RSiteSearch on 'Fieller' might show something useful.


On Mon, Aug 21, 2006 at 08:46:24AM -0700, carlos riveira wrote:
> thanks a lot Renaud. 
>   but i was interested in Finney's fiducial confidence intervals of LD50 so to obtain comparable results with SPSS. 
>    
>   But your reply leads me to the next question: does anybody know what is the best method (asymptotic,  bootstrap etc.) for calculating confidence intervals of LD50? 
>    
>   i could "get rid" of Finney's fiducial confidence intervals but only if there was a better method..
>    
>   any idea? 
>    
>   
> 
> Renaud Lancelot <renaud.lancelot at gmail.com> wrote:
>   Date: Mon, 21 Aug 2006 16:35:49 +0200
> From: "Renaud Lancelot" <renaud.lancelot at gmail.com>
> To: "carlos riveira" <carlos.riveira at yahoo.com>
> Subject: Re: [R] Finney's fiducial confidence intervals of LD50
> CC: r-help at stat.math.ethz.ch
> 
> Sorry there was a typo in my previous reply:
> 
> > D50 <- 10^c(logD50 + c(0, -1.96, 1.96) * attr(logD50, "SE"))
> > names(D50) <- c("D50", "lower", "upper")
> > D50
> D50 lower upper
> 140.8353 103.3171 191.9777
> 
> Best,
> 
> Renaud
> 
> 2006/8/21, Renaud Lancelot :
> > I don't know what Finney's fiducial confidence interval is but if your
> > problem is to handle the output of dose.p (from MASS), you can do as
> > follows:
> >
> > > library(MASS)
> > > Response <- c(0, 7, 26, 27, 0, 5, 13, 29, 0, 4, 11, 25)
> > > Tot <- rep(30.5, 12)
> > > Dose <- rep(c(10, 40, 160, 640), 3)
> > > fm <- glm(Response/Tot ~ log10(Dose), family = quasibinomial(link = probit))
> > > logD50 <- dose.p(fm, cf = 1:2, p = 0.5)
> > > D50 <- 10^c(logD50 + c(1, -1.96, 1.96) * attr(logD50, "SE"))
> > > names(D50) <- c("D50", "lower", "upper")
> > > D50
> > D50 lower upper
> > 164.9506 103.3171 191.9777
> >
> > Best,
> >
> > Renaud
> >
> > 2006/8/21, carlos riveira :
> > > I am working with Probit regression (I cannot switch to logit) can anybody help me in finding out how to obtain with R Finney's fiducial confidence intervals for the levels of the predictor (Dose) needed to produce a proportion of 50% of responses(LD50, ED50 etc.)?
> > > If the Pearson chi-square goodness-of-fit test is significant (by default), a heterogeneity factor should be used to calculate the limits.
> > >
> > > Response<-c(0,7,26,27,0,5,13,29,0,4,11,25)
> > > Tot<-rep(30.5,12)
> > > Dose<-rep(c(10,40,160,640),3)
> > > probit<-glm(formula = Response/Tot~ log10(Dose), family=quasibinomial
> > > (link=probit))
> > > D50<- round(10^(dose.p(probit,cf=1:2,p=0.5)))
> > >
> > > #This is what SPSS calculates. I would like to reproduce these results with R:
> > > #SPSS RESULTS:
> > > #PRNT50= 140,83525
> > > #CI = 98,37857;205,34483
> > > #Regr.coeff= 1,91676 (S.E.=0,16765)
> > > #Intercept=-4,11856 (S.E.=0,36355)
> > > Thanks a lot for your help.
> > >
> > > Carlos
> > >
> > > __________________________________________________
> > >
> > >
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Renaud LANCELOT
> > D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
> > Directeur adjoint charg? des affaires scientifiques
> >
> > CIRAD, Animal Production and Veterinary Medicine Department
> > Deputy director for scientific affairs
> >
> > Campus international de Baillarguet
> > TA 30 / B (B?t. B, Bur. 214)
> > 34398 Montpellier Cedex 5 - France
> > T?l +33 (0)4 67 59 37 17
> > Secr. +33 (0)4 67 59 39 04
> > Fax +33 (0)4 67 59 37 95
> >
> 
> 
> -- 
> Renaud LANCELOT
> D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
> Directeur adjoint charg? des affaires scientifiques
> 
> CIRAD, Animal Production and Veterinary Medicine Department
> Deputy director for scientific affairs
> 
> Campus international de Baillarguet
> TA 30 / B (B?t. B, Bur. 214)
> 34398 Montpellier Cedex 5 - France
> T?l +33 (0)4 67 59 37 17
> Secr. +33 (0)4 67 59 39 04
> Fax +33 (0)4 67 59 37 95
> 
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
************************************************
*    I.White                                   *
*    University of Edinburgh                   *
*    Ashworth Laboratories, West Mains Road    *
*    Edinburgh EH9 3JT                         *
*    Fax: 0131 650 6564   Tel: 0131 650 5490   *
*    E-mail: i.m.s.white at ed.ac.uk              *


From jholtman at gmail.com  Tue Aug 22 12:08:04 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 22 Aug 2006 06:08:04 -0400
Subject: [R] Successive subsets from a vector?
In-Reply-To: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
References: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
Message-ID: <644e1f320608220308n2acf2b6dpf1be1143dd361aca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/8190d0bf/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Aug 22 12:13:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Aug 2006 11:13:48 +0100 (BST)
Subject: [R] Successive subsets from a vector?
In-Reply-To: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
References: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
Message-ID: <Pine.LNX.4.64.0608221049570.9453@gannet.stats.ox.ac.uk>

    embed(VECTOR, 5)[, 5:1]

gives the subsets, so something like

    apply(embed(VECTOR, 5)[, 5:1], 1, paste, collapse="")

does the job.

The following is a bit more efficient

    ind <- 1:(length(VECTOR)-4)
    do.call(paste, c(lapply(0:4, function(j) VECTOR[ind+j]), sep=""))

but by looking at how embed() works it could be made as efficient.

Larger example:

VECTOR <- sample(1:10, 1e5, replace=TRUE)
> system.time(apply(embed(VECTOR, 5)[, 5:1], 1, paste, collapse=""))
[1] 5.73 0.05 5.81   NA   NA
> system.time({ind <- 1:(length(VECTOR)-4)
+ do.call(paste, c(lapply(0:4, function(j) VECTOR[ind+j]), sep=""))
+ })
[1] 1.00 0.01 1.01   NA   NA

The loop method took 195 secs.  Just assigning to an answer of the correct 
length reduced this to 5 secs.  e.g. use

    ADDRESSES <- character(length(VECTOR)-4)

Moral: don't grow vectors repeatedly.

On Tue, 22 Aug 2006, kone wrote:

> I'd like to pick every imbricated five character long subsets from a 
> vector. I guess there is some efficient way to do this without loops...
> Here is a for-loop-version and a model for output:
> 
> VECTOR=c(1,4,2,6,5,0,11,10,4,3,6,8,6);
> 
> ADDRESSES=c();

You do not need the semicolons, and they just confuse readers.

> for(i in 1:(length(VECTOR)-4)){
> 	ADDRESSES[i]=paste(VECTOR[i:(i+4)],collapse="")	
> }
> 
>  > ADDRESSES
> [1] "14265"   "42650"   "265011"  "6501110" "5011104" "0111043" 
> "1110436" "104368"
> [9] "43686"
> 
> 
> Atte Tenkanen
> University of Turku, Finland
> 
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From researchjj at gmail.com  Tue Aug 22 12:42:31 2006
From: researchjj at gmail.com (j.joshua thomas)
Date: Tue, 22 Aug 2006 18:42:31 +0800
Subject: [R] Rgraphviz installation Problem
Message-ID: <b4485c4c0608220342l449b2444m95f6bb210d24ad67@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/e80e3dbb/attachment.pl 

From r.hankin at noc.soton.ac.uk  Tue Aug 22 12:47:26 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 22 Aug 2006 11:47:26 +0100
Subject: [R] big numbers
Message-ID: <3A57976A-D6E7-41CB-A793-BDBB1BE7CC8F@soc.soton.ac.uk>

Hi

Can I get R to handle really big numbers?    I am not interested
in more than (say) 10 sig figs, but I would like to deal with numbers
up to, say, 10^10000.

If

a <- 10^10000
b <- pi* a

I would like  "a+b" to return 3.1415926e10000.


Toy example, illustrating why I can't deal with log(a) and log(b),   
follows.



f <- function(a,n=100){
   out <- rep(0,n)
   out[1] <- a
   for(i in 2:n){
     out[i] <- sum(exp(out[1:i])) + rexp(1)
   }
   return(log(out))
}


then f(1,10)  has infinities in it, even though the values should be  
moderate size.

What are my options here?

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From csardi at rmki.kfki.hu  Tue Aug 22 12:50:57 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 22 Aug 2006 12:50:57 +0200
Subject: [R] Rgraphviz installation Problem
In-Reply-To: <b4485c4c0608220342l449b2444m95f6bb210d24ad67@mail.gmail.com>
References: <b4485c4c0608220342l449b2444m95f6bb210d24ad67@mail.gmail.com>
Message-ID: <20060822105057.GC28588@localdomain>

You also need to install the 'graph' package, i think it is also available
from bioconductor.org. Other packages might be needed as well....

Gabor

On Tue, Aug 22, 2006 at 06:42:31PM +0800, j.joshua thomas wrote:
> Dear Robert,
> 
> Thanks for your time.
> I have downloaded Rgraphviz (windows binary) from www.bioconductor.org
> and put inside R2.3.0 library then  i installed from the local zip
> its says package 'graph' couldnot be loaded.
> 
> Am i doing the installation correctly? Still the new user.
> 
> Can you guide me sir?
> 
> JJ
> 
> -- 
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From attenka at utu.fi  Tue Aug 22 13:19:38 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Tue, 22 Aug 2006 14:19:38 +0300
Subject: [R] Successive subsets from a vector?
In-Reply-To: <Pine.LNX.4.64.0608221049570.9453@gannet.stats.ox.ac.uk>
References: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
	<Pine.LNX.4.64.0608221049570.9453@gannet.stats.ox.ac.uk>
Message-ID: <f70eb9542f6e.44eb127a@utu.fi>

Thanks!

I have used tons of for- and while-loops (I'm ashamed to reveal these scripts, but I'm primarily a musician;-) http://users.utu.fi/attenka/SetTheoryScripts.r), taken some or more cup of cocoa and mostly been happy ;-) Now I got so many new ways to do these things, that it takes a while to ruminate all the ideas here.

Atte

>    embed(VECTOR, 5)[, 5:1]
> 
> gives the subsets, so something like
> 
>    apply(embed(VECTOR, 5)[, 5:1], 1, paste, collapse="")
> 
> does the job.
> 
> The following is a bit more efficient
> 
>    ind <- 1:(length(VECTOR)-4)
>    do.call(paste, c(lapply(0:4, function(j) VECTOR[ind+j]), sep=""))
> 
> but by looking at how embed() works it could be made as efficient.
> 
> Larger example:
> 
> VECTOR <- sample(1:10, 1e5, replace=TRUE)
> > system.time(apply(embed(VECTOR, 5)[, 5:1], 1, paste, collapse=""))
> [1] 5.73 0.05 5.81   NA   NA
> > system.time({ind <- 1:(length(VECTOR)-4)
> + do.call(paste, c(lapply(0:4, function(j) VECTOR[ind+j]), sep=""))
> + })
> [1] 1.00 0.01 1.01   NA   NA
> 
> The loop method took 195 secs.  Just assigning to an answer of the 
> correct 
> length reduced this to 5 secs.  e.g. use
> 
>    ADDRESSES <- character(length(VECTOR)-4)
> 
> Moral: don't grow vectors repeatedly.
> 
> On Tue, 22 Aug 2006, kone wrote:
> 
> > I'd like to pick every imbricated five character long subsets 
> from a 
> > vector. I guess there is some efficient way to do this without 
> loops...> Here is a for-loop-version and a model for output:
> > 
> > VECTOR=c(1,4,2,6,5,0,11,10,4,3,6,8,6);
> > 
> > ADDRESSES=c();
> 
> You do not need the semicolons, and they just confuse readers.
> 
> > for(i in 1:(length(VECTOR)-4)){
> > 	ADDRESSES[i]=paste(VECTOR[i:(i+4)],collapse="")	
> > }
> > 
> >  > ADDRESSES
> > [1] "14265"   "42650"   "265011"  "6501110" "5011104" "0111043" 
> > "1110436" "104368"
> > [9] "43686"
> > 
> > 
> > Atte Tenkanen
> > University of Turku, Finland
> > 
> > 	[[alternative text/enriched version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html> and provide commented, minimal, self-contained, 
> reproducible code.
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From SND at bas.ac.uk  Tue Aug 22 13:29:38 2006
From: SND at bas.ac.uk (Sara-Jane Dunn)
Date: Tue, 22 Aug 2006 12:29:38 +0100
Subject: [R] listing a sequence of vectors in a matrix
Message-ID: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/6dbdc3bb/attachment.pl 

From jrkrideau at yahoo.ca  Tue Aug 22 13:44:26 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 22 Aug 2006 07:44:26 -0400 (EDT)
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <971536df0608211803r164f6139pba6f52b7211f7efa@mail.gmail.com>
Message-ID: <20060822114426.58502.qmail@web32809.mail.mud.yahoo.com>


--- Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> It is worthwhile to note that what is being
> illustrated here is aggregating a
> numeric matrix by a factor using the
> aggregate.default method and, of course,
> a factor can't be part of a numeric matrix.

And this is exactly what the example should point out!
Of course it might have been better to use a
data.frame example with a factor to illustrate that
one can actually use aggregate with an element of the
dataset.

If we look at the example:
-------------------------------------
## Compute the averages for the variables in
'state.x77', grouped
     ## according to the region (Northeast, South,
North Central, West) that
     ## each state belongs to.
     aggregate(state.x77, list(Region = state.region),
mean)
--------------------------------------
Here, we have nothing to indicate that state.x77 is a
matrix, ( yes, one can use class to find out, assuming
one knows it exists AND thinks to use it AND
understands the implications)and nothing to explain
why the example is making a call to another dataset.  

It is pretty obvious, after a few minutes of going
"Where the blazes did "that" come from", that there is
no factor in the dataset but why there is not one and
why a call to another dataset is totally opaque.  


> 
> Of course, that is not say that the examples could
> not be improved i
> terms of clarity, simplicity and comprehensiveness
> (there is no
> example of aggregate.data.frame).

Yes they could, considerably. Two of the three
examples make a call to another dataset with no
warning or explanation. 

The time series example is, actually, pretty clear.

Thanks very much for the reply.




> >
> > --- Gabor Grothendieck <ggrothendieck at gmail.com>
> > wrote:
> >
> > > Its not part of state.x77.  Its a completely
> > > separate variable.
> > > Try ls("package:datasets") and notice its in the
> > > list
> > > or try ?state.region and note that its a
> variable in
> > > datasets.
> >
> > Thanks. I was wondering if it was going something
> like
> > that.
> >
> > However, it is a bloody stupid example, at least
> to a
> > newbie.  A call to another data.set in what is
> > supposed to be a simple example is very confusing.
> >
> > When someone is apparently illustrating a function
> > with a simple one line command I don't expect them
> to
> > call another data set, apparently create a new
> > variable (Region), and use that new variable as
> the
> > grouping variable without a word of explanation of
> > what the example is doing.
> >
> > If I sound a bit annoyed it is because I am. It
> might
> > be nice to have an example illlustate the
> funtion,not
> > do a couple of other undocumented things as well.
> > >
> > >

> wrote:
> > > > I was looking ?aggregate and ran the first
> example
> > > >
> > > >  aggregate(state.x77, list(Region =
> state.region),
> > > > mean)
> > > >
> > > > The variables in state.x77 appear to be :
> > > > > state.x77
> > > > Population Income Illiteracy Life Exp Murder
> HS
> > > Grad
> > > > Frost   Area
> > > >
> > > > Where is the "state.region" variable coming
> from?
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal,
> self-contained,
> > > reproducible code.
> > > >
> > >
> >
> >
> > __________________________________________________


> protection around

> >
>


From jrkrideau at yahoo.ca  Tue Aug 22 13:52:03 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 22 Aug 2006 07:52:03 -0400 (EDT)
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <000b01c6c57c$042ecb70$2e01a8c0@m8d4477f3de884>
Message-ID: <20060822115203.54298.qmail@web32814.mail.mud.yahoo.com>


--- MARK LEEDS <markleeds at verizon.net> wrote:

> these people/experts provide all these packages and
> documentation as a FAVOR 
> and for the fact that they enjoy spreading
> knowledge/statistical computing 
> abilities etc.  It's not their job so I think
> criticism of the docs and the 
> fact that they use a variable from another place is
> kind of harsh.
>                                 
>  Mark
> 

I am very appeciative of the time, expertise and great
helpfulness that I have seen in the R community.  

If there is no criticism of R then how do we find out
about problems that may exist?

> ----- Original Message ----- 
> From: "John Kane" <jrkrideau at yahoo.ca>
> To: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> Cc: "R R-help" <r-help at stat.math.ethz.ch>
> Sent: Monday, August 21, 2006 6:59 PM
> Subject: Re: [R] aggregate example : where is the
> state.region variable?
> 
> 
> >
> > --- Gabor Grothendieck <ggrothendieck at gmail.com>
> > wrote:
> >
> >> Its not part of state.x77.  Its a completely
> >> separate variable.
> >> Try ls("package:datasets") and notice its in the
> >> list
> >> or try ?state.region and note that its a variable
> in
> >> datasets.
> >
> > Thanks. I was wondering if it was going something
> like
> > that.
> >
> > However, it is a bloody stupid example, at least
> to a
> > newbie.  A call to another data.set in what is
> > supposed to be a simple example is very confusing.
> >
> > When someone is apparently illustrating a function
> > with a simple one line command I don't expect them
> to
> > call another data set, apparently create a new
> > variable (Region), and use that new variable as
> the
> > grouping variable without a word of explanation of
> > what the example is doing.
> >
> > If I sound a bit annoyed it is because I am. It
> might
> > be nice to have an example illlustate the
> funtion,not
> > do a couple of other undocumented things as well.
> >>
> >>
> >> On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
> >> > I was looking ?aggregate and ran the first
> example
> >> >
> >> >  aggregate(state.x77, list(Region =
> state.region),
> >> > mean)
> >> >
> >> > The variables in state.x77 appear to be :
> >> > > state.x77
> >> > Population Income Illiteracy Life Exp Murder HS
> >> Grad
> >> > Frost   Area
> >> >
> >> > Where is the "state.region" variable coming
> from?
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained,
> >> reproducible code.
> >> >
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> 
> 
>


From thomas at fam-kuster.ch  Tue Aug 22 14:27:28 2006
From: thomas at fam-kuster.ch (Thomas Kuster)
Date: Tue, 22 Aug 2006 14:27:28 +0200
Subject: [R] boxplot order of the levels
Message-ID: <200608221427.29248.thomas@fam-kuster.ch>

hello

i drew a boxplot with:
> boxplot(voxit$AGE ~ voxit$A02X)
and the boxes are from left to right:
Ja Leer Nein wn k.A.

> levels(voxit$A02X)
[1] "Ja"   "Leer" "Nein" "wn"   "k.A." "98"
there are no entries with 98

but i want:
Ja Nein Leer wn k.A.

how can i change the order of the boxes

bye
thomas


From lothar.schmid at googlemail.com  Tue Aug 22 14:28:18 2006
From: lothar.schmid at googlemail.com (Lothar Schmid)
Date: Tue, 22 Aug 2006 14:28:18 +0200
Subject: [R] How to interrupt running computation?
Message-ID: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>

If I start a computation in R, how can I interrupt it?
I' using R 2.1.0.

Thanks for your help,

Lothar


From ggrothendieck at gmail.com  Tue Aug 22 14:32:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Aug 2006 08:32:09 -0400
Subject: [R] How to share variables
In-Reply-To: <007001c6c5cf$a38f7db0$3c32428a@msi>
References: <005d01c6b636$24f1e680$3c32428a@msi>
	<971536df0608020650vbc87b7n3daa28acfc792900@mail.gmail.com>
	<003901c6c4f2$c63ddab0$3c32428a@msi>
	<971536df0608210059w1c9e85a7h640290ddbd8de997@mail.gmail.com>
	<007001c6c5cf$a38f7db0$3c32428a@msi>
Message-ID: <971536df0608220532n46bc61d7i25530fd0cf6a1583@mail.gmail.com>

On 8/22/06, Sergio Martino <s.martino at tno.it> wrote:
> Hi
>
> Thanks again. I hope not to waste to much of your time.
>
> I delete some lines of your answer
>
> > Each time myfun is run a new environment is created to hold
> > its local variables.  The parent of that environment is e in
> > this example by construction.  So e and the environment that
> > is temporarily created to hold myfun's variables are distinct.
>
> This means that the enviroment is duplicated, ie it is present twince in
> memory?

Each time myfun starts up a new environment comes into being
that contains x and each time it completes that environment is
destroyed.

> I must keep some big variables and it will be a waste of memory; moreover if
> I update a value it will be lost.

If you update a local variable then its lost upon exit (of course you
could return the variable or return the environment inside the
function) but if you update it in e then its not lost.

>
> > > If I can use inside myfun the variable as e$dat (without changing the
> > > enviroment (no environment(myfun) <- e statement)) than it will be ok.
> >
> > Yes you can.  You can either make sure that e is visible to myfun
> > via normal scoping rules or pass it explicitly:
> >
> > e <- new.env()
> > e$dat <- 1:3
> > myfun <- function(x) sum(x + e$dat)
> > myfun(10)
> >
>
> Hit!!!
> It solves the problem.
> A small drawback is that I need to modify the name of each occurrence of the
> variable.

That's why in an earlier example we set the environment of myfun
to e.

>
>
> > # or passing e explicitly
> >
> > myfun2 <- function(x, e) sum(x + e$dat)
> > myfun2(10, e)
> >
>
> Any overhead in passing the environment? Is it a pointer?

?system.time

to experiment with timings.

>
> Sergio
>
>


From sarah.goslee at gmail.com  Tue Aug 22 14:32:24 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 22 Aug 2006 08:32:24 -0400
Subject: [R] How to interrupt running computation?
In-Reply-To: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
References: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
Message-ID: <efb536d50608220532p4785c7f2h7b114ce2584f82ab@mail.gmail.com>

On 8/22/06, Lothar Schmid <lothar.schmid at googlemail.com> wrote:
> If I start a computation in R, how can I interrupt it?
> I' using R 2.1.0.

Hi Lothar,

What do you mean by "interrupt" - cancel completely and
return to the prompt? Or pause, then resume? Your operating
system would be helpful too.

If the former, try Esc on Windows or Ctrl-C on Linux.
If the latter, Ctrl-Z will send your R session to the background
on Linux, and fg will bring it back. I don't think there's a
Windows equivalent.

Sarah


-- 
Sarah Goslee
http://www.stringpage.com


From h.wickham at gmail.com  Tue Aug 22 14:37:38 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 22 Aug 2006 07:37:38 -0500
Subject: [R] Successive subsets from a vector?
In-Reply-To: <Pine.LNX.4.64.0608221049570.9453@gannet.stats.ox.ac.uk>
References: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
	<Pine.LNX.4.64.0608221049570.9453@gannet.stats.ox.ac.uk>
Message-ID: <f8e6ff050608220537r74e83258seef0630ce0ebec4a@mail.gmail.com>

> The loop method took 195 secs.  Just assigning to an answer of the correct
> length reduced this to 5 secs.  e.g. use
>
>     ADDRESSES <- character(length(VECTOR)-4)
>
> Moral: don't grow vectors repeatedly.

Other languages (eg. Java) grow the size of the vector independently
of the number of observations in it (I think Java doubles the size
whenever the vector is filled), thus changing O(n) behaviour to O(log
n).  I've always wondered why R doesn't do this.

Hadley


From MSchwartz at mn.rr.com  Tue Aug 22 14:40:58 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 22 Aug 2006 07:40:58 -0500
Subject: [R] boxplot order of the levels
In-Reply-To: <200608221427.29248.thomas@fam-kuster.ch>
References: <200608221427.29248.thomas@fam-kuster.ch>
Message-ID: <1156250458.5575.10.camel@localhost.localdomain>

On Tue, 2006-08-22 at 14:27 +0200, Thomas Kuster wrote:
> hello
> 
> i drew a boxplot with:
> > boxplot(voxit$AGE ~ voxit$A02X)
> and the boxes are from left to right:
> Ja Leer Nein wn k.A.
> 
> > levels(voxit$A02X)
> [1] "Ja"   "Leer" "Nein" "wn"   "k.A." "98"
> there are no entries with 98
> 
> but i want:
> Ja Nein Leer wn k.A.
> 
> how can i change the order of the boxes
> 
> bye
> thomas

See the following post from last week:

https://stat.ethz.ch/pipermail/r-help/2006-August/111289.html

HTH,

Marc Schwartz


From singularitaet at gmx.net  Tue Aug 22 14:46:38 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 22 Aug 2006 14:46:38 +0200
Subject: [R] How to interrupt running computation?
In-Reply-To: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
References: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
Message-ID: <44EAFCAE.40005@gmx.net>

Lothar,

Which system do you use? Windows, Linux, Mac?

Stefan




Lothar Schmid schrieb:
> If I start a computation in R, how can I interrupt it?
> I' using R 2.1.0.
>
> Thanks for your help,
>
> Lothar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Tue Aug 22 14:55:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Aug 2006 08:55:41 -0400
Subject: [R] listing a sequence of vectors in a matrix
In-Reply-To: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>
References: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>
Message-ID: <971536df0608220555mb3afe26wed0c8aa02bb4ba33@mail.gmail.com>

Here are two solutions.  seq(length = ...) instead of
just seq(...) is so that v can possibly contain zeros.

# data
v <- 3:5

# solution 1 - rbind/lapply
f <- function(n) {
	s = seq(length = n)
	replace(rep(NA, max(v)), s, s)
}
do.call(rbind, lapply(v, f))

# solution 2 - loop
mat <- matrix(NA, length(v), max(v))
for(i in seq(v)) {
	s <- seq(length = v[i])
	mat[i, s] <- s
}


On 8/22/06, Sara-Jane Dunn <SND at bas.ac.uk> wrote:
> Hi,
>
> I'm having trouble applying the matrix function. I'd like to be able to
> create a matrix of vectors filled in by rows, which are not all the same
> length, and so I need it to fill in NAs where applicable.
>
> It's easiest to explain with a simple example:
>
> Suppose vec = c(3,4,5). How can I form a matrix of the vectors 1:vec[j]
> for j=1:3?
> i.e. 1   2   3   NA NA
>      1   2   3   4   NA
>      1   2   3   4    5
> I've tried matrix(c(1:vec[j]),nrow=max(j),ncol=max(vec)) but it will
> only give me a matrix with repeated values for j=1, like   1  2  3  1
> 2
>                                            3  1  2  3  1
>                                            2  3  1  2  3
>
> Also using the list function hasn't got me anywhere either..
>
> Any help/ideas would be greatly appreciated!
>
> Many thanks,
> Sara-Jane Dunn
>
> --
> This message (and any attachments) is for the recipient only...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.hankin at noc.soton.ac.uk  Tue Aug 22 14:58:47 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 22 Aug 2006 13:58:47 +0100
Subject: [R] listing a sequence of vectors in a matrix
In-Reply-To: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>
References: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>
Message-ID: <71A3CE96-A4B0-4600-9F4B-8180E1D5800D@soc.soton.ac.uk>


Hi



 > f <- function(a,n){(1:a)[1:n]}
 > t(sapply(c(2,3,4,4,4,5,6),f,n=5))
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    2   NA   NA   NA
[2,]    1    2    3   NA   NA
[3,]    1    2    3    4   NA
[4,]    1    2    3    4   NA
[5,]    1    2    3    4   NA
[6,]    1    2    3    4    5
[7,]    1    2    3    4    5
 >
 >


HTH

rksh





On 22 Aug 2006, at 12:29, Sara-Jane Dunn wrote:

> Hi,
>
> I'm having trouble applying the matrix function. I'd like to be  
> able to
> create a matrix of vectors filled in by rows, which are not all the  
> same
> length, and so I need it to fill in NAs where applicable.
>
> It's easiest to explain with a simple example:
>
> Suppose vec = c(3,4,5). How can I form a matrix of the vectors 1:vec 
> [j]
> for j=1:3?
> i.e. 1   2   3   NA NA
>       1   2   3   4   NA
>       1   2   3   4    5
> I've tried matrix(c(1:vec[j]),nrow=max(j),ncol=max(vec)) but it will
> only give me a matrix with repeated values for j=1, like   1  2  3  1
> 2
>                                             3  1  2  3  1
>                                             2  3  1  2  3
>
> Also using the list function hasn't got me anywhere either..
>
> Any help/ideas would be greatly appreciated!
>
> Many thanks,
> Sara-Jane Dunn
>
> --  
> This message (and any attachments) is for the recipient on...{{dropped}}


From maechler at stat.math.ethz.ch  Tue Aug 22 15:01:56 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 22 Aug 2006 15:01:56 +0200
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <971536df0608211803r164f6139pba6f52b7211f7efa@mail.gmail.com>
References: <971536df0608211134l506e8bc8p2e84270ecc0d423c@mail.gmail.com>
	<20060821225927.64278.qmail@web32811.mail.mud.yahoo.com>
	<971536df0608211803r164f6139pba6f52b7211f7efa@mail.gmail.com>
Message-ID: <17643.68.878736.674844@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Mon, 21 Aug 2006 21:03:49 -0400 writes:

    Gabor> It is worthwhile to note that what is being
    Gabor> illustrated here is aggregating a numeric matrix by a
    Gabor> factor using the aggregate.default method and, of
    Gabor> course, a factor can't be part of a numeric matrix.

    Gabor> Of course, that is not say that the examples could
    Gabor> not be improved in terms of clarity, simplicity and
    Gabor> comprehensiveness (there is no example of
    Gabor> aggregate.data.frame).

yes, thank you, Gabor ..... 
and we (the R developers) have accepted and incorporated
quite a few constructive proposals for improvement.

Just offending the original authors ("bloody ..") without adding
any constructive proposal for improvement doesn't really help.
You can always get the money back you paid for R.
You can also decide to leave this mailing list and get the money
back you paid for that service.  Unfortunately, we can't get the
time and energy back we've lost when dealing with such postings...

Martin Maechler, ETH Zurich

    Gabor> On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote:
    >>  --- Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
    >> 
    >> > Its not part of state.x77.  Its a completely > separate
    >> variable.  > Try ls("package:datasets") and notice its in
    >> the > list > or try ?state.region and note that its a
    >> variable in > datasets.
    >> 
    >> Thanks. I was wondering if it was going something like
    >> that.
    >> 
    >> However, it is a bloody stupid example, at least to a
    >> newbie.  A call to another data.set in what is supposed
    >> to be a simple example is very confusing.
    >> 
    >> When someone is apparently illustrating a function with a
    >> simple one line command I don't expect them to call
    >> another data set, apparently create a new variable
    >> (Region), and use that new variable as the grouping
    >> variable without a word of explanation of what the
    >> example is doing.
    >> 
    >> If I sound a bit annoyed it is because I am. It might be
    >> nice to have an example illlustate the funtion,not do a
    >> couple of other undocumented things as well.
    >> >
    >> >
    >> > On 8/21/06, John Kane <jrkrideau at yahoo.ca> wrote: > > I
    >> was looking ?aggregate and ran the first example
    >> > >
    >> > > aggregate(state.x77, list(Region = state.region), > >
    >> mean)
    >> > >
    >> > > The variables in state.x77 appear to be : > > >
    >> state.x77 > > Population Income Illiteracy Life Exp
    >> Murder HS > Grad > > Frost Area
    >> > >
    >> > > Where is the "state.region" variable coming from?
    >> > >
    >> > > ______________________________________________ > >
    >> R-help at stat.math.ethz.ch mailing list > >
    >> https://stat.ethz.ch/mailman/listinfo/r-help > > PLEASE
    >> do read the posting guide >
    >> http://www.R-project.org/posting-guide.html > > and
    >> provide commented, minimal, self-contained, >
    >> reproducible code.
    >> > >
    >> >
    >> 
    >> 
    >> __________________________________________________ Do You
    >> Yahoo!?  Tired of spam?  Yahoo! Mail has the best spam
    >> protection around http://mail.yahoo.com
    >> 

    Gabor> ______________________________________________
    Gabor> R-help at stat.math.ethz.ch mailing list
    Gabor> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Gabor> do read the posting guide
    Gabor> http://www.R-project.org/posting-guide.html and
    Gabor> provide commented, minimal, self-contained,
    Gabor> reproducible code.


From gelman at stat.columbia.edu  Tue Aug 22 15:01:29 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Tue, 22 Aug 2006 09:01:29 -0400
Subject: [R] R2WinBugs
In-Reply-To: <44E9DCC3.5020005@statistik.uni-dortmund.de>
References: <286C9166197E0C44B94FF9762B27DAC70AFD02AC@sumac.cfs.le.ac.uk>
	<44E9DCC3.5020005@statistik.uni-dortmund.de>
Message-ID: <44EB0029.7090400@stat.columbia.edu>

We had set R2WinBugs to use the burnin as the adaptive phase.  I think 
this was changed very slightly in the latest version of R2WinBUGS so 
that the adaptive phase would equal burnin minus 1.  This was to allow 
DIC to be calculated.  So I think it should work OK now.  If you send an 
example I can take a look.

Andrew

Uwe Ligges wrote:

>
>
> Bowden, J.M. wrote:
>
>> Hi all,
>>
>>
>> I am having problems using the R2Winbugs function
>>
>> When I perform an analysis directly in Winbugs I can specify that the
>> first 'n' iterations are to be done using an 'adaptive' phase. After
>> this phase the markov chain seems to mix a lot better.
>>
>>
>> I don't seem to be able to specify R2winbugs to carry out this adaptive
>> phase, I can just specify the burnin length but this (to my knowledge)
>> is not the same thing.
>>
>> As a result my models are not fitting as well as I would like, has
>> anyone had a similar experience?
>
>
> I think this is a question for Andrew Gelman (CCing), who wrote the 
> underlying code for automatically setting the adaptive phase.
>
> Uwe Ligges
>
>
>> Jack
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Statistics department office:
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Political Science department office:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 731
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164


From r.hankin at noc.soton.ac.uk  Tue Aug 22 15:05:06 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 22 Aug 2006 14:05:06 +0100
Subject: [R] listing a sequence of vectors in a matrix
In-Reply-To: <971536df0608220555mb3afe26wed0c8aa02bb4ba33@mail.gmail.com>
References: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>
	<971536df0608220555mb3afe26wed0c8aa02bb4ba33@mail.gmail.com>
Message-ID: <1071F802-66C6-4A9C-96C1-85BE4BA1CF7B@soc.soton.ac.uk>

Gabor makes a good point about seq() vs a:b [a common gotcha
for me].


I'll revise my original function to:

 > f <- function(a,n){(seq(length=a))[1:n]}
 > t(sapply(c(2,3,4,4,4,5,6,0),f,n=5))
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    2   NA   NA   NA
[2,]    1    2    3   NA   NA
[3,]    1    2    3    4   NA
[4,]    1    2    3    4   NA
[5,]    1    2    3    4   NA
[6,]    1    2    3    4    5
[7,]    1    2    3    4    5
[8,]   NA   NA   NA   NA   NA
 >




rksh

On 22 Aug 2006, at 13:55, Gabor Grothendieck wrote:

> Here are two solutions.  seq(length = ...) instead of
> just seq(...) is so that v can possibly contain zeros.
>
> # data
> v <- 3:5
>
> # solution 1 - rbind/lapply
> f <- function(n) {
> 	s = seq(length = n)
> 	replace(rep(NA, max(v)), s, s)
> }
> do.call(rbind, lapply(v, f))
>
> # solution 2 - loop
> mat <- matrix(NA, length(v), max(v))
> for(i in seq(v)) {
> 	s <- seq(length = v[i])
> 	mat[i, s] <- s
> }
>
>
> On 8/22/06, Sara-Jane Dunn <SND at bas.ac.uk> wrote:
>> Hi,
>>
>> I'm having trouble applying the matrix function. I'd like to be  
>> able to
>> create a matrix of vectors filled in by rows, which are not all  
>> the same
>> length, and so I need it to fill in NAs where applicable.
>>
>> It's easiest to explain with a simple example:
>>
>> Suppose vec = c(3,4,5). How can I form a matrix of the vectors  
>> 1:vec[j]
>> for j=1:3?
>> i.e. 1   2   3   NA NA
>>      1   2   3   4   NA
>>      1   2   3   4    5
>> I've tried matrix(c(1:vec[j]),nrow=max(j),ncol=max(vec)) but it will
>> only give me a matrix with repeated values for j=1, like   1  2  3  1
>> 2
>>                                            3  1  2  3  1
>>                                            2  3  1  2  3
>>
>> Also using the list function hasn't got me anywhere either..
>>
>> Any help/ideas would be greatly appreciated!
>>
>> Many thanks,
>> Sara-Jane Dunn
>>
>> --
>> This message (and any attachments) is for the recipient only... 
>> {{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From carlos.riveira at yahoo.com  Tue Aug 22 15:25:02 2006
From: carlos.riveira at yahoo.com (carlos riveira)
Date: Tue, 22 Aug 2006 06:25:02 -0700 (PDT)
Subject: [R] Finney's fiducial confidence intervals of LD50
In-Reply-To: <20060822094907.GA6029@trotter.cap.ed.ac.uk>
Message-ID: <20060822132502.39354.qmail@web58201.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/63738f29/attachment.pl 

From lothar.schmid at googlemail.com  Tue Aug 22 15:27:01 2006
From: lothar.schmid at googlemail.com (Lothar Schmid)
Date: Tue, 22 Aug 2006 15:27:01 +0200
Subject: [R] How to interrupt running computation?
In-Reply-To: <eaa9854d0608220620v1d7a134eu61524543a36c7b83@mail.gmail.com>
References: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
	<44EAFCAE.40005@gmx.net>
	<eaa9854d0608220620v1d7a134eu61524543a36c7b83@mail.gmail.com>
Message-ID: <eaa9854d0608220627t1493dfe7x249eda8457f14144@mail.gmail.com>

I'm using Linux.
And I'd like just to cancel a running computation, not the entire R prompt.

Lothar


From ablukacz at utm.utoronto.ca  Tue Aug 22 15:41:00 2006
From: ablukacz at utm.utoronto.ca (Agnes Blukacz)
Date: Tue, 22 Aug 2006 09:41:00 -0400 (EDT)
Subject: [R] :Circular-Linear Correlation
Message-ID: <Pine.GSO.4.52.0608220930280.11355@credit.erin.utoronto.ca>

Dear All,

I'm looking for code that does circular-linear correlations as proposed
by Mardia (1976).

Thank you,

Agnes


From sarah.goslee at gmail.com  Tue Aug 22 15:44:44 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 22 Aug 2006 09:44:44 -0400
Subject: [R] How to interrupt running computation?
In-Reply-To: <eaa9854d0608220627t1493dfe7x249eda8457f14144@mail.gmail.com>
References: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
	<44EAFCAE.40005@gmx.net>
	<eaa9854d0608220620v1d7a134eu61524543a36c7b83@mail.gmail.com>
	<eaa9854d0608220627t1493dfe7x249eda8457f14144@mail.gmail.com>
Message-ID: <efb536d50608220644x7a88cbcbma3f97df42a345cd5@mail.gmail.com>

Then Ctrl-C should do the trick.

Sarah

On 8/22/06, Lothar Schmid <lothar.schmid at googlemail.com> wrote:
> I'm using Linux.
> And I'd like just to cancel a running computation, not the entire R prompt.
>
> Lothar
>

-- 
Sarah Goslee
http://www.stringpage.com


From sarah.goslee at gmail.com  Tue Aug 22 15:44:44 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 22 Aug 2006 09:44:44 -0400
Subject: [R] How to interrupt running computation?
In-Reply-To: <eaa9854d0608220627t1493dfe7x249eda8457f14144@mail.gmail.com>
References: <eaa9854d0608220528q1dd01c91j7db582a360858eab@mail.gmail.com>
	<44EAFCAE.40005@gmx.net>
	<eaa9854d0608220620v1d7a134eu61524543a36c7b83@mail.gmail.com>
	<eaa9854d0608220627t1493dfe7x249eda8457f14144@mail.gmail.com>
Message-ID: <efb536d50608220644x7a88cbcbma3f97df42a345cd5@mail.gmail.com>

Then Ctrl-C should do the trick.

Sarah

On 8/22/06, Lothar Schmid <lothar.schmid at googlemail.com> wrote:
> I'm using Linux.
> And I'd like just to cancel a running computation, not the entire R prompt.
>
> Lothar
>

-- 
Sarah Goslee
http://www.stringpage.com


From Ted.Harding at nessie.mcc.ac.uk  Tue Aug 22 15:45:21 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 22 Aug 2006 14:45:21 +0100 (BST)
Subject: [R] summary(lm ... conrasts=...)
Message-ID: <XFMail.060822144521.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I've encountered something I hadn't been consciously
aware of previously, and I'm wondering what the
explanation might be.

In (on another list) using R to demonstrate the difference
between different contrasts in 'lm' I set up an example
where Y is sampled from three different normal distributions
according to the levels ("A","B","C") of a factor X:

Y<-c(rnorm(mean=0,n=12),rnorm(mean=2,n=12),rnorm(mean=4,n=12))
X<-factor(c(rep("A",12),rep("B",12),rep("C",12)))

Then I do a summary(lm(Y~X)...) using first "Treatment" contrasts
and then "Helmert" contrasts. Here are the coefficient parts
of the results in each case:


summary(lm(Y~X,contrasts=list(X="contr.treatment")))
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   0.2303     0.3220   0.715  0.47944
XB            1.3057     0.4554   2.867  0.00716 **
XC            3.4204     0.4554   7.511 1.23e-08 ***


summary(lm(Y~X,contrasts=list(X="contr.helmert")))
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   1.8057     0.1859   9.713 3.34e-11 ***
X1            0.6529     0.2277   2.867  0.00716 **
X2            0.9225     0.1315   7.017 5.00e-08 ***


What I'm wondering is why the "effect names" are "X.B"
and "X.C" for Treatment, and "X1", "X2" for Helmert.

Why not "X.B" and "X.C" in both cases? Just as "XB"
contrasts B with the overall mean and "XC" contrasts C
with the overall mean, "XA" being implicit, in the
Treatment contrasts, so "X1" contrasts B with A and
"X2" contrasts C with (A+B) in Helmert, so there
is to my mind just as definite an association of "B"
with the first contrast, and "C" with the second, in
the Helmert case as in the Treatment case!

I know it's just a matter of "notation", but in the
Helmert case the association with the names of the
factor levels has been lost, and it could be useful
to have it explicit. (Or is it intended simply as a
reminder that one is using a particular system of
contrasts?)

Thanks, and best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Aug-06                                       Time: 14:45:17
------------------------------ XFMail ------------------------------


From wowen at richmond.edu  Tue Aug 22 15:50:35 2006
From: wowen at richmond.edu (Owen, Jason)
Date: Tue, 22 Aug 2006 09:50:35 -0400
Subject: [R] new version of "The R Guide" available on CRAN
Message-ID: <4619CB1B8D625544BF3764E6B346DA1F02BFB742@helena.richmond.edu>

Hello,

Version 2.2 of "The R Guide" is available for download in
the Contributed Documents section on CRAN.  "The R Guide"
is written for the beginning R user.  I use the guide in my
undergraduate probability and math stat sequence, but anyone
with a basic understanding of statistics (who wants to learn
R) should find it useful.

This updated version includes sections on multiple comparisons,
optimization, along with some improvements suggested by fellow
R users from around the world.  The entire document is under
60 pages in length.

Jason
--
Assistant Professor of Statistics
Mathematics and Computer Science Department
University of Richmond, Virginia 23173
(804) 289-8081   fax:(804) 287-6664
http://www.mathcs.richmond.edu/~wowen

"This is R. There is no if. Only how."
Simon (Yoda) Blomberg


From rmh at temple.edu  Tue Aug 22 15:53:30 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 22 Aug 2006 09:53:30 -0400 (EDT)
Subject: [R] aggregate example : where is the state.region variable?
Message-ID: <20060822095330.BGL29643@po-d.temple.edu>

> there is
> no factor in the dataset but why there is not one and
> why a call to another dataset is totally opaque.  

The reason is purely historical.  The state dataset is about
10 years older than the data.frame concept.  At the time the
state.* variables were constructed it was not possible to put
numeric data and factor data into the same rectangular structure.


From rdpeng at gmail.com  Tue Aug 22 15:58:37 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Tue, 22 Aug 2006 09:58:37 -0400
Subject: [R] big numbers
In-Reply-To: <3A57976A-D6E7-41CB-A793-BDBB1BE7CC8F@soc.soton.ac.uk>
References: <3A57976A-D6E7-41CB-A793-BDBB1BE7CC8F@soc.soton.ac.uk>
Message-ID: <44EB0D8D.2060106@gmail.com>

The 'gmp' package may be of use here, but I'm not sure.

-roger

Robin Hankin wrote:
> Hi
> 
> Can I get R to handle really big numbers?    I am not interested
> in more than (say) 10 sig figs, but I would like to deal with numbers
> up to, say, 10^10000.
> 
> If
> 
> a <- 10^10000
> b <- pi* a
> 
> I would like  "a+b" to return 3.1415926e10000.
> 
> 
> Toy example, illustrating why I can't deal with log(a) and log(b),   
> follows.
> 
> 
> 
> f <- function(a,n=100){
>    out <- rep(0,n)
>    out[1] <- a
>    for(i in 2:n){
>      out[i] <- sum(exp(out[1:i])) + rexp(1)
>    }
>    return(log(out))
> }
> 
> 
> then f(1,10)  has infinities in it, even though the values should be  
> moderate size.
> 
> What are my options here?
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From ggrothendieck at gmail.com  Tue Aug 22 16:00:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Aug 2006 10:00:52 -0400
Subject: [R] listing a sequence of vectors in a matrix
In-Reply-To: <1071F802-66C6-4A9C-96C1-85BE4BA1CF7B@soc.soton.ac.uk>
References: <s4eaf8d6.078@bsnw.nerc-bas.ac.uk>
	<971536df0608220555mb3afe26wed0c8aa02bb4ba33@mail.gmail.com>
	<1071F802-66C6-4A9C-96C1-85BE4BA1CF7B@soc.soton.ac.uk>
Message-ID: <971536df0608220700t3d21e3cfqffe4fac210dc8664@mail.gmail.com>

Here is another variation using Robin's idea of t(sapply(...))

v <- c(3:5, 0)
t(sapply(lapply(v, function(n) seq(length = n)), "length<-", max(v) ))

# which can be shortened even further for the case where
# there are no zeros in v

v <- 3:5
t(sapply(lapply(v, seq), "length<-", max(v) ))

On 8/22/06, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Gabor makes a good point about seq() vs a:b [a common gotcha
> for me].
>
>
> I'll revise my original function to:
>
>  > f <- function(a,n){(seq(length=a))[1:n]}
>  > t(sapply(c(2,3,4,4,4,5,6,0),f,n=5))
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    2   NA   NA   NA
> [2,]    1    2    3   NA   NA
> [3,]    1    2    3    4   NA
> [4,]    1    2    3    4   NA
> [5,]    1    2    3    4   NA
> [6,]    1    2    3    4    5
> [7,]    1    2    3    4    5
> [8,]   NA   NA   NA   NA   NA
>  >
>
>
>
>
> rksh
>
> On 22 Aug 2006, at 13:55, Gabor Grothendieck wrote:
>
> > Here are two solutions.  seq(length = ...) instead of
> > just seq(...) is so that v can possibly contain zeros.
> >
> > # data
> > v <- 3:5
> >
> > # solution 1 - rbind/lapply
> > f <- function(n) {
> >       s = seq(length = n)
> >       replace(rep(NA, max(v)), s, s)
> > }
> > do.call(rbind, lapply(v, f))
> >
> > # solution 2 - loop
> > mat <- matrix(NA, length(v), max(v))
> > for(i in seq(v)) {
> >       s <- seq(length = v[i])
> >       mat[i, s] <- s
> > }
> >
> >
> > On 8/22/06, Sara-Jane Dunn <SND at bas.ac.uk> wrote:
> >> Hi,
> >>
> >> I'm having trouble applying the matrix function. I'd like to be
> >> able to
> >> create a matrix of vectors filled in by rows, which are not all
> >> the same
> >> length, and so I need it to fill in NAs where applicable.
> >>
> >> It's easiest to explain with a simple example:
> >>
> >> Suppose vec = c(3,4,5). How can I form a matrix of the vectors
> >> 1:vec[j]
> >> for j=1:3?
> >> i.e. 1   2   3   NA NA
> >>      1   2   3   4   NA
> >>      1   2   3   4    5
> >> I've tried matrix(c(1:vec[j]),nrow=max(j),ncol=max(vec)) but it will
> >> only give me a matrix with repeated values for j=1, like   1  2  3  1
> >> 2
> >>                                            3  1  2  3  1
> >>                                            2  3  1  2  3
> >>
> >> Also using the list function hasn't got me anywhere either..
> >>
> >> Any help/ideas would be greatly appreciated!
> >>
> >> Many thanks,
> >> Sara-Jane Dunn
> >>
> >> --
> >> This message (and any attachments) is for the recipient only...
> >> {{dropped}}
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-
> >> guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
>


From paul_h_artes at yahoo.co.uk  Tue Aug 22 16:14:14 2006
From: paul_h_artes at yahoo.co.uk (Paul Artes)
Date: Tue, 22 Aug 2006 07:14:14 -0700 (PDT)
Subject: [R] Link
In-Reply-To: <4619CB1B8D625544BF3764E6B346DA1F02BFB742@helena.richmond.edu>
References: <4619CB1B8D625544BF3764E6B346DA1F02BFB742@helena.richmond.edu>
Message-ID: <5926389.post@talk.nabble.com>


Its a very nice document. Here is the link:

http://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf


-- 
View this message in context: http://www.nabble.com/new-version-of-%22The-R-Guide%22-available-on-CRAN-tf2146496.html#a5926389
Sent from the R help forum at Nabble.com.


From tlumley at u.washington.edu  Tue Aug 22 16:22:18 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 22 Aug 2006 07:22:18 -0700 (PDT)
Subject: [R] lean and mean lm/glm?
In-Reply-To: <20060821234446.8CA7C91FB7@xprdmxin.myway.com>
References: <20060821234446.8CA7C91FB7@xprdmxin.myway.com>
Message-ID: <Pine.LNX.4.64.0608220716260.4187@homer21.u.washington.edu>

On Mon, 21 Aug 2006, Damien Moore wrote:

>
>> For very large regression problems there is the biglm package (put you
>> data into a database, read in 500,000 rows at a time, and keep updating
>> the fit).
>
> thanks. I took a look at biglm and it seems pretty easy to use and, 
> looking at the source, avoids much of the redundancy of lm. Correct me 
> if i'm wrong, but I think it would be virtually impossible to extend to 
> glm, because of the non-linearity in glm models.

No, it is quite straightforward if you are willing to make multiple passes 
through the data.  It is hard with a single pass and may not be possible 
unless the data are in random order.

Fisher scoring for glms is just an iterative weighted least squares 
calculation using a set of 'working' weights and 'working' response. These 
can be defined chunk by chunk and fed to biglm.  Three iterations should 
be sufficient.

 	-thomas


From srini_iyyer_bio at yahoo.com  Tue Aug 22 16:32:05 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Tue, 22 Aug 2006 07:32:05 -0700 (PDT)
Subject: [R] Escaping " ' " character
In-Reply-To: <1156197113.4011.27.camel@localhost.localdomain>
Message-ID: <20060822143205.70681.qmail@web38106.mail.mud.yahoo.com>

Dear marc: thank you for your tip. 

> > cat(x.new, "\n")
> 3\',5\'-cyclic-nucleotide phosphodiesterase activity
> 

here cat is printing on screen. 

how can I direct the output to an object. 

I cannot do:

y <- cat(x.new, "\n")

is there any other way.

thanks
srini



> Try this:
> 
>  x <- "3',5'-cyclic-nucleotide phosphodiesterase
> activity"
> 
> > x
> [1] "3',5'-cyclic-nucleotide phosphodiesterase
> activity"
> 
> > gsub("'", "\\\\'", x)
> [1] "3\\',5\\'-cyclic-nucleotide phosphodiesterase
> activity"
> 
> 
> Note that I use gsub() to replace both instances of
> the ', whereas sub()
> will only replace the first.
> 
> The escape character itself needs to be escaped when
> used within the
> replacement regex, since the "\" is a metacharacter.
> You end up with
> four "\"s since R also treats the "\" specially.
> 
> When you cat() the output, you get:
> 
> > x.new
> [1] "3\\',5\\'-cyclic-nucleotide phosphodiesterase
> activity"
> 
> > cat(x.new, "\n")
> 3\',5\'-cyclic-nucleotide phosphodiesterase activity
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> 
>


From rmh at temple.edu  Tue Aug 22 16:35:08 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 22 Aug 2006 10:35:08 -0400 (EDT)
Subject: [R] HH and Rcmdr.HH packages available
Message-ID: <20060822103508.BGL37490@po-d.temple.edu>

The software for my recent book
 Statistical Analysis and Data Display
 Richard M. Heiberger and Burt Holland
 http://springeronline.com/0-387-40270-5

is now available as an R package from the book's website.
The package has been submitted to CRAN.

Description: Support software for Statistical Analysis and Data
        Display (Springer, ISBN 0-387-40270-5).  This contemporary
        presentation of statistical methods features extensive use of
        graphical displays for exploring data and for displaying the
        analysis. The authors demonstrate how to analyze data---showing
        code, graphics, and accompanying computer listings---for all the
        methods they cover. They emphasize how to construct and
        interpret graphs, discuss principles of graphical design, and
        show how accompanying traditional tabular results are used to
        confirm the visual impressions derived directly from the
        graphs. Many of the graphical formats are novel and appear
        here for the first time in print. All chapters have exercises.



Software that adds additional functions to Rcmdr 1.7 is now available from
my website and has been submitted to CRAN.
http://astro.ocis.temple.edu/~rmh/Rcmdr.HH/

Description: Our introductory course spends time on several topics
        that are not yet in the R Commander.  Therefore we wrote the menu
        items and make them available.


From wilmar.igl at gmx.de  Tue Aug 22 16:42:16 2006
From: wilmar.igl at gmx.de (Wilmar Igl)
Date: Tue, 22 Aug 2006 16:42:16 +0200
Subject: [R] Comparing pre-post effect sizes and areas under the curve, resp.
Message-ID: <44EB17C8.9030503@gmx.de>

Hello!

Does anybody know how to compare pre-post-effect sizes of different 
variables from the same sample with statistical tests? I have the same 
problem with areas under the curve (AUC) from ROC-Analysis.

Any recommendations on references or programms are welcome!

Thank you,

Will


From sfalcon at fhcrc.org  Tue Aug 22 16:59:13 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 22 Aug 2006 07:59:13 -0700
Subject: [R] [R-pkgs]   ANN: 'weaver' package, caching for Sweave
In-Reply-To: <m2u045zf1a.fsf@ziti.fhcrc.org> (Seth Falcon's message of "Mon,
	21 Aug 2006 19:21:21 -0700")
References: <m2u045zf1a.fsf@ziti.fhcrc.org>
Message-ID: <m2ejv8n7em.fsf@ziti.local>

Hi again,

Sorry for the noise, but I need to make a correction:

Seth Falcon <sfalcon at fhcrc.org> writes:
> To install:
>
>   source("http://bioconductor.org/biocLite.R")
>   biocLite("weaver")

At present, the above install sequence will _only_ work if you are
using a development version of R.

If you are using the current R release, you will have to work a bit
harder to install (put weaver works there too):

First install weaver's dependencies:

  digest (on CRAN)

  codetools:
    http://bioconductor.org/packages/1.9/omegahat/html/codetools.html

Then install weaver:
    http://www.bioconductor.org/packages/1.9/bioc/html/weaver.html

Finally, I will try to make Windows binaries available by the end of
the week.

+ seth

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Tue Aug 22 17:16:41 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Aug 2006 16:16:41 +0100 (BST)
Subject: [R] Successive subsets from a vector?
In-Reply-To: <f8e6ff050608220537r74e83258seef0630ce0ebec4a@mail.gmail.com>
References: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi> 
	<Pine.LNX.4.64.0608221049570.9453@gannet.stats.ox.ac.uk>
	<f8e6ff050608220537r74e83258seef0630ce0ebec4a@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608221557370.20523@gannet.stats.ox.ac.uk>

On Tue, 22 Aug 2006, hadley wickham wrote:

> > The loop method took 195 secs.  Just assigning to an answer of the correct
> > length reduced this to 5 secs.  e.g. use
> >
> >     ADDRESSES <- character(length(VECTOR)-4)
> >
> > Moral: don't grow vectors repeatedly.
> 
> Other languages (eg. Java) grow the size of the vector independently
> of the number of observations in it (I think Java doubles the size
> whenever the vector is filled), thus changing O(n) behaviour to O(log
> n).  I've always wondered why R doesn't do this.

At one point at least that was too expensive on memory/address space (and 
it may still be for 32-bit OSes). There is even a 'truelength' field in 
the vector header to allow for such a strategy, and the strategy is used 
in scan() and elsewhere.

In my experience it is relatively rare not to know the vector length in 
advance in R code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sue at xlsolutions-corp.com  Tue Aug 22 17:19:19 2006
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Tue, 22 Aug 2006 08:19:19 -0700
Subject: [R] 3 September Courses: (1) Regression Modeling Strategies in
	R/Splus, (2) R/Splus Advanced Programming, (3) R/Splus Fundamentals
Message-ID: <20060822081919.9f08cc34deb45d78e54b3b5664e21546.96b46fb880.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce
our September courses: 

(1) Regression Modeling Strategies in R/Splus --- by Prof Frank Harrell 
                   http://xlsolutions-corp.com/Rstats2.htm 
                           *** Washington DC, September 28-29, 2006 *** 
(2) R/Splus Advanced Programming --- by the R Development Core Team
Guru! 
                           *** New York City / September 11-12,2006*** 
             http://www.xlsolutions-corp.com/Radv.htm  
(3) R/Splus Fundamentals and Programming Techniques 
                          
                           *** Raleigh / September 12-13, 2006 *** 
             http://www.xlsolutions-corp.com/Rfund.htm 
  
Ask for group discount and reserve your seat Now - Earlybird Rates.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com


(1) Regression Modeling Strategies     at  
http://xlsolutions-corp.com/Rstats2.htm 
(2) R/Splus Advanced Programming   
Course Outline: 
- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function  
It'll also deal with lots of S-Plus efficiency issues and any special
topics from participants is welcome. 
(3) R/Splus Fundamentals and Programming Techniques 
                       
Course outline. 
- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava 
Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat! 
Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-  
3 Courses - (1) Regression Modeling Strategies in R/Splus (2) R/Splus
Advanced Programming (3) R/Splus Fundamentals


From tramni at abv.bg  Tue Aug 22 17:20:01 2006
From: tramni at abv.bg (Martin Ivanov)
Date: Tue, 22 Aug 2006 18:20:01 +0300 (EEST)
Subject: [R] NonLinearLeastSquares Trust-Region
Message-ID: <1320767161.406901156260001768.JavaMail.nobody@mail04.abv.bg>

Hello!

I am running R-2.3.1-i386-1 on Slackware Linux 10.2. I am a former matlab user, moving to R. In matlab, via the cftool, I performed nonlinear curve fitting using the method "nonlinear least squares" with the "Trust-Region" algorithm and not using robust fitting. Is it possible to perform the same  analysis in R? I read quite a lot of R documentation, but I could not find an alternative solution. If there is such, please forgive my ignorance (I am a newbie in R) and tell me which function from which package is capable of performing the same analysis. If the same analysis is not possible to carry out in R, I would be grateful if you suggest to me some alternative procedure. I found that the "nls" function performs nonlinear least squares. The problem is that I do not want to implement the Gauss-Newton algorithm. In the worst case I would be contented with the "Levenberg-Marquardt" algorithm, if it is implemented in R. R nls's documentation mentions the  "port"  package and the 
  ?nl2sol? algorithm, but I could not find that package in the CRAN repository, so that I could read and judge whether that algorithm would be appropriate.

Thank you very much in advance. I am looking forward to your answer.
Regards,
Martin

-----------------------------------------------------------------
http://ide.li/ - ?????? ?? ????????? ?? ?????. ??????, ??????, ??????, ??????, ??????????.


From kubovy at virginia.edu  Tue Aug 22 17:25:06 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 22 Aug 2006 11:25:06 -0400
Subject: [R] HH and Rcmdr.HH packages available
In-Reply-To: <20060822103508.BGL37490@po-d.temple.edu>
References: <20060822103508.BGL37490@po-d.temple.edu>
Message-ID: <DB1A2714-4968-4520-9924-749861866482@virginia.edu>

Dear R-helpers,

For those of you who wish to use the useful and interesting HH  
package on the Mac: I successfully downloaded the package from http:// 
astro.temple.edu/~rmh/HH/HH_1.4.tar.gz
Safari routinely upacks archives, so after downloading I had to (in  
the Terminal)
gzip Desktop/H_1.4.tar
and then installed it with the command
sudo R CMD INSTALL Desktop/HH_1.4.tar.gz
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From mschwartz at mn.rr.com  Tue Aug 22 17:19:09 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 22 Aug 2006 10:19:09 -0500
Subject: [R] Escaping " ' " character
In-Reply-To: <20060822143205.70681.qmail@web38106.mail.mud.yahoo.com>
References: <20060822143205.70681.qmail@web38106.mail.mud.yahoo.com>
Message-ID: <1156259949.4179.8.camel@localhost.localdomain>

On Tue, 2006-08-22 at 07:32 -0700, Srinivas Iyyer wrote:
> Dear marc: thank you for your tip. 
> 
> > > cat(x.new, "\n")
> > 3\',5\'-cyclic-nucleotide phosphodiesterase activity
> > 
> 
> here cat is printing on screen. 
> 
> how can I direct the output to an object. 
> 
> I cannot do:
> 
> y <- cat(x.new, "\n")
> 
> is there any other way.
> 
> thanks
> srini

Srini,

Going back to your initial post, try something like the following using
paste():

> x <- "3',5'-cyclic-nucleotide phosphodiesterase activity"

> x.new <- gsub("'", "\\\\'", x)

#Note the escaping of the single quotes here:
> sql.cmd <- paste("fetch_count_fterm_sql(\'", (x.new), "\');", 
                   sep = "")

# Beware any line wrapping here
> sql.cmd
[1] "fetch_count_fterm_sql('3\\',5\\'-cyclic-nucleotide phosphodiesterase activity');"


This way the character vector 'sql.cmd' has the full sql query, which
you can then pass to your statement processing code.

I'm not sure how you are passing the code, but if in a text file as
input, you can do something like:

> sink("sqlfile.txt")
> cat(sql.cmd, "\n")
> sink()


Where the text file 'sqlfile.txt' will contain the single line:

fetch_count_fterm_sql('3\',5\'-cyclic-nucleotide phosphodiesterase activity'); 

See ?sink for more information.

HTH,

Marc


From backer at psych.uib.no  Tue Aug 22 18:36:53 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Tue, 22 Aug 2006 18:36:53 +0200
Subject: [R] Authoring a book
Message-ID: <44EB32A5.7030508@psych.uib.no>

Me and some colleagues are planning to write a textbook together
("Statistics using R") where the target audience for the book is
psychologists and students of psychology.

We thought that it might be a good idea to use a Wiki when writing the
text.  Is that a good idea?  Does anybody have any experience in that
direction?  What alternatives are there?

The tool (Wiki) would have to be able to handle tables and
mathematical formulas in some manner, and of course, some mechanism to
export the contents to a word processor in the final stages.

I have my own server, Windows, based on Apache, PhP, and MySQL.

Tom


From ripley at stats.ox.ac.uk  Tue Aug 22 18:50:16 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Aug 2006 17:50:16 +0100 (BST)
Subject: [R] summary(lm ... conrasts=...)
In-Reply-To: <XFMail.060822144521.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060822144521.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.64.0608221737520.21687@gannet.stats.ox.ac.uk>

On Tue, 22 Aug 2006, Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
> 
> I've encountered something I hadn't been consciously
> aware of previously, and I'm wondering what the
> explanation might be.

Try

> contr.helmert(letters[1:3])
  [,1] [,2]
a   -1   -1
b    1   -1
c    0    2
> contr.treatment(letters[1:3])
  b c
a 0 0
b 1 0
c 0 1

and note the difference in column names.

Those who made the decision to use those column names determined this.
I agreed with them that labelling the second Helmert contrast here as 'c' 
would be confusing, especially easy to confuse with treatment contrasts.
However, I thought the treatment contrasts should be labelled b-a and c-a.
We also had arguments about xc vs x.c vs x:c.  AFAIR brevity won.

Once you know how it is done, it is easy to change the behaviour, of 
course: just roll your own contrasts function with the colnames you want.

> In (on another list) using R to demonstrate the difference
> between different contrasts in 'lm' I set up an example
> where Y is sampled from three different normal distributions
> according to the levels ("A","B","C") of a factor X:
> 
> Y<-c(rnorm(mean=0,n=12),rnorm(mean=2,n=12),rnorm(mean=4,n=12))
> X<-factor(c(rep("A",12),rep("B",12),rep("C",12)))
> 
> Then I do a summary(lm(Y~X)...) using first "Treatment" contrasts
> and then "Helmert" contrasts. Here are the coefficient parts
> of the results in each case:

Just coef() or print() gives you the coefficient names: this is not done 
by summary().

> summary(lm(Y~X,contrasts=list(X="contr.treatment")))
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   0.2303     0.3220   0.715  0.47944
> XB            1.3057     0.4554   2.867  0.00716 **
> XC            3.4204     0.4554   7.511 1.23e-08 ***
> 
> 
> summary(lm(Y~X,contrasts=list(X="contr.helmert")))
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   1.8057     0.1859   9.713 3.34e-11 ***
> X1            0.6529     0.2277   2.867  0.00716 **
> X2            0.9225     0.1315   7.017 5.00e-08 ***
> 
> 
> What I'm wondering is why the "effect names" are "X.B"
> and "X.C" for Treatment, and "X1", "X2" for Helmert.
> 
> Why not "X.B" and "X.C" in both cases? Just as "XB"
> contrasts B with the overall mean and "XC" contrasts C
> with the overall mean, "XA" being implicit, in the
> Treatment contrasts, so "X1" contrasts B with A and
> "X2" contrasts C with (A+B) in Helmert, so there
> is to my mind just as definite an association of "B"
> with the first contrast, and "C" with the second, in
> the Helmert case as in the Treatment case!
> 
> I know it's just a matter of "notation", but in the
> Helmert case the association with the names of the
> factor levels has been lost, and it could be useful
> to have it explicit. (Or is it intended simply as a
> reminder that one is using a particular system of
> contrasts?)
> 
> Thanks, and best wishes to all,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 22-Aug-06                                       Time: 14:45:17
> ------------------------------ XFMail ------------------------------

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cberry at tajo.ucsd.edu  Tue Aug 22 19:07:11 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 22 Aug 2006 10:07:11 -0700
Subject: [R] lean and mean lm/glm?
Message-ID: <Pine.LNX.4.64.0608220932070.31743@tajo.ucsd.edu>

On Mon, 21 Aug 2006, Damien Moore wrote:

>
>> For very large regression problems there is the biglm package (put you
>> data into a database, read in 500,000 rows at a time, and keep updating
>> the fit).
>
> thanks. I took a look at biglm and it seems pretty easy to use and, 
> looking at the source, avoids much of the redundancy of lm. Correct me 
> if i'm wrong, but I think it would be virtually impossible to extend to 
> glm, because of the non-linearity in glm models.
>
> I might hack around at the source code for glm.fit -- I think I can 
> avoid some of the redundancy involved in that routine pretty easily, but 
> it will mean rewriting the summary output code...


Damien,

If you know what is 'under the hood' of glm, you can use the biglm
approach to perform a one-step update of the coefficients of a glm model.

There is plenty of theory for one-step estimators that use consistent 
estimates as starting values.

You can probably get a good starting value by averaging all of the results 
returned by slicing the data set into smaller pieces and running glm.fit 
on each of them.

Chuck

>
> cheers
> Damien
>
>
> --- On Mon 08/21, Greg Snow < Greg.Snow at intermountainmail.org > wrote:From: Greg Snow [mailto: Greg.Snow at intermountainmail.org]To: damien.moore at excite.com, r-help at stat.math.ethz.chDate: Mon, 21 Aug 2006 12:01:06 -0600Subject: RE: [R] lean and mean lm/glm?
>
> For very large regression problems there is the biglm package (put you
> data into a database, read in 500,000 rows at a time, and keep updating
> the fit).
>
> This has not been extended to glm yet.
>
> Hope this helps,
>
>
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Damien Moore
> Sent: Monday, August 21, 2006 11:49 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] lean and mean lm/glm?
>
>
> Hi All: I'm new to R and have a few questions about getting R to run
> efficiently with large datasets.
>
> I'm running R on Windows XP with 1Gb ram (so about 600mb-700mb after the
> usual windows overhead). I have a dataset that has 4 million
> observations and about 20 variables. I want to run probit regressions on
> this data, but can't do this with more than about 500,000 observations
> before I start running out of ram (you could argue that I'm getting
> sufficient precision with <500,000 obs but lets pretend otherwise).
> Loading 500,000 observations into a data frame only takes about 100Mb of
> ram, so that isn't the problem. Instead it seems R uses huge amount of
> memory when running the glm methods. I called the Fortran routines that
> lm and glm use directly but even they create a large number of
> extraneous variables in the output (e.g. the Xs, ys, residuals etc) and
> during processing. For instance (sample code)
>
> x=runif(1000000)
> y=3*x+rnorm(1000000) #I notice this step chews up a lot more than the
> 7mb of ram required to store y during processing, but cleans up ok
> afterwards with a gc() call
> X=cbind(x)
> p=ncol(X)
> n=NROW(y)
> ny=NCOL(y)
> tol=1e-7
> #this is the fortran routine called by lm - regressing y on X here z <-
> .Fortran("dqrls", qr = X, n = n, p = p, y = y, ny = ny, tol =
> as.double(tol), coefficients = mat.or.vec(p, ny), residuals = y, effects
> = y, rank = integer(1), pivot = 1:p, qraux = double(p), work = double(2
> * p), PACKAGE = "base")
>
> This code runs very quickly - suggesting that in principle R should have
> no problem at all handling very large data sets, but uses >100mb during
> processing and z is about a 20mb object. Scaling this up to a much
> larger dataset with many variables its easy to see i'm going to run into
> problems
>
> My questions:
> 1. are there any memory efficient alternatives to lm/glm in R?
> 2. is there any way to prevent the Fortran routine "dqrls" from
> producing so much output? (I suspect not since its output has to be
> compatible with the summary method, which seems to rely on having a copy
> of all variables instead of just references to the relevant variables -
> correct me if i'm wrong on this) 3. failing 1 & 2 how easy would it be
> to create new versions of lm and glm that don't use so much memory? (Not
> that I'm volunteering or anything ;) ). There is no need to hold
> individual residuals in memory or make copies of the variables (at least
> for my purposes). How well documented is the source code?
>
> cheers
> Damien Moore
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>    [ Part 3.53: "Included Message" ]
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From sfalcon at fhcrc.org  Tue Aug 22 19:08:37 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 22 Aug 2006 10:08:37 -0700
Subject: [R] Rgraphviz installation Problem
In-Reply-To: <b4485c4c0608220342l449b2444m95f6bb210d24ad67@mail.gmail.com> (j.
	joshua thomas's message of "Tue, 22 Aug 2006 18:42:31 +0800")
References: <b4485c4c0608220342l449b2444m95f6bb210d24ad67@mail.gmail.com>
Message-ID: <m2y7tgitpm.fsf@ziti.local>

"j.joshua thomas" <researchjj at gmail.com> writes:

> Dear Robert,
>
> Thanks for your time.
> I have downloaded Rgraphviz (windows binary) from www.bioconductor.org
> and put inside R2.3.0 library then  i installed from the local zip
> its says package 'graph' couldnot be loaded.
>
> Am i doing the installation correctly? Still the new user.
>
> Can you guide me sir?

Questions about BioC packages are best directed to the bioconductor
mailing list.

I would recommend trying:

source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")


Best,

+ seth


From cberry at tajo.ucsd.edu  Tue Aug 22 19:13:19 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 22 Aug 2006 10:13:19 -0700
Subject: [R] Successive subsets from a vector?
Message-ID: <Pine.LNX.4.64.0608221009130.31743@tajo.ucsd.edu>



Like this:
> do.call( paste, c( list(sep=""), lapply(1:5,function(x) VECTOR[x:(length(VECTOR)-5+x)]) ))
[1] "14265"   "42650"   "265011"  "6501110" "5011104" "0111043" "1110436" "104368"  "43686"
>

HTH,

Chuck

On Tue, 22 Aug 2006, kone wrote:

> I'd like to pick every imbricated five character long subsets from a
> vector. I guess there is some efficient way to do this without loops...
> Here is a for-loop-version and a model for output:
>
> VECTOR=c(1,4,2,6,5,0,11,10,4,3,6,8,6);
>
> ADDRESSES=c();
> for(i in 1:(length(VECTOR)-4)){
> 	ADDRESSES[i]=paste(VECTOR[i:(i+4)],collapse="")
> }
>
> > ADDRESSES
> [1] "14265"   "42650"   "265011"  "6501110" "5011104" "0111043"
> "1110436" "104368"
> [9] "43686"
>
>
> Atte Tenkanen
> University of Turku, Finland
>
> 	[[alternative text/enriched version deleted]]
>
>
>
>    [ Part 3.64: "Included Message" ]
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From bonfigli at inmi.it  Tue Aug 22 20:15:42 2006
From: bonfigli at inmi.it (Bonfigli Sandro)
Date: Tue, 22 Aug 2006 20:15:42 +0200
Subject: [R] Selection on dataframe based on order of rows
Message-ID: <WorldClient-F200608222015.AA15420293@inmi.it>

I have a dataframe with the following structure

id    date         value
-------------------------
1    22/08/2006     48
1    24/08/2006     50
1    28/08/2006     150
1    30/08/2006     100
1    01/09/2006     30
2    11/08/2006     30
2    22/08/2006     100
2    28/08/2006     11
2    02/09/2006     5
3    01/07/2006     3
3    01/08/2006     100
3    01/09/2006     100
4    22/08/2006     48
4    24/08/2006     50
4    28/08/2006     150
4    30/08/2006     100
4    01/09/2006     30
4    03/09/2006     100
4    06/09/2006     100


N.B.: dates in european format; ordered dataframe

For each ID I need to select the first occurrence of
all the rows which are the first of at least two with 
"value" >= 50.

Rather convoluted explication. I mean that for each id I have to select
the first row in which value is > 50 only if at least the following row 
has "value" > 50 too. If this is not true I repeat the test for all the 
following rows in which "value" > 50 untill I find a record that respects
the condition

this means that with my example dataframe the result is :
id    date         value
-------------------------
1    28/08/2006     150
3    01/08/2006     100
4    28/08/2006     150

It's clear that a for loop would work but I think that that is a better 
way.

I tried "by" and could obtain the first row for wich "value" is > 50.

I thought of an iterative process (delete the first row > 50, find the 
second row > 50, examine if there are rows in the middle) but it
is quite inelegant as if the first value is not the "good" one I have to 
repeat the process for a a priori unknown number of times.

Thanks in advance for Your help

  Sandro Bonfigli


From rab45+ at pitt.edu  Tue Aug 22 20:27:06 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Tue, 22 Aug 2006 14:27:06 -0400
Subject: [R] Marginal Predicitions from nlme and lme4
Message-ID: <1156271226.3510.8.camel@localhost.localdomain>

Is there a way (simple or not) to get the marginal prediction from lme
(in nlme) and/or lmer (in lme4)?

Rick B.


From ggrothendieck at gmail.com  Tue Aug 22 20:29:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Aug 2006 14:29:07 -0400
Subject: [R] Successive subsets from a vector?
In-Reply-To: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
References: <ba7c18d59f3dedc69d4335bb7b3e637b@mus.utu.fi>
Message-ID: <971536df0608221129s7d2dae59vec9ccdedb7fcd84f@mail.gmail.com>

Here is a solution that uses gsub with a negative lookahead perl-style
regexp to do it:

VECTOR <- c(1,4,2,6,5,0,11,10,4,3,6,8,6)
e <- "([[:digit:]]+),(?=([[:digit:]]+),([[:digit:]]+),([[:digit:]]+),([[:digit:]]+))"
out <- gsub(e, "\\1\\2\\3\\4\\5 ", paste(VECTOR, collapse = ","), perl = TRUE)
head(strsplit(out, " ")[[1]], -1)  # uses head from R 2.4.0


On 8/22/06, kone <attenka at utu.fi> wrote:
> I'd like to pick every imbricated five character long subsets from a
> vector. I guess there is some efficient way to do this without loops...
> Here is a for-loop-version and a model for output:
>
> VECTOR=c(1,4,2,6,5,0,11,10,4,3,6,8,6);
>
> ADDRESSES=c();
> for(i in 1:(length(VECTOR)-4)){
>        ADDRESSES[i]=paste(VECTOR[i:(i+4)],collapse="")
> }
>
>  > ADDRESSES
> [1] "14265"   "42650"   "265011"  "6501110" "5011104" "0111043"
> "1110436" "104368"
> [9] "43686"
>
>
> Atte Tenkanen
> University of Turku, Finland
>
>        [[alternative text/enriched version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jasonpienaar at gmail.com  Tue Aug 22 20:51:50 2006
From: jasonpienaar at gmail.com (jason pienaar)
Date: Tue, 22 Aug 2006 23:51:50 +0500
Subject: [R] Mac os
Message-ID: <cb9d3b060608221151w1ffefba8i77575a0794427068@mail.gmail.com>

Dear Sir / Madam

Hi, I have written some code For R that uses for loops to do
2-dimensional grid searches for maximum likelhood combined with
iterated GLS estimation. As can be expected, depending on the szie of
the grid, estimation can take quite some time. However, I have noticed
that the same code run on a windows operating system is much faster
than when run on a Mac (I basically paste the code into the console
and then run things from there). I was wondering if anyone knew if
this is typical, or if there is some good reason for this? (I am not
that familiar with the mac operating system, so might be missing
something obvious). I am writing a user manual for the code and would
like to have some explanation (or possible improvement) for mac users,
so any info on this would be much appreciated
Sincerely
Jason Pienaar


From hb at stat.berkeley.edu  Tue Aug 22 21:11:29 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 22 Aug 2006 12:11:29 -0700
Subject: [R] new version of "The R Guide" available on CRAN
In-Reply-To: <4619CB1B8D625544BF3764E6B346DA1F02BFB742@helena.richmond.edu>
References: <4619CB1B8D625544BF3764E6B346DA1F02BFB742@helena.richmond.edu>
Message-ID: <59d7961d0608221211o4c6e631fg2ab8031ee921d0c@mail.gmail.com>

Hi,

thanks for this.  I'll keep it in mind next time in teaching/referring
someone to R.  BTW, before the R-core guys get you ;)   Just replace
all places where you use "library" to refer to a "package" (see all
comments on the definition of these on r-help/r-devel), e.g.

Page 17: "FYI, .GlobalEnv is your workspace and the package quantities
are libraries that contain (among other things) the functions and
datasets that we are learning about in this manual."

Cheers

Henrik

On 8/22/06, Owen, Jason <wowen at richmond.edu> wrote:
> Hello,
>
> Version 2.2 of "The R Guide" is available for download in
> the Contributed Documents section on CRAN.  "The R Guide"
> is written for the beginning R user.  I use the guide in my
> undergraduate probability and math stat sequence, but anyone
> with a basic understanding of statistics (who wants to learn
> R) should find it useful.
>
> This updated version includes sections on multiple comparisons,
> optimization, along with some improvements suggested by fellow
> R users from around the world.  The entire document is under
> 60 pages in length.
>
> Jason
> --
> Assistant Professor of Statistics
> Mathematics and Computer Science Department
> University of Richmond, Virginia 23173
> (804) 289-8081   fax:(804) 287-6664
> http://www.mathcs.richmond.edu/~wowen
>
> "This is R. There is no if. Only how."
> Simon (Yoda) Blomberg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From liulei at virginia.edu  Tue Aug 22 22:01:33 2006
From: liulei at virginia.edu (Lei Liu)
Date: Tue, 22 Aug 2006 16:01:33 -0400
Subject: [R] a generic Adaptive Gauss Quadrature function in R?
Message-ID: <7.0.1.0.0.20060822154357.0194fa68@virginia.edu>

Hi there,

I am using SAS Proc NLMIXED to maximize a likelihood with 
multivariate normal random effects. An example is the two part random 
effects model for repeated measures semi-continous data with a 
cluster at 0. I use the "model y ~ general(loglike)" statement in 
Proc NLMIXED, so I can specify a general log likelihood function 
constructed by SAS programming statements. Then the likelihood can be 
maximized by AGQ. Is there a similar generic AGQ function in R to let 
me write explicitly the log likelihood and then maximize it 
accordingly? Can nlme do the work? Thanks!

Lei Liu
Assistant Professor
Division of Biostatistics and Epidemiology
Department of Public Health Sciences
School of Medicine
University of Virginia

3181 Hospital West Complex
Charlottesville, VA 22908-0717

1-434-982-3364 (o)
1-434-806-8086 (c)
1-434-243-5787 (f)

liulei at virginia.edu
ll9f at virginia.edu


From jrkrideau at yahoo.ca  Tue Aug 22 22:31:15 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 22 Aug 2006 16:31:15 -0400 (EDT)
Subject: [R] aggregate example : where is the state.region variable?
In-Reply-To: <20060822095330.BGL29643@po-d.temple.edu>
Message-ID: <20060822203115.92520.qmail@web32801.mail.mud.yahoo.com>


--- "Richard M. Heiberger" <rmh at temple.edu> wrote:

> > there is
> > no factor in the dataset but why there is not one
> and
> > why a call to another dataset is totally opaque.  
> 
> The reason is purely historical.  The state dataset
> is about
> 10 years older than the data.frame concept.  At the
> time the
> state.* variables were constructed it was not
> possible to put
> numeric data and factor data into the same
> rectangular structure.

I see. So originally the example would have been more
obvious.  Thanks


From jeffreyvludlow at yahoo.com  Tue Aug 22 22:32:22 2006
From: jeffreyvludlow at yahoo.com (jeffrey ludlow)
Date: Tue, 22 Aug 2006 16:32:22 -0400 (EDT)
Subject: [R] Question on R Training
Message-ID: <20060822203222.41862.qmail@web56007.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/b996614e/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Tue Aug 22 22:43:14 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 23 Aug 2006 06:43:14 +1000
Subject: [R] Marginal Predicitions from nlme and lme4
In-Reply-To: <1156271226.3510.8.camel@localhost.localdomain>
References: <1156271226.3510.8.camel@localhost.localdomain>
Message-ID: <20060822204314.GV95817@ms.unimelb.edu.au>

Rick,

if by marginal prediction, you mean the prediction without random
effects, then use the "level" argument.  See ?predict.lme or ?fitted.lme

If not then I don't know :)

Cheers

Andrew

On Tue, Aug 22, 2006 at 02:27:06PM -0400, Rick Bilonick wrote:
> Is there a way (simple or not) to get the marginal prediction from lme
> (in nlme) and/or lmer (in lme4)?
> 
> Rick B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From sasha.wolosin at gmail.com  Tue Aug 22 22:59:22 2006
From: sasha.wolosin at gmail.com (Sasha Wolosin)
Date: Tue, 22 Aug 2006 16:59:22 -0400
Subject: [R] how to run ANCOVA?
Message-ID: <74537bfe0608221359l4891103eudc90b1ac2c25cce@mail.gmail.com>

Dear all,

  I would like to know how to run an analysis of covariance in R.  For
example, I have a data frame ("data") consisting of two second-degree
categorical variables ("diagnosis" and "gender"), one continous
independent variable ("age") and one continous dependent variable
("response").

I ran a simple anova to see the effects of diagnosis and gender (and
interaction):

aov.out <- aov(response~diagnosis*gender,data)
anova(aov.out)

Now I would like to covary for age, how can I add "age" as a covariate
to this equation?

Thanks,
Sasha


From jz7 at duke.edu  Tue Aug 22 23:10:42 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Tue, 22 Aug 2006 17:10:42 -0400 (EDT)
Subject: [R] error message from lm.ridge() in MASS library
In-Reply-To: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>
Message-ID: <Pine.GSO.4.58.0608221705440.21737@godzilla.acpub.duke.edu>

Dear all,

I got a wierd problem when using lm.ridge() in MASS library. When my X
matrix has few columns, there is no problem. But when my X matrix gets
larger (over 1000 columns),  I got the following error:

Error in Xs$v %*% a : non-conformable arguments
In addition: Warning messages:
1: longer object length
        is not a multiple of shorter object length in: d^2 + rep(lambda,
rep(p, k))
2: longer object length
        is not a multiple of shorter object length in: drop(d * rhs)/div

The R code I use for the calculation is "lm.ridge( y ~ x,lambda=seq(1,15,1))".

Please advice.

Thanks a lot!
Jeny


From justin_bem at yahoo.fr  Tue Aug 22 23:51:45 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Tue, 22 Aug 2006 21:51:45 +0000 (GMT)
Subject: [R] R is wonderful
Message-ID: <20060822215145.73852.qmail@web25710.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060822/0be0ea4b/attachment.pl 

From bilonickra at upmc.edu  Wed Aug 23 00:39:24 2006
From: bilonickra at upmc.edu (Rick Bilonick)
Date: Tue, 22 Aug 2006 18:39:24 -0400
Subject: [R] Marginal Predicitions from nlme and lme4
In-Reply-To: <20060822204314.GV95817@ms.unimelb.edu.au>
References: <1156271226.3510.8.camel@localhost.localdomain>
	<20060822204314.GV95817@ms.unimelb.edu.au>
Message-ID: <1156286364.3510.30.camel@localhost.localdomain>

On Wed, 2006-08-23 at 06:43 +1000, Andrew Robinson wrote:
> Rick,
> 
> if by marginal prediction, you mean the prediction without random
> effects, then use the "level" argument.  See ?predict.lme or ?fitted.lme
> 
> If not then I don't know :)
> 
> Cheers
> 
> Andrew
Thanks. I'm familiar with level in predict and fitted for lme. These
allow you to select the fixed effects and/or the random effects. The
marginal prediction integrates out the random effects and is what a GEE
marginal model produces. From what I've read, the marginal effects seem
to be less desirable than the fixed effects from an lme or a generalized
lme. But I would still like to compute them for comparison.

Rick B.


From bryant at prgs.edu  Wed Aug 23 00:46:20 2006
From: bryant at prgs.edu (Bryant, Benjamin)
Date: Tue, 22 Aug 2006 15:46:20 -0700
Subject: [R] rpart output:  rule extraction beyond path.rpart()
Message-ID: <DCEA4EF1F1B63E4D8EDA2FAE4F0070CE0AE971@smmail8.rand.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/81377509/attachment.pl 

From tlumley at u.washington.edu  Wed Aug 23 00:50:41 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 22 Aug 2006 15:50:41 -0700 (PDT)
Subject: [R] Marginal Predicitions from nlme and lme4
In-Reply-To: <1156286364.3510.30.camel@localhost.localdomain>
References: <1156271226.3510.8.camel@localhost.localdomain>
	<20060822204314.GV95817@ms.unimelb.edu.au>
	<1156286364.3510.30.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0608221546580.29095@homer23.u.washington.edu>

On Tue, 22 Aug 2006, Rick Bilonick wrote:

> On Wed, 2006-08-23 at 06:43 +1000, Andrew Robinson wrote:
>> Rick,
>>
>> if by marginal prediction, you mean the prediction without random
>> effects, then use the "level" argument.  See ?predict.lme or ?fitted.lme
>>
>> If not then I don't know :)
>>
>> Cheers
>>
>> Andrew
> Thanks. I'm familiar with level in predict and fitted for lme. These
> allow you to select the fixed effects and/or the random effects. The
> marginal prediction integrates out the random effects and is what a GEE
> marginal model produces. From what I've read, the marginal effects seem
> to be less desirable than the fixed effects from an lme or a generalized
> lme. But I would still like to compute them for comparison.
>

I don't agree that they are less useful, but they are not in general easy 
to obtain from a GLMM.  For any linear link model or a log link model that 
has only random intercepts the marginal and conditional effects are the 
same.  For the probit model there is a conversion formula, but for other 
models they typically require high-dimensional integration to compute.

It's easy just to fit a marginal glm if you want marginal coefficients and 
a mixed model if you want conditional coefficients.

 	-thomas


From ggrothendieck at gmail.com  Wed Aug 23 01:22:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Aug 2006 19:22:48 -0400
Subject: [R] Selection on dataframe based on order of rows
In-Reply-To: <WorldClient-F200608222015.AA15420293@inmi.it>
References: <WorldClient-F200608222015.AA15420293@inmi.it>
Message-ID: <971536df0608221622t119e78b3n782c2363933b4dc3@mail.gmail.com>

Try this:

# data
DF <- structure(list(id = c(1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4,
4, 4, 4, 4, 4, 4), date = structure(c(8, 9, 10, 11, 3, 7, 8,
10, 4, 1, 2, 3, 8, 9, 10, 11, 3, 5, 6), .Label = c("01/07/2006",
"01/08/2006", "01/09/2006", "02/09/2006", "03/09/2006", "06/09/2006",
"11/08/2006", "22/08/2006", "24/08/2006", "28/08/2006", "30/08/2006"
), class = "factor"), value = c(48, 50, 150, 100, 30, 30, 100,
11, 5, 3, 100, 100, 48, 50, 150, 100, 30, 100, 100)), .Names = c("id",
"date", "value"), class = "data.frame", row.names = c("1", "2",
"3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14",
"15", "16", "17", "18", "19"))

f <- function(x) {
	idx <- which(x$value > 50 & c(x$value[-1], 0) > 50)
	if (length(idx) > 0) x[idx[1],]
}
do.call(rbind, by(DF, DF$id, f))


On 8/22/06, Bonfigli Sandro <bonfigli at inmi.it> wrote:
> I have a dataframe with the following structure
>
> id    date         value
> -------------------------
> 1    22/08/2006     48
> 1    24/08/2006     50
> 1    28/08/2006     150
> 1    30/08/2006     100
> 1    01/09/2006     30
> 2    11/08/2006     30
> 2    22/08/2006     100
> 2    28/08/2006     11
> 2    02/09/2006     5
> 3    01/07/2006     3
> 3    01/08/2006     100
> 3    01/09/2006     100
> 4    22/08/2006     48
> 4    24/08/2006     50
> 4    28/08/2006     150
> 4    30/08/2006     100
> 4    01/09/2006     30
> 4    03/09/2006     100
> 4    06/09/2006     100
>
>
> N.B.: dates in european format; ordered dataframe
>
> For each ID I need to select the first occurrence of
> all the rows which are the first of at least two with
> "value" >= 50.
>
> Rather convoluted explication. I mean that for each id I have to select
> the first row in which value is > 50 only if at least the following row
> has "value" > 50 too. If this is not true I repeat the test for all the
> following rows in which "value" > 50 untill I find a record that respects
> the condition
>
> this means that with my example dataframe the result is :
> id    date         value
> -------------------------
> 1    28/08/2006     150
> 3    01/08/2006     100
> 4    28/08/2006     150
>
> It's clear that a for loop would work but I think that that is a better
> way.
>
> I tried "by" and could obtain the first row for wich "value" is > 50.
>
> I thought of an iterative process (delete the first row > 50, find the
> second row > 50, examine if there are rows in the middle) but it
> is quite inelegant as if the first value is not the "good" one I have to
> repeat the process for a a priori unknown number of times.
>
> Thanks in advance for Your help
>
>  Sandro Bonfigli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From samban_isi at yahoo.com  Wed Aug 23 02:21:34 2006
From: samban_isi at yahoo.com (Samprit Banerjee)
Date: Tue, 22 Aug 2006 17:21:34 -0700 (PDT)
Subject: [R] Rtangle in R-2.3.1
Message-ID: <20060823002134.16786.qmail@web52102.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/f7f0bcf3/attachment.pl 

From eugenedalt at yahoo.com  Wed Aug 23 02:46:48 2006
From: eugenedalt at yahoo.com (eugene dalt)
Date: Tue, 22 Aug 2006 17:46:48 -0700 (PDT)
Subject: [R] Question on R Training
In-Reply-To: <20060822203222.41862.qmail@web56007.mail.re3.yahoo.com>
Message-ID: <20060823004648.49932.qmail@web30010.mail.mud.yahoo.com>

Check out with XLSolutions Corp  
sue at xlsolutions-corp.com

--- jeffrey ludlow <jeffreyvludlow at yahoo.com> wrote:

> Dear R users, 
>    
>   The R Project website doesn't seem to have any
> links devoted to R training. Are there any R
> trainers out there?
>    
>   Thank you for your help!
>    
>   Jeffrey V. Ludlow 
>    
>    
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From epistat at gmail.com  Wed Aug 23 05:23:03 2006
From: epistat at gmail.com (zhijie zhang)
Date: Wed, 23 Aug 2006 11:23:03 +0800
Subject: [R] how to complete this task on data management
Message-ID: <2fc17e30608222023x4e66863dmdb4b956fe9a93bad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/ba5f0596/attachment.pl 

From mikewolfgang at gmail.com  Wed Aug 23 05:30:36 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Tue, 22 Aug 2006 23:30:36 -0400
Subject: [R] glm inside one self-defined function
Message-ID: <e668df8c0608222030w76a3aea2p28b758d20361dfd9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060822/a0908f6e/attachment.pl 

From Bill.Venables at csiro.au  Wed Aug 23 05:54:28 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Wed, 23 Aug 2006 13:54:28 +1000
Subject: [R] glm inside one self-defined function
Message-ID: <B998A44C8986644EA8029CFE6396A924840127@exqld2-bne.qld.csiro.au>

Mike Wolfgang asks:

>
> From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Wolfgang
> Sent: Wednesday, 23 August 2006 1:31 PM
> To: R-help list
> Subject: [R] glm inside one self-defined function
> 
> Hi list,
> 
> I've searched in R-help and found some related discussions but still
could
> not understand this type of error. My own function is pretty complex,
so I
> would not put it here, but the basic algorithm is like this:
> myfun<-function(k){
>   mydata<-...#by someway I create a data frame
>   mymodel<-glm(y~.,family=binomial(),data=mydata)
>   ...#some other stuff
> }

I think you are leaving out something.  Here is a test of what you
claim gives a problem (R 2.3.1, Windows):

    > myfun <- function(n) {
    +   z <- rnorm(n)
    +   mydata <- data.frame(x = z, 
    +     y = rbinom(n, size = 1, prob = exp(z)/(1+exp(z))))
    +   fm <- glm(y ~ x, binomial, mydata)
    +   fm
    + }
    > 
    > myfun(100)
    
    Call:  glm(formula = y ~ x, family = binomial, data = mydata) 
    
    Coefficients:
    (Intercept)            x  
    	 0.1587       1.0223  
    
    Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
    Null Deviance:      137.6 
    Residual Deviance: 118.3        AIC: 122.3 

Not even a murmur of complaint.  (This also works in S-PLUS 7.0 but
earlier versions of S-PLUS gave a problem rather like the one you note,
curiously.)

Look again at your code and see if the abstract version you give
really matches what you did, may I suggest?

> 
> as I execute this function, it gives error like this
> Error in inherits(x, "data.frame") : object "mydata" not found
> 
> So I guess glm here tries to find "mydata" in the parent environment.
Why
> doesn't it take "mydata" inside the function? How to let glm correctly
> locate it? Is this (scope/environment) mentioned in R manual? Thanks,
> 
> Mike


From gyadav at ccilindia.co.in  Wed Aug 23 06:08:46 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 23 Aug 2006 09:38:46 +0530
Subject: [R] R is wonderful
In-Reply-To: <20060822215145.73852.qmail@web25710.mail.ukl.yahoo.com>
Message-ID: <OFCD818951.B878DD2F-ON652571D3.00169527-652571D3.0016F036@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/c43c1f08/attachment.pl 

From rmh at temple.edu  Wed Aug 23 07:35:10 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 23 Aug 2006 01:35:10 -0400 (EDT)
Subject: [R] how to run ANCOVA?
Message-ID: <20060823013510.BGM55667@po-d.temple.edu>

>  aov.out <- aov(response~diagnosis*gender,data)

Just add it where you think it belongs in the
sequential sum of squares

To adjust the factors for the covariate use
aov.out <- aov(response ~ age + diagnosis*gender, data)

To adjust the covariate for the factors
aov.out <- aov(response ~ diagnosis*gender + age, data)

If you want to check for interaction of the factors with the
covariate, then use * instead of + in the formula.

Please note that I added spaces to your statement to improve human legibility.

Rich


From mel at altk.com  Wed Aug 23 07:59:45 2006
From: mel at altk.com (mel)
Date: Wed, 23 Aug 2006 07:59:45 +0200
Subject: [R] R is wonderful
In-Reply-To: <20060822215145.73852.qmail@web25710.mail.ukl.yahoo.com>
References: <20060822215145.73852.qmail@web25710.mail.ukl.yahoo.com>
Message-ID: <44EBEED1.6080308@altk.com>


RIsWonderful = function() {cat('oh yes \n')};

;-)


From christian.davies at citigroup.com  Tue Aug 22 18:30:20 2006
From: christian.davies at citigroup.com (Davies, Christian [CIR])
Date: Tue, 22 Aug 2006 17:30:20 +0100
Subject: [R] Job Opportunity - Citigroup Quantitative Equity Research
Message-ID: <F057CE4D66779741AD3FD9BAE835ACFE0935CA90@exukmb23.eur.nsroot.net>

To all:

The Quantitative Equity Research group in London provides quantitative
solutions for active and passive equity portfolio managers in every step
of the investment process. The team is looking to expand their product
mix and looking for a quantitative analyst who will be mainly
responsible for product/model building and infrastructure development.
The analyst is expected to maintain some of the already existing models
as well as enhance and develop new products in the areas of absolute and
relative risk modelling, portfolio construction and trading strategies.

The ideal candidate will have approximately 2 to 3 years experience in
financial markets, excellent programming skills in either S+ or R, with
the ability to create a user-friendly infrastructure for delivering
existing products to clients. Knowledge of industry data sources such as
Factset, Datastream and Bloomberg is also preferred. The candidate must
also be a team player, have good communication skills and be
self-motivated with the capability to work in a flat team structure and
under limited supervision. Additionally the candidate must be highly
numerical and a good problem solver with ideally a first degree in IT,
Mathematics or Engineering and an MSc in Finance and/or practical
knowledge in quantitative equity portfolio management. 
 
To apply, please email a resume and letter of application to
Christian.Davies at Citigroup.com by Friday September 1st 2006. Please do
**not** cc this list.

Christian Davies
Citigroup Global Quantitative Research
London


From rene.eschen at unifr.ch  Wed Aug 23 09:47:56 2006
From: rene.eschen at unifr.ch (ESCHEN Rene)
Date: Wed, 23 Aug 2006 09:47:56 +0200
Subject: [R] Random structure of nested design in lme
References: <2323A6D37908A847A7C32F1E3662C80E132CEE@dc1ex01.air.org>	<E632249B3E11B14AAABA75FDB5F32B24132A0B@EXCHANGE4.unifr.ch>	<44C26882.2090209@pdf.com>
	<E632249B3E11B14AAABA75FDB5F32B24132A0F@EXCHANGE4.unifr.ch>
	<44D28838.1090009@pdf.com>
Message-ID: <E632249B3E11B14AAABA75FDB5F32B24132A13@EXCHANGE4.unifr.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/fbdbfde6/attachment.pl 

From petr.pikal at precheza.cz  Wed Aug 23 11:10:31 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 23 Aug 2006 11:10:31 +0200
Subject: [R] how to complete this task on data management
In-Reply-To: <2fc17e30608222023x4e66863dmdb4b956fe9a93bad@mail.gmail.com>
Message-ID: <44EC37A7.3700.830A05@localhost>

Hi

I am not sure what you really want. If you try to preserve first part 
of your objects just exclude them from operation e.g.

data[-(1:5),] will exclude first five rows from your dataframe.

However it is unclear what you want to do next. Instead of three 
items you want only add one different?

data.frame(x=c(data[(1:5),],6))

or another vector

data.frame(x=c(data[(1:5),],some.other.data))

Following probably too complicated construction tells you which is 
the position of the second value lower then some threshold (in this 
case 3.5) in a vector.

which(diff(cumsum(diff(data<3.5)==1)<2)!=0)+2

HTH
Petr



On 23 Aug 2006 at 11:23, zhijie zhang wrote:

Date sent:      	Wed, 23 Aug 2006 11:23:03 +0800
From:           	"zhijie zhang" <epistat at gmail.com>
To:             	R-help at stat.math.ethz.ch
Subject:        	[R] how to complete this task on data management

> Dear friends,
>  When i clean my dataset , i met a difficulty
>  suppose my data set is :
> *> data<-data.frame(x=c(1:5,1,2,3))
> > data
>   x
> 1 1
> 2 2
> 3 3
> 4 4
> 5 5*
> 6 1
> 7 2
> 8 3
> Now i need to add the data which are less than 3.5 at the bottom, not
> including the top data, so the results should be :
>   x
> 1 1
> 2 2
> 3 3
> 4 4
> 5 5
> *6 6*
> I tried to use " data[data$x>3.5,]" to do it , but it also delete the
> first several numbers,* How to finish it ?* Thanks very much. -- Kind
> Regards, Zhi Jie,Zhang
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From jm540 at york.ac.uk  Wed Aug 23 11:37:58 2006
From: jm540 at york.ac.uk (Jon Minton)
Date: Wed, 23 Aug 2006 10:37:58 +0100
Subject: [R] 3d timeseries dataframe
Message-ID: <000601c6c697$d1eed750$75cc85f0$@ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/e737cfaa/attachment.pl 

From ffenics2002 at yahoo.co.uk  Wed Aug 23 11:41:37 2006
From: ffenics2002 at yahoo.co.uk (Ffenics)
Date: Wed, 23 Aug 2006 10:41:37 +0100 (BST)
Subject: [R] creating a list from distance matrix
Message-ID: <20060823094137.40615.qmail@web25514.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/7ad6abbd/attachment.pl 

From Peter.Marx at knorr-bremse.com  Wed Aug 23 13:06:14 2006
From: Peter.Marx at knorr-bremse.com (Peter.Marx at knorr-bremse.com)
Date: Wed, 23 Aug 2006 13:06:14 +0200
Subject: [R] how to get a histogram of an POSIXct vector ?
Message-ID: <462B1838272F2748ACBA83A7127F1B788D1B22@MUCS7049.corp.knorr-bremse.com>

Hi,

search on web indicates that R also includes a hist method on POSIXct
vectors.

My (perhaps too unexperienced) approach below yields an error.

Could somebody give me a hint what's wrong ?

Peter 


> str(samples)
`data.frame':   7500 obs. of  1 variable:
 $ DateTime:'POSIXct', format: chr  "2006-07-20 00:10:08" "2006-07-20
00:11:17" "2006-07-20 00:11:23" 

>hist(samples)
Error in hist.default(samples) : 'x' must be numeric


From niederlein-rstat at yahoo.de  Wed Aug 23 13:11:52 2006
From: niederlein-rstat at yahoo.de (Antje)
Date: Wed, 23 Aug 2006 13:11:52 +0200
Subject: [R] two density curves in one plot?
Message-ID: <44EC37F8.4020907@yahoo.de>

Hello,

I was wondering if I can plot two curves I get from "density(data)" into 
one plot. I want to compare both.
With the following commad, I just get one curve plotted:

plot( density(mydata) )

Sorry for this stupid question but I could not find a solution until now...

Antje


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug 23 13:25:58 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 23 Aug 2006 13:25:58 +0200
Subject: [R] two density curves in one plot?
References: <44EC37F8.4020907@yahoo.de>
Message-ID: <00ca01c6c6a6$e87b3fe0$0540210a@www.domain>

try this:

x1 <- rnorm(1000)
x2 <- rnorm(1000)

d1 <- density(x1)
d2 <- density(x2)
plot(range(d1$x, d2$x), range(d1$y, d2$y), type = "n",
     xlab = "x", ylab = "Density")
lines(d1, col = "red")
lines(d2, col = "blue")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Antje" <niederlein-rstat at yahoo.de>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, August 23, 2006 1:11 PM
Subject: [R] two density curves in one plot?


> Hello,
>
> I was wondering if I can plot two curves I get from "density(data)" 
> into
> one plot. I want to compare both.
> With the following commad, I just get one curve plotted:
>
> plot( density(mydata) )
>
> Sorry for this stupid question but I could not find a solution until 
> now...
>
> Antje
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Wed Aug 23 13:39:05 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Aug 2006 12:39:05 +0100 (BST)
Subject: [R] how to get a histogram of an POSIXct vector ?
In-Reply-To: <462B1838272F2748ACBA83A7127F1B788D1B22@MUCS7049.corp.knorr-bremse.com>
References: <462B1838272F2748ACBA83A7127F1B788D1B22@MUCS7049.corp.knorr-bremse.com>
Message-ID: <Pine.LNX.4.64.0608231235060.1482@gannet.stats.ox.ac.uk>

On Wed, 23 Aug 2006, Peter.Marx at knorr-bremse.com wrote:

> Hi,
> 
> search on web indicates that R also includes a hist method on POSIXct
> vectors.

> methods(hist)
[1] hist.Date*   hist.default hist.POSIXt*

   Non-visible functions are asterisked

so almost (objects of class POSIXct inherit from class POSIXt).

> My (perhaps too unexperienced) approach below yields an error.
> 
> Could somebody give me a hint what's wrong ?

samples is a data frame, not an object of class "POSIXct".
Try hist(samples$DateTime).

(And thanks for giving enough detail to enable us to spot the problem.)

> 
> Peter 
> 
> 
> > str(samples)
> `data.frame':   7500 obs. of  1 variable:
>  $ DateTime:'POSIXct', format: chr  "2006-07-20 00:10:08" "2006-07-20
> 00:11:17" "2006-07-20 00:11:23" 
> 
> >hist(samples)
> Error in hist.default(samples) : 'x' must be numeric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From niederlein-rstat at yahoo.de  Wed Aug 23 13:40:49 2006
From: niederlein-rstat at yahoo.de (Antje)
Date: Wed, 23 Aug 2006 13:40:49 +0200
Subject: [R] two density curves in one plot?
In-Reply-To: <44EC3A77.3040406@uni-osnabrueck.de>
References: <44EC37F8.4020907@yahoo.de> <44EC3A77.3040406@uni-osnabrueck.de>
Message-ID: <44EC3EC1.60203@yahoo.de>

Thank you both very much.
It works!


From HDoran at air.org  Wed Aug 23 13:40:10 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 23 Aug 2006 07:40:10 -0400
Subject: [R] Random structure of nested design in lme
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2773DD@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/a1e6c727/attachment.ksh 

From murdoch at stats.uwo.ca  Wed Aug 23 13:59:50 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 23 Aug 2006 07:59:50 -0400
Subject: [R] 3d timeseries dataframe
In-Reply-To: <000601c6c697$d1eed750$75cc85f0$@ac.uk>
References: <000601c6c697$d1eed750$75cc85f0$@ac.uk>
Message-ID: <44EC4336.60101@stats.uwo.ca>

Jon Minton wrote:
> Hi, 
>
>  
>
> I'm new to R so this might be a little basic for enlightened people like
> yourselves...
>
>  
>
> I have quarterly British Labour Force Survey (Local Area) data from 1992 to
> present in .tab format* in folders running from 01 (for the first time
> period) to 56 (for the most recent).
>
>  
>
> i.e. my directory structure is:
>
>       .../t/
>
>             01
>
>             ...
>
>             56
>
>        
>
> Taking the result of dir() on the t/ directory I can get access to each
> subdirectory as an element in a vector of strings (i.e. element [n] points
> to directory n) Using a regex sequence I can specify only the lad .tab file
> within each directory (as they each contain another .tab datafile for higher
> level geographical regions)
>
>  
>
> I now need to load all the data into a 3d dataframe,** where the first
> dimension refers to the t value (01...56), and I can access it to make
> longitudinal comparisons between variables.
>   

"dataframe" means a specific 2d layout, like a table in a database.  R 
has 3d structures, but calls
them "array"s.  All entries in an array must be the same type (columns 
in a dataframe need not be).

So the basic outline of what you want is something like this:

thedata <- array(NA, c(m, n, p))

where m, n and p are the extents of the 3 indices of the desired array, 
e.g. c(56, 7, 5)
if you have 5 daily variables, measured daily over 56 weeks.

Then fill the array by something like this:

for (m in 1:56) {
  # read period m
  thedata[m, ,] <- some data
}
>  
>
> How do I do this?
>
>  
>
> Thanks for your help,
>
>  
>
> Jon
>
>  
>
> * some of the data was only available in other formats: does R read SPSS
> .por (portable) data? If not I'll just convert it.
>
> ** or would it be better to just mark the data from each t with a new
> variable, and have it as a long file?
>   
Lots of functions in R are set up to use dataframes rather than arrays 
or subsets of arrays, so this might be more convenient.

Duncan Murdoch
>  
>
>  
>
>  
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From z.dalton at lancaster.ac.uk  Wed Aug 23 14:08:08 2006
From: z.dalton at lancaster.ac.uk (z.dalton at lancaster.ac.uk)
Date: Wed, 23 Aug 2006 13:08:08 +0100 (BST)
Subject: [R] negatively skewed data; reflecting
Message-ID: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/d5ffa447/attachment.ksh 

From jm540 at york.ac.uk  Wed Aug 23 14:25:06 2006
From: jm540 at york.ac.uk (Jon Minton)
Date: Wed, 23 Aug 2006 13:25:06 +0100
Subject: [R] negatively skewed data; reflecting
In-Reply-To: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
References: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
Message-ID: <002a01c6c6af$2ac52f20$804f8d60$@ac.uk>

I'm new to R myself, but am wondering whether the t() (transpose) function
would work?

> hist(t(Lsoc))

Jon

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
z.dalton at lancaster.ac.uk
Sent: 23 August 2006 13:08
To: r-help at stat.math.ethz.ch
Subject: [R] negatively skewed data; reflecting

Hi,

This problem may be very easy, but I can't think of how to do it.  I have
constructed histograms of various variables in my dataset.  Some of them are
negatively skewed, and hence need data transformations applied.  I know that
you first need to reflect the negatively skewed data and then apply another
transformation such as log, square root etc to bring it towards normailty.
How is it that I reflect data in R?  I'm sorry if this seems a very simple
task, I think it involves going back to Maths GCSE and relearning
reflection, rotation, translation etc!  I have searched the internet, but
cannot come up with anything useful on how to reflect data.

> hist(Lsoc)  #how do I reflect Lsoc in R?

I am grateful for any help regarding this matter, it is just a very small
part of my analysis and doesn't seem worth agonising hours over.  I will
probably kick myself when someone tells me the answer!

Thank you very much,

Zoe

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Wed Aug 23 14:43:31 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 23 Aug 2006 08:43:31 -0400
Subject: [R] negatively skewed data; reflecting
In-Reply-To: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
References: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
Message-ID: <44EC4D73.4070308@stats.uwo.ca>

On 8/23/2006 8:08 AM, z.dalton at lancaster.ac.uk wrote:
> Hi,
> 
> This problem may be very easy, but I can't think of how to do it.  I have constructed histograms of various variables in my dataset.  Some of them are negatively skewed, and hence need data transformations applied.  I know that you first need to reflect the negatively skewed data and then apply another transformation such as log, square root etc to bring it towards normailty. How is it that I reflect data in R?  I'm sorry if this seems a very simple task, I think it involves going back to Maths GCSE and relearning reflection, rotation, translation etc!  I have searched the internet, but cannot come up with anything useful on how to reflect data.
> 
>> hist(Lsoc)  #how do I reflect Lsoc in R?

Reflected <- -Lsoc

will reflect about zero.

> I am grateful for any help regarding this matter, it is just a very small part of my analysis and doesn't seem worth agonising hours over.  I will probably kick myself when someone tells me the answer!

Please don't kick yourself :-)

Duncan Murdoch

> 
> Thank you very much,
> 
> Zoe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From f.harrell at vanderbilt.edu  Wed Aug 23 14:56:53 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 23 Aug 2006 07:56:53 -0500
Subject: [R] negatively skewed data; reflecting
In-Reply-To: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
References: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
Message-ID: <44EC5095.1030303@vanderbilt.edu>

z.dalton at lancaster.ac.uk wrote:
> Hi,
> 
> This problem may be very easy, but I can't think of how to do it.  I have constructed histograms of various variables in my dataset.  Some of them are negatively skewed, and hence need data transformations applied.  I know that you first need to reflect the negatively skewed data and then apply another transformation such as log, square root etc to bring it towards normailty. How is it that I reflect data in R?  I'm sorry if this seems a very simple task, I think it involves going back to Maths GCSE and relearning reflection, rotation, translation etc!  I have searched the internet, but cannot come up with anything useful on how to reflect data.
> 
>> hist(Lsoc)  #how do I reflect Lsoc in R?
> 
> I am grateful for any help regarding this matter, it is just a very small part of my analysis and doesn't seem worth agonising hours over.  I will probably kick myself when someone tells me the answer!
> 
> Thank you very much,
> 
> Zoe

To add further complication, if the transformation to normality is 
empirically based, the true variance of resulting estimates will inherit 
the variance from the empirical assessment.  For example, if you use a 
histogram or empirical CDF to find the transformation, the imprecision 
of the empirical CDF will add a good deal of true variance to the final 
estimates so that they are no more precise than sample quantiles on the 
original scale.  To put it another way, the sample median seems to be 
inefficient (efficiency 2/pi) compared to the sample mean if normality 
holds, but that relative efficiency rises if normality were "rigged".
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From petr.pikal at precheza.cz  Wed Aug 23 15:03:57 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 23 Aug 2006 15:03:57 +0200
Subject: [R] how to complete this task on data management
In-Reply-To: <2fc17e30608230423v2f2daf76i58f7fde0fd5276d3@mail.gmail.com>
References: <44EC37A7.3700.830A05@localhost>
Message-ID: <44EC6E5D.15583.158CCDC@localhost>

Hi

This is a little bit more precise. My sugeestion works with unordered 
data and finds row index for second item lower then a threshold.

which(diff(cumsum(diff(data<3.5)==1)<2)!=0)+2

However with ordered data you need to slightly modify it

which(diff(cumsum(diff(data<3.5)!=0)<2)!=0)+2

I bet there is some other solution 

HTH
Petr



On 23 Aug 2006 at 19:23, zhijie zhang wrote:

Date sent:      	Wed, 23 Aug 2006 19:23:49 +0800
From:           	"zhijie zhang" <epistat at gmail.com>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>
Subject:        	Re: [R] how to complete this task on data management

> *Dear friends,*
> * I'd like to explain it clearly*
> *   x
> **1 1
> 2 2
> 3 3
> 4 4
> 5 5
> *6 1
> 7 2
> 8 3
> I want to retain the first part of the dataset(1,2,3,4,5) if the
> continuous data(1,2,3) in the latter part of dataset is less than 3.5,
> in fact ,i want to know the row index (it's 6 in this dataset)that is
> less than 3.5. In fact, my dataset is very large, so i should find the
> index automatically. My idea is: First:Find the continous data in the
> latter dataset,which is less than a certain value,here it's 3.5.
>   X
> 6 1
> 7 2
> 8 3
> 
> Second:Identify the index (here,it's 6), which corresponds to the
> first data in the  latter dataset
>    X
> *6* 1
> Finally,select the the first (index-1) number.(6-1=5)
> *   x
> **1 1
> 2 2
> 3 3
> 4 4
> 5 5
> *
> Thanks very much.
> 
> 
> On 8/23/06, Petr Pikal <petr.pikal at precheza.cz> wrote:
> >
> > Hi
> >
> > I am not sure what you really want. If you try to preserve first
> > part of your objects just exclude them from operation e.g.
> >
> > data[-(1:5),] will exclude first five rows from your dataframe.
> >
> > However it is unclear what you want to do next. Instead of three
> > items you want only add one different?
> >
> > data.frame(x=c(data[(1:5),],6))
> >
> > or another vector
> >
> > data.frame(x=c(data[(1:5),],some.other.data))
> >
> > Following probably too complicated construction tells you which is
> > the position of the second value lower then some threshold (in this
> > case 3.5) in a vector.
> >
> > which(diff(cumsum(diff(data<3.5)==1)<2)!=0)+2
> >
> > HTH
> > Petr
> >
> >
> >
> > On 23 Aug 2006 at 11:23, zhijie zhang wrote:
> >
> > Date sent:              Wed, 23 Aug 2006 11:23:03 +0800
> > From:                   "zhijie zhang" <epistat at gmail.com>
> > To:                     R-help at stat.math.ethz.ch
> > Subject:                [R] how to complete this task on data
> > management
> >
> > > Dear friends,
> > >  When i clean my dataset , i met a difficulty
> > >  suppose my data set is :
> > > *> data<-data.frame(x=c(1:5,1,2,3))
> > > > data
> > >   x
> > > 1 1
> > > 2 2
> > > 3 3
> > > 4 4
> > > 5 5*
> > > 6 1
> > > 7 2
> > > 8 3
> > > Now i need to add the data which are less than 3.5 at the bottom,
> > > not including the top data, so the results should be :
> > >   x
> > > 1 1
> > > 2 2
> > > 3 3
> > > 4 4
> > > 5 5
> > > *6 6*
> > > I tried to use " data[data$x>3.5,]" to do it , but it also delete
> > > the first several numbers,* How to finish it ?* Thanks very much.
> > > -- Kind Regards, Zhi Jie,Zhang
> > >
> > >  [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> 
> -- 
> Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
> Tel:86-21-54237149
> 

Petr Pikal
petr.pikal at precheza.cz


From vettorazzi at econ.uni-hamburg.de  Wed Aug 23 15:02:38 2006
From: vettorazzi at econ.uni-hamburg.de (Eik Vettorazzi)
Date: Wed, 23 Aug 2006 15:02:38 +0200
Subject: [R] negatively skewed data; reflecting
In-Reply-To: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
References: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
Message-ID: <op.teqmioh3j3tevv@econ.uni-hamburg.de>

a simple reflection (on the y-axis) of x is -x, but you have to ensure  
that there are only nonnegative numbers if you want to use the log  
transformation. So you should reflect on a postive number z greater than  
abs(min(x)), if min(x)<0. This is done by z-x.
Why don't you simply shift your data by this amount z or use a  
box-cox-transformation at all?

Am Wed, 23 Aug 2006 14:08:08 +0200 schrieb <z.dalton at lancaster.ac.uk>:

> Hi,
>
> This problem may be very easy, but I can't think of how to do it.  I  
> have constructed histograms of various variables in my dataset.  Some of  
> them are negatively skewed, and hence need data transformations  
> applied.  I know that you first need to reflect the negatively skewed  
> data and then apply another transformation such as log, square root etc  
> to bring it towards normailty. How is it that I reflect data in R?  I'm  
> sorry if this seems a very simple task, I think it involves going back  
> to Maths GCSE and relearning reflection, rotation, translation etc!  I  
> have searched the internet, but cannot come up with anything useful on  
> how to reflect data.
>
>> hist(Lsoc)  #how do I reflect Lsoc in R?
>
> I am grateful for any help regarding this matter, it is just a very  
> small part of my analysis and doesn't seem worth agonising hours over.   
> I will probably kick myself when someone tells me the answer!
>
> Thank you very much,
>
> Zoe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kubovy at virginia.edu  Wed Aug 23 15:11:22 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 23 Aug 2006 09:11:22 -0400
Subject: [R] negatively skewed data; reflecting
In-Reply-To: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
References: <E1GFrWi-0002Ol-00@wing1.lancs.ac.uk>
Message-ID: <294B52D2-4593-4F70-B670-EF4405431E76@virginia.edu>

On Aug 23, 2006, at 8:08 AM, z.dalton at lancaster.ac.uk wrote:

> I have constructed histograms of various variables in my dataset.   
> Some of them are negatively skewed, and hence need data  
> transformations applied.  I know that you first need to reflect the  
> negatively skewed data and then apply another transformation such  
> as log, square root etc to bring it towards normailty. How is it  
> that I reflect data in R?
>
>> hist(Lsoc)  #how do I reflect Lsoc in R?

To reflect the vector Lsoc, assuming the numbers are positive, and  
keep them positive in order to take (e.g.) logs,
log(-Lsoc + max(Lsoc) + 1).

But if you do a Google search on 'ladder powers negative skew' you'll  
find another answer. The idea of the ladder of powers is due to J W  
Tukey.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From lothar.schmid at googlemail.com  Wed Aug 23 15:39:49 2006
From: lothar.schmid at googlemail.com (Lothar Schmid)
Date: Wed, 23 Aug 2006 15:39:49 +0200
Subject: [R] problems installing odrpack
Message-ID: <eaa9854d0608230639o2747f860q8a8e4455523075fc@mail.gmail.com>

Hello,

I'm trying to install odrpack (http://www.netlib.org/odrpack/) on my
ubuntu linux system. That is a fortran package which I need to fit a
circle to 2-dim data points (if someone knows a simpler package for
that task, please tell me).

For installing odrpack, I need a utility called fsplit, which splits
fortran files into single subroutines. However, there seems to be no
ubuntu package for fsplit. Neither google nor the ubuntu search engine
produced a result.

I found several fsplit tarballs, which compiled fine. But when I start
fsplit, the error message "fsplit: can't load dynamic linker
'/lib/ld.so'" appears. I tried linking ld.so to the file ld-2.3.6.so
in my /lib  directory, but that did not help.

Thanks for your advice,

Lothar


From sasha.wolosin at gmail.com  Wed Aug 23 16:29:34 2006
From: sasha.wolosin at gmail.com (Sasha Wolosin)
Date: Wed, 23 Aug 2006 10:29:34 -0400
Subject: [R] how to run ANCOVA?
In-Reply-To: <20060823013510.BGM55667@po-d.temple.edu>
References: <20060823013510.BGM55667@po-d.temple.edu>
Message-ID: <74537bfe0608230729p7fdde699iadd8700590e223d2@mail.gmail.com>

Hello,

   Thank you very much for your response!

However, I still do not understand how this works.  I would like to
adjust the factors (diagnosis and gender) for the covariate (age), so
you are saying I should use:

        aov.out <- aov(response ~ age + diagnosis*gender, data)
or
        aov.out <- aov(response ~ age*diagnosis*gender, data)

But how is that different from just a 3-way ANOVA with age, diagnosis,
and gender as the the three effects?  Isn't ANCOVA a fundamentally
different model?

Thanks,
Sasha



On 8/23/06, Richard M. Heiberger <rmh at temple.edu> wrote:
> >  aov.out <- aov(response~diagnosis*gender,data)
>
> Just add it where you think it belongs in the
> sequential sum of squares
>
> To adjust the factors for the covariate use
> aov.out <- aov(response ~ age + diagnosis*gender, data)
>
> To adjust the covariate for the factors
> aov.out <- aov(response ~ diagnosis*gender + age, data)
>
> If you want to check for interaction of the factors with the
> covariate, then use * instead of + in the formula.
>
> Please note that I added spaces to your statement to improve human legibility.
>
> Rich
>


From ggrothendieck at gmail.com  Wed Aug 23 16:37:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 23 Aug 2006 10:37:10 -0400
Subject: [R] two density curves in one plot?
In-Reply-To: <44EC37F8.4020907@yahoo.de>
References: <44EC37F8.4020907@yahoo.de>
Message-ID: <971536df0608230737n5829f384r8a63951275e79830@mail.gmail.com>

With lattice graphics:

library(lattice)
d1 <- rnorm(100)
d2 <- runif(100)
densityplot(~ d1 + d2, auto.key = TRUE)

On 8/23/06, Antje <niederlein-rstat at yahoo.de> wrote:
> Hello,
>
> I was wondering if I can plot two curves I get from "density(data)" into
> one plot. I want to compare both.
> With the following commad, I just get one curve plotted:
>
> plot( density(mydata) )
>
> Sorry for this stupid question but I could not find a solution until now...
>
> Antje
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tramni at abv.bg  Wed Aug 23 16:55:04 2006
From: tramni at abv.bg (Martin Ivanov)
Date: Wed, 23 Aug 2006 17:55:04 +0300 (EEST)
Subject: [R] nonlinear least squares trust region fitting ?
Message-ID: <1093419308.370791156344904948.JavaMail.nobody@mail09.abv.bg>

Hello!

I am running R-2.3.1-i386-1 on Slackware Linux 10.2. I am a former matlab user, moving to R. In matlab, via the cftool, I performed nonlinear curve fitting using the method "nonlinear least squares" with the "Trust-Region" algorithm and not using robust fitting. Is it possible to perform the same analysis in R? I read quite a lot of R documentation, but I could not find an alternative solution. If there is such, please forgive my ignorance (I am a newbie in R) and tell me which function from which package is capable of performing the same analysis. If the same analysis is not possible to carry out in R, I would be grateful if you suggest to me some alternative procedure. I found that the "nls" function performs nonlinear least squares. The problem is that I do not want to implement the Gauss-Newton algorithm. In the worst case I would be contented with the "Levenberg-Marquardt" algorithm, if it is implemented in R. R nls's documentation mentions the "port" package and the ?nl
 2sol? algorithm, but I could not find that package in the CRAN repository, so that I could read and judge whether that algorithm would be appropriate.

Thank you very much in advance. I am looking forward to your answer.
Regards,
Martin

-----------------------------------------------------------------
http://ide.li/ - ?????? ?? ????????? ?? ?????. ??????, ??????, ??????, ??????, ??????????.


From damien.moore at excite.com  Wed Aug 23 17:06:43 2006
From: damien.moore at excite.com (Damien Moore)
Date: Wed, 23 Aug 2006 11:06:43 -0400 (EDT)
Subject: [R] lean and mean lm/glm?
Message-ID: <20060823150643.273AE8B410@xprdmxin.myway.com>


Thomas Lumley wrote:

> No, it is quite straightforward if you are willing to make multiple passes 
> through the data. It is hard with a single pass and may not be possible 
> unless the data are in random order.
> 
> Fisher scoring for glms is just an iterative weighted least squares 
> calculation using a set of 'working' weights and 'working' response. These 
> can be defined chunk by chunk and fed to biglm. Three iterations should 
> be sufficient.

(NB: Although not stated clearly I was referring to single pass when I wrote "impossible"). Doing as you suggest with multiple passes would entail either sticking the database input calls into the main iterative loop of a lookalike glm.fit or lumping the user with a very unattractive sequence of calls:

big_glm.init
iterate:
load_data_chunk
big_glm.newiter
iterate: #could use a subset of the chunks on the first few go rounds
load_data_chunk
update.big_glm
big_glm.check_convergence #would also need to do coefficient adjustments if convergence is failing

Because most (if not all) of my data can fit into memory anyway, I propose simply doing the calcs in a modified glm.fit in chunks (i.e. by subsetting the X and y data matrices within the loops) with a user defined chunk length. I can always add database input calls later to handle exceptionally large datasets.

If one of you has a better suggestion I'm willing to hear it.

So far, I have hacked out a lot of the (in my view) extraneous stuff from glm and halved its memory usage. I can now run a 12 variable, 1 million observation data set using "only" 200Mb of working memory (excluding the memory required to store the data). Previously fit.glm was using 500Mb to do the same. To get convergence took 9 iterations (either way). To reiterate: the inefficiency is in calculating estimates, not in storing data.


From mikewolfgang at gmail.com  Wed Aug 23 17:08:52 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Wed, 23 Aug 2006 11:08:52 -0400
Subject: [R] glm inside one self-defined function
In-Reply-To: <B998A44C8986644EA8029CFE6396A924840127@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924840127@exqld2-bne.qld.csiro.au>
Message-ID: <e668df8c0608230808r6ab963c1j3880c656a70755e5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/9950ded2/attachment.pl 

From tlumley at u.washington.edu  Wed Aug 23 17:25:54 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 23 Aug 2006 08:25:54 -0700 (PDT)
Subject: [R] lean and mean lm/glm?
In-Reply-To: <20060823150643.273AE8B410@xprdmxin.myway.com>
References: <20060823150643.273AE8B410@xprdmxin.myway.com>
Message-ID: <Pine.LNX.4.64.0608230814370.12767@homer23.u.washington.edu>

On Wed, 23 Aug 2006, Damien Moore wrote:

>
> Thomas Lumley wrote:
>
>> No, it is quite straightforward if you are willing to make multiple passes
>> through the data. It is hard with a single pass and may not be possible
>> unless the data are in random order.
>>
>> Fisher scoring for glms is just an iterative weighted least squares
>> calculation using a set of 'working' weights and 'working' response. These
>> can be defined chunk by chunk and fed to biglm. Three iterations should
>> be sufficient.
>
> (NB: Although not stated clearly I was referring to single pass when I 
> wrote "impossible"). Doing as you suggest with multiple passes would 
> entail either sticking the database input calls into the main iterative 
> loop of a lookalike glm.fit or lumping the user with a very unattractive 
> sequence of calls:

I have written most of a bigglm() function where the data= argument is a 
function with a single argument 'reset'. When called with reset=FALSE the 
function should return another chunk of data, or NULL if no data are 
available, and when called with reset=TRUE it should go back to the 
beginning of the data.  I don't think this is too inelegant.

In general I don't think a one-pass algorithm is possible. If the data are 
in random order then you could read one chunk, fit a glm, and set up a 
grid of coefficient values around the estimate.  You then read the rest of 
the data, computing the loglikelihood and score function at each point in 
the grid.  After reading all the data you can then fit a suitable smooth 
surface to the loglikelihood.  I don't know whether this will give 
sufficient accuracy, though.

For really big data sets you are probably better off with the approach 
that Brian Ripley and Fei Chen used -- they have shown that it works and 
there unlikely to be anything much simpler that also works that they 
missed.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From f.harrell at vanderbilt.edu  Wed Aug 23 17:54:04 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 23 Aug 2006 10:54:04 -0500
Subject: [R] lean and mean lm/glm?
In-Reply-To: <Pine.LNX.4.64.0608230814370.12767@homer23.u.washington.edu>
References: <20060823150643.273AE8B410@xprdmxin.myway.com>
	<Pine.LNX.4.64.0608230814370.12767@homer23.u.washington.edu>
Message-ID: <44EC7A1C.7080000@vanderbilt.edu>

Thomas Lumley wrote:
> On Wed, 23 Aug 2006, Damien Moore wrote:
> 
>> Thomas Lumley wrote:
>>
>>> No, it is quite straightforward if you are willing to make multiple passes
>>> through the data. It is hard with a single pass and may not be possible
>>> unless the data are in random order.
>>>
>>> Fisher scoring for glms is just an iterative weighted least squares
>>> calculation using a set of 'working' weights and 'working' response. These
>>> can be defined chunk by chunk and fed to biglm. Three iterations should
>>> be sufficient.
>> (NB: Although not stated clearly I was referring to single pass when I 
>> wrote "impossible"). Doing as you suggest with multiple passes would 
>> entail either sticking the database input calls into the main iterative 
>> loop of a lookalike glm.fit or lumping the user with a very unattractive 
>> sequence of calls:
> 
> I have written most of a bigglm() function where the data= argument is a 
> function with a single argument 'reset'. When called with reset=FALSE the 
> function should return another chunk of data, or NULL if no data are 
> available, and when called with reset=TRUE it should go back to the 
> beginning of the data.  I don't think this is too inelegant.
> 
> In general I don't think a one-pass algorithm is possible. If the data are 
> in random order then you could read one chunk, fit a glm, and set up a 
> grid of coefficient values around the estimate.  You then read the rest of 
> the data, computing the loglikelihood and score function at each point in 
> the grid.  After reading all the data you can then fit a suitable smooth 
> surface to the loglikelihood.  I don't know whether this will give 
> sufficient accuracy, though.
> 
> For really big data sets you are probably better off with the approach 
> that Brian Ripley and Fei Chen used -- they have shown that it works and 
> there unlikely to be anything much simpler that also works that they 
> missed.
> 
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle

What I would like to see someone work on is a kind of SQL code generator 
that given a set of weights passes through the database and computes a 
new weighted information matrix.  The code generator would make the 
design matrix a symbolic entity.  SQL or other suitable framework would 
return the p x p matrix for one iteration at a time.

Frank

Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From damien.moore at excite.com  Wed Aug 23 18:14:15 2006
From: damien.moore at excite.com (Damien Moore)
Date: Wed, 23 Aug 2006 12:14:15 -0400 (EDT)
Subject: [R] lean and mean lm/glm?
Message-ID: <20060823161415.CCD2CB5830@xprdmxin.myway.com>


Thomas Lumley < tlumley at u.washington.edu > wrote:

>I have written most of a bigglm() function where the data= argument is a 
>function with a single argument 'reset'. When called with reset=FALSE the 
>function should return another chunk of data, or NULL if no data are 
>available, and when called with reset=TRUE it should go back to the 
>beginning of the data. I don't think this is too inelegant.

yes, that does sound like a pretty elegent solution. It would be even more so if you could offer a default implementation of the data_function that simply passes chunks of large X and y matrices held in memory. (ideally you would just intialize the data_function to reference the X and y data to avoid duplicating it, don't know if that's possible in R.) how long before its ready? :)



--- On Wed 08/23, Thomas Lumley < tlumley at u.washington.edu > wrote:

From: Thomas Lumley [mailto: tlumley at u.washington.edu]
To: damien.moore at excite.com
Cc: r-help at stat.math.ethz.ch, ripley at stats.ox.ac.uk
Date: Wed, 23 Aug 2006 08:25:54 -0700 (PDT)
Subject: Re: [R] lean and mean lm/glm?

On Wed, 23 Aug 2006, Damien Moore wrote:

>
> Thomas Lumley wrote:
>
>> No, it is quite straightforward if you are willing to make multiple passes
>> through the data. It is hard with a single pass and may not be possible
>> unless the data are in random order.
>>
>> Fisher scoring for glms is just an iterative weighted least squares
>> calculation using a set of 'working' weights and 'working' response. These
>> can be defined chunk by chunk and fed to biglm. Three iterations should
>> be sufficient.
>
> (NB: Although not stated clearly I was referring to single pass when I 
> wrote "impossible"). Doing as you suggest with multiple passes would 
> entail either sticking the database input calls into the main iterative 
> loop of a lookalike glm.fit or lumping the user with a very unattractive 
> sequence of calls:

I have written most of a bigglm() function where the data= argument is a 
function with a single argument 'reset'. When called with reset=FALSE the 
function should return another chunk of data, or NULL if no data are 
available, and when called with reset=TRUE it should go back to the 
beginning of the data. I don't think this is too inelegant.

In general I don't think a one-pass algorithm is possible. If the data are 
in random order then you could read one chunk, fit a glm, and set up a 
grid of coefficient values around the estimate. You then read the rest of 
the data, computing the loglikelihood and score function at each point in 
the grid. After reading all the data you can then fit a suitable smooth 
surface to the loglikelihood. I don't know whether this will give 
sufficient accuracy, though.

For really big data sets you are probably better off with the approach 
that Brian Ripley and Fei Chen used -- they have shown that it works and 
there unlikely to be anything much simpler that also works that they 
missed.


-thomas

Thomas Lumley Assoc. Professor, Biostatistics
tlumley at u.washington.edu University of Washington, Seattle


From ripley at stats.ox.ac.uk  Wed Aug 23 18:15:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Aug 2006 17:15:47 +0100 (BST)
Subject: [R] lean and mean lm/glm?
In-Reply-To: <44EC7A1C.7080000@vanderbilt.edu>
References: <20060823150643.273AE8B410@xprdmxin.myway.com>
	<Pine.LNX.4.64.0608230814370.12767@homer23.u.washington.edu>
	<44EC7A1C.7080000@vanderbilt.edu>
Message-ID: <Pine.LNX.4.64.0608231656220.3085@gannet.stats.ox.ac.uk>

On Wed, 23 Aug 2006, Frank E Harrell Jr wrote:

> Thomas Lumley wrote:
> > On Wed, 23 Aug 2006, Damien Moore wrote:
> > 
> > > Thomas Lumley wrote:
> > >
> > > > No, it is quite straightforward if you are willing to make multiple
> > > > passes
> > > > through the data. It is hard with a single pass and may not be possible
> > > > unless the data are in random order.
> > > >
> > > > Fisher scoring for glms is just an iterative weighted least squares
> > > > calculation using a set of 'working' weights and 'working' response.
> > > > These
> > > > can be defined chunk by chunk and fed to biglm. Three iterations should
> > > > be sufficient.
> > > (NB: Although not stated clearly I was referring to single pass when I
> > > wrote "impossible"). Doing as you suggest with multiple passes would
> > > entail either sticking the database input calls into the main iterative
> > > loop of a lookalike glm.fit or lumping the user with a very unattractive
> > > sequence of calls:
> > 
> > I have written most of a bigglm() function where the data= argument is a
> > function with a single argument 'reset'. When called with reset=FALSE the
> > function should return another chunk of data, or NULL if no data are
> > available, and when called with reset=TRUE it should go back to the
> > beginning of the data.  I don't think this is too inelegant.
> > 
> > In general I don't think a one-pass algorithm is possible. If the data are
> > in random order then you could read one chunk, fit a glm, and set up a grid
> > of coefficient values around the estimate.  You then read the rest of the
> > data, computing the loglikelihood and score function at each point in the
> > grid.  After reading all the data you can then fit a suitable smooth surface
> > to the loglikelihood.  I don't know whether this will give sufficient
> > accuracy, though.

Not in general.  One of the problems with a binomial/Poisson glm is the
geometry of the likelihood can be radically changed by a single case: 
suppose that the initial sample were separable?  Misclassifications can 
really get you: one case with an incorrect label can contribute 
arbitrarily much to the log-likelihood (and I have seen 11,000 units).

> > For really big data sets you are probably better off with the approach that
> > Brian Ripley and Fei Chen used -- they have shown that it works and there
> > unlikely to be anything much simpler that also works that they missed.
> 
> What I would like to see someone work on is a kind of SQL code generator that
> given a set of weights passes through the database and computes a new weighted
> information matrix.  The code generator would make the design matrix a
> symbolic entity.  SQL or other suitable framework would return the p x p
> matrix for one iteration at a time.

That is one of the things Fei Chen and I did (using SQL extensions in 
MySQL), and probably the most successful.  We managed to explore models 
with tens of millions of cases with 30 categorical explanatory vars (and 
the data structure and problem was such that this was worthwhile, not 
least because predictions might be needed for 0.01% of the coverage).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mmalten at gmail.com  Wed Aug 23 18:34:00 2006
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Wed, 23 Aug 2006 12:34:00 -0400
Subject: [R] How would you run repeated-measures multifactorial MANCOVA?
Message-ID: <8913fde30608230934l592f4448nf460ab39e35ba8d1@mail.gmail.com>

OK, now that I've worked through Venebales and Ripley and a few other
sources, I can see more than one way of attacking a problem I expect
to be facing soon: a repeated-measures MANCOVA with more than one
X-factor.

But that got me wondering how the people who've been playing with R
longer than I have would be doing it...and thinking it might be an
interesting topic for discussion on the list.

How would you prefer to do a repeated-measures multifactor MANCOVA in
R?   Is there a way that's particularly good --- or particularly bad?


From pberming at arts.ryerson.ca  Wed Aug 23 18:48:54 2006
From: pberming at arts.ryerson.ca (Philip Bermingham)
Date: Wed, 23 Aug 2006 12:48:54 -0400
Subject: [R] Calculating the combined standared errors from two regression
	equations
Message-ID: <4CB9D91A4AD04647BCE0305E391423F702A1E1@mail3.arts.ryerson.ca>

Hello,
 
I have two regression equations, one predicts number of interactions from people-a and the other predicts number of interactions from people-b.  I am summing the number of interactions to get the combined number of interactions.  How do I calculate the combined standared error?
 
Philip.


From bchasco at u.washington.edu  Wed Aug 23 18:58:32 2006
From: bchasco at u.washington.edu (B. Chasco)
Date: Wed, 23 Aug 2006 09:58:32 -0700 (PDT)
Subject: [R] editing ".Internal" functions
Message-ID: <Pine.LNX.4.64.0608221516190.29128@homer21.u.washington.edu>

There is a function called arrows() which is an .Internal function.  How 
difficult is it to modify that function to return the xy coordinates for 
the line "segments" that make up the arrowhead?

Brandon Chasco
University of Washington
ph (206) 221-6768


From ripley at stats.ox.ac.uk  Wed Aug 23 19:12:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Aug 2006 18:12:53 +0100 (BST)
Subject: [R] glm inside one self-defined function
In-Reply-To: <e668df8c0608230808r6ab963c1j3880c656a70755e5@mail.gmail.com>
References: <B998A44C8986644EA8029CFE6396A924840127@exqld2-bne.qld.csiro.au>
	<e668df8c0608230808r6ab963c1j3880c656a70755e5@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608231804520.3085@gannet.stats.ox.ac.uk>

Your example works for me without any error messages in R 2.3.1 with the 
current (uncredited) MASS package 7.2-27.1, including giving about 20 'ok 
here.'.

Did you heed the advice in the posting guide to update (as well as to tell 
us the versions of things you were using)?  Probably not (as you sent HTML 
mail)!

[One thing you are leaving out is

> library(MASS)

]


On Wed, 23 Aug 2006, Mike Wolfgang wrote:

> Thanks Bill and Simon. I wrote a simpler function to test and found out it
> was stepAIC which causes error, and I still don't know how to solve it.
> Check out this simple function:
> 
> 
> myfun<-function(k){
>   xx<-mvrnorm(100,rep(0,10),diag(1,10),empirical=TRUE)
>   colnames(xx)<-paste("x",1:10,sep='')
>   py<-exp(sum(xx))/(1+exp(sum(xx)))
>   for (i in 1:k){
>     y<-rbinom(100,1,py)
>     mydata<-data.frame(cbind(y,xx))
>     y.glm<-glm(y~.,binomial,mydata)
>     cat("ok here.\n")
>     y.step<-stepAIC(y.glm,direction='both',trace=0)
>     cat("ok here.\n")
>     print(summary(y.step))
>   }
> }
> 
> myfun(10)
> 
> only one "ok here" is printed.
> 
> Mike
> 
> On 8/22/06, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> >
> > Mike Wolfgang asks:
> >
> > >
> > > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Wolfgang
> > > Sent: Wednesday, 23 August 2006 1:31 PM
> > > To: R-help list
> > > Subject: [R] glm inside one self-defined function
> > >
> > > Hi list,
> > >
> > > I've searched in R-help and found some related discussions but still
> > could
> > > not understand this type of error. My own function is pretty complex,
> > so I
> > > would not put it here, but the basic algorithm is like this:
> > > myfun<-function(k){
> > >   mydata<-...#by someway I create a data frame
> > >   mymodel<-glm(y~.,family=binomial(),data=mydata)
> > >   ...#some other stuff
> > > }
> >
> > I think you are leaving out something.  Here is a test of what you
> > claim gives a problem (R 2.3.1, Windows):
> >
> >     > myfun <- function(n) {
> >     +   z <- rnorm(n)
> >     +   mydata <- data.frame(x = z,
> >     +     y = rbinom(n, size = 1, prob = exp(z)/(1+exp(z))))
> >     +   fm <- glm(y ~ x, binomial, mydata)
> >     +   fm
> >     + }
> >     >
> >     > myfun(100)
> >
> >     Call:  glm(formula = y ~ x, family = binomial, data = mydata)
> >
> >     Coefficients:
> >     (Intercept)            x
> >          0.1587       1.0223
> >
> >     Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
> >     Null Deviance:      137.6
> >     Residual Deviance: 118.3        AIC: 122.3
> >
> > Not even a murmur of complaint.  (This also works in S-PLUS 7.0 but
> > earlier versions of S-PLUS gave a problem rather like the one you note,
> > curiously.)
> >
> > Look again at your code and see if the abstract version you give
> > really matches what you did, may I suggest?
> >
> > >
> > > as I execute this function, it gives error like this
> > > Error in inherits(x, "data.frame") : object "mydata" not found
> > >
> > > So I guess glm here tries to find "mydata" in the parent environment.
> > Why
> > > doesn't it take "mydata" inside the function? How to let glm correctly
> > > locate it? Is this (scope/environment) mentioned in R manual? Thanks,
> > >
> > > Mike
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tlumley at u.washington.edu  Wed Aug 23 19:15:29 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 23 Aug 2006 10:15:29 -0700 (PDT)
Subject: [R] lean and mean lm/glm?
In-Reply-To: <20060823161415.CCD2CB5830@xprdmxin.myway.com>
References: <20060823161415.CCD2CB5830@xprdmxin.myway.com>
Message-ID: <Pine.LNX.4.64.0608231003190.9702@homer24.u.washington.edu>

On Wed, 23 Aug 2006, Damien Moore wrote:

>
> Thomas Lumley < tlumley at u.washington.edu > wrote:
>
>> I have written most of a bigglm() function where the data= argument is a
>> function with a single argument 'reset'. When called with reset=FALSE the
>> function should return another chunk of data, or NULL if no data are
>> available, and when called with reset=TRUE it should go back to the
>> beginning of the data. I don't think this is too inelegant.
>
> yes, that does sound like a pretty elegent solution. It would be even 
> more so if you could offer a default implementation of the data_function 
> that simply passes chunks of large X and y matrices held in memory.

I have done that for data frames.

> (ideally you would just intialize the data_function to reference the X 
> and y data to avoid duplicating it, don't know if that's possible in R.)

The part that is extracted is a copy. The whole thing isn't copied, 
though.

The chunk would have to be a copy if it were an R matrix because matrices 
are stored in continguous column-major format and a chunk won't be 
contiguous. I think an implementation that uses precomputed design 
matrices would want to be written in C and call the incremental QR 
decomposition routines row by row.  The reason for working in chunks in R 
is to allow model.frame and model.matrix to work reasonably efficiently, 
and they aren't needed if you already have the design matrix.

> how long before its ready? :)

Depends on how many more urgent things intervene.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ripley at stats.ox.ac.uk  Wed Aug 23 19:21:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Aug 2006 18:21:09 +0100 (BST)
Subject: [R] editing ".Internal" functions
In-Reply-To: <Pine.LNX.4.64.0608221516190.29128@homer21.u.washington.edu>
References: <Pine.LNX.4.64.0608221516190.29128@homer21.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0608231813420.3085@gannet.stats.ox.ac.uk>

On Wed, 23 Aug 2006, B. Chasco wrote:

> There is a function called arrows() which is an .Internal function.  How 
> difficult is it to modify that function to return the xy coordinates for 
> the line "segments" that make up the arrowhead?

That depends on your 'core competencies'.  You have the source, which is 
in file src/main/plot.c (and GArrow in src/main/graphics.c).  I am not 
sure I understand your question, as arrows() is plural and draws multiple 
arrows with 0/1/2 heads each.  Also, the 'xy coordinates' are to be in 
which coordinate system, and you do realize that the sizes in user 
coordinates change if you resize the plot?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From arnab at myrealbox.com  Wed Aug 23 19:28:28 2006
From: arnab at myrealbox.com (Arnab mukherji)
Date: Wed, 23 Aug 2006 17:28:28 +0000
Subject: [R] covariance matrix of predictions
Message-ID: <1156354108.c7f66dbcarnab@myrealbox.com>

Hi !

I am trying to get at the covariance of the predictions of a linear model. Suppose the we have:

> x<-runif(1000)
> y<-2 + 25x*x +rnorm(1000)
> lm1 <-lm(y~x, data = data.frame(y  = y, x=x))
> x.pred <-runif(10)
> y.hat <- predict(lm1, newdata = data.frame(x=x.pred))

I was wondering how to get an estimate of the covariance of y.hat which would be a 10 x 10 matrix telling be the uncertainty in each of the predictions.

thanks

Arnab


From ggrothendieck at gmail.com  Wed Aug 23 19:39:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 23 Aug 2006 13:39:22 -0400
Subject: [R] editing ".Internal" functions
In-Reply-To: <Pine.LNX.4.64.0608221516190.29128@homer21.u.washington.edu>
References: <Pine.LNX.4.64.0608221516190.29128@homer21.u.washington.edu>
Message-ID: <971536df0608231039l607b6036yf999b4ae9784c774@mail.gmail.com>

You could also look at p.arrows in sfsmisc package which is
entirely in R.

On 8/23/06, B. Chasco <bchasco at u.washington.edu> wrote:
> There is a function called arrows() which is an .Internal function.  How
> difficult is it to modify that function to return the xy coordinates for
> the line "segments" that make up the arrowhead?
>
> Brandon Chasco
> University of Washington
> ph (206) 221-6768
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mikewolfgang at gmail.com  Wed Aug 23 20:22:11 2006
From: mikewolfgang at gmail.com (Mike Wolfgang)
Date: Wed, 23 Aug 2006 14:22:11 -0400
Subject: [R] glm inside one self-defined function
In-Reply-To: <Pine.LNX.4.64.0608231804520.3085@gannet.stats.ox.ac.uk>
References: <B998A44C8986644EA8029CFE6396A924840127@exqld2-bne.qld.csiro.au>
	<e668df8c0608230808r6ab963c1j3880c656a70755e5@mail.gmail.com>
	<Pine.LNX.4.64.0608231804520.3085@gannet.stats.ox.ac.uk>
Message-ID: <e668df8c0608231122g557d010cg1c5d149c0caf3113@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/101e250f/attachment.pl 

From llei at bccrc.ca  Wed Aug 23 20:49:55 2006
From: llei at bccrc.ca (Linda Lei)
Date: Wed, 23 Aug 2006 11:49:55 -0700
Subject: [R] putting the mark for censored time on 1-KM curve or competing
	risk curve
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72C5C@crcmail1.BCCRC.CA>

Hi All,

 

I'm trying to figure out the cumulative incidence curve in R in some
limited time. I found in package "cmprsk", the command "plot.cuminc" can
get this curve. But I noticed that there is no mark for the censored
time there, comparing with the KM curve by "plot.survfit". Here are my
codes (attached is the data):

 

----------------

dat<-read.table("F://wendy/BMT data analysis/final
data.txt",header=TRUE,sep="\t")

 

library(cmprsk)

 

library(survival)

 

attach(dat)

 

par(mfrow=c(2,1))

 

curve<-cuminc(SURVEFP/365.25,STATREP,Ritux)

 

plot(curve,xlab="years",curvlab=c("No rituximab",
"Rituximab"),lty=1:2,col=c("red","blue"),mark.time=TRUE)

 

a<-survfit(Surv(SURVEFP/365.25,STATREP)~Ritux,type="kaplan-meier")

 

plot(a,main="OS for whole
group",conf.int=F,xlab="years",ylab="probability",lty=1:2,col=c("red","b
lue"))

-------------------

 

Could you help me with how to put the mark for censored time on
cumulative incidence curve, or maybe how to get the 1-KM curve with the
mark for censored time? This will help me a lot!

 

Thank you very much!

 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: final data.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060823/6e76f3b5/attachment.txt 

From ryanteal22 at yahoo.ca  Wed Aug 23 21:07:26 2006
From: ryanteal22 at yahoo.ca (hedger22)
Date: Wed, 23 Aug 2006 12:07:26 -0700 (PDT)
Subject: [R] Wavelet Output
Message-ID: <5950734.post@talk.nabble.com>


Hello all,

I am new to r as well as wavlets.  The following is a times series from a
bond portfolio I have uploaded from a text file.

Bond1 <- scan("C:.../UploadR2.txt") 

I am trying to decompose the portfolio return using wavelets.  My goal is to
find out the wavlet variance and average at each scale to then input into a
Sharpe Ratio calculation to see what investment horizons are important
contributors to the time series variance

I used the following:

Bond1.returns <- ts(Bond1)
Bond1.volatility <- abs(Bond1.returns)

## Haar
Bond1v.haar <- mra(Bond1.volatility, "haar", 4, "dwt")
names(Bond1v.haar) <- c("d1", "d2", "d3", "d4", "s4")
## LA(8)
Bond1v.la8 <- mra(Bond1.volatility, "la8", 4, "dwt")
names(Bond1v.la8) <- c("d1", "d2", "d3", "d4", "s4")

How do I interpret the output?  
-- 
View this message in context: http://www.nabble.com/Wavelet-Output-tf2154251.html#a5950734
Sent from the R help forum at Nabble.com.


From Gaspard.Lequeux.at at biomath.ugent.be  Wed Aug 23 22:13:40 2006
From: Gaspard.Lequeux.at at biomath.ugent.be (Gaspard Lequeux)
Date: Wed, 23 Aug 2006 22:13:40 +0200 (CEST)
Subject: [R] rgl package: color of the axes
Message-ID: <Pine.LNX.4.62.0608232201350.30078@biomath.ugent.be>


Hej,

When plotting triangles with rgl.triangles and setting the axes afterwards 
with decorate3d(aspect=TRUE), the axes get the color used for the last 
triangle plotted.

Example:

rgl.triangles(c(1,2,3),c(1,2,5),c(1,3,2),col="#55FF55")
decorate3d(aspect=TRUE)

Using

decorate3d(aspect=TRUE,col="#000000")

or

decorate3d(aspect=TRUE,color="#000000")

does not help, the axes still have the last color used for plotting an 
object.

In the help page of decorate3d on can find the following line:

        ...: additional parameters which will be passed to 'par3d',
             'material3d' or 'decorate3d'.

So the 'color' argument should have an effect.

The workaround is simple. Set the color with rgl.material before calling 
decorate3d:

rgl.material(color="#000000")

Maybe this should be done directly in the decorate3d function, as written 
in the help?

Version of rgl used: 0.67-2 (2006-07-11)
Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;

Anyway, rgl is an excellent package.

/Gaspard


From Gaspard.Lequeux.at at biomath.ugent.be  Wed Aug 23 23:15:27 2006
From: Gaspard.Lequeux.at at biomath.ugent.be (Gaspard Lequeux)
Date: Wed, 23 Aug 2006 23:15:27 +0200 (CEST)
Subject: [R] rgl: exporting to pdf or png does not work
Message-ID: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>


Hej,

When exporting a image from rgl, the following error is encountered:

> rgl.postscript('testing.pdf', fmt="pdf")
RGL: ERROR: can't bind glx context to window
RGL: ERROR: can't bind glx context to window
Warning messages:
1: X11 protocol error: GLXBadContextState
2: X11 protocol error: GLXBadContextState

The pdf file is created and is readable, but all the labels are gone.

Taking a snapshot (to png) gives 'failed' and no file is created.

Version of rgl used: 0.67-2 (2006-07-11)
Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
Running Debian GNU/Linux testing (Etch).

/Gaspard


From gunter.berton at gene.com  Thu Aug 24 00:17:20 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 23 Aug 2006 15:17:20 -0700
Subject: [R] rgl: exporting to pdf or png does not work
In-Reply-To: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>
Message-ID: <008e01c6c701$e7081880$711f210a@gne.windows.gene.com>

Please contact the package maintainer.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gaspard Lequeux
> Sent: Wednesday, August 23, 2006 2:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rgl: exporting to pdf or png does not work
> 
> 
> Hej,
> 
> When exporting a image from rgl, the following error is encountered:
> 
> > rgl.postscript('testing.pdf', fmt="pdf")
> RGL: ERROR: can't bind glx context to window
> RGL: ERROR: can't bind glx context to window
> Warning messages:
> 1: X11 protocol error: GLXBadContextState
> 2: X11 protocol error: GLXBadContextState
> 
> The pdf file is created and is readable, but all the labels are gone.
> 
> Taking a snapshot (to png) gives 'failed' and no file is created.
> 
> Version of rgl used: 0.67-2 (2006-07-11)
> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
> Running Debian GNU/Linux testing (Etch).
> 
> /Gaspard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrosenba at rand.org  Thu Aug 24 00:41:16 2006
From: jrosenba at rand.org (Janet Rosenbaum)
Date: Wed, 23 Aug 2006 15:41:16 -0700
Subject: [R] polychor error
Message-ID: <44ECD98C.70103@rand.org>

Hi.

Does anyone know whether the following error is a result of a bug or a 
feature?

I can eliminate the error by making ML=F, but I would like to see the 
values of the cut-points and their variance.  Is there anything that I 
can do?


tmp.vec<-c(0,  0,  0 , 0  ,0 , 1,  0,  2,  0 , 0,  5  ,5  ,3  ,1,  0 , 
1,  5, 10, 27, 20,  9,  0,  1,  1, 12, 29, 57, 34,  0,  0,  1,  2, 11, 
31, 32)
tmp.mat<-matrix(tmp.vec, nrow=7)
rownames(tmp.mat)<-1:7
colnames(tmp.mat)<-3:7
tmp.pcc<-polychor(tmp.mat, ML=T, std.err=T)
Error in checkmvArgs(lower = lower, upper = upper, mean = mean, corr = 
corr,  :
     at least one element of ?lower? is larger than ?upper?

Thanks,

Janet


--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From murdoch at stats.uwo.ca  Thu Aug 24 00:50:31 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 23 Aug 2006 18:50:31 -0400
Subject: [R] rgl package: color of the axes
In-Reply-To: <Pine.LNX.4.62.0608232201350.30078@biomath.ugent.be>
References: <Pine.LNX.4.62.0608232201350.30078@biomath.ugent.be>
Message-ID: <44ECDBB7.2050602@stats.uwo.ca>

On 8/23/2006 4:13 PM, Gaspard Lequeux wrote:
> Hej,
> 
> When plotting triangles with rgl.triangles and setting the axes afterwards 
> with decorate3d(aspect=TRUE), the axes get the color used for the last 
> triangle plotted.
> 
> Example:
> 
> rgl.triangles(c(1,2,3),c(1,2,5),c(1,3,2),col="#55FF55")
> decorate3d(aspect=TRUE)
> 
> Using
> 
> decorate3d(aspect=TRUE,col="#000000")
> 
> or
> 
> decorate3d(aspect=TRUE,color="#000000")
> 
> does not help, the axes still have the last color used for plotting an 
> object.

You should use triangles3d if you don't want the material changes to 
carry over to the next call.  Generally speaking it's hard to get things 
right when you mix the rgl.* calls (which assume changes are persistent) 
with the *3d calls (which assume they're not).
> 
> In the help page of decorate3d on can find the following line:
> 
>         ...: additional parameters which will be passed to 'par3d',
>              'material3d' or 'decorate3d'.
> 
> So the 'color' argument should have an effect.

Yes, that's a bug, which has been fixed (but not released yet).  There 
should be a release pretty soon now.  Daniel Adler and I are hoping to 
have a version 1.0 in the next couple of months; I expect one or two 
more 0.x releases before that.

> 
> The workaround is simple. Set the color with rgl.material before calling 
> decorate3d:
> 
> rgl.material(color="#000000")
> 
> Maybe this should be done directly in the decorate3d function, as written 
> in the help?
> 
> Version of rgl used: 0.67-2 (2006-07-11)
> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
> 
> Anyway, rgl is an excellent package.

Thanks.  It's still getting better.

Duncan Murdoch

> 
> /Gaspard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Thu Aug 24 00:52:23 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 23 Aug 2006 18:52:23 -0400
Subject: [R] rgl: exporting to pdf or png does not work
In-Reply-To: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>
References: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>
Message-ID: <44ECDC27.6080109@stats.uwo.ca>

On 8/23/2006 5:15 PM, Gaspard Lequeux wrote:
> Hej,
> 
> When exporting a image from rgl, the following error is encountered:
> 
>> rgl.postscript('testing.pdf', fmt="pdf")
> RGL: ERROR: can't bind glx context to window
> RGL: ERROR: can't bind glx context to window
> Warning messages:
> 1: X11 protocol error: GLXBadContextState
> 2: X11 protocol error: GLXBadContextState
> 
> The pdf file is created and is readable, but all the labels are gone.
> 
> Taking a snapshot (to png) gives 'failed' and no file is created.
> 
> Version of rgl used: 0.67-2 (2006-07-11)
> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
> Running Debian GNU/Linux testing (Etch).

That looks like an X11 error to me, not something that I'm very likely 
to be able to fix.  If you can debug the error, it would be helpful.

Duncan Murdoch


From adrian at maths.uwa.edu.au  Thu Aug 24 02:50:11 2006
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 24 Aug 2006 08:50:11 +0800
Subject: [R] [R-pkgs] spatstat 1.9-5
Message-ID: <17644.63427.629643.303941@maths.uwa.edu.au>


A new version 1.9-5 of the package `spatstat' 
has been uploaded to CRAN.

What it is:
----------
spatstat is a package for analysing spatial data, mainly Spatial Point
Patterns. 

What's in it:
------------
Functions for exploratory data analysis, model-fitting, simulation,
spatial sampling, model diagnostics, and formal inference. Data types
include point patterns, line segment patterns, spatial windows, and
pixel images. Point patterns may include marks (e.g. multitype
points).  Spatial windows can have irregular shape (polygons with
holes, binary images).  Point process models can be fitted to point
pattern data. Cluster models are fitted by the method of minimum
contrast. Very general Gibbs point process models can be fitted to
point pattern data using a function ppm similar to lm or glm. Models
may include dependence on covariates, interpoint interaction and
dependence on marks. Fitted models can be simulated
automatically. Also provides facilities for formal inference (such as
chi-squared tests) and model diagnostics (including simulation
envelopes, residuals, residual plots and Q-Q plots).

Spatstat is one of the largest packages available on CRAN,
with over 270 functions. 

For more information:
--------------------
	See the Release Notes for version 1.9-5 at
		<www.spatstat.org>



Adrian Baddeley and Rolf Turner

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ajkoch at postoffice.utas.edu.au  Thu Aug 24 04:00:52 2006
From: ajkoch at postoffice.utas.edu.au (Amy Koch)
Date: Thu, 24 Aug 2006 12:00:52 +1000
Subject: [R] Classification tree with a random variable
Message-ID: <009801c6c721$21673d20$7128d983@geol.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/c8770c5d/attachment.pl 

From Manuel.A.Morales at williams.edu  Thu Aug 24 04:38:38 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Wed, 23 Aug 2006 22:38:38 -0400
Subject: [R] ess-remote question
Message-ID: <1156387118.23476.1.camel@solidago.localdomain>

Dear list,

Is there any way to load a local data file when connected to a remote
machine via ESS?

Thanks!

Manuel


From e.leoni at gmail.com  Thu Aug 24 04:55:51 2006
From: e.leoni at gmail.com (Eduardo Leoni)
Date: Wed, 23 Aug 2006 22:55:51 -0400
Subject: [R] "fixed effects" transformation
Message-ID: <e474b3790608231955k516411a4t64b8c773e187d26c@mail.gmail.com>

Hi -

I am doing an analysis using panel data methods, particularly what
economists call "fixed effects". It can easily be done in R through
the inclusion of factors in an lm formula. However, when the number of
groups is excessive (in my case 2000+) it is much more efficient to
demean the data by panel.

I created this function following Farnsworth
(http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)


demean <- function(x,index) {
  for (i in unique(index)) {
    for (j in 1:ncol(x)) {
      x.now <- x[index==i,j]
      x[index==i,j] <- x.now-mean(x.now,na.rm=TRUE)
    }
  }
  x
}

it is obvious that there must be a much much more efficient way to do
this, though. Any recommendations?

thanks,

-eduardo


From jzhang1982 at gmail.com  Thu Aug 24 04:58:40 2006
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Thu, 24 Aug 2006 10:58:40 +0800
Subject: [R] how to replace one row with other row?
Message-ID: <3f2938d50608231958q5902c6b2m11170853047b7190@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/46747293/attachment.pl 

From jeff.hamann at forestinformatics.com  Thu Aug 24 05:01:48 2006
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Wed, 23 Aug 2006 20:01:48 -0700 (PDT)
Subject: [R] help with pasting + expressions?
Message-ID: <1865.128.193.139.146.1156388508.squirrel@www.forestinformatics.com>

I can't believe I'm having such a hard time with this and I haven't been
able to find out how to solve this...

lab <- expression( paste( hat(v),
    as.character(round(y.hat,2)), ",",
    hat(sigma)^2, as.character(sigma.hat)) )
text( x=pt$x+2, y=pt$y,labels=lab )

## the text should be \hat{y} = <value of y.hat>, \hat{\sigma}^2 == <value
of sigma.hat>

and R keeps displaying the actual text of the non-expressions...

I must be a chowderhead, but I need a little guidance, please...

Thanks,
Jeff.


-- 
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421


From ggrothendieck at gmail.com  Thu Aug 24 05:45:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 23 Aug 2006 23:45:55 -0400
Subject: [R] help with pasting + expressions?
In-Reply-To: <1865.128.193.139.146.1156388508.squirrel@www.forestinformatics.com>
References: <1865.128.193.139.146.1156388508.squirrel@www.forestinformatics.com>
Message-ID: <971536df0608232045u34300a4ar40249effa9d27c08@mail.gmail.com>

Try bquote:

y.hat <- sigma.hat <- 1.1
plot(1)
lab <- bquote(hat(y) == .(y.hat) * "," ~ hat(sigma)^2 == .(sigma.hat))
text(1, 1, lab, pos = 4)


On 8/23/06, Jeff D. Hamann <jeff.hamann at forestinformatics.com> wrote:
> I can't believe I'm having such a hard time with this and I haven't been
> able to find out how to solve this...
>
> lab <- expression( paste( hat(v),
>    as.character(round(y.hat,2)), ",",
>    hat(sigma)^2, as.character(sigma.hat)) )
> text( x=pt$x+2, y=pt$y,labels=lab )
>
> ## the text should be \hat{y} = <value of y.hat>, \hat{\sigma}^2 == <value
> of sigma.hat>
>
> and R keeps displaying the actual text of the non-expressions...
>
> I must be a chowderhead, but I need a little guidance, please...
>
> Thanks,
> Jeff.
>
>
> --
> Forest Informatics, Inc.
> PO Box 1421
> Corvallis, Oregon 97339-1421
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Charles.Annis at StatisticalEngineering.com  Thu Aug 24 05:48:23 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 23 Aug 2006 23:48:23 -0400
Subject: [R] help with pasting + expressions?
In-Reply-To: <1865.128.193.139.146.1156388508.squirrel@www.forestinformatics.com>
Message-ID: <091c01c6c730$260139d0$6600a8c0@DD4XFW31>

Please visit the R site http://www.r-project.org/ and search the mailing
list for "paste expression"  We discussed the topic recently.  The "trick"
is that you don't paste expressions, you make an expression containing
paste.



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeff D. Hamann
Sent: Wednesday, August 23, 2006 11:02 PM
To: r-help at stat.math.ethz.ch
Subject: [R] help with pasting + expressions?

I can't believe I'm having such a hard time with this and I haven't been
able to find out how to solve this...

lab <- expression( paste( hat(v),
    as.character(round(y.hat,2)), ",",
    hat(sigma)^2, as.character(sigma.hat)) )
text( x=pt$x+2, y=pt$y,labels=lab )

## the text should be \hat{y} = <value of y.hat>, \hat{\sigma}^2 == <value
of sigma.hat>

and R keeps displaying the actual text of the non-expressions...

I must be a chowderhead, but I need a little guidance, please...

Thanks,
Jeff.


-- 
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rmh at temple.edu  Thu Aug 24 08:16:00 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 24 Aug 2006 02:16:00 -0400 (EDT)
Subject: [R] how to run ANCOVA?
Message-ID: <20060824021600.BGO44120@po-d.temple.edu>

> But how is that different from just a 3-way ANOVA with age, diagnosis,
> and gender as the the three effects?  Isn't ANCOVA a fundamentally
> different model?

> Thanks,
> Sasha

ANCOVA is a linear model with both factors and continuous variables
on the right-hand side of the model formula.  In pre-computer texts,
the commonly used algorithm hid that fact.

Rich


From Markus.Schweitzer at hilti.com  Thu Aug 24 08:27:51 2006
From: Markus.Schweitzer at hilti.com (Schweitzer, Markus)
Date: Thu, 24 Aug 2006 08:27:51 +0200
Subject: [R] Search for best ARIMA model
Message-ID: <7260411F9C32E74A86AD9567E64C3301D2FBD0@LI-HAWK.hag.hilti.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/67014a37/attachment.pl 

From backer at psych.uib.no  Thu Aug 24 08:37:40 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Thu, 24 Aug 2006 08:37:40 +0200
Subject: [R] Authoring a book
In-Reply-To: <44EB4666.6020002@comcast.net>
References: <44EB32A5.7030508@psych.uib.no> <44EB4666.6020002@comcast.net>
Message-ID: <44ED4934.7080707@psych.uib.no>

Jack B. Arnold wrote:
> Dear Tom,
> 
> Looking forward to your book.  "Psychologists and students" clearly need 
> all the encouragement to use R that they can get.  I have been using it 
> for a couple of years now, and find, that for most purposes, it is just 
> a little harder to get into than the expensive commercial packages.
> 
> That said, as an old timer, I can't pass up the opportunity to 
> discourage younger colleagues from using constructions like "Me and some 
> colleagues ..."  "Some colleagues and I" is a lot more polite and it is 
> also grammatically correct.

Thank you.  English is my second (or third, perhaps fourth) language 
and my mastering of it is (naturally) less than perfect.

Tom

> 
> The best and good luck.
> 
> Jack
> 
> Jack B. Arnold
> Professor of Psychology, Retired
> Saint Mary's College of California
> 
> Tom Backer Johnsen wrote:
>> Me and some colleagues are planning to write a textbook together
>> ("Statistics using R") where the target audience for the book is
>> psychologists and students of psychology.
>>
>> We thought that it might be a good idea to use a Wiki when writing the
>> text.  Is that a good idea?  Does anybody have any experience in that
>> direction?  What alternatives are there?
>>
>> The tool (Wiki) would have to be able to handle tables and
>> mathematical formulas in some manner, and of course, some mechanism to
>> export the contents to a word processor in the final stages.
>>
>> I have my own server, Windows, based on Apache, PhP, and MySQL.
>>
>> Tom
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


-- 
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From ronggui.huang at gmail.com  Thu Aug 24 09:33:43 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 24 Aug 2006 15:33:43 +0800
Subject: [R] "fixed effects" transformation
In-Reply-To: <e474b3790608231955k516411a4t64b8c773e187d26c@mail.gmail.com>
References: <e474b3790608231955k516411a4t64b8c773e187d26c@mail.gmail.com>
Message-ID: <38b9f0350608240033n49ad6cbdj7f6bd94fd4ad57a1@mail.gmail.com>

plm function in plm package are for panel data model.
> library(plm)
> ?plm


2006/8/24, Eduardo Leoni <e.leoni at gmail.com>:
> Hi -
>
> I am doing an analysis using panel data methods, particularly what
> economists call "fixed effects". It can easily be done in R through
> the inclusion of factors in an lm formula. However, when the number of
> groups is excessive (in my case 2000+) it is much more efficient to
> demean the data by panel.
>
> I created this function following Farnsworth
> (http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)
>
>
> demean <- function(x,index) {
>   for (i in unique(index)) {
>     for (j in 1:ncol(x)) {
>       x.now <- x[index==i,j]
>       x[index==i,j] <- x.now-mean(x.now,na.rm=TRUE)
>     }
>   }
>   x
> }
>
> it is obvious that there must be a much much more efficient way to do
> this, though. Any recommendations?
>
> thanks,
>
> -eduardo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
??????
Department of Sociology
Fudan University


From maechler at stat.math.ethz.ch  Thu Aug 24 10:17:51 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 24 Aug 2006 10:17:51 +0200
Subject: [R] error message from lm.ridge() in MASS ***package***
In-Reply-To: <Pine.GSO.4.58.0608221705440.21737@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0608211732090.16192@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0608221705440.21737@godzilla.acpub.duke.edu>
Message-ID: <17645.24751.765002.423348@stat.math.ethz.ch>

>>>>> "jz7" == jz7  <jz7 at duke.edu>
>>>>>     on Tue, 22 Aug 2006 17:10:42 -0400 (EDT) writes:

    jz7> Dear all,
    jz7> I got a wierd problem when using lm.ridge() in MASS library. 

there is "MASS the book" and "MASS the package",
and there is even a MASS library (namely the file MASS.so or
MASS.dll depending on your platform) but you are really talking
about the MASS *package* !

    jz7> When my X matrix has few columns, there is no
    jz7> problem. But when my X matrix gets larger (over 1000
    jz7> columns), I got the following error:

and where is the "selfcontained reproducible code" which we ask
you for, explicitly in the posting guide and at the footer of
every R-help message ???



    jz7> Error in Xs$v %*% a : non-conformable arguments
    jz7> In addition: Warning messages:
    jz7> 1: longer object length
    jz7> is not a multiple of shorter object length in: d^2 + rep(lambda,
    jz7> rep(p, k))
    jz7> 2: longer object length
    jz7> is not a multiple of shorter object length in: drop(d * rhs)/div

    jz7> The R code I use for the calculation is "lm.ridge( y ~ x,lambda=seq(1,15,1))".

    jz7> Please advice.

    jz7> Thanks a lot!
    jz7> Jeny


From arun.kumar.saha at gmail.com  Thu Aug 24 11:21:28 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 24 Aug 2006 14:51:28 +0530
Subject: [R] Waring message in mvBEKK.est
Message-ID: <d4c57560608240221x1390a72fy161a5f0f373bc87f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/440377d5/attachment.pl 

From Bernhard_Pfaff at fra.invesco.com  Thu Aug 24 11:31:35 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Thu, 24 Aug 2006 10:31:35 +0100
Subject: [R] Omegahat-site down?
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BCFA@DEFRAXMB01.corp.amvescap.net>

Dear R-list subscriber,

is it possible that the omegahat-site is down? I was looking for package
'RDCOMClient', but could not establish a connection. In case somebody
has the latest binary zip-file for Windows, would she/he mind to send it
directly to my emaim adress stated in the signature?

Many thanks, and sorry for bothering/misusing R-help in this instance.

Best,
Bernhard 

Dr. Bernhard Pfaff
Global Structured Products Group
(Europe)

Invesco Asset Management Deutschland GmbH
Bleichstrasse 60-62
D-60313 Frankfurt am Main

Tel: +49(0)69 29807 230
Fax: +49(0)69 29807 178
Email: bernhard_pfaff at fra.invesco.com 
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From stgries_lists at arcor.de  Thu Aug 24 11:34:00 2006
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Thu, 24 Aug 2006 11:34:00 +0200 (CEST)
Subject: [R] Lost command area in R-SciViews
Message-ID: <10124440.1156412040426.JavaMail.ngmail@webmail18>

Dear all

I am writing with a question regarding SciViews for R. It's probably a slightly stupid question but I cannot find a solution to a very elementary problem. I am using SciViews 0.8.9 on with R 2.3.1pat on a Windows XP Home machine. R is set to SDI mode, I start R, enter "library(svGUI)", SciViews starts properly, I can access the docks and everything, but

(i) the command area at the bottom cannot be found
(ii) the regular R window cannot be minimized/maximized anymore.

I don't know what to do to get the command area back. I have checked the web and the mailing list (with the search words "sciviews" and "command") but all I could come up with (<http://tolstoy.newcastle.edu.au/R/help/05/12/16841.html>) is the recommendation to click on Misc: Toolbars: Command. But I did that and it's still not visible. Am I making some kind of stupid mistake? I have uploaded a screenshot to my website at <http://www.linguistics.ucsb.edu/faculty/stgries/other/sciviews.png> to show you what's happening. I have even un- and reinstalled R and SciViews but to no avail. Any ideas would be greatly appreciated. I am currently teaching a course on R and would like to show the participants how to work with R, Tinn-R, and SciViews, but with the present problem, this is not going to work; I wrote to Philippe Grosjean but have not received a reply.

Thanks a lot,
STG
-- 
Stefan Th. Gries
-----------------------------------------------
University of California, Santa Barbara
http://www.linguistics.ucsb.edu/faculty/stgries


From p.dalgaard at biostat.ku.dk  Thu Aug 24 11:55:53 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Aug 2006 11:55:53 +0200
Subject: [R] Omegahat-site down?
In-Reply-To: <E4A9111DA23BA048B9A46686BF727CF461BCFA@DEFRAXMB01.corp.amvescap.net>
References: <E4A9111DA23BA048B9A46686BF727CF461BCFA@DEFRAXMB01.corp.amvescap.net>
Message-ID: <x2y7tev4nq.fsf@viggo.kubism.ku.dk>

"Pfaff, Bernhard Dr." <Bernhard_Pfaff at fra.invesco.com> writes:

> Dear R-list subscriber,
> 
> is it possible that the omegahat-site is down? I was looking for package
> 'RDCOMClient', but could not establish a connection. In case somebody
> has the latest binary zip-file for Windows, would she/he mind to send it
> directly to my emaim adress stated in the signature?
> 
> Many thanks, and sorry for bothering/misusing R-help in this instance.
> 
> Best,
> Bernhard 

It's a UC Davis machine (eeyore.ucdavis.edu), so I suppose Duncan
Temple Lang should know.

Incidentally, www.omegahat.com (not .org) turns out to be a strange
site with links to Omega watches and various kinds of hats (including
Fedoras!), but also statistics and data mining...


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From msubianto at gmail.com  Thu Aug 24 12:03:31 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Thu, 24 Aug 2006 12:03:31 +0200
Subject: [R] How to compare rows of two matrices
Message-ID: <c7c17cef0608240303j6f2633d2v45403f825851b8dc@mail.gmail.com>

Dear all,
I have a dataset
train <- cbind(c(0,2,2,1,0), c(8,9,4,0,2), 6:10, c(-1, 1, 1, -1, 1))
test <- cbind(1:5, c(0,1,5,1,3), c(1,1,2,0,3) ,c(1, 1, -1, 1, 1))

I want to find which rows of train and test it different in its last
column (column 4).
The solution must be something like

train
     [,1] [,2] [,3] [,4]
[1,]    0    8    6   -1
[3,]    2    4    8    1
[4,]    1    0    9   -1


test
     [,1] [,2] [,3] [,4]
[1,]    1    0    1    1
[3,]    3    5    2   -1
[4,]    4    1    0    1

I have tried with
matrix(train %in% test, dim(train))
apply(train, 1, paste, collapse="") %in% apply(test, 1, paste, collapse="")

It doesn't work.
How can I do.
Thanks for any help.

Best, Muhammad Subianto


From supton at referentia.com  Thu Aug 24 12:26:31 2006
From: supton at referentia.com (Stephen C. Upton)
Date: Thu, 24 Aug 2006 06:26:31 -0400
Subject: [R] How to compare rows of two matrices
In-Reply-To: <c7c17cef0608240303j6f2633d2v45403f825851b8dc@mail.gmail.com>
References: <c7c17cef0608240303j6f2633d2v45403f825851b8dc@mail.gmail.com>
Message-ID: <44ED7ED7.5060306@referentia.com>

Does this work for you?
dd <- mapply("==",train,test)
 > dim(dd) <- dim(train)
 > dd
      [,1]  [,2]  [,3]  [,4]
[1,] FALSE FALSE FALSE FALSE
[2,]  TRUE FALSE FALSE  TRUE
[3,] FALSE FALSE FALSE FALSE
[4,] FALSE FALSE FALSE FALSE
[5,] FALSE FALSE FALSE  TRUE

HTH
steve


Muhammad Subianto wrote:
> Dear all,
> I have a dataset
> train <- cbind(c(0,2,2,1,0), c(8,9,4,0,2), 6:10, c(-1, 1, 1, -1, 1))
> test <- cbind(1:5, c(0,1,5,1,3), c(1,1,2,0,3) ,c(1, 1, -1, 1, 1))
>
> I want to find which rows of train and test it different in its last
> column (column 4).
> The solution must be something like
>
> train
>      [,1] [,2] [,3] [,4]
> [1,]    0    8    6   -1
> [3,]    2    4    8    1
> [4,]    1    0    9   -1
>
>
> test
>      [,1] [,2] [,3] [,4]
> [1,]    1    0    1    1
> [3,]    3    5    2   -1
> [4,]    4    1    0    1
>
> I have tried with
> matrix(train %in% test, dim(train))
> apply(train, 1, paste, collapse="") %in% apply(test, 1, paste, collapse="")
>
> It doesn't work.
> How can I do.
> Thanks for any help.
>
> Best, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>


From i.m.s.white at ed.ac.uk  Thu Aug 24 12:33:39 2006
From: i.m.s.white at ed.ac.uk (i.m.s.white)
Date: Thu, 24 Aug 2006 11:33:39 +0100
Subject: [R] syntax for pdDiag (nlme)
Message-ID: <20060824103338.GA9695@trotter.cap.ed.ac.uk>

At the top of page 283 of Pinheiro and Bates, a covariance structure for
the indomethicin example is specified as

random = pdDiag(A1 + lrc1 + A2 + lrc2 ~ 1)

The argument to pdDiag() looks like a two-sided formula, and I'm struggling
to reconcile this with the syntax described in Ch4 of the book and online.
Further down page 283 the formula is translated into

list(A1 ~ 1, lrc1 ~ 1, A2 ~ 1, lrc2 ~ 1)

which I find just as puzzling. Can anyone help?

-- 
************************************************
*    I.White                                   *
*    University of Edinburgh                   *
*    Ashworth Laboratories, West Mains Road    *
*    Edinburgh EH9 3JT                         *
*    Fax: 0131 650 6564   Tel: 0131 650 5490   *
*    E-mail: i.m.s.white at ed.ac.uk              *


From petr.pikal at precheza.cz  Thu Aug 24 13:08:35 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Aug 2006 13:08:35 +0200
Subject: [R] How to compare rows of two matrices
In-Reply-To: <c7c17cef0608240303j6f2633d2v45403f825851b8dc@mail.gmail.com>
Message-ID: <44EDA4D3.5076.3A799A@localhost>

Hi

maybe simple math can do it.

different
(train[,4]-test[,4])!=0

same
(train[,4]-test[,4])==0

if you are sure the numbers are integers

HTH
Petr



On 24 Aug 2006 at 12:03, Muhammad Subianto wrote:

Date sent:      	Thu, 24 Aug 2006 12:03:31 +0200
From:           	"Muhammad Subianto" <msubianto at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] How to compare rows of two matrices

> Dear all,
> I have a dataset
> train <- cbind(c(0,2,2,1,0), c(8,9,4,0,2), 6:10, c(-1, 1, 1, -1, 1))
> test <- cbind(1:5, c(0,1,5,1,3), c(1,1,2,0,3) ,c(1, 1, -1, 1, 1))
> 
> I want to find which rows of train and test it different in its last
> column (column 4). The solution must be something like
> 
> train
>      [,1] [,2] [,3] [,4]
> [1,]    0    8    6   -1
> [3,]    2    4    8    1
> [4,]    1    0    9   -1
> 
> 
> test
>      [,1] [,2] [,3] [,4]
> [1,]    1    0    1    1
> [3,]    3    5    2   -1
> [4,]    4    1    0    1
> 
> I have tried with
> matrix(train %in% test, dim(train))
> apply(train, 1, paste, collapse="") %in% apply(test, 1, paste,
> collapse="")
> 
> It doesn't work.
> How can I do.
> Thanks for any help.
> 
> Best, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Thu Aug 24 13:41:07 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Aug 2006 13:41:07 +0200
Subject: [R] my error with augPred
Message-ID: <44EDAC73.16458.584068@localhost>

Dear all

I try to refine my nlme models and with partial success. The model is 
refined and fitted (using Pinheiro/Bates book as a tutorial) but when 
I try to plot

plot(augPred(fit4))

I obtain
Error in predict.nlme(object, value[1:(nrow(value)/nL), , drop = 
FALSE],  : 
        Levels (0,3.5],(3.5,5],(5,7],(7,Inf] not allowed for 
vykon.fac
>

Is it due to the fact that I have unbalanced design with not all 
levels of vykon.fac present in all levels of other explanatory factor 
variable?

I try to repeat 8.19 fig which is OK until I try:

fit4 <- update(fit2, fixed = list(A+B~1,xmid~vykon.fac, scal~1),  
start = c(57, 100, 700, rep(0,3), 13))

I know I should provide an example but maybe somebody will be clever 
enough to point me to an explanation without it.

nlme version 3.1-75
SSfpl model
R 2.4.0dev (but is the same in 2.3.1), W2000.

Thank you
Best regards.

Petr PikalPetr Pikal
petr.pikal at precheza.cz


From raphael.fraser at gmail.com  Thu Aug 24 14:09:03 2006
From: raphael.fraser at gmail.com (Raphael Fraser)
Date: Thu, 24 Aug 2006 07:09:03 -0500
Subject: [R] Intro to Programming R Book
Message-ID: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>

I am new to R and am looking for a book that can help in learning to
program in R. I have looked at the R website suggested books but I am
still not sure which book best suite my needs. I am interesting in
programming, data manipulation not statistics. Any suggestions?

Raphael


From backer at psych.uib.no  Thu Aug 24 14:08:48 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Thu, 24 Aug 2006 14:08:48 +0200
Subject: [R] Authoring a book
In-Reply-To: <Pine.GSO.4.63.0608240713430.15642@mango.cc.columbia.edu>
References: <44EB32A5.7030508@psych.uib.no> <44EB4666.6020002@comcast.net>
	<44ED4934.7080707@psych.uib.no>
	<Pine.GSO.4.63.0608240713430.15642@mango.cc.columbia.edu>
Message-ID: <44ED96D0.2050808@psych.uib.no>

Mark Orr wrote:
> Tom, i'm a psychologist with much interest in training future 
> psychologists (and others) to use R/S+.  So, if you need anyone to 
> review or give feedback on draft versions of your work, I'd be happy to 
> review.

Thank you!  That is a very generous offer.  The project is so far very 
much in its infancy, but I may accept the offer when we have something 
to review.

Tom

+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From mmalten at gmail.com  Thu Aug 24 14:23:44 2006
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Thu, 24 Aug 2006 08:23:44 -0400
Subject: [R] Intro to Programming R Book
In-Reply-To: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
References: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
Message-ID: <8913fde30608240523n6b33739cs6c98e0ed7cdb345d@mail.gmail.com>

I recently invested in two books: Venables and Ripley "Modern Applied
Statistics in S", and Everitt and Rabe Heskith's "Analyzing Medical
Data in S-Plus"

I think either one is a good self-teaching tool.

On 8/24/06, Raphael Fraser <raphael.fraser at gmail.com> wrote:
> I am new to R and am looking for a book that can help in learning to
> program in R. I have looked at the R website suggested books but I am
> still not sure which book best suite my needs. I am interesting in
> programming, data manipulation not statistics. Any suggestions?
>
> Raphael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
I can answer any question.
"I don't know" is an answer.
"I don't know yet" is a better answer.


From jrkrideau at yahoo.ca  Thu Aug 24 14:24:12 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 24 Aug 2006 08:24:12 -0400 (EDT)
Subject: [R] Intro to Programming R Book
In-Reply-To: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
Message-ID: <20060824122412.62787.qmail@web32809.mail.mud.yahoo.com>


--- Raphael Fraser <raphael.fraser at gmail.com> wrote:

> I am new to R and am looking for a book that can
> help in learning to
> program in R. I have looked at the R website
> suggested books but I am
> still not sure which book best suite my needs. I am
> interesting in
> programming, data manipulation not statistics. Any
> suggestions?
> 
> Raphael

I have not been able to get my hands on much since
someone has raided the local library and grabbed all
the R  books on long-term loan but I have found that
there is some very useful material on the CRAN site
under "Other" 

I have found An Introduction to S and the Hmisc and
Design Libraries? by Carlos Alzola and Frank E.
Harrell very useful as is Simple R? by John Verzani. 
There are also some other intro tutorials on line that
might be helpful. One I found useful is
http://www.math.ilstu.edu/dhkim/Rstuff/Rtutor.html

I just recently got my hands on John Fox's book "An R
and S-Plus Companion to Applied Regression" and it has
some very useful discussions of data handling. Ch.2
and Ch.7 (on graphs) is useful.


From Gaspard.Lequeux.at at biomath.ugent.be  Thu Aug 24 14:25:36 2006
From: Gaspard.Lequeux.at at biomath.ugent.be (Gaspard Lequeux)
Date: Thu, 24 Aug 2006 14:25:36 +0200 (CEST)
Subject: [R] rgl package: color of the axes
In-Reply-To: <44ECDBB7.2050602@stats.uwo.ca>
References: <Pine.LNX.4.62.0608232201350.30078@biomath.ugent.be>
	<44ECDBB7.2050602@stats.uwo.ca>
Message-ID: <Pine.LNX.4.62.0608241422340.20146@biomath.ugent.be>


Hej,

On Wed, 23 Aug 2006, Duncan Murdoch wrote:

> On 8/23/2006 4:13 PM, Gaspard Lequeux wrote:
>
>> When plotting triangles with rgl.triangles and setting the axes afterwards
>> with decorate3d(aspect=TRUE), the axes get the color used for the last
>> triangle plotted.
>>
>> Example:
>>
>> rgl.triangles(c(1,2,3),c(1,2,5),c(1,3,2),col="#55FF55")
>> decorate3d(aspect=TRUE)
>>
>> Using
>>
>> decorate3d(aspect=TRUE,col="#000000")
>>
>> or
>>
>> decorate3d(aspect=TRUE,color="#000000")
>>
>> does not help, the axes still have the last color used for plotting an
>> object.
>
> You should use triangles3d if you don't want the material changes to
> carry over to the next call.  Generally speaking it's hard to get things
> right when you mix the rgl.* calls (which assume changes are persistent)
> with the *3d calls (which assume they're not).

This worked indeed. Thanks for the hint.

>> In the help page of decorate3d on can find the following line:
>>
>>         ...: additional parameters which will be passed to 'par3d',
>>              'material3d' or 'decorate3d'.
>>
>> So the 'color' argument should have an effect.
>
> Yes, that's a bug, which has been fixed (but not released yet).  There
> should be a release pretty soon now.  Daniel Adler and I are hoping to
> have a version 1.0 in the next couple of months; I expect one or two
> more 0.x releases before that.

Very nice!

/Gaspard


From katrin.heinrichs at zmaw.de  Thu Aug 24 14:38:28 2006
From: katrin.heinrichs at zmaw.de (Anne Katrin Heinrichs)
Date: Thu, 24 Aug 2006 14:38:28 +0200
Subject: [R] metaplot and meta.summaries
In-Reply-To: <mailman.7.1156068003.6637.r-help@stat.math.ethz.ch>
Message-ID: <000401c6c77a$34b2abe0$20d2ac88@fnu.zmaw.de>

Hello,

After looking through the archive and documentation for quite some time, I'd be very happy
about some help with metaplot and meta.summaries.

metaplot:
---------

Can I change the label size? I've got 126 values and the intersection of the labels makes
it impossible to read them.

Why do I have to give sumse (Standard Error) and sumnn (Precision) of the summary
estimate? I can calculate one from the other, right? Just to make sure I'm not
misunderstanding something.

metaplot(CoeffVector, StdErrorVector, nn=NULL, labels=Name, conf.level=0.95, 
xlab=paste(CoeffNames[j], CoefficientName[i]), 
ylab="Countries",
xlim=NULL, summn=PostCoeffs[1,j], sumse=sqrt(PostVars[1,j]), 
sumnn=1/(PostVars[1,j]), 
summlabel="Summary", 
lwd=2, boxsize=1)


meta.summaries
--------------

What does it mean, when I get the warning message that NaNs were produced in "pchisq(q,
df, lower.tail, log.p)"? Is there something wrong with my data (there are no NAs in the
data)?

MetaAnalyse <- meta.summaries(CoeffVector, 
					StdErrorVector, 
					method = "random")


From BPikouni at CNTUS.JNJ.COM  Thu Aug 24 14:36:30 2006
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 24 Aug 2006 08:36:30 -0400
Subject: [R] Intro to Programming R Book
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB04C9F15E@CNTUSMAEXS4.na.jnj.com>

Hi Raphael:
You mention being interested in "programming", which covers many various
topics itself. I wholeheartedly recommend:

S Programming (2000) by Venables and Ripley .

Don't let the date of the book nor the fact that R is not mentioned in the
title dissuade you. Much like MASS by Venables and Ripley, I find there is
always something useful to learn (or re-learn), particularly if you work
through the exercises. The authors have excellent complementary materials to
both books as well at the books' web sites: see the links for [5] and [4] at
http://www.r-project.org/doc/bib/R-books.html.

Hope that helps,
Bill

Centocor, Inc.
Nonclinical Statistics


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Raphael Fraser
> Sent: Thursday, August 24, 2006 8:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Intro to Programming R Book
> 
> 
> I am new to R and am looking for a book that can help in learning to
> program in R. I have looked at the R website suggested books but I am
> still not sure which book best suite my needs. I am interesting in
> programming, data manipulation not statistics. Any suggestions?
> 
> Raphael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Thu Aug 24 14:42:12 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Aug 2006 14:42:12 +0200
Subject: [R] Authoring a book
In-Reply-To: <44EB32A5.7030508@psych.uib.no>
References: <44EB32A5.7030508@psych.uib.no>
Message-ID: <x2lkpeuwyj.fsf@viggo.kubism.ku.dk>

Tom Backer Johnsen <backer at psych.uib.no> writes:

> Me and some colleagues are planning to write a textbook together
> ("Statistics using R") where the target audience for the book is
> psychologists and students of psychology.
> 
> We thought that it might be a good idea to use a Wiki when writing the
> text.  Is that a good idea?  Does anybody have any experience in that
> direction?  What alternatives are there?
> 
> The tool (Wiki) would have to be able to handle tables and
> mathematical formulas in some manner, and of course, some mechanism to
> export the contents to a word processor in the final stages.
> 
> I have my own server, Windows, based on Apache, PhP, and MySQL.

SVN and LaTeX would be my tools of choice.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From S.Pickett at exeter.ac.uk  Thu Aug 24 14:43:18 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Thu, 24 Aug 2006 13:43:18 +0100 (BST)
Subject: [R] help: trouble using lines()
Message-ID: <1205.144.173.76.117.1156423398.squirrel@www.webmail.ex.ac.uk>

Hi R experts,
I have been using ReML as follows...
model<-lmer(late.growth~mtf+year+treat+hatch.day+hatch.day:year+hatch.day:treat+
mtf:treat+ treat:year+ year:treat:mtf+(1|fybrood), data = A)
then I wanted to plot the results of the three way interaction using
lines() as follows...

tmp<-as.vector(fixef(model))
graph1<-plot(mtf,fitted(f2), xlab=list("Brood Size"), ylab=list("Early
growth rate"), pch=16, col="darkgrey", bg="yellow")
lines(y,exp(tmp[1]+tmp[2]))

but no matter what I try I always get the error message
"Error in xy.coords(x, y) : 'x' and 'y' lengths differ"

Can anyone shed some light please?
I am basically copying the methods of the pdf entitled "Linear mixed
models in R" by S?ren Feodor Nielsen 20003.
http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf#search=%22Linear%20mixed%20models%20in%20R%22





Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From HDoran at air.org  Thu Aug 24 14:54:24 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 24 Aug 2006 08:54:24 -0400
Subject: [R] Optim question
Message-ID: <2323A6D37908A847A7C32F1E3662C80E277487@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/0188d1f9/attachment.pl 

From mr.spoon21 at gmail.com  Thu Aug 24 15:05:09 2006
From: mr.spoon21 at gmail.com (Carlo Trimarchi)
Date: Thu, 24 Aug 2006 15:05:09 +0200
Subject: [R] R and time series
Message-ID: <8f67b6f80608240605n13576212v216b67e5b09999e9@mail.gmail.com>

Hi, I'm new here.

I need to use R to analyze a particular time serie.
I have to estimate the pubblication of a news of an online newspaper,
for example in the CNN site.

I have many text files and every file correspond to a day. In every
file I have two columns:
1) in the first column there is the pubblication time of the news
2) in the second column there is the distance, expressed in seconds,
of this news from the previous

I think I can use the Holt-Winters method, but I have some doubt.

Do I need both columns or is sufficient just the first?

Do you have any helpful suggestion?
Sorry if my problem sounds strange.

Thanks, bye.
Carlo


From gyadav at ccilindia.co.in  Thu Aug 24 15:10:11 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Thu, 24 Aug 2006 18:40:11 +0530
Subject: [R] Please Ignore This is only a test mail
Message-ID: <OF132C0780.886EA91D-ON652571D4.00482F72-652571D4.00488187@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/1f32c181/attachment.pl 

From backer at psych.uib.no  Thu Aug 24 15:08:57 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Thu, 24 Aug 2006 15:08:57 +0200
Subject: [R] Authoring a book
In-Reply-To: <x2lkpeuwyj.fsf@viggo.kubism.ku.dk>
References: <44EB32A5.7030508@psych.uib.no> <x2lkpeuwyj.fsf@viggo.kubism.ku.dk>
Message-ID: <44EDA4E9.2050606@psych.uib.no>

Peter Dalgaard wrote:
> Tom Backer Johnsen <backer at psych.uib.no> writes:
> 
>> Me and some colleagues are planning to write a textbook together
>> ("Statistics using R") where the target audience for the book is
>> psychologists and students of psychology.
>>
>> We thought that it might be a good idea to use a Wiki when writing the
>> text.  Is that a good idea?  Does anybody have any experience in that
>> direction?  What alternatives are there?
>>
>> The tool (Wiki) would have to be able to handle tables and
>> mathematical formulas in some manner, and of course, some mechanism to
>> export the contents to a word processor in the final stages.
>>
>> I have my own server, Windows, based on Apache, PhP, and MySQL.
> 
> SVN and LaTeX would be my tools of choice.

A very different approach.  SVN is not something I am aquainted with, 
but should be worth looking into.  As to LaTex, the closest I have 
worked with is Lyx.

The problem is, there are two other authors I have to persuade to 
learn new tools.  So, it might be too complex.

Tom

> 


-- 
+----------------------------------------------------------------+
| Tom Backer Johnsen, Psychometrics Unit,  Faculty of Psychology |
| University of Bergen, Christies gt. 12, N-5015 Bergen,  NORWAY |
| Tel : +47-5558-9185                        Fax : +47-5558-9879 |
| Email : backer at psych.uib.no    URL : http://www.galton.uib.no/ |
+----------------------------------------------------------------+


From petr.pikal at precheza.cz  Thu Aug 24 15:17:39 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Aug 2006 15:17:39 +0200
Subject: [R] help: trouble using lines()
In-Reply-To: <1205.144.173.76.117.1156423398.squirrel@www.webmail.ex.ac.uk>
Message-ID: <44EDC313.30276.B0AEC6@localhost>

Hi

not sure but are there some NA values in your data?

what

length(mtf)
and
length(fitted(f2))

tells you?

And you need not to use assignment

graph1 <- plot(....)

to output a plot on screen.


HTH
Petr


On 24 Aug 2006 at 13:43, Simon Pickett wrote:

Date sent:      	Thu, 24 Aug 2006 13:43:18 +0100 (BST)
From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
To:             	R-help at stat.math.ethz.ch
Subject:        	[R] help: trouble using lines()

> Hi R experts,
> I have been using ReML as follows...
> model<-lmer(late.growth~mtf+year+treat+hatch.day+hatch.day:year+hatch.
> day:treat+ mtf:treat+ treat:year+ year:treat:mtf+(1|fybrood), data =
> A) then I wanted to plot the results of the three way interaction
> using lines() as follows...
> 
> tmp<-as.vector(fixef(model))
> graph1<-plot(mtf,fitted(f2), xlab=list("Brood Size"), ylab=list("Early
> growth rate"), pch=16, col="darkgrey", bg="yellow")
> lines(y,exp(tmp[1]+tmp[2]))
> 
> but no matter what I try I always get the error message
> "Error in xy.coords(x, y) : 'x' and 'y' lengths differ"
> 
> Can anyone shed some light please?
> I am basically copying the methods of the pdf entitled "Linear mixed
> models in R" by S?ren Feodor Nielsen 20003.
> http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-mod
> els.pdf#search=%22Linear%20mixed%20models%20in%20R%22
> 
> 
> 
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ggrothendieck at gmail.com  Thu Aug 24 15:46:11 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 09:46:11 -0400
Subject: [R] Intro to Programming R Book
In-Reply-To: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
References: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
Message-ID: <971536df0608240646i2f621bf7wa3c637b4bdd3e702@mail.gmail.com>

There is some online material at:
http://zoonek2.free.fr/UNIX/48_R/all.html
http://pj.freefaculty.org/R/statsRus.html

On 8/24/06, Raphael Fraser <raphael.fraser at gmail.com> wrote:
> I am new to R and am looking for a book that can help in learning to
> program in R. I have looked at the R website suggested books but I am
> still not sure which book best suite my needs. I am interesting in
> programming, data manipulation not statistics. Any suggestions?


From S.Pickett at exeter.ac.uk  Thu Aug 24 15:52:09 2006
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Thu, 24 Aug 2006 14:52:09 +0100 (BST)
Subject: [R] help: trouble using lines()
In-Reply-To: <44EDC313.30276.B0AEC6@localhost>
References: <1205.144.173.76.117.1156423398.squirrel@www.webmail.ex.ac.uk>
	<44EDC313.30276.B0AEC6@localhost>
Message-ID: <1266.144.173.76.117.1156427529.squirrel@www.webmail.ex.ac.uk>

Hi, thanks for replying.
No, there arent any NA's in the original data set....
I think I must be mis-interpreting the use of lines()?
in the example what exactly is "y"?
lines(y,exp(tmp[1]+tmp[2]))
In my case tmp[1] and tmp[2] are coeficients from the model so just one
number (not a vector) and I havent specified y....
Thanks everyone,
Simon


> Hi
>
> not sure but are there some NA values in your data?
>
> what
>
> length(mtf)
> and
> length(fitted(f2))
>
> tells you?
>
> And you need not to use assignment
>
> graph1 <- plot(....)
>
> to output a plot on screen.
>
>
> HTH
> Petr
>
>
> On 24 Aug 2006 at 13:43, Simon Pickett wrote:
>
> Date sent:      	Thu, 24 Aug 2006 13:43:18 +0100 (BST)
> From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
> To:             	R-help at stat.math.ethz.ch
> Subject:        	[R] help: trouble using lines()
>
>> Hi R experts,
>> I have been using ReML as follows...
>> model<-lmer(late.growth~mtf+year+treat+hatch.day+hatch.day:year+hatch.
>> day:treat+ mtf:treat+ treat:year+ year:treat:mtf+(1|fybrood), data > A)
>> then I wanted to plot the results of the three way interaction
>> using lines() as follows...
>>
>> tmp<-as.vector(fixef(model))
>> graph1<-plot(mtf,fitted(f2), xlab=list("Brood Size"), ylab=list("Early
>> growth rate"), pch=16, col="darkgrey", bg="yellow")
>> lines(y,exp(tmp[1]+tmp[2]))
>>
>> but no matter what I try I always get the error message
>> "Error in xy.coords(x, y) : 'x' and 'y' lengths differ"
>>
>> Can anyone shed some light please?
>> I am basically copying the methods of the pdf entitled "Linear mixed
>> models in R" by S?ren Feodor Nielsen 20003.
>> http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-mod
>> els.pdf#search=%22Linear%20mixed%20models%20in%20R%22
>>
>>
>>
>>
>>
>> Simon Pickett
>> PhD student
>> Centre For Ecology and Conservation
>> Tremough Campus
>> University of Exeter in Cornwall
>> TR109EZ
>> Tel 01326371852
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>


Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From singularitaet at gmx.net  Thu Aug 24 15:53:39 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Thu, 24 Aug 2006 15:53:39 +0200
Subject: [R] Authoring a book
In-Reply-To: <44EDA4E9.2050606@psych.uib.no>
References: <44EB32A5.7030508@psych.uib.no> <x2lkpeuwyj.fsf@viggo.kubism.ku.dk>
	<44EDA4E9.2050606@psych.uib.no>
Message-ID: <44EDAF63.7060006@gmx.net>

I think Peter Dalgaard is right.

Since you are able to use R I believe you will be very fast in learning
LaTeX.

I think it needs less then a week to learn the most common LaTeX
commands. And setting up a wiki and trying then to convert this into a
printable document format plus learning the wiki syntax is probably more
time consuming. Beside this R is able to work perfectly together with
LaTeX, it creates LaTeX output and is doing excellent graphics in the
EPS/PS format.

The best introduction for LaTeX is the not so short introduction:
http://people.ee.ethz.ch/~oetiker/lshort/lshort.pdf

If you still are not convinced have a look at UniWakkaWiki:
http://uniwakka.sourceforge.net/HomePage

It is a Wiki for Science and University purposes and claims to be able
to export to Openoffice as well as to LaTeX.

Stefan Grosse



>>>
>>> I have my own server, Windows, based on Apache, PhP, and MySQL.
>>>       
>> SVN and LaTeX would be my tools of choice.
>>     
>
> A very different approach.  SVN is not something I am aquainted with, 
> but should be worth looking into.  As to LaTex, the closest I have 
> worked with is Lyx.
>
> The problem is, there are two other authors I have to persuade to 
> learn new tools.  So, it might be too complex.
>
> Tom
>


From f.abian at gmx.net  Thu Aug 24 15:58:32 2006
From: f.abian at gmx.net (Fabian Scheipl)
Date: Thu, 24 Aug 2006 15:58:32 +0200
Subject: [R] lmer(): specifying i.i.d random slopes for multiple covariates
Message-ID: <20060824135832.52370@gmx.net>

Dear readers,

Is it possible to specify a model

y=X %*% beta + Z %*% b ; b=(b_1,..,b_k) and b_i~N(0,v^2) for i=1,..,k

that is, a model where the random slopes for different covariates are i.i.d., in lmer() and how?

In lme() one needs a constant grouping factor (e.g.: all=rep(1,n)) and would then specify:
lme(fixed= y~X, random= list(all=pdIdent(~Z-1)) ) ,
that?s how it's done in the lmeSplines- documentation.

Any hints would be greatly appreciated- I'm trying to write a suite of functions that will transform additive models into their mixed-effects representation like lmeSplines but using lmer() instead of lme().

Thank you for your time,
Fabian Scheipl
-- 


Echte DSL-Flatrate dauerhaft f?r 0,- Euro*. Nur noch kurze Zeit!


From petr.pikal at precheza.cz  Thu Aug 24 16:26:46 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Aug 2006 16:26:46 +0200
Subject: [R] help: trouble using lines()
In-Reply-To: <1266.144.173.76.117.1156427529.squirrel@www.webmail.ex.ac.uk>
References: <44EDC313.30276.B0AEC6@localhost>
Message-ID: <44EDD346.30308.EFF755@localhost>

Hi

from lines help page

x, y coordinate vectors of points to join.

and lines or points simply adds lines or points to existing plot.
What do you want to plot with lines?

HTH
Petr



On 24 Aug 2006 at 14:52, Simon Pickett wrote:

Date sent:      	Thu, 24 Aug 2006 14:52:09 +0100 (BST)
From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch, Simon Pickett <s.pickett at exeter.ac.uk>
Subject:        	Re: [R] help: trouble using lines()

> Hi, thanks for replying.
> No, there arent any NA's in the original data set....
> I think I must be mis-interpreting the use of lines()?
> in the example what exactly is "y"?
> lines(y,exp(tmp[1]+tmp[2]))
> In my case tmp[1] and tmp[2] are coeficients from the model so just
> one number (not a vector) and I havent specified y.... Thanks
> everyone, Simon
> 
> 
> > Hi
> >
> > not sure but are there some NA values in your data?
> >
> > what
> >
> > length(mtf)
> > and
> > length(fitted(f2))
> >
> > tells you?
> >
> > And you need not to use assignment
> >
> > graph1 <- plot(....)
> >
> > to output a plot on screen.
> >
> >
> > HTH
> > Petr
> >
> >
> > On 24 Aug 2006 at 13:43, Simon Pickett wrote:
> >
> > Date sent:      	Thu, 24 Aug 2006 13:43:18 +0100 (BST)
> > From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
> > To:             	R-help at stat.math.ethz.ch
> > Subject:        	[R] help: trouble using lines()
> >
> >> Hi R experts,
> >> I have been using ReML as follows...
> >> model<-lmer(late.growth~mtf+year+treat+hatch.day+hatch.day:year+hat
> >> ch. day:treat+ mtf:treat+ treat:year+ year:treat:mtf+(1|fybrood),
> >> data > A) then I wanted to plot the results of the three way
> >> interaction using lines() as follows...
> >>
> >> tmp<-as.vector(fixef(model))
> >> graph1<-plot(mtf,fitted(f2), xlab=list("Brood Size"),
> >> ylab=list("Early growth rate"), pch=16, col="darkgrey",
> >> bg="yellow") lines(y,exp(tmp[1]+tmp[2]))
> >>
> >> but no matter what I try I always get the error message
> >> "Error in xy.coords(x, y) : 'x' and 'y' lengths differ"
> >>
> >> Can anyone shed some light please?
> >> I am basically copying the methods of the pdf entitled "Linear
> >> mixed models in R" by S?ren Feodor Nielsen 20003.
> >> http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-
> >> mod els.pdf#search=%22Linear%20mixed%20models%20in%20R%22
> >>
> >>
> >>
> >>
> >>
> >> Simon Pickett
> >> PhD student
> >> Centre For Ecology and Conservation
> >> Tremough Campus
> >> University of Exeter in Cornwall
> >> TR109EZ
> >> Tel 01326371852
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html and provide commented,
> >> minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From dimitris.rizopoulos at med.kuleuven.be  Thu Aug 24 16:42:24 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 24 Aug 2006 16:42:24 +0200
Subject: [R] Optim question
References: <2323A6D37908A847A7C32F1E3662C80E277487@dc1ex01.air.org>
Message-ID: <016c01c6c78b$83520550$0540210a@www.domain>

Hi Harold,

you're probably looking for something like:

rasch.max2 <- function(x, betas){
    opt <- function(theta){
        -sum(dbinom(x, 1, plogis(theta - betas), log = TRUE))
    }
    out <- optim(log(sum(x)/(length(x)/sum(x))), opt, method = "BFGS", 
hessian = TRUE)
    cat('theta is about', round(out$par, 2), ', se', 
1/sqrt(out$hes),'\n')
}


rasch.max(c(1, 1, 0, 0), c(-1, .5, 0, 1))
rasch.max2(c(1, 1, 0, 0), c(-1, .5, 0, 1))

rasch.max(c(1, 0, 1, 1), c(-1, .5, 0, 1))
rasch.max2(c(1, 0, 1, 1), c(-1, .5, 0, 1))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 24, 2006 2:54 PM
Subject: [R] Optim question


> This is a very basic question, but I am a bit confused with optim. I
> want to get the MLEs using optim which could replace the 
> newton-raphson
> code I have below which also gives the MLEs. The function takes as 
> input
> a vector x denoting whether a respondent answered an item correctly
> (x=1) or not (x=0). It also takes as input a vector b_vector, and 
> these
> are parameters of test items (Rasch estimates in this case)
>
> For example, here is how my current function operates.
>
>> rasch.max(c(1,1,0,0), c(-1,.5,0,1))
> theta is about 0.14 , se 1.063972
>
> I'm not quite sure how to accomplish the same thing using optim. Can
> anyone offer a suggestion?
>
> rasch.max <- function(x, b_vector){
>   p  <- numeric(length(b_vector))
>   theta <- log(sum(x)/(length(x)/sum(x))) # This is a starting value
> for theta
>   rasch <- function(theta,b) 1/ (1 + exp(b-theta))
>   old   <- 0
>   updated   <- 5
>   while(abs(old-updated) > .001){
>      old <- updated
>      for(k in seq(along=b_vector)) p[k] <- rasch(theta,b_vector[k])
>      first_deriv  <- sum(x) - sum(p)
>      second_deriv <- sum((1-p)*-p)
>      change       <- (first_deriv/second_deriv)
>      theta        <- theta - change # This is the updated theta
>      updated      <- change
>      }
> cat('theta is about', round(theta,2), ', se', 1/sqrt(-second_deriv),
> '\n')
> }
>
> Harold
>
>> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.0
> year           2006
> month          04
> day            24
> svn rev        37909
> language       R
> version.string Version 2.3.0 (2006-04-24)
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From roger.bos at gmail.com  Thu Aug 24 16:53:19 2006
From: roger.bos at gmail.com (roger bos)
Date: Thu, 24 Aug 2006 10:53:19 -0400
Subject: [R] installing the x86_64 R Binary on Fedora Core 5
Message-ID: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/cac0eb40/attachment.pl 

From tlumley at u.washington.edu  Thu Aug 24 17:04:38 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Aug 2006 08:04:38 -0700 (PDT)
Subject: [R] "fixed effects" transformation
In-Reply-To: <e474b3790608231955k516411a4t64b8c773e187d26c@mail.gmail.com>
References: <e474b3790608231955k516411a4t64b8c773e187d26c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608240804150.30960@homer21.u.washington.edu>

On Wed, 23 Aug 2006, Eduardo Leoni wrote:
>
> I created this function following Farnsworth
> (http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)
>
>
> demean <- function(x,index) {
>  for (i in unique(index)) {
>    for (j in 1:ncol(x)) {
>      x.now <- x[index==i,j]
>      x[index==i,j] <- x.now-mean(x.now,na.rm=TRUE)
>    }
>  }
>  x
> }
>
> it is obvious that there must be a much much more efficient way to do
> this, though. Any recommendations?

I think  you want ave().

 	-thomas


From paul.bliese at us.army.mil  Thu Aug 24 17:06:01 2006
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Thu, 24 Aug 2006 17:06:01 +0200
Subject: [R] Why are lagged correlations typically negative?
Message-ID: <77C117DE9E87354CB7804A2DE7D566C2E1AC4E@amedmlermc132>

Recently, I was working with some lagged designs where a vector of
observations at one time was used to predict a vector of observations at
another time using a lag 1 design.  In the work, I noticed a lot of
negative correlations, so I ran a simple simulation with 2 matched
points.  The crude simulation example below shows that the correlation
can be -1 or +1, but interestingly if you do this basic simulation
thousands of times, you get negative correlations 66 to 67% of the time.
If you simulate three matched observations instead of three you get
negative correlations about 74% of the time and then as you simulate 4
and more observations the number of negative correlations asymptotically
approaches an equal 50% for negative versus positive correlations
(though then with 100 observations one has 54% negative correlations).
Creating T1 and T2 so they are related (and not correlated 1 as in the
crude simulation) attenuates the effect.  A more advanced simulation is
provided below for those interested.

Can anyone explain why this occurs in a way a non-mathematician is
likely to understand?

Thanks,

Paul

#############
# Crude simulation
#############
> (T1<-rnorm(3))
[1] -0.1594703 -1.3340677  0.2924988
> (T2<-c(T1[2:3],NA))
[1] -1.3340677  0.2924988         NA
> cor(T1,T2, use="complete")
[1] -1

> (T1<-rnorm(3))
[1] -0.84258593 -0.49161602  0.03805543
> (T2<-c(T1[2:3],NA))
[1] -0.49161602  0.03805543          NA
> cor(T1,T2, use="complete")
[1] 1

###########
# More advanced simulation example
###########
> lags
function(nobs,nreps,rho=1){
OUT<-data.frame(NEG=rep(NA,nreps),COR=rep(NA,nreps))
nran<-nobs+1  #need to generate 1 more random number than there are
observations
  for(i in 1:nreps){
      V1<-rnorm(nran)
      V2<-sqrt(1-rho^2)*rnorm(nran)+rho*V1
      #print(cor(V1,V2))
      V1<-V1[1:nran-1]
      V2<-V2[2:nran]
      OUT[i,1]<-ifelse(cor(V1,V2)<=0,1,0)
      OUT[i,2]<-cor(V1,V2)
  }
return(OUT) #out is a 1 if the corr is negative or 0; 0 if positive
}
> LAGS.2<-lags(2,10000)  #Number of observations matched = 2
> mean(LAGS.2)
    NEG     COR 
 0.6682 -0.3364


From ggrothendieck at gmail.com  Thu Aug 24 17:15:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 11:15:51 -0400
Subject: [R] [Rd] reshape scaling with large numbers of times/rows
In-Reply-To: <971536df0608240757m28a27100scc619938d97ac68@mail.gmail.com>
References: <1156399563.30567.108.camel@enzian>
	<971536df0608240016h71b3672bka6b54cb87a0aca5f@mail.gmail.com>
	<971536df0608240557t52d6014cg692d681032561e2d@mail.gmail.com>
	<1156428496.5434.70.camel@firebolt>
	<971536df0608240722u297dd2b8jbaf514cb83542784@mail.gmail.com>
	<971536df0608240757m28a27100scc619938d97ac68@mail.gmail.com>
Message-ID: <971536df0608240815s4a59be5n7ce12a3571c05ed5@mail.gmail.com>

On 8/24/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here is one more solution .  It uses the reshape package.
> Its faster than using reshape but not as fast as xtabs;
> however, it is quite simple -- just one line and if that
> matters it might be useful:
>
> library(reshape)
> system.time(w4 <- cast(melt(DF, id = 1:2), Y ~ X, head, n = 1))
>
> On 8/24/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 8/24/06, Mitch Skinner <mitch at gallo.ucsf.edu> wrote:
> > > On Thu, 2006-08-24 at 08:57 -0400, Gabor Grothendieck wrote:
> > > > If your Z in reality is not naturally numeric try representing it as a
> > > > factor and using
> > > > the numeric levels as your numbers and then put the level labels back on:
> > > >
> > > > m <- n <- 5
> > > > DF <- data.frame(X = gl(m*n, 1), Y = gl(m, n), Z = letters[1:25])
> > > > Zn <- as.numeric(DF$Z)
> > > > system.time(w1 <- reshape(DF, timevar = "X", idvar = "Y", dir = "wide"))
> > > > system.time({Zn <- as.numeric(DF$Z)
> > > >    w2 <- xtabs(Zn ~ Y + X, DF)
> > > >    w2[w2 > 0] <- levels(DF$Z)[w2]
> > > >    w2[w2 == 0] <- NA
> > > > })
> > >
> > > This is pretty slick, thanks.  It looks like it works for me.  For the
> > > archives, this is how I got back to a data frame (as.data.frame(w2)
> > > gives me a long version again):
> > >
> > > > m <- 4500
> > > > n <- 70
> > > > DF <- data.frame(X = gl(m, n), Y = 1:n, Z = letters[1:25])
> > > > system.time({Zn <- as.numeric(DF$Z)
> > > +    w2 <- xtabs(Zn ~ Y + X, DF)
> > > +    w2[w2 > 0] <- levels(DF$Z)[w2]
> > > +    w2[w2 == 0] <- NA
> > > +    WDF <- data.frame(Y=dimnames(w2)$Y)
> > > +    for (col in dimnames(w2)$X) { WDF[col]=w2[,col] }
> > > + })
> > > [1] 131.888   1.240 135.945   0.000   0.000
> > > > dim(WDF)
> > > [1]   70 4501
> > >
> > > I'll have to look; maybe I can just use w2 as is.  Next time I guess
> > > I'll try R-help first.
> > >
> > > Thanks again,
> > > Mitch
> > >
> >
> > Also try
> >  na.omit(as.data.frame(w2))
> >
>


From mschwartz at mn.rr.com  Thu Aug 24 17:18:45 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 24 Aug 2006 10:18:45 -0500
Subject: [R] installing the x86_64 R Binary on Fedora Core 5
In-Reply-To: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
References: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
Message-ID: <1156432726.4115.13.camel@localhost.localdomain>

On Thu, 2006-08-24 at 10:53 -0400, roger bos wrote:
> I am looking for help install the x86_64 R Binary onto my FC5 machine.  At
> the risk of subjecting myself to tons of criticism, I must confess that I
> don't know anything about Linux and I have never compiled R from source.
> Therefore, I choose FC5 because I see that a 64-bit binary is already
> available.
> 
> Here is what I tried:  I installed FC5 with all options (productivity,
> software development, and web server).  FC5 boots up fine.  I downloaded all
> the R binary files in that FC5 directory to my USB drive and copied
> them onto my linux machine (where I don't yet have internet access).  I
> created a folder in my rbos's Home directory and copied the files there.  I
> clicked on the R-2.3.1-1.fc5.x86_64.rpm and right-clicked to choose 'open
> with "install software"'  It asked me for my root password.  I put in the
> same root password I choose when I installed FC5.  I get a "installing
> packages" screen that shows the R filename and I click Apply.  It then give
> me an Error: "Unable to retrieve software information".
> 
> Can anyone tell me what steps I am missing?  The R install guide states that
> binary installs are platform specific so it only considers building from the
> sources.  I look forward to learning a lot about Linux and using more than
> just the GUI, but to get started, I just want to learn how to install a
> binary of R.
> 
> Thanks so much,
> 
> Roger

Roger,

I don't know if the GUI version of the RPM interface (Red Hat/Fedora's
Package Manager) is sufficiently robust these days. In the past, there
were notable problems trying to use it to install software binaries.

I would open a console and change into the folder where you copied the
files. If you are running GNOME and do not have a Terminal launcher
(icon) on any of your panels, you can go to the menus and select:

  Applications -> Accessories -> Terminal

This will open a console on your desktop, just like the Windows command
line console.

Then change to the appropriate folder:

  cd /Path/To/FolderName

Note that unlike Windows, the slashes are '/', not '\'.

Then in that folder, type:

  su

You will be prompted for the root password.  If successful, you will
note that the prompt prefix changes from a '$' to a '#'.

Then, type:

  rpm -ivh R-2.3.1-1.fc5.x86_64.rpm

This will begin the R installation process.  If you get back to the
prompt without any error messages, you should be good to go.  Then type:

  exit

at the console, which will exit root status and bring you back to your
regular user ID (prompt prefix back to '$').

Needless to say, that while you have root privileges in the console, be
careful in what you might type. You have total access to screw up the
system...  :-)

If you get any error messages, post them back here and we can help debug
the process.

HTH,

Marc Schwartz


From Gaspard.Lequeux.at at biomath.ugent.be  Thu Aug 24 17:19:10 2006
From: Gaspard.Lequeux.at at biomath.ugent.be (Gaspard Lequeux)
Date: Thu, 24 Aug 2006 17:19:10 +0200 (CEST)
Subject: [R] rgl: exporting to pdf or png does not work
In-Reply-To: <44ECDC27.6080109@stats.uwo.ca>
References: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>
	<44ECDC27.6080109@stats.uwo.ca>
Message-ID: <Pine.LNX.4.62.0608241642490.23380@biomath.ugent.be>


Hej,

On Wed, 23 Aug 2006, Duncan Murdoch wrote:

> On 8/23/2006 5:15 PM, Gaspard Lequeux wrote:
>
>> When exporting a image from rgl, the following error is encountered:
>>
>>> rgl.postscript('testing.pdf', fmt="pdf")
>> RGL: ERROR: can't bind glx context to window
>> RGL: ERROR: can't bind glx context to window
>> Warning messages:
>> 1: X11 protocol error: GLXBadContextState
>> 2: X11 protocol error: GLXBadContextState
>>
>> The pdf file is created and is readable, but all the labels are gone.
>>
>> Taking a snapshot (to png) gives 'failed' and no file is created.
>>
>> Version of rgl used: 0.67-2 (2006-07-11)
>> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
>> Running Debian GNU/Linux testing (Etch).
>
> That looks like an X11 error to me, not something that I'm very likely
> to be able to fix.  If you can debug the error, it would be helpful.

Actually after upgrading everything (debian testing (etch)) and restarting 
X, I didn't get that error anymore. It was worse: R crashed:

> library(rgl);triangles3d(c(1,,2,3),c(1,2,4),c(1,3,5));rgl.postscript('testing.pdf','pdf')
X Error of failed request:  GLXBadContextState
   Major opcode of failed request:  142 (GLX)
   Minor opcode of failed request:  5 (X_GLXMakeCurrent)
   Serial number of failed request:  85
   Current serial number in output stream:  85
glequeux at toidi:~/seqanal$


I downloaded the source package (debian testing (etch), rgl-0.67-2).

In rgl-0.67-2/src/ I changed the following files:

rglview.cpp, around line 587. Commenting the function call gl2psBeginPage 
removed the crash (but also no pdf output...)

I enabled this function again and went to gl2ps.c, to the function 
gl2psBeginPage. At the end of that function, around line 4426, commenting 
out the line
glRenderMode(GL_FEEDBACK);
removes the R crash, but of course still no pdf output (well, only the 
background).

GL_FEEDBACK is defined in /usr/include/GL/gl.h as:

/* Render Mode */
#define GL_FEEDBACK				0x1C01
#define GL_RENDER				0x1C00
#define GL_SELECT				0x1C02

Trying glRenderMode(GL_RENDER) removed the crash, but still only the 
background in the pdf.

If someone has some suggestions about what to do next...

/Gaspard


From singularitaet at gmx.net  Thu Aug 24 17:22:02 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Thu, 24 Aug 2006 17:22:02 +0200
Subject: [R] installing the x86_64 R Binary on Fedora Core 5
In-Reply-To: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
References: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
Message-ID: <44EDC41A.9000008@gmx.net>

Hi Roger,

I dunno what exactly might be the source of that mistake but I would
strongly recommend to install R while you are online. Often other
packages must be installed for dependencies.

(And then I recommend using the smart package manager (
http://labix.org/smart ) which is a great tool and handling dependencies
better.)

You could use rpm at the command line level.

open a shell, type su and give your password, change to the folder where
the rpm is downloaded to, type
rpm -ivh R-2.3.1-1.fc5.x86_64.rpm  will try to install and give you
information on whats missing...

Stefan Grosse

roger bos schrieb:
> I am looking for help install the x86_64 R Binary onto my FC5 machine.  At
> the risk of subjecting myself to tons of criticism, I must confess that I
> don't know anything about Linux and I have never compiled R from source.
> Therefore, I choose FC5 because I see that a 64-bit binary is already
> available.
>
> Here is what I tried:  I installed FC5 with all options (productivity,
> software development, and web server).  FC5 boots up fine.  I downloaded all
> the R binary files in that FC5 directory to my USB drive and copied
> them onto my linux machine (where I don't yet have internet access).  I
> created a folder in my rbos's Home directory and copied the files there.  I
> clicked on the R-2.3.1-1.fc5.x86_64.rpm and right-clicked to choose 'open
> with "install software"'  It asked me for my root password.  I put in the
> same root password I choose when I installed FC5.  I get a "installing
> packages" screen that shows the R filename and I click Apply.  It then give
> me an Error: "Unable to retrieve software information".
>
> Can anyone tell me what steps I am missing?  The R install guide states that
> binary installs are platform specific so it only considers building from the
> sources.  I look forward to learning a lot about Linux and using more than
> just the GUI, but to get started, I just want to learn how to install a
> binary of R.
>
> Thanks so much,
>
> Roger
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From bruno at est.ufpr.br  Thu Aug 24 17:23:46 2006
From: bruno at est.ufpr.br (Bruno Grimaldo Martinho Churatae)
Date: Thu, 24 Aug 2006 12:23:46 -0300 (BRT)
Subject: [R] problem in install on ubuntu
Message-ID: <Pine.LNX.4.63.0608241204460.2109@est.ufpr.br>

Hi,

the problem is:

/usr/bin/ld: cannot find -lblas-3
collect2: ld returned 1 exit status
make: ** [ape.so] Erro 1
ERROR: compilation failed for package 'ape'
** Removing '/usr/local/lib/R/site-library/ape'

Any help would be appreciated.
Thanks,

------------------------------------
Bruno G. M. Churata


From tlumley at u.washington.edu  Thu Aug 24 17:27:04 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Aug 2006 08:27:04 -0700 (PDT)
Subject: [R] Why are lagged correlations typically negative?
In-Reply-To: <77C117DE9E87354CB7804A2DE7D566C2E1AC4E@amedmlermc132>
References: <77C117DE9E87354CB7804A2DE7D566C2E1AC4E@amedmlermc132>
Message-ID: <Pine.LNX.4.64.0608240811140.30960@homer21.u.washington.edu>

On Thu, 24 Aug 2006, Bliese, Paul D LTC USAMH wrote:

> Recently, I was working with some lagged designs where a vector of
> observations at one time was used to predict a vector of observations at
> another time using a lag 1 design.  In the work, I noticed a lot of
> negative correlations, so I ran a simple simulation with 2 matched
> points.  The crude simulation example below shows that the correlation
> can be -1 or +1, but interestingly if you do this basic simulation
> thousands of times, you get negative correlations 66 to 67% of the time.
> If you simulate three matched observations instead of three you get
> negative correlations about 74% of the time and then as you simulate 4
> and more observations the number of negative correlations asymptotically
> approaches an equal 50% for negative versus positive correlations
> (though then with 100 observations one has 54% negative correlations).
> Creating T1 and T2 so they are related (and not correlated 1 as in the
> crude simulation) attenuates the effect.  A more advanced simulation is
> provided below for those interested.
>
> Can anyone explain why this occurs in a way a non-mathematician is
> likely to understand?

Consider the two points out of three case from the viewpoint of the middle 
point.  The correlation is positive if the previous point is lower and the 
following point is higher, or vice versa. It is negative if the previous 
and following points are both higher or both lower.

Now, if the middle point is higher than the first point it is probably 
higher than average, and so it has a more than 50% chance of also being 
higher than the third point.  Similarly, if it is lower than the first 
point it is likely to be lower than the third point.

So negative correlation is more likely than positive.

Working out the covariance may be useful even for non-mathematicians. Call 
the three points X,Y,Z

   cov(X-Y, Y-Z) = cov(X,Y)-cov(Y,Y)-cov(X,Z)+cov(Y,Z)
                 =    0    - var(Y) -    0   -    0

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From j.van_den_hoff at fz-rossendorf.de  Thu Aug 24 17:38:18 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Thu, 24 Aug 2006 17:38:18 +0200
Subject: [R] Intro to Programming R Book
In-Reply-To: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
References: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
Message-ID: <44EDC7EA.3060208@fz-rossendorf.de>

Raphael Fraser wrote:
> I am new to R and am looking for a book that can help in learning to
> program in R. I have looked at the R website suggested books but I am
> still not sure which book best suite my needs. I am interesting in
> programming, data manipulation not statistics. Any suggestions?
> 
> Raphael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


"S Programming" by Venables and Ripley (Springer) seems the only(?) one 
around targeting the language, not it's applications. luckily, it's very 
good. for the rest (things specific to R, e.g. package development, 
namespaces etc.) I think one can only resort to the R manuals .


From austin at botany.utoronto.ca  Thu Aug 24 17:35:58 2006
From: austin at botany.utoronto.ca (Ryan Austin)
Date: Thu, 24 Aug 2006 11:35:58 -0400
Subject: [R] problem in install on ubuntu
In-Reply-To: <Pine.LNX.4.63.0608241204460.2109@est.ufpr.br>
References: <Pine.LNX.4.63.0608241204460.2109@est.ufpr.br>
Message-ID: <44EDC75E.70407@botany.utoronto.ca>

Hi Bruno,

Your missing the Basic Linear Algebra Subroutines 3.0 package.

apt-get install refblas3

should fix the problem. (At least in Debian, should be the same for Ubuntu)
Ryan

Bruno Grimaldo Martinho Churatae wrote:

>Hi,
>
>the problem is:
>
>/usr/bin/ld: cannot find -lblas-3
>collect2: ld returned 1 exit status
>make: ** [ape.so] Erro 1
>ERROR: compilation failed for package 'ape'
>** Removing '/usr/local/lib/R/site-library/ape'
>
>Any help would be appreciated.
>Thanks,
>
>------------------------------------
>Bruno G. M. Churata
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>  
>


From austin at botany.utoronto.ca  Thu Aug 24 17:40:01 2006
From: austin at botany.utoronto.ca (Ryan Austin)
Date: Thu, 24 Aug 2006 11:40:01 -0400
Subject: [R] problem in install on ubuntu
In-Reply-To: <44EDC75E.70407@botany.utoronto.ca>
References: <Pine.LNX.4.63.0608241204460.2109@est.ufpr.br>
	<44EDC75E.70407@botany.utoronto.ca>
Message-ID: <44EDC851.1030109@botany.utoronto.ca>

Sorry, that should be:

apt-get install refblas3-dev

>apt-get install refblas3
>
>should fix the problem. (At least in Debian, should be the same for Ubuntu)
>Ryan
>
>Bruno Grimaldo Martinho Churatae wrote:
>
>  
>
>>Hi,
>>
>>the problem is:
>>
>>/usr/bin/ld: cannot find -lblas-3
>>collect2: ld returned 1 exit status
>>make: ** [ape.so] Erro 1
>>ERROR: compilation failed for package 'ape'
>>** Removing '/usr/local/lib/R/site-library/ape'
>>
>>Any help would be appreciated.
>>Thanks,
>>
>>------------------------------------
>>Bruno G. M. Churata
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>> 
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>  
>


From roger.bos at gmail.com  Thu Aug 24 17:50:20 2006
From: roger.bos at gmail.com (roger bos)
Date: Thu, 24 Aug 2006 11:50:20 -0400
Subject: [R] installing the x86_64 R Binary on Fedora Core 5
In-Reply-To: <44EDC41A.9000008@gmx.net>
References: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
	<44EDC41A.9000008@gmx.net>
Message-ID: <1db726800608240850g71ec69c6va56b50e6b836e3f3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/f13c7f9f/attachment.pl 

From tlumley at u.washington.edu  Thu Aug 24 17:50:25 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Aug 2006 08:50:25 -0700 (PDT)
Subject: [R] metaplot and meta.summaries
In-Reply-To: <000401c6c77a$34b2abe0$20d2ac88@fnu.zmaw.de>
References: <000401c6c77a$34b2abe0$20d2ac88@fnu.zmaw.de>
Message-ID: <Pine.LNX.4.64.0608240842300.30960@homer21.u.washington.edu>

On Thu, 24 Aug 2006, Anne Katrin Heinrichs wrote:
> metaplot:
> ---------
>
> Can I change the label size? I've got 126 values and the intersection of the labels makes
> it impossible to read them.

Yes. metaplot() accepts the cex graphics parameter. This doesn't alter the 
size of the summary text, so you might want to have no summlabel= and add 
it on later.

> Why do I have to give sumse (Standard Error) and sumnn (Precision) of 
> the summary estimate? I can calculate one from the other, right? Just to 
> make sure I'm not misunderstanding something.

It's an oversight by the designer.  metaplot() was basically designed to 
be called from the plot methods for the meta. objects.

You might also want to look at forestplot(), which is more flexible. I 
don't know how it copes with large numbers of intervals, but if it doesn't 
it would be worth fixing.

> metaplot(CoeffVector, StdErrorVector, nn=NULL, labels=Name, conf.level=0.95,
> xlab=paste(CoeffNames[j], CoefficientName[i]),
> ylab="Countries",
> xlim=NULL, summn=PostCoeffs[1,j], sumse=sqrt(PostVars[1,j]),
> sumnn=1/(PostVars[1,j]),
> summlabel="Summary",
> lwd=2, boxsize=1)
>
>
> meta.summaries
> --------------
>
> What does it mean, when I get the warning message that NaNs were produced in "pchisq(q,
> df, lower.tail, log.p)"? Is there something wrong with my data (there are no NAs in the
> data)?
>
> MetaAnalyse <- meta.summaries(CoeffVector,
> 					StdErrorVector,
> 					method = "random")
>

It means that NaNs were produced.  You should be able to see where from 
the output. It's hard to say more without more information.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From pj.bell at yahoo.co.uk  Thu Aug 24 17:53:02 2006
From: pj.bell at yahoo.co.uk (Piet Bell)
Date: Thu, 24 Aug 2006 16:53:02 +0100 (BST)
Subject: [R] xyplot tick marks and line thickness
Message-ID: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/e5fbba41/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 24 18:02:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 12:02:02 -0400
Subject: [R] Why are lagged correlations typically negative?
In-Reply-To: <77C117DE9E87354CB7804A2DE7D566C2E1AC4E@amedmlermc132>
References: <77C117DE9E87354CB7804A2DE7D566C2E1AC4E@amedmlermc132>
Message-ID: <971536df0608240902v366696d3x2aa84a57d3de83f5@mail.gmail.com>

The covariance has the same sign as the
correlation so lets calculate the sample covariance
of the vector T1 = (X,Y) with T2 = (Y,Z) where we ignored
the third component in each case due to use="complete".

	cov(T1, T2) = XY + YZ - (X+Y)/2 * (Y+Z)/2

X, Y and Z are random variables so we take the
expectation to get the overall average over many
runs.  Expectation is linear and all the random
variables are uncorrelated so:

	EXY + EYZ - E[(X+Y)/2 * (Y+Z)/2]
	= EXY + EYZ - EXY/4 - EXZ/4 - EYY/4 - EYZ/4
	= -EYY/4
	< 0

where the third line is due to the fact that
all terms in the second line except the surviving
term are zero.


On 8/24/06, Bliese, Paul D LTC USAMH <paul.bliese at us.army.mil> wrote:
> Recently, I was working with some lagged designs where a vector of
> observations at one time was used to predict a vector of observations at
> another time using a lag 1 design.  In the work, I noticed a lot of
> negative correlations, so I ran a simple simulation with 2 matched
> points.  The crude simulation example below shows that the correlation
> can be -1 or +1, but interestingly if you do this basic simulation
> thousands of times, you get negative correlations 66 to 67% of the time.
> If you simulate three matched observations instead of three you get
> negative correlations about 74% of the time and then as you simulate 4
> and more observations the number of negative correlations asymptotically
> approaches an equal 50% for negative versus positive correlations
> (though then with 100 observations one has 54% negative correlations).
> Creating T1 and T2 so they are related (and not correlated 1 as in the
> crude simulation) attenuates the effect.  A more advanced simulation is
> provided below for those interested.
>
> Can anyone explain why this occurs in a way a non-mathematician is
> likely to understand?
>
> Thanks,
>
> Paul
>
> #############
> # Crude simulation
> #############
> > (T1<-rnorm(3))
> [1] -0.1594703 -1.3340677  0.2924988
> > (T2<-c(T1[2:3],NA))
> [1] -1.3340677  0.2924988         NA
> > cor(T1,T2, use="complete")
> [1] -1
>
> > (T1<-rnorm(3))
> [1] -0.84258593 -0.49161602  0.03805543
> > (T2<-c(T1[2:3],NA))
> [1] -0.49161602  0.03805543          NA
> > cor(T1,T2, use="complete")
> [1] 1
>
> ###########
> # More advanced simulation example
> ###########
> > lags
> function(nobs,nreps,rho=1){
> OUT<-data.frame(NEG=rep(NA,nreps),COR=rep(NA,nreps))
> nran<-nobs+1  #need to generate 1 more random number than there are
> observations
>  for(i in 1:nreps){
>      V1<-rnorm(nran)
>      V2<-sqrt(1-rho^2)*rnorm(nran)+rho*V1
>      #print(cor(V1,V2))
>      V1<-V1[1:nran-1]
>      V2<-V2[2:nran]
>      OUT[i,1]<-ifelse(cor(V1,V2)<=0,1,0)
>      OUT[i,2]<-cor(V1,V2)
>  }
> return(OUT) #out is a 1 if the corr is negative or 0; 0 if positive
> }
> > LAGS.2<-lags(2,10000)  #Number of observations matched = 2
> > mean(LAGS.2)
>    NEG     COR
>  0.6682 -0.3364
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From petr.pikal at precheza.cz  Thu Aug 24 17:46:38 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 24 Aug 2006 17:46:38 +0200
Subject: [R] help: trouble using lines()
In-Reply-To: <1307.144.173.76.117.1156433697.squirrel@www.webmail.ex.ac.uk>
References: <44EDD346.30308.EFF755@localhost>
Message-ID: <44EDE5FE.16683.13912CA@localhost>

Hi

I have no experience with lmer and its plotting method. However If it 
uses plain (not grid) graphics you maybe shall consult abline and/or 
segments.

If it uses grid, you shall consult panel.abline from lattice package.

BTW. Better to copy your answer every time to the list as somebody 
from BigBoys can definitely be able to answer questions with more 
intuition than myself.

HTH
Petr

On 24 Aug 2006 at 16:34, Simon Pickett wrote:

Date sent:      	Thu, 24 Aug 2006 16:34:57 +0100 (BST)
Subject:        	Re: [R] help: trouble using lines()
From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>

> Hi,
> I have a model with a three way interaction
> f2<- lmer(late.growth ~ mtf+year+treat+hatch.day+ hatch.day:year+
> hatch.day:treat+ mtf:treat+ treat:year+ year:treat:mtf+(1|fybrood),
> data = A) (one continuous and two factors) and so I wanted to plot the
> fitted values of the model for each of the two factor levels. Copying
> the technique of the pdf I mentioned "Linear mixed models in R" by
> S?ren Feodor Nielsen 20003.
> http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-mod
> els.pdf#search=%22Linear%20mixed%20models%20in%20R%22. I plotted the
> fitted values against the continuous variable. Now I plan to put lines
> on the existing plot for each of the combinations of the factors I
> have. where tmp[1] and tmp[2] are coefficients from the model... I was
> under the impression that lines(y,exp(tmp[1]+tmp[2])) would give me a
> line, but it doesnt work... thanks again
> 
> 
> > Hi
> >
> > from lines help page
> >
> > x, y coordinate vectors of points to join.
> >
> > and lines or points simply adds lines or points to existing plot.
> > What do you want to plot with lines?
> >
> > HTH
> > Petr
> >
> >
> >
> > On 24 Aug 2006 at 14:52, Simon Pickett wrote:
> >
> > Date sent:      	Thu, 24 Aug 2006 14:52:09 +0100 (BST)
> > From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
> > To:             	"Petr Pikal" <petr.pikal at precheza.cz>
> > Copies to:      	r-help at stat.math.ethz.ch, Simon Pickett
> > <s.pickett at exeter.ac.uk>
> > Subject:        	Re: [R] help: trouble using lines()
> >
> >> Hi, thanks for replying.
> >> No, there arent any NA's in the original data set....
> >> I think I must be mis-interpreting the use of lines()?
> >> in the example what exactly is "y"?
> >> lines(y,exp(tmp[1]+tmp[2]))
> >> In my case tmp[1] and tmp[2] are coeficients from the model so just
> >> one number (not a vector) and I havent specified y.... Thanks
> >> everyone, Simon
> >>
> >>
> >> > Hi
> >> >
> >> > not sure but are there some NA values in your data?
> >> >
> >> > what
> >> >
> >> > length(mtf)
> >> > and
> >> > length(fitted(f2))
> >> >
> >> > tells you?
> >> >
> >> > And you need not to use assignment
> >> >
> >> > graph1 <- plot(....)
> >> >
> >> > to output a plot on screen.
> >> >
> >> >
> >> > HTH
> >> > Petr
> >> >
> >> >
> >> > On 24 Aug 2006 at 13:43, Simon Pickett wrote:
> >> >
> >> > Date sent:      	Thu, 24 Aug 2006 13:43:18 +0100 (BST)
> >> > From:           	"Simon Pickett" <S.Pickett at exeter.ac.uk>
> >> > To:             	R-help at stat.math.ethz.ch
> >> > Subject:        	[R] help: trouble using lines()
> >> >
> >> >> Hi R experts,
> >> >> I have been using ReML as follows...
> >> >> model<-lmer(late.growth~mtf+year+treat+hatch.day+hatch.day:year+
> >> >> hat ch. day:treat+ mtf:treat+ treat:year+
> >> >> year:treat:mtf+(1|fybrood), data > A) then I wanted to plot the
> >> >> results of the three way interaction using lines() as follows...
> >> >>
> >> >> tmp<-as.vector(fixef(model))
> >> >> graph1<-plot(mtf,fitted(f2), xlab=list("Brood Size"),
> >> >> ylab=list("Early growth rate"), pch=16, col="darkgrey",
> >> >> bg="yellow") lines(y,exp(tmp[1]+tmp[2]))
> >> >>
> >> >> but no matter what I try I always get the error message
> >> >> "Error in xy.coords(x, y) : 'x' and 'y' lengths differ"
> >> >>
> >> >> Can anyone shed some light please?
> >> >> I am basically copying the methods of the pdf entitled "Linear
> >> >> mixed models in R" by S?ren Feodor Nielsen 20003.
> >> >> http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mix
> >> >> ed- mod els.pdf#search=%22Linear%20mixed%20models%20in%20R%22
> >> >>
> >> >>
> >> >>
> >> >>
> >> >>
> >> >> Simon Pickett
> >> >> PhD student
> >> >> Centre For Ecology and Conservation
> >> >> Tremough Campus
> >> >> University of Exeter in Cornwall
> >> >> TR109EZ
> >> >> Tel 01326371852
> >> >>
> >> >> ______________________________________________
> >> >> R-help at stat.math.ethz.ch mailing list
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html and provide
> >> >> commented, minimal, self-contained, reproducible code.
> >> >
> >> > Petr Pikal
> >> > petr.pikal at precheza.cz
> >> >
> >> >
> >>
> >>
> >> Simon Pickett
> >> PhD student
> >> Centre For Ecology and Conservation
> >> Tremough Campus
> >> University of Exeter in Cornwall
> >> TR109EZ
> >> Tel 01326371852
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html and provide commented,
> >> minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 

Petr Pikal
petr.pikal at precheza.cz


From murdoch at stats.uwo.ca  Thu Aug 24 18:01:55 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 24 Aug 2006 12:01:55 -0400
Subject: [R] rgl: exporting to pdf or png does not work
In-Reply-To: <Pine.LNX.4.62.0608241642490.23380@biomath.ugent.be>
References: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>	<44ECDC27.6080109@stats.uwo.ca>
	<Pine.LNX.4.62.0608241642490.23380@biomath.ugent.be>
Message-ID: <44EDCD73.4090805@stats.uwo.ca>

On 8/24/2006 11:19 AM, Gaspard Lequeux wrote:
> Hej,
> 
> On Wed, 23 Aug 2006, Duncan Murdoch wrote:
> 
>> On 8/23/2006 5:15 PM, Gaspard Lequeux wrote:
>>
>>> When exporting a image from rgl, the following error is encountered:
>>>
>>>> rgl.postscript('testing.pdf', fmt="pdf")
>>> RGL: ERROR: can't bind glx context to window
>>> RGL: ERROR: can't bind glx context to window
>>> Warning messages:
>>> 1: X11 protocol error: GLXBadContextState
>>> 2: X11 protocol error: GLXBadContextState
>>>
>>> The pdf file is created and is readable, but all the labels are gone.
>>>
>>> Taking a snapshot (to png) gives 'failed' and no file is created.
>>>
>>> Version of rgl used: 0.67-2 (2006-07-11)
>>> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
>>> Running Debian GNU/Linux testing (Etch).
>>
>> That looks like an X11 error to me, not something that I'm very likely
>> to be able to fix.  If you can debug the error, it would be helpful.
> 
> Actually after upgrading everything (debian testing (etch)) and restarting 
> X, I didn't get that error anymore. It was worse: R crashed:
> 
>> library(rgl);triangles3d(c(1,,2,3),c(1,2,4),c(1,3,5));rgl.postscript('testing.pdf','pdf')
> X Error of failed request:  GLXBadContextState
>    Major opcode of failed request:  142 (GLX)
>    Minor opcode of failed request:  5 (X_GLXMakeCurrent)
>    Serial number of failed request:  85
>    Current serial number in output stream:  85
> glequeux at toidi:~/seqanal$
> 
> 
> I downloaded the source package (debian testing (etch), rgl-0.67-2).
> 
> In rgl-0.67-2/src/ I changed the following files:
> 
> rglview.cpp, around line 587. Commenting the function call gl2psBeginPage 
> removed the crash (but also no pdf output...)
> 
> I enabled this function again and went to gl2ps.c, to the function 
> gl2psBeginPage. At the end of that function, around line 4426, commenting 
> out the line
> glRenderMode(GL_FEEDBACK);
> removes the R crash, but of course still no pdf output (well, only the 
> background).
> 
> GL_FEEDBACK is defined in /usr/include/GL/gl.h as:
> 
> /* Render Mode */
> #define GL_FEEDBACK				0x1C01
> #define GL_RENDER				0x1C00
> #define GL_SELECT				0x1C02
> 
> Trying glRenderMode(GL_RENDER) removed the crash, but still only the 
> background in the pdf.
> 
> If someone has some suggestions about what to do next...

gl2ps is a separate project, whose source has been included into rgl. 
You can see the gl2ps project page at http://www.geuz.org/gl2ps/.

We're using version 1.2.2, which is a couple of years old.  The current 
stable release of gl2ps is 1.3.1.  It might fix your problem.

I don't know if we modified gl2ps.c or gl2ps.h when they were included, 
but they haven't been modified since.  (Daniel put them in, based on a 
patch from Albrecht Gebhardt, according to the log.)

It would be helpful to know:

1.  Is the rgl source identical to 1.2.2?
2.  Does rgl work if 1.3.1 is dropped in instead?
3.  Does 1.3.1 fix the bug you're seeing?

I'll look into these at some point, but probably not this week.

Duncan Murdoch


From mschwartz at mn.rr.com  Thu Aug 24 18:11:59 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 24 Aug 2006 11:11:59 -0500
Subject: [R] installing the x86_64 R Binary on Fedora Core 5
In-Reply-To: <1db726800608240850g71ec69c6va56b50e6b836e3f3@mail.gmail.com>
References: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
	<44EDC41A.9000008@gmx.net>
	<1db726800608240850g71ec69c6va56b50e6b836e3f3@mail.gmail.com>
Message-ID: <1156435920.4115.19.camel@localhost.localdomain>

Roger,

The Windows packages will not run on Linux. You will need to install
them using the Linux versions (.tar.gz files) of the CRAN packages.   

You can copy them from a CRAN mirror to your USB drive and then install
them locally using 

  R CMD INSTALL PackageName.tar.gz

This will again need to be done as root using 'su' from the console.

Note that depending upon the nature of the package (ie. does it uses C
and/or FORTRAN code or require third party libraries), you may get
errors during the installation process if you are missing required
code/applications/header files.

HTH,

Marc

On Thu, 2006-08-24 at 11:50 -0400, roger bos wrote:
> Thanks so much Marc & Stefan!  The GUI wasn't telling me what I was
> missing.  The terminal told me I was missing tk-8.4.12-1.2.x86_64.rpm so I
> went and got that and it installed without errors.  Then I could't figure
> out how to launch R through the GUI so I went back to the terminal and typed
> "R" and it launched.  I have a lot to learn but I want to thank you so much
> for getting me started.
> 
> Another question:
> As I said, I don't have internet access on the linux machine, so is there
> any way to copy the library folder from my windows machine to the linux box
> and install the packages from there?
> 
> Roger
> 
> 
> 
> 
> On 8/24/06, Stefan Grosse <singularitaet at gmx.net> wrote:
> >
> > Hi Roger,
> >
> > I dunno what exactly might be the source of that mistake but I would
> > strongly recommend to install R while you are online. Often other
> > packages must be installed for dependencies.
> >
> > (And then I recommend using the smart package manager (
> > http://labix.org/smart ) which is a great tool and handling dependencies
> > better.)
> >
> > You could use rpm at the command line level.
> >
> > open a shell, type su and give your password, change to the folder where
> > the rpm is downloaded to, type
> > rpm -ivh R-2.3.1-1.fc5.x86_64.rpm  will try to install and give you
> > information on whats missing...
> >
> > Stefan Grosse
> >
> > roger bos schrieb:
> > > I am looking for help install the x86_64 R Binary onto my FC5
> > machine.  At
> > > the risk of subjecting myself to tons of criticism, I must confess that
> > I
> > > don't know anything about Linux and I have never compiled R from source.
> > > Therefore, I choose FC5 because I see that a 64-bit binary is already
> > > available.
> > >
> > > Here is what I tried:  I installed FC5 with all options (productivity,
> > > software development, and web server).  FC5 boots up fine.  I downloaded
> > all
> > > the R binary files in that FC5 directory to my USB drive and copied
> > > them onto my linux machine (where I don't yet have internet access).  I
> > > created a folder in my rbos's Home directory and copied the files
> > there.  I
> > > clicked on the R-2.3.1-1.fc5.x86_64.rpm and right-clicked to choose
> > 'open
> > > with "install software"'  It asked me for my root password.  I put in
> > the
> > > same root password I choose when I installed FC5.  I get a "installing
> > > packages" screen that shows the R filename and I click Apply.  It then
> > give
> > > me an Error: "Unable to retrieve software information".
> > >
> > > Can anyone tell me what steps I am missing?  The R install guide states
> > that
> > > binary installs are platform specific so it only considers building from
> > the
> > > sources.  I look forward to learning a lot about Linux and using more
> > than
> > > just the GUI, but to get started, I just want to learn how to install a
> > > binary of R.
> > >
> > > Thanks so much,
> > >
> > > Roger
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From msubianto at gmail.com  Thu Aug 24 18:23:12 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Thu, 24 Aug 2006 18:23:12 +0200
Subject: [R] How to compare rows of two matrices
In-Reply-To: <c7c17cef0608240303j6f2633d2v45403f825851b8dc@mail.gmail.com>
References: <c7c17cef0608240303j6f2633d2v45403f825851b8dc@mail.gmail.com>
Message-ID: <44EDD270.8080201@gmail.com>

Dear Stephen C. Upton & Petr Pikal
Thank you both very much for the suggestions!

Best wishes, Muhammad Subianto

On this day 24/08/2006 12:03, Muhammad Subianto wrote:
> Dear all,
> I have a dataset
> train <- cbind(c(0,2,2,1,0), c(8,9,4,0,2), 6:10, c(-1, 1, 1, -1, 1))
> test <- cbind(1:5, c(0,1,5,1,3), c(1,1,2,0,3) ,c(1, 1, -1, 1, 1))
>
> I want to find which rows of train and test it different in its last
> column (column 4).
> The solution must be something like
>
> train
>     [,1] [,2] [,3] [,4]
> [1,]    0    8    6   -1
> [3,]    2    4    8    1
> [4,]    1    0    9   -1
>
>
> test
>     [,1] [,2] [,3] [,4]
> [1,]    1    0    1    1
> [3,]    3    5    2   -1
> [4,]    4    1    0    1
>
> I have tried with
> matrix(train %in% test, dim(train))
> apply(train, 1, paste, collapse="") %in% apply(test, 1, paste, 
> collapse="")
>
> It doesn't work.
> How can I do.
> Thanks for any help.
>
> Best, Muhammad Subianto
>


From Roger.Bivand at nhh.no  Thu Aug 24 18:28:49 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 24 Aug 2006 18:28:49 +0200 (CEST)
Subject: [R] Intro to Programming R Book
In-Reply-To: <44EDC7EA.3060208@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.44.0608241821110.17874-100000@reclus.nhh.no>

On Thu, 24 Aug 2006, Joerg van den Hoff wrote:

> Raphael Fraser wrote:
> > I am new to R and am looking for a book that can help in learning to
> > program in R. I have looked at the R website suggested books but I am
> > still not sure which book best suite my needs. I am interesting in
> > programming, data manipulation not statistics. Any suggestions?

If you know German, then Uwe Ligges' 2005 book "Programmieren mit R" may
be what you are looking for. Some German-speakers I was teaching found it
very useful.

http://www.springeronline.com/sgw/cda/frontpage/0,,1-40109-22-26682866-0,00.html

> > 
> > Raphael
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> "S Programming" by Venables and Ripley (Springer) seems the only(?) one 
> around targeting the language, not it's applications. luckily, it's very 
> good. for the rest (things specific to R, e.g. package development, 
> namespaces etc.) I think one can only resort to the R manuals .
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From katrin.heinrichs at zmaw.de  Thu Aug 24 18:28:37 2006
From: katrin.heinrichs at zmaw.de (Anne Katrin Heinrichs)
Date: Thu, 24 Aug 2006 18:28:37 +0200
Subject: [R] metaplot and meta.summaries
In-Reply-To: <Pine.LNX.4.64.0608240842300.30960@homer21.u.washington.edu>
Message-ID: <000001c6c79a$59f2e760$20d2ac88@fnu.zmaw.de>

Thanks!

Sorry, I forgot to add the output of meta.summaries:

Random-effects meta-analysis
Call: meta.summaries(d = CoeffVector, se = StdErrorVector, 
	method = "random", 
    	logscale = FALSE)
Summary effect=NaN   95% CI (NaN, NaN)

If anyone has further hints, what the problem is, that would be great. I've found out by
now that STATA gives me a result for my data, if that's any help.

Best regards,
Katrin


> > meta.summaries
> > --------------
> >
> > What does it mean, when I get the warning message that NaNs were 
> > produced in "pchisq(q, df, lower.tail, log.p)"? Is there something 
> > wrong with my data (there are no NAs in the data)?
> >
> > MetaAnalyse <- meta.summaries(CoeffVector,
> > 					StdErrorVector,
> > 					method = "random")
> >
> 
> It means that NaNs were produced.  You should be able to see 
> where from the output. It's hard to say more without more information.


From gattuso2 at obs-vlfr.fr  Thu Aug 24 18:30:39 2006
From: gattuso2 at obs-vlfr.fr (Gattuso, Jean-Pierre)
Date: Thu, 24 Aug 2006 18:30:39 +0200
Subject: [R] Lattice symbol size and legend margins
Message-ID: <44EDD42F.1010005@obs-vlfr.fr>

Hi:

I am using the following command:

xyplot(dat6$CO3*1e6 ~ dat6$irradiance, data=dat6, group=ref,
	xlab=list(label=expression(paste("Irradiance (", mu, "mol photons", 
m^"-2", " ", s^"-1", ")")), cex=1.3),
	ylab=list(label=expression(paste("Carbonate concentration (x ", 10^"6", 
" ", kg^"-1", ")")) , cex=1.3),
	scales = list(x = list(cex=1.1), y = list(cex=1.1)),
	type="p",
	trellis.par.set(plot.symbol = list(cex=2)),
	layout=c(1, 1),
	aspect=1,
	auto.key = list(cex=1.2, between=4, space="right", border=T)
	)

And get the following error:

	Error in multiple && !outer : invalid 'x' type in 'x && y'

I know it comes from the trellis.par.set command: I am unable to find 
the right syntax.

A second question: I am adjusting the left and right margins between the 
text of the legend and the border using "between". How can I control the 
top and bottom margins?

Thanks,

Jean-Pierre Gattuso


From ripley at stats.ox.ac.uk  Thu Aug 24 18:51:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Aug 2006 17:51:01 +0100 (BST)
Subject: [R] installing the x86_64 R Binary on Fedora Core 5
In-Reply-To: <1db726800608240850g71ec69c6va56b50e6b836e3f3@mail.gmail.com>
References: <1db726800608240753l6a0859b8g75b53ffba7e0c702@mail.gmail.com>
	<44EDC41A.9000008@gmx.net>
	<1db726800608240850g71ec69c6va56b50e6b836e3f3@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608241745110.10572@gannet.stats.ox.ac.uk>

On Thu, 24 Aug 2006, roger bos wrote:

> Thanks so much Marc & Stefan!  The GUI wasn't telling me what I was
> missing.  The terminal told me I was missing tk-8.4.12-1.2.x86_64.rpm so I
> went and got that and it installed without errors.  Then I could't figure
> out how to launch R through the GUI so I went back to the terminal and typed
> "R" and it launched.  I have a lot to learn but I want to thank you so much
> for getting me started.
> 
> Another question:
> As I said, I don't have internet access on the linux machine, so is there
> any way to copy the library folder from my windows machine to the linux box
> and install the packages from there?

No, but what you can do on Windows is use 
download.packages(..., type="source"), and copy the folder into which you 
downloaded to your Linux box.

To avoid continually needing root access, I would do

cd ~
mkdir Rlibrary
cat > .Renviron
~/Rlibrary
^D

cd folder-with-downloaded-sources
foreach f (*.tar.gz)
R CMD INSTALL -l ~/Rlibrary $f
end

The next time you start R the installed packages should be available to 
you.


> 
> Roger
> 
> 
> 
> 
> On 8/24/06, Stefan Grosse <singularitaet at gmx.net> wrote:
> >
> > Hi Roger,
> >
> > I dunno what exactly might be the source of that mistake but I would
> > strongly recommend to install R while you are online. Often other
> > packages must be installed for dependencies.
> >
> > (And then I recommend using the smart package manager (
> > http://labix.org/smart ) which is a great tool and handling dependencies
> > better.)
> >
> > You could use rpm at the command line level.
> >
> > open a shell, type su and give your password, change to the folder where
> > the rpm is downloaded to, type
> > rpm -ivh R-2.3.1-1.fc5.x86_64.rpm  will try to install and give you
> > information on whats missing...
> >
> > Stefan Grosse
> >
> > roger bos schrieb:
> > > I am looking for help install the x86_64 R Binary onto my FC5
> > machine.  At
> > > the risk of subjecting myself to tons of criticism, I must confess that
> > I
> > > don't know anything about Linux and I have never compiled R from source.
> > > Therefore, I choose FC5 because I see that a 64-bit binary is already
> > > available.
> > >
> > > Here is what I tried:  I installed FC5 with all options (productivity,
> > > software development, and web server).  FC5 boots up fine.  I downloaded
> > all
> > > the R binary files in that FC5 directory to my USB drive and copied
> > > them onto my linux machine (where I don't yet have internet access).  I
> > > created a folder in my rbos's Home directory and copied the files
> > there.  I
> > > clicked on the R-2.3.1-1.fc5.x86_64.rpm and right-clicked to choose
> > 'open
> > > with "install software"'  It asked me for my root password.  I put in
> > the
> > > same root password I choose when I installed FC5.  I get a "installing
> > > packages" screen that shows the R filename and I click Apply.  It then
> > give
> > > me an Error: "Unable to retrieve software information".
> > >
> > > Can anyone tell me what steps I am missing?  The R install guide states
> > that
> > > binary installs are platform specific so it only considers building from
> > the
> > > sources.  I look forward to learning a lot about Linux and using more
> > than
> > > just the GUI, but to get started, I just want to learn how to install a
> > > binary of R.
> > >
> > > Thanks so much,
> > >
> > > Roger
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From msubianto at gmail.com  Thu Aug 24 18:59:22 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Thu, 24 Aug 2006 18:59:22 +0200
Subject: [R] Check values in colums matrix
Message-ID: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>

Dear all,
I apologize if my question is quite simple.
I have a dataset (20 columns & 1000 rows) which
some of columns have the same value and the others
have different values.
Here are some piece of my dataset:
obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
             c(0,1,1,4,1,0,1,4,-1),
             c(1,1,1,4,2,0,1,4,-1),
             c(1,1,1,4,3,0,1,4,-1),
             c(1,1,1,4,6,0,1,5,-1),
             c(1,1,1,4,6,0,1,6,-1),
             c(1,1,1,4,6,0,1,7,-1),
             c(1,1,1,4,6,0,1,8,-1))
obj.tr <- t(obj)
obj.tr
> obj.tr
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]    1    1    1    4    0    0    1    4   -1
[2,]    0    1    1    4    1    0    1    4   -1
[3,]    1    1    1    4    2    0    1    4   -1
[4,]    1    1    1    4    3    0    1    4   -1
[5,]    1    1    1    4    6    0    1    5   -1
[6,]    1    1    1    4    6    0    1    6   -1
[7,]    1    1    1    4    6    0    1    7   -1
[8,]    1    1    1    4    6    0    1    8   -1
>

How can I do to check columns 2,3,4,6,7 and 9 have
the same value, and columns 1,5 and 8 have different values.

Best, Muhammad Subianto


From ccleland at optonline.net  Thu Aug 24 19:00:31 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 24 Aug 2006 13:00:31 -0400
Subject: [R] xyplot tick marks and line thickness
In-Reply-To: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
Message-ID: <44EDDB2F.7010907@optonline.net>

Piet Bell wrote:
> Hello,
>   A made a xyplot using the lattice library in R (latest version).
>    
>   The publisher of our paper has requested:
>    
>   1. all tick marks should point inwards instead of outwards.
>    
>   2. All lines should be thicker (lines, axes, boxes, etc. Everything). Lines is easy...I used:  lwd=1.5   but what about the lines of the axes, and the lines that build up the plot itself?....?
>    
>   Any suggestions?

library(lattice)

trellis.device()

# to find all components with lwd setting
# names(trellis.par.get()[grep("lwd", trellis.par.get())])

trellis.par.set(
  add.line = list(lwd=1.5),
  plot.polygon = list(lwd=1.5),
  box.rectangle = list(lwd=1.5),
  box.umbrella = list(lwd=1.5),
  dot.line = list(lwd=1.5),
  plot.line = list(lwd=1.5),
  reference.line = list(lwd=1.5),
  strip.border = list(lwd=1.5),
  superpose.line = list(lwd=1.5),
  superpose.polygon = list(lwd=1.5),
  axis.line = list(lwd=1.5),
  box.3d = list(lwd=1.5))

xyplot(rnorm(5) ~ 1:5, type = "b",
       scales = list(x = list(tck = -1), y = list(tck = -1)))

>   Kind regards,
>    
>   Piet Bell
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ggrothendieck at gmail.com  Thu Aug 24 19:02:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 13:02:00 -0400
Subject: [R] xyplot tick marks and line thickness
In-Reply-To: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
Message-ID: <971536df0608241002j5704714ch77d9d83aeb40840f@mail.gmail.com>

Look through the output of trellis.par.get() for the right parameters
or when all else fails use grid (which we use below for the
box around the panel since I could not locate the parameter):

library(lattice)
library(grid)
x <- 1:12
g <- gl(3,4)
lwd <- 3
xyplot(x ~ x | g, type = "l", lwd = lwd,
	scales = list(tck = -1, lwd = lwd),
	par.settings = list(add.text = list(lwd = lwd),
		strip.border = list(lwd = lwd)),
	panel = function(...) {
		grid.rect(gp = gpar(lwd = lwd))
		panel.xyplot(...)
	}
)


On 8/24/06, Piet Bell <pj.bell at yahoo.co.uk> wrote:
> Hello,
>  A made a xyplot using the lattice library in R (latest version).
>
>  The publisher of our paper has requested:
>
>  1. all tick marks should point inwards instead of outwards.
>
>  2. All lines should be thicker (lines, axes, boxes, etc. Everything). Lines is easy...I used:  lwd=1.5   but what about the lines of the axes, and the lines that build up the plot itself?....?
>
>  Any suggestions?
>
>  Kind regards,
>
>  Piet Bell
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Aug 24 19:05:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 13:05:32 -0400
Subject: [R] Lattice symbol size and legend margins
In-Reply-To: <44EDD42F.1010005@obs-vlfr.fr>
References: <44EDD42F.1010005@obs-vlfr.fr>
Message-ID: <971536df0608241005t16c5db0cs41e46631945fa871@mail.gmail.com>

Try par.settings=
You can find examples via:
    RSiteSearch("par.settings") .

On 8/24/06, Gattuso, Jean-Pierre <gattuso2 at obs-vlfr.fr> wrote:
> Hi:
>
> I am using the following command:
>
> xyplot(dat6$CO3*1e6 ~ dat6$irradiance, data=dat6, group=ref,
>        xlab=list(label=expression(paste("Irradiance (", mu, "mol photons",
> m^"-2", " ", s^"-1", ")")), cex=1.3),
>        ylab=list(label=expression(paste("Carbonate concentration (x ", 10^"6",
> " ", kg^"-1", ")")) , cex=1.3),
>        scales = list(x = list(cex=1.1), y = list(cex=1.1)),
>        type="p",
>        trellis.par.set(plot.symbol = list(cex=2)),
>        layout=c(1, 1),
>        aspect=1,
>        auto.key = list(cex=1.2, between=4, space="right", border=T)
>        )
>
> And get the following error:
>
>        Error in multiple && !outer : invalid 'x' type in 'x && y'
>
> I know it comes from the trellis.par.set command: I am unable to find
> the right syntax.
>
> A second question: I am adjusting the left and right margins between the
> text of the legend and the border using "between". How can I control the
> top and bottom margins?
>
> Thanks,
>
> Jean-Pierre Gattuso
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pburns at pburns.seanet.com  Thu Aug 24 19:43:45 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 24 Aug 2006 18:43:45 +0100
Subject: [R] Intro to Programming R Book
In-Reply-To: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
References: <509bb6a90608240509s2c27866dw41d3f00a3603ac4f@mail.gmail.com>
Message-ID: <44EDE551.6030303@pburns.seanet.com>

S Poetry may be of use to you.  Some things are now
out-of-date and some things are wrong for R, but mostly
it's right.  And the price is right.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Raphael Fraser wrote:

>I am new to R and am looking for a book that can help in learning to
>program in R. I have looked at the R website suggested books but I am
>still not sure which book best suite my needs. I am interesting in
>programming, data manipulation not statistics. Any suggestions?
>
>Raphael
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From deepayan.sarkar at gmail.com  Thu Aug 24 20:19:26 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 24 Aug 2006 13:19:26 -0500
Subject: [R] xyplot tick marks and line thickness
In-Reply-To: <971536df0608241002j5704714ch77d9d83aeb40840f@mail.gmail.com>
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
	<971536df0608241002j5704714ch77d9d83aeb40840f@mail.gmail.com>
Message-ID: <eb555e660608241119j7438ac0cs12ab04f7edb7d171@mail.gmail.com>

On 8/24/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Look through the output of trellis.par.get() for the right parameters
> or when all else fails use grid (which we use below for the
> box around the panel since I could not locate the parameter):
>
> library(lattice)
> library(grid)
> x <- 1:12
> g <- gl(3,4)
> lwd <- 3
> xyplot(x ~ x | g, type = "l", lwd = lwd,
>         scales = list(tck = -1, lwd = lwd),
>         par.settings = list(add.text = list(lwd = lwd),
>                 strip.border = list(lwd = lwd)),
>         panel = function(...) {
>                 grid.rect(gp = gpar(lwd = lwd))
>                 panel.xyplot(...)
>         }
> )

Right. the grid call shouldn't be necessary, "axis.line" controls the
panel borders. And tck can be a vector, to get rid of the ugly bumps
on top:

xyplot(x ~ x | g, type = "l", lwd = lwd,
       scales = list(tck = c(-1, 0)),
       par.settings =
       list(axis.line = list(lwd = lwd),
            strip.border = list(lwd = lwd)))

-Deepayan

>
>
> On 8/24/06, Piet Bell <pj.bell at yahoo.co.uk> wrote:
> > Hello,
> >  A made a xyplot using the lattice library in R (latest version).
> >
> >  The publisher of our paper has requested:
> >
> >  1. all tick marks should point inwards instead of outwards.
> >
> >  2. All lines should be thicker (lines, axes, boxes, etc. Everything). Lines is easy...I used:  lwd=1.5   but what about the lines of the axes, and the lines that build up the plot itself?....?
> >
> >  Any suggestions?
> >
> >  Kind regards,
> >
> >  Piet Bell


From info at lancashireclubbers.co.uk  Thu Aug 24 20:21:43 2006
From: info at lancashireclubbers.co.uk (Clubbing-UnknownLocations)
Date: Thu, 24 Aug 2006 19:21:43 +0100
Subject: [R] forum.LancashireClubbers.co.uk - LAUNCH
Message-ID: <20060824192045.81974347@lancashireclubbers.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/11565482/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 24 20:25:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 14:25:56 -0400
Subject: [R] xyplot tick marks and line thickness
In-Reply-To: <44EDDB2F.7010907@optonline.net>
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
	<44EDDB2F.7010907@optonline.net>
Message-ID: <971536df0608241125jdc74da1u4718b23747b10e11@mail.gmail.com>

Here is a way to automate finding the lwd= parameters.

library(lattice)

# test data
x <- 1:12
g <- gl(3, 4)
lwd <- 3

# set parameters
par <- trellis.par.get()
par <- lapply(par, function(x) replace(x, names(x) == "lwd", lwd))
xyplot(x ~ x | g, type = "l", par.settings = par)


On 8/24/06, Chuck Cleland <ccleland at optonline.net> wrote:
> Piet Bell wrote:
> > Hello,
> >   A made a xyplot using the lattice library in R (latest version).
> >
> >   The publisher of our paper has requested:
> >
> >   1. all tick marks should point inwards instead of outwards.
> >
> >   2. All lines should be thicker (lines, axes, boxes, etc. Everything). Lines is easy...I used:  lwd=1.5   but what about the lines of the axes, and the lines that build up the plot itself?....?
> >
> >   Any suggestions?
>
> library(lattice)
>
> trellis.device()
>
> # to find all components with lwd setting
> # names(trellis.par.get()[grep("lwd", trellis.par.get())])
>
> trellis.par.set(
>  add.line = list(lwd=1.5),
>  plot.polygon = list(lwd=1.5),
>  box.rectangle = list(lwd=1.5),
>  box.umbrella = list(lwd=1.5),
>  dot.line = list(lwd=1.5),
>  plot.line = list(lwd=1.5),
>  reference.line = list(lwd=1.5),
>  strip.border = list(lwd=1.5),
>  superpose.line = list(lwd=1.5),
>  superpose.polygon = list(lwd=1.5),
>  axis.line = list(lwd=1.5),
>  box.3d = list(lwd=1.5))
>
> xyplot(rnorm(5) ~ 1:5, type = "b",
>       scales = list(x = list(tck = -1), y = list(tck = -1)))
>
> >   Kind regards,
> >
> >   Piet Bell
> >
> >
> > ---------------------------------
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Aug 24 20:37:13 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 14:37:13 -0400
Subject: [R] xyplot tick marks and line thickness
In-Reply-To: <971536df0608241125jdc74da1u4718b23747b10e11@mail.gmail.com>
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
	<44EDDB2F.7010907@optonline.net>
	<971536df0608241125jdc74da1u4718b23747b10e11@mail.gmail.com>
Message-ID: <971536df0608241137g5f569783g43c3ead7638ac61d@mail.gmail.com>

That should read finding and setting.  Chuck already showed how
to find them.

On 8/24/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here is a way to automate finding the lwd= parameters.
>
> library(lattice)
>
> # test data
> x <- 1:12
> g <- gl(3, 4)
> lwd <- 3
>
> # set parameters
> par <- trellis.par.get()
> par <- lapply(par, function(x) replace(x, names(x) == "lwd", lwd))
> xyplot(x ~ x | g, type = "l", par.settings = par)
>
>
> On 8/24/06, Chuck Cleland <ccleland at optonline.net> wrote:
> > Piet Bell wrote:
> > > Hello,
> > >   A made a xyplot using the lattice library in R (latest version).
> > >
> > >   The publisher of our paper has requested:
> > >
> > >   1. all tick marks should point inwards instead of outwards.
> > >
> > >   2. All lines should be thicker (lines, axes, boxes, etc. Everything). Lines is easy...I used:  lwd=1.5   but what about the lines of the axes, and the lines that build up the plot itself?....?
> > >
> > >   Any suggestions?
> >
> > library(lattice)
> >
> > trellis.device()
> >
> > # to find all components with lwd setting
> > # names(trellis.par.get()[grep("lwd", trellis.par.get())])
> >
> > trellis.par.set(
> >  add.line = list(lwd=1.5),
> >  plot.polygon = list(lwd=1.5),
> >  box.rectangle = list(lwd=1.5),
> >  box.umbrella = list(lwd=1.5),
> >  dot.line = list(lwd=1.5),
> >  plot.line = list(lwd=1.5),
> >  reference.line = list(lwd=1.5),
> >  strip.border = list(lwd=1.5),
> >  superpose.line = list(lwd=1.5),
> >  superpose.polygon = list(lwd=1.5),
> >  axis.line = list(lwd=1.5),
> >  box.3d = list(lwd=1.5))
> >
> > xyplot(rnorm(5) ~ 1:5, type = "b",
> >       scales = list(x = list(tck = -1), y = list(tck = -1)))
> >
> > >   Kind regards,
> > >
> > >   Piet Bell
> > >
> > >
> > > ---------------------------------
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> >
> > --
> > Chuck Cleland, Ph.D.
> > NDRI, Inc.
> > 71 West 23rd Street, 8th floor
> > New York, NY 10010
> > tel: (212) 845-4495 (Tu, Th)
> > tel: (732) 512-0171 (M, W, F)
> > fax: (917) 438-0894
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From victor_gushchin at yahoo.co.uk  Thu Aug 24 20:51:56 2006
From: victor_gushchin at yahoo.co.uk (Victor Gushchin)
Date: Thu, 24 Aug 2006 18:51:56 +0000 (GMT)
Subject: [R] ca.po Pz test question
Message-ID: <20060824185156.44708.qmail@web25112.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060824/5781cfc7/attachment.pl 

From backer at psych.uib.no  Thu Aug 24 21:10:57 2006
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Thu, 24 Aug 2006 21:10:57 +0200
Subject: [R] Authoring a book
In-Reply-To: <44EDAF63.7060006@gmx.net>
References: <44EB32A5.7030508@psych.uib.no> <x2lkpeuwyj.fsf@viggo.kubism.ku.dk>
	<44EDA4E9.2050606@psych.uib.no> <44EDAF63.7060006@gmx.net>
Message-ID: <44EDF9C1.5090107@psych.uib.no>

Stefan Grosse wrote:
> I think Peter Dalgaard is right.
> 
> Since you are able to use R I believe you will be very fast in learning
> LaTeX.
> 
> I think it needs less then a week to learn the most common LaTeX
> commands. And setting up a wiki and trying then to convert this into a
> printable document format plus learning the wiki syntax is probably more
> time consuming. Beside this R is able to work perfectly together with
> LaTeX, it creates LaTeX output and is doing excellent graphics in the
> EPS/PS format.
> 
> The best introduction for LaTeX is the not so short introduction:
> http://people.ee.ethz.ch/~oetiker/lshort/lshort.pdf

It really was a "not too short" intro.  I'll have a look at it.
> 
> If you still are not convinced have a look at UniWakkaWiki:
> http://uniwakka.sourceforge.net/HomePage
> 
> It is a Wiki for Science and University purposes and claims to be able
> to export to Openoffice as well as to LaTeX.

Looks interesting and I really like the concept, but how stable is it? 
  It looks rather fresh from the web page, but I may be wrong.  A 
bibliography function is really a big advantage, so ... perhaps.

Tom


From szhan at uoguelph.ca  Thu Aug 24 21:41:03 2006
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Thu, 24 Aug 2006 15:41:03 -0400
Subject: [R] how to constrast with factorial experiment
Message-ID: <20060824154103.gako7y7k040s8gs8@webmail.uoguelph.ca>

Hello, R users,
I have two factors (treat, section) anova design experiment where  
there are 3 replicates. The objective of the experiment is to test if  
there is significant difference of yield between top (section 9 to 11)  
and bottom (section 9 to 11) of the fruit tree under treatment. I  
found that there are interaction between two factors. I wonder if I  
can contrast means from levels of one factor (section) under another  
factor (treat)? if so, how to do it in R and how to interpret the  
output?
Here is the data and commands I used to test the differece between  
section 1 to 8 and 9 to 11 under treatment. But I don't know if I was  
right, how to interpret the out and whether there are significant  
difference between section 1 to 8 and section 9 to 11 under treatment.

yield	replicate	treat	section
35.55	1	Ctl	1
53.70	1	Ctl	2
42.79	1	Ctl	3
434.81	1	Ctl	4
705.96	1	Ctl	5
25.91	1	Ctl	6
57.53	1	Ctl	7
41.45	1	Ctl	8
85.54	1	Ctl	9
51.23	1	Ctl	10
188.24	1	Ctl	11
35.71	2	Ctl	1
45.15	2	Ctl	2
40.10	2	Ctl	3
312.76	2	Ctl	4
804.05	2	Ctl	5
28.22	2	Ctl	6
68.51	2	Ctl	7
46.15	2	Ctl	8
123.14	2	Ctl	9
33.78	2	Ctl	10
121.28	2	Ctl	11
30.96	3	Ctl	1
36.10	3	Ctl	2
47.19	3	Ctl	3
345.80	3	Ctl	4
644.61	3	Ctl	5
27.73	3	Ctl	6
56.63	3	Ctl	7
42.63	3	Ctl	8
61.25	3	Ctl	9
59.43	3	Ctl	10
109.87	3	Ctl	11
143.50	1	Trt	1
82.76	1	Trt	2
125.03	1	Trt	3
493.76	1	Trt	4
868.48	1	Trt	5
45.09	1	Trt	6
249.43	1	Trt	7
167.28	1	Trt	8
274.72	1	Trt	9
176.40	1	Trt	10
393.10	1	Trt	11
93.75	2	Trt	1
63.83	2	Trt	2
117.50	2	Trt	3
362.68	2	Trt	4
659.40	2	Trt	5
62.10	2	Trt	6
218.24	2	Trt	7
210.98	2	Trt	8
291.48	2	Trt	9
209.36	2	Trt	10
454.68	2	Trt	11
119.62	3	Trt	1
66.50	3	Trt	2
87.37	3	Trt	3
414.01	3	Trt	4
707.70	3	Trt	5
44.40	3	Trt	6
142.59	3	Trt	7
137.37	3	Trt	8
181.03	3	Trt	9
131.65	3	Trt	10
310.18	3	Trt	11

> dat1<-read.delim("c:/testcontr.txt", header=T)
> dat1$treat<-as.factor(dat1$treat)
> dat1$replicate<-as.factor(dat1$replicate)
> dat1$section<-as.factor(dat1$section)
> attach(dat1)
> obj<-lm(log2(yield)~treat*section)
> anova(obj)
Analysis of Variance Table

Response: log2(yield)
               Df Sum Sq Mean Sq  F value    Pr(>F)
treat          1 24.608  24.608 297.8649 < 2.2e-16 ***
section       10 99.761   9.976 120.7565 < 2.2e-16 ***
treat:section 10  6.708   0.671   8.1197 2.972e-07 ***
Residuals     44  3.635   0.083
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

> contrasts(section)<-c(3,3,3,3,3,3,3,3,-8,-8,-8)
> objnew<-lm(log2(yield)~treat*section)
> summary(objnew)

Call:
lm(formula = log2(yield) ~ treat * section)

Residuals:
      Min       1Q   Median       3Q      Max
-0.49647 -0.14913 -0.01521  0.17471  0.51105

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)
(Intercept)         6.288403   0.050034 125.682  < 2e-16 ***
treatTrt            1.221219   0.070759  17.259  < 2e-16 ***
section1           -0.008502   0.010213  -0.832 0.409675
section2           -0.491175   0.165945  -2.960 0.004942 **
section3            2.569427   0.165945  15.484  < 2e-16 ***
section4            3.556067   0.165945  21.429  < 2e-16 ***
section5           -1.157069   0.165945  -6.973 1.25e-08 ***
section6           -0.003562   0.165945  -0.021 0.982971
section7           -0.487770   0.165945  -2.939 0.005223 **
section8            0.106181   0.165945   0.640 0.525585
section9           -0.776882   0.165945  -4.682 2.74e-05 ***
section10           0.759168   0.165945   4.575 3.87e-05 ***
treatTrt:section1  -0.049000   0.014444  -3.392 0.001474 **
treatTrt:section2   0.160825   0.234682   0.685 0.496757
treatTrt:section3  -0.949101   0.234682  -4.044 0.000208 ***
treatTrt:section4  -1.118870   0.234682  -4.768 2.07e-05 ***
treatTrt:section5  -0.295937   0.234682  -1.261 0.213950
treatTrt:section6   0.538638   0.234682   2.295 0.026549 *
treatTrt:section7   0.796518   0.234682   3.394 0.001468 **
treatTrt:section8  -0.548744   0.234682  -2.338 0.023984 *
treatTrt:section9  -0.191029   0.234682  -0.814 0.420033
treatTrt:section10 -0.556642   0.234682  -2.372 0.022137 *
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.2874 on 44 degrees of freedom
Multiple R-Squared: 0.973,      Adjusted R-squared: 0.9601
F-statistic: 75.55 on 21 and 44 DF,  p-value: < 2.2e-16

Thanks,
Joshua


From jg_liao at yahoo.com  Thu Aug 24 22:05:12 2006
From: jg_liao at yahoo.com (Jason Liao)
Date: Thu, 24 Aug 2006 13:05:12 -0700 (PDT)
Subject: [R] extremely slow recursion in R?
Message-ID: <20060824200512.46157.qmail@web53701.mail.yahoo.com>

I recently coded a recursion algorithm in R and ir ran a few days
without returning any result. So I decided to try a simple case of
computing binomial coefficient using recusrive relationship

choose(n,k) = choose(n-1, k)+choose(n-1,k-1)

I implemented in R and Fortran 90 the same algorithm (code follows).
The R code finishes 31 minutes and the Fortran 90 program finishes in 6
seconds. So the Fortran program is 310 times faster. I thus wondered if
there is room for speeding up recursion in R. Thanks.

Jason

R code

my.choose = function(n,k)
{  
  if(k>n) value = 0.
  else if(k==0) value = 1.
  else if(k==n) value = 1.
  else value = my.choose(n-1,k) + my.choose(n-1, k-1)

  value
}

print(date())
my.choose(30,15)
print(date())



Fortran code

  recursive function choose(n, k) result(value)
  implicit none
  integer n, k
  double precision value
  if(k>n) then
    value = 0.
  elseif(k==0) then
    value = 1.
  else if(k==n) then
    value = 1.
  else
    value = choose(n-1, k) + choose(n-1, k-1)
  end if
  end function
  
  program main
    write(*,*) choose(30, 15)
  end program

Jason Liao, http://www.geocities.com/jg_liao
Department of Epidemiology and Biostatistics
Drexel University School of Public Health
245 N. 15th Street, Mail Stop 660
Philadelphia, PA 19102-1192
phone 215-762-3934


From ggrothendieck at gmail.com  Thu Aug 24 22:15:11 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 16:15:11 -0400
Subject: [R] extremely slow recursion in R?
In-Reply-To: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
References: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
Message-ID: <971536df0608241315u1ca485e0k554f40846fe2e259@mail.gmail.com>

There was some discussion here:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/73646.html

On 8/24/06, Jason Liao <jg_liao at yahoo.com> wrote:
> I recently coded a recursion algorithm in R and ir ran a few days
> without returning any result. So I decided to try a simple case of
> computing binomial coefficient using recusrive relationship
>
> choose(n,k) = choose(n-1, k)+choose(n-1,k-1)
>
> I implemented in R and Fortran 90 the same algorithm (code follows).
> The R code finishes 31 minutes and the Fortran 90 program finishes in 6
> seconds. So the Fortran program is 310 times faster. I thus wondered if
> there is room for speeding up recursion in R. Thanks.
>
> Jason
>
> R code
>
> my.choose = function(n,k)
> {
>  if(k>n) value = 0.
>  else if(k==0) value = 1.
>  else if(k==n) value = 1.
>  else value = my.choose(n-1,k) + my.choose(n-1, k-1)
>
>  value
> }
>
> print(date())
> my.choose(30,15)
> print(date())
>
>
>
> Fortran code
>
>  recursive function choose(n, k) result(value)
>  implicit none
>  integer n, k
>  double precision value
>  if(k>n) then
>    value = 0.
>  elseif(k==0) then
>    value = 1.
>  else if(k==n) then
>    value = 1.
>  else
>    value = choose(n-1, k) + choose(n-1, k-1)
>  end if
>  end function
>
>  program main
>    write(*,*) choose(30, 15)
>  end program
>
> Jason Liao, http://www.geocities.com/jg_liao
> Department of Epidemiology and Biostatistics
> Drexel University School of Public Health
> 245 N. 15th Street, Mail Stop 660
> Philadelphia, PA 19102-1192
> phone 215-762-3934
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From markleeds at verizon.net  Thu Aug 24 22:21:15 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Thu, 24 Aug 2006 16:21:15 -0400
Subject: [R] extremely slow recursion in R?
References: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
Message-ID: <000701c6c7ba$da1bfe70$2e01a8c0@m8d4477f3de884>

i'm sure someone else will explain the recursion issue but , as far as your 
program running a few days, you don't have to wait this long. if you are in 
windows and do
a ctrl  alt delete and then click on processes, if the memory usage being 
used by that R process  is staying EXACTLY the same and not moving at all, 
this is a sign
( atleast i have found this to be true for my cases. i guess it may not 
always hold ) that nothing is happening and your job has gone into never 
never land or has somehow become frozen.



----- Original Message ----- 
From: "Jason Liao" <jg_liao at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 24, 2006 4:05 PM
Subject: [R] extremely slow recursion in R?


>I recently coded a recursion algorithm in R and ir ran a few days
> without returning any result. So I decided to try a simple case of
> computing binomial coefficient using recusrive relationship
>
> choose(n,k) = choose(n-1, k)+choose(n-1,k-1)
>
> I implemented in R and Fortran 90 the same algorithm (code follows).
> The R code finishes 31 minutes and the Fortran 90 program finishes in 6
> seconds. So the Fortran program is 310 times faster. I thus wondered if
> there is room for speeding up recursion in R. Thanks.
>
> Jason
>
> R code
>
> my.choose = function(n,k)
> {
>  if(k>n) value = 0.
>  else if(k==0) value = 1.
>  else if(k==n) value = 1.
>  else value = my.choose(n-1,k) + my.choose(n-1, k-1)
>
>  value
> }
>
> print(date())
> my.choose(30,15)
> print(date())
>
>
>
> Fortran code
>
>  recursive function choose(n, k) result(value)
>  implicit none
>  integer n, k
>  double precision value
>  if(k>n) then
>    value = 0.
>  elseif(k==0) then
>    value = 1.
>  else if(k==n) then
>    value = 1.
>  else
>    value = choose(n-1, k) + choose(n-1, k-1)
>  end if
>  end function
>
>  program main
>    write(*,*) choose(30, 15)
>  end program
>
> Jason Liao, http://www.geocities.com/jg_liao
> Department of Epidemiology and Biostatistics
> Drexel University School of Public Health
> 245 N. 15th Street, Mail Stop 660
> Philadelphia, PA 19102-1192
> phone 215-762-3934
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Gaspard.Lequeux.at at biomath.ugent.be  Thu Aug 24 22:34:25 2006
From: Gaspard.Lequeux.at at biomath.ugent.be (Gaspard Lequeux)
Date: Thu, 24 Aug 2006 22:34:25 +0200 (CEST)
Subject: [R] rgl: exporting to pdf or png does not work
In-Reply-To: <44EDCD73.4090805@stats.uwo.ca>
References: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>
	<44ECDC27.6080109@stats.uwo.ca>
	<Pine.LNX.4.62.0608241642490.23380@biomath.ugent.be>
	<44EDCD73.4090805@stats.uwo.ca>
Message-ID: <Pine.LNX.4.62.0608242028310.27470@biomath.ugent.be>


Hej,

On Thu, 24 Aug 2006, Duncan Murdoch wrote:

> On 8/24/2006 11:19 AM, Gaspard Lequeux wrote:
>>
>> On Wed, 23 Aug 2006, Duncan Murdoch wrote:
>>
>>> On 8/23/2006 5:15 PM, Gaspard Lequeux wrote:
>>>
>>>> When exporting a image from rgl, the following error is encountered:
>>>>
>>>>> rgl.postscript('testing.pdf', fmt="pdf")
>>>> RGL: ERROR: can't bind glx context to window
>>>> RGL: ERROR: can't bind glx context to window
>>>> Warning messages:
>>>> 1: X11 protocol error: GLXBadContextState
>>>> 2: X11 protocol error: GLXBadContextState
>>>>
>>>> The pdf file is created and is readable, but all the labels are gone.
>>>>
>>>> Taking a snapshot (to png) gives 'failed' and no file is created.
>>>>
>>>> Version of rgl used: 0.67-2 (2006-07-11)
>>>> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
>>>> Running Debian GNU/Linux testing (Etch).
>>>
>>> That looks like an X11 error to me, not something that I'm very likely
>>> to be able to fix.  If you can debug the error, it would be helpful.
>>
>> Actually after upgrading everything (debian testing (etch)) and restarting
>> X, I didn't get that error anymore. It was worse: R crashed:
>>
>>> library(rgl);triangles3d(c(1,,2,3),c(1,2,4),c(1,3,5));rgl.postscript('testing.pdf','pdf')
>> X Error of failed request:  GLXBadContextState
>>    Major opcode of failed request:  142 (GLX)
>>    Minor opcode of failed request:  5 (X_GLXMakeCurrent)
>>    Serial number of failed request:  85
>>    Current serial number in output stream:  85
>> glequeux at toidi:~/seqanal$
>>
>>
>> I downloaded the source package (debian testing (etch), rgl-0.67-2).
>>
>> In rgl-0.67-2/src/ I changed the following files:
>>
>> rglview.cpp, around line 587. Commenting the function call gl2psBeginPage
>> removed the crash (but also no pdf output...)
>>
>> I enabled this function again and went to gl2ps.c, to the function
>> gl2psBeginPage. At the end of that function, around line 4426, commenting
>> out the line
>> glRenderMode(GL_FEEDBACK);
>> removes the R crash, but of course still no pdf output (well, only the
>> background).
>>
>> GL_FEEDBACK is defined in /usr/include/GL/gl.h as:
>>
>> /* Render Mode */
>> #define GL_FEEDBACK				0x1C01
>> #define GL_RENDER				0x1C00
>> #define GL_SELECT				0x1C02
>>
>> Trying glRenderMode(GL_RENDER) removed the crash, but still only the
>> background in the pdf.
>>
>> If someone has some suggestions about what to do next...
>
> gl2ps is a separate project, whose source has been included into rgl.
> You can see the gl2ps project page at http://www.geuz.org/gl2ps/.
>
> We're using version 1.2.2, which is a couple of years old.  The current
> stable release of gl2ps is 1.3.1.  It might fix your problem.
>
> I don't know if we modified gl2ps.c or gl2ps.h when they were included,
> but they haven't been modified since.  (Daniel put them in, based on a
> patch from Albrecht Gebhardt, according to the log.)
>
> It would be helpful to know:
>
> 1.  Is the rgl source identical to 1.2.2?

Yes. The version of gl2ps in rgl is identical to gl2ps version 1.2.2.

> 2.  Does rgl work if 1.3.1 is dropped in instead?

No:

In version 1.3.1:

#define GL2PS_PS  0
#define GL2PS_EPS 1
#define GL2PS_TEX 2
#define GL2PS_PDF 3
#define GL2PS_SVG 4
#define GL2PS_PGF 5

while in version 1.2.2:

#define GL2PS_PS  1
#define GL2PS_EPS 2
#define GL2PS_TEX 3
#define GL2PS_PDF 4

Thus rgl.postscript('probeer.pdf','tex') should be used to generate a pdf. 
The pdf has still no characters (axes annotations).

In R/enum.R

The last line (line 54)

rgl.enum (postscripttype, ps=1, eps=2, tex=3, pdf=4)

should be

rgl.enum (postscripttype, ps=0, eps=1, tex=2, pdf=3)

and mayebe add svg and pgf...


> 3.  Does 1.3.1 fix the bug you're seeing?

No. Same error.

The error occurs also on ubuntu dapper. On that ubuntu machine, when 
installing the libgl1-mesa-swrast, the packages libgl1-mesa 
libgl1-mesa-dri and x-window-system-core are removed. rgl.postscript 
doesn't produce any errors anymore, the pdf is created but no text (axes 
decorations) is written to the pdf.

On debian testing, libgl1-mesa-swx11 can be installed. This removes the 
follwing packages:

freeglut3-dev libgl1-mesa-dev libgl1-mesa-dri libgl1-mesa-glx 
libglitz-glx1-dev libglitz1-dev libglu1-mesa-dev libglui-dev libglut3-dev 
x-window-system-core xlibmesa-gl-dev xorg

but R doesn't crash anymore and the figure is written to file (still 
without axes annotations).

Reinstal libgl1-mesa-glx removes libgl1-mesa-swx11 and the R crash 
returns.

So it seems the bug is really triggered by libgl1-mesa. I filled in a bug 
report for the debian package libgl1-mesa-glx.

> I'll look into these at some point, but probably not this week.

Thanks. No hurry however, as I can still use the classical screenshots. 
The figures will probable not have to be published, as the expected 
results are not attained.

/Gaspard


From Ted.Harding at nessie.mcc.ac.uk  Thu Aug 24 22:44:01 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 24 Aug 2006 21:44:01 +0100 (BST)
Subject: [R] how to constrast with factorial experiment
In-Reply-To: <20060824154103.gako7y7k040s8gs8@webmail.uoguelph.ca>
Message-ID: <XFMail.060824214401.Ted.Harding@nessie.mcc.ac.uk>

On 24-Aug-06 szhan at uoguelph.ca wrote:
> Hello, R users,
> I have two factors (treat, section) anova design experiment where  
> there are 3 replicates. The objective of the experiment is to test if  
> there is significant difference of yield between top (section 9 to 11) 
> and bottom (section 9 to 11)
[I think you mean sections 1 to 8]

> of the fruit tree under treatment. I found that there are interaction
> between two factors. I wonder if I can contrast means from levels of
> one factor (section) under another factor (treat)? if so, how to do
> it in R and how to interpret the output?

I think you would be well advised to look at a plot of the data.
For example, let Y stand for yield, R for replicate, T for treat
and S for section.

  ix<-(T=="Trt");plot(S[ix],Y[ix],col="red",ylim=c(0,1000))
  ix<-(T=="Ctl");points(S[ix],Y[ix],col="blue")

>From this it is clear that sections 4 and 5 are in a class of
their own. Also, in sections 1-3 and 6-11 the "Ctl" yields
are not only lower, but have smaller (in some cases hardly any)
variance, compared with the "Trt" yields. The variances for
sections 7,8,9,10,11 are greater than for 1,2,3,6 without
great change in mean value.

While there is an evident difference between "Trt" yields and
"Ctrl" yields for sections 1-3 and 6-11, this is not so for
sections 4 and 5.

This sort of behaviour no doubt provides some reasons for the
interaction you observed. You seem to have a quite complex
phenomenon here!

To some extent the problems with variance can be diminished by
working with logarithms. Compare the previous plot with

  ix<-(T=="Trt");plot(S[ix],log10(Y[ix]),col="red",ylim=c(0,3))
  ix<-(T=="Ctl");points(S[ix],log10(Y[ix]),col="blue")

(you have used log2() in your commands). The above observations
can be seen reflected in R if you look at the output of

  summary(obj)

where in particular:

treatTrt:section2  -1.11691    0.33189  -3.365 0.001595 ** 
treatTrt:section3  -0.45634    0.33189  -1.375 0.176099    
treatTrt:section4  -1.56627    0.33189  -4.719 2.42e-05 ***
treatTrt:section5  -1.73604    0.33189  -5.231 4.48e-06 ***
treatTrt:section6  -0.91311    0.33189  -2.751 0.008588 ** 
treatTrt:section7  -0.07853    0.33189  -0.237 0.814055    
treatTrt:section8   0.17935    0.33189   0.540 0.591654    
treatTrt:section9  -0.28859    0.33189  -0.870 0.389277    
treatTrt:section10  0.06913    0.33189   0.208 0.835972    
treatTrt:section11 -0.29649    0.33189  -0.893 0.376543

which, precisely, "contrasts means from levels of one factor
(section) under another factor (treat)", and shows that most
of the "interaction" arises in sections 4 and 5.

Since sections 4 and 5 (in the middle of sections 1 to 8) are
so exceptional, they will have strong influence on your comparison
between sections 1-8 and sections 9-11. You need to think about
what to do with sections 4 and 5!

> Here is the data and commands I used to test the differece between  
> section 1 to 8 and 9 to 11 under treatment. But I don't know if I was  
> right, how to interpret the out and whether there are significant  
> difference between section 1 to 8 and section 9 to 11 under treatment.
> 
> yield replicate       treat   section
> 35.55 1       Ctl     1
> 53.70 1       Ctl     2
> 42.79 1       Ctl     3
> 434.81        1       Ctl     4
> 705.96        1       Ctl     5
> 25.91 1       Ctl     6
> 57.53 1       Ctl     7
> 41.45 1       Ctl     8
> 85.54 1       Ctl     9
> 51.23 1       Ctl     10
> 188.24        1       Ctl     11
> 35.71 2       Ctl     1
> 45.15 2       Ctl     2
> 40.10 2       Ctl     3
> 312.76        2       Ctl     4
> 804.05        2       Ctl     5
> 28.22 2       Ctl     6
> 68.51 2       Ctl     7
> 46.15 2       Ctl     8
> 123.14        2       Ctl     9
> 33.78 2       Ctl     10
> 121.28        2       Ctl     11
> 30.96 3       Ctl     1
> 36.10 3       Ctl     2
> 47.19 3       Ctl     3
> 345.80        3       Ctl     4
> 644.61        3       Ctl     5
> 27.73 3       Ctl     6
> 56.63 3       Ctl     7
> 42.63 3       Ctl     8
> 61.25 3       Ctl     9
> 59.43 3       Ctl     10
> 109.87        3       Ctl     11
> 143.50        1       Trt     1
> 82.76 1       Trt     2
> 125.03        1       Trt     3
> 493.76        1       Trt     4
> 868.48        1       Trt     5
> 45.09 1       Trt     6
> 249.43        1       Trt     7
> 167.28        1       Trt     8
> 274.72        1       Trt     9
> 176.40        1       Trt     10
> 393.10        1       Trt     11
> 93.75 2       Trt     1
> 63.83 2       Trt     2
> 117.50        2       Trt     3
> 362.68        2       Trt     4
> 659.40        2       Trt     5
> 62.10 2       Trt     6
> 218.24        2       Trt     7
> 210.98        2       Trt     8
> 291.48        2       Trt     9
> 209.36        2       Trt     10
> 454.68        2       Trt     11
> 119.62        3       Trt     1
> 66.50 3       Trt     2
> 87.37 3       Trt     3
> 414.01        3       Trt     4
> 707.70        3       Trt     5
> 44.40 3       Trt     6
> 142.59        3       Trt     7
> 137.37        3       Trt     8
> 181.03        3       Trt     9
> 131.65        3       Trt     10
> 310.18        3       Trt     11
> 
>> dat1<-read.delim("c:/testcontr.txt", header=T)
>> dat1$treat<-as.factor(dat1$treat)
>> dat1$replicate<-as.factor(dat1$replicate)
>> dat1$section<-as.factor(dat1$section)
>> attach(dat1)
>> obj<-lm(log2(yield)~treat*section)
>> anova(obj)
> Analysis of Variance Table
> 
> Response: log2(yield)
>                Df Sum Sq Mean Sq  F value    Pr(>F)
> treat          1 24.608  24.608 297.8649 < 2.2e-16 ***
> section       10 99.761   9.976 120.7565 < 2.2e-16 ***
> treat:section 10  6.708   0.671   8.1197 2.972e-07 ***
> Residuals     44  3.635   0.083
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
>> contrasts(section)<-c(3,3,3,3,3,3,3,3,-8,-8,-8)
>> objnew<-lm(log2(yield)~treat*section)
>> summary(objnew)
> 
> Call:
> lm(formula = log2(yield) ~ treat * section)
> 
> Residuals:
>       Min       1Q   Median       3Q      Max
> -0.49647 -0.14913 -0.01521  0.17471  0.51105
> 
> Coefficients:
>                      Estimate Std. Error t value Pr(>|t|)
> (Intercept)         6.288403   0.050034 125.682  < 2e-16 ***
> treatTrt            1.221219   0.070759  17.259  < 2e-16 ***
> section1           -0.008502   0.010213  -0.832 0.409675
> section2           -0.491175   0.165945  -2.960 0.004942 **
> section3            2.569427   0.165945  15.484  < 2e-16 ***
> section4            3.556067   0.165945  21.429  < 2e-16 ***
> section5           -1.157069   0.165945  -6.973 1.25e-08 ***
> section6           -0.003562   0.165945  -0.021 0.982971
> section7           -0.487770   0.165945  -2.939 0.005223 **
> section8            0.106181   0.165945   0.640 0.525585
> section9           -0.776882   0.165945  -4.682 2.74e-05 ***
> section10           0.759168   0.165945   4.575 3.87e-05 ***
> treatTrt:section1  -0.049000   0.014444  -3.392 0.001474 **
> treatTrt:section2   0.160825   0.234682   0.685 0.496757
> treatTrt:section3  -0.949101   0.234682  -4.044 0.000208 ***
> treatTrt:section4  -1.118870   0.234682  -4.768 2.07e-05 ***
> treatTrt:section5  -0.295937   0.234682  -1.261 0.213950
> treatTrt:section6   0.538638   0.234682   2.295 0.026549 *
> treatTrt:section7   0.796518   0.234682   3.394 0.001468 **
> treatTrt:section8  -0.548744   0.234682  -2.338 0.023984 *
> treatTrt:section9  -0.191029   0.234682  -0.814 0.420033
> treatTrt:section10 -0.556642   0.234682  -2.372 0.022137 *
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.2874 on 44 degrees of freedom
> Multiple R-Squared: 0.973,      Adjusted R-squared: 0.9601
> F-statistic: 75.55 on 21 and 44 DF,  p-value: < 2.2e-16
> 
> Thanks,
> Joshua
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Aug-06                                       Time: 21:43:57
------------------------------ XFMail ------------------------------


From montez at bu.edu  Thu Aug 24 23:01:42 2006
From: montez at bu.edu (Maria Montez)
Date: Thu, 24 Aug 2006 14:01:42 -0700
Subject: [R] generating an expression for a formula automatically
Message-ID: <44EE13B6.20205@bu.edu>

Hi!

I would like to be able to create formulas automatically. For example, I 
want to be able to create a function that takes on two values: resp and 
x, and then creates the proper formula to regress resp on x.

My code:

fit.main <- function(resp,x) {
 form <- expression(paste(resp," ~ ",paste(x,sep="",collapse=" + "),sep=""))
  z <- lm(eval(form))
 z
}
main <- fit.main("y",c("x1","x2","x3","x4"))

and I get this error:
Error in terms.default(formula, data = data) :
        no terms component

Any suggestions?

Thanks, Maria


From phhs80 at gmail.com  Thu Aug 24 23:07:02 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 24 Aug 2006 22:07:02 +0100
Subject: [R] Check values in colums matrix
In-Reply-To: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
References: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
Message-ID: <6ade6f6c0608241407x54c586adk5eede1e404bd69f3@mail.gmail.com>

On 8/24/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> I have a dataset (20 columns & 1000 rows) which
> some of columns have the same value and the others
> have different values.
> Here are some piece of my dataset:
> obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
>              c(0,1,1,4,1,0,1,4,-1),
>              c(1,1,1,4,2,0,1,4,-1),
>              c(1,1,1,4,3,0,1,4,-1),
>              c(1,1,1,4,6,0,1,5,-1),
>              c(1,1,1,4,6,0,1,6,-1),
>              c(1,1,1,4,6,0,1,7,-1),
>              c(1,1,1,4,6,0,1,8,-1))
> obj.tr <- t(obj)
> obj.tr
> > obj.tr
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> [1,]    1    1    1    4    0    0    1    4   -1
> [2,]    0    1    1    4    1    0    1    4   -1
> [3,]    1    1    1    4    2    0    1    4   -1
> [4,]    1    1    1    4    3    0    1    4   -1
> [5,]    1    1    1    4    6    0    1    5   -1
> [6,]    1    1    1    4    6    0    1    6   -1
> [7,]    1    1    1    4    6    0    1    7   -1
> [8,]    1    1    1    4    6    0    1    8   -1
> >
>
> How can I do to check columns 2,3,4,6,7 and 9 have
> the same value, and columns 1,5 and 8 have different values.

Just try

all(obj.tr[,2]==obj.tr[1,2])

and so on for the other columns. See ? all.

Paul


From ggrothendieck at gmail.com  Thu Aug 24 23:13:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 17:13:14 -0400
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44EE13B6.20205@bu.edu>
References: <44EE13B6.20205@bu.edu>
Message-ID: <971536df0608241413h3a995939wa1bb78131e40a8ad@mail.gmail.com>

Use as.formula to convert the character string to an object of class "formula"
and note that we want to set the formula's environment appropriately:

fit.main <- function(resp, x, env = parent.frame()) {
	fo <- as.formula(paste("y", "~", paste(x, collapse = "+")))
	environment(fo) <- env
	fo
}

# test
fit.main("y", letters[1:3])


On 8/24/06, Maria Montez <montez at bu.edu> wrote:
> Hi!
>
> I would like to be able to create formulas automatically. For example, I
> want to be able to create a function that takes on two values: resp and
> x, and then creates the proper formula to regress resp on x.
>
> My code:
>
> fit.main <- function(resp,x) {
>  form <- expression(paste(resp," ~ ",paste(x,sep="",collapse=" + "),sep=""))
>  z <- lm(eval(form))
>  z
> }
> main <- fit.main("y",c("x1","x2","x3","x4"))
>
> and I get this error:
> Error in terms.default(formula, data = data) :
>        no terms component
>
> Any suggestions?
>
> Thanks, Maria
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Sinnwell.Jason at mayo.edu  Thu Aug 24 23:21:11 2006
From: Sinnwell.Jason at mayo.edu (Sinnwell, Jason P.)
Date: Thu, 24 Aug 2006 16:21:11 -0500
Subject: [R] Problem in library.dynam problems on Linux
Message-ID: <94ACA3F800EA6748A320D779148E620004988ABD@excsrv67.mayo.edu>


We have R 2.2.1 installed on a Linux cluster that seems to have problems loading either of our shared object libraries for packages.  This seems to be happening on both local and global versions of packages that we install.  However, we have only noticed this problem in the past 3 months on this R installation, whereas some users had success before then.  It could be that something on our system changed, but I am not an admin so I wouldn't know where to look.  Can anyone help with this problem?

## A LOCAL INSTALLATION OF HAPLO.STATS APPEARS SUCCESSFUL
[sinnwell at dnode0 rpack]$ R CMD INSTALL -l /home/sinnwell/rdir/tmplib haplo.stats 
* Installing *source* package 'haplo.stats' ...
** libs
make: `haplo.stats.so' is up to date.
** R
** data
** demo
** inst
** preparing package for lazy loading

** help
 >>> Building/Updating help pages for package 'haplo.stats'
     Formats: text html latex example 
** building package indices ...
* DONE (haplo.stats)

## TRY AND LOAD THE LOCALLY INSTALLED LIBRARY, YET THE SYSTEM COMMAND SHOWS THE .so FILE IS THERE

R> library(haplo.stats, lib.loc="/home/sinnwell/rdir/tmplib/")
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library '/home/sinnwell/rdir/tmplib/haplo.stats/libs/haplo.stats.so':
  /home/sinnwell/rdir/tmplib/haplo.stats/libs/haplo.stats.so: cannot open shared object file: No such file or directory
Error in library(haplo.stats, lib.loc = "/home/sinnwell/rdir/tmplib/") : 
        .First.lib failed for 'haplo.stats'
R> system('ls -al /home/sinnwell/rdir/tmplib/haplo.stats/libs')
total 88
drwxr-xr-x   2 sinnwell sinnwell  4096 Aug 24 15:14 .
drwxr-xr-x  13 sinnwell sinnwell  4096 Aug 24 15:14 ..
-rwxr-xr-x   1 sinnwell sinnwell 61566 Aug 24 15:14 haplo.stats.so
R> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    2.1              
year     2005             
month    12               
day      20               
svn rev  36812            
language R 

Also, the .First.lib for haplo.stats looks like this:

.First.lib <- function(lib, pkg) {
   library.dynam("haplo.stats", pkg, lib)
}

Thanks for you suggestions,
Jason Sinnwell

============================
 Jason Sinnwell             
 Mayo Clinic, Rochester     
 Division of Biostatistics  
 ph:  507.284.3270          
 fax: 507.284.9542


From jrkrideau at yahoo.ca  Thu Aug 24 23:30:32 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 24 Aug 2006 17:30:32 -0400 (EDT)
Subject: [R] Using a 'for' loop : there should be a better way in R
Message-ID: <20060824213032.51300.qmail@web32806.mail.mud.yahoo.com>

I need to apply a yearly inflation factor to some
wages and supply some simple sums by work category.  I
have gone at it with a brute force "for" loop approach
 which seems okay as it is a small dataset.  It looks
a bit inelegant and given all the warnings in the
Intro to R, etc, about using loops I wondered  if
anyone could suggest something a bit simpler or more
efficent?

Example:

cat1 <- c( 1,1,6,1,1,5)
cat2 <- c( 1,2,3,4,5,6)
cat3 <- c( 5,4,6,7,8,8)
cat4 <- c( 1,2,1,2,1,2)
years <- c( 'year1', 'year2', 'year3', 'year3',
'year1', 'year1') 
id <-  c('a','a','b','c','c','a')
df1 <- data.frame(id,years,cat1,cat2, cat3, cat4)

nn <- levels(df1$id)    # levels for outer loop
hh <- levels(df1$years) # levels for inter loop


mp <- c(1, 5, 10)   # inflation factor

tt <- data.frame(matrix(NA, length(nn), 2)) 
names(tt) <- c("s1","s2")
rownames(tt) <- nn 

for (i in 1:length(nn)){
scat <- data.frame(matrix(NA, length(hh),2))
dd1 <- subset(df1, id==nn[i])
for (j in 1:length(hh)){
dd2 <- subset(dd1, dd1$years==hh[j])
s1 <- sum(dd2$cat1,dd2$cat2, na.rm=T)
s2 <- sum(dd2$cat3,dd2$cat4,na.rm=T)
scat[j,] <- c(s1,s2) *mp[j]    # multiply by the
inflation factor
}
crush <- apply(scat, 2, sum)
tt[i,] <- crush 
}
tt


From mschwartz at mn.rr.com  Thu Aug 24 23:39:19 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 24 Aug 2006 16:39:19 -0500
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44EE13B6.20205@bu.edu>
References: <44EE13B6.20205@bu.edu>
Message-ID: <1156455559.19364.24.camel@localhost.localdomain>

On Thu, 2006-08-24 at 14:01 -0700, Maria Montez wrote:
> Hi!
> 
> I would like to be able to create formulas automatically. For example, I 
> want to be able to create a function that takes on two values: resp and 
> x, and then creates the proper formula to regress resp on x.
> 
> My code:
> 
> fit.main <- function(resp,x) {
>  form <- expression(paste(resp," ~ ",paste(x,sep="",collapse=" + "),sep=""))
>   z <- lm(eval(form))
>  z
> }
> main <- fit.main("y",c("x1","x2","x3","x4"))
> 
> and I get this error:
> Error in terms.default(formula, data = data) :
>         no terms component
> 
> Any suggestions?
> 
> Thanks, Maria

See the last example in ?as.formula:

BTW, I would pay note to the ability to use subset()'s of data frames in
model functions. For example, let's say that your data frame above is
called DF and contains columns 'y' and then 'x1' through 'x50' in
sequence. However, you only want to use the columns you have indicated
in your code above.  You can then do:

  lm(y ~ ., data = subset(DF, select = y:x4))

The use of the '.' on the RHS of the formula indicates to use all other
columns besides the response column in the formula.  In the subset()
function, you can specify a sequential group of columns using the ':'
operator.

For a specific example, let's use the iris data set, which has columns:

> names(iris)
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"
[5] "Species"

We want to use 'Sepal.Length' as the response variable and then all
columns, other than 'Species', as terms:

> lm(Sepal.Length ~ ., data = subset(iris, select = -Species))

Call:
lm(formula = Sepal.Length ~ ., data = subset(iris, select = -Species))

Coefficients:
 (Intercept)   Sepal.Width  Petal.Length   Petal.Width
      1.8560        0.6508        0.7091       -0.5565


In this case, I excluded the Species columns by using the '-' before the
column name.  However, I could have easily used:

> lm(Sepal.Length ~ ., 
     data = subset(iris, select = Sepal.Length:Petal.Width))

Call:
lm(formula = Sepal.Length ~ ., data = subset(iris, select =
Sepal.Length:Petal.Width))

Coefficients:
 (Intercept)   Sepal.Width  Petal.Length   Petal.Width
      1.8560        0.6508        0.7091       -0.5565



See ?subset for additional information.

HTH,

Marc Schwartz


From sundar.dorai-raj at pdf.com  Thu Aug 24 23:50:44 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 24 Aug 2006 14:50:44 -0700
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44EE13B6.20205@bu.edu>
References: <44EE13B6.20205@bu.edu>
Message-ID: <44EE1F34.6010400@pdf.com>


Maria Montez wrote:
> Hi!
> 
> I would like to be able to create formulas automatically. For example, I 
> want to be able to create a function that takes on two values: resp and 
> x, and then creates the proper formula to regress resp on x.
> 
> My code:
> 
> fit.main <- function(resp,x) {
>  form <- expression(paste(resp," ~ ",paste(x,sep="",collapse=" + "),sep=""))
>   z <- lm(eval(form))
>  z
> }
> main <- fit.main("y",c("x1","x2","x3","x4"))
> 
> and I get this error:
> Error in terms.default(formula, data = data) :
>         no terms component
> 
> Any suggestions?
> 
> Thanks, Maria
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Maria,

Try

regr <- paste(x, collapse = "+")
form <- as.formula(sprintf("%s ~ %s", resp, regr))

HTH,

--sundar


From jrkrideau at yahoo.ca  Thu Aug 24 23:52:09 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 24 Aug 2006 17:52:09 -0400 (EDT)
Subject: [R] Check values in colums matrix
In-Reply-To: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
Message-ID: <20060824215209.79163.qmail@web32804.mail.mud.yahoo.com>


--- Muhammad Subianto <msubianto at gmail.com> wrote:

> Dear all,
> I apologize if my question is quite simple.
> I have a dataset (20 columns & 1000 rows) which
> some of columns have the same value and the others
> have different values.
> Here are some piece of my dataset:
> obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
>              c(0,1,1,4,1,0,1,4,-1),
>              c(1,1,1,4,2,0,1,4,-1),
>              c(1,1,1,4,3,0,1,4,-1),
>              c(1,1,1,4,6,0,1,5,-1),
>              c(1,1,1,4,6,0,1,6,-1),
>              c(1,1,1,4,6,0,1,7,-1),
>              c(1,1,1,4,6,0,1,8,-1))
> obj.tr <- t(obj)
> obj.tr
> > obj.tr
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> [1,]    1    1    1    4    0    0    1    4   -1
> [2,]    0    1    1    4    1    0    1    4   -1
> [3,]    1    1    1    4    2    0    1    4   -1
> [4,]    1    1    1    4    3    0    1    4   -1
> [5,]    1    1    1    4    6    0    1    5   -1
> [6,]    1    1    1    4    6    0    1    6   -1
> [7,]    1    1    1    4    6    0    1    7   -1
> [8,]    1    1    1    4    6    0    1    8   -1
> >
> 
> How can I do to check columns 2,3,4,6,7 and 9 have
> the same value, and columns 1,5 and 8 have different
> values.
> 
> Best, Muhammad Subianto
 There has to be a better way but this will let you
check visually since you only have 20 columns

for(i in 1:8) {
tt <-table(obj.tr[,i])
print( i)
print (tt)
}


From p.murrell at auckland.ac.nz  Fri Aug 25 00:17:16 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 25 Aug 2006 10:17:16 +1200
Subject: [R] R News, volume 6, issue 3 is now available
Message-ID: <44EE256C.8070506@stat.auckland.ac.nz>

Hi

The August 2006 issue of R News is now available on CRAN under the
Documentation/Newsletter link.

Many thanks to Ron Wehrens, our guest editor for this special issue.

Paul
(on behalf of the R News EditorialBoard)
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From p.dalgaard at biostat.ku.dk  Fri Aug 25 01:01:39 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Aug 2006 01:01:39 +0200
Subject: [R] Problem in library.dynam problems on Linux
In-Reply-To: <94ACA3F800EA6748A320D779148E620004988ABD@excsrv67.mayo.edu>
References: <94ACA3F800EA6748A320D779148E620004988ABD@excsrv67.mayo.edu>
Message-ID: <x2hd01eo18.fsf@turmalin.kubism.ku.dk>

"Sinnwell, Jason P." <Sinnwell.Jason at mayo.edu> writes:

> We have R 2.2.1 installed on a Linux cluster that seems to have problems loading either of our shared object libraries for packages.  This seems to be happening on both local and global versions of packages that we install.  However, we have only noticed this problem in the past 3 months on this R installation, whereas some users had success before then.  It could be that something on our system changed, but I am not an admin so I wouldn't know where to look.  Can anyone help with this problem?
> 
> ## A LOCAL INSTALLATION OF HAPLO.STATS APPEARS SUCCESSFUL
> [sinnwell at dnode0 rpack]$ R CMD INSTALL -l /home/sinnwell/rdir/tmplib haplo.stats 
> * Installing *source* package 'haplo.stats' ...
> ** libs
> make: `haplo.stats.so' is up to date.
> ** R
> ** data
> ** demo
> ** inst
> ** preparing package for lazy loading
> 
> ** help
>  >>> Building/Updating help pages for package 'haplo.stats'
>      Formats: text html latex example 
> ** building package indices ...
> * DONE (haplo.stats)
> 
> ## TRY AND LOAD THE LOCALLY INSTALLED LIBRARY, YET THE SYSTEM COMMAND SHOWS THE .so FILE IS THERE
> 
> R> library(haplo.stats, lib.loc="/home/sinnwell/rdir/tmplib/")
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
>         unable to load shared library '/home/sinnwell/rdir/tmplib/haplo.stats/libs/haplo.stats.so':
>   /home/sinnwell/rdir/tmplib/haplo.stats/libs/haplo.stats.so: cannot open shared object file: No such file or directory
> Error in library(haplo.stats, lib.loc = "/home/sinnwell/rdir/tmplib/") : 
>         .First.lib failed for 'haplo.stats'
> R> system('ls -al /home/sinnwell/rdir/tmplib/haplo.stats/libs')
> total 88
> drwxr-xr-x   2 sinnwell sinnwell  4096 Aug 24 15:14 .
> drwxr-xr-x  13 sinnwell sinnwell  4096 Aug 24 15:14 ..
> -rwxr-xr-x   1 sinnwell sinnwell 61566 Aug 24 15:14 haplo.stats.so

Hmmmm.. Does "mount -l" show something interesting on the filesystem
containing the .so file? (e.g., option noexec).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Fri Aug 25 01:04:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 19:04:09 -0400
Subject: [R] Using a 'for' loop : there should be a better way in R
In-Reply-To: <20060824213032.51300.qmail@web32806.mail.mud.yahoo.com>
References: <20060824213032.51300.qmail@web32806.mail.mud.yahoo.com>
Message-ID: <971536df0608241604g1725bed7re9f983396bfc6d92@mail.gmail.com>

Use cbind to create a two column matrix, mat,
and multiply that by the appropriate inflation factors.
Then use rowsum to sum the rows according to the
id grouping factor.

inf.fac <- list(year1 = 1, year2 = 5, year3 = 10)
mat <- cbind(s1 = df1$cat1 + df1$cat2, s2 = df1$cat3 + df1$cat4)
rowsum(mat * unlist(inf.fac[df1$year]), df1$id)


On 8/24/06, John Kane <jrkrideau at yahoo.ca> wrote:
> I need to apply a yearly inflation factor to some
> wages and supply some simple sums by work category.  I
> have gone at it with a brute force "for" loop approach
>  which seems okay as it is a small dataset.  It looks
> a bit inelegant and given all the warnings in the
> Intro to R, etc, about using loops I wondered  if
> anyone could suggest something a bit simpler or more
> efficent?
>
> Example:
>
> cat1 <- c( 1,1,6,1,1,5)
> cat2 <- c( 1,2,3,4,5,6)
> cat3 <- c( 5,4,6,7,8,8)
> cat4 <- c( 1,2,1,2,1,2)
> years <- c( 'year1', 'year2', 'year3', 'year3',
> 'year1', 'year1')
> id <-  c('a','a','b','c','c','a')
> df1 <- data.frame(id,years,cat1,cat2, cat3, cat4)
>
> nn <- levels(df1$id)    # levels for outer loop
> hh <- levels(df1$years) # levels for inter loop
>
>
> mp <- c(1, 5, 10)   # inflation factor
>
> tt <- data.frame(matrix(NA, length(nn), 2))
> names(tt) <- c("s1","s2")
> rownames(tt) <- nn
>
> for (i in 1:length(nn)){
> scat <- data.frame(matrix(NA, length(hh),2))
> dd1 <- subset(df1, id==nn[i])
> for (j in 1:length(hh)){
> dd2 <- subset(dd1, dd1$years==hh[j])
> s1 <- sum(dd2$cat1,dd2$cat2, na.rm=T)
> s2 <- sum(dd2$cat3,dd2$cat4,na.rm=T)
> scat[j,] <- c(s1,s2) *mp[j]    # multiply by the
> inflation factor
> }
> crush <- apply(scat, 2, sum)
> tt[i,] <- crush
> }
> tt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Fri Aug 25 01:05:19 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Aug 2006 16:05:19 -0700 (PDT)
Subject: [R] extremely slow recursion in R?
In-Reply-To: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
References: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608241558140.28297@homer21.u.washington.edu>

On Thu, 24 Aug 2006, Jason Liao wrote:

> I recently coded a recursion algorithm in R and ir ran a few days
> without returning any result. So I decided to try a simple case of
> computing binomial coefficient using recusrive relationship
>
> choose(n,k) = choose(n-1, k)+choose(n-1,k-1)
>
> I implemented in R and Fortran 90 the same algorithm (code follows).
> The R code finishes 31 minutes and the Fortran 90 program finishes in 6
> seconds. So the Fortran program is 310 times faster. I thus wondered if
> there is room for speeding up recursion in R. Thanks.
>

Recursive code that computes the same case many times can often be sped up 
by memoization, eg

memo<-new.env(hash=TRUE)
chewse<-function(n,k) {
     if (n==k) return(1)
     if(k==1) return(n)

     if(exists(paste(n,k),memo,inherits=FALSE))
         return(get(paste(n,k),memo))
     rval<-chewse(n-1,k)+chewse(n-1,k-1)
     assign(paste(n,k),rval,envir=memo)
     return(rval)
}

This idea was discussed in an early Programmers' Niche article by Bill 
Venables in R News.

However, I'm surprised that you're surprised that compiled Fortran 90 is 
310 times faster than interpreted R.  That would be about what I would 
expect for code that isn't making use of vectorized functions in R.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ggrothendieck at gmail.com  Fri Aug 25 01:27:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 19:27:51 -0400
Subject: [R] Check values in colums matrix
In-Reply-To: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
References: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
Message-ID: <971536df0608241627l7c09a1f6m8f4654b286dd574d@mail.gmail.com>

Try sd(obj.tr) which will give a vector of standard deviations, one per column.
A column's entry will be zero if and only if all values in the column
are the same.

On 8/24/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> I apologize if my question is quite simple.
> I have a dataset (20 columns & 1000 rows) which
> some of columns have the same value and the others
> have different values.
> Here are some piece of my dataset:
> obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
>             c(0,1,1,4,1,0,1,4,-1),
>             c(1,1,1,4,2,0,1,4,-1),
>             c(1,1,1,4,3,0,1,4,-1),
>             c(1,1,1,4,6,0,1,5,-1),
>             c(1,1,1,4,6,0,1,6,-1),
>             c(1,1,1,4,6,0,1,7,-1),
>             c(1,1,1,4,6,0,1,8,-1))
> obj.tr <- t(obj)
> obj.tr
> > obj.tr
>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> [1,]    1    1    1    4    0    0    1    4   -1
> [2,]    0    1    1    4    1    0    1    4   -1
> [3,]    1    1    1    4    2    0    1    4   -1
> [4,]    1    1    1    4    3    0    1    4   -1
> [5,]    1    1    1    4    6    0    1    5   -1
> [6,]    1    1    1    4    6    0    1    6   -1
> [7,]    1    1    1    4    6    0    1    7   -1
> [8,]    1    1    1    4    6    0    1    8   -1
> >
>
> How can I do to check columns 2,3,4,6,7 and 9 have
> the same value, and columns 1,5 and 8 have different values.
>
> Best, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bhattach at simbiosys-biowares.com  Fri Aug 25 01:29:51 2006
From: bhattach at simbiosys-biowares.com (Debashis Bhattacharya)
Date: Thu, 24 Aug 2006 18:29:51 -0500
Subject: [R] Need help with difficulty loading page www.bioconductor.org
Message-ID: <44EE366F.2030906@simbiosys-biowares.com>

The page is either too busy, or there is something seriously wrong with 
access to this page.

Most of the time, trying to reach www.bioconductor.org results in 
failure. Only once in a
blue moon, do I get through.

In fact, thus far, I have not been able to install bioconductor, since 
the first source(...)
command from the R command window -- following instruction on 
www.bioconductor.org
page, that I did manage to reach, one time -- has failed, every time.

Please help.



Debashis Bhattacharya.


From gunter.berton at gene.com  Fri Aug 25 01:37:16 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 24 Aug 2006 16:37:16 -0700
Subject: [R] Check values in colums matrix
In-Reply-To: <971536df0608241627l7c09a1f6m8f4654b286dd574d@mail.gmail.com>
Message-ID: <002401c6c7d6$3c17f690$396810ac@gne.windows.gene.com>

Absolutely. But do note that if the values in obj are the product of
numerical computations then columns of equal values may turn out to be only
**nearly** equal and so the sd may turn out to be **nearly** 0 and not
exactly 0. This is a standard issue in numerical computation, of course, and
has been commented on in this list at least dozens of times, but it's still
a gotcha for the unwary (so now dozens +1).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Thursday, August 24, 2006 4:28 PM
> To: Muhammad Subianto
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Check values in colums matrix
> 
> Try sd(obj.tr) which will give a vector of standard 
> deviations, one per column.
> A column's entry will be zero if and only if all values in the column
> are the same.
> 
> On 8/24/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > Dear all,
> > I apologize if my question is quite simple.
> > I have a dataset (20 columns & 1000 rows) which
> > some of columns have the same value and the others
> > have different values.
> > Here are some piece of my dataset:
> > obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
> >             c(0,1,1,4,1,0,1,4,-1),
> >             c(1,1,1,4,2,0,1,4,-1),
> >             c(1,1,1,4,3,0,1,4,-1),
> >             c(1,1,1,4,6,0,1,5,-1),
> >             c(1,1,1,4,6,0,1,6,-1),
> >             c(1,1,1,4,6,0,1,7,-1),
> >             c(1,1,1,4,6,0,1,8,-1))
> > obj.tr <- t(obj)
> > obj.tr
> > > obj.tr
> >     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> > [1,]    1    1    1    4    0    0    1    4   -1
> > [2,]    0    1    1    4    1    0    1    4   -1
> > [3,]    1    1    1    4    2    0    1    4   -1
> > [4,]    1    1    1    4    3    0    1    4   -1
> > [5,]    1    1    1    4    6    0    1    5   -1
> > [6,]    1    1    1    4    6    0    1    6   -1
> > [7,]    1    1    1    4    6    0    1    7   -1
> > [8,]    1    1    1    4    6    0    1    8   -1
> > >
> >
> > How can I do to check columns 2,3,4,6,7 and 9 have
> > the same value, and columns 1,5 and 8 have different values.
> >
> > Best, Muhammad Subianto
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Aug 25 02:09:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Aug 2006 20:09:02 -0400
Subject: [R] Check values in colums matrix
In-Reply-To: <002401c6c7d6$3c17f690$396810ac@gne.windows.gene.com>
References: <971536df0608241627l7c09a1f6m8f4654b286dd574d@mail.gmail.com>
	<002401c6c7d6$3c17f690$396810ac@gne.windows.gene.com>
Message-ID: <971536df0608241709n4c9b2681n3003a2d40078429a@mail.gmail.com>

Fair enough although in the case of the example it does not appear to
be a problem:

> sd(obj.tr)
[1] 0.3535534 0.0000000 0.0000000 0.0000000 2.5495098 0.0000000 0.0000000
[8] 1.5811388 0.0000000

Further, if all entries in the matrix are integers, as in the example,
then we know that:

> nr <- nrow(obj.tr)
> round(nr * (nr-1) * sd(obj.tr))
[1]  20   0   0   0 143   0   0  89   0

is all integer too.

On 8/24/06, Berton Gunter <gunter.berton at gene.com> wrote:
> Absolutely. But do note that if the values in obj are the product of
> numerical computations then columns of equal values may turn out to be only
> **nearly** equal and so the sd may turn out to be **nearly** 0 and not
> exactly 0. This is a standard issue in numerical computation, of course, and
> has been commented on in this list at least dozens of times, but it's still
> a gotcha for the unwary (so now dozens +1).
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
> > Grothendieck
> > Sent: Thursday, August 24, 2006 4:28 PM
> > To: Muhammad Subianto
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Check values in colums matrix
> >
> > Try sd(obj.tr) which will give a vector of standard
> > deviations, one per column.
> > A column's entry will be zero if and only if all values in the column
> > are the same.
> >
> > On 8/24/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > > Dear all,
> > > I apologize if my question is quite simple.
> > > I have a dataset (20 columns & 1000 rows) which
> > > some of columns have the same value and the others
> > > have different values.
> > > Here are some piece of my dataset:
> > > obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
> > >             c(0,1,1,4,1,0,1,4,-1),
> > >             c(1,1,1,4,2,0,1,4,-1),
> > >             c(1,1,1,4,3,0,1,4,-1),
> > >             c(1,1,1,4,6,0,1,5,-1),
> > >             c(1,1,1,4,6,0,1,6,-1),
> > >             c(1,1,1,4,6,0,1,7,-1),
> > >             c(1,1,1,4,6,0,1,8,-1))
> > > obj.tr <- t(obj)
> > > obj.tr
> > > > obj.tr
> > >     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> > > [1,]    1    1    1    4    0    0    1    4   -1
> > > [2,]    0    1    1    4    1    0    1    4   -1
> > > [3,]    1    1    1    4    2    0    1    4   -1
> > > [4,]    1    1    1    4    3    0    1    4   -1
> > > [5,]    1    1    1    4    6    0    1    5   -1
> > > [6,]    1    1    1    4    6    0    1    6   -1
> > > [7,]    1    1    1    4    6    0    1    7   -1
> > > [8,]    1    1    1    4    6    0    1    8   -1
> > > >
> > >
> > > How can I do to check columns 2,3,4,6,7 and 9 have
> > > the same value, and columns 1,5 and 8 have different values.
> > >
> > > Best, Muhammad Subianto
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


From Achim.Zeileis at R-project.org  Fri Aug 25 03:19:52 2006
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Fri, 25 Aug 2006 03:19:52 +0200
Subject: [R] [R-pkgs] zoo: new version 1.2-0
Message-ID: <20060825031952.1792caf0.Achim.Zeileis@R-project.org>

Dear useRs,

the new version 1.2-0 of the zoo package for dealing with regular and
irregular time series data is available from the CRAN mirrors.

This version includes two important changes/enhancements:

  - rapply() was re-named to rollapply() because from R 2.4.0 on,
    base R provides a function rapply() for recursive (not rolling)
    application of functions, which was already described in the Green
    Book. zoo::rapply() currently still exists for backward
    compatibility, however, it is flagged as deprecated and now
    dispatches to rollapply() methods. We recommend to change existing
    scripts from using rapply() to rollapply().

  - xyplot() methods for "zoo", "ts", and "its" objects have been
    added for creating trellis time series graphs. The functions
    are still under development and suggestions for improvement are
    welcome.

For general introductions to the package see:
  vignette("zoo", package = "zoo")
  vignette("zoo-quickref", package = "zoo")

Best wishes,
Z

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From Achim.Zeileis at R-project.org  Fri Aug 25 03:28:55 2006
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Fri, 25 Aug 2006 03:28:55 +0200
Subject: [R] [R-pkgs] sandwich: new version 2.0-0
Message-ID: <20060825032855.4ecccc38.Achim.Zeileis@R-project.org>

Dear useRs,

a new version 2.0-0 of the sandwich package for estimating sandwich
covariance matrices is available from the CRAN mirrors.

The tools for computing heteroskedasticity (and autocorrelation)
consistent covariance matrix estimators (also called HC
and HAC estimators, including the Eicker-Huber-White estimator)
have been generalized over the last releases from linear regression to
general parametric models. These new object-oriented features of the
sandwich package are also described in paper published in the Journal
of Statistical Software (JSS) that accompanies this release. See
  http://www.jstatsoft.org/

The new JSS paper and the previous one (accompanying version 1.0-0) are
also available as package vignettes:
  vignette("sandwich", package = "sandwich")
  vignette("sandwich-OOP", package = "sandwich")

Best wishes,
Z

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From petyuk at mail.ru  Fri Aug 25 04:25:14 2006
From: petyuk at mail.ru (Vladislav Petyuk)
Date: Fri, 25 Aug 2006 06:25:14 +0400
Subject: [R] tcltk command to figure out which widget in active or in focus
Message-ID: <E1GGRNi-00025f-00.petyuk-mail-ru@f9.mail.ru>

Hi,
I'm making an interface, where a Tcl/Tk window have few listbox widgets.
I need to select separate parameters from separate listboxes.
It is clear how to get cursor selection value, once you know which listbox widget you clicked.
The problem is I can't figure out which one tcltk command to use to get an information which listbox widget I clicked.
Thank you,
Vlad


From Bill.Venables at csiro.au  Fri Aug 25 05:39:48 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Fri, 25 Aug 2006 13:39:48 +1000
Subject: [R] Check values in colums matrix
Message-ID: <B998A44C8986644EA8029CFE6396A924840160@exqld2-bne.qld.csiro.au>

As a minor footnote to both of these, I would add that both assume
that all the columns of the dataset are numeric.  It doesn't cost much
to generalize it to cover any matrix structure, of any mode:

constantColmuns <- function(Xmat) 
    which(apply(Xmat, 2, function(z) length(unique(z)) == 1))

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
> Sent: Friday, 25 August 2006 9:37 AM
> To: 'Gabor Grothendieck'; 'Muhammad Subianto'
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Check values in colums matrix
> 
> Absolutely. But do note that if the values in obj are the product of
> numerical computations then columns of equal values may turn out to be
only
> **nearly** equal and so the sd may turn out to be **nearly** 0 and not
> exactly 0. This is a standard issue in numerical computation, of
course, and
> has been commented on in this list at least dozens of times, but it's
still
> a gotcha for the unwary (so now dozens +1).
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> > Grothendieck
> > Sent: Thursday, August 24, 2006 4:28 PM
> > To: Muhammad Subianto
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Check values in colums matrix
> > 
> > Try sd(obj.tr) which will give a vector of standard 
> > deviations, one per column.
> > A column's entry will be zero if and only if all values in the
column
> > are the same.
> > 
> > On 8/24/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> > > Dear all,
> > > I apologize if my question is quite simple.
> > > I have a dataset (20 columns & 1000 rows) which
> > > some of columns have the same value and the others
> > > have different values.
> > > Here are some piece of my dataset:
> > > obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
> > >             c(0,1,1,4,1,0,1,4,-1),
> > >             c(1,1,1,4,2,0,1,4,-1),
> > >             c(1,1,1,4,3,0,1,4,-1),
> > >             c(1,1,1,4,6,0,1,5,-1),
> > >             c(1,1,1,4,6,0,1,6,-1),
> > >             c(1,1,1,4,6,0,1,7,-1),
> > >             c(1,1,1,4,6,0,1,8,-1))
> > > obj.tr <- t(obj)
> > > obj.tr
> > > > obj.tr
> > >     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> > > [1,]    1    1    1    4    0    0    1    4   -1
> > > [2,]    0    1    1    4    1    0    1    4   -1
> > > [3,]    1    1    1    4    2    0    1    4   -1
> > > [4,]    1    1    1    4    3    0    1    4   -1
> > > [5,]    1    1    1    4    6    0    1    5   -1
> > > [6,]    1    1    1    4    6    0    1    6   -1
> > > [7,]    1    1    1    4    6    0    1    7   -1
> > > [8,]    1    1    1    4    6    0    1    8   -1
> > > >
> > >
> > > How can I do to check columns 2,3,4,6,7 and 9 have
> > > the same value, and columns 1,5 and 8 have different values.
> > >
> > > Best, Muhammad Subianto


From singularitaet at gmx.net  Fri Aug 25 08:10:38 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Fri, 25 Aug 2006 08:10:38 +0200
Subject: [R] Authoring a book
In-Reply-To: <44EDF9C1.5090107@psych.uib.no>
References: <44EB32A5.7030508@psych.uib.no> <x2lkpeuwyj.fsf@viggo.kubism.ku.dk>
	<44EDA4E9.2050606@psych.uib.no> <44EDAF63.7060006@gmx.net>
	<44EDF9C1.5090107@psych.uib.no>
Message-ID: <44EE945E.3010200@gmx.net>


>> //people.ee.ethz.ch/~oetiker/lshort/lshort.pdf
>
> It really was a "not too short" intro.  I'll have a look at it.

Yes definitly not too short. But it states in LaTeX in 133 min ...

>Seems to be for Linux only.  My server is Windows, even if I have the
rest of the components.

Hm at the projects' homepage its stated that it is OS independent:
http://sourceforge.net/projects/uniwakka/

I also dont see a hint that it is Linux dependent in the installation notes.

Maybe you think its Linux stuff because of the *.tar.gz ending? You can
unzip those files also on Windows, its just a packing file format.
For unpacking on a windows machine use either 7zip:
http://www.7-zip.org/ or the total commander: http://www.ghisler.com .
The first is a free packing program and the second is the best ever
windows file manager (if you want to open it there just double click at
the archive).

Stefan Grosse

The unpacked instl. notes:

Wakka Installation

Not much to it (as long as it works, ahem). Unpack/upload the
distribution files
into a directory that can be accessed via the web. Then go to the
corresponding URL.
A web-based installer will walk you through the rest.

Example:

If your website, say, http://www.mysite.com, is mapped to the directory
/home/jdoe/www/,
and you place the Wakka distribution files into /home/jdoe/www/wakka/,
you should go to
http://www.mysite.com/wakka/.

Note that Wakka distributions normally unpack into directories that
include the version
in their name; you'll probably want to rename those to just "wakka" --
or, if you're
on a unixoid system, set up a symbolic link.

During first installs, the installer will try to create a file called
wakka.config.php
in your Wakka directory. In order to do this, you will need to either
make the Wakka
directory writable by the web server, or create a new (empty) file called
wakka.config.php which is writable by the web server. If the installer
still fails to
create the file, it will dump the file's contents which you can then
upload manually.

IMPORTANT: for installing or upgrading Wakka, do NOT access any of the
files contained
in the setup/ subdirectory. They're used by the web-based
installer/updater, but you
should really just access the Wakka directory itself, and it will (or at
least should)
work perfectly.

Detailed instructions are available at
<http://www.wakkawiki.com/WakkaInstallation>.

- Hendrik Mans <hendrik at mans.de>


From 10133msb at comb.es  Fri Aug 25 08:49:05 2006
From: 10133msb at comb.es (Manel Salamero)
Date: Fri, 25 Aug 2006 08:49:05 +0200
Subject: [R] tktoplevel & tkgetSaveFile options
Message-ID: <200608250849.AA524288200@comb.es>

Dear list, 

Previously I posted these question to R-SIG-GUI. Perhaps here is a better place.

1. Is there some option for mantaining the tktoplevel window always on top?
2. Is there some option to eliminate the border icons maximize, minimize and close of a tktoplevel window?
3. Is possible to avoid the warning message (or changing its contents) in tkgetSaveFile when the file to save already exists?

Thanks,

Manel


From dieter.menne at menne-biomed.de  Fri Aug 25 08:56:00 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 25 Aug 2006 06:56:00 +0000 (UTC)
Subject: [R] xyplot tick marks and line thickness
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
	<971536df0608241002j5704714ch77d9d83aeb40840f@mail.gmail.com>
	<eb555e660608241119j7438ac0cs12ab04f7edb7d171@mail.gmail.com>
Message-ID: <loom.20060825T085317-589@post.gmane.org>

Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:

> Right. the grid call shouldn't be necessary, "axis.line" controls the
> panel borders. And tck can be a vector, to get rid of the ugly bumps
> on top:
> 
> xyplot(x ~ x | g, type = "l", lwd = lwd,
>        scales = list(tck = c(-1, 0)),
>        par.settings =
>        list(axis.line = list(lwd = lwd),
>             strip.border = list(lwd = lwd)))
> 

The tickmarks look strange for me (Windows, R 2.3.1, all packages updated today)

See http://www.menne-biomed.de/uni/gridit.png.

This was created on screen, saved as a bitmap, and 3x enlarged lower portion.

Dieter


From blomsp at ozemail.com.au  Fri Aug 25 09:04:10 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Fri, 25 Aug 2006 17:04:10 +1000
Subject: [R] R in Nature
Message-ID: <44EEA0EA.6070809@ozemail.com.au>

Hi all,

We've just had a paper accepted for publication in Nature. We used R for 
95% of our analyses (one of my co-authors sneaked in some GenStat when I 
wasn't looking.). The preprint is available from the Nature web site, in 
the open peer-review trial section. I searched Nature for previous 
references to "R Development Core Team", and I received no hits. So I 
tentatively conclude that our paper is the first Nature paper to cite R.

A great many thanks to the R Development Core Team for R, and Prof. 
Bates for lmer.

Cheers,

Simon.
(I'm off to the pub to celebrate.)

-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat. 
Centre for Resource and Environmental Studies
The Australian National University              
Canberra ACT 0200                               
Australia                                       
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer 
can be extracted from a given body of data.
- John Tukey.


From singularitaet at gmx.net  Fri Aug 25 10:08:10 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Fri, 25 Aug 2006 10:08:10 +0200
Subject: [R] exact Wilcoxon signed rank test with ties and the "no longer
 under development" exactRanksumTests package
Message-ID: <44EEAFEA.4050301@gmx.net>

Dear List,

after updating the exactRanksumTests package I receive a warning that
the package is not developed any further and that one should consider
the coin package.

I don't find the signed rank test in the coin package, only the Wilcoxon
Mann Whitney U-Test. I only found a signed rank test in the stats
package (wilcox.test) which is able to calculate the exact pvalues but
unfortunately the procedure cannot calculate the exact values with ties.

Is there any other package that is providing a similiar test? Or is
there an easy work out how to take the ties into account? (Or a chance
that the correction is taken into account for the stats package?)

Stefan Grosse

Take the following example from Bortz/Lienert/Boehnke:

> x1<-c(9,14,8,11,14,10,8,14,12,14,13,9,15,12,9)
> x2<-c(13,15,9,12,16,10,8,13,12,16,9,10,16,12,9)

# exactRankTests package:

> wilcox.exact(x1,x2,paired=TRUE)

        Exact Wilcoxon signed rank test

data:  x1 and x2
V = 13, p-value = 0.1367
alternative hypothesis: true mu is not equal to 0

# wilcox.test by stats package:

> wilcox.test(x1,x2,paired=TRUE,exact=TRUE)

        Wilcoxon signed rank test with continuity correction

data:  x1 and x2
V = 13, p-value = 0.1436
alternative hypothesis: true mu is not equal to 0

Warning messages:
1: cannot compute exact p-value with ties in: wilcox.test.default(x1,
x2, paired = TRUE, exact = TRUE)
2: cannot compute exact p-value with zeroes in: wilcox.test.default(x1,
x2, paired = TRUE, exact = TRUE)


From karloh at mi.uib.no  Fri Aug 25 10:14:39 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Fri, 25 Aug 2006 10:14:39 +0200
Subject: [R] xyplot tick marks and line thickness
References: <20060824155302.38364.qmail@web27614.mail.ukl.yahoo.com>
Message-ID: <ecmbhf$v64$1@sea.gmane.org>

Piet Bell skreiv:

> The?publisher?of?our?paper?has?requested:
> 
> 1.?all?tick?marks?should?point?inwards?instead?of?outwards.

Point him to William S. Cleveland?s excellent book /The Elements of Graphing
Data/, where Cleveland strongly recommends that tick marks should point
*outwards* ?because ticks that point inward can obscure data?. See the
discussion on pages 31?35, and especially figure 2.12 and 2.13.

> 2.?All?lines?should?be?thicker?(lines,?axes,?boxes,?etc.?Everything).
> Lines?is?easy...I?used:??lwd=1.5???but?what?about?the?lines?of?the?axes,
> and?the?lines?that?build?up?the?plot?itself?....?

I find that

library(Hmisc)
setps("filename") # Or setpdf. You might also want to add 'color=TRUE'.
... plotting commands ...
dev.off()

usually gives much better-looking plots, and with thicker lines.

-- 
Karl Ove Hufthammer
E-mail and Jabber: karl at huftis.org


From subianto at cs.uu.nl  Fri Aug 25 10:28:33 2006
From: subianto at cs.uu.nl (Muhammad Subianto)
Date: Fri, 25 Aug 2006 10:28:33 +0200
Subject: [R] Check values in colums matrix
In-Reply-To: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
References: <c7c17cef0608240959h1dccba64t777e6e76a3bf677@mail.gmail.com>
Message-ID: <44EEB4B1.1060200@cs.uu.nl>

Dear all,
I would like to thank everybody who replied for their useful 
suggestions. Maybe, I am going through the book statistics to teach 
(fresh) myself.
Wish you have a nice weekend.

Regards, Muhammad Subianto


On this day 24/08/2006 18:59, Muhammad Subianto wrote:
> Dear all,
> I apologize if my question is quite simple.
> I have a dataset (20 columns & 1000 rows) which
> some of columns have the same value and the others
> have different values.
> Here are some piece of my dataset:
> obj <- cbind(c(1,1,1,4,0,0,1,4,-1),
>              c(0,1,1,4,1,0,1,4,-1),
>              c(1,1,1,4,2,0,1,4,-1),
>              c(1,1,1,4,3,0,1,4,-1),
>              c(1,1,1,4,6,0,1,5,-1),
>              c(1,1,1,4,6,0,1,6,-1),
>              c(1,1,1,4,6,0,1,7,-1),
>              c(1,1,1,4,6,0,1,8,-1))
> obj.tr <- t(obj)
> obj.tr
>> obj.tr
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
> [1,]    1    1    1    4    0    0    1    4   -1
> [2,]    0    1    1    4    1    0    1    4   -1
> [3,]    1    1    1    4    2    0    1    4   -1
> [4,]    1    1    1    4    3    0    1    4   -1
> [5,]    1    1    1    4    6    0    1    5   -1
> [6,]    1    1    1    4    6    0    1    6   -1
> [7,]    1    1    1    4    6    0    1    7   -1
> [8,]    1    1    1    4    6    0    1    8   -1
> 
> How can I do to check columns 2,3,4,6,7 and 9 have
> the same value, and columns 1,5 and 8 have different values.
> 
> Best, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rdiaz at cnio.es  Fri Aug 25 10:38:05 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 25 Aug 2006 10:38:05 +0200
Subject: [R] Authoring a book
In-Reply-To: <44EDF9C1.5090107@psych.uib.no>
References: <44EB32A5.7030508@psych.uib.no> <44EDAF63.7060006@gmx.net>
	<44EDF9C1.5090107@psych.uib.no>
Message-ID: <200608251038.05544.rdiaz@cnio.es>

Dear Tom,

To add a few things to explore:

- I'd definitely go with LaTeX. Depending on how much formatting control you 
want, though, and if your coworkers are reluctant to jump into LaTeX, you 
might start with reStructuredText (http://docutils.sourceforge.net/rst.html) 
or text2tags (http://txt2tags.sourceforge.net/). With both, you can produce 
LaTeX, but innitially at least it allows you to write text with structure 
using markup that is a lot simpler than latex.


- I'd definitely use a version control system. Instead of CVS or SVN, though, 
I'd suggest you take a look at some of the distributed ones, in particular 
Bazaar-NG (http://bazaar-vcs.org), Mercurial 
(http://www.selenic.com/mercurial/wiki/index.cgi) or Darcs 
(http://abridgegame.org/darcs/). 

These three are probably among the most mature ones (though oppinions will 
vary, of course; I have some notes and links at: 
http://www.ligarto.org/rdiaz/VersionControl.html). 

What I like about any of these is that I think they provide you essentially 
all SVN can provide (except for the user-base and years of existence of SVN) 
plus a lot more. For instance, if you often work without access to the remote 
repository, with any of these three systems you can enjoy all the benifits of 
version control. Cherry-picking is easier with any of these than with 
CVS/SVN, and Darcs in particular excels at it.


- For bibliography, I find CiteULike (http://www.citeulike.org/) fabulous. 
Needs internet access, and might not work with the journals/data bases that 
you use, though. It can export as bibtex.


- If you find outliners useful (or absolutely essential) then you might want 
to look at Leo (http://webpages.charter.net/edreamleo/front.html). Leo is 
agnostic regarding whether you write LaTeX, plain text, or R code (though it 
has great support for some languages such as Python or rst), and you can use 
Leo and still edit files in your editor of choice (I use Leo for working with 
fairly large latex files that I edit under Emacs). However, for this to work, 
all of you should agree to use Leo (or at least not disturb the "sentinel 
lines" that leo uses).


Hope this helps (or at least provides entertaining links :-).

R.


On Thursday 24 August 2006 21:10, Tom Backer Johnsen wrote:
> Stefan Grosse wrote:
> > I think Peter Dalgaard is right.
> >
> > Since you are able to use R I believe you will be very fast in learning
> > LaTeX.
> >
> > I think it needs less then a week to learn the most common LaTeX
> > commands. And setting up a wiki and trying then to convert this into a
> > printable document format plus learning the wiki syntax is probably more
> > time consuming. Beside this R is able to work perfectly together with
> > LaTeX, it creates LaTeX output and is doing excellent graphics in the
> > EPS/PS format.
> >
> > The best introduction for LaTeX is the not so short introduction:
> > http://people.ee.ethz.ch/~oetiker/lshort/lshort.pdf
>
> It really was a "not too short" intro.  I'll have a look at it.
>
> > If you still are not convinced have a look at UniWakkaWiki:
> > http://uniwakka.sourceforge.net/HomePage
> >
> > It is a Wiki for Science and University purposes and claims to be able
> > to export to Openoffice as well as to LaTeX.
>
> Looks interesting and I really like the concept, but how stable is it?
>   It looks rather fresh from the web page, but I may be wrong.  A
> bibliography function is really a big advantage, so ... perhaps.
>
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Aug 25 11:49:36 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 25 Aug 2006 11:49:36 +0200 (CEST)
Subject: [R] exact Wilcoxon signed rank test with ties and the "no
	longerunder development" exactRanksumTests package
In-Reply-To: <44EEAFEA.4050301@gmx.net>
References: <44EEAFEA.4050301@gmx.net>
Message-ID: <Pine.LNX.4.64.0608251145040.28525@imbe153.imbe.med.uni-erlangen.de>


On Fri, 25 Aug 2006, Stefan Grosse wrote:

> Dear List,
>

Stefan,

> after updating the exactRanksumTests package I receive a warning that
> the package is not developed any further and that one should consider
> the coin package.
>
> I don't find the signed rank test in the coin package, only the Wilcoxon
> Mann Whitney U-Test. I only found a signed rank test in the stats
> package (wilcox.test) which is able to calculate the exact pvalues but
> unfortunately the procedure cannot calculate the exact values with ties.
>

indeed, this is the only gap to be filled in `coin', I just haven't had 
the time to implement this. And of course, `exactRankTests' is still 
available and will be available in the future. So you _can_ use it!

The message just means that I'm not going to add _new_ features
to `exactRankTests' and that we as the authors of both packages
believe that the `coin'-way of doing things is more appropriate.

Hope that helps & sorry for the confusion!

Torsten

ps: please cc emails about packages to the maintainer.

> Is there any other package that is providing a similiar test? Or is
> there an easy work out how to take the ties into account? (Or a chance
> that the correction is taken into account for the stats package?)
>
> Stefan Grosse
>
> Take the following example from Bortz/Lienert/Boehnke:
>
>> x1<-c(9,14,8,11,14,10,8,14,12,14,13,9,15,12,9)
>> x2<-c(13,15,9,12,16,10,8,13,12,16,9,10,16,12,9)
>
> # exactRankTests package:
>
>> wilcox.exact(x1,x2,paired=TRUE)
>
>        Exact Wilcoxon signed rank test
>
> data:  x1 and x2
> V = 13, p-value = 0.1367
> alternative hypothesis: true mu is not equal to 0
>
> # wilcox.test by stats package:
>
>> wilcox.test(x1,x2,paired=TRUE,exact=TRUE)
>
>        Wilcoxon signed rank test with continuity correction
>
> data:  x1 and x2
> V = 13, p-value = 0.1436
> alternative hypothesis: true mu is not equal to 0
>
> Warning messages:
> 1: cannot compute exact p-value with ties in: wilcox.test.default(x1,
> x2, paired = TRUE, exact = TRUE)
> 2: cannot compute exact p-value with zeroes in: wilcox.test.default(x1,
> x2, paired = TRUE, exact = TRUE)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From andrej.kastrin at siol.net  Fri Aug 25 12:28:28 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Fri, 25 Aug 2006 12:28:28 +0200
Subject: [R] R in Nature
In-Reply-To: <44EEA0EA.6070809@ozemail.com.au>
References: <44EEA0EA.6070809@ozemail.com.au>
Message-ID: <44EED0CC.4070005@siol.net>

Simon Blomberg wrote:
> Hi all,
>
> We've just had a paper accepted for publication in Nature. We used R for 
> 95% of our analyses (one of my co-authors sneaked in some GenStat when I 
> wasn't looking.). The preprint is available from the Nature web site, in 
> the open peer-review trial section. I searched Nature for previous 
> references to "R Development Core Team", and I received no hits. So I 
> tentatively conclude that our paper is the first Nature paper to cite R.
>
> A great many thanks to the R Development Core Team for R, and Prof. 
> Bates for lmer.
>
> Cheers,
>
> Simon.
> (I'm off to the pub to celebrate.)
>
>   
Congratulations....
but I cannot find your article on 
http://blogs.nature.com/nature/peerreview/trial
Can you post valid link to it? Thanks in advance.

Andrej


From p.dalgaard at biostat.ku.dk  Fri Aug 25 13:41:30 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Aug 2006 13:41:30 +0200
Subject: [R] R in Nature
In-Reply-To: <44EED0CC.4070005@siol.net>
References: <44EEA0EA.6070809@ozemail.com.au> <44EED0CC.4070005@siol.net>
Message-ID: <x21wr5rqj9.fsf@viggo.kubism.ku.dk>

Andrej Kastrin <andrej.kastrin at siol.net> writes:

> Simon Blomberg wrote:
> > Hi all,
> >
> > We've just had a paper accepted for publication in Nature. We used R for 
> > 95% of our analyses (one of my co-authors sneaked in some GenStat when I 
> > wasn't looking.). The preprint is available from the Nature web site, in 
> > the open peer-review trial section. I searched Nature for previous 
> > references to "R Development Core Team", and I received no hits. So I 
> > tentatively conclude that our paper is the first Nature paper to cite R.
> >
> > A great many thanks to the R Development Core Team for R, and Prof. 
> > Bates for lmer.
> >
> > Cheers,
> >
> > Simon.
> > (I'm off to the pub to celebrate.)
> >
> >   
> Congratulations....
> but I cannot find your article on 
> http://blogs.nature.com/nature/peerreview/trial
> Can you post valid link to it? Thanks in advance.
> 
> Andrej

Found it in the Ecology section. 

http://blogs.nature.com/nature/peerreview/trial/Fisher%20manuscript.pdf

(BTW: The year in the reference is wrong, R 2.3.1 is from 2006.)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From j.van_den_hoff at fz-rossendorf.de  Fri Aug 25 13:52:04 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Fri, 25 Aug 2006 13:52:04 +0200
Subject: [R] extremely slow recursion in R?
In-Reply-To: <Pine.LNX.4.64.0608241558140.28297@homer21.u.washington.edu>
References: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
	<Pine.LNX.4.64.0608241558140.28297@homer21.u.washington.edu>
Message-ID: <44EEE464.3010603@fz-rossendorf.de>

Thomas Lumley wrote:
> On Thu, 24 Aug 2006, Jason Liao wrote:
> 
>> I recently coded a recursion algorithm in R and ir ran a few days
>> without returning any result. So I decided to try a simple case of
>> computing binomial coefficient using recusrive relationship
>>
>> choose(n,k) = choose(n-1, k)+choose(n-1,k-1)
>>
>> I implemented in R and Fortran 90 the same algorithm (code follows).
>> The R code finishes 31 minutes and the Fortran 90 program finishes in 6
>> seconds. So the Fortran program is 310 times faster. I thus wondered if
>> there is room for speeding up recursion in R. Thanks.
>>
> 
> Recursive code that computes the same case many times can often be sped up 
> by memoization, eg
> 
> memo<-new.env(hash=TRUE)
> chewse<-function(n,k) {
>      if (n==k) return(1)
>      if(k==1) return(n)
> 
>      if(exists(paste(n,k),memo,inherits=FALSE))
>          return(get(paste(n,k),memo))
>      rval<-chewse(n-1,k)+chewse(n-1,k-1)
>      assign(paste(n,k),rval,envir=memo)
>      return(rval)
> }
> 
> This idea was discussed in an early Programmers' Niche article by Bill 
> Venables in R News.
> 
> However, I'm surprised that you're surprised that compiled Fortran 90 is 
> 310 times faster than interpreted R.  That would be about what I would 
> expect for code that isn't making use of vectorized functions in R.
> 
> 
>  	-thomas
>

maybe someone's interested:
I made the same observation of seemingly very slow recursion recently: 
just for fun I used the (in)famously inefficient

fib <- function(n = 1) {
    if (n < 2)
       fn <- 1
    else
       fn <- fib(n - 1) + fib(n - 2)
    fn
}

for calculating the fibonacci numbers and compared `fib(30)' (about 
1.3e6 recursive function calls ...) to some other languages (times in sec):

language  time
==============
C          0.034  (compiled, using integers)
Ocaml      0.041  (compiled, using integers)
Ocaml      0.048  (interpreted, using integers)
C          0.059  (compiled, using floats)
Lua        1.1
ruby       3.4
R          21
octave    120

apart from octave (which seems to have a _real_ issue with recursive 
function calls), R is by far the slowest in this list and still a factor 
7-20 slower than the interpreter based Lua and ruby. the speed loss 
compared to C or Ocaml is about a factor of 350-600 here (Ocaml keeps 
its speed more or less in this simple example even in 'script mode', 
which is remarkable, I think (and it usually loses only a factor of 
about 7 or so in script mode compared to the compiled variant)

for the specialists the bad performance of R in this situation might not 
be surprising, but I was not aware that recursive function calls are 
seemingly as expensive as explicit loops (where the execution time ratio 
of R to C again is of the same order, i.e. =~ 400).

of course, such comparsions don't make too much sense: the reason to use 
R will definitely not be its speed (which, luckily, often does not 
matter), but the combination of flexible language, the number of 
available libraries and the good 2D graphics.



joerg


From jrkrideau at yahoo.ca  Fri Aug 25 14:26:38 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 25 Aug 2006 08:26:38 -0400 (EDT)
Subject: [R] Using a 'for' loop : there should be a better way in R
In-Reply-To: <971536df0608241604g1725bed7re9f983396bfc6d92@mail.gmail.com>
Message-ID: <20060825122638.18915.qmail@web32805.mail.mud.yahoo.com>


--- Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> Use cbind to create a two column matrix, mat,
> and multiply that by the appropriate inflation
> factors.
> Then use rowsum to sum the rows according to the
> id grouping factor.
> 
> inf.fac <- list(year1 = 1, year2 = 5, year3 = 10)
> mat <- cbind(s1 = df1$cat1 + df1$cat2, s2 = df1$cat3
> + df1$cat4)
> rowsum(mat * unlist(inf.fac[df1$year]), df1$id)

Thanks very much.  It took me a few minutes to see
what was happening but it is lovely.  I would never
have thought of using a list like that.
> 
> 
> On 8/24/06, John Kane <jrkrideau at yahoo.ca> wrote:
> > I need to apply a yearly inflation factor to some
> > wages and supply some simple sums by work
> category.  I
> > have gone at it with a brute force "for" loop
> approach
> >  which seems okay as it is a small dataset.  It
> looks
> > a bit inelegant and given all the warnings in the
> > Intro to R, etc, about using loops I wondered  if
> > anyone could suggest something a bit simpler or
> more
> > efficent?
> >
> > Example:
> >
> > cat1 <- c( 1,1,6,1,1,5)
> > cat2 <- c( 1,2,3,4,5,6)
> > cat3 <- c( 5,4,6,7,8,8)
> > cat4 <- c( 1,2,1,2,1,2)
> > years <- c( 'year1', 'year2', 'year3', 'year3',
> > 'year1', 'year1')
> > id <-  c('a','a','b','c','c','a')
> > df1 <- data.frame(id,years,cat1,cat2, cat3, cat4)
> >
> > nn <- levels(df1$id)    # levels for outer loop
> > hh <- levels(df1$years) # levels for inter loop
> >
> >
> > mp <- c(1, 5, 10)   # inflation factor
> >
> > tt <- data.frame(matrix(NA, length(nn), 2))
> > names(tt) <- c("s1","s2")
> > rownames(tt) <- nn
> >
> > for (i in 1:length(nn)){
> > scat <- data.frame(matrix(NA, length(hh),2))
> > dd1 <- subset(df1, id==nn[i])
> > for (j in 1:length(hh)){
> > dd2 <- subset(dd1, dd1$years==hh[j])
> > s1 <- sum(dd2$cat1,dd2$cat2, na.rm=T)
> > s2 <- sum(dd2$cat3,dd2$cat4,na.rm=T)
> > scat[j,] <- c(s1,s2) *mp[j]    # multiply by the
> > inflation factor
> > }
> > crush <- apply(scat, 2, sum)
> > tt[i,] <- crush
> > }
> > tt
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
>


From clcarter at umd.edu  Fri Aug 25 15:08:06 2006
From: clcarter at umd.edu (Catherine Carter)
Date: Fri, 25 Aug 2006 09:08:06 -0400
Subject: [R] plot question
Message-ID: <44EEF636.9080702@umd.edu>

Hi everyone,

I have what may appear to be a newbie question, but I have looked 
everywhere I can think to look and I cannot find an answer. On page 35 
of "An Introduction to R" the following command appears: 
plot(ecdf(eruptions), do.points=FALSE, verticals=TRUE). What is the 
do.points argument? I know what it does (suppresses printing of the 
points) but where can I find help on it? I want to be able to explain it 
fully to my students.

Thanks for your help,
Cathy

-- 
Dr. Cathy Carter
Department of Geography
University of Maryland
1111A LeFrak Hall
301.405.4620


From rdpeng at gmail.com  Fri Aug 25 15:21:04 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Fri, 25 Aug 2006 09:21:04 -0400
Subject: [R] plot question
In-Reply-To: <44EEF636.9080702@umd.edu>
References: <44EEF636.9080702@umd.edu>
Message-ID: <44EEF940.70504@gmail.com>

Take a look at ?plot.stepfun.

'ecdf()' returns an object of class "ecdf" inheriting from class "stepfun" and 
'plot.ecdf()' calls 'plot.stepfun'.

-roger

Catherine Carter wrote:
> Hi everyone,
> 
> I have what may appear to be a newbie question, but I have looked 
> everywhere I can think to look and I cannot find an answer. On page 35 
> of "An Introduction to R" the following command appears: 
> plot(ecdf(eruptions), do.points=FALSE, verticals=TRUE). What is the 
> do.points argument? I know what it does (suppresses printing of the 
> points) but where can I find help on it? I want to be able to explain it 
> fully to my students.
> 
> Thanks for your help,
> Cathy
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From jg_liao at yahoo.com  Fri Aug 25 15:22:31 2006
From: jg_liao at yahoo.com (Jason Liao)
Date: Fri, 25 Aug 2006 06:22:31 -0700 (PDT)
Subject: [R] extremely slow recursion in R?
In-Reply-To: <44EEE464.3010603@fz-rossendorf.de>
Message-ID: <20060825132232.24368.qmail@web53713.mail.yahoo.com>


Thank you, Thomas and Joerg! Joerg's example is extremely useful. In
fact I had wanted to compare R with other interpreting language myself.
I heard that Ruby is not fast among scripting languages. I thus believe
that there is room to improve R's recursive function.

Jason
 
--- Joerg van den Hoff <j.van_den_hoff at fz-rossendorf.de> wrote:
> maybe someone's interested:
> I made the same observation of seemingly very slow recursion
> recently: 
> just for fun I used the (in)famously inefficient
> 
> fib <- function(n = 1) {
>     if (n < 2)
>        fn <- 1
>     else
>        fn <- fib(n - 1) + fib(n - 2)
>     fn
> }
> 
> for calculating the fibonacci numbers and compared `fib(30)' (about 
> 1.3e6 recursive function calls ...) to some other languages (times in
> sec):
> 
> language  time
> ==============
> C          0.034  (compiled, using integers)
> Ocaml      0.041  (compiled, using integers)
> Ocaml      0.048  (interpreted, using integers)
> C          0.059  (compiled, using floats)
> Lua        1.1
> ruby       3.4
> R          21
> octave    120
> 
> apart from octave (which seems to have a _real_ issue with recursive 
> function calls), R is by far the slowest in this list and still a
> factor 
> 7-20 slower than the interpreter based Lua and ruby. the speed loss 
> compared to C or Ocaml is about a factor of 350-600 here (Ocaml keeps
> 
> its speed more or less in this simple example even in 'script mode', 
> which is remarkable, I think (and it usually loses only a factor of 
> about 7 or so in script mode compared to the compiled variant)
> 
> for the specialists the bad performance of R in this situation might
> not 
> be surprising, but I was not aware that recursive function calls are 
> seemingly as expensive as explicit loops (where the execution time
> ratio 
> of R to C again is of the same order, i.e. =~ 400).
> 
> of course, such comparsions don't make too much sense: the reason to
> use 
> R will definitely not be its speed (which, luckily, often does not 
> matter), but the combination of flexible language, the number of 
> available libraries and the good 2D graphics.
> 
> 
> 
> joerg
> 


Jason Liao, http://www.geocities.com/jg_liao
Department of Epidemiology and Biostatistics
Drexel University School of Public Health
245 N. 15th Street, Mail Stop 660
Philadelphia, PA 19102-1192
phone 215-762-3934


From MSchwartz at mn.rr.com  Fri Aug 25 15:24:03 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 25 Aug 2006 08:24:03 -0500
Subject: [R] plot question
In-Reply-To: <44EEF636.9080702@umd.edu>
References: <44EEF636.9080702@umd.edu>
Message-ID: <1156512243.5558.10.camel@localhost.localdomain>

On Fri, 2006-08-25 at 09:08 -0400, Catherine Carter wrote:
> Hi everyone,
> 
> I have what may appear to be a newbie question, but I have looked 
> everywhere I can think to look and I cannot find an answer. On page 35 
> of "An Introduction to R" the following command appears: 
> plot(ecdf(eruptions), do.points=FALSE, verticals=TRUE). What is the 
> do.points argument? I know what it does (suppresses printing of the 
> points) but where can I find help on it? I want to be able to explain it 
> fully to my students.
> 
> Thanks for your help,
> Cathy

A couple of options:

1. If you are aware of how R uses "method dispatch", then you might know
that the plot() function is a generic method and that plot.ecdf() is the
specific method that is called when the initial argument is of class
"ecdf", as is the case above. 

Thus, using ?plot.ecdf will get you to the help page, where there is a
notation in the Arguments section that the '...' arguments are then
passed to plot.stepfun(). 'do.points' is passed in this fashion, so
using ?plot.stepfun will then get you to the help page where 'do.points'
is defined as:

  logical; if true, also draw points at the (xlim restricted)
  knot locations.


2. Using:

  RSiteSearch("do.points", restrict = "functions")

will search the online function documentation, bringing up a browser
window, where the first link gets you to ?plot.stepfun.

HTH,

Marc Schwartz


From clcarter at umd.edu  Fri Aug 25 15:25:24 2006
From: clcarter at umd.edu (Catherine Carter)
Date: Fri, 25 Aug 2006 09:25:24 -0400
Subject: [R] plot question
In-Reply-To: <44EEF940.70504@gmail.com>
References: <44EEF636.9080702@umd.edu> <44EEF940.70504@gmail.com>
Message-ID: <44EEFA44.7050109@umd.edu>

Thank you! That is exactly what I needed.

Roger D. Peng wrote:

> Take a look at ?plot.stepfun.
>
> 'ecdf()' returns an object of class "ecdf" inheriting from class 
> "stepfun" and 'plot.ecdf()' calls 'plot.stepfun'.
>
> -roger
>
> Catherine Carter wrote:
>
>> Hi everyone,
>>
>> I have what may appear to be a newbie question, but I have looked 
>> everywhere I can think to look and I cannot find an answer. On page 
>> 35 of "An Introduction to R" the following command appears: 
>> plot(ecdf(eruptions), do.points=FALSE, verticals=TRUE). What is the 
>> do.points argument? I know what it does (suppresses printing of the 
>> points) but where can I find help on it? I want to be able to explain 
>> it fully to my students.
>>
>> Thanks for your help,
>> Cathy
>>
>

-- 
Dr. Cathy Carter
Department of Geography
University of Maryland
1111A LeFrak Hall
301.405.4620


From bianca.vieru at free.fr  Fri Aug 25 16:14:54 2006
From: bianca.vieru at free.fr (Bianca Vieru-Dimulescu)
Date: Fri, 25 Aug 2006 16:14:54 +0200
Subject: [R] correlation between 3 vectors
Message-ID: <44EF05DE.8040604@free.fr>

Hi R-users,

I am trying to calculate the correlation between 3 vectors of numbers.
Is there any other solution then passing by kendall.w function which 
need ranks in input?

Thanks,
Bianca Dimulescu


From damien.moore at excite.com  Fri Aug 25 16:28:23 2006
From: damien.moore at excite.com (Damien Moore)
Date: Fri, 25 Aug 2006 10:28:23 -0400 (EDT)
Subject: [R] extremely slow recursion in R?
Message-ID: <20060825142823.68EE58B33F@xprdmxin.myway.com>


i tried fib(30):

R: 8.99s
Python (interpreted): 0.96s
Python (interpreted but using psyco library): 0.03s
C++: 0.015s

cheers
Damien

 --- On Fri 08/25, Joerg van den Hoff < j.van_den_hoff at fz-rossendorf.de > wrote:
From: Joerg van den Hoff [mailto: j.van_den_hoff at fz-rossendorf.de]
To: tlumley at u.washington.edu
     Cc: jg_liao at yahoo.com, r-help at stat.math.ethz.ch
Date: Fri, 25 Aug 2006 13:52:04 +0200
Subject: Re: [R] extremely slow recursion in R?

Thomas Lumley wrote:<br>> On Thu, 24 Aug 2006, Jason Liao wrote:<br>> <br>>> I recently coded a recursion algorithm in R and ir ran a few days<br>>> without returning any result. So I decided to try a simple case of<br>>> computing binomial coefficient using recusrive relationship<br>>><br>>> choose(n,k) = choose(n-1, k)+choose(n-1,k-1)<br>>><br>>> I implemented in R and Fortran 90 the same algorithm (code follows).<br>>> The R code finishes 31 minutes and the Fortran 90 program finishes in 6<br>>> seconds. So the Fortran program is 310 times faster. I thus wondered if<br>>> there is room for speeding up recursion in R. Thanks.<br>>><br>> <br>> Recursive code that computes the same case many times can often be sped up <br>> by memoization, eg<br>> <br>> memo<-new.env(hash=TRUE)<br>> chewse<-function(n,k) {<br>>      if (n==k) return(1)<br>>      if(k==1) return(n)<br>> <br>>      if(exists(paste(n,k),memo,inherits=FALSE))<br>>          return(get(paste(n,k),memo))<br>>      
rval<-chewse(n-1,k)+chewse(n-1,k-1)<br>>      assign(paste(n,k),rval,envir=memo)<br>>      return(rval)<br>> }<br>> <br>> This idea was discussed in an early Programmers' Niche article by Bill <br>> Venables in R News.<br>> <br>> However, I'm surprised that you're surprised that compiled Fortran 90 is <br>> 310 times faster than interpreted R.  That would be about what I would <br>> expect for code that isn't making use of vectorized functions in R.<br>> <br>> <br>>  	-thomas<br>><br><br>maybe someone's interested:<br>I made the same observation of seemingly very slow recursion recently: <br>just for fun I used the (in)famously inefficient<br><br>fib <- function(n = 1) {<br>    if (n < 2)<br>       fn <- 1<br>    else<br>       fn <- fib(n - 1) + fib(n - 2)<br>    fn<br>}<br><br>for calculating the fibonacci numbers and compared `fib(30)' (about <br>1.3e6 recursive function calls ...) to some other languages (times in sec):<br><br>language  time<br>==============<br>C         
 0.034  (compiled, using integers)<br>Ocaml      0.041  (compiled, using integers)<br>Ocaml      0.048  (interpreted, using integers)<br>C          0.059  (compiled, using floats)<br>Lua        1.1<br>ruby       3.4<br>R          21<br>octave    120<br><br>apart from octave (which seems to have a _real_ issue with recursive <br>function calls), R is by far the slowest in this list and still a factor <br>7-20 slower than the interpreter based Lua and ruby. the speed loss <br>compared to C or Ocaml is about a factor of 350-600 here (Ocaml keeps <br>its speed more or less in this simple example even in 'script mode', <br>which is remarkable, I think (and it usually loses only a factor of <br>about 7 or so in script mode compared to the compiled variant)<br><br>for the specialists the bad performance of R in this situation might not <br>be surprising, but I was not aware that recursive function calls are <br>seemingly as expensive as explicit loops (where the execution time ratio 
<br>of R to C again is of the same order, i.e. =~ 400).<br><br>of course, such comparsions don't make too much sense: the reason to use <br>R will definitely not be its speed (which, luckily, often does not <br>matter), but the combination of flexible language, the number of <br>available libraries and the good 2D graphics.<br><br><br><br>joerg<br><br>______________________________________________<br>R-help at stat.math.ethz.ch mailing list<br>https://stat.ethz.ch/mailman/listinfo/r-help<br>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<br>and provide commented, minimal, self-contained, reproducible code.<br>


From jfox at mcmaster.ca  Fri Aug 25 16:30:32 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 25 Aug 2006 10:30:32 -0400
Subject: [R] Total (un)standardized effects in SEM?
Message-ID: <20060825143032.XEPD13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Rense,

(This question was posted a few days ago when I wasn't reading my email.)

So-called effect decompositions are simple functions of the structural
coefficients of the model, which in a model fit by sem() are contained in
the $A component of the returned object. (See ?sem.) One approach,
therefore, would be to put the coefficients in the appropriate locations of
the estimated Beta, Gamma, Lamda-x, and Lambda-y matrices of the LISREL
model, and then to compute the "effects" in the usual manner.

It should be possible to do this for the RAM formulation of the model as
well, simply by distinguishing exogenous from endogenous variables. Here's
an illustration using model C in the LISREL 7 Manual, pp. 169-177, for the
Wheaton et al. "stability of alienation" data (a common example--I happen to
have an old LISREL manual handy):

> S.wh <- matrix(c(
+    11.834,     0,        0,        0,       0,        0,
+     6.947,    9.364,     0,        0,       0,        0,
+     6.819,    5.091,   12.532,     0,       0,        0,
+     4.783,    5.028,    7.495,    9.986,    0,        0,
+    -3.839,   -3.889,   -3.841,   -3.625,   9.610,     0,
+    -2.190,   -1.883,   -2.175,   -1.878,   3.552,    4.503), 
+   6, 6)
> 
> rownames(S.wh) <- colnames(S.wh) <- 
+     c('Anomia67','Powerless67','Anomia71','Powerless71','Education','SEI')

> 
> model.wh <- specify.model()
1:     Alienation67   ->  Anomia67,      NA,     1
2:     Alienation67   ->  Powerless67,   lam1,   NA
3:     Alienation71   ->  Anomia71,      NA,     1
4:     Alienation71   ->  Powerless71,   lam2,   NA 
5:     SES            ->  Education,     NA,     1     
6:     SES            ->  SEI,           lam3,   NA
7:     SES            ->  Alienation67,  gam1,   NA
8:     Alienation67   ->  Alienation71,  beta,   NA
9:     SES            ->  Alienation71,  gam2,   NA
10:     Anomia67       <-> Anomia67,      the1,   NA
11:     Anomia71       <-> Anomia71,      the3,   NA
12:     Powerless67    <-> Powerless67,   the2,   NA
13:     Powerless71    <-> Powerless71,   the4,   NA
14:     Education      <-> Education,     thd1,   NA
15:     SEI            <-> SEI,           thd2,   NA
16:     Anomia67       <-> Anomia71,      the13,  NA
17:     Alienation67   <-> Alienation67,  psi1,   NA
18:     Alienation71   <-> Alienation71,  psi2,   NA
19:     SES            <-> SES,           phi,    NA
20:     
Read 19 records
> 
> sem.wh <- sem(model.wh, S.wh, 932)
> 
> summary(sem.wh)

 Model Chisquare =  6.3349   Df =  5 Pr(>Chisq) = 0.27498
 Chisquare (null model) =  17973   Df =  15
 Goodness-of-fit index =  0.99773
 Adjusted goodness-of-fit index =  0.99046
 RMSEA index =  0.016934   90 % CI: (NA, 0.05092)
 Bentler-Bonnett NFI =  0.99965
 Tucker-Lewis NNFI =  0.99978
 Bentler CFI =  0.99993
 BIC =  -27.852 

 Normalized Residuals
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-9.57e-01 -1.34e-01 -4.24e-02 -9.17e-02  6.43e-05  5.47e-01 

 Parameter Estimates
      Estimate Std Error z value  Pr(>|z|)                                 
lam1   1.02656 0.053424   19.2152 0.0000e+00 Powerless67 <--- Alienation67 
lam2   0.97089 0.049608   19.5712 0.0000e+00 Powerless71 <--- Alienation71 
lam3   0.51632 0.042247   12.2214 0.0000e+00 SEI <--- SES                  
gam1  -0.54981 0.054298  -10.1258 0.0000e+00 Alienation67 <--- SES         
beta   0.61732 0.049486   12.4746 0.0000e+00 Alienation71 <--- Alienation67
gam2  -0.21151 0.049862   -4.2419 2.2164e-05 Alienation71 <--- SES         
the1   5.06546 0.373464   13.5635 0.0000e+00 Anomia67 <--> Anomia67        
the3   4.81176 0.397345   12.1098 0.0000e+00 Anomia71 <--> Anomia71        
the2   2.21438 0.319740    6.9256 4.3423e-12 Powerless67 <--> Powerless67  
the4   2.68322 0.331274    8.0997 4.4409e-16 Powerless71 <--> Powerless71  
thd1   2.73051 0.517737    5.2739 1.3353e-07 Education <--> Education      
thd2   2.66905 0.182260   14.6442 0.0000e+00 SEI <--> SEI                  
the13  1.88739 0.241627    7.8112 5.7732e-15 Anomia71 <--> Anomia67        
psi1   4.70477 0.427511   11.0050 0.0000e+00 Alienation67 <--> Alienation67
psi2   3.86642 0.343971   11.2406 0.0000e+00 Alienation71 <--> Alienation71
phi    6.87948 0.659208   10.4360 0.0000e+00 SES <--> SES                  

 Iterations =  58 
> 
> A <- sem.wh$A  # structural coefficients
> exog <- apply(A, 1, function(x) all(x == 0))
> endog <- !exog

> (B <- A[endog, endog, drop=FALSE])  # direct effects, endogenous ->
endogenous
             Anomia67 Powerless67 Anomia71 Powerless71 Education SEI
Anomia67            0           0        0           0         0   0
Powerless67         0           0        0           0         0   0
Anomia71            0           0        0           0         0   0
Powerless71         0           0        0           0         0   0
Education           0           0        0           0         0   0
SEI                 0           0        0           0         0   0
Alienation67        0           0        0           0         0   0
Alienation71        0           0        0           0         0   0
             Alienation67 Alienation71
Anomia67        1.0000000     0.000000
Powerless67     1.0265597     0.000000
Anomia71        0.0000000     1.000000
Powerless71     0.0000000     0.970892
Education       0.0000000     0.000000
SEI             0.0000000     0.000000
Alienation67    0.0000000     0.000000
Alienation71    0.6173153     0.000000

> (C <- A[endog, exog, drop=FALSE]) # direct effects, exogenous ->
endogenous
                    SES
Anomia67      0.0000000
Powerless67   0.0000000
Anomia71      0.0000000
Powerless71   0.0000000
Education     1.0000000
SEI           0.5163168
Alienation67 -0.5498096
Alienation71 -0.2115088

> I <- diag(nrow(B))
> IBinv <- solve(I - B)
> (Ty <- IBinv - I)  # total effects, endogenous -> endogenous
             Anomia67 Powerless67 Anomia71 Powerless71 Education SEI
Anomia67            0           0        0           0         0   0
Powerless67         0           0        0           0         0   0
Anomia71            0           0        0           0         0   0
Powerless71         0           0        0           0         0   0
Education           0           0        0           0         0   0
SEI                 0           0        0           0         0   0
Alienation67        0           0        0           0         0   0
Alienation71        0           0        0           0         0   0
             Alienation67 Alienation71
Anomia67        1.0000000     0.000000
Powerless67     1.0265597     0.000000
Anomia71        0.6173153     1.000000
Powerless71     0.5993465     0.970892
Education       0.0000000     0.000000
SEI             0.0000000     0.000000
Alienation67    0.0000000     0.000000
Alienation71    0.6173153     0.000000

> (Tx <- IBinv %*% C) # total effects, exogenous -> endogenous
                    SES
Anomia67     -0.5498096
Powerless67  -0.5644124
Anomia71     -0.5509147
Powerless71  -0.5348786
Education     1.0000000
SEI           0.5163168
Alienation67 -0.5498096
Alienation71 -0.5509147

> Ty - B  # indirect effects, endogenous -> endogenous
             Anomia67 Powerless67 Anomia71 Powerless71 Education SEI
Anomia67            0           0        0           0         0   0
Powerless67         0           0        0           0         0   0
Anomia71            0           0        0           0         0   0
Powerless71         0           0        0           0         0   0
Education           0           0        0           0         0   0
SEI                 0           0        0           0         0   0
Alienation67        0           0        0           0         0   0
Alienation71        0           0        0           0         0   0
             Alienation67 Alienation71
Anomia67        0.0000000            0
Powerless67     0.0000000            0
Anomia71        0.6173153            0
Powerless71     0.5993465            0
Education       0.0000000            0
SEI             0.0000000            0
Alienation67    0.0000000            0
Alienation71    0.0000000            0

> Tx - C # indirect effects, exogenous -> endogenous
                    SES
Anomia67     -0.5498096
Powerless67  -0.5644124
Anomia71     -0.5509147
Powerless71  -0.5348786
Education     0.0000000
SEI           0.0000000
Alienation67  0.0000000
Alienation71 -0.3394059

These results agree with those in the LISREL manual (and for another example
there as well), but I haven't checked the method carefully.

It would, of course, be simple to encapsulate the steps above in a function,
but here's a caveat: The idea of indirect and total effects makes sense to
me for a recursive model, and for the exogenous variables in a nonrecursive
model, where they are the reduced-form coefficients (supposing, of course,
that the model makes sense in the first place, which is often problematic),
but not for the endogenous variables in a nonrecursive model. That is why I
haven't put such a function in the sem package; perhaps I should reconsider.

Having said that, I'm ashamed to add that I believe that I was the person
who suggested the definition of total and indirect effects currently used
for these models.

Finally, you can get standardized effects similarly by using standardized
structural coefficients. In the sem package, these are computed and printed
by standardized.eoefficients(). This function doesn't return the
standardized A matrix in a usable form, but could be made to do so.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox


From tlumley at u.washington.edu  Fri Aug 25 16:43:49 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Aug 2006 07:43:49 -0700 (PDT)
Subject: [R] extremely slow recursion in R?
In-Reply-To: <44EEE464.3010603@fz-rossendorf.de>
References: <20060824200512.46157.qmail@web53701.mail.yahoo.com>
	<Pine.LNX.4.64.0608241558140.28297@homer21.u.washington.edu>
	<44EEE464.3010603@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.64.0608250712220.6086@homer21.u.washington.edu>

On Fri, 25 Aug 2006, Joerg van den Hoff wrote:
>
> maybe someone's interested:
> I made the same observation of seemingly very slow recursion recently: just 
> for fun I used the (in)famously inefficient
>
> fib <- function(n = 1) {
>   if (n < 2)
>      fn <- 1
>   else
>      fn <- fib(n - 1) + fib(n - 2)
>   fn
> }
>
> for calculating the fibonacci numbers and compared `fib(30)' (about 1.3e6 
> recursive function calls ...) to some other languages (times in sec):
>
> language  time
> ==============
> C          0.034  (compiled, using integers)
> Ocaml      0.041  (compiled, using integers)
> Ocaml      0.048  (interpreted, using integers)
> C          0.059  (compiled, using floats)
> Lua        1.1
> ruby       3.4
> R          21
> octave    120
>
> apart from octave (which seems to have a _real_ issue with recursive function 
> calls), R is by far the slowest in this list and still a factor 7-20 slower 
> than the interpreter based Lua and ruby. the speed loss compared to C or 
> Ocaml is about a factor of 350-600 here (Ocaml keeps its speed more or less 
> in this simple example even in 'script mode', which is remarkable, I think 
> (and it usually loses only a factor of about 7 or so in script mode compared 
> to the compiled variant)
>
> for the specialists the bad performance of R in this situation might not be 
> surprising, but I was not aware that recursive function calls are seemingly 
> as expensive as explicit loops (where the execution time ratio of R to C 
> again is of the same order, i.e. =~ 400).

Recursive function calls are likely to be *more* expensive than explicit 
loops, because of the memory and function call overhead.  This is true in 
most languages, and is why tail recursion optimization is a basic feature 
of compilers for functional languages.

> of course, such comparsions don't make too much sense: the reason to use R 
> will definitely not be its speed (which, luckily, often does not matter), but 
> the combination of flexible language, the number of available libraries and 
> the good 2D graphics.

The benchmarks are interesting (and reinforce my feeling that I need to
look at ocaml sometime).  They might be more interesting on a problem for 
which recursion was a sensible solution -- something simple like printing 
the nodes of a tree, or a more complicated problem like depth-first search 
in a graph.

It's certainly true that people don't pick R over other minority languages 
like Ocaml for its blazing speed, but for other reasons. As programming 
blogger Steve Yegge has observed when comparing advantages and 
disadvantages of various languages: "ML and Haskell are both from Planet 
Weird. And the people of that planet evidently do not believe in 
libraries".

More to the point, the ratio of speeds is much lower for many uses. For 
example, I recently translated to C an R program that computed the 
conditional score estimator for a Cox model with measurement error, and 
the C version ran about five times faster.  This was about the same speed 
increase than I had already obtained in the interpreted code by replacing 
a bisection search with uniroot().

In some cases the ratio may even be less than 1 -- since R can use 
optimized BLAS routines you might expect that some matrix operations would 
be faster in interpreted R than in C code written by the average user.  I 
have a second-hand report from a student here that this was actually the 
case -- he wrote compiled matrix routines and his code got slower.

It would certainly be nice if there were a faster statistical language 
with nice features, whether by speeding up R or by adding statistical 
features to something that compiles to native code, like Common Lisp or 
ocaml, or by taking better advantage of multicore computers as they 
proliferate.  These problems do need more people to work on them 
(especially as Bell Labs isn't in the statistical computing business any 
more).  There's a conference in New Zealand in February and abstract 
submissions are still open.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From marc.kirchner at iwr.uni-heidelberg.de  Fri Aug 25 16:48:18 2006
From: marc.kirchner at iwr.uni-heidelberg.de (Marc Kirchner)
Date: Fri, 25 Aug 2006 14:48:18 +0000
Subject: [R] increasing the # of socket connections
Message-ID: <20060825144809.GC4862@iwr.uni-heidelberg.de>

Dear "R-help"ers,

using snow on socket connections, I ran into the following error

	>  cl <- makeSOCKcluster(hosts)
	Error in socketConnection(port = port, server = TRUE, 
	blocking = TRUE : all connections are in use

with "showConnections(all=T)" showing 50 open connections.

As - for administrative reasons - I would prefer to use snow's
SOCK capabilities (instead of MPI and the like), I wonder if there is a
way to increase the number of simultaneous open connections allowed in
an R session (~100 would be sufficient).

Any help/hints are greatly appreciated,
best regards,
Marc

-- 
========================================================
Dipl. Inform. Med. Marc Kirchner
Interdisciplinary Centre for Scientific Computing (IWR)
Multidimensional Image Processing
INF 368
University of Heidelberg
D-69120 Heidelberg
Tel: ++49-6221-54 87 97
Fax: ++49-6221-54 88 50
marc.kirchner at iwr.uni-heidelberg.de

-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060825/254a76b5/attachment.bin 

From tlumley at u.washington.edu  Fri Aug 25 16:47:13 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Aug 2006 07:47:13 -0700 (PDT)
Subject: [R] [R-pkgs] biglm 0.4
Message-ID: <Pine.LNX.4.64.0608250744120.6086@homer21.u.washington.edu>


biglm fits linear and generalized linear models to large data sets, using 
bounded memory.

What's New:  generalized linear models.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From jennystadt at yahoo.ca  Fri Aug 25 17:28:15 2006
From: jennystadt at yahoo.ca (jennystadt)
Date: Fri, 25 Aug 2006 09:28:15 -0600
Subject: [R] Plot y ~ x under condition of variable a and b
References: <44EC37F8.4020907@yahoo.de> <200608231523295060438@yahoo.ca>
	<200608240952585215037@yahoo.ca>
Message-ID: <200608250928138911382@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060825/2863529a/attachment.pl 

From Peter.Marx at knorr-bremse.com  Fri Aug 25 17:37:31 2006
From: Peter.Marx at knorr-bremse.com (Peter.Marx at knorr-bremse.com)
Date: Fri, 25 Aug 2006 17:37:31 +0200
Subject: [R] How to get back POSIXct format after calculating with hist()
	results
Message-ID: <462B1838272F2748ACBA83A7127F1B788D1B47@MUCS7049.corp.knorr-bremse.com>

Hi,

I have a casting/formatting question on hist.POSIXt:

The histogram plot from POSIXct works perfect (with help of Prof. Ripley
-thanks!).
When processing the hist(plot=FALSE) output and then plotting the
results over the x-axis (bins) coming from hist(), I lose the date/time
labels, getting instead integers displayed.

Trying to cast the $breaks with as.POSIXct gives silly results with
StarTrek-like years.

What is the proper way to get back the old YY-MM-DD hh:mm:ss labels ?

Peter  



> str(logins)
`data.frame':   25792 obs. of  1 variable:
 $ DateTime:'POSIXct', format: chr  "2006-08-01 00:00:02" "2006-08-01
00:00:25" "2006-08-01 00:00:41" ...

> login_bins<-hist(samples$DateTime, "hours", freq=TRUE,plot=FALSE)

> str(login_bins)
List of 7
 $ breaks     : num [1:337] 1.15e+09 1.15e+09 1.15e+09 1.15e+09 1.15e+09
...
 $ counts     : int [1:336] 6 10 45 25 40 87 257 356 309 214 ...
 $ intensities: num [1:336] 0.000233 0.000388 0.001744 0.000969 0.001550
...
 $ density    : num [1:336] 6.46e-08 1.08e-07 4.85e-07 2.69e-07 4.31e-07
...
 $ mids       : num [1:336] 1.15e+09 1.15e+09 1.15e+09 1.15e+09 1.15e+09
...
 $ xname      : chr "logins$DateTime"
 $ equidist   : logi TRUE
 - attr(*, "class")= chr "histogram"

... Then I do some calculations with the login_bins$counts, resulting in
a new vector:

>str(userabs)
 int [1:336] 1 -3 34 39 68 127 347 641 844 943 ...

When I now plot this y over the x-axis coming from the previous
histogram x-axis with

>plot(login_bins$mids, userabs) 

I get the x-axis of labelled with the nums 1154400000, 1154800000...
(number of seconds since 1970 ?)

Trying to convert this gives funny results.

>as.POSIXct(as.Date(login_bins$mids[1]))
[1] "2568-10-12 02:00:00 W. Europe Daylight Time"

> as.POSIXct(as.Date(login_bins$mids[2]))
[1] "2578-08-21 02:00:00 W. Europe Daylight Time"


From tlumley at u.washington.edu  Fri Aug 25 17:38:08 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Aug 2006 08:38:08 -0700 (PDT)
Subject: [R] increasing the # of socket connections
In-Reply-To: <20060825144809.GC4862@iwr.uni-heidelberg.de>
References: <20060825144809.GC4862@iwr.uni-heidelberg.de>
Message-ID: <Pine.LNX.4.64.0608250836460.27104@homer21.u.washington.edu>

On Fri, 25 Aug 2006, Marc Kirchner wrote:

>
> As - for administrative reasons - I would prefer to use snow's
> SOCK capabilities (instead of MPI and the like), I wonder if there is a
> way to increase the number of simultaneous open connections allowed in
> an R session (~100 would be sufficient).
>

You need to change
   #define NCONNECTIONS 50
at the top of src/main/connections.c and recompile.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From matthew_wiener at merck.com  Fri Aug 25 17:46:07 2006
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 25 Aug 2006 11:46:07 -0400
Subject: [R] Plot y ~ x under condition of variable a and b [Broadcast ]
Message-ID: <4E9A692D8755DF478B56A2892388EE1FF56D8D@usctmx1118.merck.com>

It's the "|source" in your formula that tells lattice to separate them.

If you drop that, you'll get all points without S and P distinguished at
all.  If you add a groups argument, you should get them presented with
different colors/symbols/etc. depending on your trellis settings (warning:
untested code):

par.plot(lnvol~lnden, groups = source,data=dat,sub=as.factor(plotid),col=T)

Hope this helps,

Matt Wiener 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jennystadt
Sent: Friday, August 25, 2006 11:28 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Plot y ~ x under condition of variable a and b [Broadcast]



Hi All,

I want to plot y~ x under the condition of variable a and b. Followed is the
dataset:

         plotid     lnden        lnvol     source
369  9037.0 10.419002 -4.101039226      S
370  9037.0  9.840548 -2.432385723      S
371  9037.0  8.973351 -1.374842169      S
372  9037.0  8.242756 -0.813800113      S
373  9037.0  8.006368 -0.366743413      S
374  9037.0  7.396335 -0.041375532      S
375  9037.0  6.194405  0.744573249      S
376  9038.0 10.417209 -2.938129138      S
377  9038.0  9.709296 -1.906228589      S
378  9038.0  8.581107 -1.187441385      S
379  9038.0  7.539027 -0.748873856      S
380  9038.0  6.866933 -0.228547521      S
381  9038.0  6.672033  0.222818889      S
382  9038.0  6.380123  0.863026089      S
1100    3.1  7.281089  5.563470357      P
2100    3.1  7.165854  5.587837467      P
3100    3.1  7.126938  5.604757978      P
4100    3.1  6.833951  5.709078555      P
560     3.1  6.634462  5.678818058      P
610     3.2  7.052830  5.534234273      P
710     3.2  6.905777  5.559511276      P
810     3.2  6.885776  5.590614404      P
910     3.2  6.685106  5.716040812      P
1010    3.2  6.495349  5.631784504      P
1110    3.3  6.697376  5.414815010      P
1210    3.3  6.553336  5.441823472      P
1310    3.3  6.581116  5.455788329      P
1410    3.3  6.279641  5.543868038      P
1510    3.3  6.119298  5.528003301      P
1610    3.4  7.035589  5.783924732      P
1710    3.4  6.875624  5.798852319      P
1810    3.4  6.812445  5.807787244      P

I used  par.plot(lnvol~lnden|source,data=dat,sub=as.factor(plotid),col=T);
It gave good plots, but it put the different data sources to separated
graphs, i.e. S and P. What I want is to plot them on the same graph. If
anyone has the experience in doing plotting like this, please kindly give me
some hints. Thanks!

Jen.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From ripley at stats.ox.ac.uk  Fri Aug 25 17:47:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Aug 2006 16:47:40 +0100 (BST)
Subject: [R] increasing the # of socket connections
In-Reply-To: <20060825144809.GC4862@iwr.uni-heidelberg.de>
References: <20060825144809.GC4862@iwr.uni-heidelberg.de>
Message-ID: <Pine.LNX.4.64.0608251644190.8338@gannet.stats.ox.ac.uk>

On Fri, 25 Aug 2006, Marc Kirchner wrote:

> Dear "R-help"ers,
> 
> using snow on socket connections, I ran into the following error
> 
> 	>  cl <- makeSOCKcluster(hosts)
> 	Error in socketConnection(port = port, server = TRUE, 
> 	blocking = TRUE : all connections are in use
> 
> with "showConnections(all=T)" showing 50 open connections.
> 
> As - for administrative reasons - I would prefer to use snow's
> SOCK capabilities (instead of MPI and the like), I wonder if there is a
> way to increase the number of simultaneous open connections allowed in
> an R session (~100 would be sufficient).

If you really need them open at once (and are not just forgetting to close 
them), then change the constant in the file and recompile.

> Any help/hints are greatly appreciated,

Is this the appropriate list: not on my reading of the posting guide!

> best regards,
> Marc

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From singularitaet at gmx.net  Fri Aug 25 17:51:24 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Fri, 25 Aug 2006 17:51:24 +0200
Subject: [R] xyplot with different symbols and colors?
Message-ID: <44EF1C7C.9020608@gmx.net>

Dear List,

I try to make a xyplot with different colors and symbols, I came this far:

library(DAAG)
xyplot(csoa~it|sex*agegp,data=tinting,groups=target,pch=list(1,2),auto.key=list(space
= "right"))


this produces a plot with different colors and symbols but unfortunately
the legend does only follow the color scheme and not the different symbols.

Any suggestions what to change?

And how do I change or switch of that background colors in each plots title?

Stefan Grosse


From michael at isis.spa.umn.edu  Fri Aug 25 17:52:56 2006
From: michael at isis.spa.umn.edu (Michael Koppelman)
Date: Fri, 25 Aug 2006 10:52:56 -0500
Subject: [R] fitting a gaussian to some x,y data
Message-ID: <45E8A22D-02EF-4180-8679-631B067D24CF@aps.umn.edu>

I apologize if this is redundant. I've been Googling, searching the  
archive and reading the help all morning and I am not getting closer  
to my goal.

I have a series of data( xi, yi). It is not evenly sampled and it is  
messy (meaning that there is a lot of scatter in the data). I want to  
fit a normal distribution (i.e. a gaussian) to the data in order to  
find the center. (The data has a loose "normal" look to it.) I have  
done this with polynomial fitting in R with nls but want to try it  
with a Gaussian (I am a student astronomer and have not a lot of  
experience in statistics).

In looking at the fitdistr function, it seems to take as input a  
bunch of x values and it will fit a gaussian to the histogram. That  
is not what I need to do, I want to fit a normal distribution to the  
x,y values and get out the parameters of the fit. I'm fooling with  
nls but it seems to want the data in some other format or something  
because it is complaining about "singular gradient".

I'm sure this isn't hard and the answer must be out there somewhere  
but I can't find it. I appreciate any assistance.

Cheers,
Michael



filepath <- system.file("data", infile , package="datasets")
mm <-read.table(filepath)
mm
dmk <- data.frame( x=mm$V1, y=mm$V2)
attach(dmk)
plot(x,y)
#ymk <-nls(y~c*x^2+b*x+a,start=list(a=1,b=1,c=1),trace=TRUE)
ymk <-nls(y~a*exp(-x^2/sig),start=list(a=1,sig=1),trace=TRUE)
co = coef(ymk)
cmk <- list(a=co[[1]], b=co[[2]], c=co[[3]] )


From ripley at stats.ox.ac.uk  Fri Aug 25 17:53:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Aug 2006 16:53:03 +0100 (BST)
Subject: [R] How to get back POSIXct format after calculating with
 hist() results
In-Reply-To: <462B1838272F2748ACBA83A7127F1B788D1B47@MUCS7049.corp.knorr-bremse.com>
References: <462B1838272F2748ACBA83A7127F1B788D1B47@MUCS7049.corp.knorr-bremse.com>
Message-ID: <Pine.LNX.4.64.0608251648500.8338@gannet.stats.ox.ac.uk>

On Fri, 25 Aug 2006, Peter.Marx at knorr-bremse.com wrote:

> Hi,
> 
> I have a casting/formatting question on hist.POSIXt:
> 
> The histogram plot from POSIXct works perfect (with help of Prof. Ripley
> -thanks!).
> When processing the hist(plot=FALSE) output and then plotting the
> results over the x-axis (bins) coming from hist(), I lose the date/time
> labels, getting instead integers displayed.
> 
> Trying to cast the $breaks with as.POSIXct gives silly results with
> StarTrek-like years.

But you actually called as.POSIXct(as.Date()) on them, which was `silly'.

class(login_bins$mids) <- class(logins$DateTime)

should do the trick.

> What is the proper way to get back the old YY-MM-DD hh:mm:ss labels ?
> 
> Peter  
> 
> 
> 
> > str(logins)
> `data.frame':   25792 obs. of  1 variable:
>  $ DateTime:'POSIXct', format: chr  "2006-08-01 00:00:02" "2006-08-01
> 00:00:25" "2006-08-01 00:00:41" ...
> 
> > login_bins<-hist(samples$DateTime, "hours", freq=TRUE,plot=FALSE)
> 
> > str(login_bins)
> List of 7
>  $ breaks     : num [1:337] 1.15e+09 1.15e+09 1.15e+09 1.15e+09 1.15e+09
> ...
>  $ counts     : int [1:336] 6 10 45 25 40 87 257 356 309 214 ...
>  $ intensities: num [1:336] 0.000233 0.000388 0.001744 0.000969 0.001550
> ...
>  $ density    : num [1:336] 6.46e-08 1.08e-07 4.85e-07 2.69e-07 4.31e-07
> ...
>  $ mids       : num [1:336] 1.15e+09 1.15e+09 1.15e+09 1.15e+09 1.15e+09
> ...
>  $ xname      : chr "logins$DateTime"
>  $ equidist   : logi TRUE
>  - attr(*, "class")= chr "histogram"
> 
> ... Then I do some calculations with the login_bins$counts, resulting in
> a new vector:
> 
> >str(userabs)
>  int [1:336] 1 -3 34 39 68 127 347 641 844 943 ...
> 
> When I now plot this y over the x-axis coming from the previous
> histogram x-axis with
> 
> >plot(login_bins$mids, userabs) 
> 
> I get the x-axis of labelled with the nums 1154400000, 1154800000...
> (number of seconds since 1970 ?)
> 
> Trying to convert this gives funny results.
> 
> >as.POSIXct(as.Date(login_bins$mids[1]))
> [1] "2568-10-12 02:00:00 W. Europe Daylight Time"
> 
> > as.POSIXct(as.Date(login_bins$mids[2]))
> [1] "2578-08-21 02:00:00 W. Europe Daylight Time"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Peter.Marx at knorr-bremse.com  Fri Aug 25 18:03:14 2006
From: Peter.Marx at knorr-bremse.com (Peter.Marx at knorr-bremse.com)
Date: Fri, 25 Aug 2006 18:03:14 +0200
Subject: [R] How to get back POSIXct format after calculating with
	hist() results
In-Reply-To: <Pine.LNX.4.64.0608251648500.8338@gannet.stats.ox.ac.uk>
Message-ID: <462B1838272F2748ACBA83A7127F1B788D1B49@MUCS7049.corp.knorr-bremse.com>

yep, it works!

impressingly elegant. R seems worth to dig into the details. I'll try harder.

Thank You very much

Peter

-----Urspr?ngliche Nachricht-----
Von: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Gesendet: Freitag, 25. August 2006 17:53
An: Marx Peter
Cc: r-help at stat.math.ethz.ch
Betreff: Re: [R] How to get back POSIXct format after calculating with hist() results


On Fri, 25 Aug 2006, Peter.Marx at knorr-bremse.com wrote:

> Hi,
> 
> I have a casting/formatting question on hist.POSIXt:
> 
> The histogram plot from POSIXct works perfect (with help of Prof. 
> Ripley -thanks!). When processing the hist(plot=FALSE) output and then 
> plotting the results over the x-axis (bins) coming from hist(), I lose 
> the date/time labels, getting instead integers displayed.
> 
> Trying to cast the $breaks with as.POSIXct gives silly results with 
> StarTrek-like years.

But you actually called as.POSIXct(as.Date()) on them, which was `silly'.

class(login_bins$mids) <- class(logins$DateTime)

should do the trick.

> What is the proper way to get back the old YY-MM-DD hh:mm:ss labels ?
> 
> Peter
> 
> 
> 
> > str(logins)
> `data.frame':   25792 obs. of  1 variable:
>  $ DateTime:'POSIXct', format: chr  "2006-08-01 00:00:02" "2006-08-01 
> 00:00:25" "2006-08-01 00:00:41" ...
> 
> > login_bins<-hist(samples$DateTime, "hours", freq=TRUE,plot=FALSE)
> 
> > str(login_bins)
> List of 7
>  $ breaks     : num [1:337] 1.15e+09 1.15e+09 1.15e+09 1.15e+09 1.15e+09
> ...
>  $ counts     : int [1:336] 6 10 45 25 40 87 257 356 309 214 ...
>  $ intensities: num [1:336] 0.000233 0.000388 0.001744 0.000969 
> 0.001550 ...
>  $ density    : num [1:336] 6.46e-08 1.08e-07 4.85e-07 2.69e-07 4.31e-07
> ...
>  $ mids       : num [1:336] 1.15e+09 1.15e+09 1.15e+09 1.15e+09 1.15e+09
> ...
>  $ xname      : chr "logins$DateTime"
>  $ equidist   : logi TRUE
>  - attr(*, "class")= chr "histogram"
> 
> ... Then I do some calculations with the login_bins$counts, resulting 
> in a new vector:
> 
> >str(userabs)
>  int [1:336] 1 -3 34 39 68 127 347 641 844 943 ...
> 
> When I now plot this y over the x-axis coming from the previous 
> histogram x-axis with
> 
> >plot(login_bins$mids, userabs)
> 
> I get the x-axis of labelled with the nums 1154400000, 1154800000... 
> (number of seconds since 1970 ?)
> 
> Trying to convert this gives funny results.
> 
> >as.POSIXct(as.Date(login_bins$mids[1]))
> [1] "2568-10-12 02:00:00 W. Europe Daylight Time"
> 
> > as.POSIXct(as.Date(login_bins$mids[2]))
> [1] "2578-08-21 02:00:00 W. Europe Daylight Time"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From adik-rhelp at ilovebacon.org  Fri Aug 25 18:18:53 2006
From: adik-rhelp at ilovebacon.org (Adam D. I. Kramer)
Date: Fri, 25 Aug 2006 09:18:53 -0700 (PDT)
Subject: [R] Calculating critical values
In-Reply-To: <mailman.15.1156327204.11846.r-help@stat.math.ethz.ch>
References: <mailman.15.1156327204.11846.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0608241513220.4867@parser.ilovebacon.org>

Hello,

 	I have an HLM-like situation: a data frame with rows representing
individuals, and columns representing some statistics for the individual
(e.g., a mean and variance for that individual, plus the number of
observations for that person).

 	I'm interested in computing a confidence interval around each
individual's mean, but to do that, I would need to know the critical t-value
for each individual.

 	Ergo, is there any sort of function that returns a critical t-value
for a given p/alpha and df?

Thanks,
Adam Kramer


From spluque at gmail.com  Fri Aug 25 18:26:57 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Fri, 25 Aug 2006 11:26:57 -0500
Subject: [R] xyplot with different symbols and colors?
References: <44EF1C7C.9020608@gmx.net>
Message-ID: <87u040vl0u.fsf@patagonia.sebmags.homelinux.org>

On Fri, 25 Aug 2006 17:51:24 +0200,
Stefan Grosse <singularitaet at gmx.net> wrote:

> Dear List, I try to make a xyplot with different colors and symbols, I
> came this far:

> library(DAAG)
> xyplot(csoa~it|sex*agegp,data=tinting,groups=target,pch=list(1,2),auto.key=list(space
> = "right"))


> this produces a plot with different colors and symbols but unfortunately
> the legend does only follow the color scheme and not the different
> symbols.

> Any suggestions what to change?


I think that 'pch' is passed only to the panel function, and the key uses
whatever the current trellis.par.set() is, if you don't define it
explicitly (all explained in ?xyplot).  I find it easier to call
trellis.par.set before the xyplot:


R> trellis.par.set(superpose.symbol=list(pch=c(1, 2)))
Warning message:
Note: The default device has been opened to honour attempt to modify trellis settings in: trellis.par.set(superpose.symbol = list(pch = c(1, 2))) 
R> xyplot(csoa~it|sex*agegp,data=tinting,groups=target,auto.key=list(space="right"))


Cheers,

-- 
Seb


From ccleland at optonline.net  Fri Aug 25 18:29:22 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 25 Aug 2006 12:29:22 -0400
Subject: [R] Calculating critical values
In-Reply-To: <Pine.LNX.4.64.0608241513220.4867@parser.ilovebacon.org>
References: <mailman.15.1156327204.11846.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0608241513220.4867@parser.ilovebacon.org>
Message-ID: <44EF2562.7000508@optonline.net>

Adam D. I. Kramer wrote:
> Hello,
> 
>  	I have an HLM-like situation: a data frame with rows representing
> individuals, and columns representing some statistics for the individual
> (e.g., a mean and variance for that individual, plus the number of
> observations for that person).
> 
>  	I'm interested in computing a confidence interval around each
> individual's mean, but to do that, I would need to know the critical t-value
> for each individual.
> 
>  	Ergo, is there any sort of function that returns a critical t-value
> for a given p/alpha and df?

?qt

qt(p=.05, df=100)
[1] -1.66023

> Thanks,
> Adam Kramer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jennystadt at yahoo.ca  Fri Aug 25 18:36:52 2006
From: jennystadt at yahoo.ca (Jenny Stadt)
Date: Fri, 25 Aug 2006 10:36:52 -0600
Subject: [R] Plot y ~ x under condition of variable a and b[Broadcast ]
References: <4E9A692D8755DF478B56A2892388EE1FF56D8D@usctmx1118.merck.com>
Message-ID: <200608251036507814021@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060825/192c9284/attachment.pl 

From petr.pikal at precheza.cz  Fri Aug 25 08:54:32 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 25 Aug 2006 08:54:32 +0200
Subject: [R] Check values in colums matrix
In-Reply-To: <B998A44C8986644EA8029CFE6396A924840160@exqld2-bne.qld.csiro.au>
Message-ID: <44EEBAC8.20741.314930@localhost>

Hi

and if speed is an issue and object is numeric matrix something like

function(x) which(colSums(abs(diff(x)))==0)

is a little bit quicker

Cheers
Petr


On 25 Aug 2006 at 13:39, Bill.Venables at csiro.au wrote:

Date sent:      	Fri, 25 Aug 2006 13:39:48 +1000
From:           	<Bill.Venables at csiro.au>
To:             	<gunter.berton at gene.com>, <ggrothendieck at gmail.com>,
	<msubianto at gmail.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Check values in colums matrix

> As a minor footnote to both of these, I would add that both assume
> that all the columns of the dataset are numeric.  It doesn't cost much
> to generalize it to cover any matrix structure, of any mode:
> 
> constantColmuns <- function(Xmat) 
>     which(apply(Xmat, 2, function(z) length(unique(z)) == 1))
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter >
> Sent: Friday, 25 August 2006 9:37 AM > To: 'Gabor Grothendieck';
> 'Muhammad Subianto' > Cc: r-help at stat.math.ethz.ch > Subject: Re: [R]
> Check values in colums matrix > > Absolutely. But do note that if the
> values in obj are the product of > numerical computations then columns
> of equal values may turn out to be only > **nearly** equal and so the
> sd may turn out to be **nearly** 0 and not > exactly 0. This is a
> standard issue in numerical computation, of course, and > has been
> commented on in this list at least dozens of times, but it's still > a
> gotcha for the unwary (so now dozens +1). > > -- Bert Gunter >
> Genentech Non-Clinical Statistics > South San Francisco, CA >  >  > >
> > -----Original Message----- > > From:
> r-help-bounces at stat.math.ethz.ch > >
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor > >
> Grothendieck > > Sent: Thursday, August 24, 2006 4:28 PM > > To:
> Muhammad Subianto > > Cc: r-help at stat.math.ethz.ch > > Subject: Re:
> [R] Check values in colums matrix > > > > Try sd(obj.tr) which will
> give a vector of standard > > deviations, one per column. > > A
> column's entry will be zero if and only if all values in the column >
> > are the same. > > > > On 8/24/06, Muhammad Subianto
> <msubianto at gmail.com> wrote: > > > Dear all, > > > I apologize if my
> question is quite simple. > > > I have a dataset (20 columns & 1000
> rows) which > > > some of columns have the same value and the others >
> > > have different values. > > > Here are some piece of my dataset: >
> > > obj <- cbind(c(1,1,1,4,0,0,1,4,-1), > > >            
> c(0,1,1,4,1,0,1,4,-1), > > >             c(1,1,1,4,2,0,1,4,-1), > > > 
>            c(1,1,1,4,3,0,1,4,-1), > > >            
> c(1,1,1,4,6,0,1,5,-1), > > >             c(1,1,1,4,6,0,1,6,-1), > > > 
>            c(1,1,1,4,6,0,1,7,-1), > > >            
> c(1,1,1,4,6,0,1,8,-1)) > > > obj.tr <- t(obj) > > > obj.tr > > > >
> obj.tr > > >     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] > > >
> [1,]    1    1    1    4    0    0    1    4   -1 > > > [2,]    0    1
>    1    4    1    0    1    4   -1 > > > [3,]    1    1    1    4    2
>    0    1    4   -1 > > > [4,]    1    1    1    4    3    0    1    4
>   -1 > > > [5,]    1    1    1    4    6    0    1    5   -1 > > >
> [6,]    1    1    1    4    6    0    1    6   -1 > > > [7,]    1    1
>    1    4    6    0    1    7   -1 > > > [8,]    1    1    1    4    6
>    0    1    8   -1 > > > > > > > > > > How can I do to check columns
> 2,3,4,6,7 and 9 have > > > the same value, and columns 1,5 and 8 have
> different values. > > > > > > Best, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From mtmorgan at fhcrc.org  Fri Aug 25 18:25:48 2006
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Fri, 25 Aug 2006 09:25:48 -0700
Subject: [R] Need help with difficulty loading page www.bioconductor.org
In-Reply-To: <44EE366F.2030906@simbiosys-biowares.com> (Debashis
	Bhattacharya's message of "Thu, 24 Aug 2006 18:29:51 -0500")
References: <44EE366F.2030906@simbiosys-biowares.com>
Message-ID: <6phac5sdboz.fsf@gopher4.fhcrc.org>

I didn't see a response to this, and am not an expert, but wanted to
point you in the right direction.

You should ask this question on the bioconductor mailing list.

The site is very much alive at this end (though I'm very close to it,
physically), so you'll need to provide some more information about the
problems you're experiencing. At a minimum it would help to include
the output of the R command

sessionInfo()

If you're using Windows you might also look at mailing list threads
dealing with server proxies. Searchable archives are at

http://dir.gmane.org/gmane.science.biology.informatics.conductor

this link

http://article.gmane.org/gmane.science.biology.informatics.conductor/2152/match=internet2

and searching for internet2 might be a good start.

Martin


Debashis Bhattacharya <bhattach at simbiosys-biowares.com> writes:

> The page is either too busy, or there is something seriously wrong with 
> access to this page.
>
> Most of the time, trying to reach www.bioconductor.org results in 
> failure. Only once in a
> blue moon, do I get through.
>
> In fact, thus far, I have not been able to install bioconductor, since 
> the first source(...)
> command from the R command window -- following instruction on 
> www.bioconductor.org
> page, that I did manage to reach, one time -- has failed, every time.
>
> Please help.
>
>
>
> Debashis Bhattacharya.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From singularitaet at gmx.net  Fri Aug 25 18:38:07 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Fri, 25 Aug 2006 18:38:07 +0200
Subject: [R] xyplot with different symbols and colors?
In-Reply-To: <87u040vl0u.fsf@patagonia.sebmags.homelinux.org>
References: <44EF1C7C.9020608@gmx.net>
	<87u040vl0u.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <44EF276F.5010207@gmx.net>

R> trellis.par.set(superpose.symbol=list(pch=c(1, 2)))
Warning message:
Note: The default device has been opened to honour attempt to modify trellis settings in: trellis.par.set(superpose.symbol = list(pch = c(1, 2))) 
R> xyplot(csoa~it|sex*agegp,data=tinting,groups=target,auto.key=list(space="right"))



Thanks that did it!


From deepayan.sarkar at gmail.com  Fri Aug 25 18:44:25 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 25 Aug 2006 11:44:25 -0500
Subject: [R] xyplot with different symbols and colors?
In-Reply-To: <87u040vl0u.fsf@patagonia.sebmags.homelinux.org>
References: <44EF1C7C.9020608@gmx.net>
	<87u040vl0u.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <eb555e660608250944p238e4454y6542d29f4ed59194@mail.gmail.com>

On 8/25/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> On Fri, 25 Aug 2006 17:51:24 +0200,
> Stefan Grosse <singularitaet at gmx.net> wrote:
>
> > Dear List, I try to make a xyplot with different colors and symbols, I
> > came this far:
>
> > library(DAAG)
> > xyplot(csoa~it|sex*agegp,data=tinting,groups=target,pch=list(1,2),auto.key=list(space
> > = "right"))
>
>
> > this produces a plot with different colors and symbols but unfortunately
> > the legend does only follow the color scheme and not the different
> > symbols.
>
> > Any suggestions what to change?
>
>
> I think that 'pch' is passed only to the panel function, and the key uses
> whatever the current trellis.par.set() is, if you don't define it
> explicitly (all explained in ?xyplot).  I find it easier to call
> trellis.par.set before the xyplot:
>
>
> R> trellis.par.set(superpose.symbol=list(pch=c(1, 2)))
> Warning message:
> Note: The default device has been opened to honour attempt to modify trellis settings in: trellis.par.set(superpose.symbol = list(pch = c(1, 2)))
> R> xyplot(csoa~it|sex*agegp,data=tinting,groups=target,auto.key=list(space="right"))

Also, see the entry for 'par.settings' in help(xyplot).

-Deepayan


From boom2k1 at yahoo.com.au  Fri Aug 25 18:49:09 2006
From: boom2k1 at yahoo.com.au (Charles)
Date: Fri, 25 Aug 2006 12:49:09 -0400 (EDT)
Subject: [R] R.squared in Weighted Least Square using the Lm Function
Message-ID: <20060825164909.15855.qmail@web54105.mail.yahoo.com>

Hello all,
I am using the function lm to do my weighted least
square regression.

model<-lm(Y~X1+X2, weight=w)

What I am confused is the r.squared.
It does not seem that the r.squared for the weighted
case is an ordinary 1-RSS/TSS.
What is that precisely?
Is the r.squared measure comparable to that obtained
by the ordinary least square?

<I also notice that
model$res is the unweighted residual while
summary(model)$res  is the weighted residual>


Thank you in advance for helping!

Charles


From rolf at erdos.math.unb.ca  Fri Aug 25 19:12:19 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 25 Aug 2006 14:12:19 -0300 (ADT)
Subject: [R] fitting a gaussian to some x,y data
Message-ID: <200608251712.k7PHCJGt021268@erdos.math.unb.ca>

Michael Koppelman wrote:

> I apologize if this is redundant. I've been Googling, searching the
> archive and reading the help all morning and I am not getting closer
> to my goal.
> 
> I have a series of data( xi, yi). It is not evenly sampled and it is
> messy (meaning that there is a lot of scatter in the data). I want to
> fit a normal distribution (i.e. a gaussian) to the data in order to
> find the center. (The data has a loose "normal" look to it.) I have
> done this with polynomial fitting in R with nls but want to try it
> with a Gaussian (I am a student astronomer and have not a lot of
> experience in statistics).
> 
> In looking at the fitdistr function, it seems to take as input a
> bunch of x values and it will fit a gaussian to the histogram. That
> is not what I need to do, I want to fit a normal distribution to the
> x,y values and get out the parameters of the fit. I'm fooling with
> nls but it seems to want the data in some other format or something
> because it is complaining about "singular gradient".
> 
> I'm sure this isn't hard and the answer must be out there somewhere  
> but I can't find it. I appreciate any assistance.

	Fitting a distribution simply means estimating
	the parameters of that distribution.

	For a Gaussian distribution this problem is trivial.
	The parameters are the mean vector mu and the
	covariance matrix Sigma.

	The (optimal; maximum-likelihood-adjusted-to-be-unbiased)
	estimates of these are the obvious ones --- the sample
	mean and the sample covariance.  In R you most easily
	get these by

		o cbinding your ``x'' and ``y'' vectors into
		matrix:

			> xy <- cbind(x,y)

		o applying mean() to this matrix:

			> mu.hat <- apply(xy,2,mean)

		o calling upon var():

			> Sigma.hat <- var(xy)

	That is all there is to it.

	If all you really want is an estimate of the ``centre''
	then this estimate is just mu.hat; you don't need to
	``fit a distribution'' at all.

	From your description of the problem there may well be
	other issues --- lack of independence of your data,
	biased sample, outliers, skewness, god knows what. Such
	issues should be dealt with as carefully as possible.
	It may be the case that you should be doing some sort
	of robust estimation.  Or it may be the case that this
	data set is hopeless and should be thrown away and a
	new data set collected in a manner that will actually
	yield some information.

	But ``fitting a distribution'' is *NOT* the issue. 

	I don't mean to be rude, but this question illustrates the
	point that you really shouldn't be *doing* statistics unless
	you *understand* some statistics.  At the very best you'll
	waste a great deal of time.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From ripley at stats.ox.ac.uk  Fri Aug 25 19:17:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Aug 2006 18:17:04 +0100 (BST)
Subject: [R] R.squared in Weighted Least Square using the Lm Function
In-Reply-To: <20060825164909.15855.qmail@web54105.mail.yahoo.com>
References: <20060825164909.15855.qmail@web54105.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608251807510.9097@gannet.stats.ox.ac.uk>

On Fri, 25 Aug 2006, Charles wrote:

> Hello all,
> I am using the function lm to do my weighted least
> square regression.
> 
> model<-lm(Y~X1+X2, weight=w)
> 
> What I am confused is the r.squared.

What r.squared?  There is no r.squared in that object, but it is 
calculated by the summary method.

> It does not seem that the r.squared for the weighted
> case is an ordinary 1-RSS/TSS.
> What is that precisely?

Precisely that, with weights in the SS.  The code is

    r <- z$residuals
    f <- z$fitted
    w <- z$weights
    if (is.null(w)) {
        mss <- if (attr(z$terms, "intercept"))
            sum((f - mean(f))^2)
        else sum(f^2)
        rss <- sum(r^2)
    } else {
        mss <- if (attr(z$terms, "intercept")) {
            m <- sum(w * f/sum(w))
            sum(w * (f - m)^2)
        }
        else sum(w * f^2)
        rss <- sum(w * r^2)
        r <- sqrt(w) * r
    }
    ans$r.squared <- mss/(mss + rss)

That's the great thing about R: you can answer your own question by 
reading the code for yourself.

> Is the r.squared measure comparable to that obtained
> by the ordinary least square?
> 
> <I also notice that
> model$res is the unweighted residual while
> summary(model)$res  is the weighted residual>

Yes, as documented with added emphasis in ?summary.lm .

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From attenka at utu.fi  Fri Aug 25 19:23:54 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Fri, 25 Aug 2006 20:23:54 +0300
Subject: [R] Modifying the embed-results
Message-ID: <1156526634.12112.13.camel@attenka>

Hi,

Here is a vector and the result from the embed-command:

VECTOR=c(0,3,6,3,11,2,4,3,7,6,4,5,10,2,3,5,8)

> embed(VECTOR, dimension=5)
      [,1] [,2] [,3] [,4] [,5]
 [1,]   11    3    6    3    0
 [2,]    2   11    3    6    3
 [3,]    4    2   11    3    6
 [4,]    3    4    2   11    3
 [5,]    7    3    4    2   11
 [6,]    6    7    3    4    2
 [7,]    4    6    7    3    4
 [8,]    5    4    6    7    3
 [9,]   10    5    4    6    7
[10,]    2   10    5    4    6
[11,]    3    2   10    5    4
[12,]    5    3    2   10    5
[13,]    8    5    3    2   10

Is there a way to little modify the algorithm so that the result looks
like this:

[1]  0  3  6 11  2 <- beginning from the first number of the VECTOR
[1]  3  6 11  2  4 <- beginning from the second number of the VECTOR etc
[1]  6  3 11  2  4 
[1]  3 11  2  4  7
[1] 11  2  4  3  7
[1] 2 4 3 7 6
[1] 4 3 7 6 5
[1] 3 7 6 4 5
[1]  7  6  4  5 10
[1]  6  4  5 10  2
[1]  4  5 10  2  3
[1]  5 10  2  3  8
[1] 10  2  3  5  8

Every row consists of next five unique(!) member of the VECTOR. I made
this example result with a time consuming algorithm which uses for-loops
and whiles. 

How to do this better??

Thanks in advance!

Atte Tenkanen
University of Turku


From thomas.harte at yahoo.com  Fri Aug 25 20:05:48 2006
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Fri, 25 Aug 2006 11:05:48 -0700 (PDT)
Subject: [R] issues with Sweave and inclusion of graphics in a document
In-Reply-To: <Pine.LNX.4.64.0608200706380.6643@gannet.stats.ox.ac.uk>
Message-ID: <20060825180548.39228.qmail@web30213.mail.mud.yahoo.com>

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> savePlot is just an internal version of dev.copy, part of the support for 
> the menus on the windows() graphics device.
> 
> It is described in `An Introduction to R' (the most basic R manual).

"the most basic R manual" doesn't quite answer my question. 

by itself, dev.copy doesn't copy the width and height of the device whereas savePlot
copies whatever is displayed on the screen giving me what-you-see-is-what-you-save
capabilities (but only under the Windows OS). 

i can get pretty close to this in linux by writing a function to save the
plot to a pdf device:
<<label=first.ar.1, results=hide>>=
	# no savePlot in Linux ... so write my own function
	savePlotAsPdf<- function(pdfname, from=dev.cur()) {
		from<- from
		pdf(pdfname, width=width, height=height)
		to<- dev.cur()
		dev.set(from)
		dev.copy(which=to)
		dev.off()
	}
	# a long AR process is best viewed in a wide window ... 
	# width & height are now variables
	width<- 20; height<- 5
	x11(width=width, height=height)
	sp<- make.ar.1(alpha=.5, n=800)
	plot(sp, type="l", col="blue")
	# width & height via dynamic scoping in savePlotAsPdf
	savePlotAsPdf("ar.pdf")
@

the only (minor) inconvenience is that i have to specify width and height as variables to
take advantage of dynamic scoping in order to minimize mess.

while this is a workaround, via dev.copy, as you pointed out, it still doesn't answer why
Sweave doesn't like x11() devices (or windows() devices under the Windows OS for that
matter) within figure environments. 

perhaps this is a question for the package maintainer? but i was hoping that all the avid
Sweave users would pitch in with how they work with graphics in practice.

here is a revised .Rnw example illustrating the above:

% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-linux2.Rnw
\documentclass[a4paper]{article}

\begin{document}

\noindent This is an example of how I can use \texttt{Sweave} under Linux. 

<<echo=false,results=hide>>=
	# create a simple AR process:
	make.ar.1<- function(alpha=1,n=300) {
		Z<- rnorm(n); 
		Y<- numeric(n); 
		Y[1]<- Z[1]; 
		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
		return(Y)
	}
@

\texttt{Sweave} doesn't like the \verb at x11(width=width, height=height)@ command 
in the following code chunk if it is placed within a \texttt{figure} environment.
Instead, I have to save the plot to a file and then I use \verb@\includegraphics@ in 
the \texttt{figure} environment. This isn't a bad thing, as it allows me to fine-tune
the \LaTeX\ graphics placement.
<<label=first.ar.1, results=hide>>=
	# no savePlot in Linux ... so write my own function
	savePlotAsPdf<- function(pdfname, from=dev.cur()) {
		from<- from
		pdf(pdfname, width=width, height=height)
		to<- dev.cur()
		dev.set(from)
		dev.copy(which=to)
		dev.off()
	}
	# a long AR process is best viewed in a wide window ... 
	# width & height are now variables
	width<- 20; height<- 5
	x11(width=width, height=height)
	sp<- make.ar.1(alpha=.5, n=800)
	plot(sp, type="l", col="blue")
	# width & height via dynamic scoping in savePlotAsPdf
	savePlotAsPdf("ar.pdf")
@
\begin{figure}
\begin{center}
% 	i retain direct control over graphics in LaTeX; i can fine-tune the 
% 	the graphics placement as much as i want:
	\includegraphics[width=14.5cm]{./ar.pdf}
\caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
is best viewed in a wide window.}
\end{center}
\end{figure}

Under X, then,
\begin{itemize}
\item Why doesn't \texttt{Sweave} like \verb at x11(width=width, height=height)@?
\end{itemize}
There are, however, advantages to doing things this way:
\begin{itemize}
\item I can save the plot to a file without writing any other code;
\item I can include the saved plot in my \LaTeX\ figure, allowing me to 
	fine-tune with the \verb@\includegraphics{}@ command.
\end{itemize}

\end{document}
% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-linux2.Rnw




> 
> On Sat, 19 Aug 2006, Thomas Harte wrote:
> 
> > the problem is a little hard to explain; the .Rnw files (below)
> > probably do a better job, but here goes ...
> > 
> > Sweave doesn't like it when i size a graphical device in a code
> > chunk using either, e.g.:
> > 
> > 	windows(width=20, height=5)
> > 
> > in Windows, or, e.g.
> > 
> > 	x11(width=20, height=5)
> > 
> > under X, when i then plot something in said device and try to 
> > include this graphical output in the resulting document.
> > 
> > Sweave does not object to my writing code chunks in the above
> > manner, so long as i do not wish to include the code in a LaTeX 
> > figure environment.
> > 
> > oftentimes i want to do precisely what Sweave doesn't appear
> > to allow. for example, with time-series data, i want to see a 
> > wide window on the screen as i code, and then i want to include 
> > the graphical output in my document the way that i fine tuned 
> > it on the screen. i don't want to write two pieces of code:
> > the first, to view output on the sceen; the second, to save
> > the output to a .pdf file for inclusion in the document.
> > 
> > some example .Rnw files should illustrate my plight.
> > suggestions on a workaround (i.e. how to do what i describe in 
> > linux/X) welcome.
> > 
> > 
> > % >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-windows.Rnw
> > \documentclass[a4paper]{article}
> > 
> > \begin{document}
> > 
> > \noindent This is an example of what I can do on Windows. Unhappily, I seem to be
> > able to squeeze marginally more out of \texttt{Sweave} \emph{chez\/} Bill Gates
> > than I can under Linux. Ho, hum.
> > 
> > <<echo=false,results=hide>>=
> > 	# create a simple AR process:
> > 	make.ar.1<- function(alpha=1,n=300) {
> > 		Z<- rnorm(n); 
> > 		Y<- numeric(n); 
> > 		Y[1]<- Z[1]; 
> > 		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
> > 		return(Y)
> > 	}
> > @
> > 
> > <<label=ar.1>>=
> > 	# a long AR process is best viewed in a wide window:
> > 	windows(width=20, height=5)
> > 	sp<- make.ar.1(alpha=.5, n=800)
> > 	plot(sp, type="l", col="blue")
> > 	# WISIWIS: What I See Is What I Save ;)
> > 	savePlot("ar",type="pdf")
> > @
> > \begin{figure}
> > \begin{center}
> > % 	imporantly, by saving the plot i have direct control over graphics in LaTeX, 
> > % 	and i can fine-tune the the graphics placement as much as i want:
> > 	\includegraphics[width=14.5cm]{./ar.pdf}
> > \caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
> > is best viewed in a wide window.}
> > \end{center}
> > \end{figure}
> > 
> > 
> > \noindent Had I tried to do the following, \texttt{Sweave} would have blown up!
> > \begin{verbatim}
> > 	<<label=ar.1>>=
> > 		windows(width=20, height=5) 	# <- this is the offending command:
> > 		sp<- make.ar.1(alpha=.5, n=800)
> > 		plot(sp, type="l", col="blue")
> > 	@
> > 	\begin{figure}
> > 	\begin{center}
> > 	<<fig=true>>=
> > 	<<ar.1>>
> > 	@
> > 	\caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
> > 	is best viewed in a wide window.}
> > 	\end{center}
> > 	\end{figure}
> > \end{verbatim}
> > 
> > 
> > \noindent The take-home message is that \texttt{savePlot} saves the day under
> Windows.
> > As far as I know, there is no equivalent under Linux, or rather, under X.
> > 
> > In Windows, then,
> > \begin{itemize}
> > \item I can plot the way I want on the screen;
> > \item I can save that plot to a file without writing any other code;
> > \item I can include the saved plot in my \LaTeX\ figure, allowing me to 
> > 	fine-tune with the \verb@\includegraphics{}@ command.
> > \end{itemize}
> > Strike one for the Evil Empire.
> > 
> > \end{document}
> > % <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-windows.Rnw
> > 
> > 
> > 
> > % >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> example-linux.Rnw
> > \documentclass[a4paper]{article}
> > 
> > \begin{document}
> > 
> > \noindent This is an example of the hapless state of my \texttt{Sweave}ing under
> Linux. 
> > 
> > <<echo=false,results=hide>>=
> > 	# create a simple AR process:
> > 	make.ar.1<- function(alpha=1,n=300) {
> > 		Z<- rnorm(n); 
> > 		Y<- numeric(n); 
> > 		Y[1]<- Z[1]; 
> > 		for (i in 2:n) Y[i]<- alpha*Y[i-1]+Z[i]; 
> > 		return(Y)
> > 	}
> > @
> > 
> > \noindent Because of the \verb at x11(width=20, height=5)@ command, 
> > I can't embed the graphical output that the following piece of code 
> > produces in my document, although I can view the results on screen:
> > <<label=first.ar.1>>=
> > 	# a long AR process is best viewed in a wide window:
> > 	x11(width=20, height=5)
> > 	sp<- make.ar.1(alpha=.5, n=800)
> > 	plot(sp, type="l", col="blue")
> > 	# no savePlot ... can't seem to do anything with this plot
> > 	# if i try to include this code in a figure environment then
> > 	# Sweave blows up
> > 	# so i have to stop here :(
> > @
> > 
> > \noindent Instead, I have to do something like the following, which has the
> unfortunate 
> > side effects of disallowing me from seeing the graphical output on the screen, and,
> > probably
> > more importantly, of duplicating the above code:
> > <<label=ar.1,echo=true>>=
> > 	sp<- make.ar.1(alpha=.5, n=800)
> > 	pdf("ar.pdf", width=20, height=5)
> > 	plot(sp, type="l", col="blue")
> > 	dev.off()
> > @
> > \begin{figure}
> > \begin{center}
> > % 	at least i still retain direct control over graphics in LaTeX; i can fine-tune the
> 
> > % 	the graphics placement as much as i want:
> > 	\includegraphics[width=14.5cm]{./ar.pdf}
> > \caption{An AR(1) process of length~\protect\Sexpr{length(sp)} 
> > is best viewed in a wide window.}
> > \end{center}
> > \end{figure}
> > 
> > Under X, then,
> > \begin{itemize}
> > \item I have to use a device such as \texttt{pdf} and I lose the ability to first 
> > 	see the output on screen;
> > \item I can still save that plot to a file without writing any other code;
> > \item I can still include the saved plot in my \LaTeX\ figure, allowing me to 
> > 	fine-tune with the \verb@\includegraphics{}@ command.
> > \end{itemize}
> > 
> > \end{document}
> > % <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< example-linux.Rnw
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From petyuk at mail.ru  Fri Aug 25 20:36:26 2006
From: petyuk at mail.ru (Vladislav Petyuk)
Date: Fri, 25 Aug 2006 22:36:26 +0400
Subject: [R] tcltk command to figure out which widget is on focus (or
	clicked)
Message-ID: <E1GGgXa-0009ah-00.petyuk-mail-ru@f12.mail.ru>

Hi,
I'm making an interface, where a Tcl/Tk window have few listbox widgets.
I need to select separate parameters from separate listboxes.
It is clear how to get cursor selection value, once you know which listbox widget you clicked.
The problem is I can't figure out which one tcltk command to use to get an information which listbox widget is in focus (or clicked).
Thank you,
Vlad


From szhan at uoguelph.ca  Fri Aug 25 20:38:24 2006
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Fri, 25 Aug 2006 14:38:24 -0400
Subject: [R] how to contrast with factorial experiment
In-Reply-To: <XFMail.060824214401.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060824214401.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20060825143824.tlokqxm9ycoo00c8@webmail.uoguelph.ca>

Hello, Ted,
Thank you for your help!
So I can not contrast the mean yields between sections 1-8 and 9-11  
under "Trt" but I can contrast mean yields for sections 1-3 and 6-11  
because there exists significant interaction between two factors  
(Trt:section4, Trt:section5). Could I use the commands below to test  
the difference between sections 1-3 and 6-11 ?
> contrasts(section)<-c(-2,-2,-2,0,0,1,1,1,1,1,1)
> newobj<-lm(log2(yield)~treat*section)
> summary(newobj)

Call:
lm(formula = log2(yield) ~ treat * section)

Residuals:
      Min       1Q   Median       3Q      Max
-0.49647 -0.14913 -0.01521  0.17471  0.51105

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)
(Intercept)         6.28840    0.05003 125.682  < 2e-16 ***
treatTrt            1.22122    0.07076  17.259  < 2e-16 ***
section1            0.17831    0.03911   4.559 4.08e-05 ***
section2           -0.23102    0.16595  -1.392  0.17087
section3            2.38170    0.16595  14.352  < 2e-16 ***
section4            3.36834    0.16595  20.298  < 2e-16 ***
section5           -1.56873    0.16595  -9.453 3.67e-12 ***
section6           -0.41522    0.16595  -2.502  0.01613 *
section7           -0.89943    0.16595  -5.420 2.38e-06 ***
section8            0.09522    0.16595   0.574  0.56901
section9           -0.78784    0.16595  -4.748 2.21e-05 ***
section10           0.74821    0.16595   4.509 4.79e-05 ***
treatTrt:section1   0.10101    0.05532   1.826  0.07461 .
treatTrt:section2   0.27270    0.23468   1.162  0.25151
treatTrt:section3  -1.22210    0.23468  -5.207 4.85e-06 ***
treatTrt:section4  -1.39187    0.23468  -5.931 4.26e-07 ***
treatTrt:section5  -0.76137    0.23468  -3.244  0.00225 **
treatTrt:section6   0.07320    0.23468   0.312  0.75658
treatTrt:section7   0.33108    0.23468   1.411  0.16535
treatTrt:section8  -0.13686    0.23468  -0.583  0.56276
treatTrt:section9   0.22086    0.23468   0.941  0.35180
treatTrt:section10 -0.14476    0.23468  -0.617  0.54054
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.2874 on 44 degrees of freedom
Multiple R-Squared: 0.973,      Adjusted R-squared: 0.9601
F-statistic: 75.55 on 21 and 44 DF,  p-value: < 2.2e-16

How can I infer that there is significant difference between sections  
1-3 and sections 6-11 for the "Trt" from the output above?

Joshua
Quoting "(Ted Harding)" <Ted.Harding at nessie.mcc.ac.uk>:

> On 24-Aug-06 szhan at uoguelph.ca wrote:
>> Hello, R users,
>> I have two factors (treat, section) anova design experiment where
>> there are 3 replicates. The objective of the experiment is to test if
>> there is significant difference of yield between top (section 9 to 11)
>> and bottom (section 9 to 11)
> [I think you mean sections 1 to 8]
>
>> of the fruit tree under treatment. I found that there are interaction
>> between two factors. I wonder if I can contrast means from levels of
>> one factor (section) under another factor (treat)? if so, how to do
>> it in R and how to interpret the output?
>
> I think you would be well advised to look at a plot of the data.
> For example, let Y stand for yield, R for replicate, T for treat
> and S for section.
>
>   ix<-(T=="Trt");plot(S[ix],Y[ix],col="red",ylim=c(0,1000))
>   ix<-(T=="Ctl");points(S[ix],Y[ix],col="blue")
>
>> From this it is clear that sections 4 and 5 are in a class of
> their own. Also, in sections 1-3 and 6-11 the "Ctl" yields
> are not only lower, but have smaller (in some cases hardly any)
> variance, compared with the "Trt" yields. The variances for
> sections 7,8,9,10,11 are greater than for 1,2,3,6 without
> great change in mean value.
>
> While there is an evident difference between "Trt" yields and
> "Ctrl" yields for sections 1-3 and 6-11, this is not so for
> sections 4 and 5.
>
> This sort of behaviour no doubt provides some reasons for the
> interaction you observed. You seem to have a quite complex
> phenomenon here!
>
> To some extent the problems with variance can be diminished by
> working with logarithms. Compare the previous plot with
>
>   ix<-(T=="Trt");plot(S[ix],log10(Y[ix]),col="red",ylim=c(0,3))
>   ix<-(T=="Ctl");points(S[ix],log10(Y[ix]),col="blue")
>
> (you have used log2() in your commands). The above observations
> can be seen reflected in R if you look at the output of
>
>   summary(obj)
>
> where in particular:
>
> treatTrt:section2  -1.11691    0.33189  -3.365 0.001595 **
> treatTrt:section3  -0.45634    0.33189  -1.375 0.176099
> treatTrt:section4  -1.56627    0.33189  -4.719 2.42e-05 ***
> treatTrt:section5  -1.73604    0.33189  -5.231 4.48e-06 ***
> treatTrt:section6  -0.91311    0.33189  -2.751 0.008588 **
> treatTrt:section7  -0.07853    0.33189  -0.237 0.814055
> treatTrt:section8   0.17935    0.33189   0.540 0.591654
> treatTrt:section9  -0.28859    0.33189  -0.870 0.389277
> treatTrt:section10  0.06913    0.33189   0.208 0.835972
> treatTrt:section11 -0.29649    0.33189  -0.893 0.376543
>
> which, precisely, "contrasts means from levels of one factor
> (section) under another factor (treat)", and shows that most
> of the "interaction" arises in sections 4 and 5.
>
> Since sections 4 and 5 (in the middle of sections 1 to 8) are
> so exceptional, they will have strong influence on your comparison
> between sections 1-8 and sections 9-11. You need to think about
> what to do with sections 4 and 5!
>
>> Here is the data and commands I used to test the differece between
>> section 1 to 8 and 9 to 11 under treatment. But I don't know if I was
>> right, how to interpret the out and whether there are significant
>> difference between section 1 to 8 and section 9 to 11 under treatment.
>>
>> yield replicate       treat   section
>> 35.55 1       Ctl     1
>> 53.70 1       Ctl     2
>> 42.79 1       Ctl     3
>> 434.81        1       Ctl     4
>> 705.96        1       Ctl     5
>> 25.91 1       Ctl     6
>> 57.53 1       Ctl     7
>> 41.45 1       Ctl     8
>> 85.54 1       Ctl     9
>> 51.23 1       Ctl     10
>> 188.24        1       Ctl     11
>> 35.71 2       Ctl     1
>> 45.15 2       Ctl     2
>> 40.10 2       Ctl     3
>> 312.76        2       Ctl     4
>> 804.05        2       Ctl     5
>> 28.22 2       Ctl     6
>> 68.51 2       Ctl     7
>> 46.15 2       Ctl     8
>> 123.14        2       Ctl     9
>> 33.78 2       Ctl     10
>> 121.28        2       Ctl     11
>> 30.96 3       Ctl     1
>> 36.10 3       Ctl     2
>> 47.19 3       Ctl     3
>> 345.80        3       Ctl     4
>> 644.61        3       Ctl     5
>> 27.73 3       Ctl     6
>> 56.63 3       Ctl     7
>> 42.63 3       Ctl     8
>> 61.25 3       Ctl     9
>> 59.43 3       Ctl     10
>> 109.87        3       Ctl     11
>> 143.50        1       Trt     1
>> 82.76 1       Trt     2
>> 125.03        1       Trt     3
>> 493.76        1       Trt     4
>> 868.48        1       Trt     5
>> 45.09 1       Trt     6
>> 249.43        1       Trt     7
>> 167.28        1       Trt     8
>> 274.72        1       Trt     9
>> 176.40        1       Trt     10
>> 393.10        1       Trt     11
>> 93.75 2       Trt     1
>> 63.83 2       Trt     2
>> 117.50        2       Trt     3
>> 362.68        2       Trt     4
>> 659.40        2       Trt     5
>> 62.10 2       Trt     6
>> 218.24        2       Trt     7
>> 210.98        2       Trt     8
>> 291.48        2       Trt     9
>> 209.36        2       Trt     10
>> 454.68        2       Trt     11
>> 119.62        3       Trt     1
>> 66.50 3       Trt     2
>> 87.37 3       Trt     3
>> 414.01        3       Trt     4
>> 707.70        3       Trt     5
>> 44.40 3       Trt     6
>> 142.59        3       Trt     7
>> 137.37        3       Trt     8
>> 181.03        3       Trt     9
>> 131.65        3       Trt     10
>> 310.18        3       Trt     11
>>
>>> dat1<-read.delim("c:/testcontr.txt", header=T)
>>> dat1$treat<-as.factor(dat1$treat)
>>> dat1$replicate<-as.factor(dat1$replicate)
>>> dat1$section<-as.factor(dat1$section)
>>> attach(dat1)
>>> obj<-lm(log2(yield)~treat*section)
>>> anova(obj)
>> Analysis of Variance Table
>>
>> Response: log2(yield)
>>                Df Sum Sq Mean Sq  F value    Pr(>F)
>> treat          1 24.608  24.608 297.8649 < 2.2e-16 ***
>> section       10 99.761   9.976 120.7565 < 2.2e-16 ***
>> treat:section 10  6.708   0.671   8.1197 2.972e-07 ***
>> Residuals     44  3.635   0.083
>> ---
>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>
>>> contrasts(section)<-c(3,3,3,3,3,3,3,3,-8,-8,-8)
>>> objnew<-lm(log2(yield)~treat*section)
>>> summary(objnew)
>>
>> Call:
>> lm(formula = log2(yield) ~ treat * section)
>>
>> Residuals:
>>       Min       1Q   Median       3Q      Max
>> -0.49647 -0.14913 -0.01521  0.17471  0.51105
>>
>> Coefficients:
>>                      Estimate Std. Error t value Pr(>|t|)
>> (Intercept)         6.288403   0.050034 125.682  < 2e-16 ***
>> treatTrt            1.221219   0.070759  17.259  < 2e-16 ***
>> section1           -0.008502   0.010213  -0.832 0.409675
>> section2           -0.491175   0.165945  -2.960 0.004942 **
>> section3            2.569427   0.165945  15.484  < 2e-16 ***
>> section4            3.556067   0.165945  21.429  < 2e-16 ***
>> section5           -1.157069   0.165945  -6.973 1.25e-08 ***
>> section6           -0.003562   0.165945  -0.021 0.982971
>> section7           -0.487770   0.165945  -2.939 0.005223 **
>> section8            0.106181   0.165945   0.640 0.525585
>> section9           -0.776882   0.165945  -4.682 2.74e-05 ***
>> section10           0.759168   0.165945   4.575 3.87e-05 ***
>> treatTrt:section1  -0.049000   0.014444  -3.392 0.001474 **
>> treatTrt:section2   0.160825   0.234682   0.685 0.496757
>> treatTrt:section3  -0.949101   0.234682  -4.044 0.000208 ***
>> treatTrt:section4  -1.118870   0.234682  -4.768 2.07e-05 ***
>> treatTrt:section5  -0.295937   0.234682  -1.261 0.213950
>> treatTrt:section6   0.538638   0.234682   2.295 0.026549 *
>> treatTrt:section7   0.796518   0.234682   3.394 0.001468 **
>> treatTrt:section8  -0.548744   0.234682  -2.338 0.023984 *
>> treatTrt:section9  -0.191029   0.234682  -0.814 0.420033
>> treatTrt:section10 -0.556642   0.234682  -2.372 0.022137 *
>> ---
>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>
>> Residual standard error: 0.2874 on 44 degrees of freedom
>> Multiple R-Squared: 0.973,      Adjusted R-squared: 0.9601
>> F-statistic: 75.55 on 21 and 44 DF,  p-value: < 2.2e-16
>>
>> Thanks,
>> Joshua
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 24-Aug-06                                       Time: 21:43:57
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From xprt.wannabe at gmail.com  Fri Aug 25 20:52:51 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Fri, 25 Aug 2006 13:52:51 -0500
Subject: [R] How to iteratively extract elements out of a list
Message-ID: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>

Dear List,

The following code produces a list, which is what I what:

set.seed(123)
tmpf <- function() {
x <- rpois(rpois(1,4),2)
}
n <- 3
m <- replicate(n,tmpf())
m

[[1]]
[1] 3 2 4

[[2]]
[1] 0 2 4 2 2 5 2

[[3]]
[1] 2 0 4 1 0


Now I need something that would to extract iteratively (or as many
times as
the size of 'n') the values that are greater than 2 in each component
of
'm' into another list such that the sub-list would be:

[[1]]
[1] 3 4

[[2]]
[1] 4 5

[[3]]
[1] 4

Below is what I tried:

for(i in 1:3)
sub.list <- lapply(m,subset,m[[i]]>2)

> sub.list

which gives me something different from what I want:

[[1]]
[1] 4

[[2]]
[1] 4

[[3]]
[1] 4

Any help would be appreciated.

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R


From btyner at stat.purdue.edu  Fri Aug 25 20:54:02 2006
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Fri, 25 Aug 2006 14:54:02 -0400
Subject: [R] tick.number for date in xyplot
Message-ID: <1156532042.12112.15.camel@capella.stat.purdue.edu>

I would like a tick mark for each month; for example,

xyplot(runif(365)~I(as.Date("1999-01-01") + 1:365),
       scales=list(x=list(format="%b %Y",tick.number=12)))

I know I could make x numeric and use 'at' and 'labels', but I was
wondering if there is a more direct route I'm missing. (In particular,
one that doesn't have to be modified for new data).

Thanks,
Ben


From Horace.Tso at pgn.com  Fri Aug 25 21:16:53 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 25 Aug 2006 12:16:53 -0700
Subject: [R] Quickie : unload library
Message-ID: <s4eeea3e.096@pgn.com>

Dear list,

I know it must be obvious and I did my homework. (In fact I've
RSiteSearched with keyword "remove AND library" but got timed
out.(why?)....)

How do I unload a library? I don't mean getting ride of it permanently
but just to unload it for the time being.

A related problem : I have some libraries loaded at startup in .First()
which I have in .Rprofile. Now, I exited R and commented out the lines
in .First(). Next time I launch R the same libraries are loaded again.
I.e. there seems to be a memory of the old .First() somewhere which
refuses to die.

Thanks in adv.

Horace


From michael at isis.spa.umn.edu  Fri Aug 25 21:35:31 2006
From: michael at isis.spa.umn.edu (Michael Koppelman)
Date: Fri, 25 Aug 2006 14:35:31 -0500
Subject: [R] fitting a gaussian to some x,y data
In-Reply-To: <000701c6c879$4c662b40$2e01a8c0@m8d4477f3de884>
References: <45E8A22D-02EF-4180-8679-631B067D24CF@aps.umn.edu>
	<000701c6c879$4c662b40$2e01a8c0@m8d4477f3de884>
Message-ID: <F2382B08-7B7C-4257-87B1-FC232EE4AF17@aps.umn.edu>

Thank you. Yes, I do feel that I am under-qualified to even ask  
questions of y'all. Plus I'm an astronomer, which doesn't help! ;)  
I'll try again.

I have two columns of data, the first column (x) is a distance (or  
length or separation) and the second column (y) is a flux (or number  
of counts or brightness) at that distance. Thus, when you plot y vs.  
x you get a spatial profile which shows how bright this thing is as a  
function of position. (See the small, attached PNG file. You can see  
there is a vague gaussian shape to the data.) This is measured data  
from a bizarre technique which yields data that is not evenly-spaced  
in x and it does not represent a pure mathematical function (i.e. it  
is not a point spread function or something like that), it represents  
the actual, non-uniform shape of an astronomical object. We are  
making the assumption that the shape of this object can be roughly  
represented with a gaussian.

I want to fit a gaussian to this with the purpose of determining  
systematically the "center" of the normal-like shape of the spatial  
feature. I have successfully done so in R with a polynomial but I  
can't figure out how to do it with a gaussian.

Does that make sense?

Thanks!
Michael


On Aug 25, 2006, at 2:04 PM, MARK LEEDS wrote:

> hi : i'm not clear on what you mean but someone else might be ? if you
> say ( x,y), then it sounds like you are talking about a bivariate  
> normal
> distribution. to fit a regular one dimensional gaussian distribution,
> you can't have 2 dimensional data. i honestly don't mean to sound  
> rude but
> i think you should explain what you want to do  more clearly  
> because I don't think
> I am the only one that will be confused by what you said.
> send out a clearer email and you will get quite a response because
> there are a lot of really smart people ( compared to me ) on this  
> list  that love to help.
> it's an amazing list in that sense.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: example.png
Type: image/png
Size: 15487 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060825/4d0aedbb/attachment.png 

From keele.4 at polisci.osu.edu  Fri Aug 25 21:35:44 2006
From: keele.4 at polisci.osu.edu (Luke Keele)
Date: Fri, 25 Aug 2006 15:35:44 -0400
Subject: [R] horizontal direct product
Message-ID: <B8B5D3B3-0914-4010-ACCF-BD92B99727D3@polisci.osu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060825/51884569/attachment.pl 

From murdoch at stats.uwo.ca  Fri Aug 25 21:44:20 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 25 Aug 2006 15:44:20 -0400
Subject: [R] rgl: exporting to pdf or png does not work
In-Reply-To: <Pine.LNX.4.62.0608242028310.27470@biomath.ugent.be>
References: <Pine.LNX.4.62.0608232308120.31405@biomath.ugent.be>	<44ECDC27.6080109@stats.uwo.ca>	<Pine.LNX.4.62.0608241642490.23380@biomath.ugent.be>	<44EDCD73.4090805@stats.uwo.ca>
	<Pine.LNX.4.62.0608242028310.27470@biomath.ugent.be>
Message-ID: <44EF5314.1010100@stats.uwo.ca>

On 8/24/2006 4:34 PM, Gaspard Lequeux wrote:
> Hej,
> 
> On Thu, 24 Aug 2006, Duncan Murdoch wrote:
> 
>> On 8/24/2006 11:19 AM, Gaspard Lequeux wrote:
>>>
>>> On Wed, 23 Aug 2006, Duncan Murdoch wrote:
>>>
>>>> On 8/23/2006 5:15 PM, Gaspard Lequeux wrote:
>>>>
>>>>> When exporting a image from rgl, the following error is encountered:
>>>>>
>>>>>> rgl.postscript('testing.pdf', fmt="pdf")
>>>>> RGL: ERROR: can't bind glx context to window
>>>>> RGL: ERROR: can't bind glx context to window
>>>>> Warning messages:
>>>>> 1: X11 protocol error: GLXBadContextState
>>>>> 2: X11 protocol error: GLXBadContextState
>>>>>
>>>>> The pdf file is created and is readable, but all the labels are gone.
>>>>>
>>>>> Taking a snapshot (to png) gives 'failed' and no file is created.
>>>>>
>>>>> Version of rgl used: 0.67-2 (2006-07-11)
>>>>> Version of R used: R 2.3.1; i486-pc-linux-gnu; 2006-07-13 01:31:16;
>>>>> Running Debian GNU/Linux testing (Etch).
>>>>
>>>> That looks like an X11 error to me, not something that I'm very likely
>>>> to be able to fix.  If you can debug the error, it would be helpful.
>>>
>>> Actually after upgrading everything (debian testing (etch)) and restarting
>>> X, I didn't get that error anymore. It was worse: R crashed:
>>>
>>>> library(rgl);triangles3d(c(1,,2,3),c(1,2,4),c(1,3,5));rgl.postscript('testing.pdf','pdf')
>>> X Error of failed request:  GLXBadContextState
>>>    Major opcode of failed request:  142 (GLX)
>>>    Minor opcode of failed request:  5 (X_GLXMakeCurrent)
>>>    Serial number of failed request:  85
>>>    Current serial number in output stream:  85
>>> glequeux at toidi:~/seqanal$
>>>
>>>
>>> I downloaded the source package (debian testing (etch), rgl-0.67-2).
>>>
>>> In rgl-0.67-2/src/ I changed the following files:
>>>
>>> rglview.cpp, around line 587. Commenting the function call gl2psBeginPage
>>> removed the crash (but also no pdf output...)
>>>
>>> I enabled this function again and went to gl2ps.c, to the function
>>> gl2psBeginPage. At the end of that function, around line 4426, commenting
>>> out the line
>>> glRenderMode(GL_FEEDBACK);
>>> removes the R crash, but of course still no pdf output (well, only the
>>> background).
>>>
>>> GL_FEEDBACK is defined in /usr/include/GL/gl.h as:
>>>
>>> /* Render Mode */
>>> #define GL_FEEDBACK				0x1C01
>>> #define GL_RENDER				0x1C00
>>> #define GL_SELECT				0x1C02
>>>
>>> Trying glRenderMode(GL_RENDER) removed the crash, but still only the
>>> background in the pdf.
>>>
>>> If someone has some suggestions about what to do next...
>>
>> gl2ps is a separate project, whose source has been included into rgl.
>> You can see the gl2ps project page at http://www.geuz.org/gl2ps/.
>>
>> We're using version 1.2.2, which is a couple of years old.  The current
>> stable release of gl2ps is 1.3.1.  It might fix your problem.
>>
>> I don't know if we modified gl2ps.c or gl2ps.h when they were included,
>> but they haven't been modified since.  (Daniel put them in, based on a
>> patch from Albrecht Gebhardt, according to the log.)
>>
>> It would be helpful to know:
>>
>> 1.  Is the rgl source identical to 1.2.2?
> 
> Yes. The version of gl2ps in rgl is identical to gl2ps version 1.2.2.
> 
>> 2.  Does rgl work if 1.3.1 is dropped in instead?
> 
> No:
> 
> In version 1.3.1:
> 
> #define GL2PS_PS  0
> #define GL2PS_EPS 1
> #define GL2PS_TEX 2
> #define GL2PS_PDF 3
> #define GL2PS_SVG 4
> #define GL2PS_PGF 5
> 
> while in version 1.2.2:
> 
> #define GL2PS_PS  1
> #define GL2PS_EPS 2
> #define GL2PS_TEX 3
> #define GL2PS_PDF 4
> 
> Thus rgl.postscript('probeer.pdf','tex') should be used to generate a pdf. 
> The pdf has still no characters (axes annotations).
> 
> In R/enum.R
> 
> The last line (line 54)
> 
> rgl.enum (postscripttype, ps=1, eps=2, tex=3, pdf=4)
> 
> should be
> 
> rgl.enum (postscripttype, ps=0, eps=1, tex=2, pdf=3)
> 
> and mayebe add svg and pgf...
> 
> 
>> 3.  Does 1.3.1 fix the bug you're seeing?
> 
> No. Same error.
> 
> The error occurs also on ubuntu dapper. On that ubuntu machine, when 
> installing the libgl1-mesa-swrast, the packages libgl1-mesa 
> libgl1-mesa-dri and x-window-system-core are removed. rgl.postscript 
> doesn't produce any errors anymore, the pdf is created but no text (axes 
> decorations) is written to the pdf.
> 
> On debian testing, libgl1-mesa-swx11 can be installed. This removes the 
> follwing packages:
> 
> freeglut3-dev libgl1-mesa-dev libgl1-mesa-dri libgl1-mesa-glx 
> libglitz-glx1-dev libglitz1-dev libglu1-mesa-dev libglui-dev libglut3-dev 
> x-window-system-core xlibmesa-gl-dev xorg
> 
> but R doesn't crash anymore and the figure is written to file (still 
> without axes annotations).
> 
> Reinstal libgl1-mesa-glx removes libgl1-mesa-swx11 and the R crash 
> returns.
> 
> So it seems the bug is really triggered by libgl1-mesa. I filled in a bug 
> report for the debian package libgl1-mesa-glx.
> 
>> I'll look into these at some point, but probably not this week.
> 
> Thanks. No hurry however, as I can still use the classical screenshots. 
> The figures will probable not have to be published, as the expected 
> results are not attained.

Thanks for your work on this.  I'll put the gl2ps 1.3.1 code into rgl 
with the changes you found.

Duncan Murdoch


From Sinnwell.Jason at mayo.edu  Fri Aug 25 16:34:18 2006
From: Sinnwell.Jason at mayo.edu (Sinnwell, Jason P.)
Date: Fri, 25 Aug 2006 09:34:18 -0500
Subject: [R] Problem in library.dynam problems on Linux
Message-ID: <94ACA3F800EA6748A320D779148E620004988ABE@excsrv67.mayo.edu>

I don't know if mount -l shows "something interesting" below.  

We're wondering if there is an issue with the gcc compiler, which has been updated since we last updated R.  Do you see anything that could be wrong with the info below? 

We plan on updating to R 2.3.1, hoping that will clear things up.     

Thanks for your help, Jason

[sinnwell at dnode0 haplo.stats]$ which gcc
/usr/bin/gcc
[sinnwell at dnode0 haplo.stats]$ ls -al /usr/bin/gcc
-rwxr-xr-x  2 root root 103336 Mar  8 15:18 /usr/bin/gcc
[sinnwell at dnode0 haplo.stats]$ ls -al /usr/local/biotools/r
total 36
drwxr-xr-x   4 magee rcf  4096 May  1 13:17 .
drwxr-xr-x  23 magee rcf  4096 Jun 21 15:25 ..
lrwxrwxrwx   1 root  root    7 May  1 13:17 current -> R-2.2.1
drwxr-xr-x   5 magee rcf  4096 Jan  3  2005 R-2.0.1
drwxr-xr-x   5 magee rcf  4096 Feb 22  2006 R-2.2.1
[sinnwell at dnode0 rpack]$ gcc -dumpversion
3.4.5
[sinnwell at dnode0 rpack]$ mount -l
/dev/sda5 on / type ext3 (rw) [/]
none on /proc type proc (rw)
none on /sys type sysfs (rw)
none on /dev/pts type devpts (rw,gid=5,mode=620)
usbfs on /proc/bus/usb type usbfs (rw)
/dev/sda1 on /boot type ext3 (rw) [/boot]
none on /dev/shm type tmpfs (rw)
/dev/sda10 on /local1 type ext3 (rw) [/local1]
/dev/sda9 on /tmp type ext3 (rw) [/tmp]
/dev/sda7 on /usr type ext3 (rw) [/usr]
/dev/sda8 on /var type ext3 (rw) [/var]
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)
rcfcluster-nfs2:/home on /home type nfs (rw,rsize=8192,wsize=8192,intr,timeo=15,addr=172.23.70.7)
rcfcluster-nfs2:/data on /data type nfs (rw,rsize=8192,wsize=8192,intr,timeo=15,addr=172.23.70.7)
rcfcluster-nfs2:/scratch on /scratch type nfs (rw,rsize=8192,wsize=8192,intr,timeo=15,addr=172.23.70.7)

> -----Original Message-----
> From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On 
> Behalf Of Peter Dalgaard
> Sent: Thursday, August 24, 2006 6:02 PM
> To: Sinnwell, Jason P.
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Problem in library.dynam problems on Linux
> 
> "Sinnwell, Jason P." <Sinnwell.Jason at mayo.edu> writes:
> 
> > We have R 2.2.1 installed on a Linux cluster that seems to 
> have problems loading either of our shared object libraries 
> for packages.  This seems to be happening on both local and 
> global versions of packages that we install.  However, we 
> have only noticed this problem in the past 3 months on this R 
> installation, whereas some users had success before then.  It 
> could be that something on our system changed, but I am not 
> an admin so I wouldn't know where to look.  Can anyone help 
> with this problem?
> > 
> > ## A LOCAL INSTALLATION OF HAPLO.STATS APPEARS SUCCESSFUL 
> > [sinnwell at dnode0 rpack]$ R CMD INSTALL -l 
> /home/sinnwell/rdir/tmplib 
> > haplo.stats
> > * Installing *source* package 'haplo.stats' ...
> > ** libs
> > make: `haplo.stats.so' is up to date.
> > ** R
> > ** data
> > ** demo
> > ** inst
> > ** preparing package for lazy loading
> > 
> > ** help
> >  >>> Building/Updating help pages for package 'haplo.stats'
> >      Formats: text html latex example
> > ** building package indices ...
> > * DONE (haplo.stats)
> > 
> > ## TRY AND LOAD THE LOCALLY INSTALLED LIBRARY, YET THE 
> SYSTEM COMMAND 
> > SHOWS THE .so FILE IS THERE
> > 
> > R> library(haplo.stats, lib.loc="/home/sinnwell/rdir/tmplib/")
> > Error in dyn.load(x, as.logical(local), as.logical(now)) : 
> >         unable to load shared library 
> '/home/sinnwell/rdir/tmplib/haplo.stats/libs/haplo.stats.so':
> >   
> /home/sinnwell/rdir/tmplib/haplo.stats/libs/haplo.stats.so: cannot 
> > open shared object file: No such file or directory Error in 
> library(haplo.stats, lib.loc = "/home/sinnwell/rdir/tmplib/") :
> >         .First.lib failed for 'haplo.stats'
> > R> system('ls -al /home/sinnwell/rdir/tmplib/haplo.stats/libs')
> > total 88
> > drwxr-xr-x   2 sinnwell sinnwell  4096 Aug 24 15:14 .
> > drwxr-xr-x  13 sinnwell sinnwell  4096 Aug 24 15:14 ..
> > -rwxr-xr-x   1 sinnwell sinnwell 61566 Aug 24 15:14 haplo.stats.so
> 
> Hmmmm.. Does "mount -l" show something interesting on the 
> filesystem containing the .so file? (e.g., option noexec).
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
>


From sachinj.2006 at yahoo.com  Fri Aug 25 21:56:13 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 25 Aug 2006 12:56:13 -0700 (PDT)
Subject: [R] Quickie : unload library
In-Reply-To: <s4eeea3e.096@pgn.com>
Message-ID: <20060825195613.15173.qmail@web37614.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060825/a1b46191/attachment.pl 

From jfox at mcmaster.ca  Fri Aug 25 22:10:19 2006
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 25 Aug 2006 16:10:19 -0400
Subject: [R] polychor error
Message-ID: <20060825201019.TCKM10262.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Janet,

Performing a traceback after the error gives a hint:

> tmp.pcc<-polychor(tmp.mat, ML=T, std.err=T)
> traceback()
8: stop("at least one element of ", sQuote("lower"), " is larger than ", 
       sQuote("upper"))
7: checkmvArgs(lower = lower, upper = upper, mean = mean, corr = corr, 
       sigma = sigma)
6: pmvnorm(lower = c(row.cuts[i], col.cuts[j]), upper = c(row.cuts[i + 
       1], col.cuts[j + 1]), corr = R)
5: binBvn(rho, row.cuts, col.cuts)
4: fn(par, ...)
3: function (par) 
   fn(par, ...)(c(0.422816748044139, -2.20343446037221, -2.2163627792244, 
   -1.79075055316993, -1.11077161663679, -0.323037731981826,
0.664036943094355, 
   -2.71305188847272, -1.58338678422633, -0.534011853182102,
0.65365619155084
   ))
2: optim(c(optimise(f, interval = c(-1, 1))$minimum, rc, cc), f, 
       control = control, hessian = std.err)
1: polychor(tmp.mat, ML = T, std.err = T)

The parameters are in the order correlation, row thresholds (of which there
are 6), column thresholds (of which there are 4), and at this point in the
maximization of the likelihood the first row threshold (-2.20343446037221)
is *above* the second threshold (-2.2163627792244), causing pmvnorm() to
complain. This can happen when the problem is ill-conditioned, since optim()
doesn't constrain the order of the thresholds.

So, why is the problem ill-conditioned? Here is your contingency table:

> tmp.mat
  3 4  5  6  7
1 0 2  0  0  0
2 0 0  1  1  0
3 0 0  5  1  1
4 0 5 10 12  2
5 0 5 27 29 11
6 1 3 20 57 31
7 0 1  9 34 32

There are only two observations in the first row, two in the second row, and
one in the first column.  You're expecting a lot out of ML to get estimates
of the first couple of thresholds for rows and the first for columns.

One approach would be to eliminate one or more sparse rows or columns; e.g.,

> polychor(tmp.mat[-1,], ML = TRUE, std.err = TRUE)

Polychoric Correlation, ML est. = 0.3932 (0.05719)
Test of bivariate normality: Chisquare = 14.21, df = 19, p = 0.7712

  Row Thresholds
  Threshold Std.Err.
1   -2.4410  0.24270
2   -1.8730  0.14280
3   -1.1440  0.09236
4   -0.3353  0.07408
5    0.6636  0.07857


  Column Thresholds
  Threshold Std.Err.
1   -2.6500  0.30910
2   -1.6420  0.12100
3   -0.5494  0.07672
4    0.6508  0.07833
> polychor(tmp.mat[,-1], ML = TRUE, std.err = TRUE)

Polychoric Correlation, ML est. = 0.4364 (0.05504)
Test of bivariate normality: Chisquare = 14.85, df = 17, p = 0.6062

  Row Thresholds
  Threshold Std.Err.
1   -2.4940  0.26020
2   -2.2080  0.19160
3   -1.7850  0.13400
4   -1.1090  0.09113
5   -0.3154  0.07371
6    0.6625  0.07821


  Column Thresholds
  Threshold Std.Err.
1   -1.6160  0.11970
2   -0.5341  0.07639
3    0.6507  0.07800
> 

A more defensible alternative would be to collapse sparse rows or columns.

BTW, you *can* get estimated standard-errors from polychor() for your
original table for the two-step estimator:

> polychor(tmp.mat, ML = FALSE, std.err = TRUE)

Polychoric Correlation, 2-step est. = 0.4228 (0.05298)
Test of bivariate normality: Chisquare = 19.22, df = 23, p = 0.6883

That's because the two-step estimator estimates the thresholds from the
marginal distributions of the variables rather than from their joint
distribution.

So, is this a bug or a feature? I suppose that it's a bug to allow the
thresholds to get out of order, though to constrain the optimization to
prevent that from happening is probably not worth the effort and could cause
some strange results. On the other hand, the error tells you something about
the data, so maybe it's a feature.

I noticed that you posted another version of this question two days before
this one. I apologize for the slow response -- I wasn't able to read my
email for a few days and it's taken me most of today to catch up.

Regards,
 John

---- original message -------

Janet Rosenbaum jrosenba at rand.org
Thu Aug 24 00:41:16 CEST 2006

Hi.

Does anyone know whether the following error is a result of a bug or a 
feature?

I can eliminate the error by making ML=F, but I would like to see the 
values of the cut-points and their variance.  Is there anything that I 
can do?


tmp.vec<-c(0,  0,  0 , 0  ,0 , 1,  0,  2,  0 , 0,  5  ,5  ,3  ,1,  0 , 
1,  5, 10, 27, 20,  9,  0,  1,  1, 12, 29, 57, 34,  0,  0,  1,  2, 11, 
31, 32)
tmp.mat<-matrix(tmp.vec, nrow=7)
rownames(tmp.mat)<-1:7
colnames(tmp.mat)<-3:7
tmp.pcc<-polychor(tmp.mat, ML=T, std.err=T)
Error in checkmvArgs(lower = lower, upper = upper, mean = mean, corr = 
corr,  :
     at least one element of 'lower' is larger than 'upper'

Thanks,

Janet

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox


From Horace.Tso at pgn.com  Fri Aug 25 22:11:08 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 25 Aug 2006 13:11:08 -0700
Subject: [R] Quickie : unload library
Message-ID: <s4eef6fb.051@pgn.com>

Sachin,

I did try that, ex

detach(zoo)

Error in detach(zoo) : invalid name

detach("zoo")

Error in detach("zoo") : invalid name

But zoo has been loaded,

sessionInfo()
Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods"   "datasets"  "stats"     "tcltk"     "utils"    
"graphics" 
[7] "grDevices" "base"     

other attached packages:
   tseries   quadprog        zoo       MASS       Rpad 
  "0.10-1"    "1.4-8"    "1.2-0" "7.2-27.1"    "1.1.1" 

Thks,

H.


>>> Sachin J <sachinj.2006 at yahoo.com> 8/25/2006 12:56 PM >>>
see ?detach 
  

Horace Tso <Horace.Tso at pgn.com> wrote:
  Dear list,

I know it must be obvious and I did my homework. (In fact I've
RSiteSearched with keyword "remove AND library" but got timed
out.(why?)....)

How do I unload a library? I don't mean getting ride of it permanently
but just to unload it for the time being.

A related problem : I have some libraries loaded at startup in
.First()
which I have in .Rprofile. Now, I exited R and commented out the lines
in .First(). Next time I launch R the same libraries are loaded again.
I.e. there seems to be a memory of the old .First() somewhere which
refuses to die.

Thanks in adv.

Horace

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


 				
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Aug 25 22:15:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Aug 2006 21:15:28 +0100 (BST)
Subject: [R] Quickie : unload library
In-Reply-To: <s4eeea3e.096@pgn.com>
References: <s4eeea3e.096@pgn.com>
Message-ID: <Pine.LNX.4.64.0608252110180.15895@gannet.stats.ox.ac.uk>

On Fri, 25 Aug 2006, Horace Tso wrote:

> Dear list,
> 
> I know it must be obvious and I did my homework. (In fact I've
> RSiteSearched with keyword "remove AND library" but got timed
> out.(why?)....)

Probably because the site is offline.

> How do I unload a library? I don't mean getting ride of it permanently
> but just to unload it for the time being.

Do you mean 'package' (see detach and unLoadNamespace) or 'library' (see
dyn.unload and library.dynam.unload)?

> A related problem : I have some libraries loaded at startup in .First()
> which I have in .Rprofile. Now, I exited R and commented out the lines
> in .First(). Next time I launch R the same libraries are loaded again.
> I.e. there seems to be a memory of the old .First() somewhere which
> refuses to die.

Are you restoring a workspace containing .First?  Always try with 
--vanilla to check.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sachinj.2006 at yahoo.com  Fri Aug 25 22:16:01 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 25 Aug 2006 13:16:01 -0700 (PDT)
Subject: [R] Quickie : unload library
In-Reply-To: <s4eef6fb.052@pgn.com>
Message-ID: <20060825201601.2309.qmail@web37608.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060825/745ce72f/attachment.pl 

From Horace.Tso at pgn.com  Fri Aug 25 22:20:49 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 25 Aug 2006 13:20:49 -0700
Subject: [R] Quickie : unload library
Message-ID: <s4eef936.094@pgn.com>

Aah, that works. The missing "package:..."

H.

>>> Sachin J <sachinj.2006 at yahoo.com> 8/25/2006 1:16 PM >>>
try detach("package:zoo")
   
  Sachin

Horace Tso <Horace.Tso at pgn.com> wrote:
  Sachin,

I did try that, ex

detach(zoo)

Error in detach(zoo) : invalid name

detach("zoo")

Error in detach("zoo") : invalid name

But zoo has been loaded,

sessionInfo()
Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods" "datasets" "stats" "tcltk" "utils" 
"graphics" 
[7] "grDevices" "base" 

other attached packages:
tseries quadprog zoo MASS Rpad 
"0.10-1" "1.4-8" "1.2-0" "7.2-27.1" "1.1.1" 

Thks,

H.


>>> Sachin J 8/25/2006 12:56 PM >>>
see ?detach 


Horace Tso wrote:
Dear list,

I know it must be obvious and I did my homework. (In fact I've
RSiteSearched with keyword "remove AND library" but got timed
out.(why?)....)

How do I unload a library? I don't mean getting ride of it permanently
but just to unload it for the time being.

A related problem : I have some libraries loaded at startup in
.First()
which I have in .Rprofile. Now, I exited R and commented out the lines
in .First(). Next time I launch R the same libraries are loaded again.
I.e. there seems to be a memory of the old .First() somewhere which
refuses to die.

Thanks in adv.

Horace

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.



---------------------------------

[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


 		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From luke at stat.uiowa.edu  Fri Aug 25 22:22:51 2006
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 25 Aug 2006 15:22:51 -0500 (CDT)
Subject: [R] increasing the # of socket connections
In-Reply-To: <Pine.LNX.4.64.0608251644190.8338@gannet.stats.ox.ac.uk>
References: <20060825144809.GC4862@iwr.uni-heidelberg.de>
	<Pine.LNX.4.64.0608251644190.8338@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0608251521490.3853@itasca2.wildberry.org>

On Fri, 25 Aug 2006, Prof Brian Ripley wrote:

> On Fri, 25 Aug 2006, Marc Kirchner wrote:
>
>> Dear "R-help"ers,
>>
>> using snow on socket connections, I ran into the following error
>>
>> >  cl <- makeSOCKcluster(hosts)
>> 	Error in socketConnection(port = port, server = TRUE,
>> 	blocking = TRUE : all connections are in use
>>
>> with "showConnections(all=T)" showing 50 open connections.
>>
>> As - for administrative reasons - I would prefer to use snow's
>> SOCK capabilities (instead of MPI and the like), I wonder if there is a
>> way to increase the number of simultaneous open connections allowed in
>> an R session (~100 would be sufficient).
>
> If you really need them open at once (and are not just forgetting to close
> them), then change the constant in the file and recompile.
>
>> Any help/hints are greatly appreciated,
>
> Is this the appropriate list: not on my reading of the posting guide!
>

Would you have asked that question if the answer had been
options(max.connections=100)?  I for one would not.

Best,

luke

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From sarah.goslee at gmail.com  Fri Aug 25 22:23:59 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 25 Aug 2006 16:23:59 -0400
Subject: [R] Quickie : unload library
In-Reply-To: <20060825201601.2309.qmail@web37608.mail.mud.yahoo.com>
References: <s4eef6fb.052@pgn.com>
	<20060825201601.2309.qmail@web37608.mail.mud.yahoo.com>
Message-ID: <efb536d50608251323s4a5a5defx1fcace9c4265f9d6@mail.gmail.com>

Hi,

There are two ways that I know of:
detach("package:zoo")
or use detach() with the number of that package's
place in the search list.

> library(zoo)
> search()
 [1] ".GlobalEnv"        "package:zoo"       "package:methods"
 [4] "package:graphics"  "package:grDevices" "package:utils"
 [7] "package:datasets"  "package:ecoutils"  "package:ecodist"
[10] "package:stats"     "package:gtkDevice" "Autoloads"
[13] "package:base"
> detach("package:zoo")
> search()
 [1] ".GlobalEnv"        "package:methods"   "package:graphics"
 [4] "package:grDevices" "package:utils"     "package:datasets"
 [7] "package:ecoutils"  "package:ecodist"   "package:stats"
[10] "package:gtkDevice" "Autoloads"         "package:base"

OR

> library(zoo)
> search()
 [1] ".GlobalEnv"        "package:zoo"       "package:methods"
 [4] "package:graphics"  "package:grDevices" "package:utils"
 [7] "package:datasets"  "package:ecoutils"  "package:ecodist"
[10] "package:stats"     "package:gtkDevice" "Autoloads"
[13] "package:base"
> detach(2)
> search()
 [1] ".GlobalEnv"        "package:methods"   "package:graphics"
 [4] "package:grDevices" "package:utils"     "package:datasets"
 [7] "package:ecoutils"  "package:ecodist"   "package:stats"
[10] "package:gtkDevice" "Autoloads"         "package:base"

Sarah

-- 
Sarah Goslee
http://www.stringpage.com


From Dimitris.Rizopoulos at med.kuleuven.be  Fri Aug 25 22:34:18 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Fri, 25 Aug 2006 22:34:18 +0200
Subject: [R] horizontal direct product
In-Reply-To: <B8B5D3B3-0914-4010-ACCF-BD92B99727D3@polisci.osu.edu>
References: <B8B5D3B3-0914-4010-ACCF-BD92B99727D3@polisci.osu.edu>
Message-ID: <20060825223418.uajtafmofockk088@webmail4.kuleuven.be>

maybe something like:

"%*~%" <- function(x, y){
     n <- nrow(x)
     out <- matrix(0, n, ncol(x) * ncol(y))
     for(i in 1:n)
         out[i, ] <- c(y[i, ] %o% x[i, ])
     out
}

x <- matrix(1:4, 2, 2, TRUE)
y <- matrix(5:8, 2, 2, TRUE)

x %*~% y


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Luke Keele <keele.4 at polisci.osu.edu>:

> II am translating some gauss code into R, and gauss has a matrix
> product function called the horizontal direct product (*~), which is
> some sort of variant on the Kronecker product.
>
> For example if x is 2x2 and y is 2x2
>
> the horizontal direct product, z, of x and y is defined (in the Gauss
> manual) as:
>
> row 1 = x11*y11 x11*y12 x12*y11 x12*y12
> row 2 = x21*y21 x21*y22 x22*y21 x22*y22
>
> Or in R code if:
>
> x <- matrix(seq(1,4,by=1),2,2, byrow=TRUE)
> y <- matrix(seq(5,8,by=1),2,2, byrow=TRUE)
>
> The resulting matrix, if I had an operator, would be the following
> matrix z, here formed in a contrived manner:
>
> z.1 <- c(5, 6, 10, 12)
> z.2 <- c(21,24,28,32)
> z <- rbind(z.1,z.2)
>
> I realize that this is just the first and last row of x%*%y when x
> and y are two by two but this  won't generalize with larger
> matrices.  Any ideas about whether this can be done with existing R
> functions in a general way short of writing my own function?
>
>    Thanks
>
> Luke
>
>
>
>
> Luke Keele
> Department of Political Science
> Ohio State University
> keele.4 at polisci.osu.edu
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From markleeds at verizon.net  Fri Aug 25 22:35:14 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Fri, 25 Aug 2006 16:35:14 -0400
Subject: [R] fitting a gaussian to some x,y data
References: <45E8A22D-02EF-4180-8679-631B067D24CF@aps.umn.edu>
	<000701c6c879$4c662b40$2e01a8c0@m8d4477f3de884>
	<F2382B08-7B7C-4257-87B1-FC232EE4AF17@aps.umn.edu>
Message-ID: <003d01c6c885$f8f32e10$2e01a8c0@m8d4477f3de884>

hi michael : ( i stupidly didn't send the initial email to the whole list so 
they will have to read from the bottom ),  it's clearer now but "fitting a 
gaussian" still may not be the right thing to do in this case. someone else 
can answer better but it sounds to me like you want to do some kind of 
regression ( i dont know whether it would be linear or glm or what )  using 
distance versus brightness.
i have no clue about astronomy but if you think there is a relationship 
between them  then this is probably the more appropriate approach to take. 
again, don't take my word in stone. someone else
will likely reply.

----- Original Message ----- 
From: "Michael Koppelman" <michael at isis.spa.umn.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 25, 2006 3:35 PM
Subject: Re: [R] fitting a gaussian to some x,y data


> Thank you. Yes, I do feel that I am under-qualified to even ask
> questions of y'all. Plus I'm an astronomer, which doesn't help! ;)
> I'll try again.
>
> I have two columns of data, the first column (x) is a distance (or
> length or separation) and the second column (y) is a flux (or number
> of counts or brightness) at that distance. Thus, when you plot y vs.
> x you get a spatial profile which shows how bright this thing is as a
> function of position. (See the small, attached PNG file. You can see
> there is a vague gaussian shape to the data.) This is measured data
> from a bizarre technique which yields data that is not evenly-spaced
> in x and it does not represent a pure mathematical function (i.e. it
> is not a point spread function or something like that), it represents
> the actual, non-uniform shape of an astronomical object. We are
> making the assumption that the shape of this object can be roughly
> represented with a gaussian.
>
> I want to fit a gaussian to this with the purpose of determining
> systematically the "center" of the normal-like shape of the spatial
> feature. I have successfully done so in R with a polynomial but I
> can't figure out how to do it with a gaussian.
>
> Does that make sense?
>
> Thanks!
> Michael
>
>
> On Aug 25, 2006, at 2:04 PM, MARK LEEDS wrote:
>
>> hi : i'm not clear on what you mean but someone else might be ? if you
>> say ( x,y), then it sounds like you are talking about a bivariate
>> normal
>> distribution. to fit a regular one dimensional gaussian distribution,
>> you can't have 2 dimensional data. i honestly don't mean to sound
>> rude but
>> i think you should explain what you want to do  more clearly
>> because I don't think
>> I am the only one that will be confused by what you said.
>> send out a clearer email and you will get quite a response because
>> there are a lot of really smart people ( compared to me ) on this
>> list  that love to help.
>> it's an amazing list in that sense.
>
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Fri Aug 25 23:02:54 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 25 Aug 2006 17:02:54 -0400
Subject: [R] tick.number for date in xyplot
In-Reply-To: <1156532042.12112.15.camel@capella.stat.purdue.edu>
References: <1156532042.12112.15.camel@capella.stat.purdue.edu>
Message-ID: <644e1f320608251402ie3ee5b0u405aaf35bb39517b@mail.gmail.com>

Try this:


xyplot(runif(365)~I(as.Date("1999-01-01") + 1:365),
      scales=list(x=list(at=seq(as.Date('1999-1-1'), length=12, by='1
month'), labels=NULL)))



On 8/25/06, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> I would like a tick mark for each month; for example,
>
> xyplot(runif(365)~I(as.Date("1999-01-01") + 1:365),
>       scales=list(x=list(format="%b %Y",tick.number=12)))
>
> I know I could make x numeric and use 'at' and 'labels', but I was
> wondering if there is a more direct route I'm missing. (In particular,
> one that doesn't have to be modified for new data).
>
> Thanks,
> Ben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Fri Aug 25 23:26:38 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 25 Aug 2006 17:26:38 -0400
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
Message-ID: <644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>

try this:

> set.seed(123)
> tmpf <- function() {
+ x <- rpois(rpois(1,4),2)
+ }
> n <- 3
> m <- replicate(n,tmpf())
> m
[[1]]
[1] 3 2 4

[[2]]
[1] 0 2 4 2 2 5 2

[[3]]
[1] 2 0 4 1 0

> lapply(m, function(x)x[x>2])
[[1]]
[1] 3 4

[[2]]
[1] 4 5

[[3]]
[1] 4

>

On 8/25/06, xpRt.wannabe <xprt.wannabe at gmail.com> wrote:
> Dear List,
>
> The following code produces a list, which is what I what:
>
> set.seed(123)
> tmpf <- function() {
> x <- rpois(rpois(1,4),2)
> }
> n <- 3
> m <- replicate(n,tmpf())
> m
>
> [[1]]
> [1] 3 2 4
>
> [[2]]
> [1] 0 2 4 2 2 5 2
>
> [[3]]
> [1] 2 0 4 1 0
>
>
> Now I need something that would to extract iteratively (or as many
> times as
> the size of 'n') the values that are greater than 2 in each component
> of
> 'm' into another list such that the sub-list would be:
>
> [[1]]
> [1] 3 4
>
> [[2]]
> [1] 4 5
>
> [[3]]
> [1] 4
>
> Below is what I tried:
>
> for(i in 1:3)
> sub.list <- lapply(m,subset,m[[i]]>2)
>
> > sub.list
>
> which gives me something different from what I want:
>
> [[1]]
> [1] 4
>
> [[2]]
> [1] 4
>
> [[3]]
> [1] 4
>
> Any help would be appreciated.
>
> > version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rab45+ at pitt.edu  Sat Aug 26 00:41:34 2006
From: rab45+ at pitt.edu (Rick Bilonick)
Date: Fri, 25 Aug 2006 18:41:34 -0400
Subject: [R] Problem with geeglm
Message-ID: <1156545695.3519.19.camel@localhost.localdomain>

event.nab.2 is 0/1 and I dichotomized va to get va.2 to see if I could
get geeglm to work. glm has no problem with the data but geeglm chokes.
Each subject (patient.id) has at most 2 observations and more than 3/4
of the subjects have 2 observations. I have even worse problems trying
to use glmmPQL from MASS and worse still trying to use lmer from lme4.
But I figured a marginal model would work. (geeglm seems to work OK with
most of the explanatory factors in the data set I have but a couple of
them show similar problems.)

> summary(glm(event.nab.2~va.2,family=binomial(link="logit"),data=test))

Call:
glm(formula = event.nab.2 ~ va.2, family = binomial(link = "logit"),
    data = test)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-0.3787  -0.3787  -0.2246  -0.2246   2.7177

Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept)    -2.5993     0.1804  -14.41  < 2e-16 ***
va.2(84, Inf]  -1.0685     0.3435   -3.11  0.00187 **
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 363.19  on 958  degrees of freedom
Residual deviance: 352.28  on 957  degrees of freedom
AIC: 356.28

Number of Fisher Scoring iterations: 6


summary(geeglm(event.nab.2~va.2,family=binomial(link="logit"),id=patient.id,cor="exch",data=test))
Error in geese.fit(xx, yy, id, offset, soffset, w, waves = waves,
zsca,  :
        nrow(zsca) and length(y) not match


> head(test)
  patient.id event.nab.2      va.2  
1          1           0 (-Inf,84]         
2          1           0 (-Inf,84]         
3          2           0 (84, Inf]         
4          2           0 (84, Inf]         
5          3           0 (84, Inf]         
6          3           0 (84, Inf]         

I'm using R 2.3.1 and the latest version of geepack. I get a similar
error message if I use va which is continuous.

I don't know what the error message from geeglm/geese means.

Rick B.


From tlumley at u.washington.edu  Sat Aug 26 02:21:16 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Aug 2006 17:21:16 -0700 (PDT)
Subject: [R] horizontal direct product
In-Reply-To: <B8B5D3B3-0914-4010-ACCF-BD92B99727D3@polisci.osu.edu>
References: <B8B5D3B3-0914-4010-ACCF-BD92B99727D3@polisci.osu.edu>
Message-ID: <Pine.LNX.4.64.0608251720070.27104@homer21.u.washington.edu>

On Fri, 25 Aug 2006, Luke Keele wrote:

> II am translating some gauss code into R, and gauss has a matrix
> product function called the horizontal direct product (*~), which is
> some sort of variant on the Kronecker product.
>
> For example if x is 2x2 and y is 2x2
>
> the horizontal direct product, z, of x and y is defined (in the Gauss
> manual) as:
>
> row 1 = x11*y11 x11*y12 x12*y11 x12*y12
> row 2 = x21*y21 x21*y22 x22*y21 x22*y22
>

It looks as though
"%~%" <- function (A, B)
{
     m <- ncol(A)
     n <- ncol(B)
     A[, rep(1:m, each = n)] * B[, rep(1:n, m)]
}

would do it.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From blomsp at ozemail.com.au  Sat Aug 26 02:32:26 2006
From: blomsp at ozemail.com.au (simon blomberg)
Date: Sat, 26 Aug 2006 10:32:26 +1000
Subject: [R] R in Nature
Message-ID: <a06110404c11546e5f6d0@[10.1.1.3]>

AAh. Then my hypothesis has been rejected. Oh well!

Cheers,

Simon.


>Simon,
>
>Congratulations!
>
>It used to be that
>
>	R Ihaka and R Gentleman
>	R: A Language for Data Analysis and Graphics
>	Journal of Computational and Graphical Statistics, 1996
>
>was used to cite R.
>
>I see a handful of these in Nature from around 2003.
>
>Chuck
>
>On Fri, 25 Aug 2006, Simon Blomberg wrote:
>
>>Hi all,
>>
>>We've just had a paper accepted for publication in Nature. We used R for
>>95% of our analyses (one of my co-authors sneaked in some GenStat when I
>>wasn't looking.). The preprint is available from the Nature web site, in
>>the open peer-review trial section. I searched Nature for previous
>>references to "R Development Core Team", and I received no hits. So I
>>tentatively conclude that our paper is the first Nature paper to cite R.
>>
>>A great many thanks to the R Development Core Team for R, and Prof.
>>Bates for lmer.
>>
>>Cheers,
>>
>>Simon.
>>(I'm off to the pub to celebrate.)
>>
>>--
>>Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
>>Centre for Resource and Environmental Studies
>>The Australian National University
>>Canberra ACT 0200
>>Australia
>>T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
>>F: +61 2 6125 0757
>>CRICOS Provider # 00120C
>>
>>The combination of some data and an aching desire for
>>an answer does not ensure that a reasonable answer
>>can be extracted from a given body of data.
>>- John Tukey.
>>
>>
>>
>>    [ Part 3.91: "Included Message" ]
>>
>
>Charles C. Berry                        (858) 534-2098
>                                          Dept of Family/Preventive Medicine
>E mailto:cberry at tajo.ucsd.edu	         UC San Diego
>http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 7800
F: +61 2 6125 0757

CRICOS Provider # 00120C


From ggrothendieck at gmail.com  Sat Aug 26 03:25:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Aug 2006 21:25:31 -0400
Subject: [R] Modifying the embed-results
In-Reply-To: <1156526634.12112.13.camel@attenka>
References: <1156526634.12112.13.camel@attenka>
Message-ID: <971536df0608251825v200418dfxab19e22b36283887@mail.gmail.com>

Try:

embed(VECTOR, 5)[,5:1]

On 8/25/06, Atte Tenkanen <attenka at utu.fi> wrote:
> Hi,
>
> Here is a vector and the result from the embed-command:
>
> VECTOR=c(0,3,6,3,11,2,4,3,7,6,4,5,10,2,3,5,8)
>
> > embed(VECTOR, dimension=5)
>      [,1] [,2] [,3] [,4] [,5]
>  [1,]   11    3    6    3    0
>  [2,]    2   11    3    6    3
>  [3,]    4    2   11    3    6
>  [4,]    3    4    2   11    3
>  [5,]    7    3    4    2   11
>  [6,]    6    7    3    4    2
>  [7,]    4    6    7    3    4
>  [8,]    5    4    6    7    3
>  [9,]   10    5    4    6    7
> [10,]    2   10    5    4    6
> [11,]    3    2   10    5    4
> [12,]    5    3    2   10    5
> [13,]    8    5    3    2   10
>
> Is there a way to little modify the algorithm so that the result looks
> like this:
>
> [1]  0  3  6 11  2 <- beginning from the first number of the VECTOR
> [1]  3  6 11  2  4 <- beginning from the second number of the VECTOR etc
> [1]  6  3 11  2  4
> [1]  3 11  2  4  7
> [1] 11  2  4  3  7
> [1] 2 4 3 7 6
> [1] 4 3 7 6 5
> [1] 3 7 6 4 5
> [1]  7  6  4  5 10
> [1]  6  4  5 10  2
> [1]  4  5 10  2  3
> [1]  5 10  2  3  8
> [1] 10  2  3  5  8
>
> Every row consists of next five unique(!) member of the VECTOR. I made
> this example result with a time consuming algorithm which uses for-loops
> and whiles.
>
> How to do this better??
>
> Thanks in advance!
>
> Atte Tenkanen
> University of Turku
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From montez at bu.edu  Sat Aug 26 03:44:49 2006
From: montez at bu.edu (Maria Montez)
Date: Fri, 25 Aug 2006 18:44:49 -0700
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44EE1F34.6010400@pdf.com>
References: <44EE13B6.20205@bu.edu> <44EE1F34.6010400@pdf.com>
Message-ID: <44EFA791.5030703@bu.edu>

Thank you for your answers yesterday. I now have another question!

Suppose that instead of creating a formula for a regression model I 
simply wanted to add the variables. I believe I cannot use the 
as.formula anymore. Again I tried to use expression to no avail. I get 
an expression but I can't use it.

fit.sum <- function(x) {
    fo <- expression(paste(x, collapse = "+"))
   eval( fo)
}
fit.sum(c("x1","x2"))

Basically what I need is to learn how to use variables when what is 
given to me are their names (character list).

Thanks again, Maria




Sundar Dorai-Raj wrote:

>
> Maria Montez wrote:
>
>> Hi!
>>
>> I would like to be able to create formulas automatically. For 
>> example, I want to be able to create a function that takes on two 
>> values: resp and x, and then creates the proper formula to regress 
>> resp on x.
>>
>> My code:
>>
>> fit.main <- function(resp,x) {
>>  form <- expression(paste(resp," ~ ",paste(x,sep="",collapse=" + 
>> "),sep=""))
>>   z <- lm(eval(form))
>>  z
>> }
>> main <- fit.main("y",c("x1","x2","x3","x4"))
>>
>> and I get this error:
>> Error in terms.default(formula, data = data) :
>>         no terms component
>>
>> Any suggestions?
>>
>> Thanks, Maria
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> Hi, Maria,
>
> Try
>
> regr <- paste(x, collapse = "+")
> form <- as.formula(sprintf("%s ~ %s", resp, regr))
>
> HTH,
>
> --sundar
>


From ggrothendieck at gmail.com  Sat Aug 26 03:54:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Aug 2006 21:54:08 -0400
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44EFA791.5030703@bu.edu>
References: <44EE13B6.20205@bu.edu> <44EE1F34.6010400@pdf.com>
	<44EFA791.5030703@bu.edu>
Message-ID: <971536df0608251854i3d3b32f4oc0f0f797ba124507@mail.gmail.com>

Try this and note we must pay attention to which environment
the expression is evaluated in.


fit.sum <- function(x, env = parent.frame())
   eval(parse(text = paste(x, collapse = "+")), env = env)

# test
x1 <- x2 <- 1
fit.sum(c("x1","x2"))



On 8/25/06, Maria Montez <montez at bu.edu> wrote:
> Thank you for your answers yesterday. I now have another question!
>
> Suppose that instead of creating a formula for a regression model I
> simply wanted to add the variables. I believe I cannot use the
> as.formula anymore. Again I tried to use expression to no avail. I get
> an expression but I can't use it.
>
> fit.sum <- function(x) {
>    fo <- expression(paste(x, collapse = "+"))
>   eval( fo)
> }
> fit.sum(c("x1","x2"))
>
> Basically what I need is to learn how to use variables when what is
> given to me are their names (character list).
>
> Thanks again, Maria
>
>
>
>
> Sundar Dorai-Raj wrote:
>
> >
> > Maria Montez wrote:
> >
> >> Hi!
> >>
> >> I would like to be able to create formulas automatically. For
> >> example, I want to be able to create a function that takes on two
> >> values: resp and x, and then creates the proper formula to regress
> >> resp on x.
> >>
> >> My code:
> >>
> >> fit.main <- function(resp,x) {
> >>  form <- expression(paste(resp," ~ ",paste(x,sep="",collapse=" +
> >> "),sep=""))
> >>   z <- lm(eval(form))
> >>  z
> >> }
> >> main <- fit.main("y",c("x1","x2","x3","x4"))
> >>
> >> and I get this error:
> >> Error in terms.default(formula, data = data) :
> >>         no terms component
> >>
> >> Any suggestions?
> >>
> >> Thanks, Maria
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
> > Hi, Maria,
> >
> > Try
> >
> > regr <- paste(x, collapse = "+")
> > form <- as.formula(sprintf("%s ~ %s", resp, regr))
> >
> > HTH,
> >
> > --sundar
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sat Aug 26 04:28:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Aug 2006 22:28:49 -0400
Subject: [R] Quickie : unload library
In-Reply-To: <efb536d50608251323s4a5a5defx1fcace9c4265f9d6@mail.gmail.com>
References: <s4eef6fb.052@pgn.com>
	<20060825201601.2309.qmail@web37608.mail.mud.yahoo.com>
	<efb536d50608251323s4a5a5defx1fcace9c4265f9d6@mail.gmail.com>
Message-ID: <971536df0608251928j3f819b33sa9b71ae4e007c4db@mail.gmail.com>

You likely want the answer that Sarah has already given but in
addition you might also want to look at the thread below,
the point being that detaching a package still leaves portions:

https://www.stat.math.ethz.ch/pipermail/r-help/2006-July/109056.html


On 8/25/06, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Hi,
>
> There are two ways that I know of:
> detach("package:zoo")
> or use detach() with the number of that package's
> place in the search list.
>
> > library(zoo)
> > search()
>  [1] ".GlobalEnv"        "package:zoo"       "package:methods"
>  [4] "package:graphics"  "package:grDevices" "package:utils"
>  [7] "package:datasets"  "package:ecoutils"  "package:ecodist"
> [10] "package:stats"     "package:gtkDevice" "Autoloads"
> [13] "package:base"
> > detach("package:zoo")
> > search()
>  [1] ".GlobalEnv"        "package:methods"   "package:graphics"
>  [4] "package:grDevices" "package:utils"     "package:datasets"
>  [7] "package:ecoutils"  "package:ecodist"   "package:stats"
> [10] "package:gtkDevice" "Autoloads"         "package:base"
>
> OR
>
> > library(zoo)
> > search()
>  [1] ".GlobalEnv"        "package:zoo"       "package:methods"
>  [4] "package:graphics"  "package:grDevices" "package:utils"
>  [7] "package:datasets"  "package:ecoutils"  "package:ecodist"
> [10] "package:stats"     "package:gtkDevice" "Autoloads"
> [13] "package:base"
> > detach(2)
> > search()
>  [1] ".GlobalEnv"        "package:methods"   "package:graphics"
>  [4] "package:grDevices" "package:utils"     "package:datasets"
>  [7] "package:ecoutils"  "package:ecodist"   "package:stats"
> [10] "package:gtkDevice" "Autoloads"         "package:base"
>
> Sarah


From kthul at mac.com  Sat Aug 26 06:35:26 2006
From: kthul at mac.com (Klaus Thul)
Date: Sat, 26 Aug 2006 12:35:26 +0800
Subject: [R] Memory usage decreases drastically after save workspace, quit,
	restart, load workspace
Message-ID: <A2BB1A0E-1232-41EF-A4FB-80A2D6092128@mac.com>

Dear all,

I have the following problem:

  - I have written a program in R which runs out of physical memory  
on my MacBook Pro with 1.5 GB RAM

  - The memory usage is much higher then I would expect from the  
actual data in my global environment

  - When I save the workspace, quit R, restart R and load the  
workspace again, memory usage is much less then before
    (~50 MB instead of ~1.5 GB), although nothing seems to be missing.

  - It doesn't seem to be possible to reduce the amount of memory  
used by calling gc()

  - the behavior is independent of if I use R in command line or from  
GUI, so I can exclude a problem of the Mac GUI

Any suggestions what the problem might be or how I could debug this?

Thanks in advance for any help.

Best regards,
Klaus


platform       i386-apple-darwin8.6.1
arch           i386
os             darwin8.6.1
system         i386, darwin8.6.1
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)


From xprt.wannabe at gmail.com  Sat Aug 26 06:45:50 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Fri, 25 Aug 2006 23:45:50 -0500
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
	<644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>
Message-ID: <a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>

Jim and Patrick,

Both of you made the same suggestion, which works great!

A follow-up question: Suppose I change the condition 'x>2' in 'lapply'
to 'x>4', as follows:

set.seed(123)
tmpf <- function() {
x <- rpois(rpois(1,4),2)
}
n <- 3
m <- replicate(n,tmpf())
m
sub.m <- lapply(m, function(x)x[x>4])  # was x>2

As a result, I'd get:

> sub.m

[[1]]
numeric(0)

[[2]]
[1] 5

[[3]]
numeric(0)

However, what would I need to do such that 'sub.m' contains only the
non-zero length component; namely, the 'sub.m[[2]]'?  In essence, I'd
like to drop all the components of zero length such that 'sub.m'
results in:

[[1]]
[1] 5

My best effort was to use 'lapply' again:

lapply(sub.m, function(x)x[length(x)>0])

which still gives me:

[[1]]
numeric(0)

[[2]]
[1] 5

[[3]]
numeric(0)

Again, any help would be greately appreciated.

p.s. Sorry to bug you again.   I should have thought through a little
more prior to composing an example that would represent all possible
scenarios.

On 8/25/06, jim holtman <jholtman at gmail.com> wrote:
> try this:
>
> > set.seed(123)
> > tmpf <- function() {
> + x <- rpois(rpois(1,4),2)
> + }
> > n <- 3
> > m <- replicate(n,tmpf())
> > m
> [[1]]
> [1] 3 2 4
>
> [[2]]
> [1] 0 2 4 2 2 5 2
>
> [[3]]
> [1] 2 0 4 1 0
>
> > lapply(m, function(x)x[x>2])
> [[1]]
> [1] 3 4
>
> [[2]]
> [1] 4 5
>
> [[3]]
> [1] 4
>
> >
>
> On 8/25/06, xpRt.wannabe <xprt.wannabe at gmail.com> wrote:
> > Dear List,
> >
> > The following code produces a list, which is what I what:
> >
> > set.seed(123)
> > tmpf <- function() {
> > x <- rpois(rpois(1,4),2)
> > }
> > n <- 3
> > m <- replicate(n,tmpf())
> > m
> >
> > [[1]]
> > [1] 3 2 4
> >
> > [[2]]
> > [1] 0 2 4 2 2 5 2
> >
> > [[3]]
> > [1] 2 0 4 1 0
> >
> >
> > Now I need something that would to extract iteratively (or as many
> > times as
> > the size of 'n') the values that are greater than 2 in each component
> > of
> > 'm' into another list such that the sub-list would be:
> >
> > [[1]]
> > [1] 3 4
> >
> > [[2]]
> > [1] 4 5
> >
> > [[3]]
> > [1] 4
> >
> > Below is what I tried:
> >
> > for(i in 1:3)
> > sub.list <- lapply(m,subset,m[[i]]>2)
> >
> > > sub.list
> >
> > which gives me something different from what I want:
> >
> > [[1]]
> > [1] 4
> >
> > [[2]]
> > [1] 4
> >
> > [[3]]
> > [1] 4
> >
> > Any help would be appreciated.
> >
> > > version
> >         _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    2
> > minor    2.1
> > year     2005
> > month    12
> > day      20
> > svn rev  36812
> > language R
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


From xprt.wannabe at gmail.com  Sat Aug 26 07:36:32 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Sat, 26 Aug 2006 00:36:32 -0500
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
	<644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>
	<a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>
Message-ID: <a4fecdd70608252236v6491384ape9962761214a680e@mail.gmail.com>

Yet another question:

Let's say I do the following:

set.seed(123)
tmpf <- function(){
x <- rpois(rpois(1,4),2)
}
n <- 3
m <- replicate(n, tmpf())
m
sub.m <- lapply(m, function(x)x[x>2])

'sub.m' gives me:

[[1]]
[1] 3 4

[[2]]
[1] 4 5

[[3]]
[1] 4

The question is:  What do I need to do such that I can extract
componets of length 2 in 'sub.m' into another sublist, which would
look like this:

[[1]]
[1] 3 4

[[2]]
[1] 4 5

I think that's all the questions I can think of -- for now.

Many, many thanks!!!

On 8/25/06, xpRt. wannabe <xprt.wannabe at gmail.com> wrote:
> Jim and Patrick,
>
> Both of you made the same suggestion, which works great!
>
> A follow-up question: Suppose I change the condition 'x>2' in 'lapply'
> to 'x>4', as follows:
>
> set.seed(123)
> tmpf <- function() {
> x <- rpois(rpois(1,4),2)
> }
> n <- 3
> m <- replicate(n,tmpf())
> m
> sub.m <- lapply(m, function(x)x[x>4])  # was x>2
>
> As a result, I'd get:
>
> > sub.m
>
> [[1]]
> numeric(0)
>
> [[2]]
> [1] 5
>
> [[3]]
> numeric(0)
>
> However, what would I need to do such that 'sub.m' contains only the
> non-zero length component; namely, the 'sub.m[[2]]'?  In essence, I'd
> like to drop all the components of zero length such that 'sub.m'
> results in:
>
> [[1]]
> [1] 5
>
> My best effort was to use 'lapply' again:
>
> lapply(sub.m, function(x)x[length(x)>0])
>
> which still gives me:
>
> [[1]]
> numeric(0)
>
> [[2]]
> [1] 5
>
> [[3]]
> numeric(0)
>
> Again, any help would be greately appreciated.
>
> p.s. Sorry to bug you again.   I should have thought through a little
> more prior to composing an example that would represent all possible
> scenarios.
>
> On 8/25/06, jim holtman <jholtman at gmail.com> wrote:
> > try this:
> >
> > > set.seed(123)
> > > tmpf <- function() {
> > + x <- rpois(rpois(1,4),2)
> > + }
> > > n <- 3
> > > m <- replicate(n,tmpf())
> > > m
> > [[1]]
> > [1] 3 2 4
> >
> > [[2]]
> > [1] 0 2 4 2 2 5 2
> >
> > [[3]]
> > [1] 2 0 4 1 0
> >
> > > lapply(m, function(x)x[x>2])
> > [[1]]
> > [1] 3 4
> >
> > [[2]]
> > [1] 4 5
> >
> > [[3]]
> > [1] 4
> >
> > >
> >
> > On 8/25/06, xpRt.wannabe <xprt.wannabe at gmail.com> wrote:
> > > Dear List,
> > >
> > > The following code produces a list, which is what I what:
> > >
> > > set.seed(123)
> > > tmpf <- function() {
> > > x <- rpois(rpois(1,4),2)
> > > }
> > > n <- 3
> > > m <- replicate(n,tmpf())
> > > m
> > >
> > > [[1]]
> > > [1] 3 2 4
> > >
> > > [[2]]
> > > [1] 0 2 4 2 2 5 2
> > >
> > > [[3]]
> > > [1] 2 0 4 1 0
> > >
> > >
> > > Now I need something that would to extract iteratively (or as many
> > > times as
> > > the size of 'n') the values that are greater than 2 in each component
> > > of
> > > 'm' into another list such that the sub-list would be:
> > >
> > > [[1]]
> > > [1] 3 4
> > >
> > > [[2]]
> > > [1] 4 5
> > >
> > > [[3]]
> > > [1] 4
> > >
> > > Below is what I tried:
> > >
> > > for(i in 1:3)
> > > sub.list <- lapply(m,subset,m[[i]]>2)
> > >
> > > > sub.list
> > >
> > > which gives me something different from what I want:
> > >
> > > [[1]]
> > > [1] 4
> > >
> > > [[2]]
> > > [1] 4
> > >
> > > [[3]]
> > > [1] 4
> > >
> > > Any help would be appreciated.
> > >
> > > > version
> > >         _
> > > platform i386-pc-mingw32
> > > arch     i386
> > > os       mingw32
> > > system   i386, mingw32
> > > status
> > > major    2
> > > minor    2.1
> > > year     2005
> > > month    12
> > > day      20
> > > svn rev  36812
> > > language R
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>


From attenka at utu.fi  Sat Aug 26 08:13:47 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Sat, 26 Aug 2006 09:13:47 +0300
Subject: [R] Modifying the embed-results
In-Reply-To: <971536df0608251825v200418dfxab19e22b36283887@mail.gmail.com>
References: <1156526634.12112.13.camel@attenka>
	<971536df0608251825v200418dfxab19e22b36283887@mail.gmail.com>
Message-ID: <f894d958a06.44f010cb@utu.fi>

Again my example was't very clear: there were not enough same numbers in the VECTOR.
What I need is something like this:

VECTOR<-c(0,3,6,3,11,2,11,4,3,4,7,7,6,4,8)
MATRIX<-c()
for(i in 1:length(VECTOR)){
	v<-(unique(VECTOR[i:length(VECTOR)])[1:5])
	MATRIX<-rbind(MATRIX,v)
}

MATRIX<-na.omit(MATRIX)
MATRIX

> data.frame(MATRIX, row.names=NULL)
  X1 X2 X3 X4 X5
1       0  3  6 11  2
2       3  6 11  2  4
3       6  3 11  2  4
4       3 11  2  4  7
5      11  2  4  3  7
6       2 11  4  3  7
7      11  4  3  7  6
8       4  3  7  6  8
9       3  4  7  6  8

So, there are no duplicates in rows. 
VECTOR is always scanned forward as long as the number of  items (here 5) becomes full.

Atte

> Try:
> 
> embed(VECTOR, 5)[,5:1]
> 
> On 8/25/06, Atte Tenkanen <attenka at utu.fi> wrote:
> > Hi,
> >
> > Here is a vector and the result from the embed-command:
> >
> > VECTOR=c(0,3,6,3,11,2,4,3,7,6,4,5,10,2,3,5,8)
> >
> > > embed(VECTOR, dimension=5)
> >      [,1] [,2] [,3] [,4] [,5]
> >  [1,]   11    3    6    3    0
> >  [2,]    2   11    3    6    3
> >  [3,]    4    2   11    3    6
> >  [4,]    3    4    2   11    3
> >  [5,]    7    3    4    2   11
> >  [6,]    6    7    3    4    2
> >  [7,]    4    6    7    3    4
> >  [8,]    5    4    6    7    3
> >  [9,]   10    5    4    6    7
> > [10,]    2   10    5    4    6
> > [11,]    3    2   10    5    4
> > [12,]    5    3    2   10    5
> > [13,]    8    5    3    2   10
> >
> > Is there a way to little modify the algorithm so that the result 
> looks> like this:
> >
> > [1]  0  3  6 11  2 <- beginning from the first number of the VECTOR
> > [1]  3  6 11  2  4 <- beginning from the second number of the 
> VECTOR etc
> > [1]  6  3 11  2  4
> > [1]  3 11  2  4  7
> > [1] 11  2  4  3  7
> > [1] 2 4 3 7 6
> > [1] 4 3 7 6 5
> > [1] 3 7 6 4 5
> > [1]  7  6  4  5 10
> > [1]  6  4  5 10  2
> > [1]  4  5 10  2  3
> > [1]  5 10  2  3  8
> > [1] 10  2  3  5  8
> >
> > Every row consists of next five unique(!) member of the VECTOR. I 
> made> this example result with a time consuming algorithm which 
> uses for-loops
> > and whiles.
> >
> > How to do this better??
> >
> > Thanks in advance!
> >
> > Atte Tenkanen
> > University of Turku
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html> and provide commented, minimal, self-contained, 
> reproducible code.
> >
>


From stat700004 at yahoo.co.in  Sat Aug 26 09:11:37 2006
From: stat700004 at yahoo.co.in (stat stat)
Date: Sat, 26 Aug 2006 08:11:37 +0100 (BST)
Subject: [R] Problem on Histogtam
Message-ID: <20060826071137.25966.qmail@web7614.mail.in.yahoo.com>

Dear all,

May be  question seems trivial for most of the R
users, but really at least for me, this comes out to
be very problematic. 

Suppose I have the following data:

> r
  [1] -0.0008179960 -0.0277968529 -0.0105731583
-0.0254050262  0.0321847131  0.0328170674
  [7]  0.0431894392 -0.0217614918 -0.0218366946 
0.0048939739 -0.0012212499  0.0032533579
 [13] -0.0081533269 -0.0098725606 -0.0099710008
-0.0130281313  0.0121927094  0.0029209284
 [19] -0.0206279678 -0.0210624593 -0.0100415600
-0.0266801063 -0.0241744954  0.0178453603
 [25]  0.0452204703  0.0133478839  0.0274220937 
0.0091135676  0.0380304730 -0.0087719861
 [31] -0.0412893310 -0.0092243841 -0.0016863410
-0.0080491856 -0.0293504020  0.0411898638
 [37] -0.0037902762 -0.0076239251  0.0193690266
-0.0012518257  0.0259647803  0.0241169245
 [43]  0.0446589449  0.0536073638 -0.0028839241 
0.0168251687  0.0095390230 -0.0213227695
 [49] -0.0519741069 -0.0287872498 -0.0035108287 
0.0197336642 -0.0065347148 -0.0011576308
 [55] -0.0085304899  0.0162228355  0.0087736592 
0.0007593015  0.0113208756  0.0229980462
 [61]  0.0370746078  0.0679558204  0.0401218485
-0.0022225759  0.0188922496 -0.0230330422
 [67] -0.0261938985 -0.0503969590 -0.0069156569
-0.0031277176 -0.0235967983 -0.0100287373
 [73] -0.0014409224 -0.0307491283 -0.0240794177
-0.0091813958  0.0076570053  0.0621042105
 [79]  0.0156475454  0.0353589695  0.0078032627 
0.0070719271  0.0026809668 -0.0111056164
 [85] -0.0088375830  0.0128902735  0.0279180644 
0.0268407382 -0.0038363218 -0.0194055249
 [91] -0.0284881803  0.0016786977 -0.0145297369 
0.0081356381 -0.0187430943  0.0123078477
 [97]  0.0148400700 -0.0070552955  0.0103975446 
0.0109508312  0.0147422817  0.0218728566
[103] -0.0041447533 -0.0096308931  0.0289322358 
0.0216995495  0.0499431039  0.0717506367
[109]  0.0032529169 -0.0040678022 -0.0331522073
-0.0164263110 -0.0322093707  0.0542379373
[115]  0.0259181864  0.0387076215  0.0044415488
-0.0078513881  0.0411792612  0.0062837960
[121] -0.0243481467  0.0328312315 -0.0520127424
-0.0137024542 -0.0367509610 -0.0149753215
[127]  0.0256530410  0.0376798081  0.0106952891 
0.0525638932  0.0281572000  0.0324963843
[133] -0.0170584812 -0.0063837550  0.0294482602
-0.0118152654 -0.0169216729  0.0061436866
[139] -0.0245630568  0.0093716894 -0.0122730454 
0.0234516610 -0.0090283341  0.0059488575
[145] -0.0104937762 -0.0123026519  0.0263488297
-0.0087855249  0.0021441342  0.0204930031
[151] -0.0079588435 -0.0021173987  0.0135674596 
0.0053296384 -0.0004623209  0.0199159610
[157] -0.0034056112 -0.0052445679 -0.0142760886
-0.0158956129 -0.0137605967 -0.0007169316
[163]  0.0236250062  0.0002334540  0.0007000350
-0.0023353584 -0.0210285362 -0.0178275018
[169] -0.0663226429  0.0098217383 -0.0111212836
-0.0597472435  0.0371266051 -0.0247790755
[175]  0.0337833736  0.0265364166 -0.0160458929 
0.0085725944  0.0161645598  0.0700273748
[181] -0.0308458190  0.0362889752 -0.0195430433 
0.0284716741  0.0097766142 -0.0037131628
[187]  0.0275183610  0.0276618128  0.0109410282
-0.0302685208  0.0191074465 -0.0086197904
[193]  0.0013309674  0.0169253408 -0.0010903937 
0.0073913380  0.0330203607  0.0116667990
[199] -0.0099917567  0.0035501760 -0.0231980036 
0.0023441673 -0.0150120769 -0.0165581125
[205] -0.0101568548 -0.0031118051 -0.0051333669 
0.0312800833 -0.0050005540 -0.0109578129
[211] -0.0222825618 -0.0388242167 -0.0037549915
-0.0185102580 -0.0120483385  0.0336106003
[217]  0.0146599484  0.0146757965  0.0117675040 
0.0196041030 -0.0030932415 -0.0024371343
[223] -0.0158751238 -0.0145292000  0.0050182587 
0.0061245513  0.0139234228 -0.0116645558
[229] -0.0475950303 -0.0346688548  0.0041559771
-0.0246987117  0.0195623423  0.0167785091
[235] -0.0133513332 -0.0113051093  0.0215167225
-0.0009680543 -0.0141431277 -0.0150936925
[241] -0.0183676066 -0.0117498159  0.0165629981
-0.0248189789 -0.0133004960 -0.0132137229
[247] -0.0429413799 -0.0055694937  0.0005589715 
0.0168908903  0.0321518982  0.0007973422
[253]  0.0573060001 -0.0078076206 -0.0253502245 
0.0210426035  0.0025361414 -0.0091603694
[259] -0.0064111003 -0.0126862543 -0.0067974118 
0.0023581827 -0.0359703092  0.0185464260
[265] -0.0007992541 -0.0193762790  0.0225694695 
0.0052994295 -0.0039719369  0.0144872809
[271]  0.0002614721 -0.0076105865 -0.0153971717 
0.0056022555 -0.0104292963 -0.0189965007
[277]  0.0079138085  0.0254944708 -0.0165646320
-0.0162958522 -0.0107365106  0.0049696403
[283]  0.0041225838  0.0000000000 -0.0035719230
-0.0200173489 -0.0022490873 -0.0039481157
[289] -0.0182497223  0.0157033919 -0.0154156635
-0.0023041485 -0.0028876717  0.0000000000
[295] -0.0195655241  0.0210104023 -0.0020234145
-0.0026075634  0.0149729259  0.0045623119
[301]  0.0431521368  0.0228966231 -0.0077530130 
0.0285733724 -0.0091707704 -0.0137823544
[307]  0.0092974504 -0.0076974501  0.0106017426 
0.0481245566  0.0164469194  0.0356877294
[313]  0.0002384643  0.0151446340 -0.0184883199 
0.0000000000

Now I want to plot a histogram:

> histo = hist(r,freq=F,breaks=50)
> histo$density
 [1]  1.582278  0.000000  0.000000  0.000000  1.582278
 0.000000  0.000000  1.582278  3.164557
[10]  0.000000  1.582278  0.000000  1.582278  1.582278
 1.582278  1.582278  3.164557  3.164557
[19]  4.746835  4.746835  4.746835 14.240506  6.329114
11.075949 15.822785 12.658228 17.405063
[28] 17.405063 22.151899 25.316456 20.569620  9.493671
34.810127 22.151899 14.240506 14.240506
[37] 17.405063 12.658228 14.240506 12.658228  9.493671
14.240506 14.240506 12.658228  9.493671
[46]  7.911392  7.911392 11.075949  7.911392  1.582278
12.658228  3.164557  6.329114  3.164557
[55]  4.746835  3.164557  3.164557  0.000000  3.164557
 0.000000  3.164557  1.582278  1.582278
[64]  0.000000  0.000000  1.582278  0.000000  1.582278
 0.000000  3.164557

 
Now I want to plot a NORMAL reference plot

> x = rnorm(length(r),mean(r),sd(r))
> curve(dnorm(x), add=T, col="red")

But I am not getting what it should be. I get an
straight line. Can anyone give me any suggestion?

Also please let me know how R calculate "density" in
"Hist" function

Sincerely yours,
Stat
 

thanks in advance


From ripley at stats.ox.ac.uk  Sat Aug 26 10:41:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Aug 2006 09:41:00 +0100 (BST)
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44EFA791.5030703@bu.edu>
References: <44EE13B6.20205@bu.edu> <44EE1F34.6010400@pdf.com>
	<44EFA791.5030703@bu.edu>
Message-ID: <Pine.LNX.4.64.0608260919310.28706@gannet.stats.ox.ac.uk>

On Fri, 25 Aug 2006, Maria Montez wrote:

> Thank you for your answers yesterday. I now have another question!
> 
> Suppose that instead of creating a formula for a regression model I 
> simply wanted to add the variables. I believe I cannot use the 
> as.formula anymore. Again I tried to use expression to no avail. I get 
> an expression but I can't use it.
> 
> fit.sum <- function(x) {
>     fo <- expression(paste(x, collapse = "+"))
>    eval( fo)
> }
> fit.sum(c("x1","x2"))
> 
> Basically what I need is to learn how to use variables when what is 
> given to me are their names (character list).

I think we do need to tell you that parse(text=) is how to turn a 
character vector into an expression, although it is rarely a good way (see 
the fortune in the 'fortunes' package about it) and you need to be careful 
where you evaluate:

fit.sum <- function(x) eval.parent(parse(text=paste(x, collapse = "+")))

As an alternative, the answer to

> Basically what I need is to learn how to use variables when what is
> given to me are their names (character list).

is most often 'get'.  So for two objects

fit.sum2 <- function(x) {
   env <- parent.frame()
   do.call("+", lapply(x, get, envir=env))
}

but again you need to be careful where you get() from.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sat Aug 26 10:55:25 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Aug 2006 09:55:25 +0100 (BST)
Subject: [R] Memory usage decreases drastically after save workspace,
 quit, restart, load workspace
In-Reply-To: <A2BB1A0E-1232-41EF-A4FB-80A2D6092128@mac.com>
References: <A2BB1A0E-1232-41EF-A4FB-80A2D6092128@mac.com>
Message-ID: <Pine.LNX.4.64.0608260924570.28706@gannet.stats.ox.ac.uk>

On Sat, 26 Aug 2006, Klaus Thul wrote:

> Dear all,
> 
> I have the following problem:
> 
>   - I have written a program in R which runs out of physical memory  
> on my MacBook Pro with 1.5 GB RAM

How does R know about physical memory on a virtual-memory OS?  I presume 
the symptom is swapping by your OS, but how do you attribute that to R?

>   - The memory usage is much higher then I would expect from the  
> actual data in my global environment
> 
>   - When I save the workspace, quit R, restart R and load the  
> workspace again, memory usage is much less then before
>     (~50 MB instead of ~1.5 GB), although nothing seems to be missing.
> 
>   - It doesn't seem to be possible to reduce the amount of memory  
> used by calling gc()
> 
>   - the behavior is independent of if I use R in command line or from  
> GUI, so I can exclude a problem of the Mac GUI
> 
> Any suggestions what the problem might be or how I could debug this?

How are you measuring memory usage?

If this is by gc(), note that lazyloading is one-way, and memory increases 
after you use a lazyloaded object in a session.  This can be dramatic 
where datasets are involved.

If you are asking the OS about memory usage, return of memory to the OS is 
a rather haphazard and OS-specific issue.  On some OSes virtual memory is 
not actually reclaimed from applications until it is needed (or ever).

It is certainly possible that something you are using has a memory leak, 
but valgrind has been used to plug those on the most-used parts of R.


I at least need more precise indications of what you observed to be able 
to comment more.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pburns at pburns.seanet.com  Sat Aug 26 10:57:17 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 26 Aug 2006 09:57:17 +0100
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <a4fecdd70608252236v6491384ape9962761214a680e@mail.gmail.com>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>	<644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>	<a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>
	<a4fecdd70608252236v6491384ape9962761214a680e@mail.gmail.com>
Message-ID: <44F00CED.9000700@pburns.seanet.com>

 > sub.m <- lapply(m, function(x)x[x>2])
 > sub.m
[[1]]
[1] 3 4

[[2]]
[1] 4 5

[[3]]
[1] 4

 > sub.m[unlist(lapply(sub.m, function(x) length(x) == 2))]
[[1]]
[1] 3 4

[[2]]
[1] 4 5

 > sub4.m <- lapply(m, function(x)x[x>4])
 > sub4.m[unlist(lapply(sub4.m, function(x) length(x) > 0))]
[[1]]
[1] 5


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

xpRt.wannabe wrote:

>Yet another question:
>
>Let's say I do the following:
>
>set.seed(123)
>tmpf <- function(){
>x <- rpois(rpois(1,4),2)
>}
>n <- 3
>m <- replicate(n, tmpf())
>m
>sub.m <- lapply(m, function(x)x[x>2])
>
>'sub.m' gives me:
>
>[[1]]
>[1] 3 4
>
>[[2]]
>[1] 4 5
>
>[[3]]
>[1] 4
>
>The question is:  What do I need to do such that I can extract
>componets of length 2 in 'sub.m' into another sublist, which would
>look like this:
>
>[[1]]
>[1] 3 4
>
>[[2]]
>[1] 4 5
>
>I think that's all the questions I can think of -- for now.
>
>Many, many thanks!!!
>
>On 8/25/06, xpRt. wannabe <xprt.wannabe at gmail.com> wrote:
>  
>
>>Jim and Patrick,
>>
>>Both of you made the same suggestion, which works great!
>>
>>A follow-up question: Suppose I change the condition 'x>2' in 'lapply'
>>to 'x>4', as follows:
>>
>>set.seed(123)
>>tmpf <- function() {
>>x <- rpois(rpois(1,4),2)
>>}
>>n <- 3
>>m <- replicate(n,tmpf())
>>m
>>sub.m <- lapply(m, function(x)x[x>4])  # was x>2
>>
>>As a result, I'd get:
>>
>>    
>>
>>>sub.m
>>>      
>>>
>>[[1]]
>>numeric(0)
>>
>>[[2]]
>>[1] 5
>>
>>[[3]]
>>numeric(0)
>>
>>However, what would I need to do such that 'sub.m' contains only the
>>non-zero length component; namely, the 'sub.m[[2]]'?  In essence, I'd
>>like to drop all the components of zero length such that 'sub.m'
>>results in:
>>
>>[[1]]
>>[1] 5
>>
>>My best effort was to use 'lapply' again:
>>
>>lapply(sub.m, function(x)x[length(x)>0])
>>
>>which still gives me:
>>
>>[[1]]
>>numeric(0)
>>
>>[[2]]
>>[1] 5
>>
>>[[3]]
>>numeric(0)
>>
>>Again, any help would be greately appreciated.
>>
>>p.s. Sorry to bug you again.   I should have thought through a little
>>more prior to composing an example that would represent all possible
>>scenarios.
>>
>>On 8/25/06, jim holtman <jholtman at gmail.com> wrote:
>>    
>>
>>>try this:
>>>
>>>      
>>>
>>>>set.seed(123)
>>>>tmpf <- function() {
>>>>        
>>>>
>>>+ x <- rpois(rpois(1,4),2)
>>>+ }
>>>      
>>>
>>>>n <- 3
>>>>m <- replicate(n,tmpf())
>>>>m
>>>>        
>>>>
>>>[[1]]
>>>[1] 3 2 4
>>>
>>>[[2]]
>>>[1] 0 2 4 2 2 5 2
>>>
>>>[[3]]
>>>[1] 2 0 4 1 0
>>>
>>>      
>>>
>>>>lapply(m, function(x)x[x>2])
>>>>        
>>>>
>>>[[1]]
>>>[1] 3 4
>>>
>>>[[2]]
>>>[1] 4 5
>>>
>>>[[3]]
>>>[1] 4
>>>
>>>      
>>>
>>>On 8/25/06, xpRt.wannabe <xprt.wannabe at gmail.com> wrote:
>>>      
>>>
>>>>Dear List,
>>>>
>>>>The following code produces a list, which is what I what:
>>>>
>>>>set.seed(123)
>>>>tmpf <- function() {
>>>>x <- rpois(rpois(1,4),2)
>>>>}
>>>>n <- 3
>>>>m <- replicate(n,tmpf())
>>>>m
>>>>
>>>>[[1]]
>>>>[1] 3 2 4
>>>>
>>>>[[2]]
>>>>[1] 0 2 4 2 2 5 2
>>>>
>>>>[[3]]
>>>>[1] 2 0 4 1 0
>>>>
>>>>
>>>>Now I need something that would to extract iteratively (or as many
>>>>times as
>>>>the size of 'n') the values that are greater than 2 in each component
>>>>of
>>>>'m' into another list such that the sub-list would be:
>>>>
>>>>[[1]]
>>>>[1] 3 4
>>>>
>>>>[[2]]
>>>>[1] 4 5
>>>>
>>>>[[3]]
>>>>[1] 4
>>>>
>>>>Below is what I tried:
>>>>
>>>>for(i in 1:3)
>>>>sub.list <- lapply(m,subset,m[[i]]>2)
>>>>
>>>>        
>>>>
>>>>>sub.list
>>>>>          
>>>>>
>>>>which gives me something different from what I want:
>>>>
>>>>[[1]]
>>>>[1] 4
>>>>
>>>>[[2]]
>>>>[1] 4
>>>>
>>>>[[3]]
>>>>[1] 4
>>>>
>>>>Any help would be appreciated.
>>>>
>>>>        
>>>>
>>>>>version
>>>>>          
>>>>>
>>>>        _
>>>>platform i386-pc-mingw32
>>>>arch     i386
>>>>os       mingw32
>>>>system   i386, mingw32
>>>>status
>>>>major    2
>>>>minor    2.1
>>>>year     2005
>>>>month    12
>>>>day      20
>>>>svn rev  36812
>>>>language R
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>        
>>>>
>>>--
>>>Jim Holtman
>>>Cincinnati, OH
>>>+1 513 646 9390
>>>
>>>What is the problem you are trying to solve?
>>>
>>>      
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From Friedrich.Leisch at stat.uni-muenchen.de  Sat Aug 26 11:57:39 2006
From: Friedrich.Leisch at stat.uni-muenchen.de (Friedrich Leisch)
Date: Sat, 26 Aug 2006 11:57:39 +0200
Subject: [R] issues with Sweave and inclusion of graphics in a document
In-Reply-To: <20060825180548.39228.qmail@web30213.mail.mud.yahoo.com>
References: <Pine.LNX.4.64.0608200706380.6643@gannet.stats.ox.ac.uk>
	<20060825180548.39228.qmail@web30213.mail.mud.yahoo.com>
Message-ID: <17648.6931.881206.437803@celebrian.ci.tuwien.ac.at>

>>>>> On Fri, 25 Aug 2006 11:05:48 -0700 (PDT),
>>>>> Thomas Harte (TH) wrote:

  > --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
  >> savePlot is just an internal version of dev.copy, part of the support for 
  >> the menus on the windows() graphics device.
  >> 
  >> It is described in `An Introduction to R' (the most basic R manual).

  > "the most basic R manual" doesn't quite answer my question. 

  > by itself, dev.copy doesn't copy the width and height of the device whereas savePlot
  > copies whatever is displayed on the screen giving me what-you-see-is-what-you-save
  > capabilities (but only under the Windows OS). 

  > i can get pretty close to this in linux by writing a function to save the
  > plot to a pdf device:
  > <<label=first.ar.1, results=hide>>=
  > 	# no savePlot in Linux ... so write my own function
  > 	savePlotAsPdf<- function(pdfname, from=dev.cur()) {
  > 		from<- from
  > 		pdf(pdfname, width=width, height=height)
  > 		to<- dev.cur()
  > 		dev.set(from)
  > 		dev.copy(which=to)
  > 		dev.off()
  > 	}
  > 	# a long AR process is best viewed in a wide window ... 
  > 	# width & height are now variables
  > 	width<- 20; height<- 5
  > 	x11(width=width, height=height)
  > 	sp<- make.ar.1(alpha=.5, n=800)
  > 	plot(sp, type="l", col="blue")
  > 	# width & height via dynamic scoping in savePlotAsPdf
  > 	savePlotAsPdf("ar.pdf")
  > @

Umm, maybe I don't get your point, but in what way does the complicated
code above do anythingdifferent from

**********************************************************

<<results=hide>>=
sp<- make.ar.1(alpha=.5, n=800)
@

<<ar,fig=true,width=20,height=5,include=false>>=
plot(sp, type="l", col="blue")
@

\begin{figure}
  \includegraphics[width=14.5cm]{myprefix-ar}
\end{figure}

**********************************************************

???

Best,

-- 
-----------------------------------------------------------------------
Prof. Dr. Friedrich Leisch 

Institut f?r Statistik                          Tel: (+49 89) 2180 3165
Ludwig-Maximilians-Universit?t                  Fax: (+49 89) 2180 5308
Ludwigstra?e 33
D-80539 M?nchen                 http://www.stat.uni-muenchen.de/~leisch


From jholtman at gmail.com  Sat Aug 26 14:13:05 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 26 Aug 2006 08:13:05 -0400
Subject: [R] Problem on Histogtam
In-Reply-To: <20060826071137.25966.qmail@web7614.mail.in.yahoo.com>
References: <20060826071137.25966.qmail@web7614.mail.in.yahoo.com>
Message-ID: <644e1f320608260513sbe511bct2ac0f362d7f152f9@mail.gmail.com>

try thie:

r <- rnorm(100,4,1)
hist(r, freq=FALSE)
r.f <- function(x)dnorm(x, mean(r), sd(r))
curve(r.f, from=min(r), to=max(r), add=TRUE, col='red')


On 8/26/06, stat stat <stat700004 at yahoo.co.in> wrote:
> Dear all,
>
> May be  question seems trivial for most of the R
> users, but really at least for me, this comes out to
> be very problematic.
>
> Suppose I have the following data:
>
> > r
>  [1] -0.0008179960 -0.0277968529 -0.0105731583
> -0.0254050262  0.0321847131  0.0328170674
>  [7]  0.0431894392 -0.0217614918 -0.0218366946
> 0.0048939739 -0.0012212499  0.0032533579
>  [13] -0.0081533269 -0.0098725606 -0.0099710008
> -0.0130281313  0.0121927094  0.0029209284
>  [19] -0.0206279678 -0.0210624593 -0.0100415600
> -0.0266801063 -0.0241744954  0.0178453603
>  [25]  0.0452204703  0.0133478839  0.0274220937
> 0.0091135676  0.0380304730 -0.0087719861
>  [31] -0.0412893310 -0.0092243841 -0.0016863410
> -0.0080491856 -0.0293504020  0.0411898638
>  [37] -0.0037902762 -0.0076239251  0.0193690266
> -0.0012518257  0.0259647803  0.0241169245
>  [43]  0.0446589449  0.0536073638 -0.0028839241
> 0.0168251687  0.0095390230 -0.0213227695
>  [49] -0.0519741069 -0.0287872498 -0.0035108287
> 0.0197336642 -0.0065347148 -0.0011576308
>  [55] -0.0085304899  0.0162228355  0.0087736592
> 0.0007593015  0.0113208756  0.0229980462
>  [61]  0.0370746078  0.0679558204  0.0401218485
> -0.0022225759  0.0188922496 -0.0230330422
>  [67] -0.0261938985 -0.0503969590 -0.0069156569
> -0.0031277176 -0.0235967983 -0.0100287373
>  [73] -0.0014409224 -0.0307491283 -0.0240794177
> -0.0091813958  0.0076570053  0.0621042105
>  [79]  0.0156475454  0.0353589695  0.0078032627
> 0.0070719271  0.0026809668 -0.0111056164
>  [85] -0.0088375830  0.0128902735  0.0279180644
> 0.0268407382 -0.0038363218 -0.0194055249
>  [91] -0.0284881803  0.0016786977 -0.0145297369
> 0.0081356381 -0.0187430943  0.0123078477
>  [97]  0.0148400700 -0.0070552955  0.0103975446
> 0.0109508312  0.0147422817  0.0218728566
> [103] -0.0041447533 -0.0096308931  0.0289322358
> 0.0216995495  0.0499431039  0.0717506367
> [109]  0.0032529169 -0.0040678022 -0.0331522073
> -0.0164263110 -0.0322093707  0.0542379373
> [115]  0.0259181864  0.0387076215  0.0044415488
> -0.0078513881  0.0411792612  0.0062837960
> [121] -0.0243481467  0.0328312315 -0.0520127424
> -0.0137024542 -0.0367509610 -0.0149753215
> [127]  0.0256530410  0.0376798081  0.0106952891
> 0.0525638932  0.0281572000  0.0324963843
> [133] -0.0170584812 -0.0063837550  0.0294482602
> -0.0118152654 -0.0169216729  0.0061436866
> [139] -0.0245630568  0.0093716894 -0.0122730454
> 0.0234516610 -0.0090283341  0.0059488575
> [145] -0.0104937762 -0.0123026519  0.0263488297
> -0.0087855249  0.0021441342  0.0204930031
> [151] -0.0079588435 -0.0021173987  0.0135674596
> 0.0053296384 -0.0004623209  0.0199159610
> [157] -0.0034056112 -0.0052445679 -0.0142760886
> -0.0158956129 -0.0137605967 -0.0007169316
> [163]  0.0236250062  0.0002334540  0.0007000350
> -0.0023353584 -0.0210285362 -0.0178275018
> [169] -0.0663226429  0.0098217383 -0.0111212836
> -0.0597472435  0.0371266051 -0.0247790755
> [175]  0.0337833736  0.0265364166 -0.0160458929
> 0.0085725944  0.0161645598  0.0700273748
> [181] -0.0308458190  0.0362889752 -0.0195430433
> 0.0284716741  0.0097766142 -0.0037131628
> [187]  0.0275183610  0.0276618128  0.0109410282
> -0.0302685208  0.0191074465 -0.0086197904
> [193]  0.0013309674  0.0169253408 -0.0010903937
> 0.0073913380  0.0330203607  0.0116667990
> [199] -0.0099917567  0.0035501760 -0.0231980036
> 0.0023441673 -0.0150120769 -0.0165581125
> [205] -0.0101568548 -0.0031118051 -0.0051333669
> 0.0312800833 -0.0050005540 -0.0109578129
> [211] -0.0222825618 -0.0388242167 -0.0037549915
> -0.0185102580 -0.0120483385  0.0336106003
> [217]  0.0146599484  0.0146757965  0.0117675040
> 0.0196041030 -0.0030932415 -0.0024371343
> [223] -0.0158751238 -0.0145292000  0.0050182587
> 0.0061245513  0.0139234228 -0.0116645558
> [229] -0.0475950303 -0.0346688548  0.0041559771
> -0.0246987117  0.0195623423  0.0167785091
> [235] -0.0133513332 -0.0113051093  0.0215167225
> -0.0009680543 -0.0141431277 -0.0150936925
> [241] -0.0183676066 -0.0117498159  0.0165629981
> -0.0248189789 -0.0133004960 -0.0132137229
> [247] -0.0429413799 -0.0055694937  0.0005589715
> 0.0168908903  0.0321518982  0.0007973422
> [253]  0.0573060001 -0.0078076206 -0.0253502245
> 0.0210426035  0.0025361414 -0.0091603694
> [259] -0.0064111003 -0.0126862543 -0.0067974118
> 0.0023581827 -0.0359703092  0.0185464260
> [265] -0.0007992541 -0.0193762790  0.0225694695
> 0.0052994295 -0.0039719369  0.0144872809
> [271]  0.0002614721 -0.0076105865 -0.0153971717
> 0.0056022555 -0.0104292963 -0.0189965007
> [277]  0.0079138085  0.0254944708 -0.0165646320
> -0.0162958522 -0.0107365106  0.0049696403
> [283]  0.0041225838  0.0000000000 -0.0035719230
> -0.0200173489 -0.0022490873 -0.0039481157
> [289] -0.0182497223  0.0157033919 -0.0154156635
> -0.0023041485 -0.0028876717  0.0000000000
> [295] -0.0195655241  0.0210104023 -0.0020234145
> -0.0026075634  0.0149729259  0.0045623119
> [301]  0.0431521368  0.0228966231 -0.0077530130
> 0.0285733724 -0.0091707704 -0.0137823544
> [307]  0.0092974504 -0.0076974501  0.0106017426
> 0.0481245566  0.0164469194  0.0356877294
> [313]  0.0002384643  0.0151446340 -0.0184883199
> 0.0000000000
>
> Now I want to plot a histogram:
>
> > histo = hist(r,freq=F,breaks=50)
> > histo$density
>  [1]  1.582278  0.000000  0.000000  0.000000  1.582278
>  0.000000  0.000000  1.582278  3.164557
> [10]  0.000000  1.582278  0.000000  1.582278  1.582278
>  1.582278  1.582278  3.164557  3.164557
> [19]  4.746835  4.746835  4.746835 14.240506  6.329114
> 11.075949 15.822785 12.658228 17.405063
> [28] 17.405063 22.151899 25.316456 20.569620  9.493671
> 34.810127 22.151899 14.240506 14.240506
> [37] 17.405063 12.658228 14.240506 12.658228  9.493671
> 14.240506 14.240506 12.658228  9.493671
> [46]  7.911392  7.911392 11.075949  7.911392  1.582278
> 12.658228  3.164557  6.329114  3.164557
> [55]  4.746835  3.164557  3.164557  0.000000  3.164557
>  0.000000  3.164557  1.582278  1.582278
> [64]  0.000000  0.000000  1.582278  0.000000  1.582278
>  0.000000  3.164557
>
>
> Now I want to plot a NORMAL reference plot
>
> > x = rnorm(length(r),mean(r),sd(r))
> > curve(dnorm(x), add=T, col="red")
>
> But I am not getting what it should be. I get an
> straight line. Can anyone give me any suggestion?
>
> Also please let me know how R calculate "density" in
> "Hist" function
>
> Sincerely yours,
> Stat
>
>
> thanks in advance
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Sat Aug 26 14:24:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Aug 2006 08:24:10 -0400
Subject: [R] Modifying the embed-results
In-Reply-To: <f894d958a06.44f010cb@utu.fi>
References: <1156526634.12112.13.camel@attenka>
	<971536df0608251825v200418dfxab19e22b36283887@mail.gmail.com>
	<f894d958a06.44f010cb@utu.fi>
Message-ID: <971536df0608260524w5a718f4fk11e696946ce2a905@mail.gmail.com>

You can replace the for with lapply like this:

VECTOR <- c(0, 3, 6, 3, 11, 2, 11, 4, 3, 4, 7, 7, 6, 4, 8)
f <- function(i) unique(tail(VECTOR, length(VECTOR)-i+1))[1:5]
out <- do.call(rbind, lapply(seq(along = VECTOR), f))
na.omit(rbind(rep(NA, 5), out))

Note that  a matrix with zero rows is returned in the case
that VECTOR has zero length and in the case that VECTOR
has fewer than 5 unique elements.


On 8/26/06, Atte Tenkanen <attenka at utu.fi> wrote:
> Again my example was't very clear: there were not enough same numbers in the VECTOR.
> What I need is something like this:
>
> VECTOR<-c(0,3,6,3,11,2,11,4,3,4,7,7,6,4,8)
> MATRIX<-c()
> for(i in 1:length(VECTOR)){
>        v<-(unique(VECTOR[i:length(VECTOR)])[1:5])
>        MATRIX<-rbind(MATRIX,v)
> }
>
> MATRIX<-na.omit(MATRIX)
> MATRIX
>
> > data.frame(MATRIX, row.names=NULL)
>  X1 X2 X3 X4 X5
> 1       0  3  6 11  2
> 2       3  6 11  2  4
> 3       6  3 11  2  4
> 4       3 11  2  4  7
> 5      11  2  4  3  7
> 6       2 11  4  3  7
> 7      11  4  3  7  6
> 8       4  3  7  6  8
> 9       3  4  7  6  8
>
> So, there are no duplicates in rows.
> VECTOR is always scanned forward as long as the number of  items (here 5) becomes full.
>
> Atte
>
> > Try:
> >
> > embed(VECTOR, 5)[,5:1]
> >
> > On 8/25/06, Atte Tenkanen <attenka at utu.fi> wrote:
> > > Hi,
> > >
> > > Here is a vector and the result from the embed-command:
> > >
> > > VECTOR=c(0,3,6,3,11,2,4,3,7,6,4,5,10,2,3,5,8)
> > >
> > > > embed(VECTOR, dimension=5)
> > >      [,1] [,2] [,3] [,4] [,5]
> > >  [1,]   11    3    6    3    0
> > >  [2,]    2   11    3    6    3
> > >  [3,]    4    2   11    3    6
> > >  [4,]    3    4    2   11    3
> > >  [5,]    7    3    4    2   11
> > >  [6,]    6    7    3    4    2
> > >  [7,]    4    6    7    3    4
> > >  [8,]    5    4    6    7    3
> > >  [9,]   10    5    4    6    7
> > > [10,]    2   10    5    4    6
> > > [11,]    3    2   10    5    4
> > > [12,]    5    3    2   10    5
> > > [13,]    8    5    3    2   10
> > >
> > > Is there a way to little modify the algorithm so that the result
> > looks> like this:
> > >
> > > [1]  0  3  6 11  2 <- beginning from the first number of the VECTOR
> > > [1]  3  6 11  2  4 <- beginning from the second number of the
> > VECTOR etc
> > > [1]  6  3 11  2  4
> > > [1]  3 11  2  4  7
> > > [1] 11  2  4  3  7
> > > [1] 2 4 3 7 6
> > > [1] 4 3 7 6 5
> > > [1] 3 7 6 4 5
> > > [1]  7  6  4  5 10
> > > [1]  6  4  5 10  2
> > > [1]  4  5 10  2  3
> > > [1]  5 10  2  3  8
> > > [1] 10  2  3  5  8
> > >
> > > Every row consists of next five unique(!) member of the VECTOR. I
> > made> this example result with a time consuming algorithm which
> > uses for-loops
> > > and whiles.
> > >
> > > How to do this better??
> > >
> > > Thanks in advance!
> > >
> > > Atte Tenkanen
> > > University of Turku
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html> and provide commented, minimal, self-contained,
> > reproducible code.
> > >
> >
>


From thomas.harte at yahoo.com  Sat Aug 26 14:38:58 2006
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Sat, 26 Aug 2006 05:38:58 -0700 (PDT)
Subject: [R] issues with Sweave and inclusion of graphics in a document
In-Reply-To: <17648.6931.881206.437803@celebrian.ci.tuwien.ac.at>
Message-ID: <20060826123858.66620.qmail@web30202.mail.mud.yahoo.com>

--- Friedrich Leisch <Friedrich.Leisch at stat.uni-muenchen.de> wrote:

> >>>>> On Fri, 25 Aug 2006 11:05:48 -0700 (PDT),
> >>>>> Thomas Harte (TH) wrote:
>   > i can get pretty close to this in linux by writing a function to save the
>   > plot to a pdf device:
>   > <<label=first.ar.1, results=hide>>=
>   > 	# no savePlot in Linux ... so write my own function
>   > 	savePlotAsPdf<- function(pdfname, from=dev.cur()) {
>   > 		from<- from
>   > 		pdf(pdfname, width=width, height=height)
>   > 		to<- dev.cur()
>   > 		dev.set(from)
>   > 		dev.copy(which=to)
>   > 		dev.off()
>   > 	}
>   > 	# a long AR process is best viewed in a wide window ... 
>   > 	# width & height are now variables
>   > 	width<- 20; height<- 5
>   > 	x11(width=width, height=height)
>   > 	sp<- make.ar.1(alpha=.5, n=800)
>   > 	plot(sp, type="l", col="blue")
>   > 	# width & height via dynamic scoping in savePlotAsPdf
>   > 	savePlotAsPdf("ar.pdf")
>   > @
> 
> Umm, maybe I don't get your point, but in what way does the complicated
> code above do anythingdifferent from
> 
> **********************************************************
> 
> <<results=hide>>=
> sp<- make.ar.1(alpha=.5, n=800)
> @
> 
> <<ar,fig=true,width=20,height=5,include=false>>=
> plot(sp, type="l", col="blue")
> @
> 
> \begin{figure}
>   \includegraphics[width=14.5cm]{myprefix-ar}
> \end{figure}
> 
> **********************************************************

hallo, friedrich, and thanks for your reply.

if i Stangle your code i get:

        sp<- make.ar.1(alpha=.5, n=800)
        plot(sp, type="l", col="blue")

whereas if i Stangle my code, i get:

        width<- 20; height<- 5
        x11(width=width, height=height)
        sp<- make.ar.1(alpha=.5, n=800)
        plot(sp, type="l", col="blue")
        savePlotAsPdf("ar.pdf")

this, i think, is the problem in a nutshell.

i want my R code to look like this because i (often) want to open a separate
device (for example, to have specific dimensions as above) and i may wish to 
save its contents. on the other hand, i may not wish to save a device's contents: 
i may wish to open several devices for comparison purposes and i may wish to save only
certain devices, in keeping with the Sweave options echo=false and results=hide.

if i use your, admittedly uncomplicated, code chunk, then i am limited to seeing the
output in the final document. i can't cut and paste the code chunk into an R session and
see the results as i wish to see them before saving. 

currently for each project i work on, i usually end up with a very lengthy .R file with
all the code (and embedded mathematical annotation in LaTeX), and then i have to write a
set of notes or a report in LaTeX duplicating or expanding on the mathematics. i'm hoping
to supplant this, rather inefficient, method with a single .Rnw file and the use of
Sweave. this, i believe, is what Sweave was designed to do. but i do wish to write my R
code (warts and all) the way that i want to in the .Rnw file and only include the output
(graphics or text) of certain parts of that code in the document. i hope that seems
reasonable.

perhaps i was not clear, or succinct enough, in my original post
(i tried to get the point across by providing example .Rnw files,
but i'm aware that it's asking a lot to wade through a lengthy
example to retrieve a point).

thanks again for your reply.

cheers,

thomas.


From s.ruegg at access.unizh.ch  Sat Aug 26 14:47:16 2006
From: s.ruegg at access.unizh.ch (Simon Ruegg)
Date: Sat, 26 Aug 2006 14:47:16 +0200
Subject: [R] problems with loop
Message-ID: <200608261247.k7QClIgn000395@idmailgate2.unizh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060826/79795aa2/attachment.pl 

From arun.kumar.saha at gmail.com  Sat Aug 26 14:56:12 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Sat, 26 Aug 2006 18:26:12 +0530
Subject: [R] Adding a footnote in plot-window in R
Message-ID: <d4c57560608260556m3e8a3511y643ebf834cd263ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060826/e9c7defa/attachment.pl 

From rolf at erdos.math.unb.ca  Sat Aug 26 15:02:19 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Sat, 26 Aug 2006 10:02:19 -0300 (ADT)
Subject: [R] Adding a footnote in plot-window in R
Message-ID: <200608261302.k7QD2JiE021859@erdos.math.unb.ca>

?legend


From Friedrich.Leisch at stat.uni-muenchen.de  Sat Aug 26 15:05:47 2006
From: Friedrich.Leisch at stat.uni-muenchen.de (Friedrich Leisch)
Date: Sat, 26 Aug 2006 15:05:47 +0200
Subject: [R] issues with Sweave and inclusion of graphics in a document
In-Reply-To: <20060826123858.66620.qmail@web30202.mail.mud.yahoo.com>
References: <17648.6931.881206.437803@celebrian.ci.tuwien.ac.at>
	<20060826123858.66620.qmail@web30202.mail.mud.yahoo.com>
Message-ID: <17648.18219.150226.240250@celebrian.ci.tuwien.ac.at>

>>>>> On Sat, 26 Aug 2006 05:38:58 -0700 (PDT),
>>>>> Thomas Harte (TH) wrote:


  > hallo, friedrich, and thanks for your reply.

  > if i Stangle your code i get:

  >         sp<- make.ar.1(alpha=.5, n=800)
  >         plot(sp, type="l", col="blue")

  > whereas if i Stangle my code, i get:

  >         width<- 20; height<- 5
  >         x11(width=width, height=height)
  >         sp<- make.ar.1(alpha=.5, n=800)
  >         plot(sp, type="l", col="blue")
  >         savePlotAsPdf("ar.pdf")

  > this, i think, is the problem in a nutshell.

  > i want my R code to look like this because i (often) want to open
  > a separate device (for example, to have specific dimensions as
  > above) and i may wish to save its contents. on the other hand, i
  > may not wish to save a device's contents: i may wish to open
  > several devices for comparison purposes and i may wish to save
  > only certain devices, in keeping with the Sweave options
  > echo=false and results=hide.

  > if i use your, admittedly uncomplicated, code chunk, then i am
  > limited to seeing the output in the final document. i can't cut
  > and paste the code chunk into an R session and see the results as
  > i wish to see them before saving.

  > currently for each project i work on, i usually end up with a very
  > lengthy .R file with all the code (and embedded mathematical
  > annotation in LaTeX), and then i have to write a set of notes or a
  > report in LaTeX duplicating or expanding on the mathematics. i'm
  > hoping to supplant this, rather inefficient, method with a single
  > .Rnw file and the use of Sweave. this, i believe, is what Sweave
  > was designed to do. but i do wish to write my R code (warts and
  > all) the way that i want to in the .Rnw file and only include the
  > output (graphics or text) of certain parts of that code in the
  > document. i hope that seems reasonable.

  > perhaps i was not clear, or succinct enough, in my original post
  > (i tried to get the point across by providing example .Rnw files,
  > but i'm aware that it's asking a lot to wade through a lengthy
  > example to retrieve a point).

OK, no I think I understand what you want: running the *tangled* code
should open windows with the same height/width ratio as in the Latex
document, right? Your first posting suggested to me more the other way
round ...

No, what you want is currently not directly possible with Sweave, you
will have to use workarounds like the one you posted. You can even
easily write your own tangle function doing it automatically (Rtangle
is really trivial can be easily adjusted to ones personal needs).

Best,
Fritz


From mark.lyman at gmail.com  Sat Aug 26 17:32:39 2006
From: mark.lyman at gmail.com (Mark Lyman)
Date: Sat, 26 Aug 2006 15:32:39 +0000 (UTC)
Subject: [R] How to iteratively extract elements out of a list
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
Message-ID: <loom.20060826T173027-995@post.gmane.org>

lapply(m,function(x)x[x>2])
[[1]]
[1] 3 4

[[2]]
[1] 4 5

[[3]]
[1] 4


From pushkar.here at gmail.com  Sat Aug 26 13:48:14 2006
From: pushkar.here at gmail.com (Pushkar Kumar)
Date: Sat, 26 Aug 2006 04:48:14 -0700
Subject: [R] Implementing EM Algorithm in R!
Message-ID: <4965b4cd0608260448t5d88a632lb177e3af859d2767@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060826/cfaa9346/attachment.pl 

From xprt.wannabe at gmail.com  Sat Aug 26 18:37:32 2006
From: xprt.wannabe at gmail.com (xpRt.wannabe)
Date: Sat, 26 Aug 2006 11:37:32 -0500
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <44F00CED.9000700@pburns.seanet.com>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
	<644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>
	<a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>
	<a4fecdd70608252236v6491384ape9962761214a680e@mail.gmail.com>
	<44F00CED.9000700@pburns.seanet.com>
Message-ID: <a4fecdd70608260937g76f781cfu6bb347b7bb0b510b@mail.gmail.com>

Thank you!

On 8/26/06, Patrick Burns <pburns at pburns.seanet.com> wrote:
>  > sub.m <- lapply(m, function(x)x[x>2])
>  > sub.m
> [[1]]
> [1] 3 4
>
> [[2]]
> [1] 4 5
>
> [[3]]
> [1] 4
>
>  > sub.m[unlist(lapply(sub.m, function(x) length(x) == 2))]
> [[1]]
> [1] 3 4
>
> [[2]]
> [1] 4 5
>
>  > sub4.m <- lapply(m, function(x)x[x>4])
>  > sub4.m[unlist(lapply(sub4.m, function(x) length(x) > 0))]
> [[1]]
> [1] 5
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> xpRt.wannabe wrote:
>
> >Yet another question:
> >
> >Let's say I do the following:
> >
> >set.seed(123)
> >tmpf <- function(){
> >x <- rpois(rpois(1,4),2)
> >}
> >n <- 3
> >m <- replicate(n, tmpf())
> >m
> >sub.m <- lapply(m, function(x)x[x>2])
> >
> >'sub.m' gives me:
> >
> >[[1]]
> >[1] 3 4
> >
> >[[2]]
> >[1] 4 5
> >
> >[[3]]
> >[1] 4
> >
> >The question is:  What do I need to do such that I can extract
> >componets of length 2 in 'sub.m' into another sublist, which would
> >look like this:
> >
> >[[1]]
> >[1] 3 4
> >
> >[[2]]
> >[1] 4 5
> >
> >I think that's all the questions I can think of -- for now.
> >
> >Many, many thanks!!!
> >
> >On 8/25/06, xpRt. wannabe <xprt.wannabe at gmail.com> wrote:
> >
> >
> >>Jim and Patrick,
> >>
> >>Both of you made the same suggestion, which works great!
> >>
> >>A follow-up question: Suppose I change the condition 'x>2' in 'lapply'
> >>to 'x>4', as follows:
> >>
> >>set.seed(123)
> >>tmpf <- function() {
> >>x <- rpois(rpois(1,4),2)
> >>}
> >>n <- 3
> >>m <- replicate(n,tmpf())
> >>m
> >>sub.m <- lapply(m, function(x)x[x>4])  # was x>2
> >>
> >>As a result, I'd get:
> >>
> >>
> >>
> >>>sub.m
> >>>
> >>>
> >>[[1]]
> >>numeric(0)
> >>
> >>[[2]]
> >>[1] 5
> >>
> >>[[3]]
> >>numeric(0)
> >>
> >>However, what would I need to do such that 'sub.m' contains only the
> >>non-zero length component; namely, the 'sub.m[[2]]'?  In essence, I'd
> >>like to drop all the components of zero length such that 'sub.m'
> >>results in:
> >>
> >>[[1]]
> >>[1] 5
> >>
> >>My best effort was to use 'lapply' again:
> >>
> >>lapply(sub.m, function(x)x[length(x)>0])
> >>
> >>which still gives me:
> >>
> >>[[1]]
> >>numeric(0)
> >>
> >>[[2]]
> >>[1] 5
> >>
> >>[[3]]
> >>numeric(0)
> >>
> >>Again, any help would be greately appreciated.
> >>
> >>p.s. Sorry to bug you again.   I should have thought through a little
> >>more prior to composing an example that would represent all possible
> >>scenarios.
> >>
> >>On 8/25/06, jim holtman <jholtman at gmail.com> wrote:
> >>
> >>
> >>>try this:
> >>>
> >>>
> >>>
> >>>>set.seed(123)
> >>>>tmpf <- function() {
> >>>>
> >>>>
> >>>+ x <- rpois(rpois(1,4),2)
> >>>+ }
> >>>
> >>>
> >>>>n <- 3
> >>>>m <- replicate(n,tmpf())
> >>>>m
> >>>>
> >>>>
> >>>[[1]]
> >>>[1] 3 2 4
> >>>
> >>>[[2]]
> >>>[1] 0 2 4 2 2 5 2
> >>>
> >>>[[3]]
> >>>[1] 2 0 4 1 0
> >>>
> >>>
> >>>
> >>>>lapply(m, function(x)x[x>2])
> >>>>
> >>>>
> >>>[[1]]
> >>>[1] 3 4
> >>>
> >>>[[2]]
> >>>[1] 4 5
> >>>
> >>>[[3]]
> >>>[1] 4
> >>>
> >>>
> >>>
> >>>On 8/25/06, xpRt.wannabe <xprt.wannabe at gmail.com> wrote:
> >>>
> >>>
> >>>>Dear List,
> >>>>
> >>>>The following code produces a list, which is what I what:
> >>>>
> >>>>set.seed(123)
> >>>>tmpf <- function() {
> >>>>x <- rpois(rpois(1,4),2)
> >>>>}
> >>>>n <- 3
> >>>>m <- replicate(n,tmpf())
> >>>>m
> >>>>
> >>>>[[1]]
> >>>>[1] 3 2 4
> >>>>
> >>>>[[2]]
> >>>>[1] 0 2 4 2 2 5 2
> >>>>
> >>>>[[3]]
> >>>>[1] 2 0 4 1 0
> >>>>
> >>>>
> >>>>Now I need something that would to extract iteratively (or as many
> >>>>times as
> >>>>the size of 'n') the values that are greater than 2 in each component
> >>>>of
> >>>>'m' into another list such that the sub-list would be:
> >>>>
> >>>>[[1]]
> >>>>[1] 3 4
> >>>>
> >>>>[[2]]
> >>>>[1] 4 5
> >>>>
> >>>>[[3]]
> >>>>[1] 4
> >>>>
> >>>>Below is what I tried:
> >>>>
> >>>>for(i in 1:3)
> >>>>sub.list <- lapply(m,subset,m[[i]]>2)
> >>>>
> >>>>
> >>>>
> >>>>>sub.list
> >>>>>
> >>>>>
> >>>>which gives me something different from what I want:
> >>>>
> >>>>[[1]]
> >>>>[1] 4
> >>>>
> >>>>[[2]]
> >>>>[1] 4
> >>>>
> >>>>[[3]]
> >>>>[1] 4
> >>>>
> >>>>Any help would be appreciated.
> >>>>
> >>>>
> >>>>
> >>>>>version
> >>>>>
> >>>>>
> >>>>        _
> >>>>platform i386-pc-mingw32
> >>>>arch     i386
> >>>>os       mingw32
> >>>>system   i386, mingw32
> >>>>status
> >>>>major    2
> >>>>minor    2.1
> >>>>year     2005
> >>>>month    12
> >>>>day      20
> >>>>svn rev  36812
> >>>>language R
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>>
> >>>>
> >>>--
> >>>Jim Holtman
> >>>Cincinnati, OH
> >>>+1 513 646 9390
> >>>
> >>>What is the problem you are trying to solve?
> >>>
> >>>
> >>>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
>


From rhuntsinger at verizon.net  Sat Aug 26 17:53:21 2006
From: rhuntsinger at verizon.net (Reid Huntsinger)
Date: Sat, 26 Aug 2006 11:53:21 -0400
Subject: [R] problems with loop
References: <200608261247.k7QClIgn000395@idmailgate2.unizh.ch>
Message-ID: <44F06E71.3000805@verizon.net>

I think it's a scoping problem. Your function NLL() looks for "new" in 
the environment in which NLL() was defined, but you generate your 
simulated datasets in a different environment (local to sim.estim()). 
There are a number of ways to deal with this:
- pass the dataset as a parameter to NLL()
- manipulate environments with "assign()" or "<<-"
- put "new" in its own environment and use "assign()" and "get()"
- define NLL() within sim.estim() after generating "new", like "NLL <- 
negative.log.likelihood(new)" where negative.log.likelihood() is a 
function like
negative.log.likelihood <- function(new) {
NLL <- function(coef) {
... (your definition here)...
}
NLL
}
This defines a new NLL() each time, using the dataset "new" just generated.

Reid Huntsinger

Simon Ruegg wrote:

>Dear all,
>
> 
>
>I am trying to evaluate the optimisation behaviour of a function. Originally
>I have optimised a model with real data and got a set of parameters. Now I
>am creating simulated data sets based on these estimates. With these
>simulations I am estimating the parameters again to see how variable the
>estimation is. To this end I have written a loop which should generate a new
>simulated data set each time. However, the optimisation algorithm, which
>works fine if only one data set is used, does not recognise the simulated
>data in the loop. Can anyone tell me where the error is? The code is below.
>
> 
>
>Thanks for your help
>
> 
>
>Simon
>
> 
>
> 
>
># The loop in which nothing works anymore. The NLL in the optim function
>appears not to recognise the "new" data set. The functions used by this loop
>are given below.
>
> 
>
>sim.estim=function(r)
>
>{
>
>      res=matrix(nrow=r,ncol=5)
>
>      for (s in 1:r)
>
>      {
>
>            new=new.set()
>
> 
>Min=optim(c(0.5,0.5,0.01,0.1),NLL,method="L-BFGS-B",lower=c(0,0,0,0))
>
>            res[s,1]=Min$par[1]
>
>            res[s,2]=Min$par[2]
>
>            res[s,3]=Min$par[3]
>
>            res[s,4]=Min$par[4]
>
>            res[s,5]=Min$value[1]
>
>      }
>
>return(res)
>
>}
>
> 
>
> 
>
> 
>
># function to create a new data set. Works fine on its own
>
> 
>
>new.set=function()
>
>{
>
>      tmp=matrix(nrow=510,ncol=2)
>
>      v=numeric()
>
>      sim=Model_1(1.2761707, 0.1953354, 2.7351930, 0.1032929)
>
> 
>
>      tmp[,1]=sample(sim[,1],510,replace=T)
>
>      v=runif(510,0,1)
>
> 
>
>      for (k in 1 : 510)
>
>      {
>
>            for (n in 1 : length(sim[,1]))
>
>            {
>
>                  if (tmp[k,1]==sim[n,1] && v[k]<=sim[n,2]){tmp[k,2]=1}
>
>                  if(tmp[k,1]==sim[n,1] && v[k]>sim[n,2]) {tmp[k,2]=0}
>
>            }
>
>      }
>
>return(tmp)
>
>}
>
> 
>
> 
>
># The model to be fitted
>
> 
>
>Model_1=function(beta0_mod,mu_mod,k_mod,I_mod)
>
>{
>
>      parms = c(beta0=beta0_mod, mu=mu_mod, k=k_mod)
>
>      my.atol = 1e-6
>
> 
>times1=c(0,0.002884615,0.003846154,0.005769231,0.009615385,0.01,0.019230769)
>
>
>      times2=c(0.028846154,0.038461538,0.057692308,
>0.076923077,0.083333333,0.096153846)
>
> 
>times3=c(0.115384615,0.125,0.134615385,0.166666667,0.25,0.416666667,0.5) 
>
>      times4=c(0.58333333,0.666666667,0.75,0.83333333,0.916666667,1)
>
>      times5=c( 1.166666667,1.25,1.5,1.6,1.75,2,2.5)
>
>      times6=seq(3,20)
>
>      times = c(times1,times2,times3,times4,times5,times6)
>
>      ODE_sys = function(t, x, w)  #w=c("beta0","mu","k")
>
>             {
>
>                  I=x[1]
>
>                                   
>
>            dI=-w["mu"]*I+w["beta0"]*exp(-w["k"]*t)*(1-I)
>
>            
>
>            list(c(dI),c(S=1-I))
>
>             }
>
> 
>
>      output = lsoda(c(I_mod),times,ODE_sys, parms, rtol=1e-6, atol=
>my.atol)
>
>      return(output)
>
>}
>
> 
>
># The negative log likelihood of this model given a data set "new". This
>works fine if it is used in the optim() routine with only one data set.
>
> 
>
>NLL=function(coef) 
>
>{      
>
>      beta0=coef[1]
>
>      mu=coef[2]
>
>      k=coef[3]
>
>      I=coef[4]
>
>            
>
>      data_sim=Model_1(beta0,mu,k,I)
>
> 
>
>      LLsum=0  #log likelihood sum
>
>      
>
>      for (j in 1:length(data_sim[,1]))
>
>            {
>
>            if (data_sim[j,2]<0 || data_sim[j,3]<0)
>
>            {return(1500)}
>
> 
>
>            for (i in 1:length(new[,1]))
>
>            {
>
>                  if (new[i,1]==data_sim[j,1] && is.na(new[i,1])==F &&
>is.na(data_sim[j,1])==F)
>
>                  # if age of real data = age of simulated model and not
>equal to NA
>
>                  {
>
>                        if (is.na(new[i,2])==F && is.na(data_sim[j,2])==F &&
>is.na(data_sim[j,3])==F)
>
>                        # if state of real data and state of simulated model
>are not equal to NA
>
>                        {
>
>                             tmp1=new[i,2]*log(data_sim[j,2])
>
>                             tmp2=(1-new[i,2])*log(data_sim[j,3])
>
> 
>
>                             if (tmp1=="-Inf" || tmp1=="NaN"){tmp1=-700}
>
>                             if (tmp2=="-Inf" || tmp2=="NaN"){tmp2=-700}
>
>                             LLsum=LLsum+tmp1+tmp2
>
>                        }
>
>                  }
>
>                  
>
>            }
>
>      }
>
> 
>
>return(-LLsum)
>
>}
>
> 
>
> 
>
> 
>
> 
>
> 
>
>********************************************************************
>
>Simon Ruegg
>
>Dr.med.vet.,  PhD student
>
>Institute for Parasitology
>
>Winterthurstr. 266a
>
>8057 Zurich
>
>Switzerland
>
> 
>
>phone: +41 44 635 85 93
>
>fax: +41 44 635 89 07
>
>e-mail: s.ruegg at access.unizh.ch
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>  
>


From liuwensui at gmail.com  Sat Aug 26 19:06:14 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 26 Aug 2006 13:06:14 -0400
Subject: [R] for() loop question
Message-ID: <1115a2b00608261006j516456f3jb964a527e360e232@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060826/305314a7/attachment.pl 

From thomas.harte at yahoo.com  Sat Aug 26 19:15:27 2006
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Sat, 26 Aug 2006 10:15:27 -0700 (PDT)
Subject: [R] issues with Sweave and inclusion of graphics in a document
In-Reply-To: <17648.18219.150226.240250@celebrian.ci.tuwien.ac.at>
Message-ID: <20060826171527.37299.qmail@web30210.mail.mud.yahoo.com>



--- Friedrich Leisch <Friedrich.Leisch at stat.uni-muenchen.de> wrote:

> >>>>> On Sat, 26 Aug 2006 05:38:58 -0700 (PDT),
> >>>>> Thomas Harte (TH) wrote:
> 
> 
>   > hallo, friedrich, and thanks for your reply.
> 
>   > if i Stangle your code i get:
> 
>   >         sp<- make.ar.1(alpha=.5, n=800)
>   >         plot(sp, type="l", col="blue")
> 
>   > whereas if i Stangle my code, i get:
> 
>   >         width<- 20; height<- 5
>   >         x11(width=width, height=height)
>   >         sp<- make.ar.1(alpha=.5, n=800)
>   >         plot(sp, type="l", col="blue")
>   >         savePlotAsPdf("ar.pdf")
> 
>   > this, i think, is the problem in a nutshell.
> 
>   > i want my R code to look like this because i (often) want to open
>   > a separate device (for example, to have specific dimensions as
>   > above) and i may wish to save its contents. on the other hand, i
>   > may not wish to save a device's contents: i may wish to open
>   > several devices for comparison purposes and i may wish to save
>   > only certain devices, in keeping with the Sweave options
>   > echo=false and results=hide.
> 
>   > if i use your, admittedly uncomplicated, code chunk, then i am
>   > limited to seeing the output in the final document. i can't cut
>   > and paste the code chunk into an R session and see the results as
>   > i wish to see them before saving.
> 
>   > currently for each project i work on, i usually end up with a very
>   > lengthy .R file with all the code (and embedded mathematical
>   > annotation in LaTeX), and then i have to write a set of notes or a
>   > report in LaTeX duplicating or expanding on the mathematics. i'm
>   > hoping to supplant this, rather inefficient, method with a single
>   > .Rnw file and the use of Sweave. this, i believe, is what Sweave
>   > was designed to do. but i do wish to write my R code (warts and
>   > all) the way that i want to in the .Rnw file and only include the
>   > output (graphics or text) of certain parts of that code in the
>   > document. i hope that seems reasonable.
> 
>   > perhaps i was not clear, or succinct enough, in my original post
>   > (i tried to get the point across by providing example .Rnw files,
>   > but i'm aware that it's asking a lot to wade through a lengthy
>   > example to retrieve a point).
> 
> OK, no I think I understand what you want: running the *tangled* code
> should open windows with the same height/width ratio as in the Latex
> document, right? 

yes, that is correct. the goal is to develop R code in exactly the same
way as i always do (a monolithic script with lots of inroads into a 
problem, solutions i tried and rejected, helpful pointers to the 
real solution, and so on), but to have a document (.pdf, .ps, whatever)
that goes along with the monolithic script noting all the key points. 
hence, six months after writing the code i want to be able to go back, 
refresh my memory with the document and then delve into the R code
if i have to. 

maintaining both the R code and the LaTeX documentation
in one .Rnw file is the genius of your Sweave package and this is 
what i want to adopt as my standard development methodology.

> Your first posting suggested to me more the other way round ...

mea maxima culpa. 
 
> No, what you want is currently not directly possible with Sweave, you
> will have to use workarounds like the one you posted. You can even
> easily write your own tangle function doing it automatically (Rtangle
> is really trivial can be easily adjusted to ones personal needs).

thank you for the clarification. i feel a lot less awkward in using
that complicated workaround code ;)

cheers / gruss,

thomas.


From MSchwartz at mn.rr.com  Sat Aug 26 19:16:20 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 26 Aug 2006 12:16:20 -0500
Subject: [R] for() loop question
In-Reply-To: <1115a2b00608261006j516456f3jb964a527e360e232@mail.gmail.com>
References: <1115a2b00608261006j516456f3jb964a527e360e232@mail.gmail.com>
Message-ID: <1156612581.5046.7.camel@localhost.localdomain>

On Sat, 2006-08-26 at 13:06 -0400, Wensui Liu wrote:
> Dear Lister,
> 
> If I have a list of number, say x<-c(0.1, 0.5, 0.6...), how to use a for()
> to loop through each number in x one by one?
> 
> Thank you so much!
> 
> wensui

Two options:

x <- c(0.1, 0.5, 0.6)

> for (i in x) {print (i)}
[1] 0.1
[1] 0.5
[1] 0.6


> for (i in seq(along = x)) {print (x[i])}
[1] 0.1
[1] 0.5
[1] 0.6


Which approach you take tends to depends upon what else you are doing
within the loop.

I would also take a look at ?sapply, depending up what is it you are
doing.

HTH,

Marc Schwartz


From markleeds at verizon.net  Sat Aug 26 19:26:09 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Sat, 26 Aug 2006 13:26:09 -0400
Subject: [R] for() loop question
References: <1115a2b00608261006j516456f3jb964a527e360e232@mail.gmail.com>
Message-ID: <002401c6c934$b8a10300$2e01a8c0@m8d4477f3de884>

let us know what you want to do because the beauty of R is that, in many 
cases, you may not have to loop.


----- Original Message ----- 
From: "Wensui Liu" <liuwensui at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, August 26, 2006 1:06 PM
Subject: [R] for() loop question


> Dear Lister,
>
> If I have a list of number, say x<-c(0.1, 0.5, 0.6...), how to use a for()
> to loop through each number in x one by one?
>
> Thank you so much!
>
> wensui
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From liuwensui at gmail.com  Sat Aug 26 19:56:20 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 26 Aug 2006 13:56:20 -0400
Subject: [R] for() loop question
In-Reply-To: <1156612581.5046.7.camel@localhost.localdomain>
References: <1115a2b00608261006j516456f3jb964a527e360e232@mail.gmail.com>
	<1156612581.5046.7.camel@localhost.localdomain>
Message-ID: <1115a2b00608261056x22bb6b85p1f267273ab3cf4f4@mail.gmail.com>

Thank you so much Marc.

Your solution is exactly what I am looking for.

Have a nice weekend.

wensui

On 8/26/06, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On Sat, 2006-08-26 at 13:06 -0400, Wensui Liu wrote:
> > Dear Lister,
> >
> > If I have a list of number, say x<-c(0.1, 0.5, 0.6...), how to use a for()
> > to loop through each number in x one by one?
> >
> > Thank you so much!
> >
> > wensui
>
> Two options:
>
> x <- c(0.1, 0.5, 0.6)
>
> > for (i in x) {print (i)}
> [1] 0.1
> [1] 0.5
> [1] 0.6
>
>
> > for (i in seq(along = x)) {print (x[i])}
> [1] 0.1
> [1] 0.5
> [1] 0.6
>
>
> Which approach you take tends to depends upon what else you are doing
> within the loop.
>
> I would also take a look at ?sapply, depending up what is it you are
> doing.
>
> HTH,
>
> Marc Schwartz
>
>
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From phhs80 at gmail.com  Sat Aug 26 21:01:18 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Sat, 26 Aug 2006 20:01:18 +0100
Subject: [R] Can R compute the expected value of a random variable?
Message-ID: <6ade6f6c0608261201w28afd112kabca3bd902564da4@mail.gmail.com>

Dear All

Can R compute the expected value of a random variable?

Thanks in advance,

Paul


From mr.blacksheep at gmail.com  Sat Aug 26 21:04:40 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Sat, 26 Aug 2006 13:04:40 -0600
Subject: [R] Can R compute the expected value of a random variable?
In-Reply-To: <6ade6f6c0608261201w28afd112kabca3bd902564da4@mail.gmail.com>
References: <6ade6f6c0608261201w28afd112kabca3bd902564da4@mail.gmail.com>
Message-ID: <46a360560608261204k1ffd0d62y570b2d7b8ab3d310@mail.gmail.com>

Yes.

On 8/26/06, Paul Smith <phhs80 at gmail.com> wrote:
> Dear All
>
> Can R compute the expected value of a random variable?
>
> Thanks in advance,
>
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Regards,

Mike Nielsen


From michael at isis.spa.umn.edu  Sat Aug 26 21:51:54 2006
From: michael at isis.spa.umn.edu (Michael Koppelman)
Date: Sat, 26 Aug 2006 14:51:54 -0500
Subject: [R] fitting a gaussian to some x,y data
In-Reply-To: <79AE200F-A92D-46B9-AEB7-BD7340652572@aps.umn.edu>
References: <200608251712.k7PHCJGt021268@erdos.math.unb.ca>
	<000701c6c892$d4fd2c60$2e01a8c0@m8d4477f3de884>
	<79AE200F-A92D-46B9-AEB7-BD7340652572@aps.umn.edu>
Message-ID: <C2FDF9DD-992C-4A2D-B09E-475207B414F8@aps.umn.edu>

Success! The line I needed was:

gcoeffs <-nls(y~(a/b)*exp(-(x-c)^2/(2*b^2)),start=list 
(a=0.4,b=2,c=-10), trace=TRUE)

I also needed to provide good guesses for a, b and c. The attached  
PNG should explain what I was going after, which is the line in the  
center of that curve. I am sorry for my confusing and perhaps  
incorrect usage of terminology.

Cheers,
Michael


-------------- next part --------------
A non-text attachment was scrubbed...
Name: example3.png
Type: image/png
Size: 21420 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060826/b405c7dd/attachment.png 

From ritwik.sinha at gmail.com  Sat Aug 26 21:53:44 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sat, 26 Aug 2006 15:53:44 -0400
Subject: [R] Implementing EM Algorithm in R!
In-Reply-To: <4965b4cd0608260448t5d88a632lb177e3af859d2767@mail.gmail.com>
References: <4965b4cd0608260448t5d88a632lb177e3af859d2767@mail.gmail.com>
Message-ID: <42bc98300608261253j151c3beby498e7790f159e874@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060826/1a2f5522/attachment.pl 

From michael at isis.spa.umn.edu  Sat Aug 26 21:55:00 2006
From: michael at isis.spa.umn.edu (Michael Koppelman)
Date: Sat, 26 Aug 2006 14:55:00 -0500
Subject: [R] fitting a gaussian to some x,y data
In-Reply-To: <C2FDF9DD-992C-4A2D-B09E-475207B414F8@aps.umn.edu>
References: <200608251712.k7PHCJGt021268@erdos.math.unb.ca>
	<000701c6c892$d4fd2c60$2e01a8c0@m8d4477f3de884>
	<79AE200F-A92D-46B9-AEB7-BD7340652572@aps.umn.edu>
	<C2FDF9DD-992C-4A2D-B09E-475207B414F8@aps.umn.edu>
Message-ID: <0972E060-4094-4E61-AAF6-E964FE58C1A1@aps.umn.edu>

Whoops, I forgot to add, many thanks to all who replied publicly and  
privately. I am very appreciative of all comments and suggestions.  
Mark Leeds was especially kind in terms of clarifying why my  
description of the situation was so confusing.

Cheers,
Michael

On Aug 26, 2006, at 2:51 PM, Michael Koppelman wrote:

> Success! The line I needed was:
>
> gcoeffs <-nls(y~(a/b)*exp(-(x-c)^2/(2*b^2)),start=list 
> (a=0.4,b=2,c=-10), trace=TRUE)
>
> I also needed to provide good guesses for a, b and c. The attached  
> PNG should explain what I was going after, which is the line in the  
> center of that curve. I am sorry for my confusing and perhaps  
> incorrect usage of terminology.
>
> Cheers,
> Michael


From landon.sego at pnl.gov  Sat Aug 26 22:28:32 2006
From: landon.sego at pnl.gov (Sego, Landon H)
Date: Sat, 26 Aug 2006 13:28:32 -0700
Subject: [R] boxplot( ..args.., axes=FALSE,
	frame.plot=TRUE) doesn't frame the plot
Message-ID: <64F04C9398A52E43B208634452B7D00B7C2041@EMAIL01.pnl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060826/1a2d88f0/attachment.pl 

From miralisus at gmail.com  Sun Aug 27 00:07:22 2006
From: miralisus at gmail.com (Amasco Miralisus)
Date: Sun, 27 Aug 2006 00:07:22 +0200
Subject: [R] Type II and III sum of square in Anova (R, car package)
Message-ID: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060827/3a13620c/attachment.pl 

From terrywschulz at comcast.net  Sun Aug 27 00:42:17 2006
From: terrywschulz at comcast.net (Terry W. Schulz)
Date: Sat, 26 Aug 2006 16:42:17 -0600
Subject: [R] Capture of iterative integration output
Message-ID: <44F0CE49.2040701@comcast.net>

Hello,

I am a novice R user and am having difficulty retrieving  the values  
from 21 iterations of the R function integrate.
The only way I have found is to do a write.table and then a read.table 
as shown in the code below.  I would rather capture the 21 values inside 
the braces ( sapply might work, but I can't set it up without getting an 
error in function) so I could compute the negative log-likelihood inside 
the braces.  My goal is to then use optim to find the best fitting mu 
and sigma.  In the code that follows mu and sigma are given, but 
normally they would not be known.

Any help on y$value capture would be appreciated.  I would be ecstatic 
for help on implementation of the optim function using finite 
differences for the following code. 

By the way in case anyone is interested function(x) is the 
Poisson-lognormal pdf.

q <- read.table(file="c:/Bayes/FigA3-1.prn",header=TRUE)
attach(q)
V <- c(volume)
c <- c(count)
n <- length(count)
mu <- -8.202900475
sigma <- 1.609437912
for(i in c(1:n))
{
integrand <- function(x) 
{(x*V[[i]])^c[[i]]*exp(-x*V[[i]])/factorial(c[[i]])*0.3989422804014327/
                           (x*sigma)*exp(-.5*((log(x)-mu)/sigma)^2)}
y <- integrate(integrand, lower = 0, upper = Inf)
write.table(y[[1]],file="c:/Bayes/PLNA3-1.out",append=TRUE,
            quote=FALSE,row.names=FALSE,col.names=FALSE)
}
z <-read.table(file="c:/Bayes/PLNA3-1.out")
z <- c(z)
LL <- log(c(z$V1))
#negative Log-Likelihood
print(-sum(LL),digits=6)
file.remove("c:/Bayes/PLNA3-1.out")

-- 
Terry W. Schulz
Applied Statistician
1218 Pomegranate Lane
Golden, CO 80401
USA

terrywschulz at comcast.net
(303) 526-1461


From dkaplan at education.wisc.edu  Sun Aug 27 00:44:47 2006
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Sat, 26 Aug 2006 17:44:47 -0500
Subject: [R] Permanently changing gui preferences
Message-ID: <44F0CEDF.8010902@education.wisc.edu>

Greetings,

I made changes to my gui preferences and saved them.  When I close and 
then open R, it reverts back to default preferences.  How do I 
permanently change gui preferences?

Thanks in advance.

David


-- 
========================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
Fax:   608-262-0843


From r.nieuwenhuis at student.ru.nl  Sun Aug 27 01:00:38 2006
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Sun, 27 Aug 2006 01:00:38 +0200
Subject: [R] Importing data from clipboard on Mac OSX
Message-ID: <51028E84-5275-4F9E-86F7-5B7504C2AEA7@student.ru.nl>

Dear R users,

I am trying to get data from the clipboard into R on MacOSX. I tried  
the following, but got an error message:

read.delim("clipboard")
Error in file(file, "r") : unable to open connection
In addition: Warning message:
unable to contact X11 display

Obviously, I'm not running R using X11. I'm wondering, can I import  
data from the clipboard on MacosX?

Thanks in advance,

Rense Nieuwenhuis

 > version
                _
platform       powerpc-apple-darwin8.6.0
arch           powerpc
os             darwin8.6.0
system         powerpc, darwin8.6.0
status
major          2
minor          3.0
year           2006
month          04
day            24
svn rev        37909
language       R
version.string Version 2.3.0 (2006-04-24)


From landon.sego at pnl.gov  Sun Aug 27 01:04:55 2006
From: landon.sego at pnl.gov (Sego, Landon H)
Date: Sat, 26 Aug 2006 16:04:55 -0700
Subject: [R] Capture of iterative integration output
In-Reply-To: <44F0CE49.2040701@comcast.net>
Message-ID: <64F04C9398A52E43B208634452B7D00B7C2049@EMAIL01.pnl.gov>

Here's one way to do this:

Before you begin the loop, define a vector to capture the output.  Then
each iteration of the loop will be assigned to the corresponding element
in that vector: 

capture <- double(n) # creates a vector of length n, all with values of
0

for (i in 1:n) {

  _ some code _

  capture[i] <- integrate( _your arguments_ )$value
  
}

"capture" should be the same as your "z".

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Terry W. Schulz
> Sent: Saturday, August 26, 2006 3:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Capture of iterative integration output
> 
> Hello,
> 
> I am a novice R user and am having difficulty retrieving  the 
> values from 21 iterations of the R function integrate.
> The only way I have found is to do a write.table and then a 
> read.table as shown in the code below.  I would rather 
> capture the 21 values inside the braces ( sapply might work, 
> but I can't set it up without getting an error in function) 
> so I could compute the negative log-likelihood inside the 
> braces.  My goal is to then use optim to find the best 
> fitting mu and sigma.  In the code that follows mu and sigma 
> are given, but normally they would not be known.
> 
> Any help on y$value capture would be appreciated.  I would be 
> ecstatic for help on implementation of the optim function 
> using finite differences for the following code. 
> 
> By the way in case anyone is interested function(x) is the 
> Poisson-lognormal pdf.
> 
> q <- read.table(file="c:/Bayes/FigA3-1.prn",header=TRUE)
> attach(q)
> V <- c(volume)
> c <- c(count)
> n <- length(count)
> mu <- -8.202900475
> sigma <- 1.609437912
> for(i in c(1:n))
> {
> integrand <- function(x)
> {(x*V[[i]])^c[[i]]*exp(-x*V[[i]])/factorial(c[[i]])*0.39894228
04014327/
>                            (x*sigma)*exp(-.5*((log(x)-mu)/sigma)^2)}
> y <- integrate(integrand, lower = 0, upper = Inf) 
> write.table(y[[1]],file="c:/Bayes/PLNA3-1.out",append=TRUE,
>             quote=FALSE,row.names=FALSE,col.names=FALSE)
> }
> z <-read.table(file="c:/Bayes/PLNA3-1.out")
> z <- c(z)
> LL <- log(c(z$V1))
> #negative Log-Likelihood
> print(-sum(LL),digits=6)
> file.remove("c:/Bayes/PLNA3-1.out")
> 
> --
> Terry W. Schulz
> Applied Statistician
> 1218 Pomegranate Lane
> Golden, CO 80401
> USA
> 
> terrywschulz at comcast.net
> (303) 526-1461
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p_connolly at ihug.co.nz  Sun Aug 27 01:55:03 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Sun, 27 Aug 2006 11:55:03 +1200
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <44F00CED.9000700@pburns.seanet.com>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
	<644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>
	<a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>
	<a4fecdd70608252236v6491384ape9962761214a680e@mail.gmail.com>
	<44F00CED.9000700@pburns.seanet.com>
Message-ID: <20060826235503.GA3606@ihug.co.nz>

On Sat, 26-Aug-2006 at 09:57AM +0100, Patrick Burns wrote:

|>  > sub.m <- lapply(m, function(x)x[x>2])
|>  > sub.m
|> [[1]]
|> [1] 3 4
|> 
|> [[2]]
|> [1] 4 5
|> 
|> [[3]]
|> [1] 4
|> 
|>  > sub.m[unlist(lapply(sub.m, function(x) length(x) == 2))]
|> [[1]]
|> [1] 3 4
|> 
|> [[2]]
|> [1] 4 5
|> 
|>  > sub4.m <- lapply(m, function(x)x[x>4])
|>  > sub4.m[unlist(lapply(sub4.m, function(x) length(x) > 0))]
|> [[1]]
|> [1] 5

Or slightly shorter in this case:

sub.m[sapply(sub.m, function(x) length(x) == 2)]

etc.



-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From ggrothendieck at gmail.com  Sun Aug 27 02:23:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Aug 2006 20:23:07 -0400
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <20060826235503.GA3606@ihug.co.nz>
References: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
	<644e1f320608251426j25c12badk469c81417def5dba@mail.gmail.com>
	<a4fecdd70608252145m1512f7beqe0edf46db1d4a9ad@mail.gmail.com>
	<a4fecdd70608252236v6491384ape9962761214a680e@mail.gmail.com>
	<44F00CED.9000700@pburns.seanet.com>
	<20060826235503.GA3606@ihug.co.nz>
Message-ID: <971536df0608261723q68d2db5cyd9b3c4930adcda2f@mail.gmail.com>

On 8/26/06, Patrick Connolly <p_connolly at ihug.co.nz> wrote:
> On Sat, 26-Aug-2006 at 09:57AM +0100, Patrick Burns wrote:
>
> |>  > sub.m <- lapply(m, function(x)x[x>2])
> |>  > sub.m
> |> [[1]]
> |> [1] 3 4
> |>
> |> [[2]]
> |> [1] 4 5
> |>
> |> [[3]]
> |> [1] 4
> |>
> |>  > sub.m[unlist(lapply(sub.m, function(x) length(x) == 2))]
> |> [[1]]
> |> [1] 3 4
> |>
> |> [[2]]
> |> [1] 4 5
> |>
> |>  > sub4.m <- lapply(m, function(x)x[x>4])
> |>  > sub4.m[unlist(lapply(sub4.m, function(x) length(x) > 0))]
> |> [[1]]
> |> [1] 5
>
> Or slightly shorter in this case:
>
> sub.m[sapply(sub.m, function(x) length(x) == 2)]
>

or even shorter:

sub.m[sapply(sub.m, length) == 2]


From terrywschulz at comcast.net  Sun Aug 27 02:29:45 2006
From: terrywschulz at comcast.net (Terry W. Schulz)
Date: Sat, 26 Aug 2006 18:29:45 -0600
Subject: [R] Capture of iterative integration output
In-Reply-To: <64F04C9398A52E43B208634452B7D00B7C2049@EMAIL01.pnl.gov>
References: <64F04C9398A52E43B208634452B7D00B7C2049@EMAIL01.pnl.gov>
Message-ID: <44F0E779.20705@comcast.net>

Thanks for the reply Sego, Landon H.   The vector is created as you 
say.  However, only the first iteration of integrate is captured.
The other twenty vector values remain at 0.  Integrate output includes 
several attributes and I only want the first one which is under $names 
and called "value"  

Terry W. Schulz
Applied Statistician
1218 Pomegranate Lane
Golden, CO 80401
USA

terrywschulz at comcast.net
(303) 526-1461




Sego, Landon H wrote:
> Here's one way to do this:
>
> Before you begin the loop, define a vector to capture the output.  Then
> each iteration of the loop will be assigned to the corresponding element
> in that vector: 
>
> capture <- double(n) # creates a vector of length n, all with values of
> 0
>
> for (i in 1:n) {
>
>   _ some code _
>
>   capture[i] <- integrate( _your arguments_ )$value
>   
> }
>
> "capture" should be the same as your "z".
>
>   
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Terry W. Schulz
>> Sent: Saturday, August 26, 2006 3:42 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Capture of iterative integration output
>>
>> Hello,
>>
>> I am a novice R user and am having difficulty retrieving  the 
>> values from 21 iterations of the R function integrate.
>> The only way I have found is to do a write.table and then a 
>> read.table as shown in the code below.  I would rather 
>> capture the 21 values inside the braces ( sapply might work, 
>> but I can't set it up without getting an error in function) 
>> so I could compute the negative log-likelihood inside the 
>> braces.  My goal is to then use optim to find the best 
>> fitting mu and sigma.  In the code that follows mu and sigma 
>> are given, but normally they would not be known.
>>
>> Any help on y$value capture would be appreciated.  I would be 
>> ecstatic for help on implementation of the optim function 
>> using finite differences for the following code. 
>>
>> By the way in case anyone is interested function(x) is the 
>> Poisson-lognormal pdf.
>>
>> q <- read.table(file="c:/Bayes/FigA3-1.prn",header=TRUE)
>> attach(q)
>> V <- c(volume)
>> c <- c(count)
>> n <- length(count)
>> mu <- -8.202900475
>> sigma <- 1.609437912
>> for(i in c(1:n))
>> {
>> integrand <- function(x)
>> {(x*V[[i]])^c[[i]]*exp(-x*V[[i]])/factorial(c[[i]])*0.39894228
>>     
> 04014327/
>   
>>                            (x*sigma)*exp(-.5*((log(x)-mu)/sigma)^2)}
>> y <- integrate(integrand, lower = 0, upper = Inf) 
>> write.table(y[[1]],file="c:/Bayes/PLNA3-1.out",append=TRUE,
>>             quote=FALSE,row.names=FALSE,col.names=FALSE)
>> }
>> z <-read.table(file="c:/Bayes/PLNA3-1.out")
>> z <- c(z)
>> LL <- log(c(z$V1))
>> #negative Log-Likelihood
>> print(-sum(LL),digits=6)
>> file.remove("c:/Bayes/PLNA3-1.out")
>>
>> --
>> Terry W. Schulz
>> Applied Statistician
>> 1218 Pomegranate Lane
>> Golden, CO 80401
>> USA
>>
>> terrywschulz at comcast.net
>> (303) 526-1461
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
>


From liuwensui at gmail.com  Sun Aug 27 03:50:23 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 26 Aug 2006 21:50:23 -0400
Subject: [R] how to create many objects with sequencial names?
Message-ID: <1115a2b00608261850p131216d5w17d5b5d31eda443f@mail.gmail.com>

Dear Lister,

Is there a way to create many objects with sequencial names, say lm1,
lm2...lm100?

Thanks.


From jholtman at gmail.com  Sun Aug 27 04:22:38 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 26 Aug 2006 22:22:38 -0400
Subject: [R] how to create many objects with sequencial names?
In-Reply-To: <1115a2b00608261850p131216d5w17d5b5d31eda443f@mail.gmail.com>
References: <1115a2b00608261850p131216d5w17d5b5d31eda443f@mail.gmail.com>
Message-ID: <644e1f320608261922w6c736da8kcad8b09a64dc7237@mail.gmail.com>

Yes there is with statements like:

assign(paste('m', i, sep='', value)

But I would suggest that you put the values in a list to make it
easier to access since all the data is in a single object.  You could
do it in a loop:

result <- list()
for(i in 1:100){
    ......computation.....
    result[[i]] <- yourResult
}


On 8/26/06, Wensui Liu <liuwensui at gmail.com> wrote:
> Dear Lister,
>
> Is there a way to create many objects with sequencial names, say lm1,
> lm2...lm100?
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From liuwensui at gmail.com  Sun Aug 27 04:37:12 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 26 Aug 2006 22:37:12 -0400
Subject: [R] how to create many objects with sequencial names?
In-Reply-To: <644e1f320608261922w6c736da8kcad8b09a64dc7237@mail.gmail.com>
References: <1115a2b00608261850p131216d5w17d5b5d31eda443f@mail.gmail.com>
	<644e1f320608261922w6c736da8kcad8b09a64dc7237@mail.gmail.com>
Message-ID: <1115a2b00608261937u7d2cb23i39da9fd97178321f@mail.gmail.com>

Hi, Jim,

It is you again. I couldn't remember how many times you answered my
silly questions. ^_^

I am not sure assign() is what I want. Say, if I want to create 1000
linear model objects with names lm1, lm2....lm1000, it seems assign
can't solve it.

But your second solution is close to what I am looking for.

Any idea?

Thanks.

wesui
On 8/26/06, jim holtman <jholtman at gmail.com> wrote:
> Yes there is with statements like:
>
> assign(paste('m', i, sep='', value)
>
> But I would suggest that you put the values in a list to make it
> easier to access since all the data is in a single object.  You could
> do it in a loop:
>
> result <- list()
> for(i in 1:100){
>     ......computation.....
>     result[[i]] <- yourResult
> }
>
>
> On 8/26/06, Wensui Liu <liuwensui at gmail.com> wrote:
> > Dear Lister,
> >
> > Is there a way to create many objects with sequencial names, say lm1,
> > lm2...lm100?
> >
> > Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


-- 
WenSui Liu
(http://spaces.msn.com/statcompute/blog)
Senior Decision Support Analyst
Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center


From bbvaughn at bellsouth.net  Sun Aug 27 04:53:59 2006
From: bbvaughn at bellsouth.net (bbvaughn at bellsouth.net)
Date: Sat, 26 Aug 2006 22:53:59 -0400
Subject: [R] Simulations in R during power failure
Message-ID: <20060827025359.COFJ21151.ibm59aec.bellsouth.net@mail.bellsouth.net>

Hi everyone,

I recently ran a simulation on a computer using R that was hooked up to a UPS.  There was one time when the power was out for length and the computer shut down.  I was worried that I had lost the simulation, but upon booting the machine up, I heard the processor kick in.  It sounded like the simulation had resumed.

Does anyone have any experience with this?  Because I live in Florida and there is a possibility of another hurricane.  And I am running a huge simulation which may or may not finish before this hurricane finishes.

What options are there in saving a simulation and it picking up where it left off?

Thanks,
Brandon


From attenka at utu.fi  Sun Aug 27 07:22:33 2006
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 27 Aug 2006 08:22:33 +0300
Subject: [R] Modifying the embed-results
In-Reply-To: <971536df0608260524w5a718f4fk11e696946ce2a905@mail.gmail.com>
References: <1156526634.12112.13.camel@attenka>
	<971536df0608251825v200418dfxab19e22b36283887@mail.gmail.com>
	<f894d958a06.44f010cb@utu.fi>
	<971536df0608260524w5a718f4fk11e696946ce2a905@mail.gmail.com>
Message-ID: <fcc1b1aa7f3e.44f15649@utu.fi>

Thanks! 
I see, that do.call-function is used often in R-algorithms...  Looking over some extra do.call-examples seems useful. This tail-function is also new for me. 

Is there some reason to use seq(along = VECTOR) instead of 1:length(VECTOR)?
Atte

> You can replace the for with lapply like this:
> 
> VECTOR <- c(0, 3, 6, 3, 11, 2, 11, 4, 3, 4, 7, 7, 6, 4, 8)
> f <- function(i) unique(tail(VECTOR, length(VECTOR)-i+1))[1:5]
> out <- do.call(rbind, lapply(seq(along = VECTOR), f))
> na.omit(rbind(rep(NA, 5), out))
> 
> Note that  a matrix with zero rows is returned in the case
> that VECTOR has zero length and in the case that VECTOR
> has fewer than 5 unique elements.
> 
> 
> On 8/26/06, Atte Tenkanen <attenka at utu.fi> wrote:
> > Again my example was't very clear: there were not enough same 
> numbers in the VECTOR.
> > What I need is something like this:
> >
> > VECTOR<-c(0,3,6,3,11,2,11,4,3,4,7,7,6,4,8)
> > MATRIX<-c()
> > for(i in 1:length(VECTOR)){
> >        v<-(unique(VECTOR[i:length(VECTOR)])[1:5])
> >        MATRIX<-rbind(MATRIX,v)
> > }
> >
> > MATRIX<-na.omit(MATRIX)
> > MATRIX
> >
> > > data.frame(MATRIX, row.names=NULL)
> >  X1 X2 X3 X4 X5
> > 1       0  3  6 11  2
> > 2       3  6 11  2  4
> > 3       6  3 11  2  4
> > 4       3 11  2  4  7
> > 5      11  2  4  3  7
> > 6       2 11  4  3  7
> > 7      11  4  3  7  6
> > 8       4  3  7  6  8
> > 9       3  4  7  6  8
> >
> > So, there are no duplicates in rows.
> > VECTOR is always scanned forward as long as the number of  items 
> (here 5) becomes full.
> >
> > Atte
> >
> > > Try:
> > >
> > > embed(VECTOR, 5)[,5:1]
> > >
> > > On 8/25/06, Atte Tenkanen <attenka at utu.fi> wrote:
> > > > Hi,
> > > >
> > > > Here is a vector and the result from the embed-command:
> > > >
> > > > VECTOR=c(0,3,6,3,11,2,4,3,7,6,4,5,10,2,3,5,8)
> > > >
> > > > > embed(VECTOR, dimension=5)
> > > >      [,1] [,2] [,3] [,4] [,5]
> > > >  [1,]   11    3    6    3    0
> > > >  [2,]    2   11    3    6    3
> > > >  [3,]    4    2   11    3    6
> > > >  [4,]    3    4    2   11    3
> > > >  [5,]    7    3    4    2   11
> > > >  [6,]    6    7    3    4    2
> > > >  [7,]    4    6    7    3    4
> > > >  [8,]    5    4    6    7    3
> > > >  [9,]   10    5    4    6    7
> > > > [10,]    2   10    5    4    6
> > > > [11,]    3    2   10    5    4
> > > > [12,]    5    3    2   10    5
> > > > [13,]    8    5    3    2   10
> > > >
> > > > Is there a way to little modify the algorithm so that the result
> > > looks> like this:
> > > >
> > > > [1]  0  3  6 11  2 <- beginning from the first number of the 
> VECTOR> > > [1]  3  6 11  2  4 <- beginning from the second number 
> of the
> > > VECTOR etc
> > > > [1]  6  3 11  2  4
> > > > [1]  3 11  2  4  7
> > > > [1] 11  2  4  3  7
> > > > [1] 2 4 3 7 6
> > > > [1] 4 3 7 6 5
> > > > [1] 3 7 6 4 5
> > > > [1]  7  6  4  5 10
> > > > [1]  6  4  5 10  2
> > > > [1]  4  5 10  2  3
> > > > [1]  5 10  2  3  8
> > > > [1] 10  2  3  5  8
> > > >
> > > > Every row consists of next five unique(!) member of the 
> VECTOR. I
> > > made> this example result with a time consuming algorithm which
> > > uses for-loops
> > > > and whiles.
> > > >
> > > > How to do this better??
> > > >
> > > > Thanks in advance!
> > > >
> > > > Atte Tenkanen
> > > > University of Turku
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-
> project.org/posting-
> > > guide.html> and provide commented, minimal, self-contained,
> > > reproducible code.
> > > >
> > >
> >
>


From mel at altk.com  Sun Aug 27 07:37:35 2006
From: mel at altk.com (mel)
Date: Sun, 27 Aug 2006 07:37:35 +0200
Subject: [R] Permanently changing gui preferences
In-Reply-To: <44F0CEDF.8010902@education.wisc.edu>
References: <44F0CEDF.8010902@education.wisc.edu>
Message-ID: <44F12F9F.40907@altk.com>

David Kaplan a ?crit :

> Greetings,
> I made changes to my gui preferences and saved them.  When I close and 
> then open R, it reverts back to default preferences.  How do I 
> permanently change gui preferences?

one way is using options()
and also using your Rprofile.site file (in ~/etc).
edit it.
See also .First(), source(), etc

you may also directly edit your Rconsole file (also in ~/etc)

or use the config editor in R and save

hih


From mel at altk.com  Sun Aug 27 07:45:42 2006
From: mel at altk.com (mel)
Date: Sun, 27 Aug 2006 07:45:42 +0200
Subject: [R] Simulations in R during power failure
In-Reply-To: <20060827025359.COFJ21151.ibm59aec.bellsouth.net@mail.bellsouth.net>
References: <20060827025359.COFJ21151.ibm59aec.bellsouth.net@mail.bellsouth.net>
Message-ID: <44F13186.5050005@altk.com>

bbvaughn at bellsouth.net a ?crit :

> Hi everyone,
> I recently ran a simulation on a computer using R that was hooked up to a UPS.  There was one time when the power was out for length and the computer shut down.  I was worried that I had lost the simulation, but upon booting the machine up, I heard the processor kick in.  It sounded like the simulation had resumed.
> Does anyone have any experience with this?  Because I live in Florida and there is a possibility of another hurricane.  And I am running a huge simulation which may or may not finish before this hurricane finishes.
> What options are there in saving a simulation and it picking up where it left off?
> Thanks,
> Brandon

write your results, let's say every hour or 30' on your disk.
something like
if (substr(date(),...)=='30') write.table(...)
where it could take place


From mark.lyman at gmail.com  Sun Aug 27 07:50:40 2006
From: mark.lyman at gmail.com (Mark Lyman)
Date: Sun, 27 Aug 2006 05:50:40 +0000 (UTC)
Subject: [R] Type II and III sum of square in Anova (R, car package)
References: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
Message-ID: <loom.20060827T072719-494@post.gmane.org>

> 1. First of all, more general question. Standard anova() function for lm()
> or aov() models in R implements Type I sum of squares (sequential), which
> is not well suited for unbalanced ANOVA. Therefore it is better to use
> Anova() function from car package, which was programmed by John Fox to use
> Type II and Type III sum of squares. Did I get the point?
> 
> 2. Now more specific question. Type II sum of squares is not well suited
> for unbalanced ANOVA designs too (as stated in STATISTICA help), therefore
> the general rule of thumb is to use Anova() function using Type II SS
> only for balanced ANOVA and Anova() function using Type III SS for
> unbalanced ANOVA? Is this correct interpretation?
> 
> 3. I have found a post from John Fox in which he wrote that Type III SS
> could be misleading in case someone use some contrasts. What is this about?
> Could you please advice, when it is appropriate to use Type II and when
> Type III SS? I do not use contrasts for comparisons, just general ANOVA
> with subsequent Tukey post-hoc comparisons.
 
There are many threads on this list that discuss this issue. Not being a great
statistician myself, I would suggest you read through some of these as a start.
As I understand, the best philosophy with regards to types of sums of squares is
to use the type that tests the hypothesis you want. They were developed as a
convenience to test many of the hypotheses a person might want "automatically,"
and put it into a nice, neat little table. However, with an interactive system
like R it is usually even easier to test a full model vs. a reduced model. That
is if I want to test the significance of an interaction, I would use
anova(lm.fit2, lm.fit1) where lm.fit2 contains the interaction and lm.fit2 does
not. The anova function will return the appropriate F-test. The danger with
worrying about what type of sums of squares to use is that often we do not think
about what hypotheses we are testing and if those hypotheses make sense in our
situation.

Mark Lyman


From kknoblauch at free.fr  Sun Aug 27 09:30:31 2006
From: kknoblauch at free.fr (Ken Knoblauch)
Date: Sun, 27 Aug 2006 09:30:31 +0200
Subject: [R] (no subject)
Message-ID: <c06e42e49a0682e0298b7f1d192e9583@free.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060827/c920210e/attachment.pl 

From Ted.Harding at nessie.mcc.ac.uk  Sun Aug 27 09:31:50 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 27 Aug 2006 08:31:50 +0100 (BST)
Subject: [R] Simulations in R during power failure
In-Reply-To: <44F13186.5050005@altk.com>
Message-ID: <XFMail.060827083150.Ted.Harding@nessie.mcc.ac.uk>

On 27-Aug-06 mel wrote:
> bbvaughn at bellsouth.net a ?crit :
> 
>> Hi everyone,
>> I recently ran a simulation on a computer using R that was hooked up
>> to a UPS.  There was one time when the power was out for length and
>> the computer shut down.  I was worried that I had lost the simulation,
>> but upon booting the machine up, I heard the processor kick in.

I'm not sure what you meant by "heard the processor kick in"!
If you really were rebooting, then everything would start from scratch
anyway.

Most UPS's can be connected with a cable -- serial or network -- to the
computer so as to cause the computer to shut-down cleanly, or execute
some other procedure, once the UPS has been standing-in for the mains
for a certain length of time and is about to shut itself down.

So one things that may have happened is that when all programs were
cleanly shut down in this way, R (if configured to automatically
save its state when being closed down in ".Rdata" etc -- see comment
about save.image() below and also see "?quit") would have been in a
position to resume when re-started.

Another possibility (depending on UPS and hardware configuration)
is that when the mains power had been off for a certain length of time
the UPS communicated with the computer to put it into "stand-by",
with the current state being swapped out to disk; the computer would
then draw minimum power from the UPS, which in turn would be able to
remain active for a much longer duration. Then, when power came back on,
the UPS would tell the computer to "resume". However, I doubt this
would be able to cope with a power outage of several days such as
might ensue from a hurricane!

However, all this is just guesses. You need to find out what has been
configured for that comnputer and UPS.

In any case, whenever there is a serious risk of power interruptions,
Mel's advice below is the way to go! Though simply writing tables
to disk my not be sufficient. If you really want to be able to resume
where you left off, then use the function save.image() whose options
and description can be found under "?save".

>> It sounded like the simulation had resumed.
>> Does anyone have any experience with this?  Because I live in Florida
>> and there is a possibility of another hurricane.  And I am running a
>> huge simulation which may or may not finish before this hurricane
>> finishes.
>> What options are there in saving a simulation and it picking up where
>> it left off?
>> Thanks,
>> Brandon
> 
> write your results, let's say every hour or 30' on your disk.
> something like
> if (substr(date(),...)=='30') write.table(...)
> where it could take place
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 27-Aug-06                                       Time: 08:31:44
------------------------------ XFMail ------------------------------


From kknoblauch at free.fr  Sun Aug 27 09:33:41 2006
From: kknoblauch at free.fr (Ken Knoblauch)
Date: Sun, 27 Aug 2006 09:33:41 +0200
Subject: [R]  Importing data from clipboard on Mac OSX
Message-ID: <dc766706b2c549ee355077d6b16b1124@free.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060827/68fbb88e/attachment.pl 

From ripley at stats.ox.ac.uk  Sun Aug 27 09:37:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Aug 2006 08:37:01 +0100 (BST)
Subject: [R] Type II and III sum of square in Anova (R, car package)
In-Reply-To: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
References: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608270824270.6622@gannet.stats.ox.ac.uk>

I think this starts from the position of a batch-oriented package.
In R you can refit models with update(), add1() and drop1(), and 
experienced S/R users almost never use ANOVA tables for unbalanced 
designs.  Rather than fit a pre-specified set of sub-models, why not fit 
those sub-models that appear to make some sense for your problem and data?

SInce your post lacks a signature and your credentials we have no idea of 
your background, which makes it very difficult even to know what reading 
to suggest to you.  But Bill Venables' 'exegeses' paper 
(http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf) may be a good start.
It does explain your comment '3.'.

On Sun, 27 Aug 2006, Amasco Miralisus wrote:

> Hello everybody,
> 
> I have some questions on ANOVA in general and on ANOVA in R particularly.
> I am not Statistician, therefore I would be very appreciated if you answer
> it in a simple way.
> 
> 1. First of all, more general question. Standard anova() function for lm()
> or aov() models in R implements Type I sum of squares (sequential), which
> is not well suited for unbalanced ANOVA. Therefore it is better to use
> Anova() function from car package, which was programmed by John Fox to use
> Type II and Type III sum of squares. Did I get the point?
> 
> 2. Now more specific question. Type II sum of squares is not well suited
> for unbalanced ANOVA designs too (as stated in STATISTICA help), therefore
> the general rule of thumb is to use Anova() function using Type II SS
> only for balanced ANOVA and Anova() function using Type III SS for
> unbalanced ANOVA? Is this correct interpretation?
> 
> 3. I have found a post from John Fox in which he wrote that Type III SS
> could be misleading in case someone use some contrasts. What is this about?
> Could you please advice, when it is appropriate to use Type II and when
> Type III SS? I do not use contrasts for comparisons, just general ANOVA
> with subsequent Tukey post-hoc comparisons.
> 
> Thank you in advance,
> Amasco
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sun Aug 27 09:43:18 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Aug 2006 08:43:18 +0100 (BST)
Subject: [R] Permanently changing gui preferences
In-Reply-To: <44F0CEDF.8010902@education.wisc.edu>
References: <44F0CEDF.8010902@education.wisc.edu>
Message-ID: <Pine.LNX.4.64.0608270837340.6622@gannet.stats.ox.ac.uk>

On Sat, 26 Aug 2006, David Kaplan wrote:

> Greetings,
> 
> I made changes to my gui preferences and saved them.  When I close and 
> then open R, it reverts back to default preferences.  How do I 
> permanently change gui preferences?

Which 'gui' is this?

If it is Rgui under Windows, the 'GUI preferences' dialog box has a 'Save' 
button.  You need to save to a file Rconsole somewhere that will be used
(see the rw-FAQ Q5.2 and ?Rconsole).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Sun Aug 27 14:40:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 27 Aug 2006 08:40:55 -0400
Subject: [R] Modifying the embed-results
In-Reply-To: <fcc1b1aa7f3e.44f15649@utu.fi>
References: <1156526634.12112.13.camel@attenka>
	<971536df0608251825v200418dfxab19e22b36283887@mail.gmail.com>
	<f894d958a06.44f010cb@utu.fi>
	<971536df0608260524w5a718f4fk11e696946ce2a905@mail.gmail.com>
	<fcc1b1aa7f3e.44f15649@utu.fi>
Message-ID: <971536df0608270540o3f23425co623d0eb0966a4343@mail.gmail.com>

On 8/27/06, Atte Tenkanen <attenka at utu.fi> wrote:
> Thanks!
> I see, that do.call-function is used often in R-algorithms...  Looking over some extra do.call-examples seems useful. This tail-function is also new for me.
>
> Is there some reason to use seq(along = VECTOR) instead of 1:length(VECTOR)?

If length(VECTOR) is 0 then 1:length(VECTOR) gives c(1,0) but
seq(along = VECTOR) gives a zero length vector.

> Atte
>
> > You can replace the for with lapply like this:
> >
> > VECTOR <- c(0, 3, 6, 3, 11, 2, 11, 4, 3, 4, 7, 7, 6, 4, 8)
> > f <- function(i) unique(tail(VECTOR, length(VECTOR)-i+1))[1:5]
> > out <- do.call(rbind, lapply(seq(along = VECTOR), f))
> > na.omit(rbind(rep(NA, 5), out))
> >
> > Note that  a matrix with zero rows is returned in the case
> > that VECTOR has zero length and in the case that VECTOR
> > has fewer than 5 unique elements.
> >
> >
> > On 8/26/06, Atte Tenkanen <attenka at utu.fi> wrote:
> > > Again my example was't very clear: there were not enough same
> > numbers in the VECTOR.
> > > What I need is something like this:
> > >
> > > VECTOR<-c(0,3,6,3,11,2,11,4,3,4,7,7,6,4,8)
> > > MATRIX<-c()
> > > for(i in 1:length(VECTOR)){
> > >        v<-(unique(VECTOR[i:length(VECTOR)])[1:5])
> > >        MATRIX<-rbind(MATRIX,v)
> > > }
> > >
> > > MATRIX<-na.omit(MATRIX)
> > > MATRIX
> > >
> > > > data.frame(MATRIX, row.names=NULL)
> > >  X1 X2 X3 X4 X5
> > > 1       0  3  6 11  2
> > > 2       3  6 11  2  4
> > > 3       6  3 11  2  4
> > > 4       3 11  2  4  7
> > > 5      11  2  4  3  7
> > > 6       2 11  4  3  7
> > > 7      11  4  3  7  6
> > > 8       4  3  7  6  8
> > > 9       3  4  7  6  8
> > >
> > > So, there are no duplicates in rows.
> > > VECTOR is always scanned forward as long as the number of  items
> > (here 5) becomes full.
> > >
> > > Atte
> > >
> > > > Try:
> > > >
> > > > embed(VECTOR, 5)[,5:1]
> > > >
> > > > On 8/25/06, Atte Tenkanen <attenka at utu.fi> wrote:
> > > > > Hi,
> > > > >
> > > > > Here is a vector and the result from the embed-command:
> > > > >
> > > > > VECTOR=c(0,3,6,3,11,2,4,3,7,6,4,5,10,2,3,5,8)
> > > > >
> > > > > > embed(VECTOR, dimension=5)
> > > > >      [,1] [,2] [,3] [,4] [,5]
> > > > >  [1,]   11    3    6    3    0
> > > > >  [2,]    2   11    3    6    3
> > > > >  [3,]    4    2   11    3    6
> > > > >  [4,]    3    4    2   11    3
> > > > >  [5,]    7    3    4    2   11
> > > > >  [6,]    6    7    3    4    2
> > > > >  [7,]    4    6    7    3    4
> > > > >  [8,]    5    4    6    7    3
> > > > >  [9,]   10    5    4    6    7
> > > > > [10,]    2   10    5    4    6
> > > > > [11,]    3    2   10    5    4
> > > > > [12,]    5    3    2   10    5
> > > > > [13,]    8    5    3    2   10
> > > > >
> > > > > Is there a way to little modify the algorithm so that the result
> > > > looks> like this:
> > > > >
> > > > > [1]  0  3  6 11  2 <- beginning from the first number of the
> > VECTOR> > > [1]  3  6 11  2  4 <- beginning from the second number
> > of the
> > > > VECTOR etc
> > > > > [1]  6  3 11  2  4
> > > > > [1]  3 11  2  4  7
> > > > > [1] 11  2  4  3  7
> > > > > [1] 2 4 3 7 6
> > > > > [1] 4 3 7 6 5
> > > > > [1] 3 7 6 4 5
> > > > > [1]  7  6  4  5 10
> > > > > [1]  6  4  5 10  2
> > > > > [1]  4  5 10  2  3
> > > > > [1]  5 10  2  3  8
> > > > > [1] 10  2  3  5  8
> > > > >
> > > > > Every row consists of next five unique(!) member of the
> > VECTOR. I
> > > > made> this example result with a time consuming algorithm which
> > > > uses for-loops
> > > > > and whiles.
> > > > >
> > > > > How to do this better??
> > > > >
> > > > > Thanks in advance!
> > > > >
> > > > > Atte Tenkanen
> > > > > University of Turku
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide http://www.R-
> > project.org/posting-
> > > > guide.html> and provide commented, minimal, self-contained,
> > > > reproducible code.
> > > > >
> > > >
> > >
> >
>


From jfox at mcmaster.ca  Sun Aug 27 15:51:57 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 27 Aug 2006 09:51:57 -0400
Subject: [R] Type II and III sum of square in Anova (R, car package)
In-Reply-To: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
Message-ID: <20060827135157.QPGG24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Amasco,

A complete explanation of the issues that you raise is awkward in an email,
so I'll address your questions briefly. Section 8.2 of my text, Applied
Regression Analysis, Linear Models, and Related Methods (Sage, 1997) has a
detailed discussion.

(1) In balanced designs, so-called "Type I," "II," and "III" sums of squares
are identical. If the STATA manual says that Type II tests are only
appropriate in balanced designs, then that doesn't make a whole lot of sense
(unless one believes that Type-II tests are nonsense, which is not the
case).

(2) One should concentrate not directly on different "types" of sums of
squares, but on the hypotheses to be tested. Sums of squares and F-tests
should follow from the hypotheses. Type-II and Type-III tests (if the latter
are properly formulated) test hypotheses that are reasonably construed as
tests of main effects and interactions in unbalanced designs. In unbalanced
designs, Type-I sums of squares usually test hypotheses of interest only by
accident. 

(3) Type-II sums of squares are constructed obeying the principle of
marginality, so the kinds of contrasts employed to represent factors are
irrelevant to the sums of squares produced. You get the same answer for any
full set of contrasts for each factor. In general, the hypotheses tested
assume that terms to which a particular term is marginal are zero. So, for
example, in a three-way ANOVA with factors A, B, and C, the Type-II test for
the AB interaction assumes that the ABC interaction is absent, and the test
for the A main effect assumes that the ABC, AB, and AC interaction are
absent (but not necessarily the BC interaction, since the A main effect is
not marginal to this term). A general justification is that we're usually
not interested, e.g., in a main effect that's marginal to a nonzero
interaction.

(4) Type-III tests do not assume that terms higher-order to the term in
question are zero. For example, in a two-way design with factors A and B,
the type-III test for the A main effect tests whether the population
marginal means at the levels of A (i.e., averaged across the levels of B)
are the same. One can test this hypothesis whether or not A and B interact,
since the marginal means can be formed whether or not the profiles of means
for A within levels of B are parallel. Whether the hypothesis is of interest
in the presence of interaction is another matter, however. To compute
Type-III tests using incremental F-tests, one needs contrasts that are
orthogonal in the row-basis of the model matrix. In R, this means, e.g.,
using contr.sum, contr.helmert, or contr.poly (all of which will give you
the same SS), but not contr.treatment. Failing to be careful here will
result in testing hypotheses that are not reasonably construed, e.g., as
hypotheses concerning main effects.

(5) The same considerations apply to linear models that include quantitative
predictors -- e.g., ANCOVA. Most software will not automatically produce
sensible Type-III tests, however.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Amasco 
> Miralisus
> Sent: Saturday, August 26, 2006 5:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Type II and III sum of square in Anova (R, car package)
> 
> Hello everybody,
> 
> I have some questions on ANOVA in general and on ANOVA in R 
> particularly.
> I am not Statistician, therefore I would be very appreciated 
> if you answer it in a simple way.
> 
> 1. First of all, more general question. Standard anova() 
> function for lm() or aov() models in R implements Type I sum 
> of squares (sequential), which is not well suited for 
> unbalanced ANOVA. Therefore it is better to use
> Anova() function from car package, which was programmed by 
> John Fox to use Type II and Type III sum of squares. Did I 
> get the point?
> 
> 2. Now more specific question. Type II sum of squares is not 
> well suited for unbalanced ANOVA designs too (as stated in 
> STATISTICA help), therefore the general rule of thumb is to 
> use Anova() function using Type II SS only for balanced ANOVA 
> and Anova() function using Type III SS for unbalanced ANOVA? 
> Is this correct interpretation?
> 
> 3. I have found a post from John Fox in which he wrote that 
> Type III SS could be misleading in case someone use some 
> contrasts. What is this about?
> Could you please advice, when it is appropriate to use Type 
> II and when Type III SS? I do not use contrasts for 
> comparisons, just general ANOVA with subsequent Tukey 
> post-hoc comparisons.
> 
> Thank you in advance,
> Amasco
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From goedman at mac.com  Sun Aug 27 01:38:55 2006
From: goedman at mac.com (Rob J Goedman)
Date: Sat, 26 Aug 2006 16:38:55 -0700
Subject: [R] Importing data from clipboard on Mac OSX
In-Reply-To: <51028E84-5275-4F9E-86F7-5B7504C2AEA7@student.ru.nl>
References: <51028E84-5275-4F9E-86F7-5B7504C2AEA7@student.ru.nl>
Message-ID: <2E90F36F-CA5D-4494-990B-9731F8618CDA@mac.com>

Hi Rense,

Not sure how robust this is, but maybe to get you started:

In R console:

 > a <- 1:10

Copy that line to or part of it to the clipboard.

 > system("osascript -e 'set y to the clipboard' -e 'tell application  
\"R.app\" to cmd y'")

If course you could use this in an R function. I would opt for using  
a file to read in the data.

As this is Mac specific, maybe a better list is R-SIG-Mac.

Rob


On Aug 26, 2006, at 4:00 PM, Rense Nieuwenhuis wrote:

> Dear R users,
>
> I am trying to get data from the clipboard into R on MacOSX. I tried
> the following, but got an error message:
>
> read.delim("clipboard")
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> unable to contact X11 display
>
> Obviously, I'm not running R using X11. I'm wondering, can I import
> data from the clipboard on MacosX?
>
> Thanks in advance,
>
> Rense Nieuwenhuis
>
>> version
>                 _
> platform       powerpc-apple-darwin8.6.0
> arch           powerpc
> os             darwin8.6.0
> system         powerpc, darwin8.6.0
> status
> major          2
> minor          3.0
> year           2006
> month          04
> day            24
> svn rev        37909
> language       R
> version.string Version 2.3.0 (2006-04-24)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From liuwensui at gmail.com  Sun Aug 27 18:02:50 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 27 Aug 2006 12:02:50 -0400
Subject: [R] refer to objects with sequential names
Message-ID: <1115a2b00608270902p72d69b6bw929fe81f100f74fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060827/a423469d/attachment.pl 

From renaud.lancelot at gmail.com  Sun Aug 27 18:32:26 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sun, 27 Aug 2006 18:32:26 +0200
Subject: [R] refer to objects with sequential names
In-Reply-To: <1115a2b00608270902p72d69b6bw929fe81f100f74fe@mail.gmail.com>
References: <1115a2b00608270902p72d69b6bw929fe81f100f74fe@mail.gmail.com>
Message-ID: <c2ee56800608270932h416e2627pd206843eb9c58121@mail.gmail.com>

Assuming newdata is the same for all the models and is included in a
data.frame called New:

list_glm <- paste("glm", 1:10, sep = "")
sapply(list_glm, function(x) predict(get(x), newdata = New))

Best,

Renaud


2006/8/27, Wensui Liu <liuwensui at gmail.com>:
> Dear Listers,
>
> If I have several glm objects with names glm1, glm2.... and want to apply
> new data to these objects. Instead of typing "predict(glm1, newdata)..." 100
> times, is there way I could do so in a loop?
>
> Thank you so much!
>
> wensui
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From dieter.menne at menne-biomed.de  Sun Aug 27 18:46:47 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 27 Aug 2006 16:46:47 +0000 (UTC)
Subject: [R] S4 documentation -  Was: "exact Wilcoxon signed rank test "
References: <44EEAFEA.4050301@gmx.net>
	<Pine.LNX.4.64.0608251145040.28525@imbe153.imbe.med.uni-erlangen.de>
Message-ID: <loom.20060827T182327-131@post.gmane.org>

Torsten Hothorn <Torsten.Hothorn <at> rzmail.uni-erlangen.de> writes:

... 
> indeed, this is the only gap to be filled in `coin', I just haven't had 
> the time to implement this. And of course, `exactRankTests' is still 
> available and will be available in the future. So you _can_ use it!
....
 
Torsten, and R-Coders,

"coin" is really a great step forward, but looking at the documentation of both
"exactRankTest" and "coin" together shows one bad thing about S4 documentation: 

You can't find the important things any more. (not your fault, Torsten)

The functions you are going to need are drowned in a dozen "initialize" and
other stuff that is irrelevant for those who only want to use do the tests. It's
similar in nlme/lme4(currently documented in matrix). Normally I just try
print.xx and plot.xx. When I really have problems with some specialized
versions, I look into the code, because the documentation rarely give the right
answers for special cases.

Therefore, I would like to propose that S4 documentation should be conditionally
made available in an condensed form. Since in C++ code one probably would mark
these secondary functions as "protected", I propose a keyword "protected" in the
.rd files to mark those parts that should only be included in the "full"
documentation.

(For fear of being rip...ed into pieces, going off for vacation now).

Dieter Menne


From phhs80 at gmail.com  Sun Aug 27 18:52:15 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Sun, 27 Aug 2006 17:52:15 +0100
Subject: [R] Can R compute the expected value of a random variable?
In-Reply-To: <46a360560608261204k1ffd0d62y570b2d7b8ab3d310@mail.gmail.com>
References: <6ade6f6c0608261201w28afd112kabca3bd902564da4@mail.gmail.com>
	<46a360560608261204k1ffd0d62y570b2d7b8ab3d310@mail.gmail.com>
Message-ID: <6ade6f6c0608270952m3a42d51fh9be8151986c8b48f@mail.gmail.com>

On 8/26/06, Mike Nielsen <mr.blacksheep at gmail.com> wrote:
> Yes.
>
> > Can R compute the expected value of a random variable?

Mike: thank you very much indeed for your so insightful and complete
answer. I have  meanwhile deepened my research and, as a consequence,
I have found the following solution, which seems to work fine:

> integrand <- function(x){x*dlnorm(x,meanlog=2,sdlog=3)}
> integrate(integrand,-Inf, Inf)
665.146 with absolute error < 0.046
>

There is also a package apt to calculate expected values: it is called
distrEx. (Thanks, Matthias.)

Paul


From roger at ysidro.econ.uiuc.edu  Sun Aug 27 19:17:57 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 27 Aug 2006 12:17:57 -0500
Subject: [R] Can R compute the expected value of a random variable?
In-Reply-To: <6ade6f6c0608270952m3a42d51fh9be8151986c8b48f@mail.gmail.com>
References: <6ade6f6c0608261201w28afd112kabca3bd902564da4@mail.gmail.com>
	<46a360560608261204k1ffd0d62y570b2d7b8ab3d310@mail.gmail.com>
	<6ade6f6c0608270952m3a42d51fh9be8151986c8b48f@mail.gmail.com>
Message-ID: <A8AB6A50-40A2-4716-B88C-7F883C72250C@ysidro.econ.uiuc.edu>

General questions elicit general answers; more specific questions
elicit more specific answers.    For example,

 > exp(2+9/2)
[1] 665.1416

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Aug 27, 2006, at 11:52 AM, Paul Smith wrote:

> On 8/26/06, Mike Nielsen <mr.blacksheep at gmail.com> wrote:
>> Yes.
>>
>>> Can R compute the expected value of a random variable?
>
> Mike: thank you very much indeed for your so insightful and complete
> answer. I have  meanwhile deepened my research and, as a consequence,
> I have found the following solution, which seems to work fine:
>
>> integrand <- function(x){x*dlnorm(x,meanlog=2,sdlog=3)}
>> integrate(integrand,-Inf, Inf)
> 665.146 with absolute error < 0.046
>>
>
> There is also a package apt to calculate expected values: it is called
> distrEx. (Thanks, Matthias.)
>
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Sun Aug 27 19:24:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 27 Aug 2006 13:24:14 -0400
Subject: [R] Can R compute the expected value of a random variable?
In-Reply-To: <6ade6f6c0608270952m3a42d51fh9be8151986c8b48f@mail.gmail.com>
References: <6ade6f6c0608261201w28afd112kabca3bd902564da4@mail.gmail.com>
	<46a360560608261204k1ffd0d62y570b2d7b8ab3d310@mail.gmail.com>
	<6ade6f6c0608270952m3a42d51fh9be8151986c8b48f@mail.gmail.com>
Message-ID: <971536df0608271024u6abeff99s58856a2d029a15ac@mail.gmail.com>

On 8/27/06, Paul Smith <phhs80 at gmail.com> wrote:
> On 8/26/06, Mike Nielsen <mr.blacksheep at gmail.com> wrote:
> > Yes.
> >
> > > Can R compute the expected value of a random variable?
>
> Mike: thank you very much indeed for your so insightful and complete
> answer.

Answers are often in proportion to the questions that solicited them.
Suggest you read the posting guide (link near bottom of every message
to r-help) or at least read the advice in the last line on every message to
r-help.


From r.nieuwenhuis at student.ru.nl  Sun Aug 27 21:13:26 2006
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Sun, 27 Aug 2006 21:13:26 +0200
Subject: [R] Total (un)standardized effects in SEM?
In-Reply-To: <20060825143032.XEPD13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20060825143032.XEPD13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <01004B52-E59A-4A4A-98D4-B618B32AE2E1@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060827/4894d78d/attachment.pl 

From ritwik.sinha at gmail.com  Mon Aug 28 00:54:05 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sun, 27 Aug 2006 18:54:05 -0400
Subject: [R] covariance matrix of predictions
In-Reply-To: <1156354108.c7f66dbcarnab@myrealbox.com>
References: <1156354108.c7f66dbcarnab@myrealbox.com>
Message-ID: <42bc98300608271554o210a773s77c62fe148f62c3b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060827/c97b89ce/attachment.pl 

From jfox at mcmaster.ca  Mon Aug 28 01:16:12 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 27 Aug 2006 19:16:12 -0400
Subject: [R] Total (un)standardized effects in SEM?
In-Reply-To: <01004B52-E59A-4A4A-98D4-B618B32AE2E1@student.ru.nl>
Message-ID: <20060827231613.RAKE13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Rense,

I already wrote a function, appended below, to compute effects for SEMs, as
a method for the effects() generic. I've been thinking about adding this to
the sem package, possibly with asymptotic standard errors computed by the
delta method, as suggested by Michael Sobel. If I do so, I can also
incorporate standardized effects.

As I mentioned I have serious doubts about the utility of these ideas, and
worry about encouraging them, but I guess that I could say the same thing
for SEMs in general. My original purpose in writing the sem package was to
have something in R that I use in teaching.

Regards,
 John

---------- snip ---------------

effects.sem <- function(object, ...){
    A <- object$A
    exog <- apply(A, 1, function(x) all(x == 0))
    endog <- !exog
    B <- A[endog, endog, drop=FALSE]
    C <- A[endog, exog, drop=FALSE]
    I <- diag(nrow(B))
    IBinv <- solve(I - B)
    TotEndog <- IBinv - I
    TotExog <- IBinv %*% C
    result <- list(B=B, C=C, TotEndog=TotEndog, TotExog=TotExog)
    class(result) <- "semEffect"
    result
    }
    
print.semEffect <- function(x, digits=5, ...){
    B <- x$B
    C <- x$C
    cat("\nDirect Effects of Exogenous on Endogenous Variables:\n")
    print(C, digits=digits)
    cat("\n\nDirect Effects of Endogenous Variables on Each Other:\n")
    print(B, digits=digits)
    cat("\n\nIndirect Effects of Exogenous on Endogenous Variables:\n")
    print(x$TotExog - C, digits=digits)
    cat("\n\nIndirect Effects of Endogenous Variables on Each Other:\n")
    print(x$TotEndog - B, digits=digits)
    cat("\n\nTotal Effects of Exogenous on Endogenous Variables:\n")
    print(x$TotExog, digits=digits)
    cat("\n\nTotal Effects of Endogenous Variables on Each Other:\n")
    print(x$TotEndog, digits=digits)
    invisible(x)
    }
        
--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rense 
> Nieuwenhuis
> Sent: Sunday, August 27, 2006 2:13 PM
> To: r-help at stat.math.ethz.ch
> Cc: John Fox
> Subject: Re: [R] Total (un)standardized effects in SEM?
> 
> Dear John,
> 
> thank you very much for your reply. The suggestions you make 
> for calculating the direct and indirect effects are exactly 
> what I was looking for. Although I'm very new to SEM and not 
> at all very experienced in R, I tried to put them together in 
> a function (called
> decomp) and expanded it to be able to calculate standardized 
> effects as well. For that, I made a few changes to your
> standardized.coefficients() and added the little function std.matrix
> () which converts the output of standardized.coefficients. 
> It's not a very coherent set of functions, but it works (for 
> now). I have performed a preliminary test using LISREL. The 
> results where identical.
> 
> Again, I would like to thank you for the reply and the work 
> on the SEM package.
> 
> With highest regards,
> 
> Rense Nieuwenhuis
> 
> 
> (Note: The syntax below consists of three functions. The 
> first one decomposes SEM-objects. It needs the other two 
> functions to work. I'm sure this can be programmed a lot 
> cleaner, but for now, it serves my
> needs.)
> 
> 
> 
> decomp <- function(x, std=FALSE)
> 	{
> 		if(std==FALSE)
> 			{
> 				A <- x$A			
> 			# unstandardized structural coefficients
> 			}
> 		
> 		if(std==TRUE)
> 			{
> 				A <- std.matrix(std.coef(x))	
> # standardized structural coefficients
> 			}
> 	
> 	exog <- apply(A, 1, function(x) all(x == 0))
> 	endog <- !exog
> 
> 	B <- A[endog, endog, drop=FALSE]  			
> # direct effects, endogenous ->  
> endogenous
> 	C <- A[endog, exog, drop=FALSE]				
> # direct effects, exogenous ->  
> endogenous
> 
> 	I <- diag(nrow(B))
> 	IBinv <- solve(I - B)
> 		
> 	total.endo.endo <- IBinv - I				
> # total effects, endogenous ->  
> endogenous
> 	total.exo.endo <- IBinv %*% C				
> # total effects, exogenous ->  
> endogenous
> 	ind.endo.endo <- total.endo.endo - B  		# 
> indirect effects,  
> endogenous -> endogenous
> 	ind.exo.endo <- total.exo.endo - C 			
> # indirect effects, exogenous - 
>  > endogenous
> 	
> 	temp <- list(	total.endo.endo = total.endo.endo,
> 					total.exo.endo = total.exo.endo,
> 					ind.endo.endo = ind.endo.endo,
> 					ind.exo.endo = ind.exo.endo)
> 	
> 	return (temp)
>        	
> 	}
> 
> 
> 
> std.matrix <- function(x)
> 	{
> 		i <- length(x$D)
> 		ii <- length(x$A)
> 		zero <- rep(0,i^2)
> 		
> 		temp.matrix <- matrix(zero,nrow=i,ncol=i)
> 		colnames(temp.matrix) <- x$D
> 		rownames(temp.matrix) <- x$D
> 		
> 		for (t in c(1:ii))
> 			{
> 				temp.matrix[x$A[t],x$B[t]] <- x$C[t,1]
> 			}
> 		
> 	return(temp.matrix)			
> 	}
> 
> 
> 
> 
> std.coef <- function(object, digits=5)
> {
>      old.digits <- options(digits = digits)
>      on.exit(options(old.digits))
>      P <- object$P
>      A <- object$A
>      t <- object$t
>      par <- object$coeff
>      par.posn <- object$par.posn
>      IAinv <- solve(diag(nrow(A)) - A)
>      C <- IAinv %*% P %*% t(IAinv)
>      ram <- object$ram
>      par.names <- rep(" ", nrow(ram))
>      for (i in 1:t) {
>          which.par <- ram[, 4] == i
>          ram[which.par, 5] <- par[i]
>          par.names[which.par] <- names(par)[i]
>      }
>      one.head <- ram[, 1] == 1
>      coeff <- ram[one.head, 5]
>      coeff <- coeff * sqrt(diag(C[ram[one.head, 3], ram[one.head,
>          3]])/diag(C[ram[one.head, 2], ram[one.head, 2]]))
>      var.names <- rownames(A)
>      par.code <- paste(var.names[ram[one.head, 2]], c("<---",
>          "<-->")[ram[one.head, 1]], var.names[ram[one.head, 3]])
>      coeff <- data.frame(par.names[one.head], coeff, par.code)
>      names(coeff) <- c(" ", "Std. Estimate", " ")
> 
>      ## From here on added / changed by me
>      ## print(coeff, rowlab = rep(" ", nrow(coeff)), right = FALSE)
> 
>      temp <- list(	A = var.names[ram[one.head,2]],
>      				B = var.names[ram[one.head,3]],
>      				C = coeff[2],
>      				D = colnames(object$A) )
>      return(temp)
> }
> 
> 
> 
> 
> 
> On Aug 25, 2006, at 16:30 , John Fox wrote:
> 
> > Dear Rense,
> >
> > (This question was posted a few days ago when I wasn't reading my
> > email.)
> >
> > So-called effect decompositions are simple functions of the 
> structural 
> > coefficients of the model, which in a model fit by sem() 
> are contained 
> > in the $A component of the returned object. (See ?sem.) One 
> approach, 
> > therefore, would be to put the coefficients in the appropriate 
> > locations of the estimated Beta, Gamma, Lamda-x, and 
> Lambda-y matrices 
> > of the LISREL model, and then to compute the "effects" in the usual 
> > manner.
> >
> > It should be possible to do this for the RAM formulation of 
> the model 
> > as well, simply by distinguishing exogenous from endogenous 
> variables.
> > Here's
> > an illustration using model C in the LISREL 7 Manual, pp. 
> 169-177, for 
> > the Wheaton et al. "stability of alienation" data (a common 
> example--I 
> > happen to have an old LISREL manual handy):
> >
> >> S.wh <- matrix(c(
> > +    11.834,     0,        0,        0,       0,        0,
> > +     6.947,    9.364,     0,        0,       0,        0,
> > +     6.819,    5.091,   12.532,     0,       0,        0,
> > +     4.783,    5.028,    7.495,    9.986,    0,        0,
> > +    -3.839,   -3.889,   -3.841,   -3.625,   9.610,     0,
> > +    -2.190,   -1.883,   -2.175,   -1.878,   3.552,    4.503),
> > +   6, 6)
> >>
> >> rownames(S.wh) <- colnames(S.wh) <-
> > +     c
> > 
> ('Anomia67','Powerless67','Anomia71','Powerless71','Education','SEI')
> >
> >>
> >> model.wh <- specify.model()
> > 1:     Alienation67   ->  Anomia67,      NA,     1
> > 2:     Alienation67   ->  Powerless67,   lam1,   NA
> > 3:     Alienation71   ->  Anomia71,      NA,     1
> > 4:     Alienation71   ->  Powerless71,   lam2,   NA
> > 5:     SES            ->  Education,     NA,     1
> > 6:     SES            ->  SEI,           lam3,   NA
> > 7:     SES            ->  Alienation67,  gam1,   NA
> > 8:     Alienation67   ->  Alienation71,  beta,   NA
> > 9:     SES            ->  Alienation71,  gam2,   NA
> > 10:     Anomia67       <-> Anomia67,      the1,   NA
> > 11:     Anomia71       <-> Anomia71,      the3,   NA
> > 12:     Powerless67    <-> Powerless67,   the2,   NA
> > 13:     Powerless71    <-> Powerless71,   the4,   NA
> > 14:     Education      <-> Education,     thd1,   NA
> > 15:     SEI            <-> SEI,           thd2,   NA
> > 16:     Anomia67       <-> Anomia71,      the13,  NA
> > 17:     Alienation67   <-> Alienation67,  psi1,   NA
> > 18:     Alienation71   <-> Alienation71,  psi2,   NA
> > 19:     SES            <-> SES,           phi,    NA
> > 20:
> > Read 19 records
> >>
> >> sem.wh <- sem(model.wh, S.wh, 932)
> >>
> >> summary(sem.wh)
> >
> >  Model Chisquare =  6.3349   Df =  5 Pr(>Chisq) = 0.27498
> >  Chisquare (null model) =  17973   Df =  15
> >  Goodness-of-fit index =  0.99773
> >  Adjusted goodness-of-fit index =  0.99046
> >  RMSEA index =  0.016934   90 % CI: (NA, 0.05092)
> >  Bentler-Bonnett NFI =  0.99965
> >  Tucker-Lewis NNFI =  0.99978
> >  Bentler CFI =  0.99993
> >  BIC =  -27.852
> >
> >  Normalized Residuals
> >      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
> > -9.57e-01 -1.34e-01 -4.24e-02 -9.17e-02  6.43e-05  5.47e-01
> >
> >  Parameter Estimates
> >       Estimate Std Error z value  Pr(>|z|)
> > lam1   1.02656 0.053424   19.2152 0.0000e+00 Powerless67 <---  
> > Alienation67
> > lam2   0.97089 0.049608   19.5712 0.0000e+00 Powerless71 <---  
> > Alienation71
> > lam3   0.51632 0.042247   12.2214 0.0000e+00 SEI <--- SES
> > gam1  -0.54981 0.054298  -10.1258 0.0000e+00 Alienation67 <--- SES
> > beta   0.61732 0.049486   12.4746 0.0000e+00 Alienation71 <---  
> > Alienation67
> > gam2  -0.21151 0.049862   -4.2419 2.2164e-05 Alienation71 <--- SES
> > the1   5.06546 0.373464   13.5635 0.0000e+00 Anomia67 <--> Anomia67
> > the3   4.81176 0.397345   12.1098 0.0000e+00 Anomia71 <--> Anomia71
> > the2   2.21438 0.319740    6.9256 4.3423e-12 Powerless67 <-->  
> > Powerless67
> > the4   2.68322 0.331274    8.0997 4.4409e-16 Powerless71 <-->  
> > Powerless71
> > thd1   2.73051 0.517737    5.2739 1.3353e-07 Education <--> 
> Education
> > thd2   2.66905 0.182260   14.6442 0.0000e+00 SEI <--> SEI
> > the13  1.88739 0.241627    7.8112 5.7732e-15 Anomia71 <--> Anomia67
> > psi1   4.70477 0.427511   11.0050 0.0000e+00 Alienation67 <-->  
> > Alienation67
> > psi2   3.86642 0.343971   11.2406 0.0000e+00 Alienation71 <-->  
> > Alienation71
> > phi    6.87948 0.659208   10.4360 0.0000e+00 SES <--> SES
> >
> >  Iterations =  58
> >>
> >> A <- sem.wh$A  # structural coefficients exog <- apply(A, 1, 
> >> function(x) all(x == 0)) endog <- !exog
> >
> >> (B <- A[endog, endog, drop=FALSE])  # direct effects, endogenous ->
> > endogenous
> >              Anomia67 Powerless67 Anomia71 Powerless71 Education SEI
> > Anomia67            0           0        0           0         0   0
> > Powerless67         0           0        0           0         0   0
> > Anomia71            0           0        0           0         0   0
> > Powerless71         0           0        0           0         0   0
> > Education           0           0        0           0         0   0
> > SEI                 0           0        0           0         0   0
> > Alienation67        0           0        0           0         0   0
> > Alienation71        0           0        0           0         0   0
> >              Alienation67 Alienation71
> > Anomia67        1.0000000     0.000000
> > Powerless67     1.0265597     0.000000
> > Anomia71        0.0000000     1.000000
> > Powerless71     0.0000000     0.970892
> > Education       0.0000000     0.000000
> > SEI             0.0000000     0.000000
> > Alienation67    0.0000000     0.000000
> > Alienation71    0.6173153     0.000000
> >
> >> (C <- A[endog, exog, drop=FALSE]) # direct effects, exogenous ->
> > endogenous
> >                     SES
> > Anomia67      0.0000000
> > Powerless67   0.0000000
> > Anomia71      0.0000000
> > Powerless71   0.0000000
> > Education     1.0000000
> > SEI           0.5163168
> > Alienation67 -0.5498096
> > Alienation71 -0.2115088
> >
> >> I <- diag(nrow(B))
> >> IBinv <- solve(I - B)
> >> (Ty <- IBinv - I)  # total effects, endogenous -> endogenous
> >              Anomia67 Powerless67 Anomia71 Powerless71 Education SEI
> > Anomia67            0           0        0           0         0   0
> > Powerless67         0           0        0           0         0   0
> > Anomia71            0           0        0           0         0   0
> > Powerless71         0           0        0           0         0   0
> > Education           0           0        0           0         0   0
> > SEI                 0           0        0           0         0   0
> > Alienation67        0           0        0           0         0   0
> > Alienation71        0           0        0           0         0   0
> >              Alienation67 Alienation71
> > Anomia67        1.0000000     0.000000
> > Powerless67     1.0265597     0.000000
> > Anomia71        0.6173153     1.000000
> > Powerless71     0.5993465     0.970892
> > Education       0.0000000     0.000000
> > SEI             0.0000000     0.000000
> > Alienation67    0.0000000     0.000000
> > Alienation71    0.6173153     0.000000
> >
> >> (Tx <- IBinv %*% C) # total effects, exogenous -> endogenous
> >                     SES
> > Anomia67     -0.5498096
> > Powerless67  -0.5644124
> > Anomia71     -0.5509147
> > Powerless71  -0.5348786
> > Education     1.0000000
> > SEI           0.5163168
> > Alienation67 -0.5498096
> > Alienation71 -0.5509147
> >
> >> Ty - B  # indirect effects, endogenous -> endogenous
> >              Anomia67 Powerless67 Anomia71 Powerless71 Education SEI
> > Anomia67            0           0        0           0         0   0
> > Powerless67         0           0        0           0         0   0
> > Anomia71            0           0        0           0         0   0
> > Powerless71         0           0        0           0         0   0
> > Education           0           0        0           0         0   0
> > SEI                 0           0        0           0         0   0
> > Alienation67        0           0        0           0         0   0
> > Alienation71        0           0        0           0         0   0
> >              Alienation67 Alienation71
> > Anomia67        0.0000000            0
> > Powerless67     0.0000000            0
> > Anomia71        0.6173153            0
> > Powerless71     0.5993465            0
> > Education       0.0000000            0
> > SEI             0.0000000            0
> > Alienation67    0.0000000            0
> > Alienation71    0.0000000            0
> >
> >> Tx - C # indirect effects, exogenous -> endogenous
> >                     SES
> > Anomia67     -0.5498096
> > Powerless67  -0.5644124
> > Anomia71     -0.5509147
> > Powerless71  -0.5348786
> > Education     0.0000000
> > SEI           0.0000000
> > Alienation67  0.0000000
> > Alienation71 -0.3394059
> >
> > These results agree with those in the LISREL manual (and 
> for another 
> > example there as well), but I haven't checked the method carefully.
> >
> > It would, of course, be simple to encapsulate the steps above in a 
> > function, but here's a caveat: The idea of indirect and 
> total effects 
> > makes sense to me for a recursive model, and for the exogenous 
> > variables in a nonrecursive model, where they are the reduced-form 
> > coefficients (supposing, of course, that the model makes 
> sense in the 
> > first place, which is often problematic), but not for the 
> endogenous 
> > variables in a nonrecursive model. That is why I haven't put such a 
> > function in the sem package; perhaps I should reconsider.
> >
> > Having said that, I'm ashamed to add that I believe that I was the 
> > person who suggested the definition of total and indirect effects 
> > currently used for these models.
> >
> > Finally, you can get standardized effects similarly by using 
> > standardized structural coefficients. In the sem package, these are 
> > computed and printed by standardized.eoefficients(). This function 
> > doesn't return the standardized A matrix in a usable form, 
> but could 
> > be made to do so.
> >
> > Regards,
> >  John
> >
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> >
> >
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ritwik.sinha at gmail.com  Mon Aug 28 02:18:38 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Sun, 27 Aug 2006 20:18:38 -0400
Subject: [R] two density curves in one plot?
In-Reply-To: <971536df0608271641j6c341eana6fc11239f37731f@mail.gmail.com>
References: <44EC37F8.4020907@yahoo.de>
	<971536df0608230737n5829f384r8a63951275e79830@mail.gmail.com>
	<42bc98300608271617s5c53dc4ck62aef258455e9bfa@mail.gmail.com>
	<971536df0608271641j6c341eana6fc11239f37731f@mail.gmail.com>
Message-ID: <42bc98300608271718u45b2d1c9o2dece46cd8acd048@mail.gmail.com>

Hi Gabor and Dimitris,

I was wondering if this question was frequent enough to be in the R
FAQ under R Miscellanea and thought of something like this

Q. How do I plot two curves on the same graph?

A. Plot the first curve using the plot() command and add lines using
lines(). For example

d1 <- density(rnorm(100))
d2 <- density(rnorm(100))
plot(range(d1$x, d2$x), range(d1$y, d2$y), type = "n", xlab = "x",
ylab = "Density")
lines(d1, col = "red")
lines(d2, col = "blue")

Alternatively one can use points() to add points to the plot.

If you think this question should be in the FAQ and if you have any
comments/changes to the QA then I can request the maintainer of the
FAQ to include it. We could also include a lattice solution but I was
thinking of not complicating things.

Ritwik Sinha

On 8/27/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Not sure who maintains the FAQ but its not me.
>
>
> On 8/27/06, Ritwik Sinha <ritwik.sinha at gmail.com> wrote:
> > This seems to be a common question for new commers to R, does it make sense
> > to add it to the R FAQ page? I checked it is not currently there.
> >
> > Ritwik
> >
> >
> > On 8/23/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > >
> > With lattice graphics:
> >
> > library(lattice)
> > d1 <- rnorm(100)
> > d2 <- runif(100)
> > densityplot(~ d1 + d2, auto.key = TRUE)
> >
> > On 8/23/06, Antje <niederlein-rstat at yahoo.de > wrote:
> > > Hello,
> > >
> > > I was wondering if I can plot two curves I get from "density(data)" into
> > > one plot. I want to compare both.
> > > With the following commad, I just get one curve plotted:
> > >
> > > plot( density(mydata) )
> > >
> > > Sorry for this stupid question but I could not find a solution until
> > now...
> > >
> > > Antje
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> >
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > --
> > Ritwik Sinha
> > Graduate Student
> > Epidemiology and Biostatistics
> > Case Western Reserve University
> >
> > http://darwin.cwru.edu/~rsinha
>



-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha


From fhm at unimelb.edu.au  Mon Aug 28 06:56:57 2006
From: fhm at unimelb.edu.au (Felicity Meakins)
Date: Mon, 28 Aug 2006 14:56:57 +1000
Subject: [R] Download problems
Message-ID: <C118B4B9.41FF%fhm@unimelb.edu.au>

Hallo,

I am a new user of 'R'. I have been trying to download it onto my Mac (OSX
2.8). For some reason it seems to download ok and look ok but then it
doesn't actually open. Can somebody help me this?

Felicity


From rfrancois at mango-solutions.com  Mon Aug 28 08:23:28 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Mon, 28 Aug 2006 07:23:28 +0100
Subject: [R] Firefox extension fo "R Site Search"
Message-ID: <44F28BE0.3050807@mango-solutions.com>

Dear useRs,

=================================================
Search for your R routines directly from Firefox!
 >> http://addictedtor.free.fr/rsitesearch
=================================================

Mango Solutions, providers of R and S-Plus consulting, application 
development and training, are happy to announce the first release of the 
R Site Search extension for Mozilla Firefox. This extension enables you 
to search from Jonathan Baron's R Site Search 
(http://search.r-project.org) in a convenient way.

It will enable you to conveniently browse or search the content of the 
packages listed in Jon's server.
Browsing is arranged in a 'package/documentation file' tree. Each hit is 
accompanied with a small icon which highlights the type of found object. 
This is useful as you can quickly find which page is refers to a 
dataset, clustering, time series etc...

Searching builds a new tree transparently on http://search.r-project.org 
and it shows only the relevant pages which may be viewed from the tree 
itself. Additionally the extension handles filters on the package names. 
Some examples are given on the homepage but I'm sure there are already 
regex-friendly people around with more ideas!


Comments, Feedbacks, Contributions, ... are always more than welcome.

Cheers,

Romain
rfrancois at mango-solutions.com


From petr.pikal at precheza.cz  Mon Aug 28 09:58:42 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 28 Aug 2006 09:58:42 +0200
Subject: [R] How to iteratively extract elements out of a list
In-Reply-To: <a4fecdd70608251152h1c629b44k3f85b28a60b8b376@mail.gmail.com>
Message-ID: <44F2BE52.15308.F4A5E@localhost>

Hi

try to do it without loop

lapply(m,function(x) x[x>2])

HTH
Petr


On 25 Aug 2006 at 13:52, xpRt.wannabe wrote:

Date sent:      	Fri, 25 Aug 2006 13:52:51 -0500
From:           	"xpRt.wannabe" <xprt.wannabe at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] How to iteratively extract elements out of a list

> Dear List,
> 
> The following code produces a list, which is what I what:
> 
> set.seed(123)
> tmpf <- function() {
> x <- rpois(rpois(1,4),2)
> }
> n <- 3
> m <- replicate(n,tmpf())
> m
> 
> [[1]]
> [1] 3 2 4
> 
> [[2]]
> [1] 0 2 4 2 2 5 2
> 
> [[3]]
> [1] 2 0 4 1 0
> 
> 
> Now I need something that would to extract iteratively (or as many
> times as the size of 'n') the values that are greater than 2 in each
> component of 'm' into another list such that the sub-list would be:
> 
> [[1]]
> [1] 3 4
> 
> [[2]]
> [1] 4 5
> 
> [[3]]
> [1] 4
> 
> Below is what I tried:
> 
> for(i in 1:3)
> sub.list <- lapply(m,subset,m[[i]]>2)
> 
> > sub.list
> 
> which gives me something different from what I want:
> 
> [[1]]
> [1] 4
> 
> [[2]]
> [1] 4
> 
> [[3]]
> [1] 4
> 
> Any help would be appreciated.
> 
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From franz.quehenberger at meduni-graz.at  Mon Aug 28 10:56:11 2006
From: franz.quehenberger at meduni-graz.at (Franz Quehenberger)
Date: Mon, 28 Aug 2006 10:56:11 +0200
Subject: [R] Rgraphviz - neato layout - edge weights do not have an effect
Message-ID: <44F2AFAB.2040800@meduni-graz.at>

Dear all,

neato makes layouts according to a physical model in which the length of 
the edges is determined by springs. The weight of the edge is the 
strength of the spring. However, I was not able to find any influence of 
edge weight on the layout. In the 
http://www.graphviz.org/Documentation/neatoguide.pdf the attribute "len" 
is another parameter of the neato layout.

set.seed(31)
V=letters[1:5]
g2=randomEGraph(V,0.5)
plot(g2,"neato",main="All weights 1")

# change and edge weight
edgeData(g2, from = "d", to = "e", attr = "weight") <- 20
plot(g2,"neato",main="Nothing changed!")

#try out length attribute change
edgeDataDefaults(g2,"len")=1
edgeData(g2, from = "d", to = "e", attr = "len") <- 5
plot(g2,"neato",main="Nothing changed again!")


Has anyone an idea how to achieve a change in the graph layout ?

Best

Franz Quehenberger

-- 
--
Franz Quehenberger
Institute for Medical Informatics, Statistics and Documentation
Graz, Austria


From tramni at abv.bg  Mon Aug 28 11:59:23 2006
From: tramni at abv.bg (Martin Ivanov)
Date: Mon, 28 Aug 2006 12:59:23 +0300 (EEST)
Subject: [R] nonlinear least squares trust region fitting ?
Message-ID: <1677550589.63741156759163644.JavaMail.nobody@mail03.abv.bg>

Hello!

I am running R-2.3.1-i386-1 on Slackware Linux 10.2. I am a former matlab user, moving to R. In matlab, via the cftool, I performed nonlinear curve fitting using the method "nonlinear least squares" with the "Trust-Region" algorithm and not using robust fitting. Is it possible to perform the same analysis in R? I read quite a lot of R documentation, but I could not find an alternative solution. If there is such, please forgive my ignorance (I am a newbie in R) and tell me which function from which package is capable of performing the same analysis. If the same analysis is not possible to carry out in R, I would be grateful if you suggest to me some alternative procedure. I found that the "nls" function performs nonlinear least squares. The problem is that I do not want to implement the Gauss-Newton algorithm. In the worst case I would be contented with the "Levenberg-Marquardt" algorithm, if it is implemented in R. R nls's documentation mentions the "port" package and the ?nl
 2sol? algorithm, but I could not find that package in the CRAN repository, so that I could read and judge whether that algorithm would be appropriate.

Thank you very much in advance. I am looking forward to your answer.
Regards,
Martin

-----------------------------------------------------------------
http://ide.li/ - ?????? ?? ????????? ?? ?????. ??????, ??????, ??????, ??????, ??????????.


From robert-mcfadden at o2.pl  Mon Aug 28 12:19:16 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Mon, 28 Aug 2006 12:19:16 +0200
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <44F28BE0.3050807@mango-solutions.com>
Message-ID: <000001c6ca8b$6ac4b8e0$1191680a@robert>

It's excellent. Good work. 
Robert


From msubianto at gmail.com  Mon Aug 28 13:44:28 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Mon, 28 Aug 2006 13:44:28 +0200
Subject: [R] Merge list to list - as matrix
Message-ID: <c7c17cef0608280444j765737fwb00af9b35be530f1@mail.gmail.com>

Dear all,

I have dataset
x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))

I need merge x and y as list (y put in last column).
The result is something like

[[1]]
     [,1] [,2] [,3] [,4]  [,5]
[1,]    1    6   11   16   110
[2,]    2    7   12   17   111
[3,]    3    8   13   18   112
[4,]    4    9   14   19   113
[5,]    5   10   15   20   114

[[2]]
     [,1] [,2] [,3] [,4]  [,5]
[1,]    1    6   11   16   110
[2,]    2    7   12   17   111
[3,]    3    8   13   18   112
[4,]    4    9   14   19   113
[5,]    5   10   15   20   114

[[3]]
     [,1] [,2] [,3] [,4]  [,5]
[1,]    1    6   11   16   110
[2,]    2    7   12   17   111
[3,]    3    8   13   18   112
[4,]    4    9   14   19   113
[5,]    5   10   15   20   114

I have tried
a <- list(x,y)
as.data.frame(t(sapply(a, rbind)))
lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
but I don't know how to fix it.

Regards, Muhammad Subianto


From ggrothendieck at gmail.com  Mon Aug 28 13:53:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 07:53:52 -0400
Subject: [R] Merge list to list - as matrix
In-Reply-To: <c7c17cef0608280444j765737fwb00af9b35be530f1@mail.gmail.com>
References: <c7c17cef0608280444j765737fwb00af9b35be530f1@mail.gmail.com>
Message-ID: <971536df0608280453p267388cfl9ad96c8f69474d02@mail.gmail.com>

Here are two ways:

1. use indexes:

lapply(seq(along = x), function(i) cbind(x[[i]], y[[i]]))

2. use mapply:

mapply(cbind, x, y, SIMPLIFY = FALSE)


On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
>
> I have dataset
> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> y <- list(matrix(110:114, 5, 1),matrix(110:114, 5, 1),matrix(110:114, 5, 1))
>
> I need merge x and y as list (y put in last column).
> The result is something like
>
> [[1]]
>     [,1] [,2] [,3] [,4]  [,5]
> [1,]    1    6   11   16   110
> [2,]    2    7   12   17   111
> [3,]    3    8   13   18   112
> [4,]    4    9   14   19   113
> [5,]    5   10   15   20   114
>
> [[2]]
>     [,1] [,2] [,3] [,4]  [,5]
> [1,]    1    6   11   16   110
> [2,]    2    7   12   17   111
> [3,]    3    8   13   18   112
> [4,]    4    9   14   19   113
> [5,]    5   10   15   20   114
>
> [[3]]
>     [,1] [,2] [,3] [,4]  [,5]
> [1,]    1    6   11   16   110
> [2,]    2    7   12   17   111
> [3,]    3    8   13   18   112
> [4,]    4    9   14   19   113
> [5,]    5   10   15   20   114
>
> I have tried
> a <- list(x,y)
> as.data.frame(t(sapply(a, rbind)))
> lapply(a, function(x) matrix(unlist(x), nrow = length(x), byrow = TRUE))
> but I don't know how to fix it.
>
> Regards, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ambuscadeacademy at earthlink.net  Mon Aug 28 14:29:58 2006
From: ambuscadeacademy at earthlink.net (barbecue)
Date: Mon, 28 Aug 2006 14:29:58 +0200
Subject: [R] debugger
Message-ID: <00308304121799.477D68C3A1@A899WT>

W a t c h   o u t!

ALLAINCE ETNERPRISE (A ETR)
Current Prcie: 0.80
Add this g e m to your wat ch list, and wa tch it tard closely!

Nwes Relaese!

Teacorp announces breackrough in removing deadly land mines.

Mill Valley, California August 25, 2006 - The Alliacne Entreprise Coproration announced today a breakthrough in developing an Areial Lnadmine Sytsem aimed at locating, detecting and mapping deadly landmi nes.

TaCeorp's mission is to reclaim lands around the globe embedded with landmi nes that victimize countries and their stakeholders.

More than 100 m i l l i o n lan dmines in 83 countries are holding international communities and industries hostage, preventing the investmen t in and development of produc tive lands and the re-building of infrastructure. A broad variety of landmin es have been scattered over product ive areas effectively crippling the econ omy and disabling thousands of children and adults.  There are no reliable records that accurately show where these d e v a s t a t i n g landmi nes lie in wait for their victim s.
 
With the present day cos ts to clear a single land mine ranging between $1,000 to $1,500, solving the problem of de-mining lands will reach billions of do llars.  TaeCorp has developed a technology based, c ost effective solution to this problem using its three tiered approach to scanning, mapping and removing landmine s. TaeCorp's System will provide many social and economi c ben efits to countries and their industries including oil and gas, mining, agriculture, roads and infrastructure development.

About TaeCorp.

TaeCorp's vision is to be the recognized leader in providing Aerail Detectoin Systmes including global de-mining, clearing a path to a safer planet for all humankind.

Here comes the big one! 
All signs show that AE TR is going to Explode!

C onclusion:
The examples above show the awesome, earning potential of little known companies that explode onto invsetor's radar screens; Many of you are already familiar with this. Is A ETR poised and positioned to do that for you? Then you may feel the time has come to act... And please watch this one tarde tomorrow! Go AET R.
Pen ny sotcks are considered highly speculative and may be unsuitable for all but very aggressive invsetors.  This pro file is not in any way a ffiliated with the featured company.  This report is for entertainment and advertising purposes only and should not be used as invsetment advice.  If you wish to stop future m a i lings, or if you feel you have been wrongfully placed in our membership, send a blank e m a i l with No Thanks in the subject to


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 14:50:16 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 08:50:16 -0400
Subject: [R] screen resolution effects on graphics
Message-ID: <006001c6caa0$83250f60$6400a8c0@DD4XFW31>

Greetings, R-Citizens:

I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.  My
R-code produces nice-looking graphics on this machine but the same code
results in crowded plots on an older machine with 800 X 600 resolution.  In
hindsight this seems obvious, but I didn't anticipate it.

My code will be used on machines with varying graphics (and memory)
capacity.  Is there a way I can check the native resolution of the machine
so that I can make adjustments to my code for the possible limitations of
the machine running it?

Thanks.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From singularitaet at gmx.net  Mon Aug 28 14:58:32 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Mon, 28 Aug 2006 14:58:32 +0200
Subject: [R] screen resolution effects on graphics
In-Reply-To: <006001c6caa0$83250f60$6400a8c0@DD4XFW31>
References: <006001c6caa0$83250f60$6400a8c0@DD4XFW31>
Message-ID: <44F2E878.1040704@gmx.net>

how about producing pdf output- should be the same on every PC...

Charles Annis, P.E. schrieb:
> Greetings, R-Citizens:
>
> I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.  My
> R-code produces nice-looking graphics on this machine but the same code
> results in crowded plots on an older machine with 800 X 600 resolution.  In
> hindsight this seems obvious, but I didn't anticipate it.
>
> My code will be used on machines with varying graphics (and memory)
> capacity.  Is there a way I can check the native resolution of the machine
> so that I can make adjustments to my code for the possible limitations of
> the machine running it?
>
> Thanks.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From daniel.guggisberg at gte.net  Mon Aug 28 15:06:39 2006
From: daniel.guggisberg at gte.net (daniel.guggisberg at gte.net)
Date: Mon, 28 Aug 2006 08:06:39 -0500 (CDT)
Subject: [R] I'm on vacation
In-Reply-To: <0J4P002IQLQZZA80@vms041.mailsrvcs.net>
References: <0J4P002IQLQZZA80@vms041.mailsrvcs.net>
Message-ID: <0J4P002JGLR3ZA80@vms041.mailsrvcs.net>

I am no longer using this email address as it is overwhelmed by unwanted SPAM therefore, please manually resend your message to  e n g l i s b e r g [at] g m a i l [dot] com


From arun.kumar.saha at gmail.com  Mon Aug 28 15:09:17 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Mon, 28 Aug 2006 18:39:17 +0530
Subject: [R] How to change the color of Plot area.
Message-ID: <d4c57560608280609ibb5bf85o2f3bc6b64e64f5e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/11c855aa/attachment.pl 

From ggrothendieck at gmail.com  Mon Aug 28 15:17:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 09:17:09 -0400
Subject: [R] How to change the color of Plot area.
In-Reply-To: <d4c57560608280609ibb5bf85o2f3bc6b64e64f5e2@mail.gmail.com>
References: <d4c57560608280609ibb5bf85o2f3bc6b64e64f5e2@mail.gmail.com>
Message-ID: <971536df0608280617l340daa1dw77c781c6ad6a6e1e@mail.gmail.com>

Try:

x <- 1:10 # test data
plot(x ~ x, type = "n")
u <- par("usr")
rect(u[1], u[3], u[2], u[4], col = "grey", border = "red")
points(x ~ x)

On 8/28/06, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> Dear all R users,
>
> Is there any effective way to change the body color of any plot? I am aware
> of the function par(bg="black"), but the problem with this is, it change the
> color of entire graphics window. But I want to see that only plot area will
> have color for example RED and rest of the area should have BLACK color.
>
> And I also I want to know is there any way to change the border-color (which
> is by default BLACK) of the plot area.
>
> Hope anyone can help me to solve this.
>
> Thanks and regards,
> Arun
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From L.Nanetti at med.umcg.nl  Mon Aug 28 15:34:50 2006
From: L.Nanetti at med.umcg.nl (L.Nanetti at med.umcg.nl)
Date: Mon, 28 Aug 2006 15:34:50 +0200
Subject: [R] Write signed short into a binary file
Message-ID: <20060828153450250.00000034128@NANETTI>

I'm using R 2.3.1 on WinXPsp2, 32 bit
What I want to do is to write a signed short integer (16 bit) (actually, a sequence of them) into a binary file.
I'm trying with writeBin, but the "what" clause is not supported, and an integer is 4 bytes long, nor it seems to exist something like as.int (coerce doesn't know the "int" type).
My workaround until now has been to use python to write a binary files containing my values, open that file in "rb" mode, reading it using "raw" data, and thus writing values in my file.
But of course, this workaround is poorly satisfactory...any ideas?
Thank you in advance,
Luca Nanetti


From L.Nanetti at med.umcg.nl  Mon Aug 28 16:52:59 2006
From: L.Nanetti at med.umcg.nl (L.Nanetti at med.umcg.nl)
Date: Mon, 28 Aug 2006 16:52:59 +0200
Subject: [R] Write signed short into a binary file (follow up and conclusion)
Message-ID: <20060828165259453.00000032772@NANETTI>

I've solved my problem using:
(purpose: write the signed integer -19 as a two byte integer into a binary file)

writeBin(as.integer(-19),myconnection, size=2)

There is no need to coerce.

Thanks anyway!

Luca NanettiUniversity Medical Center Groningen
BCN-NeuroImagingCenter
A.Deusinglaan 2 9713AW Groningen
The Nethterlands
l dot nanetti at med dot rug dot nl


From br44114 at gmail.com  Mon Aug 28 16:55:20 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 28 Aug 2006 10:55:20 -0400
Subject: [R] screen resolution effects on graphics
Message-ID: <8d5a36350608280755x77533db3v5e5aa881e4d8db2c@mail.gmail.com>

You forgot to mention your OS. This was asked before and if I recall
correctly the answer for Windows was no. An acceptable solution (imho)
is to edit the Rprofile.site files and add something like
  pngplotwidth <- 990 ; pngplotheight <- 700
  pdfplotwidth <- 14 ; pdfplotheight <- 10
Then, use these values in your functions. It's manual, but you only
need to do this once for each machine.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Charles Annis, P.E.
> Sent: Monday, August 28, 2006 8:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] screen resolution effects on graphics
>
> Greetings, R-Citizens:
>
> I have the good fortune of working with a 19" 1280 X 1024
> pixel monitor.  My
> R-code produces nice-looking graphics on this machine but the
> same code
> results in crowded plots on an older machine with 800 X 600
> resolution.  In
> hindsight this seems obvious, but I didn't anticipate it.
>
> My code will be used on machines with varying graphics (and memory)
> capacity.  Is there a way I can check the native resolution
> of the machine
> so that I can make adjustments to my code for the possible
> limitations of
> the machine running it?
>
> Thanks.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sfalcon at fhcrc.org  Mon Aug 28 17:06:14 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 28 Aug 2006 08:06:14 -0700
Subject: [R] Rgraphviz - neato layout - edge weights do not have an
	effect
In-Reply-To: <44F2AFAB.2040800@meduni-graz.at> (Franz Quehenberger's message
	of "Mon, 28 Aug 2006 10:56:11 +0200")
References: <44F2AFAB.2040800@meduni-graz.at>
Message-ID: <m2odu499y1.fsf@ziti.local>

Hi Franz,

This might be better directed to the bioconductor mail list.

Franz Quehenberger <franz.quehenberger at meduni-graz.at> writes:
> Dear all,
>
> neato makes layouts according to a physical model in which the length
> of the edges is determined by springs. The weight of the edge is the
> strength of the spring. However, I was not able to find any influence
> of edge weight on the layout. In the
> http://www.graphviz.org/Documentation/neatoguide.pdf the attribute
> "len" is another parameter of the neato layout.
>
> set.seed(31) V=letters[1:5] g2=randomEGraph(V,0.5)
> plot(g2,"neato",main="All weights 1")
>
> # change and edge weight edgeData(g2, from = "d", to = "e", attr =
> "weight") <- 20 plot(g2,"neato",main="Nothing changed!")
>
> #try out length attribute change edgeDataDefaults(g2,"len")=1
> edgeData(g2, from = "d", to = "e", attr = "len") <- 5
> plot(g2,"neato",main="Nothing changed again!")
>
>
> Has anyone an idea how to achieve a change in the graph layout ?

Have you read through the Rgraphviz vignettes?  

Rgraphviz currently ignores attributes of the graph.  This is
currently by design: display attributes are Rgraphviz's business.
There are ways to specify node and edge attributes for Rgraphviz, but
I'm not sure if the feature you want is implemented.

+ seth


From ripley at stats.ox.ac.uk  Mon Aug 28 17:10:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Aug 2006 16:10:06 +0100 (BST)
Subject: [R] screen resolution effects on graphics
In-Reply-To: <006001c6caa0$83250f60$6400a8c0@DD4XFW31>
References: <006001c6caa0$83250f60$6400a8c0@DD4XFW31>
Message-ID: <Pine.LNX.4.64.0608281356380.15632@gannet.stats.ox.ac.uk>

On Mon, 28 Aug 2006, Charles Annis, P.E. wrote:

> Greetings, R-Citizens:
> 
> I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.  My

(Similar to our student lab has used for many years.)

> R-code produces nice-looking graphics on this machine but the same code
> results in crowded plots on an older machine with 800 X 600 resolution.  In
> hindsight this seems obvious, but I didn't anticipate it.

It is not obvious to me: I have never experienced it.  What OS and 
graphics device is this?

Almost all of R's graphics is independent of the screen resolution (the 
exception being the bitmapped devices such as jpeg), with things sized in 
inches or points. My machines are 1600x1200 (apart from 1280x800 on my 
laptop), so I meet a considerable reduction when using a computer 
projector, and my plots do not look crowded.

However, one issue is when the OS has a seriously incorrect setting for 
the screen resolution and so does not give the sizes asked for by R.  We 
have seen that on both Linux and Windows, and the windows() device has 
arguments to set the correct values.  (On X11 you should be able to set 
this in Xconfig files.)

If this is Windows, check carefully the description of the initial screen 
size in ?windows.  That can have unexpected effects on physically small 
screens.

At one time the X11() device was set up to assume 75dpi unless the 
reported resolution was 100+/-0.5dpi.  My then monitor reported 99.2 dpi 
and so things came out at 3/4 of the intended size.  We fixed that quite a 
while back.

> My code will be used on machines with varying graphics (and memory)
> capacity.  Is there a way I can check the native resolution of the machine
> so that I can make adjustments to my code for the possible limitations of
> the machine running it?

Only via C code, which is how R does it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 17:12:02 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 11:12:02 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <8d5a36350608280755x77533db3v5e5aa881e4d8db2c@mail.gmail.com>
Message-ID: <007e01c6cab4$51084ba0$6400a8c0@DD4XFW31>

My apologies for my oversight.  I am using WindowsXP.  The code that
produces a nice-looking jpg (when viewed on my screen) produces cramped
graphics on a 800 X 600 screen.  I can change the spacings on the plot and
remedy the situation for 800 X 600, but that looks awkward at 1280 X 1024.

Thanks

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Monday, August 28, 2006 10:55 AM
To: Charles.Annis at statisticalengineering.com
Cc: r-help
Subject: Re: [R] screen resolution effects on graphics

You forgot to mention your OS. This was asked before and if I recall
correctly the answer for Windows was no. An acceptable solution (imho)
is to edit the Rprofile.site files and add something like
  pngplotwidth <- 990 ; pngplotheight <- 700
  pdfplotwidth <- 14 ; pdfplotheight <- 10
Then, use these values in your functions. It's manual, but you only
need to do this once for each machine.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Charles Annis, P.E.
> Sent: Monday, August 28, 2006 8:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] screen resolution effects on graphics
>
> Greetings, R-Citizens:
>
> I have the good fortune of working with a 19" 1280 X 1024
> pixel monitor.  My
> R-code produces nice-looking graphics on this machine but the
> same code
> results in crowded plots on an older machine with 800 X 600
> resolution.  In
> hindsight this seems obvious, but I didn't anticipate it.
>
> My code will be used on machines with varying graphics (and memory)
> capacity.  Is there a way I can check the native resolution
> of the machine
> so that I can make adjustments to my code for the possible
> limitations of
> the machine running it?
>
> Thanks.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From adiaba1978 at hotmail.com  Mon Aug 28 10:45:53 2006
From: adiaba1978 at hotmail.com (emmanuel abatih)
Date: Mon, 28 Aug 2006 09:45:53 +0100
Subject: [R] Splancs Query
Message-ID: <BAY19-F49A5DDEAB70E8DFBB0C35C6380@phx.gbl>

Dear R Users;
I have been using the space-time K-function analysis to detect the existence 
of clustering in both space and time. I wish to ask and know what the 
minimum number of cases should be for such analyses to be valid. I have  
data with 23 cases and don't know how careful i should be in the 
interpreting the results.

Secondly, i wish to find out if there is any bond between the results from 
the SatScan software and Splancs. In an exercise, SatScan suggested the 
presence of space-time clustering but Splancs did not present significant 
results. This might have been due to the small sample size or the way in 
which the spatial distances and times were specified for the analysis. Using 
the Burkitts data supplied with the software and changing the spatial 
distances and times for the analyses, affected the significance of the 
results.  Before or after certain spatial distances, the results were no 
more significant, also certain specifications of the times ruined the 
analyses. A follow-up question to this is if there is a paritcular techniqe 
for chosing the spatial distances and times for the analyses.

My approach of  space-time clustering is to use the space-time K-function 
analysis to check for the presence of clustering. Once present,  a search 
for their locations and tests for their significance could be carried out 
using SatScan. If not, then the plots of the spatial K-function and the 
temporal K-function could be used to drive the SatScan analysis into the 
spatial or temporal domains. Has anyone used this same approach and found it 
correct?

All help will be warmly embraced.
Thanks in advance for your help.
Abatih



KENNIS IS MACHT


From ygati at yahoo.fr  Mon Aug 28 14:27:37 2006
From: ygati at yahoo.fr (Yousra Gati)
Date: Mon, 28 Aug 2006 14:27:37 +0200 (CEST)
Subject: [R] Modified Bessel function of third kind (fractional or real
	order)
Message-ID: <20060828122737.77878.qmail@web26015.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/698a76a8/attachment.pl 

From ggrothendieck at gmail.com  Mon Aug 28 17:39:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 11:39:02 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <007e01c6cab4$51084ba0$6400a8c0@DD4XFW31>
References: <8d5a36350608280755x77533db3v5e5aa881e4d8db2c@mail.gmail.com>
	<007e01c6cab4$51084ba0$6400a8c0@DD4XFW31>
Message-ID: <971536df0608280839h38811522ya25872f7c6fd2061@mail.gmail.com>

1.   Put the code from
  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
into, say, \bin\displayconfiguration.vbs, and then from R do this:

as.numeric(gsub(".* ", "", grep("resolution",
  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
value = TRUE)))


or we can translate that into R to eliminate the need for a vbs routine
and then run it directly from R (although we will still need the indicated
dll):

# must have GenericEnum.dll registered. That is, download and unzip:
#  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
# and register it: regsvr32 GenericEnum.dll

library(RDCOMClient)
strComputer = "."
SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
colItems <- objWMIService$ExecQuery ("Select * from Win32_DisplayConfiguration")

lEnum <- COMCreate("GenericEnum.AutomationEnum")
lEnum[["Collection"]] <- colItems

if (lEnum$SetFirst()) {
	repeat {
		cat(lEnum[["Item"]][["DeviceName"]], "\n")
		cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
		cat(lEnum[["Item"]][["Pelswidth"]], "\n")
		cat(lEnum[["Item"]][["Pelsheight"]], "\n")
		if (!lEnum$SetNext()) break
	}
}



On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> My apologies for my oversight.  I am using WindowsXP.  The code that
> produces a nice-looking jpg (when viewed on my screen) produces cramped
> graphics on a 800 X 600 screen.  I can change the spacings on the plot and
> remedy the situation for 800 X 600, but that looks awkward at 1280 X 1024.
>
> Thanks
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> Sent: Monday, August 28, 2006 10:55 AM
> To: Charles.Annis at statisticalengineering.com
> Cc: r-help
> Subject: Re: [R] screen resolution effects on graphics
>
> You forgot to mention your OS. This was asked before and if I recall
> correctly the answer for Windows was no. An acceptable solution (imho)
> is to edit the Rprofile.site files and add something like
>  pngplotwidth <- 990 ; pngplotheight <- 700
>  pdfplotwidth <- 14 ; pdfplotheight <- 10
> Then, use these values in your functions. It's manual, but you only
> need to do this once for each machine.
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Charles Annis, P.E.
> > Sent: Monday, August 28, 2006 8:50 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] screen resolution effects on graphics
> >
> > Greetings, R-Citizens:
> >
> > I have the good fortune of working with a 19" 1280 X 1024
> > pixel monitor.  My
> > R-code produces nice-looking graphics on this machine but the
> > same code
> > results in crowded plots on an older machine with 800 X 600
> > resolution.  In
> > hindsight this seems obvious, but I didn't anticipate it.
> >
> > My code will be used on machines with varying graphics (and memory)
> > capacity.  Is there a way I can check the native resolution
> > of the machine
> > so that I can make adjustments to my code for the possible
> > limitations of
> > the machine running it?
> >
> > Thanks.
> >
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mesomeris at yahoo.co.uk  Mon Aug 28 17:39:31 2006
From: mesomeris at yahoo.co.uk (Spiros Mesomeris)
Date: Mon, 28 Aug 2006 16:39:31 +0100 (BST)
Subject: [R] Help on function adf.test
Message-ID: <20060828153931.95962.qmail@web86904.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/6d01fe99/attachment.pl 

From ggrothendieck at gmail.com  Mon Aug 28 18:10:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 12:10:35 -0400
Subject: [R] Help on function adf.test
In-Reply-To: <20060828153931.95962.qmail@web86904.mail.ukl.yahoo.com>
References: <20060828153931.95962.qmail@web86904.mail.ukl.yahoo.com>
Message-ID: <971536df0608280910s5e8d91a8r3c02af8803072924@mail.gmail.com>

Put your time series into a ts or zoo object.  Now using
EuStockMarkets which is builtin data set in R.  (You might
want to use align = "right" in rollapply.)

library(tseries)
library(zoo)
eu91 <- window(EuStockMarkets, end = 1992)  # use portion for test data
eu91.p.value <- rollapply(eu91, 61, function(x) adf.test(x)$p.value)

On 8/28/06, Spiros Mesomeris <mesomeris at yahoo.co.uk> wrote:
> Hello everybody,
>
>  I've got a matrix called EUROPEDATA and I want to calculate the adf test statistic (part of the tseries package) on a rolling basis for window my.win on each column; i.e. each column of EUROPEDATA represents a particular variable; for the first column I calculate the adf test statistic for window my.win = 60 for example, roll forward one observation, calculate the adf again, and so on, until the end of the first column is reached and then I jump to the second column etc. The code for doing this is given below:
>
>  adfroll <- sapply(1:(ncol(EUROPEDATA)), function(i, my.data, my.win)
>  {
>  sss <- sapply(1:(nrow(my.data)-my.win), function (j, my.data, my.win)
>  { my.data <- as.matrix(my.data)
>  ans <- adf.test(na.omit(my.data[j:(j+my.win)]))
>  return(ans$p.value)
>  },my.data=my.data[,i],my.win=my.win)
>  },my.data=EUROPEDATA,my.win=60,simplify=T)
>
>  The problem is that the adf test does not calculate this way. There is an error saying:
>
>  "Error in embed(y, k) : wrong embedding dimension"
>
>  This error is generated from within the adf.test function. The embed function is part of the stats package, which I load before doing the adf.test.
>
>  I would be very obliged if anybody were to explain to me why this happens and how I can correct it/ estimate what I want in a different way that will not invoke this error.
>
>  P.S. the function works fine if the adf.test on each column of the dataset is calculated, that is, without the rolling window for each column:
>
>
>  summaryadf <- sapply(1:ncol(EUROPEDATA),function(i,my.data)
>  {
>  tt <- adf.test(na.omit(my.data[,i]))
>  return(tt$p.value)
>  },my.data=EUROPEDATA)
>
>  Please note that the same error is generated with the pp.test function of the tseries package
>
>  Thanks in advance,
>  Spyros
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mel at altk.com  Mon Aug 28 18:45:40 2006
From: mel at altk.com (mel)
Date: Mon, 28 Aug 2006 18:45:40 +0200
Subject: [R] Modified Bessel function of third kind (fractional or real
 order)
In-Reply-To: <20060828122737.77878.qmail@web26015.mail.ukl.yahoo.com>
References: <20060828122737.77878.qmail@web26015.mail.ukl.yahoo.com>
Message-ID: <44F31DB4.8050303@altk.com>

Yousra Gati a ?crit :

> Hello,
>   I am searching for code in C++ or fortran for Modified Bessel function of third kind (fractional or real order). Can someone help me?
>   Thank you

http://library.lanl.gov/numerical/index.html
http://library.lanl.gov/numerical/bookcpdf.html
see section 6 special functions
hih


From lord.tyranus.96 at gmail.com  Mon Aug 28 18:50:15 2006
From: lord.tyranus.96 at gmail.com (Lord Tyranus)
Date: Mon, 28 Aug 2006 10:50:15 -0600
Subject: [R] Help with Functions
Message-ID: <4f31b0bd0608280950h41c260b3w6c6acf6e0ea3de5b@mail.gmail.com>

Hello wizards, I need to convert the following functions (prestd,
poststd, prepca)   of matlab to R. Does Somebody knows how to do it. A
description of the functions is:

prestd preprocesses the network training set by normalizing the inputs
and targets so that they have means of zero and standard deviations of
1.

poststd postprocesses the network training set which was preprocessed
by prestd. It converts the data back into unnormalized units.

prepca preprocesses the network input training set by applying a
principal component analysis. This analysis transforms the input data
so that the elements of the input vector set will be uncorrelated. In
addition, the size of the input vectors may be reduced by retaining
only those components which contribute more than a specified fraction
(min_frac) of the total variation in the data set.

Thanks in advance.


From p.dalgaard at biostat.ku.dk  Mon Aug 28 18:55:45 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Aug 2006 18:55:45 +0200
Subject: [R] Modified Bessel function of third kind (fractional or real
	order)
In-Reply-To: <44F31DB4.8050303@altk.com>
References: <20060828122737.77878.qmail@web26015.mail.ukl.yahoo.com>
	<44F31DB4.8050303@altk.com>
Message-ID: <x2ejv094vi.fsf@turmalin.kubism.ku.dk>

mel <mel at altk.com> writes:

> Yousra Gati a ?crit :
> 
> > Hello,
> >   I am searching for code in C++ or fortran for Modified Bessel function of third kind (fractional or real order). Can someone help me?
> >   Thank you
> 
> http://library.lanl.gov/numerical/index.html
> http://library.lanl.gov/numerical/bookcpdf.html
> see section 6 special functions
> hih

Or,

1. This is a list about R
2. R contains the besselK function
3. R is open source
4. Guess what...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From msubianto at gmail.com  Mon Aug 28 19:20:30 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Mon, 28 Aug 2006 19:20:30 +0200
Subject: [R] Remove empty list from list
Message-ID: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>

Dear all,
I am still working with "list".
If I have an empty list how can I remove from list data.
Here is a toy example:
x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5,
4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
y <- list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1),c(1, 1, 1, 1, 1),c(1,
1, -1, 1, -1),c(-1, -1, -1, -1, -1))
## Thanks to Gabor Grothendieck for this trick.
## SIMPLIFY? SIMPLIFY >< simplify
xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)

point.class <- t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
pd.class <- points.diff(class.diffsame,xy.list)

nc.test <- vector("list",length(pd.class))
for (i in 1:length(pd.class)) {
     nc.test[[i]] <- pd.class[[i]]$point.diff
}
nc.test
> nc.test
[[1]]
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    2    7   12   17    1

[[3]]
     [,1] [,2] [,3] [,4] [,5]

[[4]]
     [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1

[[5]]
     [,1] [,2] [,3] [,4] [,5]

>
I want to remove these:
nc.test[[3]]
nc.test[[5]]
Because my list data have more 1000 lists are there any simple way to do this?

Best, Muhammad Subianto


points.neighb <- function(p.class, list.nc, class.col) {
   ntuples <- nrow(p.class)
   instvec <- vector("list",length=ntuples)
   for (i in 1:ntuples) {
        # Thanks to Petr Pikal for this trick
        instvec[[i]]$class.diff <- (p.class[i,class.col] -
list.nc[[i]][,class.col])!=0
        instvec[[i]]$class.same <- (p.class[i,class.col] -
list.nc[[i]][,class.col])==0
   }
   instvec
}

points.diff <- function(p.class, list.nc) {
   ntuples <- length(list.nc)
   instvec <- vector("list",ntuples)
   for (i in 1:ntuples) {
        instvec[[i]]$point.diff <- list.nc[[i]][p.class[[i]]$class.diff,]
        instvec[[i]]$point.same <- list.nc[[i]][p.class[[i]]$class.same,]
   }
   instvec
}


From ggrothendieck at gmail.com  Mon Aug 28 19:29:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 13:29:46 -0400
Subject: [R] Remove empty list from list
In-Reply-To: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
References: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
Message-ID: <971536df0608281029s52bdf5fesd8eae95b560d297a@mail.gmail.com>

See this post from the weekend:
http://www.nabble.com/Re%3A-How-to-iteratively-extract-elements-out-of-a-list-p6002980.html



On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> I am still working with "list".
> If I have an empty list how can I remove from list data.
> Here is a toy example:
> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5,
> 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> y <- list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1),c(1, 1, 1, 1, 1),c(1,
> 1, -1, 1, -1),c(-1, -1, -1, -1, -1))
> ## Thanks to Gabor Grothendieck for this trick.
> ## SIMPLIFY? SIMPLIFY >< simplify
> xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)
>
> point.class <- t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
> class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
> pd.class <- points.diff(class.diffsame,xy.list)
>
> nc.test <- vector("list",length(pd.class))
> for (i in 1:length(pd.class)) {
>     nc.test[[i]] <- pd.class[[i]]$point.diff
> }
> nc.test
> > nc.test
> [[1]]
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    4    9   14   19    1
> [3,]    5   10   15   20    1
>
> [[2]]
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    2    7   12   17    1
>
> [[3]]
>     [,1] [,2] [,3] [,4] [,5]
>
> [[4]]
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    3    8   13   18   -1
> [2,]    5   10   15   20   -1
>
> [[5]]
>     [,1] [,2] [,3] [,4] [,5]
>
> >
> I want to remove these:
> nc.test[[3]]
> nc.test[[5]]
> Because my list data have more 1000 lists are there any simple way to do this?
>
> Best, Muhammad Subianto
>
>
> points.neighb <- function(p.class, list.nc, class.col) {
>   ntuples <- nrow(p.class)
>   instvec <- vector("list",length=ntuples)
>   for (i in 1:ntuples) {
>        # Thanks to Petr Pikal for this trick
>        instvec[[i]]$class.diff <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])!=0
>        instvec[[i]]$class.same <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])==0
>   }
>   instvec
> }
>
> points.diff <- function(p.class, list.nc) {
>   ntuples <- length(list.nc)
>   instvec <- vector("list",ntuples)
>   for (i in 1:ntuples) {
>        instvec[[i]]$point.diff <- list.nc[[i]][p.class[[i]]$class.diff,]
>        instvec[[i]]$point.same <- list.nc[[i]][p.class[[i]]$class.same,]
>   }
>   instvec
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Mon Aug 28 19:29:54 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 28 Aug 2006 13:29:54 -0400
Subject: [R] Remove empty list from list
In-Reply-To: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
References: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
Message-ID: <644e1f320608281029u3364cbe6l5259a0850adc7819@mail.gmail.com>

Here is a function I use a lot to do this:

delete.NULLs  <-  function(x.list){   # delele null/empty entries in a list
    x.list[unlist(lapply(x.list, length) != 0)]
}


> delete.NULLs(nc.test)
[[1]]
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    2    7   12   17    1

[[3]]
     [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1



On 8/28/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> I am still working with "list".
> If I have an empty list how can I remove from list data.
> Here is a toy example:
> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5,
> 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> y <- list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1),c(1, 1, 1, 1, 1),c(1,
> 1, -1, 1, -1),c(-1, -1, -1, -1, -1))
> ## Thanks to Gabor Grothendieck for this trick.
> ## SIMPLIFY? SIMPLIFY >< simplify
> xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)
>
> point.class <- t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
> class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
> pd.class <- points.diff(class.diffsame,xy.list)
>
> nc.test <- vector("list",length(pd.class))
> for (i in 1:length(pd.class)) {
>     nc.test[[i]] <- pd.class[[i]]$point.diff
> }
> nc.test
> > nc.test
> [[1]]
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    4    9   14   19    1
> [3,]    5   10   15   20    1
>
> [[2]]
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    2    7   12   17    1
>
> [[3]]
>     [,1] [,2] [,3] [,4] [,5]
>
> [[4]]
>     [,1] [,2] [,3] [,4] [,5]
> [1,]    3    8   13   18   -1
> [2,]    5   10   15   20   -1
>
> [[5]]
>     [,1] [,2] [,3] [,4] [,5]
>
> >
> I want to remove these:
> nc.test[[3]]
> nc.test[[5]]
> Because my list data have more 1000 lists are there any simple way to do this?
>
> Best, Muhammad Subianto
>
>
> points.neighb <- function(p.class, list.nc, class.col) {
>   ntuples <- nrow(p.class)
>   instvec <- vector("list",length=ntuples)
>   for (i in 1:ntuples) {
>        # Thanks to Petr Pikal for this trick
>        instvec[[i]]$class.diff <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])!=0
>        instvec[[i]]$class.same <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])==0
>   }
>   instvec
> }
>
> points.diff <- function(p.class, list.nc) {
>   ntuples <- length(list.nc)
>   instvec <- vector("list",ntuples)
>   for (i in 1:ntuples) {
>        instvec[[i]]$point.diff <- list.nc[[i]][p.class[[i]]$class.diff,]
>        instvec[[i]]$point.same <- list.nc[[i]][p.class[[i]]$class.same,]
>   }
>   instvec
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jm540 at york.ac.uk  Mon Aug 28 19:54:05 2006
From: jm540 at york.ac.uk (Jon Minton)
Date: Mon, 28 Aug 2006 18:54:05 +0100
Subject: [R] regex scares me
Message-ID: <001301c6caca$f4704200$dd50c600$@ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/17b1f036/attachment.pl 

From msubianto at gmail.com  Mon Aug 28 19:43:50 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Mon, 28 Aug 2006 19:43:50 +0200
Subject: [R] Remove empty list from list
In-Reply-To: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
References: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
Message-ID: <44F32B56.7010002@gmail.com>

On this day 28/08/2006 19:20, Muhammad Subianto wrote:
> Dear all,
> I am still working with "list".
> If I have an empty list how can I remove from list data.
> Here is a toy example:
> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5,
> 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> y <- list(c(1, -1, -1, 1, 1),c(1, 1, -1, -1, -1),c(1, 1, 1, 1, 1),c(1,
> 1, -1, 1, -1),c(-1, -1, -1, -1, -1))
> ## Thanks to Gabor Grothendieck for this trick.
> ## SIMPLIFY? SIMPLIFY >< simplify
> xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)
> 
> point.class <- t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
> class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
> pd.class <- points.diff(class.diffsame,xy.list)
> 
> nc.test <- vector("list",length(pd.class))
> for (i in 1:length(pd.class)) {
>      nc.test[[i]] <- pd.class[[i]]$point.diff
> }
> nc.test
>> nc.test
> [[1]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    4    9   14   19    1
> [3,]    5   10   15   20    1
> 
> [[2]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    2    7   12   17    1
> 
> [[3]]
>      [,1] [,2] [,3] [,4] [,5]
> 
> [[4]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    3    8   13   18   -1
> [2,]    5   10   15   20   -1
> 
> [[5]]
>      [,1] [,2] [,3] [,4] [,5]
> 
> I want to remove these:
> nc.test[[3]]
> nc.test[[5]]
> Because my list data have more 1000 lists are there any simple way to do this?
> 
> Best, Muhammad Subianto
> 
> 
> points.neighb <- function(p.class, list.nc, class.col) {
>    ntuples <- nrow(p.class)
>    instvec <- vector("list",length=ntuples)
>    for (i in 1:ntuples) {
>         # Thanks to Petr Pikal for this trick
>         instvec[[i]]$class.diff <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])!=0
>         instvec[[i]]$class.same <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])==0
>    }
>    instvec
> }
> 
> points.diff <- function(p.class, list.nc) {
>    ntuples <- length(list.nc)
>    instvec <- vector("list",ntuples)
>    for (i in 1:ntuples) {
>         instvec[[i]]$point.diff <- list.nc[[i]][p.class[[i]]$class.diff,]
>         instvec[[i]]$point.same <- list.nc[[i]][p.class[[i]]$class.same,]
>    }
>    instvec
> }
> 

Dear
Jim Holtman and Gabor Grothendieck,
Thank you both very much for the suggestions!
These is exactly what I was looking for.

Best wishes, Muhammad Subianto


## delete null/empty entries in a list
delete.NULLs  <-  function(x.list){
     x.list[unlist(lapply(x.list, length) != 0)]
}

 > delete.NULLs  <-  function(x.list){
+     x.list[unlist(lapply(x.list, length) != 0)]
+ }
 > delete.NULLs  <-  function(x.list){
+     x.list[unlist(lapply(x.list, length) != 0)]
+ }
 >
 > delete.NULLs(nc.test)
[[1]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    2    7   12   17    1

[[3]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1

 >


From tplate at acm.org  Mon Aug 28 20:00:52 2006
From: tplate at acm.org (Tony Plate)
Date: Mon, 28 Aug 2006 12:00:52 -0600
Subject: [R] regex scares me
In-Reply-To: <001301c6caca$f4704200$dd50c600$@ac.uk>
References: <001301c6caca$f4704200$dd50c600$@ac.uk>
Message-ID: <44F32F54.7010009@acm.org>

I think this does the trick.  Note that it is case sensitive.

 > x <- c("lad.tab", "xxladyy.tab", "xxyy.tab", "lad.tabx", "LAD.tab", 
"lad.TAB")
 > grep("lad.*\\.tab$", x, value=T)
[1] "lad.tab"     "xxladyy.tab"
 >

Jon Minton wrote:
> Hi, apologies if this is too simple but I've been stuck on the following for
> a while:
> 
>  
> 
> I have a vector of strings: filenames with a name before the extension and a
> variety of possible extensions
> 
>  
> 
> I want to select only those files with:
> 
>  1) a ".tab" extension
> 
> AND 
> 
> 2) the character sequence "lad" anywhere in the name of the file before the
> extension.
> 
>  
> 
> Surely this won't take long to do, I thought. (But I was wrong.)
> 
>  
> 
> What's the regexp pattern to specify here?
> 
>  
> 
> Thanks,
> 
>  
> 
> Jon Minton
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 20:16:50 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 14:16:50 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608280839h38811522ya25872f7c6fd2061@mail.gmail.com>
Message-ID: <009a01c6cace$22607ba0$6400a8c0@DD4XFW31>

Gabor:

I am afraid I am demonstrating my lack of computer savvy.

As you instructed, I downloaded the code, saved it as the file you
suggested, and executed this within R

as.numeric(gsub(".* ", "", grep("resolution",
  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
value = TRUE)))

The DOS window opened, some magic occurred in the blink of an eye, and the
DOS window closed.  I haven't the foggiest idea what to do next since I can
see no evidence of having done anything.

Thanks for your patience.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: Monday, August 28, 2006 11:39 AM
To: Charles.Annis at statisticalengineering.com
Cc: r-help
Subject: Re: [R] screen resolution effects on graphics

1.   Put the code from
  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
into, say, \bin\displayconfiguration.vbs, and then from R do this:

as.numeric(gsub(".* ", "", grep("resolution",
  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
value = TRUE)))


or we can translate that into R to eliminate the need for a vbs routine
and then run it directly from R (although we will still need the indicated
dll):

# must have GenericEnum.dll registered. That is, download and unzip:
#  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
# and register it: regsvr32 GenericEnum.dll

library(RDCOMClient)
strComputer = "."
SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
colItems <- objWMIService$ExecQuery ("Select * from
Win32_DisplayConfiguration")

lEnum <- COMCreate("GenericEnum.AutomationEnum")
lEnum[["Collection"]] <- colItems

if (lEnum$SetFirst()) {
	repeat {
		cat(lEnum[["Item"]][["DeviceName"]], "\n")
		cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
		cat(lEnum[["Item"]][["Pelswidth"]], "\n")
		cat(lEnum[["Item"]][["Pelsheight"]], "\n")
		if (!lEnum$SetNext()) break
	}
}



On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> My apologies for my oversight.  I am using WindowsXP.  The code that
> produces a nice-looking jpg (when viewed on my screen) produces cramped
> graphics on a 800 X 600 screen.  I can change the spacings on the plot and
> remedy the situation for 800 X 600, but that looks awkward at 1280 X 1024.
>
> Thanks
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> Sent: Monday, August 28, 2006 10:55 AM
> To: Charles.Annis at statisticalengineering.com
> Cc: r-help
> Subject: Re: [R] screen resolution effects on graphics
>
> You forgot to mention your OS. This was asked before and if I recall
> correctly the answer for Windows was no. An acceptable solution (imho)
> is to edit the Rprofile.site files and add something like
>  pngplotwidth <- 990 ; pngplotheight <- 700
>  pdfplotwidth <- 14 ; pdfplotheight <- 10
> Then, use these values in your functions. It's manual, but you only
> need to do this once for each machine.
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Charles Annis, P.E.
> > Sent: Monday, August 28, 2006 8:50 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] screen resolution effects on graphics
> >
> > Greetings, R-Citizens:
> >
> > I have the good fortune of working with a 19" 1280 X 1024
> > pixel monitor.  My
> > R-code produces nice-looking graphics on this machine but the
> > same code
> > results in crowded plots on an older machine with 800 X 600
> > resolution.  In
> > hindsight this seems obvious, but I didn't anticipate it.
> >
> > My code will be used on machines with varying graphics (and memory)
> > capacity.  Is there a way I can check the native resolution
> > of the machine
> > so that I can make adjustments to my code for the possible
> > limitations of
> > the machine running it?
> >
> > Thanks.
> >
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rdpeng at gmail.com  Mon Aug 28 20:17:51 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Mon, 28 Aug 2006 14:17:51 -0400
Subject: [R] regex scares me
In-Reply-To: <001301c6caca$f4704200$dd50c600$@ac.uk>
References: <001301c6caca$f4704200$dd50c600$@ac.uk>
Message-ID: <44F3334F.9070903@gmail.com>

You might want to try 'glob2rx()', as in

grep(glob2rx("*.tab"), x)

where 'x' is a vector of strings.

-roger

Jon Minton wrote:
> Hi, apologies if this is too simple but I've been stuck on the following for
> a while:
> 
>  
> 
> I have a vector of strings: filenames with a name before the extension and a
> variety of possible extensions
> 
>  
> 
> I want to select only those files with:
> 
>  1) a ".tab" extension
> 
> AND 
> 
> 2) the character sequence "lad" anywhere in the name of the file before the
> extension.
> 
>  
> 
> Surely this won't take long to do, I thought. (But I was wrong.)
> 
>  
> 
> What's the regexp pattern to specify here?
> 
>  
> 
> Thanks,
> 
>  
> 
> Jon Minton
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From ggrothendieck at gmail.com  Mon Aug 28 20:20:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 14:20:55 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <009a01c6cace$22607ba0$6400a8c0@DD4XFW31>
References: <971536df0608280839h38811522ya25872f7c6fd2061@mail.gmail.com>
	<009a01c6cace$22607ba0$6400a8c0@DD4XFW31>
Message-ID: <971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>

This is what happens when I run it in R 2.3.1:

> as.numeric(gsub(".* ", "", grep("resolution",
+  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
+ value = TRUE)))
[1] 1280 1024

The result is obviously dependent on your particular video
card (I have a radeon).  Try running it from outside of R and see
what you get:

cd \bin
cscript displayconfiguration.vbs

Also make sure you copied the code correctly from the web site
and also try the second solution too just in case.

Its possible that virus detection software will interfere since some
antivirus programs prevent all vbscript routines from running.


On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Gabor:
>
> I am afraid I am demonstrating my lack of computer savvy.
>
> As you instructed, I downloaded the code, saved it as the file you
> suggested, and executed this within R
>
> as.numeric(gsub(".* ", "", grep("resolution",
>  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> value = TRUE)))
>
> The DOS window opened, some magic occurred in the blink of an eye, and the
> DOS window closed.  I haven't the foggiest idea what to do next since I can
> see no evidence of having done anything.
>
> Thanks for your patience.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Monday, August 28, 2006 11:39 AM
> To: Charles.Annis at statisticalengineering.com
> Cc: r-help
> Subject: Re: [R] screen resolution effects on graphics
>
> 1.   Put the code from
>  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> into, say, \bin\displayconfiguration.vbs, and then from R do this:
>
> as.numeric(gsub(".* ", "", grep("resolution",
>  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> value = TRUE)))
>
>
> or we can translate that into R to eliminate the need for a vbs routine
> and then run it directly from R (although we will still need the indicated
> dll):
>
> # must have GenericEnum.dll registered. That is, download and unzip:
> #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> # and register it: regsvr32 GenericEnum.dll
>
> library(RDCOMClient)
> strComputer = "."
> SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> colItems <- objWMIService$ExecQuery ("Select * from
> Win32_DisplayConfiguration")
>
> lEnum <- COMCreate("GenericEnum.AutomationEnum")
> lEnum[["Collection"]] <- colItems
>
> if (lEnum$SetFirst()) {
>        repeat {
>                cat(lEnum[["Item"]][["DeviceName"]], "\n")
>                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
>                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
>                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
>                if (!lEnum$SetNext()) break
>        }
> }
>
>
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > My apologies for my oversight.  I am using WindowsXP.  The code that
> > produces a nice-looking jpg (when viewed on my screen) produces cramped
> > graphics on a 800 X 600 screen.  I can change the spacings on the plot and
> > remedy the situation for 800 X 600, but that looks awkward at 1280 X 1024.
> >
> > Thanks
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > Sent: Monday, August 28, 2006 10:55 AM
> > To: Charles.Annis at statisticalengineering.com
> > Cc: r-help
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > You forgot to mention your OS. This was asked before and if I recall
> > correctly the answer for Windows was no. An acceptable solution (imho)
> > is to edit the Rprofile.site files and add something like
> >  pngplotwidth <- 990 ; pngplotheight <- 700
> >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > Then, use these values in your functions. It's manual, but you only
> > need to do this once for each machine.
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > Charles Annis, P.E.
> > > Sent: Monday, August 28, 2006 8:50 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] screen resolution effects on graphics
> > >
> > > Greetings, R-Citizens:
> > >
> > > I have the good fortune of working with a 19" 1280 X 1024
> > > pixel monitor.  My
> > > R-code produces nice-looking graphics on this machine but the
> > > same code
> > > results in crowded plots on an older machine with 800 X 600
> > > resolution.  In
> > > hindsight this seems obvious, but I didn't anticipate it.
> > >
> > > My code will be used on machines with varying graphics (and memory)
> > > capacity.  Is there a way I can check the native resolution
> > > of the machine
> > > so that I can make adjustments to my code for the possible
> > > limitations of
> > > the machine running it?
> > >
> > > Thanks.
> > >
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Mon Aug 28 20:26:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 14:26:54 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>
References: <971536df0608280839h38811522ya25872f7c6fd2061@mail.gmail.com>
	<009a01c6cace$22607ba0$6400a8c0@DD4XFW31>
	<971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>
Message-ID: <971536df0608281126k42f0f8aeld839a657caf3a60e@mail.gmail.com>

One other idea.  Replace gsub with sub and see if that helps.
Maybe the output from the video driver has spaces in it.


On 8/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This is what happens when I run it in R 2.3.1:
>
> > as.numeric(gsub(".* ", "", grep("resolution",
> +  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> + value = TRUE)))
> [1] 1280 1024
>
> The result is obviously dependent on your particular video
> card (I have a radeon).  Try running it from outside of R and see
> what you get:
>
> cd \bin
> cscript displayconfiguration.vbs
>
> Also make sure you copied the code correctly from the web site
> and also try the second solution too just in case.
>
> Its possible that virus detection software will interfere since some
> antivirus programs prevent all vbscript routines from running.
>
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > Gabor:
> >
> > I am afraid I am demonstrating my lack of computer savvy.
> >
> > As you instructed, I downloaded the code, saved it as the file you
> > suggested, and executed this within R
> >
> > as.numeric(gsub(".* ", "", grep("resolution",
> >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > value = TRUE)))
> >
> > The DOS window opened, some magic occurred in the blink of an eye, and the
> > DOS window closed.  I haven't the foggiest idea what to do next since I can
> > see no evidence of having done anything.
> >
> > Thanks for your patience.
> >
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> > Sent: Monday, August 28, 2006 11:39 AM
> > To: Charles.Annis at statisticalengineering.com
> > Cc: r-help
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > 1.   Put the code from
> >  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> > into, say, \bin\displayconfiguration.vbs, and then from R do this:
> >
> > as.numeric(gsub(".* ", "", grep("resolution",
> >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > value = TRUE)))
> >
> >
> > or we can translate that into R to eliminate the need for a vbs routine
> > and then run it directly from R (although we will still need the indicated
> > dll):
> >
> > # must have GenericEnum.dll registered. That is, download and unzip:
> > #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> > # and register it: regsvr32 GenericEnum.dll
> >
> > library(RDCOMClient)
> > strComputer = "."
> > SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> > objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> > colItems <- objWMIService$ExecQuery ("Select * from
> > Win32_DisplayConfiguration")
> >
> > lEnum <- COMCreate("GenericEnum.AutomationEnum")
> > lEnum[["Collection"]] <- colItems
> >
> > if (lEnum$SetFirst()) {
> >        repeat {
> >                cat(lEnum[["Item"]][["DeviceName"]], "\n")
> >                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
> >                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
> >                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
> >                if (!lEnum$SetNext()) break
> >        }
> > }
> >
> >
> >
> > On 8/28/06, Charles Annis, P.E.
> > <Charles.Annis at statisticalengineering.com> wrote:
> > > My apologies for my oversight.  I am using WindowsXP.  The code that
> > > produces a nice-looking jpg (when viewed on my screen) produces cramped
> > > graphics on a 800 X 600 screen.  I can change the spacings on the plot and
> > > remedy the situation for 800 X 600, but that looks awkward at 1280 X 1024.
> > >
> > > Thanks
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > > Sent: Monday, August 28, 2006 10:55 AM
> > > To: Charles.Annis at statisticalengineering.com
> > > Cc: r-help
> > > Subject: Re: [R] screen resolution effects on graphics
> > >
> > > You forgot to mention your OS. This was asked before and if I recall
> > > correctly the answer for Windows was no. An acceptable solution (imho)
> > > is to edit the Rprofile.site files and add something like
> > >  pngplotwidth <- 990 ; pngplotheight <- 700
> > >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > > Then, use these values in your functions. It's manual, but you only
> > > need to do this once for each machine.
> > >
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > Charles Annis, P.E.
> > > > Sent: Monday, August 28, 2006 8:50 AM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] screen resolution effects on graphics
> > > >
> > > > Greetings, R-Citizens:
> > > >
> > > > I have the good fortune of working with a 19" 1280 X 1024
> > > > pixel monitor.  My
> > > > R-code produces nice-looking graphics on this machine but the
> > > > same code
> > > > results in crowded plots on an older machine with 800 X 600
> > > > resolution.  In
> > > > hindsight this seems obvious, but I didn't anticipate it.
> > > >
> > > > My code will be used on machines with varying graphics (and memory)
> > > > capacity.  Is there a way I can check the native resolution
> > > > of the machine
> > > > so that I can make adjustments to my code for the possible
> > > > limitations of
> > > > the machine running it?
> > > >
> > > > Thanks.
> > > >
> > > >
> > > > Charles Annis, P.E.
> > > >
> > > > Charles.Annis at StatisticalEngineering.com
> > > > phone: 561-352-9699
> > > > eFax:  614-455-3265
> > > > http://www.StatisticalEngineering.com
> > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 20:28:30 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 14:28:30 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>
Message-ID: <009b01c6cacf$c31b5ff0$6400a8c0@DD4XFW31>

This is what happened here:

> ls()
character(0)
> as.numeric(gsub(".* ", "", grep("resolution",
+   shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE), value =
TRUE)))
numeric(0)
> 
> 
> ls()
character(0)
>

Nothing displayed; nothing created.  But the DOS window DID open and
something happened, but I don't know what.

Agin thanks for your patience.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Monday, August 28, 2006 2:21 PM
To: Charles.Annis at statisticalengineering.com
Cc: r-help
Subject: Re: [R] screen resolution effects on graphics

This is what happens when I run it in R 2.3.1:

> as.numeric(gsub(".* ", "", grep("resolution",
+  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
+ value = TRUE)))
[1] 1280 1024

The result is obviously dependent on your particular video
card (I have a radeon).  Try running it from outside of R and see
what you get:

cd \bin
cscript displayconfiguration.vbs

Also make sure you copied the code correctly from the web site
and also try the second solution too just in case.

Its possible that virus detection software will interfere since some
antivirus programs prevent all vbscript routines from running.


On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Gabor:
>
> I am afraid I am demonstrating my lack of computer savvy.
>
> As you instructed, I downloaded the code, saved it as the file you
> suggested, and executed this within R
>
> as.numeric(gsub(".* ", "", grep("resolution",
>  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> value = TRUE)))
>
> The DOS window opened, some magic occurred in the blink of an eye, and the
> DOS window closed.  I haven't the foggiest idea what to do next since I
can
> see no evidence of having done anything.
>
> Thanks for your patience.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Monday, August 28, 2006 11:39 AM
> To: Charles.Annis at statisticalengineering.com
> Cc: r-help
> Subject: Re: [R] screen resolution effects on graphics
>
> 1.   Put the code from
>  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> into, say, \bin\displayconfiguration.vbs, and then from R do this:
>
> as.numeric(gsub(".* ", "", grep("resolution",
>  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> value = TRUE)))
>
>
> or we can translate that into R to eliminate the need for a vbs routine
> and then run it directly from R (although we will still need the indicated
> dll):
>
> # must have GenericEnum.dll registered. That is, download and unzip:
> #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> # and register it: regsvr32 GenericEnum.dll
>
> library(RDCOMClient)
> strComputer = "."
> SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> colItems <- objWMIService$ExecQuery ("Select * from
> Win32_DisplayConfiguration")
>
> lEnum <- COMCreate("GenericEnum.AutomationEnum")
> lEnum[["Collection"]] <- colItems
>
> if (lEnum$SetFirst()) {
>        repeat {
>                cat(lEnum[["Item"]][["DeviceName"]], "\n")
>                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
>                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
>                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
>                if (!lEnum$SetNext()) break
>        }
> }
>
>
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > My apologies for my oversight.  I am using WindowsXP.  The code that
> > produces a nice-looking jpg (when viewed on my screen) produces cramped
> > graphics on a 800 X 600 screen.  I can change the spacings on the plot
and
> > remedy the situation for 800 X 600, but that looks awkward at 1280 X
1024.
> >
> > Thanks
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > Sent: Monday, August 28, 2006 10:55 AM
> > To: Charles.Annis at statisticalengineering.com
> > Cc: r-help
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > You forgot to mention your OS. This was asked before and if I recall
> > correctly the answer for Windows was no. An acceptable solution (imho)
> > is to edit the Rprofile.site files and add something like
> >  pngplotwidth <- 990 ; pngplotheight <- 700
> >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > Then, use these values in your functions. It's manual, but you only
> > need to do this once for each machine.
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > Charles Annis, P.E.
> > > Sent: Monday, August 28, 2006 8:50 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] screen resolution effects on graphics
> > >
> > > Greetings, R-Citizens:
> > >
> > > I have the good fortune of working with a 19" 1280 X 1024
> > > pixel monitor.  My
> > > R-code produces nice-looking graphics on this machine but the
> > > same code
> > > results in crowded plots on an older machine with 800 X 600
> > > resolution.  In
> > > hindsight this seems obvious, but I didn't anticipate it.
> > >
> > > My code will be used on machines with varying graphics (and memory)
> > > capacity.  Is there a way I can check the native resolution
> > > of the machine
> > > so that I can make adjustments to my code for the possible
> > > limitations of
> > > the machine running it?
> > >
> > > Thanks.
> > >
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From rfrancois at mango-solutions.com  Mon Aug 28 20:29:32 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Mon, 28 Aug 2006 19:29:32 +0100
Subject: [R] screen resolution effects on graphics
In-Reply-To: <Pine.LNX.4.64.0608281356380.15632@gannet.stats.ox.ac.uk>
References: <006001c6caa0$83250f60$6400a8c0@DD4XFW31>
	<Pine.LNX.4.64.0608281356380.15632@gannet.stats.ox.ac.uk>
Message-ID: <44F3360C.5020606@mango-solutions.com>

Prof Brian Ripley a ?crit :
> On Mon, 28 Aug 2006, Charles Annis, P.E. wrote:
>
>   
>> Greetings, R-Citizens:
>>
>> I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.  My
>>     
>
> (Similar to our student lab has used for many years.)
>
>   
>> R-code produces nice-looking graphics on this machine but the same code
>> results in crowded plots on an older machine with 800 X 600 resolution.  In
>> hindsight this seems obvious, but I didn't anticipate it.
>>     
>
> It is not obvious to me: I have never experienced it.  What OS and 
> graphics device is this?
>
> Almost all of R's graphics is independent of the screen resolution (the 
> exception being the bitmapped devices such as jpeg), with things sized in 
> inches or points. My machines are 1600x1200 (apart from 1280x800 on my 
> laptop), so I meet a considerable reduction when using a computer 
> projector, and my plots do not look crowded.
>
> However, one issue is when the OS has a seriously incorrect setting for 
> the screen resolution and so does not give the sizes asked for by R.  We 
> have seen that on both Linux and Windows, and the windows() device has 
> arguments to set the correct values.  (On X11 you should be able to set 
> this in Xconfig files.)
>
> If this is Windows, check carefully the description of the initial screen 
> size in ?windows.  That can have unexpected effects on physically small 
> screens.
>
> At one time the X11() device was set up to assume 75dpi unless the 
> reported resolution was 100+/-0.5dpi.  My then monitor reported 99.2 dpi 
> and so things came out at 3/4 of the intended size.  We fixed that quite a 
> while back.
>
>   
>> My code will be used on machines with varying graphics (and memory)
>> capacity.  Is there a way I can check the native resolution of the machine
>> so that I can make adjustments to my code for the possible limitations of
>> the machine running it?
>>     
>
> Only via C code, which is how R does it.
Hi,

Javascript knows, can we ask him ?

I mean, if I do that in R :

a <- tempfile()
cat('<html><script type="text/javascript"> document.write(screen.width) 
;  </script></html>', file=a)
browseURL(a)

I get "1920" in my browser's window. Can R read it ?

Romain


From ggrothendieck at gmail.com  Mon Aug 28 20:30:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 14:30:56 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608281126k42f0f8aeld839a657caf3a60e@mail.gmail.com>
References: <971536df0608280839h38811522ya25872f7c6fd2061@mail.gmail.com>
	<009a01c6cace$22607ba0$6400a8c0@DD4XFW31>
	<971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>
	<971536df0608281126k42f0f8aeld839a657caf3a60e@mail.gmail.com>
Message-ID: <971536df0608281130r5ff19880mf38f1aa1d7994707@mail.gmail.com>

Sorry, that still woudl not be good enough.  Try this:

as.numeric(sub("^[^:]*:", "", grep("resolution",
shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
value = TRUE)))



On 8/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> One other idea.  Replace gsub with sub and see if that helps.
> Maybe the output from the video driver has spaces in it.
>
>
> On 8/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > This is what happens when I run it in R 2.3.1:
> >
> > > as.numeric(gsub(".* ", "", grep("resolution",
> > +  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > + value = TRUE)))
> > [1] 1280 1024
> >
> > The result is obviously dependent on your particular video
> > card (I have a radeon).  Try running it from outside of R and see
> > what you get:
> >
> > cd \bin
> > cscript displayconfiguration.vbs
> >
> > Also make sure you copied the code correctly from the web site
> > and also try the second solution too just in case.
> >
> > Its possible that virus detection software will interfere since some
> > antivirus programs prevent all vbscript routines from running.
> >
> >
> > On 8/28/06, Charles Annis, P.E.
> > <Charles.Annis at statisticalengineering.com> wrote:
> > > Gabor:
> > >
> > > I am afraid I am demonstrating my lack of computer savvy.
> > >
> > > As you instructed, I downloaded the code, saved it as the file you
> > > suggested, and executed this within R
> > >
> > > as.numeric(gsub(".* ", "", grep("resolution",
> > >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > > value = TRUE)))
> > >
> > > The DOS window opened, some magic occurred in the blink of an eye, and the
> > > DOS window closed.  I haven't the foggiest idea what to do next since I can
> > > see no evidence of having done anything.
> > >
> > > Thanks for your patience.
> > >
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> > > Sent: Monday, August 28, 2006 11:39 AM
> > > To: Charles.Annis at statisticalengineering.com
> > > Cc: r-help
> > > Subject: Re: [R] screen resolution effects on graphics
> > >
> > > 1.   Put the code from
> > >  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> > > into, say, \bin\displayconfiguration.vbs, and then from R do this:
> > >
> > > as.numeric(gsub(".* ", "", grep("resolution",
> > >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > > value = TRUE)))
> > >
> > >
> > > or we can translate that into R to eliminate the need for a vbs routine
> > > and then run it directly from R (although we will still need the indicated
> > > dll):
> > >
> > > # must have GenericEnum.dll registered. That is, download and unzip:
> > > #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> > > # and register it: regsvr32 GenericEnum.dll
> > >
> > > library(RDCOMClient)
> > > strComputer = "."
> > > SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> > > objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> > > colItems <- objWMIService$ExecQuery ("Select * from
> > > Win32_DisplayConfiguration")
> > >
> > > lEnum <- COMCreate("GenericEnum.AutomationEnum")
> > > lEnum[["Collection"]] <- colItems
> > >
> > > if (lEnum$SetFirst()) {
> > >        repeat {
> > >                cat(lEnum[["Item"]][["DeviceName"]], "\n")
> > >                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
> > >                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
> > >                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
> > >                if (!lEnum$SetNext()) break
> > >        }
> > > }
> > >
> > >
> > >
> > > On 8/28/06, Charles Annis, P.E.
> > > <Charles.Annis at statisticalengineering.com> wrote:
> > > > My apologies for my oversight.  I am using WindowsXP.  The code that
> > > > produces a nice-looking jpg (when viewed on my screen) produces cramped
> > > > graphics on a 800 X 600 screen.  I can change the spacings on the plot and
> > > > remedy the situation for 800 X 600, but that looks awkward at 1280 X 1024.
> > > >
> > > > Thanks
> > > >
> > > > Charles Annis, P.E.
> > > >
> > > > Charles.Annis at StatisticalEngineering.com
> > > > phone: 561-352-9699
> > > > eFax:  614-455-3265
> > > > http://www.StatisticalEngineering.com
> > > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > > > Sent: Monday, August 28, 2006 10:55 AM
> > > > To: Charles.Annis at statisticalengineering.com
> > > > Cc: r-help
> > > > Subject: Re: [R] screen resolution effects on graphics
> > > >
> > > > You forgot to mention your OS. This was asked before and if I recall
> > > > correctly the answer for Windows was no. An acceptable solution (imho)
> > > > is to edit the Rprofile.site files and add something like
> > > >  pngplotwidth <- 990 ; pngplotheight <- 700
> > > >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > > > Then, use these values in your functions. It's manual, but you only
> > > > need to do this once for each machine.
> > > >
> > > >
> > > > > -----Original Message-----
> > > > > From: r-help-bounces at stat.math.ethz.ch
> > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > > Charles Annis, P.E.
> > > > > Sent: Monday, August 28, 2006 8:50 AM
> > > > > To: r-help at stat.math.ethz.ch
> > > > > Subject: [R] screen resolution effects on graphics
> > > > >
> > > > > Greetings, R-Citizens:
> > > > >
> > > > > I have the good fortune of working with a 19" 1280 X 1024
> > > > > pixel monitor.  My
> > > > > R-code produces nice-looking graphics on this machine but the
> > > > > same code
> > > > > results in crowded plots on an older machine with 800 X 600
> > > > > resolution.  In
> > > > > hindsight this seems obvious, but I didn't anticipate it.
> > > > >
> > > > > My code will be used on machines with varying graphics (and memory)
> > > > > capacity.  Is there a way I can check the native resolution
> > > > > of the machine
> > > > > so that I can make adjustments to my code for the possible
> > > > > limitations of
> > > > > the machine running it?
> > > > >
> > > > > Thanks.
> > > > >
> > > > >
> > > > > Charles Annis, P.E.
> > > > >
> > > > > Charles.Annis at StatisticalEngineering.com
> > > > > phone: 561-352-9699
> > > > > eFax:  614-455-3265
> > > > > http://www.StatisticalEngineering.com
> > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> >
>


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 20:33:13 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 14:33:13 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608281126k42f0f8aeld839a657caf3a60e@mail.gmail.com>
Message-ID: <009c01c6cad0$6be8b920$6400a8c0@DD4XFW31>

Using sub rather than gsub appears to have no effect.  The DOS window opens,
then closes having had no noticeable effect.



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Monday, August 28, 2006 2:27 PM
To: Charles.Annis at statisticalengineering.com
Cc: r-help
Subject: Re: [R] screen resolution effects on graphics

One other idea.  Replace gsub with sub and see if that helps.
Maybe the output from the video driver has spaces in it.


On 8/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This is what happens when I run it in R 2.3.1:
>
> > as.numeric(gsub(".* ", "", grep("resolution",
> +  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> + value = TRUE)))
> [1] 1280 1024
>
> The result is obviously dependent on your particular video
> card (I have a radeon).  Try running it from outside of R and see
> what you get:
>
> cd \bin
> cscript displayconfiguration.vbs
>
> Also make sure you copied the code correctly from the web site
> and also try the second solution too just in case.
>
> Its possible that virus detection software will interfere since some
> antivirus programs prevent all vbscript routines from running.
>
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > Gabor:
> >
> > I am afraid I am demonstrating my lack of computer savvy.
> >
> > As you instructed, I downloaded the code, saved it as the file you
> > suggested, and executed this within R
> >
> > as.numeric(gsub(".* ", "", grep("resolution",
> >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > value = TRUE)))
> >
> > The DOS window opened, some magic occurred in the blink of an eye, and
the
> > DOS window closed.  I haven't the foggiest idea what to do next since I
can
> > see no evidence of having done anything.
> >
> > Thanks for your patience.
> >
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
> > Sent: Monday, August 28, 2006 11:39 AM
> > To: Charles.Annis at statisticalengineering.com
> > Cc: r-help
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > 1.   Put the code from
> >
www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> > into, say, \bin\displayconfiguration.vbs, and then from R do this:
> >
> > as.numeric(gsub(".* ", "", grep("resolution",
> >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > value = TRUE)))
> >
> >
> > or we can translate that into R to eliminate the need for a vbs routine
> > and then run it directly from R (although we will still need the
indicated
> > dll):
> >
> > # must have GenericEnum.dll registered. That is, download and unzip:
> > #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> > # and register it: regsvr32 GenericEnum.dll
> >
> > library(RDCOMClient)
> > strComputer = "."
> > SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> > objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> > colItems <- objWMIService$ExecQuery ("Select * from
> > Win32_DisplayConfiguration")
> >
> > lEnum <- COMCreate("GenericEnum.AutomationEnum")
> > lEnum[["Collection"]] <- colItems
> >
> > if (lEnum$SetFirst()) {
> >        repeat {
> >                cat(lEnum[["Item"]][["DeviceName"]], "\n")
> >                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
> >                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
> >                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
> >                if (!lEnum$SetNext()) break
> >        }
> > }
> >
> >
> >
> > On 8/28/06, Charles Annis, P.E.
> > <Charles.Annis at statisticalengineering.com> wrote:
> > > My apologies for my oversight.  I am using WindowsXP.  The code that
> > > produces a nice-looking jpg (when viewed on my screen) produces
cramped
> > > graphics on a 800 X 600 screen.  I can change the spacings on the plot
and
> > > remedy the situation for 800 X 600, but that looks awkward at 1280 X
1024.
> > >
> > > Thanks
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > > Sent: Monday, August 28, 2006 10:55 AM
> > > To: Charles.Annis at statisticalengineering.com
> > > Cc: r-help
> > > Subject: Re: [R] screen resolution effects on graphics
> > >
> > > You forgot to mention your OS. This was asked before and if I recall
> > > correctly the answer for Windows was no. An acceptable solution (imho)
> > > is to edit the Rprofile.site files and add something like
> > >  pngplotwidth <- 990 ; pngplotheight <- 700
> > >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > > Then, use these values in your functions. It's manual, but you only
> > > need to do this once for each machine.
> > >
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > Charles Annis, P.E.
> > > > Sent: Monday, August 28, 2006 8:50 AM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] screen resolution effects on graphics
> > > >
> > > > Greetings, R-Citizens:
> > > >
> > > > I have the good fortune of working with a 19" 1280 X 1024
> > > > pixel monitor.  My
> > > > R-code produces nice-looking graphics on this machine but the
> > > > same code
> > > > results in crowded plots on an older machine with 800 X 600
> > > > resolution.  In
> > > > hindsight this seems obvious, but I didn't anticipate it.
> > > >
> > > > My code will be used on machines with varying graphics (and memory)
> > > > capacity.  Is there a way I can check the native resolution
> > > > of the machine
> > > > so that I can make adjustments to my code for the possible
> > > > limitations of
> > > > the machine running it?
> > > >
> > > > Thanks.
> > > >
> > > >
> > > > Charles Annis, P.E.
> > > >
> > > > Charles.Annis at StatisticalEngineering.com
> > > > phone: 561-352-9699
> > > > eFax:  614-455-3265
> > > > http://www.StatisticalEngineering.com
> > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 20:42:51 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 14:42:51 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>
Message-ID: <00a001c6cad1$c443ef80$6400a8c0@DD4XFW31>

Gabor:

Success!  (Sort of)  Running outside R in the DOS window produces

C:\Program Files\R\R-2.3.1\bin>cscript displayconfiguration.vbs
Microsoft (R) Windows Script Host Version 5.6
Copyright (C) Microsoft Corporation 1996-2001. All rights reserved.

Name: NVIDIA GeForce FX 5200
Color depth: 32
Horizontal resolution: 1280
Vertical resolution: 1024

I don't have immediate access to a machine with 800X600 native resolution so
I can't compare.  

What's next?

The problem isn't that a jpg produced on this machine looks crowed on an 800
X 600 machine.  It's that the same R code on this machine generates a
crowded jpeg on the older machine.  My hope was to check the capability of
the current machine and make some modest adjustments to my R code to produce
an acceptable jpg on any machine running it.

Thanks.



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: Monday, August 28, 2006 2:21 PM
To: Charles.Annis at statisticalengineering.com
Cc: r-help
Subject: Re: [R] screen resolution effects on graphics

This is what happens when I run it in R 2.3.1:

> as.numeric(gsub(".* ", "", grep("resolution",
+  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
+ value = TRUE)))
[1] 1280 1024

The result is obviously dependent on your particular video
card (I have a radeon).  Try running it from outside of R and see
what you get:

cd \bin
cscript displayconfiguration.vbs

Also make sure you copied the code correctly from the web site
and also try the second solution too just in case.

Its possible that virus detection software will interfere since some
antivirus programs prevent all vbscript routines from running.


On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Gabor:
>
> I am afraid I am demonstrating my lack of computer savvy.
>
> As you instructed, I downloaded the code, saved it as the file you
> suggested, and executed this within R
>
> as.numeric(gsub(".* ", "", grep("resolution",
>  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> value = TRUE)))
>
> The DOS window opened, some magic occurred in the blink of an eye, and the
> DOS window closed.  I haven't the foggiest idea what to do next since I
can
> see no evidence of having done anything.
>
> Thanks for your patience.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Monday, August 28, 2006 11:39 AM
> To: Charles.Annis at statisticalengineering.com
> Cc: r-help
> Subject: Re: [R] screen resolution effects on graphics
>
> 1.   Put the code from
>  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> into, say, \bin\displayconfiguration.vbs, and then from R do this:
>
> as.numeric(gsub(".* ", "", grep("resolution",
>  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> value = TRUE)))
>
>
> or we can translate that into R to eliminate the need for a vbs routine
> and then run it directly from R (although we will still need the indicated
> dll):
>
> # must have GenericEnum.dll registered. That is, download and unzip:
> #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> # and register it: regsvr32 GenericEnum.dll
>
> library(RDCOMClient)
> strComputer = "."
> SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> colItems <- objWMIService$ExecQuery ("Select * from
> Win32_DisplayConfiguration")
>
> lEnum <- COMCreate("GenericEnum.AutomationEnum")
> lEnum[["Collection"]] <- colItems
>
> if (lEnum$SetFirst()) {
>        repeat {
>                cat(lEnum[["Item"]][["DeviceName"]], "\n")
>                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
>                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
>                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
>                if (!lEnum$SetNext()) break
>        }
> }
>
>
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > My apologies for my oversight.  I am using WindowsXP.  The code that
> > produces a nice-looking jpg (when viewed on my screen) produces cramped
> > graphics on a 800 X 600 screen.  I can change the spacings on the plot
and
> > remedy the situation for 800 X 600, but that looks awkward at 1280 X
1024.
> >
> > Thanks
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > Sent: Monday, August 28, 2006 10:55 AM
> > To: Charles.Annis at statisticalengineering.com
> > Cc: r-help
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > You forgot to mention your OS. This was asked before and if I recall
> > correctly the answer for Windows was no. An acceptable solution (imho)
> > is to edit the Rprofile.site files and add something like
> >  pngplotwidth <- 990 ; pngplotheight <- 700
> >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > Then, use these values in your functions. It's manual, but you only
> > need to do this once for each machine.
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > Charles Annis, P.E.
> > > Sent: Monday, August 28, 2006 8:50 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] screen resolution effects on graphics
> > >
> > > Greetings, R-Citizens:
> > >
> > > I have the good fortune of working with a 19" 1280 X 1024
> > > pixel monitor.  My
> > > R-code produces nice-looking graphics on this machine but the
> > > same code
> > > results in crowded plots on an older machine with 800 X 600
> > > resolution.  In
> > > hindsight this seems obvious, but I didn't anticipate it.
> > >
> > > My code will be used on machines with varying graphics (and memory)
> > > capacity.  Is there a way I can check the native resolution
> > > of the machine
> > > so that I can make adjustments to my code for the possible
> > > limitations of
> > > the machine running it?
> > >
> > > Thanks.
> > >
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Mon Aug 28 20:55:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 14:55:49 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <00a001c6cad1$c443ef80$6400a8c0@DD4XFW31>
References: <971536df0608281120i1bf5f626s755e5affe5107d25@mail.gmail.com>
	<00a001c6cad1$c443ef80$6400a8c0@DD4XFW31>
Message-ID: <971536df0608281155u6da876afm9c621337688ed52d@mail.gmail.com>

The problem then was likely just with my regexp.  The last revision
that I posted should hopefully work on your machine (all of them
worked on mine).



On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Gabor:
>
> Success!  (Sort of)  Running outside R in the DOS window produces
>
> C:\Program Files\R\R-2.3.1\bin>cscript displayconfiguration.vbs
> Microsoft (R) Windows Script Host Version 5.6
> Copyright (C) Microsoft Corporation 1996-2001. All rights reserved.
>
> Name: NVIDIA GeForce FX 5200
> Color depth: 32
> Horizontal resolution: 1280
> Vertical resolution: 1024
>
> I don't have immediate access to a machine with 800X600 native resolution so
> I can't compare.
>
> What's next?
>
> The problem isn't that a jpg produced on this machine looks crowed on an 800
> X 600 machine.  It's that the same R code on this machine generates a
> crowded jpeg on the older machine.  My hope was to check the capability of
> the current machine and make some modest adjustments to my R code to produce
> an acceptable jpg on any machine running it.
>
> Thanks.
>
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Monday, August 28, 2006 2:21 PM
> To: Charles.Annis at statisticalengineering.com
> Cc: r-help
> Subject: Re: [R] screen resolution effects on graphics
>
> This is what happens when I run it in R 2.3.1:
>
> > as.numeric(gsub(".* ", "", grep("resolution",
> +  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> + value = TRUE)))
> [1] 1280 1024
>
> The result is obviously dependent on your particular video
> card (I have a radeon).  Try running it from outside of R and see
> what you get:
>
> cd \bin
> cscript displayconfiguration.vbs
>
> Also make sure you copied the code correctly from the web site
> and also try the second solution too just in case.
>
> Its possible that virus detection software will interfere since some
> antivirus programs prevent all vbscript routines from running.
>
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > Gabor:
> >
> > I am afraid I am demonstrating my lack of computer savvy.
> >
> > As you instructed, I downloaded the code, saved it as the file you
> > suggested, and executed this within R
> >
> > as.numeric(gsub(".* ", "", grep("resolution",
> >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > value = TRUE)))
> >
> > The DOS window opened, some magic occurred in the blink of an eye, and the
> > DOS window closed.  I haven't the foggiest idea what to do next since I
> can
> > see no evidence of having done anything.
> >
> > Thanks for your patience.
> >
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> > Sent: Monday, August 28, 2006 11:39 AM
> > To: Charles.Annis at statisticalengineering.com
> > Cc: r-help
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > 1.   Put the code from
> >  www.microsoft.com/technet/scriptcenter/resources/qanda/jul05/hey0721.mspx
> > into, say, \bin\displayconfiguration.vbs, and then from R do this:
> >
> > as.numeric(gsub(".* ", "", grep("resolution",
> >  shell('cscript \\bin\\displayconfiguration.vbs', intern = TRUE),
> > value = TRUE)))
> >
> >
> > or we can translate that into R to eliminate the need for a vbs routine
> > and then run it directly from R (although we will still need the indicated
> > dll):
> >
> > # must have GenericEnum.dll registered. That is, download and unzip:
> > #  http://sunsite.univie.ac.at/rcom/download/GenericEnum.zip
> > # and register it: regsvr32 GenericEnum.dll
> >
> > library(RDCOMClient)
> > strComputer = "."
> > SWBemlocator <- COMCreate("WbemScripting.SWbemLocator")
> > objWMIService <- SWBemlocator$ConnectServer(strComputer,"\\root\\CIMV2")
> > colItems <- objWMIService$ExecQuery ("Select * from
> > Win32_DisplayConfiguration")
> >
> > lEnum <- COMCreate("GenericEnum.AutomationEnum")
> > lEnum[["Collection"]] <- colItems
> >
> > if (lEnum$SetFirst()) {
> >        repeat {
> >                cat(lEnum[["Item"]][["DeviceName"]], "\n")
> >                cat(lEnum[["Item"]][["BitsPerPel"]], "\n")
> >                cat(lEnum[["Item"]][["Pelswidth"]], "\n")
> >                cat(lEnum[["Item"]][["Pelsheight"]], "\n")
> >                if (!lEnum$SetNext()) break
> >        }
> > }
> >
> >
> >
> > On 8/28/06, Charles Annis, P.E.
> > <Charles.Annis at statisticalengineering.com> wrote:
> > > My apologies for my oversight.  I am using WindowsXP.  The code that
> > > produces a nice-looking jpg (when viewed on my screen) produces cramped
> > > graphics on a 800 X 600 screen.  I can change the spacings on the plot
> and
> > > remedy the situation for 800 X 600, but that looks awkward at 1280 X
> 1024.
> > >
> > > Thanks
> > >
> > > Charles Annis, P.E.
> > >
> > > Charles.Annis at StatisticalEngineering.com
> > > phone: 561-352-9699
> > > eFax:  614-455-3265
> > > http://www.StatisticalEngineering.com
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> > > Sent: Monday, August 28, 2006 10:55 AM
> > > To: Charles.Annis at statisticalengineering.com
> > > Cc: r-help
> > > Subject: Re: [R] screen resolution effects on graphics
> > >
> > > You forgot to mention your OS. This was asked before and if I recall
> > > correctly the answer for Windows was no. An acceptable solution (imho)
> > > is to edit the Rprofile.site files and add something like
> > >  pngplotwidth <- 990 ; pngplotheight <- 700
> > >  pdfplotwidth <- 14 ; pdfplotheight <- 10
> > > Then, use these values in your functions. It's manual, but you only
> > > need to do this once for each machine.
> > >
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > Charles Annis, P.E.
> > > > Sent: Monday, August 28, 2006 8:50 AM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] screen resolution effects on graphics
> > > >
> > > > Greetings, R-Citizens:
> > > >
> > > > I have the good fortune of working with a 19" 1280 X 1024
> > > > pixel monitor.  My
> > > > R-code produces nice-looking graphics on this machine but the
> > > > same code
> > > > results in crowded plots on an older machine with 800 X 600
> > > > resolution.  In
> > > > hindsight this seems obvious, but I didn't anticipate it.
> > > >
> > > > My code will be used on machines with varying graphics (and memory)
> > > > capacity.  Is there a way I can check the native resolution
> > > > of the machine
> > > > so that I can make adjustments to my code for the possible
> > > > limitations of
> > > > the machine running it?
> > > >
> > > > Thanks.
> > > >
> > > >
> > > > Charles Annis, P.E.
> > > >
> > > > Charles.Annis at StatisticalEngineering.com
> > > > phone: 561-352-9699
> > > > eFax:  614-455-3265
> > > > http://www.StatisticalEngineering.com
> > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From Charles.Annis at StatisticalEngineering.com  Mon Aug 28 20:58:44 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 28 Aug 2006 14:58:44 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <44F3360C.5020606@mango-solutions.com>
Message-ID: <00a101c6cad3$fcbea2e0$6400a8c0@DD4XFW31>

Romain:

> a <- tempfile()
> cat('<html><script type="text/javascript"> document.write(screen.width) ;
</script></html>', file=a)
> browseURL(a)
>

The object "a" was created, but  no browser opened.
 
> ls()
[1] "a"
> a
[1] "C:\\DOCUME~1\\CHARLE~1\\LOCALS~1\\Temp\\RtmpRgWrqb\\file678418be"
>



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Romain Francois
Sent: Monday, August 28, 2006 2:30 PM
To: Prof Brian Ripley
Cc: Charles Annis, P.E.; r-help at stat.math.ethz.ch
Subject: Re: [R] screen resolution effects on graphics

Prof Brian Ripley a ?crit :
> On Mon, 28 Aug 2006, Charles Annis, P.E. wrote:
>
>   
>> Greetings, R-Citizens:
>>
>> I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.
My
>>     
>
> (Similar to our student lab has used for many years.)
>
>   
>> R-code produces nice-looking graphics on this machine but the same code
>> results in crowded plots on an older machine with 800 X 600 resolution.
In
>> hindsight this seems obvious, but I didn't anticipate it.
>>     
>
> It is not obvious to me: I have never experienced it.  What OS and 
> graphics device is this?
>
> Almost all of R's graphics is independent of the screen resolution (the 
> exception being the bitmapped devices such as jpeg), with things sized in 
> inches or points. My machines are 1600x1200 (apart from 1280x800 on my 
> laptop), so I meet a considerable reduction when using a computer 
> projector, and my plots do not look crowded.
>
> However, one issue is when the OS has a seriously incorrect setting for 
> the screen resolution and so does not give the sizes asked for by R.  We 
> have seen that on both Linux and Windows, and the windows() device has 
> arguments to set the correct values.  (On X11 you should be able to set 
> this in Xconfig files.)
>
> If this is Windows, check carefully the description of the initial screen 
> size in ?windows.  That can have unexpected effects on physically small 
> screens.
>
> At one time the X11() device was set up to assume 75dpi unless the 
> reported resolution was 100+/-0.5dpi.  My then monitor reported 99.2 dpi 
> and so things came out at 3/4 of the intended size.  We fixed that quite a

> while back.
>
>   
>> My code will be used on machines with varying graphics (and memory)
>> capacity.  Is there a way I can check the native resolution of the
machine
>> so that I can make adjustments to my code for the possible limitations of
>> the machine running it?
>>     
>
> Only via C code, which is how R does it.
Hi,

Javascript knows, can we ask him ?

I mean, if I do that in R :

a <- tempfile()
cat('<html><script type="text/javascript"> document.write(screen.width) 
;  </script></html>', file=a)
browseURL(a)

I get "1920" in my browser's window. Can R read it ?

Romain

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Mon Aug 28 21:13:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 15:13:38 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <00a101c6cad3$fcbea2e0$6400a8c0@DD4XFW31>
References: <44F3360C.5020606@mango-solutions.com>
	<00a101c6cad3$fcbea2e0$6400a8c0@DD4XFW31>
Message-ID: <971536df0608281213r3a061b7br4284d7358bb711@mail.gmail.com>

I think there are problems with browseURL on Windows XP with IE.
Put the file in screen.htm and then run:

shell.exec("screen.htm")

it will come up -- it will block it and you will have to click to
unblock it; however,
that will interfere with any automatic procedure.

On 8/28/06, Charles Annis, P.E.
<Charles.Annis at statisticalengineering.com> wrote:
> Romain:
>
> > a <- tempfile()
> > cat('<html><script type="text/javascript"> document.write(screen.width) ;
> </script></html>', file=a)
> > browseURL(a)
> >
>
> The object "a" was created, but  no browser opened.
>
> > ls()
> [1] "a"
> > a
> [1] "C:\\DOCUME~1\\CHARLE~1\\LOCALS~1\\Temp\\RtmpRgWrqb\\file678418be"
> >
>
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Romain Francois
> Sent: Monday, August 28, 2006 2:30 PM
> To: Prof Brian Ripley
> Cc: Charles Annis, P.E.; r-help at stat.math.ethz.ch
> Subject: Re: [R] screen resolution effects on graphics
>
> Prof Brian Ripley a ?crit :
> > On Mon, 28 Aug 2006, Charles Annis, P.E. wrote:
> >
> >
> >> Greetings, R-Citizens:
> >>
> >> I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.
> My
> >>
> >
> > (Similar to our student lab has used for many years.)
> >
> >
> >> R-code produces nice-looking graphics on this machine but the same code
> >> results in crowded plots on an older machine with 800 X 600 resolution.
> In
> >> hindsight this seems obvious, but I didn't anticipate it.
> >>
> >
> > It is not obvious to me: I have never experienced it.  What OS and
> > graphics device is this?
> >
> > Almost all of R's graphics is independent of the screen resolution (the
> > exception being the bitmapped devices such as jpeg), with things sized in
> > inches or points. My machines are 1600x1200 (apart from 1280x800 on my
> > laptop), so I meet a considerable reduction when using a computer
> > projector, and my plots do not look crowded.
> >
> > However, one issue is when the OS has a seriously incorrect setting for
> > the screen resolution and so does not give the sizes asked for by R.  We
> > have seen that on both Linux and Windows, and the windows() device has
> > arguments to set the correct values.  (On X11 you should be able to set
> > this in Xconfig files.)
> >
> > If this is Windows, check carefully the description of the initial screen
> > size in ?windows.  That can have unexpected effects on physically small
> > screens.
> >
> > At one time the X11() device was set up to assume 75dpi unless the
> > reported resolution was 100+/-0.5dpi.  My then monitor reported 99.2 dpi
> > and so things came out at 3/4 of the intended size.  We fixed that quite a
>
> > while back.
> >
> >
> >> My code will be used on machines with varying graphics (and memory)
> >> capacity.  Is there a way I can check the native resolution of the
> machine
> >> so that I can make adjustments to my code for the possible limitations of
> >> the machine running it?
> >>
> >
> > Only via C code, which is how R does it.
> Hi,
>
> Javascript knows, can we ask him ?
>
> I mean, if I do that in R :
>
> a <- tempfile()
> cat('<html><script type="text/javascript"> document.write(screen.width)
> ;  </script></html>', file=a)
> browseURL(a)
>
> I get "1920" in my browser's window. Can R read it ?
>
> Romain
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmmaia at unity.ncsu.edu  Mon Aug 28 21:16:11 2006
From: jmmaia at unity.ncsu.edu (Jessica M. Maia)
Date: Mon, 28 Aug 2006 15:16:11 -0400 (EDT)
Subject: [R] matrix "Adjoint" function
Message-ID: <Pine.GSO.4.60.0608281514010.14017@uni03du.unity.ncsu.edu>

Hi there,

I'm new to R and despite searching today, I can't
find a function which will compute the adjoint of
a matrix A. Does this adjoint function exist in R?

Thanks in advance!


From miralisus at gmail.com  Mon Aug 28 21:20:44 2006
From: miralisus at gmail.com (Amasco Miralisus)
Date: Mon, 28 Aug 2006 21:20:44 +0200
Subject: [R] Type II and III sum of square in Anova (R, car package)
In-Reply-To: <20060827135157.QPGG24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
	<20060827135157.QPGG24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <66d34c7a0608281220h2f460cc2q861d19962e7b4301@mail.gmail.com>

Hello,

First of all, I would like to thank everybody who answered my
question. Every post has added something to my knowledge of the topic.
I now know why Type III SS are so questionable.

As I understood form R FAQ, there is disagreement among Statisticians
which SS to use
(http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-does-the-output-from-anova_0028_0029-depend-on-the-order-of-factors-in-the-model_003f).
However, most commercial statistical packages use Type III as the
default (with orthogonal contrasts), just as STATISTICA, from which I
am currently trying to migrate to R. This was probably was done for
the convenience of end-users who are not very experienced in
theoretical statistics.

I am aware that the same result could be produced using the standard
anova() function with Type I "sequential" SS, supplemented by drop1()
function, but this approach will look quite complicated for persons
without any substantial background in statistics, like no-math
students. I would prefer easier way, possibly more universal, though
also probably more "for dummies" :) If am not mistaken, car package by
John Fox with his nice Anova() function is the reasonable alternative
for any, who wish to simply perform quick statistical analysis,
without afraid to mess something with model fitting. Of course
orthogonal contrasts have to be specified (for example contr.sum) in
case of Type III SS.

Therefore, I would like to reformulate my questions, to make it easier
for you to answer:

1. The first question related to answer by Professor Brian Ripley: Did
I understood correctly from the advised paper (Bill Venables'
'exegeses' paper) that there is not much sense to test main effects if
the interaction is significant?

2. If I understood the post by John Fox correctly, I could safely use
Anova(?,type="III") function from car for ANOVA analyses in R, both
for balanced and unbalanced designs? Of course providing the model was
fitted with orthogonal contrasts. Something like below:
mod <- aov(response ~ factor1 * factor2, data=mydata,
                   contrasts=list(factor1=contr.sum, factor2=contr.sum))
Anova(mod, type="III")

It was also said in most of your posts that the decision of which of
Type of SS to use has to be done on the basis of the hypothesis we
want to test. Therefore, let's assume that I would like to test the
significance of both factors, and if some of them significant, I plan
to use post-hoc tests to explore difference(s) between levels of this
significant factor(s).

Thank you in advance, Amasco

On 8/27/06, John Fox <jfox at mcmaster.ca> wrote:
> Dear Amasco,
>
> A complete explanation of the issues that you raise is awkward in an email,
> so I'll address your questions briefly. Section 8.2 of my text, Applied
> Regression Analysis, Linear Models, and Related Methods (Sage, 1997) has a
> detailed discussion.
>
> (1) In balanced designs, so-called "Type I," "II," and "III" sums of squares
> are identical. If the STATA manual says that Type II tests are only
> appropriate in balanced designs, then that doesn't make a whole lot of sense
> (unless one believes that Type-II tests are nonsense, which is not the
> case).
>
> (2) One should concentrate not directly on different "types" of sums of
> squares, but on the hypotheses to be tested. Sums of squares and F-tests
> should follow from the hypotheses. Type-II and Type-III tests (if the latter
> are properly formulated) test hypotheses that are reasonably construed as
> tests of main effects and interactions in unbalanced designs. In unbalanced
> designs, Type-I sums of squares usually test hypotheses of interest only by
> accident.
>
> (3) Type-II sums of squares are constructed obeying the principle of
> marginality, so the kinds of contrasts employed to represent factors are
> irrelevant to the sums of squares produced. You get the same answer for any
> full set of contrasts for each factor. In general, the hypotheses tested
> assume that terms to which a particular term is marginal are zero. So, for
> example, in a three-way ANOVA with factors A, B, and C, the Type-II test for
> the AB interaction assumes that the ABC interaction is absent, and the test
> for the A main effect assumes that the ABC, AB, and AC interaction are
> absent (but not necessarily the BC interaction, since the A main effect is
> not marginal to this term). A general justification is that we're usually
> not interested, e.g., in a main effect that's marginal to a nonzero
> interaction.
>
> (4) Type-III tests do not assume that terms higher-order to the term in
> question are zero. For example, in a two-way design with factors A and B,
> the type-III test for the A main effect tests whether the population
> marginal means at the levels of A (i.e., averaged across the levels of B)
> are the same. One can test this hypothesis whether or not A and B interact,
> since the marginal means can be formed whether or not the profiles of means
> for A within levels of B are parallel. Whether the hypothesis is of interest
> in the presence of interaction is another matter, however. To compute
> Type-III tests using incremental F-tests, one needs contrasts that are
> orthogonal in the row-basis of the model matrix. In R, this means, e.g.,
> using contr.sum, contr.helmert, or contr.poly (all of which will give you
> the same SS), but not contr.treatment. Failing to be careful here will
> result in testing hypotheses that are not reasonably construed, e.g., as
> hypotheses concerning main effects.
>
> (5) The same considerations apply to linear models that include quantitative
> predictors -- e.g., ANCOVA. Most software will not automatically produce
> sensible Type-III tests, however.
>
> I hope this helps,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Amasco
> > Miralisus
> > Sent: Saturday, August 26, 2006 5:07 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Type II and III sum of square in Anova (R, car package)
> >
> > Hello everybody,
> >
> > I have some questions on ANOVA in general and on ANOVA in R
> > particularly.
> > I am not Statistician, therefore I would be very appreciated
> > if you answer it in a simple way.
> >
> > 1. First of all, more general question. Standard anova()
> > function for lm() or aov() models in R implements Type I sum
> > of squares (sequential), which is not well suited for
> > unbalanced ANOVA. Therefore it is better to use
> > Anova() function from car package, which was programmed by
> > John Fox to use Type II and Type III sum of squares. Did I
> > get the point?
> >
> > 2. Now more specific question. Type II sum of squares is not
> > well suited for unbalanced ANOVA designs too (as stated in
> > STATISTICA help), therefore the general rule of thumb is to
> > use Anova() function using Type II SS only for balanced ANOVA
> > and Anova() function using Type III SS for unbalanced ANOVA?
> > Is this correct interpretation?
> >
> > 3. I have found a post from John Fox in which he wrote that
> > Type III SS could be misleading in case someone use some
> > contrasts. What is this about?
> > Could you please advice, when it is appropriate to use Type
> > II and when Type III SS? I do not use contrasts for
> > comparisons, just general ANOVA with subsequent Tukey
> > post-hoc comparisons.
> >
> > Thank you in advance,
> > Amasco
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>


From p.dalgaard at biostat.ku.dk  Mon Aug 28 21:31:10 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Aug 2006 21:31:10 +0200
Subject: [R] matrix "Adjoint" function
In-Reply-To: <Pine.GSO.4.60.0608281514010.14017@uni03du.unity.ncsu.edu>
References: <Pine.GSO.4.60.0608281514010.14017@uni03du.unity.ncsu.edu>
Message-ID: <x24pvw8xoh.fsf@turmalin.kubism.ku.dk>

"Jessica M. Maia" <jmmaia at unity.ncsu.edu> writes:

> Hi there,
> 
> I'm new to R and despite searching today, I can't
> find a function which will compute the adjoint of
> a matrix A. Does this adjoint function exist in R?

I don't think so, but the adjoint matrix is also known as the
conjugate transpose and we do have t() and Conj(). (If your matrix is
real, t() will do.)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From montez at bu.edu  Mon Aug 28 21:51:24 2006
From: montez at bu.edu (Maria Montez)
Date: Mon, 28 Aug 2006 12:51:24 -0700
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <Pine.LNX.4.64.0608260919310.28706@gannet.stats.ox.ac.uk>
References: <44EE13B6.20205@bu.edu> <44EE1F34.6010400@pdf.com>
	<44EFA791.5030703@bu.edu>
	<Pine.LNX.4.64.0608260919310.28706@gannet.stats.ox.ac.uk>
Message-ID: <44F3493C.8050601@bu.edu>

Thank you for your responses.

Prof Ripley's code only worked for a vector with 2 variables but I need 
the code to work for a vector of any size. To solve this I created a for 
loop. I'm not sure this is the most efficient way of doing it but it works.

 x1 <- c(1,0,0,1)
 x2 <- c(0,0,1,1)
 x3 <- c(0,0,0,0)
 x4 <- c(1,0,0,1)
 data1 <- cbind(x1,x2,x3,x4)
 x <-  c("x1","x2","x3","x4")

do.sum <- function(x,env=parent.frame()) {
   s <- get(x[1],env=env)
   for(i in 2:length(x)) {
     s <- s + get(x[i],env=env)
  }
 }
totx <- do.sum(x)
data2 <- cbind(data1,totx)
data2

Maria

Prof Brian Ripley wrote:

>On Fri, 25 Aug 2006, Maria Montez wrote:
>
>  
>
>>Thank you for your answers yesterday. I now have another question!
>>
>>Suppose that instead of creating a formula for a regression model I 
>>simply wanted to add the variables. I believe I cannot use the 
>>as.formula anymore. Again I tried to use expression to no avail. I get 
>>an expression but I can't use it.
>>
>>fit.sum <- function(x) {
>>    fo <- expression(paste(x, collapse = "+"))
>>   eval( fo)
>>}
>>fit.sum(c("x1","x2"))
>>
>>Basically what I need is to learn how to use variables when what is 
>>given to me are their names (character list).
>>    
>>
>
>I think we do need to tell you that parse(text=) is how to turn a 
>character vector into an expression, although it is rarely a good way (see 
>the fortune in the 'fortunes' package about it) and you need to be careful 
>where you evaluate:
>
>fit.sum <- function(x) eval.parent(parse(text=paste(x, collapse = "+")))
>
>As an alternative, the answer to
>
>  
>
>>Basically what I need is to learn how to use variables when what is
>>given to me are their names (character list).
>>    
>>
>
>is most often 'get'.  So for two objects
>
>fit.sum2 <- function(x) {
>   env <- parent.frame()
>   do.call("+", lapply(x, get, envir=env))
>}
>
>but again you need to be careful where you get() from.
>
>
>  
>


From btyner at stat.purdue.edu  Mon Aug 28 21:54:02 2006
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Mon, 28 Aug 2006 15:54:02 -0400
Subject: [R] tick.number for date in xyplot
In-Reply-To: <644e1f320608251402ie3ee5b0u405aaf35bb39517b@mail.gmail.com>
References: <1156532042.12112.15.camel@capella.stat.purdue.edu>
	<644e1f320608251402ie3ee5b0u405aaf35bb39517b@mail.gmail.com>
Message-ID: <1156794842.14556.50.camel@capella.stat.purdue.edu>

Thanks; this will have to do until a data-independent solution is
implemented. I tried things like

 seq(min(x),max(x),by='1 month')

but I always forget how to make 'x' visible inside a call to xyplot. 

Ben

On Fri, 2006-08-25 at 17:02 -0400, jim holtman wrote:
> Try this:
> 
> 
> xyplot(runif(365)~I(as.Date("1999-01-01") + 1:365),
>       scales=list(x=list(at=seq(as.Date('1999-1-1'), length=12, by='1
> month'), labels=NULL)))
> 
> 
> 
> On 8/25/06, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> > I would like a tick mark for each month; for example,
> >
> > xyplot(runif(365)~I(as.Date("1999-01-01") + 1:365),
> >       scales=list(x=list(format="%b %Y",tick.number=12)))
> >
> > I know I could make x numeric and use 'at' and 'labels', but I was
> > wondering if there is a more direct route I'm missing. (In particular,
> > one that doesn't have to be modified for new data).
> >
> > Thanks,
> > Ben
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
>


From jmmaia at unity.ncsu.edu  Mon Aug 28 22:06:36 2006
From: jmmaia at unity.ncsu.edu (Jessica M. Maia)
Date: Mon, 28 Aug 2006 16:06:36 -0400 (EDT)
Subject: [R] matrix "Adjoint" function
Message-ID: <Pine.GSO.4.60.0608281601120.14017@uni03du.unity.ncsu.edu>

Sorry but I wasn't very clear in my previous message.

I want to compute the adjoint of a real matrix A,
which is the transpose of the cofactor matrix of A.

There is an example here: http://www.mathwords.com/a/adjoint.htm

Does R have a function which will compute the cofactor of
matrix A or the adjoint of a real matrix A?

Thanks!


From jrkrideau at yahoo.ca  Mon Aug 28 22:08:01 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 28 Aug 2006 16:08:01 -0400 (EDT)
Subject: [R] Cannot get simple data.frame binding.
Message-ID: <20060828200801.50209.qmail@web32803.mail.mud.yahoo.com>

I am stuck on a simple problem where an example works
fine but the real one does not.

I have a data.frame where I wish to sum up some values
across the rows and create a new data.frame with some
of old data.frame variables and the new summed
variable.

It works fine in my simple example but I am doing
something wrong in the real world.  In the real world
I am loading a labeled data.frame. The orginal data
comes from a spss file imported using spss.get but the
current data.frame is a subset of the orginal spss
file.

EXAMPLE
cata <- c( 1,1,6,1,1,NA)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)
dogb <- c(2,4,6,8,10, 12)
rata <- c (NA, 9, 9, 8, 9, 8)
ratb <- c( 1,2,3,4,5,6)
bata <- c( 12, 42,NA, 45, 32, 54)
batb <- c( 13, 15, 17,19,21,23)
id <- c('a', 'b', 'b', 'c', 'a', 'b')
site <- c(1,1,4,4,1,4)
mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
bata, batb)

data1 <- data.frame(site, id, mat1)
attach(data1)
data1
aa <- which(names(data1)=="rata")
bb <- length(names(data1))

mat1 <- as.matrix(data1[,aa:bb])
food <- apply( mat1, 1, sum , na.rm=T)
food

abba <- data.frame(data1[, 1:6], food)
abba

----------------------------------
Real life problem

>load("C:/start/R.objects/partly.corrected.materials.Rdata")
> md1<-partly.corrected.materials
> aa <- which(names(md1)=="oaks")
> bb <- length(names(md1))
> 
> # sum the values of the "other" variables
> mat1 <- as.matrix( md1[, aa:bb] )
> other <- apply(mat1,1, sum, na.rm=T)
> ire1 <- data.frame(md1[, 1:11], other)
Error in data.frame(md1[, 1:11], other) : arguments
imply differing number of rows: 11, 75

---------------------------------------------

I have simply worked around the problem by using 
ire1 <- data.frame(md1$site, md1$colour, md1$ss1 ... ,
other) 
but I would like to know what stupid thing I am doing.

Thanks


From ripley at stats.ox.ac.uk  Mon Aug 28 22:09:18 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Aug 2006 21:09:18 +0100 (BST)
Subject: [R] generating an expression for a formula automatically
In-Reply-To: <44F3493C.8050601@bu.edu>
References: <44EE13B6.20205@bu.edu> <44EE1F34.6010400@pdf.com>
	<44EFA791.5030703@bu.edu>
	<Pine.LNX.4.64.0608260919310.28706@gannet.stats.ox.ac.uk>
	<44F3493C.8050601@bu.edu>
Message-ID: <Pine.LNX.4.64.0608282102150.27175@gannet.stats.ox.ac.uk>

On Mon, 28 Aug 2006, Maria Montez wrote:

> Thank you for your responses.
> 
> Prof Ripley's code only worked for a vector with 2 variables but I need the

True for my second version (as stated).

If you want elementwise sums (which is news to me), use

  rowSums(do.call("cbind", lapply(x, get)))

or

  X <- do.call("cbind", lapply(x, get))
  colnames(X) <- x
  (data2 <- cbind(X, totx=rowSums(X)))

to reproduce your answer.

> code to work for a vector of any size. To solve this I created a for loop. I'm
> not sure this is the most efficient way of doing it but it works.
> 
> x1 <- c(1,0,0,1)
> x2 <- c(0,0,1,1)
> x3 <- c(0,0,0,0)
> x4 <- c(1,0,0,1)
> data1 <- cbind(x1,x2,x3,x4)
> x <-  c("x1","x2","x3","x4")
> 
> do.sum <- function(x,env=parent.frame()) {
>   s <- get(x[1],env=env)
>   for(i in 2:length(x)) {
>     s <- s + get(x[i],env=env)
> }
> }
> totx <- do.sum(x)
> data2 <- cbind(data1,totx)
> data2
> 
> Maria
> 
> Prof Brian Ripley wrote:
> 
> >On Fri, 25 Aug 2006, Maria Montez wrote:
> >
> >  
> >
> > >Thank you for your answers yesterday. I now have another question!
> > >
> > >Suppose that instead of creating a formula for a regression model I simply
> > >wanted to add the variables. I believe I cannot use the as.formula anymore.
> > >Again I tried to use expression to no avail. I get an expression but I
> > >can't use it.
> > >
> > >fit.sum <- function(x) {
> > >   fo <- expression(paste(x, collapse = "+"))
> > >   eval( fo)
> >>}
> > >fit.sum(c("x1","x2"))
> > >
> > >Basically what I need is to learn how to use variables when what is given
> > >to me are their names (character list).
> > >    
> > >
> >
> >I think we do need to tell you that parse(text=) is how to turn a character
> >vector into an expression, although it is rarely a good way (see the fortune
> >in the 'fortunes' package about it) and you need to be careful where you
> >evaluate:
> >
> >fit.sum <- function(x) eval.parent(parse(text=paste(x, collapse = "+")))
> >
> >As an alternative, the answer to
> >
> >  
> >
> > >Basically what I need is to learn how to use variables when what is
> > >given to me are their names (character list).
> > >    
> > >
> >
> >is most often 'get'.  So for two objects
> >
> >fit.sum2 <- function(x) {
> >   env <- parent.frame()
> >   do.call("+", lapply(x, get, envir=env))
> >}
> >
> >but again you need to be careful where you get() from.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Mon Aug 28 22:09:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 16:09:46 -0400
Subject: [R] matrix "Adjoint" function
In-Reply-To: <Pine.GSO.4.60.0608281601120.14017@uni03du.unity.ncsu.edu>
References: <Pine.GSO.4.60.0608281601120.14017@uni03du.unity.ncsu.edu>
Message-ID: <971536df0608281309h7183a0f5yd0e2a31dfe84c2ce@mail.gmail.com>

Try:

RSiteSearch("cofactor")

On 8/28/06, Jessica M. Maia <jmmaia at unity.ncsu.edu> wrote:
> Sorry but I wasn't very clear in my previous message.
>
> I want to compute the adjoint of a real matrix A,
> which is the transpose of the cofactor matrix of A.
>
> There is an example here: http://www.mathwords.com/a/adjoint.htm
>
> Does R have a function which will compute the cofactor of
> matrix A or the adjoint of a real matrix A?
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Aug 28 22:14:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 16:14:39 -0400
Subject: [R] Cannot get simple data.frame binding.
In-Reply-To: <20060828200801.50209.qmail@web32803.mail.mud.yahoo.com>
References: <20060828200801.50209.qmail@web32803.mail.mud.yahoo.com>
Message-ID: <971536df0608281314l4eff7746j6c17e2725f3fc12b@mail.gmail.com>

The error message says that md1 and other have different
number of rows.   Please read the last line of every message
to r-help.

On 8/28/06, John Kane <jrkrideau at yahoo.ca> wrote:
> I am stuck on a simple problem where an example works
> fine but the real one does not.
>
> I have a data.frame where I wish to sum up some values
> across the rows and create a new data.frame with some
> of old data.frame variables and the new summed
> variable.
>
> It works fine in my simple example but I am doing
> something wrong in the real world.  In the real world
> I am loading a labeled data.frame. The orginal data
> comes from a spss file imported using spss.get but the
> current data.frame is a subset of the orginal spss
> file.
>
> EXAMPLE
> cata <- c( 1,1,6,1,1,NA)
> catb <- c( 1,2,3,4,5,6)
> doga <- c(3,5,3,6,4, 0)
> dogb <- c(2,4,6,8,10, 12)
> rata <- c (NA, 9, 9, 8, 9, 8)
> ratb <- c( 1,2,3,4,5,6)
> bata <- c( 12, 42,NA, 45, 32, 54)
> batb <- c( 13, 15, 17,19,21,23)
> id <- c('a', 'b', 'b', 'c', 'a', 'b')
> site <- c(1,1,4,4,1,4)
> mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> bata, batb)
>
> data1 <- data.frame(site, id, mat1)
> attach(data1)
> data1
> aa <- which(names(data1)=="rata")
> bb <- length(names(data1))
>
> mat1 <- as.matrix(data1[,aa:bb])
> food <- apply( mat1, 1, sum , na.rm=T)
> food
>
> abba <- data.frame(data1[, 1:6], food)
> abba
>
> ----------------------------------
> Real life problem
>
> >load("C:/start/R.objects/partly.corrected.materials.Rdata")
> > md1<-partly.corrected.materials
> > aa <- which(names(md1)=="oaks")
> > bb <- length(names(md1))
> >
> > # sum the values of the "other" variables
> > mat1 <- as.matrix( md1[, aa:bb] )
> > other <- apply(mat1,1, sum, na.rm=T)
> > ire1 <- data.frame(md1[, 1:11], other)
> Error in data.frame(md1[, 1:11], other) : arguments
> imply differing number of rows: 11, 75
>
> ---------------------------------------------
>
> I have simply worked around the problem by using
> ire1 <- data.frame(md1$site, md1$colour, md1$ss1 ... ,
> other)
> but I would like to know what stupid thing I am doing.
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Mon Aug 28 22:16:39 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 28 Aug 2006 13:16:39 -0700 (PDT)
Subject: [R] matrix "Adjoint" function
In-Reply-To: <Pine.GSO.4.60.0608281601120.14017@uni03du.unity.ncsu.edu>
References: <Pine.GSO.4.60.0608281601120.14017@uni03du.unity.ncsu.edu>
Message-ID: <Pine.LNX.4.64.0608281314480.1688@homer21.u.washington.edu>

On Mon, 28 Aug 2006, Jessica M. Maia wrote:

> Sorry but I wasn't very clear in my previous message.
>
> I want to compute the adjoint of a real matrix A,
> which is the transpose of the cofactor matrix of A.
>
> There is an example here: http://www.mathwords.com/a/adjoint.htm
>
> Does R have a function which will compute the cofactor of
> matrix A or the adjoint of a real matrix A?

If that's what you mean by adjoint, then the adjoint is the inverse 
multiplied by the determinant.

    adjoint<-function(A) det(A)*solve(A)

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ndesilsky at yahoo.com  Mon Aug 28 22:36:03 2006
From: ndesilsky at yahoo.com (Nick Desilsky)
Date: Mon, 28 Aug 2006 13:36:03 -0700 (PDT)
Subject: [R] Extracting column name in apply/lapply
Message-ID: <20060828203603.83934.qmail@web55508.mail.re4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/5399cd3b/attachment.pl 

From tplate at acm.org  Mon Aug 28 22:47:11 2006
From: tplate at acm.org (Tony Plate)
Date: Mon, 28 Aug 2006 14:47:11 -0600
Subject: [R] Cannot get simple data.frame binding.
In-Reply-To: <20060828200801.50209.qmail@web32803.mail.mud.yahoo.com>
References: <20060828200801.50209.qmail@web32803.mail.mud.yahoo.com>
Message-ID: <44F3564F.5000805@acm.org>

Maybe I'm missing something, but your "Real life code" looks like it 
should work.  What happens when you do:

 > ire1 <- data.frame(md1[, 1:11], other)
Error in data.frame(md1[, 1:11], other) : arguments
imply differing number of rows: 11, 75
 > str(md1[, 1:11])
 > str(other)

?

Maybe the labelled data frame is causing the problem?  Did you try 
as.data.frame(md1[,1:11])? (I'm guessing that will strip off extra 
attributes).

-- Tony Plate

John Kane wrote:
> I am stuck on a simple problem where an example works
> fine but the real one does not.
> 
> I have a data.frame where I wish to sum up some values
> across the rows and create a new data.frame with some
> of old data.frame variables and the new summed
> variable.
> 
> It works fine in my simple example but I am doing
> something wrong in the real world.  In the real world
> I am loading a labeled data.frame. The orginal data
> comes from a spss file imported using spss.get but the
> current data.frame is a subset of the orginal spss
> file.
> 
> EXAMPLE
> cata <- c( 1,1,6,1,1,NA)
> catb <- c( 1,2,3,4,5,6)
> doga <- c(3,5,3,6,4, 0)
> dogb <- c(2,4,6,8,10, 12)
> rata <- c (NA, 9, 9, 8, 9, 8)
> ratb <- c( 1,2,3,4,5,6)
> bata <- c( 12, 42,NA, 45, 32, 54)
> batb <- c( 13, 15, 17,19,21,23)
> id <- c('a', 'b', 'b', 'c', 'a', 'b')
> site <- c(1,1,4,4,1,4)
> mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> bata, batb)
> 
> data1 <- data.frame(site, id, mat1)
> attach(data1)
> data1
> aa <- which(names(data1)=="rata")
> bb <- length(names(data1))
> 
> mat1 <- as.matrix(data1[,aa:bb])
> food <- apply( mat1, 1, sum , na.rm=T)
> food
> 
> abba <- data.frame(data1[, 1:6], food)
> abba
> 
> ----------------------------------
> Real life problem
> 
> 
>>load("C:/start/R.objects/partly.corrected.materials.Rdata")
>>md1<-partly.corrected.materials
>>aa <- which(names(md1)=="oaks")
>>bb <- length(names(md1))
>>
>># sum the values of the "other" variables
>>mat1 <- as.matrix( md1[, aa:bb] )
>>other <- apply(mat1,1, sum, na.rm=T)
>>ire1 <- data.frame(md1[, 1:11], other)
> 
> Error in data.frame(md1[, 1:11], other) : arguments
> imply differing number of rows: 11, 75
> 
> ---------------------------------------------
> 
> I have simply worked around the problem by using 
> ire1 <- data.frame(md1$site, md1$colour, md1$ss1 ... ,
> other) 
> but I would like to know what stupid thing I am doing.
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sundar.dorai-raj at pdf.com  Mon Aug 28 22:59:15 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 28 Aug 2006 15:59:15 -0500
Subject: [R] Extracting column name in apply/lapply
In-Reply-To: <20060828203603.83934.qmail@web55508.mail.re4.yahoo.com>
References: <20060828203603.83934.qmail@web55508.mail.re4.yahoo.com>
Message-ID: <44F35923.4070404@pdf.com>



Nick Desilsky wrote:
>   Hi,
>    
>   any good trick to get the column names for title() aside from running lapply on the column indexes?
>    
>   Thanks
>    
>   Nick.
>    
>   apply(X[,numCols],2,function(x){
>   nunqs <- length(unique(x))
>   nnans <- sum(is.na(x))
>   info <- paste("uniques:",nunqs,"(",nunqs/n,")","NAs:",nnans,"(",nnans/n,")")
>   hist(x,xlab=info) 
>   # --> title("???")
>   })
> 
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

This is where a for loop is much more useful (and readable):

for(i in numCols) {
   nunqs <- length(unique(X[, i]))
   nnans <- sum(is.na(X[, i]))
   ## `n' is missing from this example
   info <- 
paste("uniques:",nunqs,"(",nunqs/n,")","NAs:",nnans,"(",nnans/n,")")
   hist(X[, i],xlab=info)
   title(colnames(X)[i])
}

HTH,

--sundar


From ggrothendieck at gmail.com  Mon Aug 28 23:31:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 17:31:46 -0400
Subject: [R] Extracting column name in apply/lapply
In-Reply-To: <44F35923.4070404@pdf.com>
References: <20060828203603.83934.qmail@web55508.mail.re4.yahoo.com>
	<44F35923.4070404@pdf.com>
Message-ID: <971536df0608281431k341c8562u63edeafce99bb232@mail.gmail.com>

This can be done using a similar style but without a loop by
using sapply over the indexes rather than over the columns:

sapply(1:ncol(X), f)
f <- function(i) {...whatever...}

...whatever... can now refer to colnames(X)[i] and X[,i]

On 8/28/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>
>
> Nick Desilsky wrote:
> >   Hi,
> >
> >   any good trick to get the column names for title() aside from running lapply on the column indexes?
> >
> >   Thanks
> >
> >   Nick.
> >
> >   apply(X[,numCols],2,function(x){
> >   nunqs <- length(unique(x))
> >   nnans <- sum(is.na(x))
> >   info <- paste("uniques:",nunqs,"(",nunqs/n,")","NAs:",nnans,"(",nnans/n,")")
> >   hist(x,xlab=info)
> >   # --> title("???")
> >   })
> >
> >
> >
> > ---------------------------------
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> This is where a for loop is much more useful (and readable):
>
> for(i in numCols) {
>   nunqs <- length(unique(X[, i]))
>   nnans <- sum(is.na(X[, i]))
>   ## `n' is missing from this example
>   info <-
> paste("uniques:",nunqs,"(",nunqs/n,")","NAs:",nnans,"(",nnans/n,")")
>   hist(X[, i],xlab=info)
>   title(colnames(X)[i])
> }
>
> HTH,
>
> --sundar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Tue Aug 29 00:07:37 2006
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 28 Aug 2006 18:07:37 -0400
Subject: [R] Type II and III sum of square in Anova (R, car package)
In-Reply-To: <66d34c7a0608281220h2f460cc2q861d19962e7b4301@mail.gmail.com>
Message-ID: <20060828220738.UIZD24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Amasco,

Again, I'll answer briefly (since the written source that I previously
mentioned has an extensive discussion):

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Amasco 
> Miralisus
> Sent: Monday, August 28, 2006 2:21 PM
> To: r-help at stat.math.ethz.ch
> Cc: John Fox; Prof Brian Ripley; Mark Lyman
> Subject: Re: [R] Type II and III sum of square in Anova (R, 
> car package)
> 
> Hello,
> 
> First of all, I would like to thank everybody who answered my 
> question. Every post has added something to my knowledge of the topic.
> I now know why Type III SS are so questionable.
> 
> As I understood form R FAQ, there is disagreement among 
> Statisticians which SS to use 
> (http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-does-the-out
> put-from-anova_0028_0029-depend-on-the-order-of-factors-in-the
> -model_003f).
> However, most commercial statistical packages use Type III as 
> the default (with orthogonal contrasts), just as STATISTICA, 
> from which I am currently trying to migrate to R. This was 
> probably was done for the convenience of end-users who are 
> not very experienced in theoretical statistics.
> 

Note that the contrasts are only orthogonal in the row basis of the model
matrix, not, with unbalanced data, in the model matrix itself.

> I am aware that the same result could be produced using the standard
> anova() function with Type I "sequential" SS, supplemented by 
> drop1() function, but this approach will look quite 
> complicated for persons without any substantial background in 
> statistics, like no-math students. I would prefer easier way, 
> possibly more universal, though also probably more "for 
> dummies" :) If am not mistaken, car package by John Fox with 
> his nice Anova() function is the reasonable alternative for 
> any, who wish to simply perform quick statistical analysis, 
> without afraid to mess something with model fitting. Of 
> course orthogonal contrasts have to be specified (for example 
> contr.sum) in case of Type III SS.
> 
> Therefore, I would like to reformulate my questions, to make 
> it easier for you to answer:
> 
> 1. The first question related to answer by Professor Brian 
> Ripley: Did I understood correctly from the advised paper 
> (Bill Venables'
> 'exegeses' paper) that there is not much sense to test main 
> effects if the interaction is significant?
> 

Many are of this opinion. I would put it a bit differently: Properly
formulated, tests of main effects in the presence of interactions make sense
(i.e., have a straightforward interpretation in terms of population marginal
means) but probably are not of interest.

> 2. If I understood the post by John Fox correctly, I could safely use
> Anova(.,type="III") function from car for ANOVA analyses in 
> R, both for balanced and unbalanced designs? Of course 
> providing the model was fitted with orthogonal contrasts. 
> Something like below:
> mod <- aov(response ~ factor1 * factor2, data=mydata,
>                    contrasts=list(factor1=contr.sum, 
> factor2=contr.sum)) Anova(mod, type="III")
> 

Yes (or you could reset the contrasts option), but why do you appear to
prefer the "type-III" tests to the "type-II" tests?

> It was also said in most of your posts that the decision of 
> which of Type of SS to use has to be done on the basis of the 
> hypothesis we want to test. Therefore, let's assume that I 
> would like to test the significance of both factors, and if 
> some of them significant, I plan to use post-hoc tests to 
> explore difference(s) between levels of this significant factor(s).
> 

Your statement is too vague to imply what kind of tests you should use. I
think that people are almost always interested in "main effects" when
interactions to which they are marginal are negligible. In this situation,
both "type-II" and "type-III" tests are appropriate, and "type-II" tests
would usually be more powerful.

Regards,
John

> Thank you in advance, Amasco
> 
> On 8/27/06, John Fox <jfox at mcmaster.ca> wrote:
> > Dear Amasco,
> >
> > A complete explanation of the issues that you raise is 
> awkward in an 
> > email, so I'll address your questions briefly. Section 8.2 
> of my text, 
> > Applied Regression Analysis, Linear Models, and Related 
> Methods (Sage, 
> > 1997) has a detailed discussion.
> >
> > (1) In balanced designs, so-called "Type I," "II," and 
> "III" sums of 
> > squares are identical. If the STATA manual says that Type 
> II tests are 
> > only appropriate in balanced designs, then that doesn't 
> make a whole 
> > lot of sense (unless one believes that Type-II tests are nonsense, 
> > which is not the case).
> >
> > (2) One should concentrate not directly on different 
> "types" of sums 
> > of squares, but on the hypotheses to be tested. Sums of squares and 
> > F-tests should follow from the hypotheses. Type-II and 
> Type-III tests 
> > (if the latter are properly formulated) test hypotheses that are 
> > reasonably construed as tests of main effects and interactions in 
> > unbalanced designs. In unbalanced designs, Type-I sums of squares 
> > usually test hypotheses of interest only by accident.
> >
> > (3) Type-II sums of squares are constructed obeying the 
> principle of 
> > marginality, so the kinds of contrasts employed to 
> represent factors 
> > are irrelevant to the sums of squares produced. You get the same 
> > answer for any full set of contrasts for each factor. In 
> general, the 
> > hypotheses tested assume that terms to which a particular term is 
> > marginal are zero. So, for example, in a three-way ANOVA 
> with factors 
> > A, B, and C, the Type-II test for the AB interaction 
> assumes that the 
> > ABC interaction is absent, and the test for the A main 
> effect assumes 
> > that the ABC, AB, and AC interaction are absent (but not 
> necessarily 
> > the BC interaction, since the A main effect is not marginal to this 
> > term). A general justification is that we're usually not 
> interested, 
> > e.g., in a main effect that's marginal to a nonzero interaction.
> >
> > (4) Type-III tests do not assume that terms higher-order to 
> the term 
> > in question are zero. For example, in a two-way design with 
> factors A 
> > and B, the type-III test for the A main effect tests whether the 
> > population marginal means at the levels of A (i.e., averaged across 
> > the levels of B) are the same. One can test this hypothesis 
> whether or 
> > not A and B interact, since the marginal means can be 
> formed whether 
> > or not the profiles of means for A within levels of B are parallel. 
> > Whether the hypothesis is of interest in the presence of 
> interaction 
> > is another matter, however. To compute Type-III tests using 
> > incremental F-tests, one needs contrasts that are orthogonal in the 
> > row-basis of the model matrix. In R, this means, e.g., using 
> > contr.sum, contr.helmert, or contr.poly (all of which will give you 
> > the same SS), but not contr.treatment. Failing to be 
> careful here will 
> > result in testing hypotheses that are not reasonably 
> construed, e.g., as hypotheses concerning main effects.
> >
> > (5) The same considerations apply to linear models that include 
> > quantitative predictors -- e.g., ANCOVA. Most software will not 
> > automatically produce sensible Type-III tests, however.
> >
> > I hope this helps,
> >  John
> >
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Amasco 
> > > Miralisus
> > > Sent: Saturday, August 26, 2006 5:07 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Type II and III sum of square in Anova (R, 
> car package)
> > >
> > > Hello everybody,
> > >
> > > I have some questions on ANOVA in general and on ANOVA in R 
> > > particularly.
> > > I am not Statistician, therefore I would be very 
> appreciated if you 
> > > answer it in a simple way.
> > >
> > > 1. First of all, more general question. Standard anova() function 
> > > for lm() or aov() models in R implements Type I sum of squares 
> > > (sequential), which is not well suited for unbalanced ANOVA. 
> > > Therefore it is better to use
> > > Anova() function from car package, which was programmed 
> by John Fox 
> > > to use Type II and Type III sum of squares. Did I get the point?
> > >
> > > 2. Now more specific question. Type II sum of squares is not well 
> > > suited for unbalanced ANOVA designs too (as stated in STATISTICA 
> > > help), therefore the general rule of thumb is to use Anova() 
> > > function using Type II SS only for balanced ANOVA and Anova() 
> > > function using Type III SS for unbalanced ANOVA?
> > > Is this correct interpretation?
> > >
> > > 3. I have found a post from John Fox in which he wrote 
> that Type III 
> > > SS could be misleading in case someone use some 
> contrasts. What is 
> > > this about?
> > > Could you please advice, when it is appropriate to use 
> Type II and 
> > > when Type III SS? I do not use contrasts for comparisons, just 
> > > general ANOVA with subsequent Tukey post-hoc comparisons.
> > >
> > > Thank you in advance,
> > > Amasco
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Tue Aug 29 00:13:47 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 28 Aug 2006 18:13:47 -0400 (EDT)
Subject: [R] Cannot get simple data.frame binding.
In-Reply-To: <971536df0608281314l4eff7746j6c17e2725f3fc12b@mail.gmail.com>
Message-ID: <20060828221347.67145.qmail@web32802.mail.mud.yahoo.com>


--- Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> The error message says that md1 and other have
> different
> number of rows.   Please read the last line of every
> message
> to r-help.

My appologies, I forgot to include that infomation. 
As far as I can tell 'md1' is a data.frame of
dimensions ( 75 rows ,19 columns) and 'other' is a
numeric vector of length 75.   I have tried to
duplicate the dataframe and vector in the example.  

I assume that I am missing something obvious since I
can create the new dataframe in a more labourious
manner but not with an indexed command.


> 
> On 8/28/06, John Kane <jrkrideau at yahoo.ca> wrote:
> > I am stuck on a simple problem where an example
> works
> > fine but the real one does not.
> >
> > I have a data.frame where I wish to sum up some
> values
> > across the rows and create a new data.frame with
> some
> > of old data.frame variables and the new summed
> > variable.
> >
> > It works fine in my simple example but I am doing
> > something wrong in the real world.  In the real
> world
> > I am loading a labeled data.frame. The orginal
> data
> > comes from a spss file imported using spss.get but
> the
> > current data.frame is a subset of the orginal spss
> > file.
> >
> > EXAMPLE
> > cata <- c( 1,1,6,1,1,NA)
> > catb <- c( 1,2,3,4,5,6)
> > doga <- c(3,5,3,6,4, 0)
> > dogb <- c(2,4,6,8,10, 12)
> > rata <- c (NA, 9, 9, 8, 9, 8)
> > ratb <- c( 1,2,3,4,5,6)
> > bata <- c( 12, 42,NA, 45, 32, 54)
> > batb <- c( 13, 15, 17,19,21,23)
> > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > site <- c(1,1,4,4,1,4)
> > mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> > bata, batb)
> >
> > data1 <- data.frame(site, id, mat1)
> > attach(data1)
> > data1
> > aa <- which(names(data1)=="rata")
> > bb <- length(names(data1))
> >
> > mat1 <- as.matrix(data1[,aa:bb])
> > food <- apply( mat1, 1, sum , na.rm=T)
> > food
> >
> > abba <- data.frame(data1[, 1:6], food)
> > abba
> >
> > ----------------------------------
> > Real life problem
> >
> >
>
>load("C:/start/R.objects/partly.corrected.materials.Rdata")
> > > md1<-partly.corrected.materials
> > > aa <- which(names(md1)=="oaks")
> > > bb <- length(names(md1))
> > >
> > > # sum the values of the "other" variables
> > > mat1 <- as.matrix( md1[, aa:bb] )
> > > other <- apply(mat1,1, sum, na.rm=T)
> > > ire1 <- data.frame(md1[, 1:11], other)
> > Error in data.frame(md1[, 1:11], other) :
> arguments
> > imply differing number of rows: 11, 75
> >
> > ---------------------------------------------
> >
> > I have simply worked around the problem by using
> > ire1 <- data.frame(md1$site, md1$colour, md1$ss1
> ... ,
> > other)
> > but I would like to know what stupid thing I am
> doing.
> >
> > Thanks
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
>


From jrkrideau at yahoo.ca  Tue Aug 29 00:30:05 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 28 Aug 2006 18:30:05 -0400 (EDT)
Subject: [R] Cannot get simple data.frame binding.
In-Reply-To: <44F3564F.5000805@acm.org>
Message-ID: <20060828223006.72011.qmail@web32812.mail.mud.yahoo.com>


--- Tony Plate <tplate at acm.org> wrote:

> Maybe I'm missing something, but your "Real life
> code" looks like it 
> should work.  What happens when you do:
> 
>  > ire1 <- data.frame(md1[, 1:11], other)
> Error in data.frame(md1[, 1:11], other) : arguments
> imply differing number of rows: 11, 75
>  > str(md1[, 1:11])
>  > str(other)
> 
> ?
> 
> Maybe the labelled data frame is causing the
> problem?  Did you try 
> as.data.frame(md1[,1:11])? (I'm guessing that will
> strip off extra 
> attributes).
> 

I seem to have gotten it to work.  I had given up, and
simply coded it using md1$x1 etc.  

I was using an intermediate data.frame that I had
saved as an Rdata object. Once I got it working with
the md1$1 approach I dropped the intermediate object
and just used the orignal data.frame that I had
loaded.

I  made a copy of the file to run your suggestions and
when I ran the program the thing worked!  I must have
done something in saving the data.frame and reloading
but I have no idea what. since I simply said  dat1 <-
df1 and then saved dat1.

Thanks a lot. I was going a bit nuts on this.

> -- Tony Plate
> 
> John Kane wrote:
> > I am stuck on a simple problem where an example
> works
> > fine but the real one does not.
> > 
> > I have a data.frame where I wish to sum up some
> values
> > across the rows and create a new data.frame with
> some
> > of old data.frame variables and the new summed
> > variable.
> > 
> > It works fine in my simple example but I am doing
> > something wrong in the real world.  In the real
> world
> > I am loading a labeled data.frame. The orginal
> data
> > comes from a spss file imported using spss.get but
> the
> > current data.frame is a subset of the orginal spss
> > file.
> > 
> > EXAMPLE
> > cata <- c( 1,1,6,1,1,NA)
> > catb <- c( 1,2,3,4,5,6)
> > doga <- c(3,5,3,6,4, 0)
> > dogb <- c(2,4,6,8,10, 12)
> > rata <- c (NA, 9, 9, 8, 9, 8)
> > ratb <- c( 1,2,3,4,5,6)
> > bata <- c( 12, 42,NA, 45, 32, 54)
> > batb <- c( 13, 15, 17,19,21,23)
> > id <- c('a', 'b', 'b', 'c', 'a', 'b')
> > site <- c(1,1,4,4,1,4)
> > mat1 <-  cbind(cata, catb, doga, dogb, rata, ratb,
> > bata, batb)
> > 
> > data1 <- data.frame(site, id, mat1)
> > attach(data1)
> > data1
> > aa <- which(names(data1)=="rata")
> > bb <- length(names(data1))
> > 
> > mat1 <- as.matrix(data1[,aa:bb])
> > food <- apply( mat1, 1, sum , na.rm=T)
> > food
> > 
> > abba <- data.frame(data1[, 1:6], food)
> > abba
> > 
> > ----------------------------------
> > Real life problem
> > 
> > 
>
>>load("C:/start/R.objects/partly.corrected.materials.Rdata")
> >>md1<-partly.corrected.materials
> >>aa <- which(names(md1)=="oaks")
> >>bb <- length(names(md1))
> >>
> >># sum the values of the "other" variables
> >>mat1 <- as.matrix( md1[, aa:bb] )
> >>other <- apply(mat1,1, sum, na.rm=T)
> >>ire1 <- data.frame(md1[, 1:11], other)
> > 
> > Error in data.frame(md1[, 1:11], other) :
> arguments
> > imply differing number of rows: 11, 75
> > 
> > ---------------------------------------------
> > 
> > I have simply worked around the problem by using 
> > ire1 <- data.frame(md1$site, md1$colour, md1$ss1
> ... ,
> > other) 
> > but I would like to know what stupid thing I am
> doing.
> > 
> > Thanks
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> 
>


From hpbenton at scripps.edu  Tue Aug 29 00:28:56 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:28:56 -0700
Subject: [R] Time plots
Message-ID: <002701c6caf1$59e77d30$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


Research Technician
Mass Spectrometry
?  o The
? /
o Scripps
? \
?  o Research
? /
o Institute


From hpbenton at scripps.edu  Tue Aug 29 00:31:12 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:31:12 -0700
Subject: [R] Time plots
Message-ID: <002801c6caf1$aaabe940$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


Research Technician
Mass Spectrometry
?  o The
? /
o Scripps
? \
?  o Research
? /
o Institute


From hpbenton at scripps.edu  Tue Aug 29 00:32:07 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:32:07 -0700
Subject: [R] Time plots
Message-ID: <002901c6caf1$e273b240$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


From hpbenton at scripps.edu  Tue Aug 29 00:34:46 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:34:46 -0700
Subject: [R] Time plots
Message-ID: <002a01c6caf2$4d84e810$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


From hpbenton at scripps.edu  Tue Aug 29 00:37:06 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:37:06 -0700
Subject: [R] Time plots
Message-ID: <002b01c6caf2$7df54440$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


From rhuntsinger at verizon.net  Mon Aug 28 23:52:55 2006
From: rhuntsinger at verizon.net (Reid Huntsinger)
Date: Mon, 28 Aug 2006 17:52:55 -0400
Subject: [R] matrix "Adjoint" function
References: <Pine.GSO.4.60.0608281514010.14017@uni03du.unity.ncsu.edu>
	<x24pvw8xoh.fsf@turmalin.kubism.ku.dk>
Message-ID: <44F365B7.2080009@verizon.net>

Sometimes the term "adjoint matrix" refers to the "matrix of cofactors", 
that is, the matrix of signed determinants of n-1 x n-1 dimensional 
submatrices. This is just the inverse multiplied by the determinant. As 
with both the inverse and determinant, if this is part of a larger 
computation there are often better ways to solve the problem avoiding 
explicit computation of determinants or inverses.

Reid Huntsinger

Peter Dalgaard wrote:

>"Jessica M. Maia" <jmmaia at unity.ncsu.edu> writes:
>
>  
>
>>Hi there,
>>
>>I'm new to R and despite searching today, I can't
>>find a function which will compute the adjoint of
>>a matrix A. Does this adjoint function exist in R?
>>    
>>
>
>I don't think so, but the adjoint matrix is also known as the
>conjugate transpose and we do have t() and Conj(). (If your matrix is
>real, t() will do.)
>
>  
>


From hpbenton at scripps.edu  Tue Aug 29 00:41:08 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:41:08 -0700
Subject: [R] Time plots
Message-ID: <002d01c6caf3$0dece350$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


From hpbenton at scripps.edu  Tue Aug 29 00:39:07 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 15:39:07 -0700
Subject: [R] Time plots
Message-ID: <002c01c6caf2$c60d61e0$aa92fea9@lama>

Hello all and thank you,

	I will try to make this as clear as I can. I have a matrix, at the
top of the matrix are 4 time points, the first col describes the sample,
then under each time point I have the intensity values of that sample. 
So :
Samples	0	10	30	120 <- time 
A		200	300	400	1000 <- data
B		100	400	200	100
C		500	400	200	1000
^
| samples

I would like to plot these however, all I get is a very messed up plot. It
may be due to the size of the matrix, it is 7746 rows long. But I thought
for R that shouldn't make a difference. 

	Cheers,

	Paul


From smckinney at bccrc.ca  Tue Aug 29 00:45:57 2006
From: smckinney at bccrc.ca (Steven McKinney)
Date: Mon, 28 Aug 2006 15:45:57 -0700
Subject: [R] Bug/problem reporting:  Possible to modify posting guide FAQ?
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB01D55528@crcmail1.BCCRC.CA>


If users post a bug or problem issue to an R-based news group
(R-devel, R-help, BioC - though BioC is far more forgiving)
they get yelled at for not reading the posting guide
and FAQ.

"Please *_do_* read the FAQ, the posting guide, ..."
the yellers do say.  So I read the BioC FAQ and it says...

http://www.bioconductor.org/docs/faq/

    "Bug reports on packages should perhaps be sent to the 
     package maintainer rather than to r-bugs."


So I send email to a maintainer, who I believe rightly points out

   "best to send this kind of questions to the bioc mailing list, rather
    than to myself privately, because other people might (a) also have
    answers or (b) benefit from the questions & answers."

Could the FAQ possibly be revised to some sensible combination
that generates less finger pointing, such as

   "Bug reports on packages should be sent to the Bioconductor mailing list, 
    and sent or copied to the package maintainer, rather than to r-bugs."

or

   "Bug reports on packages should be sent to the package maintainer, 
    and copied to the Bioconductor mailing list, rather than to r-bugs."


Could the posting guides to R-help and R-devel do something
similar?


Sign me
<Tired of all the finger pointing>


http://www.r-project.org/posting-guide.html

 "If the question relates to a contributed package , e.g., one downloaded 
  from CRAN, try contacting the package maintainer first. You can also 
  use find("functionname") and packageDescription("packagename") to 
  find this information. Only send such questions to R-help or R-devel if 
  you get no reply or need further assistance. This applies to both 
  requests for help and to bug reports."


How about

If the question relates to a contributed package , e.g., one downloaded 
from CRAN, email the list and be sure to additionally send to or copy to 
the package maintainer as well. You can use find("functionname") 
and packageDescription("packagename") to find this information. 
Only send such questions to one of R-help or R-devel. This applies to both 
requests for help and to bug reports.


From hpbenton at scripps.edu  Tue Aug 29 01:19:29 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 16:19:29 -0700
Subject: [R] Time plots
In-Reply-To: <644e1f320608281601q7c9b4244g9c1aa9e346664e47@mail.gmail.com>
Message-ID: <003101c6caf8$693e0450$aa92fea9@lama>

Opps thanks sorry,

So I was just using 
plot(data) # where data was the matrix
I would like it if the plots had time along the bottom and intensity up the
y axis. Intensity is just a value, if you want intensity is the number of
molecules hitting the detector per millisecond

pb

-----Original Message-----
From: jim holtman [mailto:jholtman at gmail.com] 
Sent: Monday, August 28, 2006 4:02 PM
To: H. Paul Benton
Subject: Re: [R] Time plots

You need to show the commands that you were using to generate your
plot.  What do you want the plot to look like?  What is the intensity
value supposed to be?

On 8/28/06, H. Paul Benton <hpbenton at scripps.edu> wrote:
> Hello all and thank you,
>
>        I will try to make this as clear as I can. I have a matrix, at the
> top of the matrix are 4 time points, the first col describes the sample,
> then under each time point I have the intensity values of that sample.
> So :
> Samples 0       10      30      120 <- time
> A               200     300     400     1000 <- data
> B               100     400     200     100
> C               500     400     200     1000
> ^
> | samples
>
> I would like to plot these however, all I get is a very messed up plot. It
> may be due to the size of the matrix, it is 7746 rows long. But I thought
> for R that shouldn't make a difference.
>
>        Cheers,
>
>        Paul
>
>
> Research Technician
> Mass Spectrometry
>  o The
>  /
> o Scripps
>  \
>  o Research
>  /
> o Institute
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From spencer.graves at pdf.com  Tue Aug 29 01:31:14 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Aug 2006 16:31:14 -0700
Subject: [R] Time plots
In-Reply-To: <002c01c6caf2$c60d61e0$aa92fea9@lama>
References: <002c01c6caf2$c60d61e0$aa92fea9@lama>
Message-ID: <44F37CC2.4020903@pdf.com>

Dear Paul: 

      1.  The Internet moves NOT at the speed of light but at the speed 
of store and forward. 

      2.  Have you worked the examples with the "matplot" help page? 

      3.  Have you worked through the 'zoo' vignette ?  (If no, please 
see "http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html".)

      4.  If you would like further help from this listserve, please 
submit another post -- including commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"www.R-project.org/posting-guide.html".   Questions without commented, 
minimal, self-contained, reproducible code are generally much harder to 
understand, which means that people's propensity to reply and the 
relevance of any replies are both lower.  Help others help you. 

      hope this helps. 
      Spencer Graves  

H. Paul Benton wrote:
> Hello all and thank you,
>
> 	I will try to make this as clear as I can. I have a matrix, at the
> top of the matrix are 4 time points, the first col describes the sample,
> then under each time point I have the intensity values of that sample. 
> So :
> Samples	0	10	30	120 <- time 
> A		200	300	400	1000 <- data
> B		100	400	200	100
> C		500	400	200	1000
> ^
> | samples
>
> I would like to plot these however, all I get is a very messed up plot. It
> may be due to the size of the matrix, it is 7746 rows long. But I thought
> for R that shouldn't make a difference. 
>
> 	Cheers,
>
> 	Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmmaia at unity.ncsu.edu  Tue Aug 29 01:45:42 2006
From: jmmaia at unity.ncsu.edu (Jessica M. Maia)
Date: Mon, 28 Aug 2006 19:45:42 -0400 (EDT)
Subject: [R] matrix "Adjoint" function
Message-ID: <Pine.GSO.4.60.0608281944360.17584@uni03du.unity.ncsu.edu>


Thanks for all the replies!


From rgentlem at fhcrc.org  Tue Aug 29 01:57:55 2006
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Mon, 28 Aug 2006 16:57:55 -0700
Subject: [R] Bug/problem reporting: Possible to modify posting guide FAQ?
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB01D55528@crcmail1.BCCRC.CA>
References: <0BE438149FF2254DB4199E2682C8DFEB01D55528@crcmail1.BCCRC.CA>
Message-ID: <44F38303.50105@fhcrc.org>

Hi,
   I guess the question often comes down to whether it is a bug report, 
or a question. If you know it is a bug, and have a complete and correct 
example where the obviously incorrect behavior occurs and you are 
positive that the problem is the package then sending it to the 
maintainer is appropriate.  When I get these I try to deal with them. 
Real bug reports that go to the mailing list may be missed so in my 
opinion it would be best to cc the maintainer and we will amend the FAQ 
in that direction. If instead you are asking a question, of the form, is 
this a bug, or why is this happening, then for BioC at least, it is 
better to post directly to the list, as there are many folks who can 
help and you are more likely to get an answer.  When I get one of these 
emails I always refer the person to the mailing lists.  I see little 
problem with being redirected by a maintainer to the mailing list if 
they feel that the question is better asked there.

Bioconductor is different from R, clearly our mailing list has to be 
more about the constituent packages, since we will direct questions 
about R to the appropriate R mailing lists.  R mailing lists tend to be 
about R, so asking about a specific package there (among the 1000 or so) 
often does not get you very far, but sometimes it does.


  best wishes
    Robert


Steven McKinney wrote:
> If users post a bug or problem issue to an R-based news group
> (R-devel, R-help, BioC - though BioC is far more forgiving)
> they get yelled at for not reading the posting guide
> and FAQ.
> 
> "Please *_do_* read the FAQ, the posting guide, ..."
> the yellers do say.  So I read the BioC FAQ and it says...
> 
> http://www.bioconductor.org/docs/faq/
> 
>     "Bug reports on packages should perhaps be sent to the 
>      package maintainer rather than to r-bugs."
> 
> 
> So I send email to a maintainer, who I believe rightly points out
> 
>    "best to send this kind of questions to the bioc mailing list, rather
>     than to myself privately, because other people might (a) also have
>     answers or (b) benefit from the questions & answers."
> 
> Could the FAQ possibly be revised to some sensible combination
> that generates less finger pointing, such as
> 
>    "Bug reports on packages should be sent to the Bioconductor mailing list, 
>     and sent or copied to the package maintainer, rather than to r-bugs."
> 
> or
> 
>    "Bug reports on packages should be sent to the package maintainer, 
>     and copied to the Bioconductor mailing list, rather than to r-bugs."
> 
> 
> Could the posting guides to R-help and R-devel do something
> similar?
> 
> 
> Sign me
> <Tired of all the finger pointing>
> 
> 
> http://www.r-project.org/posting-guide.html
> 
>  "If the question relates to a contributed package , e.g., one downloaded 
>   from CRAN, try contacting the package maintainer first. You can also 
>   use find("functionname") and packageDescription("packagename") to 
>   find this information. Only send such questions to R-help or R-devel if 
>   you get no reply or need further assistance. This applies to both 
>   requests for help and to bug reports."
> 
> 
> How about
> 
> If the question relates to a contributed package , e.g., one downloaded 
> from CRAN, email the list and be sure to additionally send to or copy to 
> the package maintainer as well. You can use find("functionname") 
> and packageDescription("packagename") to find this information. 
> Only send such questions to one of R-help or R-devel. This applies to both 
> requests for help and to bug reports.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From ysapir at uga.edu  Tue Aug 29 02:15:08 2006
From: ysapir at uga.edu (Yuval Sapir)
Date: Mon, 28 Aug 2006 20:15:08 -0400
Subject: [R] standardized partial regression coefficients
Message-ID: <44F3870C.7040209@uga.edu>

Hi all,
I am relatively new to R, so let me know if I missed something trivial.
I want to perform path-analysis with at least three levels of explaining 
variables (i.e., A affects B that affects C). I perform multiple 
regression for each set of variables, and get estimates for the partial 
coefficients and their SE. How can I translate it to standardized 
partial regression coefficients ("beta" in SPSS)?
Thanks
Yuval

-- 
Yuval Sapir, PhD
Post-doctorate research associate
Dept of Genetics
University of Georgia
Athens, GA 30602, USA


From hpbenton at scripps.edu  Tue Aug 29 02:23:37 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Mon, 28 Aug 2006 17:23:37 -0700
Subject: [R] Time plots
In-Reply-To: <44F37CC2.4020903@pdf.com>
Message-ID: <004c01c6cb01$60dab6b0$aa92fea9@lama>

Will do!
So thanks to everyone who wrote me, I'm going to go away have a look at some
of the links that you all have given me and hopefully will be able to solve
it. As you can all probably guess I'm new to the whole R thing.
 
Cheers,

Paul

Research Technician
Mass Spectrometry
?  o The
? /
o Scripps
? \
?  o Research
? /
o Institute


-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Monday, August 28, 2006 4:31 PM
To: H. Paul Benton
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Time plots

Dear Paul: 

      1.  The Internet moves NOT at the speed of light but at the speed 
of store and forward. 

      2.  Have you worked the examples with the "matplot" help page? 

      3.  Have you worked through the 'zoo' vignette ?  (If no, please 
see "http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html".)

      4.  If you would like further help from this listserve, please 
submit another post -- including commented, minimal, self-contained, 
reproducible code, as suggested in the posting guide 
"www.R-project.org/posting-guide.html".   Questions without commented, 
minimal, self-contained, reproducible code are generally much harder to 
understand, which means that people's propensity to reply and the 
relevance of any replies are both lower.  Help others help you. 

      hope this helps. 
      Spencer Graves  

H. Paul Benton wrote:
> Hello all and thank you,
>
> 	I will try to make this as clear as I can. I have a matrix, at the
> top of the matrix are 4 time points, the first col describes the sample,
> then under each time point I have the intensity values of that sample. 
> So :
> Samples	0	10	30	120 <- time 
> A		200	300	400	1000 <- data
> B		100	400	200	100
> C		500	400	200	1000
> ^
> | samples
>
> I would like to plot these however, all I get is a very messed up plot. It
> may be due to the size of the matrix, it is 7746 rows long. But I thought
> for R that shouldn't make a difference. 
>
> 	Cheers,
>
> 	Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Philip.Turk at NAU.EDU  Tue Aug 29 02:55:23 2006
From: Philip.Turk at NAU.EDU (Phil Turk)
Date: Mon, 28 Aug 2006 17:55:23 -0700
Subject: [R] Legend box line thickness
Message-ID: <44F8BEC3@webmail.nau.edu>

I am merely trying to increase the line thickness, or line width, of the box 
drawn around the legend in a plot I am constructing.  The help page on 
'legend' was of no use.  Does anyone have an idea on how to do this?  Please 
respond to philip.turk at nau.edu.  Thanks!


From miltinho_astronauta at yahoo.com.br  Tue Aug 29 03:13:59 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Mon, 28 Aug 2006 22:13:59 -0300 (ART)
Subject: [R] Bootstraping for groups and subgroups and joing with other table
Message-ID: <20060829011359.3965.qmail@web53405.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/0400158f/attachment.pl 

From macq at llnl.gov  Tue Aug 29 03:15:06 2006
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 28 Aug 2006 18:15:06 -0700
Subject: [R] Download problems
In-Reply-To: <C118B4B9.41FF%fhm@unimelb.edu.au>
References: <C118B4B9.41FF%fhm@unimelb.edu.au>
Message-ID: <p06230900c11944788818@[192.168.2.6]>

If by "2.8" you mean 10.2.8, that's a very old OS X.

To use a current version of R on OS X, you need a current version of OS X.

Version information is given pretty clearly on CRAN, so make sure you 
have downloaded a suitable version -- if there is one for 10.2.8. 
Even if there is, you would be much better off upgrading your OS, 
since the Mac binary of R has improved a great deal since OS X 10.2.8.

-Don


At 2:56 PM +1000 8/28/06, Felicity Meakins wrote:
>Hallo,
>
>I am a new user of 'R'. I have been trying to download it onto my Mac (OSX
>2.8). For some reason it seems to download ok and look ok but then it
>doesn't actually open. Can somebody help me this?
>
>Felicity
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA


From Nongluck.Klibbua at newcastle.edu.au  Tue Aug 29 03:12:29 2006
From: Nongluck.Klibbua at newcastle.edu.au (Nongluck Klibbua)
Date: Tue, 29 Aug 2006 11:12:29 +1000
Subject: [R] singular matrix
Message-ID: <44F4211D020000DA000001A7@MC-GWDOM2.newcastle.edu.au>

Dear R-users,
I try to use "solve" to get the solution of this matrix.But it has error
happen below.
How  I can solve this problem. 
[1] "a"
          [,1]
[1,] 0.8109437
[2,] 5.7569740
[1] "b"
          [,1]      [,2]
[1,] 0.3141293  2.230037
[2,] 2.2300367 15.831264

Error in solve.default(b, a) : system is computationally singular:
reciprocal condition number = 1.37415e-018

Thanks 
Luck


From miltinho_astronauta at yahoo.com.br  Tue Aug 29 03:34:26 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Tue, 29 Aug 2006 01:34:26 +0000 (GMT)
Subject: [R] En:  Bootstraping for groups (right data tables)
Message-ID: <20060829013426.72541.qmail@web53412.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/a5221220/attachment.pl 

From MSchwartz at mn.rr.com  Tue Aug 29 03:46:10 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 28 Aug 2006 20:46:10 -0500
Subject: [R] Legend box line thickness
In-Reply-To: <44F8BEC3@webmail.nau.edu>
References: <44F8BEC3@webmail.nau.edu>
Message-ID: <1156815971.6353.26.camel@localhost.localdomain>

On Mon, 2006-08-28 at 17:55 -0700, Phil Turk wrote:
> I am merely trying to increase the line thickness, or line width, of the box 
> drawn around the legend in a plot I am constructing.  The help page on 
> 'legend' was of no use.  Does anyone have an idea on how to do this?  Please 
> respond to philip.turk at nau.edu.  Thanks!

There are two options.

The easier one is not immediately evident from the help page and
requires reviewing the R code for the legend function, which reveals
that there is an internal function called rect2(), which is built on top
of rect(). It is this function that draws the outer box.

The help page for rect() shows that the line width argument 'lwd' in the
function defaults to par("lwd"). See ?par for more information.

Thus using:

  par(lwd = SomethingGreaterThan1)

before the call to legend will set the box to a wider line thickness.

Be sure to set par(lwd = 1) before any other plot calls to return to the
default setting.



Second, the Value section of ?legend clearly indicates:


Value

A list with list components

rect
a list with components 
w, h
        positive numbers giving
        width and height of the
        legend's box.
left, top
        x and y coordinates of upper
        left corner of the box.
text
a list with components 
x, y
        numeric vectors of length
        length(legend), giving the x
        and y coordinates of the
        legend's text(s).

returned invisibly.



Thus, expanding on the third example in ?legend:

## right-justifying a set of labels: thanks to Uwe Ligges
x <- 1:5; y1 <- 1/x; y2 <- 2/x
plot(rep(x, 2), c(y1, y2), type="n", xlab="x", ylab="y")
lines(x, y1); lines(x, y2, lty=2)

# Key call here
temp <- legend("topright", legend = c(" ", " "),
               text.width = strwidth("1,000,000"),
               lty = 1:2, xjust = 1, yjust = 1,
               title = "Line Types")

text(temp$rect$left + temp$rect$w, temp$text$y,
     c("1,000", "1,000,000"), pos=2)


# Now do the legend box using a wide line:
rect(temp$rect$left, temp$rect$top - temp$rect$h, 
     temp$rect$left + temp$rect$w, temp$rect$top + temp$rect$h, lwd = 2)


It would not seem unreasonable to add new arguments to legend(), perhaps
calling them box.lwd and box.lty, which can then be passed to the
rect2() internal function call for the box by modifying the existing
call to:

 rect2(left, top, dx = w, dy = h, col = bg, density = NULL, 
       lwd = box.lwd, lty = box.lty)


HTH,

Marc Schwartz


From Bill.Venables at csiro.au  Tue Aug 29 05:36:10 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 29 Aug 2006 13:36:10 +1000
Subject: [R] singular matrix
Message-ID: <B998A44C8986644EA8029CFE6396A9248401A0@exqld2-bne.qld.csiro.au>

Nongluck Klibbua reports:

> -----Original Message----- 

> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nongluck
> Klibbua
> Sent: Tuesday, 29 August 2006 11:12 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] singular matrix
> 
> Dear R-users,
> I try to use "solve" to get the solution of this matrix.But it has
error
> happen below.
> How  I can solve this problem. 
> [1] "a"
> 	    [,1]
> [1,] 0.8109437
> [2,] 5.7569740
> [1] "b"
> 	    [,1]      [,2]
> [1,] 0.3141293  2.230037
> [2,] 2.2300367 15.831264
> 
> Error in solve.default(b, a) : system is computationally singular:
> reciprocal condition number = 1.37415e-018

The irony seems to be that if you quote them to only this number of
digits then the system is no longer quite singular enough for the
problem to appear, at least on Windows R 2.3.1:

> a
          [,1]
[1,] 0.8109437
[2,] 5.7569740
> b
          [,1]      [,2]
[1,] 0.3141293  2.230037
[2,] 2.2300367 15.831264
> solve(b, a)
              [,1]
[1,]  2.5831242104
[2,] -0.0002203103
> b %*% solve(b, a) - a  ### check
              [,1]
[1,] -1.110223e-16
[2,] -8.881784e-16

In general, though, in dealing with singular systems you might want to
look at the function ginv in the MASS library.


From ggrothendieck at gmail.com  Tue Aug 29 05:39:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Aug 2006 23:39:02 -0400
Subject: [R] singular matrix
In-Reply-To: <44F4211D020000DA000001A7@MC-GWDOM2.newcastle.edu.au>
References: <44F4211D020000DA000001A7@MC-GWDOM2.newcastle.edu.au>
Message-ID: <971536df0608282039x44834221h31023c77da01a236@mail.gmail.com>

b is nearly singular.  Note that one of its eigenvalues is -2.935e-8
which is close to zero.  We can use the generalized inverse from
MASS to get one solution, x, but any multiple of the eigenvector
corresponding to the near-zero eigenvalue when added to that
will also give a solution as shown:

> eigen(b)
$values
[1]  1.614539e+01 -2.935343e-08

$vectors
           [,1]       [,2]
[1,] -0.1394858 -0.9902241
[2,] -0.9902241  0.1394858

> library(MASS)
> x <- ginv(b) %*% a
> a
[1] 0.8109437 5.7569740
> # bx gives a showing x is a solution
> b %*% x
          [,1]
[1,] 0.8109438
[2,] 5.7569740
> # but b(x + e) where e is 2nd eigenvector is also solution
> b %*% (x + eigen(b)$vectors[,2])
          [,1]
[1,] 0.8109438
[2,] 5.7569740

On 8/28/06, Nongluck Klibbua <Nongluck.Klibbua at newcastle.edu.au> wrote:
> Dear R-users,
> I try to use "solve" to get the solution of this matrix.But it has error
> happen below.
> How  I can solve this problem.
> [1] "a"
>          [,1]
> [1,] 0.8109437
> [2,] 5.7569740
> [1] "b"
>          [,1]      [,2]
> [1,] 0.3141293  2.230037
> [2,] 2.2300367 15.831264
>
> Error in solve.default(b, a) : system is computationally singular:
> reciprocal condition number = 1.37415e-018
>
> Thanks
> Luck
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From e.catchpole at adfa.edu.au  Tue Aug 29 06:07:10 2006
From: e.catchpole at adfa.edu.au (ecatchpole)
Date: Tue, 29 Aug 2006 14:07:10 +1000
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <44F28BE0.3050807@mango-solutions.com>
References: <44F28BE0.3050807@mango-solutions.com>
Message-ID: <44F3BD6E.2070107@adfa.edu.au>

Romain Francois wrote on 08/28/2006 04:23 PM:
> =================================================
> Search for your R routines directly from Firefox!
>  >> http://addictedtor.free.fr/rsitesearch
> =================================================
>   
When I try to search I get the Firefox error

TypeError: document.getElementById(currentPackage + ":" +
currentHelpPage) has no properties

Help!  (Screen shot emailed separately to Romain.)

Ted.

-- 
Dr E.A. Catchpole  
Visiting Fellow
Univ of New South Wales at ADFA, Canberra, Australia
and University of Kent, Canterbury, England
- www.pems.adfa.edu.au/~ecatchpole          
- fax: +61 2 6268 8786		   
- ph:  +61 2 6268 8895


From Philip.Turk at NAU.EDU  Tue Aug 29 02:43:25 2006
From: Philip.Turk at NAU.EDU (Philip Turk)
Date: Mon, 28 Aug 2006 17:43:25 -0700
Subject: [R] Legend box line thickness
Message-ID: <000001c6cb04$234cc7e0$a8947286@math.nau.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/9d414890/attachment.pl 

From Bill.Venables at csiro.au  Tue Aug 29 01:36:25 2006
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 29 Aug 2006 09:36:25 +1000
Subject: [R] Type II and III sum of square in Anova (R, car package)
Message-ID: <B998A44C8986644EA8029CFE6396A924840190@exqld2-bne.qld.csiro.au>

I cannot resist a very brief entry into this old and seemingly
immortal issue, but I will be very brief, I promise!

Amasco Miralisus suggests:

> As I understood form R FAQ, there is disagreement among Statisticians
> which SS to use
>
(http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-does-the-output-from-a
nova_0028_0029-depend-on-the-order-of-factors-in-the-model_003f).

To let this go is to concede way too much.  The 'disagreement' is
really over whether this is a sensible question to ask in the first
place.  One side of the debate suggests that the real question is what
hypotheses does it make sense to test and within what outer
hypotheses.  Settle that question and no issue on "types" of sums of
squares arises.

This is often a hard question to get your head around, and the
attraction of offering a variety of 'types of sums of squares' holds
out the false hope that perhaps you don't need to do so.  The bad
news is that for good science and good decision making, you do.

Bill Venables.


From szdobrowski at ucdavis.edu  Tue Aug 29 05:32:41 2006
From: szdobrowski at ucdavis.edu (Solomon Dobrowski)
Date: Mon, 28 Aug 2006 20:32:41 -0700
Subject: [R] Deviance function in regression trees
Message-ID: <200608290332.k7T3WYiB000588@flpvm09.prodigy.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060828/892a3a43/attachment.pl 

From r.hankin at noc.soton.ac.uk  Tue Aug 29 09:09:01 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 29 Aug 2006 08:09:01 +0100
Subject: [R] Help with Functions
In-Reply-To: <4f31b0bd0608280950h41c260b3w6c6acf6e0ea3de5b@mail.gmail.com>
References: <4f31b0bd0608280950h41c260b3w6c6acf6e0ea3de5b@mail.gmail.com>
Message-ID: <F020B714-93C9-4633-A5CC-1D8E77EC170F@soc.soton.ac.uk>

Hi

check out R-and-octave.txt on the contributed docs section of CRAN.


HTH


Robin

On 28 Aug 2006, at 17:50, Lord Tyranus wrote:

> Hello wizards, I need to convert the following functions (prestd,
> poststd, prepca)   of matlab to R. Does Somebody knows how to do it. A
> description of the functions is:
>
> prestd preprocesses the network training set by normalizing the inputs
> and targets so that they have means of zero and standard deviations of
> 1.
>
> poststd postprocesses the network training set which was preprocessed
> by prestd. It converts the data back into unnormalized units.
>
> prepca preprocesses the network input training set by applying a
> principal component analysis. This analysis transforms the input data
> so that the elements of the input vector set will be uncorrelated. In
> addition, the size of the input vectors may be reduced by retaining
> only those components which contribute more than a specified fraction
> (min_frac) of the total variation in the data set.
>
> Thanks in advance.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ripley at stats.ox.ac.uk  Tue Aug 29 09:13:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Aug 2006 08:13:54 +0100 (BST)
Subject: [R] Deviance function in regression trees
In-Reply-To: <200608290332.k7T3WYiB000588@flpvm09.prodigy.net>
References: <200608290332.k7T3WYiB000588@flpvm09.prodigy.net>
Message-ID: <Pine.LNX.4.64.0608290804360.29102@gannet.stats.ox.ac.uk>

The short answer is that a Poisson distribution is a discrete 
distribution: if that is appropriate to your data the rpart function (in 
the package of that name) has a suitable option.

On Mon, 28 Aug 2006, Solomon Dobrowski wrote:

> Hello all. I have heard over and over that CART and its various tree-like
> brethren are "non-parametric" techniques.  When I read the chapter in
> Chambers and Hastie on tree-based models it states that tree-based models
> can be generalized (GTMs) in a manner similar to GLMs by specifying a
> different deviance function to distributions other than the gaussian error
> distribution ( section 9.4.3).  I have an application in which the response
> variable is a continuous variable representing tree counts within a unit
> area and thus would be best described by a poisson distribution. The error
> distribution for this data is not gaussian. If this is the case, will the
> gaussian error distribution used in most regression tree packages, be
> appropriate? Are there ways to specify the error distribution in R or Should
> I log transform the response variable?  If the specification of error
> distribution in regression trees is important, than are these techniques
> truly " non-parametric". Thanks for your inputs. 
> 
> 
> Solomon Dobrowski
> Tahoe Environmental Research Center (TERC)
> John Muir Institute of the Environment
> University of California, Davis
> 530 754 9354
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rfrancois at mango-solutions.com  Tue Aug 29 09:21:26 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Tue, 29 Aug 2006 08:21:26 +0100
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <44F3BD6E.2070107@adfa.edu.au>
References: <44F28BE0.3050807@mango-solutions.com>
	<44F3BD6E.2070107@adfa.edu.au>
Message-ID: <44F3EAF6.3090503@mango-solutions.com>

ecatchpole a ?crit :
> Romain Francois wrote on 08/28/2006 04:23 PM:
>   
>> =================================================
>> Search for your R routines directly from Firefox!
>>  >> http://addictedtor.free.fr/rsitesearch
>> =================================================
>>   
>>     
> When I try to search I get the Firefox error
>
> TypeError: document.getElementById(currentPackage + ":" +
> currentHelpPage) has no properties
>
> Help!  (Screen shot emailed separately to Romain.)
>
> Ted
Hi Ted,

Good catch. I'll fix it today.
Did you try to search for something ?

BTW, I'll set up a page in the developers section of the wiki to discuss 
(problems with) this project. To appear soon.

Cheers,

Romain


From ccleland at optonline.net  Tue Aug 29 11:01:40 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 29 Aug 2006 05:01:40 -0400
Subject: [R] standardized partial regression coefficients
In-Reply-To: <44F3870C.7040209@uga.edu>
References: <44F3870C.7040209@uga.edu>
Message-ID: <44F40274.4000804@optonline.net>

Yuval Sapir wrote:
> Hi all,
> I am relatively new to R, so let me know if I missed something trivial.
> I want to perform path-analysis with at least three levels of explaining 
> variables (i.e., A affects B that affects C). I perform multiple 
> regression for each set of variables, and get estimates for the partial 
> coefficients and their SE. How can I translate it to standardized 
> partial regression coefficients ("beta" in SPSS)?
> Thanks
> Yuval

  RSiteSearch("standardized coefficients") is helpful.  For example:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68367.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/6038.html

  You also might consider the sem package by John Fox with its
standardized.coefficients function.

http://cran.r-project.org/doc/packages/sem.pdf

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From isidora10 at yahoo.com  Tue Aug 29 12:53:18 2006
From: isidora10 at yahoo.com (isidora k)
Date: Tue, 29 Aug 2006 03:53:18 -0700 (PDT)
Subject: [R] EOF and CCA analysis
Message-ID: <20060829105318.50406.qmail@web52102.mail.yahoo.com>

Hello!
I would like to do Empirical Orthogonal functions and
Canonical correlation analysis on satellite data. My
matrices are going to be very big (more than 10,000
locations). Are there limitations in R regarding the
size of the matrix?
thanks a lot
regards
Isidora


From petr.pikal at precheza.cz  Tue Aug 29 12:53:40 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 29 Aug 2006 12:53:40 +0200
Subject: [R] Help with Functions
In-Reply-To: <4f31b0bd0608280950h41c260b3w6c6acf6e0ea3de5b@mail.gmail.com>
Message-ID: <44F438D4.5833.F43AB4@localhost>

Hi

On 28 Aug 2006 at 10:50, Lord Tyranus wrote:

Date sent:      	Mon, 28 Aug 2006 10:50:15 -0600
From:           	"Lord Tyranus" <lord.tyranus.96 at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Help with Functions

> Hello wizards, I need to convert the following functions (prestd,
> poststd, prepca)   of matlab to R. Does Somebody knows how to do it. A
> description of the functions is:
> 
> prestd preprocesses the network training set by normalizing the inputs
> and targets so that they have means of zero and standard deviations of
> 1.

Maybe scale function

> 
> poststd postprocesses the network training set which was preprocessed
> by prestd. It converts the data back into unnormalized units.

work with attributes of scaled values.

HTH
Petr

> 
> prepca preprocesses the network input training set by applying a
> principal component analysis. This analysis transforms the input data
> so that the elements of the input vector set will be uncorrelated. In
> addition, the size of the input vectors may be reduced by retaining
> only those components which contribute more than a specified fraction
> (min_frac) of the total variation in the data set.
> 
> Thanks in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From gunther.hoening at ukmainz.de  Tue Aug 29 13:11:50 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Tue, 29 Aug 2006 13:11:50 +0200
Subject: [R] AffyChip Background Analysis
Message-ID: <006501c6cb5b$ed4125d0$0f1e0b0a@3med.klinik.unimainz.de>

Dear list,

I want to analyse some HG133Plus2.0 Affymetrix chips.
The first thing I intent to do, is just to perform a "background correction
(mas, rma, gcrma,...)" with the cel files.
Then I want to take a look at the files.
How can I do this ? 

Gunther


From msubianto at gmail.com  Tue Aug 29 13:22:36 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Tue, 29 Aug 2006 13:22:36 +0200
Subject: [R] Remove empty list from list - remove only one row and make
 as matrix
In-Reply-To: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
References: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
Message-ID: <44F4237C.60009@gmail.com>

Dear all,
After I work around, I found in my list of data with only one row which 
need to remove or make it as matrix.
Here I write again my other toy example:
x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5, 
4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
y <- list(c(1, -1, -1, 1, 1),c(1, -1, -1, -1, -1),c(1, 1, 1, 1, 1),c(1, 
1, -1, 1, -1),c(-1, -1, -1, -1, -1))
## Thanks to Gabor Grothendieck for this trick.
## SIMPLIFY? SIMPLIFY >< simplify
xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)

point.class <- 
t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
pd.class <- points.diff(class.diffsame,xy.list)

nc.test <- vector("list",length(pd.class))
for (i in 1:length(pd.class)) {
      nc.test[[i]] <- pd.class[[i]]$point.diff
}
nc.test

# delete null/empty entries in a list
dff <- delete.NULLs(nc.test)
dff; str(dff)
 > dff
[[1]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
[1]  1  6 11 16  1

[[3]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1

 >

lapply(dff, nrow)
 > lapply(dff, nrow)
[[1]]
[1] 3

[[2]]
NULL

[[3]]
[1] 2

 >

#I can use
#dff[unlist(lapply(dff, nrow) == 1)] #2,3, etc

I have two questions here:
a. I need to remove dff[[2]]
b. How to make it as matrix (in list). I mean the result something like

[[1]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
      [,1] [,2] [,3] [,4] [,5]
[1]    1    6    11   16    1

[[3]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1


Best, Muhammad Subianto


points.neighb <- function(p.class, list.nc, class.col) {
    ntuples <- nrow(p.class)
    instvec <- vector("list",length=ntuples)
    for (i in 1:ntuples) {
         # Thanks to Petr Pikal for this trick
         instvec[[i]]$class.diff <- (p.class[i,class.col] - 
list.nc[[i]][,class.col])!=0
         instvec[[i]]$class.same <- (p.class[i,class.col] - 
list.nc[[i]][,class.col])==0
    }
    instvec
}

points.diff <- function(p.class, list.nc) {
    ntuples <- length(list.nc)
    instvec <- vector("list",ntuples)
    for (i in 1:ntuples) {
         instvec[[i]]$point.diff <- list.nc[[i]][p.class[[i]]$class.diff,]
         instvec[[i]]$point.same <- list.nc[[i]][p.class[[i]]$class.same,]
    }
    instvec
}

# Thanks to Jim Holtman for this trick
delete.NULLs  <-  function(x.list){
     x.list[unlist(lapply(x.list, length) != 0)]
}


From jsorkin at grecc.umaryland.edu  Tue Aug 29 13:24:14 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 29 Aug 2006 07:24:14 -0400
Subject: [R] Type II and III sum of square in Anova (R, car	package)
In-Reply-To: <66d34c7a0608281220h2f460cc2q861d19962e7b4301@mail.gmail.com>
References: <66d34c7a0608261507t27709825g214b42ff46e200bb@mail.gmail.com>
	<20060827135157.QPGG24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<66d34c7a0608281220h2f460cc2q861d19962e7b4301@mail.gmail.com>
Message-ID: <44F31124.A712.00CB.0@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/00d35d59/attachment.pl 

From ggrothendieck at gmail.com  Tue Aug 29 13:30:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 07:30:53 -0400
Subject: [R] screen resolution effects on graphics
In-Reply-To: <971536df0608281213r3a061b7br4284d7358bb711@mail.gmail.com>
References: <44F3360C.5020606@mango-solutions.com>
	<00a101c6cad3$fcbea2e0$6400a8c0@DD4XFW31>
	<971536df0608281213r3a061b7br4284d7358bb711@mail.gmail.com>
Message-ID: <971536df0608290430g685f9fe6x168eccc4b348bdb4@mail.gmail.com>

In thinking about this some more Romain's idea of using
the browser to get this information can be done like this.
This solution only works with Internet Explorer:

library(RDCOMClient)
ie <- COMCreate("InternetExplorer.Application")
ie$Navigate("about:blank")
width <- ie[["document"]][["ParentWindow"]][["screen"]][["width"]]
height <- ie[["document"]][["ParentWindow"]][["screen"]][["height"]]
ie$Quit()

On 8/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I think there are problems with browseURL on Windows XP with IE.
> Put the file in screen.htm and then run:
>
> shell.exec("screen.htm")
>
> it will come up -- it will block it and you will have to click to
> unblock it; however,
> that will interfere with any automatic procedure.
>
> On 8/28/06, Charles Annis, P.E.
> <Charles.Annis at statisticalengineering.com> wrote:
> > Romain:
> >
> > > a <- tempfile()
> > > cat('<html><script type="text/javascript"> document.write(screen.width) ;
> > </script></html>', file=a)
> > > browseURL(a)
> > >
> >
> > The object "a" was created, but  no browser opened.
> >
> > > ls()
> > [1] "a"
> > > a
> > [1] "C:\\DOCUME~1\\CHARLE~1\\LOCALS~1\\Temp\\RtmpRgWrqb\\file678418be"
> > >
> >
> >
> >
> > Charles Annis, P.E.
> >
> > Charles.Annis at StatisticalEngineering.com
> > phone: 561-352-9699
> > eFax:  614-455-3265
> > http://www.StatisticalEngineering.com
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Romain Francois
> > Sent: Monday, August 28, 2006 2:30 PM
> > To: Prof Brian Ripley
> > Cc: Charles Annis, P.E.; r-help at stat.math.ethz.ch
> > Subject: Re: [R] screen resolution effects on graphics
> >
> > Prof Brian Ripley a ?crit :
> > > On Mon, 28 Aug 2006, Charles Annis, P.E. wrote:
> > >
> > >
> > >> Greetings, R-Citizens:
> > >>
> > >> I have the good fortune of working with a 19" 1280 X 1024 pixel monitor.
> > My
> > >>
> > >
> > > (Similar to our student lab has used for many years.)
> > >
> > >
> > >> R-code produces nice-looking graphics on this machine but the same code
> > >> results in crowded plots on an older machine with 800 X 600 resolution.
> > In
> > >> hindsight this seems obvious, but I didn't anticipate it.
> > >>
> > >
> > > It is not obvious to me: I have never experienced it.  What OS and
> > > graphics device is this?
> > >
> > > Almost all of R's graphics is independent of the screen resolution (the
> > > exception being the bitmapped devices such as jpeg), with things sized in
> > > inches or points. My machines are 1600x1200 (apart from 1280x800 on my
> > > laptop), so I meet a considerable reduction when using a computer
> > > projector, and my plots do not look crowded.
> > >
> > > However, one issue is when the OS has a seriously incorrect setting for
> > > the screen resolution and so does not give the sizes asked for by R.  We
> > > have seen that on both Linux and Windows, and the windows() device has
> > > arguments to set the correct values.  (On X11 you should be able to set
> > > this in Xconfig files.)
> > >
> > > If this is Windows, check carefully the description of the initial screen
> > > size in ?windows.  That can have unexpected effects on physically small
> > > screens.
> > >
> > > At one time the X11() device was set up to assume 75dpi unless the
> > > reported resolution was 100+/-0.5dpi.  My then monitor reported 99.2 dpi
> > > and so things came out at 3/4 of the intended size.  We fixed that quite a
> >
> > > while back.
> > >
> > >
> > >> My code will be used on machines with varying graphics (and memory)
> > >> capacity.  Is there a way I can check the native resolution of the
> > machine
> > >> so that I can make adjustments to my code for the possible limitations of
> > >> the machine running it?
> > >>
> > >
> > > Only via C code, which is how R does it.
> > Hi,
> >
> > Javascript knows, can we ask him ?
> >
> > I mean, if I do that in R :
> >
> > a <- tempfile()
> > cat('<html><script type="text/javascript"> document.write(screen.width)
> > ;  </script></html>', file=a)
> > browseURL(a)
> >
> > I get "1920" in my browser's window. Can R read it ?
> >
> > Romain
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From dimitris.rizopoulos at med.kuleuven.be  Tue Aug 29 13:34:13 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 29 Aug 2006 13:34:13 +0200
Subject: [R] Remove empty list from list - remove only one row and make
	as matrix
References: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>
	<44F4237C.60009@gmail.com>
Message-ID: <001201c6cb5f$0d9a3760$0540210a@www.domain>

try the following:

# a
dff[sapply(dff, is.matrix)]

# b
lapply(dff, function(x) if(!is.matrix(x)) rbind(x) else x)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Muhammad Subianto" <msubianto at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, August 29, 2006 1:22 PM
Subject: Re: [R] Remove empty list from list - remove only one row and 
make as matrix


> Dear all,
> After I work around, I found in my list of data with only one row 
> which
> need to remove or make it as matrix.
> Here I write again my other toy example:
> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5,
> 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
> y <- list(c(1, -1, -1, 1, 1),c(1, -1, -1, -1, -1),c(1, 1, 1, 1, 
> 1),c(1,
> 1, -1, 1, -1),c(-1, -1, -1, -1, -1))
> ## Thanks to Gabor Grothendieck for this trick.
> ## SIMPLIFY? SIMPLIFY >< simplify
> xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)
>
> point.class <-
> t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
> class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
> pd.class <- points.diff(class.diffsame,xy.list)
>
> nc.test <- vector("list",length(pd.class))
> for (i in 1:length(pd.class)) {
>      nc.test[[i]] <- pd.class[[i]]$point.diff
> }
> nc.test
>
> # delete null/empty entries in a list
> dff <- delete.NULLs(nc.test)
> dff; str(dff)
> > dff
> [[1]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    4    9   14   19    1
> [3,]    5   10   15   20    1
>
> [[2]]
> [1]  1  6 11 16  1
>
> [[3]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    3    8   13   18   -1
> [2,]    5   10   15   20   -1
>
> >
>
> lapply(dff, nrow)
> > lapply(dff, nrow)
> [[1]]
> [1] 3
>
> [[2]]
> NULL
>
> [[3]]
> [1] 2
>
> >
>
> #I can use
> #dff[unlist(lapply(dff, nrow) == 1)] #2,3, etc
>
> I have two questions here:
> a. I need to remove dff[[2]]
> b. How to make it as matrix (in list). I mean the result something 
> like
>
> [[1]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16    1
> [2,]    4    9   14   19    1
> [3,]    5   10   15   20    1
>
> [[2]]
>      [,1] [,2] [,3] [,4] [,5]
> [1]    1    6    11   16    1
>
> [[3]]
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    3    8   13   18   -1
> [2,]    5   10   15   20   -1
>
>
> Best, Muhammad Subianto
>
>
> points.neighb <- function(p.class, list.nc, class.col) {
>    ntuples <- nrow(p.class)
>    instvec <- vector("list",length=ntuples)
>    for (i in 1:ntuples) {
>         # Thanks to Petr Pikal for this trick
>         instvec[[i]]$class.diff <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])!=0
>         instvec[[i]]$class.same <- (p.class[i,class.col] -
> list.nc[[i]][,class.col])==0
>    }
>    instvec
> }
>
> points.diff <- function(p.class, list.nc) {
>    ntuples <- length(list.nc)
>    instvec <- vector("list",ntuples)
>    for (i in 1:ntuples) {
>         instvec[[i]]$point.diff <- 
> list.nc[[i]][p.class[[i]]$class.diff,]
>         instvec[[i]]$point.same <- 
> list.nc[[i]][p.class[[i]]$class.same,]
>    }
>    instvec
> }
>
> # Thanks to Jim Holtman for this trick
> delete.NULLs  <-  function(x.list){
>     x.list[unlist(lapply(x.list, length) != 0)]
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Tue Aug 29 14:04:46 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Aug 2006 13:04:46 +0100 (BST)
Subject: [R] EOF and CCA analysis
In-Reply-To: <20060829105318.50406.qmail@web52102.mail.yahoo.com>
References: <20060829105318.50406.qmail@web52102.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608291301110.28817@gannet.stats.ox.ac.uk>

More than 10,000 locations but how many variables?

Like all computer programs there are limitations, but they are largely 
imposed by your environment, so what OS and how much RAM do you have?

On Tue, 29 Aug 2006, isidora k wrote:

> Hello!
> I would like to do Empirical Orthogonal functions and
> Canonical correlation analysis on satellite data. My
> matrices are going to be very big (more than 10,000
> locations). Are there limitations in R regarding the
> size of the matrix?
> thanks a lot
> regards
> Isidora
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From msubianto at gmail.com  Tue Aug 29 14:14:45 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Tue, 29 Aug 2006 14:14:45 +0200
Subject: [R] Remove empty list from list - remove only one row and make
 as matrix
In-Reply-To: <001201c6cb5f$0d9a3760$0540210a@www.domain>
References: <c7c17cef0608281020g6f7897ffr81e78cae34f8a9f7@mail.gmail.com>	<44F4237C.60009@gmail.com>
	<001201c6cb5f$0d9a3760$0540210a@www.domain>
Message-ID: <44F42FB5.6000400@gmail.com>


Dear all,
Dimitris, thanks for your great help and quick response.

Best, Muhammad Subianto


 > dff[sapply(dff, is.matrix)]
[[1]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1

 > lapply(dff, function(x) if(!is.matrix(x)) rbind(x) else x)
[[1]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16    1
[2,]    4    9   14   19    1
[3,]    5   10   15   20    1

[[2]]
   [,1] [,2] [,3] [,4] [,5]
x    1    6   11   16    1

[[3]]
      [,1] [,2] [,3] [,4] [,5]
[1,]    3    8   13   18   -1
[2,]    5   10   15   20   -1

 >

On this day 29/08/2006 13:34, Dimitris Rizopoulos wrote:
> try the following:
> 
> # a
> dff[sapply(dff, is.matrix)]
> 
> # b
> lapply(dff, function(x) if(!is.matrix(x)) rbind(x) else x)
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Muhammad Subianto" <msubianto at gmail.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, August 29, 2006 1:22 PM
> Subject: Re: [R] Remove empty list from list - remove only one row and 
> make as matrix
> 
> 
>> Dear all,
>> After I work around, I found in my list of data with only one row 
>> which
>> need to remove or make it as matrix.
>> Here I write again my other toy example:
>> x <- list(matrix(1:20, 5, 4),matrix(1:20, 5, 4),matrix(1:20, 5,
>> 4),matrix(1:20, 5, 4),matrix(1:20, 5, 4))
>> y <- list(c(1, -1, -1, 1, 1),c(1, -1, -1, -1, -1),c(1, 1, 1, 1, 
>> 1),c(1,
>> 1, -1, 1, -1),c(-1, -1, -1, -1, -1))
>> ## Thanks to Gabor Grothendieck for this trick.
>> ## SIMPLIFY? SIMPLIFY >< simplify
>> xy.list <- mapply(cbind, x, y, SIMPLIFY=FALSE)
>>
>> point.class <-
>> t(cbind(c(10,20,15,4,-1),c(21,10,15,34,-1),c(11,13,6,3,1),c(7,5,5,2,1),c(8,9,5,12,-1)))
>> class.diffsame <- points.neighb(as.matrix(point.class), xy.list, 5)
>> pd.class <- points.diff(class.diffsame,xy.list)
>>
>> nc.test <- vector("list",length(pd.class))
>> for (i in 1:length(pd.class)) {
>>      nc.test[[i]] <- pd.class[[i]]$point.diff
>> }
>> nc.test
>>
>> # delete null/empty entries in a list
>> dff <- delete.NULLs(nc.test)
>> dff; str(dff)
>>> dff
>> [[1]]
>>      [,1] [,2] [,3] [,4] [,5]
>> [1,]    1    6   11   16    1
>> [2,]    4    9   14   19    1
>> [3,]    5   10   15   20    1
>>
>> [[2]]
>> [1]  1  6 11 16  1
>>
>> [[3]]
>>      [,1] [,2] [,3] [,4] [,5]
>> [1,]    3    8   13   18   -1
>> [2,]    5   10   15   20   -1
>>
>> lapply(dff, nrow)
>>> lapply(dff, nrow)
>> [[1]]
>> [1] 3
>>
>> [[2]]
>> NULL
>>
>> [[3]]
>> [1] 2
>>
>> #I can use
>> #dff[unlist(lapply(dff, nrow) == 1)] #2,3, etc
>>
>> I have two questions here:
>> a. I need to remove dff[[2]]
>> b. How to make it as matrix (in list). I mean the result something 
>> like
>>
>> [[1]]
>>      [,1] [,2] [,3] [,4] [,5]
>> [1,]    1    6   11   16    1
>> [2,]    4    9   14   19    1
>> [3,]    5   10   15   20    1
>>
>> [[2]]
>>      [,1] [,2] [,3] [,4] [,5]
>> [1]    1    6    11   16    1
>>
>> [[3]]
>>      [,1] [,2] [,3] [,4] [,5]
>> [1,]    3    8   13   18   -1
>> [2,]    5   10   15   20   -1
>>
>>
>> Best, Muhammad Subianto
>>
>>
>> points.neighb <- function(p.class, list.nc, class.col) {
>>    ntuples <- nrow(p.class)
>>    instvec <- vector("list",length=ntuples)
>>    for (i in 1:ntuples) {
>>         # Thanks to Petr Pikal for this trick
>>         instvec[[i]]$class.diff <- (p.class[i,class.col] -
>> list.nc[[i]][,class.col])!=0
>>         instvec[[i]]$class.same <- (p.class[i,class.col] -
>> list.nc[[i]][,class.col])==0
>>    }
>>    instvec
>> }
>>
>> points.diff <- function(p.class, list.nc) {
>>    ntuples <- length(list.nc)
>>    instvec <- vector("list",ntuples)
>>    for (i in 1:ntuples) {
>>         instvec[[i]]$point.diff <- 
>> list.nc[[i]][p.class[[i]]$class.diff,]
>>         instvec[[i]]$point.same <- 
>> list.nc[[i]][p.class[[i]]$class.same,]
>>    }
>>    instvec
>> }
>>
>> # Thanks to Jim Holtman for this trick
>> delete.NULLs  <-  function(x.list){
>>     x.list[unlist(lapply(x.list, length) != 0)]
>> }
>>


From isidora10 at yahoo.com  Tue Aug 29 14:18:49 2006
From: isidora10 at yahoo.com (isidora k)
Date: Tue, 29 Aug 2006 05:18:49 -0700 (PDT)
Subject: [R] EOF and CCA analysis
In-Reply-To: <Pine.LNX.4.64.0608291301110.28817@gannet.stats.ox.ac.uk>
Message-ID: <20060829121850.51020.qmail@web52106.mail.yahoo.com>


I have got windows and 1Gb ram. my matrices are
monthly data for 10 years for sometimes more than
10,000 locations. I am going to perform the EOF on one
variable at a time and then the CCA on the principal
components that I will get from EOF. Do you think I
could do that in R?I could also find a macintosh I
could work with or get more RAM if you think this will
cause problems, but I am more concerned with the
software limitations if any!
thank you so much for your help.
Kind Regards
Isidora


From ryanteal22 at yahoo.ca  Tue Aug 29 14:24:24 2006
From: ryanteal22 at yahoo.ca (hedger22)
Date: Tue, 29 Aug 2006 05:24:24 -0700 (PDT)
Subject: [R] MODWT exceeds sample size
Message-ID: <6037934.post@talk.nabble.com>


Hello,

I get the following message when I try to load a time series of 128 stock
returns.

Error in modwt(stock, wf = "la8", n.levels = 8, boundary = "periodic") : 
        wavelet transform exceeds sample size in modwt

I thought that MODWT had no restrictions on the sample size?

Any help?


-- 
View this message in context: http://www.nabble.com/MODWT-exceeds-sample-size-tf2183217.html#a6037934
Sent from the R help forum at Nabble.com.


From ripley at stats.ox.ac.uk  Tue Aug 29 15:22:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Aug 2006 14:22:04 +0100 (BST)
Subject: [R] EOF and CCA analysis
In-Reply-To: <20060829121850.51020.qmail@web52106.mail.yahoo.com>
References: <20060829121850.51020.qmail@web52106.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0608291419240.24476@gannet.stats.ox.ac.uk>

That's a 10,000 x 120 matrix, if I understand you right.  That is quite 
modest.  The key issue is to be able to do an SVD:

> A <- matrix(rnorm(1e4*120), 1e4, 120)
> dim(A)
[1] 10000   120
> system.time(svd(A))
[1] 2.79 0.13 2.93   NA   NA

on a similar machine to yours, using 50Mb.

It would be worth considering downloading a suitable ATLAS-based 
Rblas.dll if you are doing much of this.


On Tue, 29 Aug 2006, isidora k wrote:

> 
> I have got windows and 1Gb ram. my matrices are
> monthly data for 10 years for sometimes more than
> 10,000 locations. I am going to perform the EOF on one
> variable at a time and then the CCA on the principal
> components that I will get from EOF. Do you think I
> could do that in R?I could also find a macintosh I
> could work with or get more RAM if you think this will
> cause problems, but I am more concerned with the
> software limitations if any!
> thank you so much for your help.
> Kind Regards
> Isidora
> 
> __________________________________________________
> Do You Yahoo!?
> Tired of spam?  Yahoo! Mail has the best spam protection around 
> http://mail.yahoo.com 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sfalcon at fhcrc.org  Tue Aug 29 15:45:44 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 29 Aug 2006 06:45:44 -0700
Subject: [R] AffyChip Background Analysis
In-Reply-To: <006501c6cb5b$ed4125d0$0f1e0b0a@3med.klinik.unimainz.de>
	(Gunther =?iso-8859-1?Q?H=F6ning's?= message of "Tue,
	29 Aug 2006 13:11:50 +0200")
References: <006501c6cb5b$ed4125d0$0f1e0b0a@3med.klinik.unimainz.de>
Message-ID: <m27j0rhczb.fsf@ziti.fhcrc.org>

Gunther H?ning <gunther.hoening at ukmainz.de> writes:

> Dear list,
>
> I want to analyse some HG133Plus2.0 Affymetrix chips.
> The first thing I intent to do, is just to perform a "background correction
> (mas, rma, gcrma,...)" with the cel files.
> Then I want to take a look at the files.
> How can I do this ? 

You might want to take a look at some of the packages in the
Bioconductor project.

Here is a link to the BioC packages that deal with visualization:
http://www.bioconductor.org/packages/release/Visualization.html

+ seth


From nipps85 at gmail.com  Tue Aug 29 16:17:56 2006
From: nipps85 at gmail.com (cory)
Date: Tue, 29 Aug 2006 09:17:56 -0500
Subject: [R] First elements of a list.
Message-ID: <af1fdf860608290717v7b0afb3erc415e6cc16772c66@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/f4ff3ed0/attachment.pl 

From jacques.veslot at good.ibl.fr  Tue Aug 29 16:24:38 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 29 Aug 2006 16:24:38 +0200
Subject: [R] First elements of a list.
In-Reply-To: <af1fdf860608290717v7b0afb3erc415e6cc16772c66@mail.gmail.com>
References: <af1fdf860608290717v7b0afb3erc415e6cc16772c66@mail.gmail.com>
Message-ID: <44F44E26.2020909@good.ibl.fr>

 > sapply(a, "[", 1)
[1] "John"   "Jane"   "koda"   "gunner"
 > sapply(a, "[", 2)
[1] "Smith" "Doe"   NA      NA
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


cory a ?crit :
> Suppose I have the following list:
> 
> a <- strsplit(c("John;Smith", "Jane;Doe", "koda", "gunner"), ";")
> 
> I want to get to these two vectors without looping...
> 
> firstNames:    c("John", "Jane", "koda", "gunner")
> lastNames:    c("Jane", "Doe", NA, NA)
> 
> Thanks
> 
> cn
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From friedman.steve at gmail.com  Tue Aug 29 16:35:59 2006
From: friedman.steve at gmail.com (Steve Friedman)
Date: Tue, 29 Aug 2006 10:35:59 -0400
Subject: [R] symbols in coplots
Message-ID: <2439f5740608290735u4488629ds1ee5a8f7cb328a98@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/c7cb987e/attachment.pl 

From weigong.ge at fda.hhs.gov  Tue Aug 29 17:08:26 2006
From: weigong.ge at fda.hhs.gov (Ge, Weigong*)
Date: Tue, 29 Aug 2006 10:08:26 -0500
Subject: [R] Bioconductor installation errors
Message-ID: <E4714BBD6A60E94F9D12E119D63E161206CA7428@exchange01.nctr.fda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/ee24dc20/attachment.pl 

From sfalcon at fhcrc.org  Tue Aug 29 17:32:17 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 29 Aug 2006 08:32:17 -0700
Subject: [R] Bioconductor installation errors
In-Reply-To: <E4714BBD6A60E94F9D12E119D63E161206CA7428@exchange01.nctr.fda.gov>
	(Weigong Ge's message of "Tue, 29 Aug 2006 10:08:26 -0500")
References: <E4714BBD6A60E94F9D12E119D63E161206CA7428@exchange01.nctr.fda.gov>
Message-ID: <m2hczveewu.fsf@ziti.fhcrc.org>

"Ge, Weigong*" <weigong.ge at fda.hhs.gov> writes:

> Hello,
>
> I follow the Bioconductor instruction (http://www.bioconductor.org/download
> <http://www.bioconductor.org/download> ) to install Biocoductor, there have
> some errors: 

Please post questions about Bioconductor to the bioconductor mailing
list:

http://www.bioconductor.org/docs/mailList.html


> /usr/lib/R/bin/SHLIB: line 115: make: command not found

Your system (Linux?) is missing very basic development tools.  It
seems you have no make program installed.  Such a program, among many
others, is required to build R packages from source.  Try to install
system packages for software development (C complier, etc).

+ seth


From sarah.goslee at gmail.com  Tue Aug 29 17:35:27 2006
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 29 Aug 2006 11:35:27 -0400
Subject: [R] Bioconductor installation errors
In-Reply-To: <E4714BBD6A60E94F9D12E119D63E161206CA7428@exchange01.nctr.fda.gov>
References: <E4714BBD6A60E94F9D12E119D63E161206CA7428@exchange01.nctr.fda.gov>
Message-ID: <efb536d50608290835v207ccd1cr60b8185dddf57b81@mail.gmail.com>

Hi,

You don't say, but the errors make me think you are using Linux.
(Please specify your OS when you have problems). It sounds like
you are missing some of the "Essential programs under Unix" -
they really are essential.

I think you need to reread the R admin manual, as instructed
on the bioconductor download page
http://cran.r-project.org/doc/manuals/R-admin.html
and in particular this part:
http://cran.r-project.org/doc/manuals/R-admin.html#Essential-and-useful-other-programs-under-Unix

Sarah

On 8/29/06, Ge, Weigong* <weigong.ge at fda.hhs.gov> wrote:
> Hello,
>
> I follow the Bioconductor instruction (http://www.bioconductor.org/download
> <http://www.bioconductor.org/download> ) to install Biocoductor, there have
> some errors:
>
> /usr/lib/R/bin/SHLIB: line 115: make: command not found



-- 
Sarah Goslee
http://www.stringpage.com


From mnair at iusb.edu  Tue Aug 29 16:51:54 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Tue, 29 Aug 2006 10:51:54 -0400
Subject: [R] spectral clustering
Message-ID: <A32055BDEA88C34BB3DBBCD229380778630D74@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/e5a3b219/attachment.pl 

From rfrancois at mango-solutions.com  Tue Aug 29 18:02:12 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Tue, 29 Aug 2006 17:02:12 +0100
Subject: [R] spectral clustering
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778630D74@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778630D74@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <44F46504.1020201@mango-solutions.com>

Nair, Murlidharan T a ?crit :
> Is there a function in R  that does spectral clustering?  
>
> Thanks ../Murli
>   

> ** PLEASE do read the posting guide http://www.R-project.org/posting-guide.html **
Hi,

You would get a faster answer asking to RSiteSearch rather than this list.

Romain


From bbands at gmail.com  Tue Aug 29 18:08:19 2006
From: bbands at gmail.com (BBands)
Date: Tue, 29 Aug 2006 09:08:19 -0700
Subject: [R] passing namees
Message-ID: <6e8360ad0608290908l4b1394cg1df2693a73e021ef@mail.gmail.com>

R 2.3.1

I wrote a little script to do some cross correlations. The symbols are
in a text file like so:

symbols.txt
ibm
dd
csco

"""
require(tseries)
symbols <- scan("symbols.txt", what = 'character')

for(line in 1:(length(symbols)-1)) {
   assign(symbols[line], get.hist.quote(instrument = symbols[line],
   start = "2005-09-01", quote = "Close"))
   }
# this results in objects ibm, dd... with the last symbol skipped

mat <- cbind(symbols) # this is the problem
(cor_mat <- cor(mat))
symnum(cor_mat)
"""

How can I pass a list of the objects to cbind()? As written cbind gets
only the names of the objects and binds the names not the objects.

Thanks in advance,

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From szhan at uoguelph.ca  Tue Aug 29 18:09:33 2006
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Tue, 29 Aug 2006 12:09:33 -0400
Subject: [R] how to contrast with factorial experiment
Message-ID: <20060829120933.42n9e0ixsk8wk8wg@webmail.uoguelph.ca>

Hello, R experts,
If I understand Ted's anwser correctly, then I can not contrast the  
mean yields between sections 1-8 and 9-11 under "Trt" but I can  
contrast mean yields for sections 1-3 and 6-11 because there exists  
significant interaction between two factors  (Trt:section4,  
Trt:section5). Could I use the commands below to test
the difference between sections 1-3 and 6-11 ?
> contrasts(section)<-c(-2,-2,-2,0,0,1,1,1,1,1,1)
> newobj<-lm(log2(yield)~treat*section)
How can I infer that there is significant difference between sections
1-3 and sections 6-11 for the "Trt" from the output below?

> summary(newobj)

Call:
lm(formula = log2(yield) ~ treat * section)

Residuals:
       Min       1Q   Median       3Q      Max
-0.49647 -0.14913 -0.01521  0.17471  0.51105

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)
(Intercept)         6.28840    0.05003 125.682  < 2e-16 ***
treatTrt            1.22122    0.07076  17.259  < 2e-16 ***
section1            0.17831    0.03911   4.559 4.08e-05 ***
section2           -0.23102    0.16595  -1.392  0.17087
section3            2.38170    0.16595  14.352  < 2e-16 ***
section4            3.36834    0.16595  20.298  < 2e-16 ***
section5           -1.56873    0.16595  -9.453 3.67e-12 ***
section6           -0.41522    0.16595  -2.502  0.01613 *
section7           -0.89943    0.16595  -5.420 2.38e-06 ***
section8            0.09522    0.16595   0.574  0.56901
section9           -0.78784    0.16595  -4.748 2.21e-05 ***
section10           0.74821    0.16595   4.509 4.79e-05 ***
treatTrt:section1   0.10101    0.05532   1.826  0.07461 .
treatTrt:section2   0.27270    0.23468   1.162  0.25151
treatTrt:section3  -1.22210    0.23468  -5.207 4.85e-06 ***
treatTrt:section4  -1.39187    0.23468  -5.931 4.26e-07 ***
treatTrt:section5  -0.76137    0.23468  -3.244  0.00225 **
treatTrt:section6   0.07320    0.23468   0.312  0.75658
treatTrt:section7   0.33108    0.23468   1.411  0.16535
treatTrt:section8  -0.13686    0.23468  -0.583  0.56276
treatTrt:section9   0.22086    0.23468   0.941  0.35180
treatTrt:section10 -0.14476    0.23468  -0.617  0.54054
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.2874 on 44 degrees of freedom
Multiple R-Squared: 0.973,      Adjusted R-squared: 0.9601
F-statistic: 75.55 on 21 and 44 DF,  p-value: < 2.2e-16

Joshua
Quoting "(Ted Harding)" <Ted.Harding at nessie.mcc.ac.uk>:

> On 24-Aug-06 szhan at uoguelph.ca wrote:
>> Hello, R users,
>> I have two factors (treat, section) anova design experiment where
>> there are 3 replicates. The objective of the experiment is to test if
>> there is significant difference of yield between top (section 9 to 11)
>> and bottom (section 9 to 11)
> [I think you mean sections 1 to 8]
>
>> of the fruit tree under treatment. I found that there are interaction
>> between two factors. I wonder if I can contrast means from levels of
>> one factor (section) under another factor (treat)? if so, how to do
>> it in R and how to interpret the output?
>
> I think you would be well advised to look at a plot of the data.
> For example, let Y stand for yield, R for replicate, T for treat
> and S for section.
>
>   ix<-(T=="Trt");plot(S[ix],Y[ix],col="red",ylim=c(0,1000))
>   ix<-(T=="Ctl");points(S[ix],Y[ix],col="blue")
>
>> From this it is clear that sections 4 and 5 are in a class of
> their own. Also, in sections 1-3 and 6-11 the "Ctl" yields
> are not only lower, but have smaller (in some cases hardly any)
> variance, compared with the "Trt" yields. The variances for
> sections 7,8,9,10,11 are greater than for 1,2,3,6 without
> great change in mean value.
>
> While there is an evident difference between "Trt" yields and
> "Ctrl" yields for sections 1-3 and 6-11, this is not so for
> sections 4 and 5.
>
> This sort of behaviour no doubt provides some reasons for the
> interaction you observed. You seem to have a quite complex
> phenomenon here!
>
> To some extent the problems with variance can be diminished by
> working with logarithms. Compare the previous plot with
>
>   ix<-(T=="Trt");plot(S[ix],log10(Y[ix]),col="red",ylim=c(0,3))
>   ix<-(T=="Ctl");points(S[ix],log10(Y[ix]),col="blue")
>
> (you have used log2() in your commands). The above observations
> can be seen reflected in R if you look at the output of
>
>   summary(obj)
>
> where in particular:
>
> treatTrt:section2  -1.11691    0.33189  -3.365 0.001595 **
> treatTrt:section3  -0.45634    0.33189  -1.375 0.176099
> treatTrt:section4  -1.56627    0.33189  -4.719 2.42e-05 ***
> treatTrt:section5  -1.73604    0.33189  -5.231 4.48e-06 ***
> treatTrt:section6  -0.91311    0.33189  -2.751 0.008588 **
> treatTrt:section7  -0.07853    0.33189  -0.237 0.814055
> treatTrt:section8   0.17935    0.33189   0.540 0.591654
> treatTrt:section9  -0.28859    0.33189  -0.870 0.389277
> treatTrt:section10  0.06913    0.33189   0.208 0.835972
> treatTrt:section11 -0.29649    0.33189  -0.893 0.376543
>
> which, precisely, "contrasts means from levels of one factor
> (section) under another factor (treat)", and shows that most
> of the "interaction" arises in sections 4 and 5.
>
> Since sections 4 and 5 (in the middle of sections 1 to 8) are
> so exceptional, they will have strong influence on your comparison
> between sections 1-8 and sections 9-11. You need to think about
> what to do with sections 4 and 5!
>
>> Here is the data and commands I used to test the differece between
>> section 1 to 8 and 9 to 11 under treatment. But I don't know if I was
>> right, how to interpret the out and whether there are significant
>> difference between section 1 to 8 and section 9 to 11 under treatment.
>>
>> yield replicate       treat   section
>> 35.55 1       Ctl     1
>> 53.70 1       Ctl     2
>> 42.79 1       Ctl     3
>> 434.81        1       Ctl     4
>> 705.96        1       Ctl     5
>> 25.91 1       Ctl     6
>> 57.53 1       Ctl     7
>> 41.45 1       Ctl     8
>> 85.54 1       Ctl     9
>> 51.23 1       Ctl     10
>> 188.24        1       Ctl     11
>> 35.71 2       Ctl     1
>> 45.15 2       Ctl     2
>> 40.10 2       Ctl     3
>> 312.76        2       Ctl     4
>> 804.05        2       Ctl     5
>> 28.22 2       Ctl     6
>> 68.51 2       Ctl     7
>> 46.15 2       Ctl     8
>> 123.14        2       Ctl     9
>> 33.78 2       Ctl     10
>> 121.28        2       Ctl     11
>> 30.96 3       Ctl     1
>> 36.10 3       Ctl     2
>> 47.19 3       Ctl     3
>> 345.80        3       Ctl     4
>> 644.61        3       Ctl     5
>> 27.73 3       Ctl     6
>> 56.63 3       Ctl     7
>> 42.63 3       Ctl     8
>> 61.25 3       Ctl     9
>> 59.43 3       Ctl     10
>> 109.87        3       Ctl     11
>> 143.50        1       Trt     1
>> 82.76 1       Trt     2
>> 125.03        1       Trt     3
>> 493.76        1       Trt     4
>> 868.48        1       Trt     5
>> 45.09 1       Trt     6
>> 249.43        1       Trt     7
>> 167.28        1       Trt     8
>> 274.72        1       Trt     9
>> 176.40        1       Trt     10
>> 393.10        1       Trt     11
>> 93.75 2       Trt     1
>> 63.83 2       Trt     2
>> 117.50        2       Trt     3
>> 362.68        2       Trt     4
>> 659.40        2       Trt     5
>> 62.10 2       Trt     6
>> 218.24        2       Trt     7
>> 210.98        2       Trt     8
>> 291.48        2       Trt     9
>> 209.36        2       Trt     10
>> 454.68        2       Trt     11
>> 119.62        3       Trt     1
>> 66.50 3       Trt     2
>> 87.37 3       Trt     3
>> 414.01        3       Trt     4
>> 707.70        3       Trt     5
>> 44.40 3       Trt     6
>> 142.59        3       Trt     7
>> 137.37        3       Trt     8
>> 181.03        3       Trt     9
>> 131.65        3       Trt     10
>> 310.18        3       Trt     11
>>
>>> dat1<-read.delim("c:/testcontr.txt", header=T)
>>> dat1$treat<-as.factor(dat1$treat)
>>> dat1$replicate<-as.factor(dat1$replicate)
>>> dat1$section<-as.factor(dat1$section)
>>> attach(dat1)
>>> obj<-lm(log2(yield)~treat*section)
>>> anova(obj)
>> Analysis of Variance Table
>>
>> Response: log2(yield)
>>                Df Sum Sq Mean Sq  F value    Pr(>F)
>> treat          1 24.608  24.608 297.8649 < 2.2e-16 ***
>> section       10 99.761   9.976 120.7565 < 2.2e-16 ***
>> treat:section 10  6.708   0.671   8.1197 2.972e-07 ***
>> Residuals     44  3.635   0.083
>> ---
>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>
>>> contrasts(section)<-c(3,3,3,3,3,3,3,3,-8,-8,-8)
>>> objnew<-lm(log2(yield)~treat*section)
>>> summary(objnew)
>>
>> Call:
>> lm(formula = log2(yield) ~ treat * section)
>>
>> Residuals:
>>       Min       1Q   Median       3Q      Max
>> -0.49647 -0.14913 -0.01521  0.17471  0.51105
>>
>> Coefficients:
>>                      Estimate Std. Error t value Pr(>|t|)
>> (Intercept)         6.288403   0.050034 125.682  < 2e-16 ***
>> treatTrt            1.221219   0.070759  17.259  < 2e-16 ***
>> section1           -0.008502   0.010213  -0.832 0.409675
>> section2           -0.491175   0.165945  -2.960 0.004942 **
>> section3            2.569427   0.165945  15.484  < 2e-16 ***
>> section4            3.556067   0.165945  21.429  < 2e-16 ***
>> section5           -1.157069   0.165945  -6.973 1.25e-08 ***
>> section6           -0.003562   0.165945  -0.021 0.982971
>> section7           -0.487770   0.165945  -2.939 0.005223 **
>> section8            0.106181   0.165945   0.640 0.525585
>> section9           -0.776882   0.165945  -4.682 2.74e-05 ***
>> section10           0.759168   0.165945   4.575 3.87e-05 ***
>> treatTrt:section1  -0.049000   0.014444  -3.392 0.001474 **
>> treatTrt:section2   0.160825   0.234682   0.685 0.496757
>> treatTrt:section3  -0.949101   0.234682  -4.044 0.000208 ***
>> treatTrt:section4  -1.118870   0.234682  -4.768 2.07e-05 ***
>> treatTrt:section5  -0.295937   0.234682  -1.261 0.213950
>> treatTrt:section6   0.538638   0.234682   2.295 0.026549 *
>> treatTrt:section7   0.796518   0.234682   3.394 0.001468 **
>> treatTrt:section8  -0.548744   0.234682  -2.338 0.023984 *
>> treatTrt:section9  -0.191029   0.234682  -0.814 0.420033
>> treatTrt:section10 -0.556642   0.234682  -2.372 0.022137 *
>> ---
>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>
>> Residual standard error: 0.2874 on 44 degrees of freedom
>> Multiple R-Squared: 0.973,      Adjusted R-squared: 0.9601
>> F-statistic: 75.55 on 21 and 44 DF,  p-value: < 2.2e-16
>>
>> Thanks,
>> Joshua
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 24-Aug-06                                       Time: 21:43:57
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From adrian at stats.gla.ac.uk  Tue Aug 29 17:36:46 2006
From: adrian at stats.gla.ac.uk (Adrian Bowman)
Date: Tue, 29 Aug 2006 16:36:46 +0100
Subject: [R] [R-pkgs] The rpanel package
Message-ID: <44F45F0E.4010802@stats.gla.ac.uk>

The rpanel package builds on the tcltk package to provide simple 
interactive controls for R functions, in particular to provide simple 
forms of dynamic graphics.  The intention is to make this form of 
control particularly easy for R users to implement, with full 
documentation.  The necessary tcltk variables are managed behind the 
scenes so that users need not be concerned with any technicalities.

A descriptive paper and some simple example scripts are available at
	www.stats.gla.ac.uk/~adrian/rpanel/
The package is now available from CRAN.

Best wishes,

Adrian Bowman
(on behalf of the authors: Bowman, Crawford, Alexander & Bowman)

_______________________________________________________________________
  Prof. Adrian Bowman           Tel: +44-141-330-4046
  Dept. of Statistics           Fax: +44-141-330-4814
  The University of Glasgow     E-mail:adrian at stats.gla.ac.uk
  Glasgow G12 8QQ, U.K.         Web: www.stats.gla.ac.uk/~adrian

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ggrothendieck at gmail.com  Tue Aug 29 18:24:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 12:24:54 -0400
Subject: [R] passing namees
In-Reply-To: <6e8360ad0608290908l4b1394cg1df2693a73e021ef@mail.gmail.com>
References: <6e8360ad0608290908l4b1394cg1df2693a73e021ef@mail.gmail.com>
Message-ID: <971536df0608290924g723f6af5h2b45fa05e6666a8d@mail.gmail.com>

Put the data into a list, not into individual variables.  Something
like this (untested):

L <- sapply(symbols[-length(symbols)],
	get.hist.quote, start = "2005-01-09", quote = "Close", simplify = FALSE)
mat <- do.call(cbind, L)

cor.mat <- cor(mat, use = "complete")
symnum(cor.mat)


On 8/29/06, BBands <bbands at gmail.com> wrote:
> R 2.3.1
>
> I wrote a little script to do some cross correlations. The symbols are
> in a text file like so:
>
> symbols.txt
> ibm
> dd
> csco
>
> """
> require(tseries)
> symbols <- scan("symbols.txt", what = 'character')
>
> for(line in 1:(length(symbols)-1)) {
>   assign(symbols[line], get.hist.quote(instrument = symbols[line],
>   start = "2005-09-01", quote = "Close"))
>   }
> # this results in objects ibm, dd... with the last symbol skipped
>
> mat <- cbind(symbols) # this is the problem
> (cor_mat <- cor(mat))
> symnum(cor_mat)
> """
>
> How can I pass a list of the objects to cbind()? As written cbind gets
> only the names of the objects and binds the names not the objects.
>
> Thanks in advance,
>
>     jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From l.nanetti at med.umcg.nl  Tue Aug 29 18:59:57 2006
From: l.nanetti at med.umcg.nl (Luca Nanetti)
Date: Tue, 29 Aug 2006 18:59:57 +0200
Subject: [R] Write signed short into a binary file (follow up and
	conclusion)(for real)
Message-ID: <002001c6cb8c$8e7b8f00$b90aa8c0@ka240301843>

Somebody pointed me to the right spot in the documentation, and kindly and 
humorously
'suggested' to me to do the following:
My Apologies, everybody!
I should have read docs a bit better, actually...
...will do better next time!
luca


----- Original Message ----- 
From: <L.Nanetti at med.umcg.nl>
To: <>
Sent: Monday, August 28, 2006 4:52 PM
Subject: Write signed short into a binary file (follow up and conclusion)


I've solved my problem using:
(purpose: write the signed integer -19 as a two byte integer into a binary 
file)

writeBin(as.integer(-19),myconnection, size=2)

There is no need to coerce.

Thanks anyway!

Luca NanettiUniversity Medical Center Groningen
BCN-NeuroImagingCenter
A.Deusinglaan 2 9713AW Groningen
The Nethterlands
l dot nanetti at med dot rug dot nl


From zzhang16 at gmail.com  Tue Aug 29 20:55:43 2006
From: zzhang16 at gmail.com (zhe zhang)
Date: Tue, 29 Aug 2006 14:55:43 -0400
Subject: [R] forestplot fucntion in rmeta package
Message-ID: <6c583e880608291155s4e847962tb6af26da92041ea6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/672d59a8/attachment.pl 

From dsohal at gmail.com  Tue Aug 29 21:40:01 2006
From: dsohal at gmail.com (Davendra Sohal)
Date: Tue, 29 Aug 2006 15:40:01 -0400
Subject: [R] Dendrogram troubles
Message-ID: <c2f237040608291240y5dd32d36le699d9f536508075@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/bd8dacad/attachment.pl 

From kingroi at hotmail.com  Tue Aug 29 21:42:25 2006
From: kingroi at hotmail.com (paul king)
Date: Tue, 29 Aug 2006 19:42:25 +0000
Subject: [R] Key() and par(mfrow)
Message-ID: <BAY107-W37DBC69AB582FB95DABBEB0390@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/c3e35a4c/attachment.pl 

From Soren.Hojsgaard at agrsci.dk  Tue Aug 29 21:59:44 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 29 Aug 2006 21:59:44 +0200
Subject: [R] lattice/xyplot: plotting 4 variables in two panels - can this
	be done?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC038783D1@DJFPOST01.djf.agrsci.dk>

Hi,
I would like to create a plot of y1,y2,y3,y4 against x for several subjects such that y1 and y2 are plotted against x in one panel and y3 and y4 against x in another panel. Thus if there are 3 subjects I should end up with 6 panels. Is there a simple way of doing so (i.e. without calling xyplot() several times, and then padding the results together)?? 
Regards
S?ren


From dsonneborn at ucdavis.edu  Tue Aug 29 22:05:40 2006
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Tue, 29 Aug 2006 13:05:40 -0700
Subject: [R] subset by two variables
Message-ID: <44F49E14.4070704@yellow.ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060829/702d2299/attachment.pl 

From laurentRhelp at free.fr  Tue Aug 29 22:18:07 2006
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Tue, 29 Aug 2006 22:18:07 +0200
Subject: [R] lattice and several groups
Message-ID: <44F4A0FF.2010708@free.fr>

Dear R-list,

     I would like to use the lattice library to show several groups on 
the same graph. Here's my example :

## the data
f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
f1 <- rep(f1,3)
f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
#############################################################
library(lattice)

para.liste <- trellis.par.get()
superpose.symbol <- para.liste$superpose.symbol
superpose.symbol$pch <- c(1,2,3)
trellis.par.set("superpose.symbol",superpose.symbol)

# Now I can see the group according to the f1 factor (with a different 
symbol for every modality)
xyplot( val~x,   
        data=df,
        group=f1,
        auto.key=list(space="right")
       )

# or I can see the group according to the f2 factor
xyplot( val~x,   
        data=df,
        type="l",
        group=f2,
        auto.key=list(space="right",points=FALSE,lines=TRUE)
       )

How can I do to highlight both the f1 and f2 factors on one panel with 
the legends, using the lattice function ?

Thanks


From mschwartz at mn.rr.com  Tue Aug 29 22:20:25 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 29 Aug 2006 15:20:25 -0500
Subject: [R] subset by two variables
In-Reply-To: <44F49E14.4070704@yellow.ucdavis.edu>
References: <44F49E14.4070704@yellow.ucdavis.edu>
Message-ID: <1156882825.2818.22.camel@localhost.localdomain>

On Tue, 2006-08-29 at 13:05 -0700, Dean Sonneborn wrote:
> I'm using this syntax to create data subsets for plots:  
> subset=source=="Both". Now I would like to create a subset defined by 
> two different variables like this: subset=source=="Both"  
> subset=site=="home" but this syntax is not correct. The documents in the 
> manual for subset seem to be creating whole new data files not just 
> selecting rows based on the contents of the variables. How do I subset 
> based on two variables.

If your data frame is called 'DF':

  plot(y ~ x, data = DF, subset = (source == "Both") & (site == "home"))

See ?Logic and ?Syntax for more information on creating logical
expressions.

HTH,

Marc Schwartz


From gregor.gorjanc at bfro.uni-lj.si  Tue Aug 29 23:03:27 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 29 Aug 2006 23:03:27 +0200
Subject: [R] Producing R demos
Message-ID: <44F4AB9F.6070105@bfro.uni-lj.si>

Hello!

I have found terrific demo or R package functionality at

http://had.co.nz/reshape/french-fries-demo.html

Author has told me off-list that he is using SnapzProX (on mac), and
ghostwriter (http://had.co.nz/ghostwriter/) to automate the typing.
Unfortunatelly, I do not have mac ;) Can anyone on the list suggest, how
such a demo (video + automated typing of a script) could be produced
under either Windows or Linux OS? I would like to create such a demo for
presentation as I will not be able to install R on the machine, but I
could play a movie.

Thanks!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From jonathan.gheyssens at hec.ca  Tue Aug 29 23:35:44 2006
From: jonathan.gheyssens at hec.ca (jonathan gheyssens aux HEC)
Date: Tue, 29 Aug 2006 17:35:44 -0400
Subject: [R] =?iso-8859-1?q?my_email=3A_jonathan_gheyssens_=28universit=E9?=
 =?iso-8859-1?q?_de_Montreal=29?=
Message-ID: <7de85cf8db7f0a4ba0f4c4091be838da@hec.ca>


Jonathan Gheyssens
M.Sc Economie Appliqu?e
IEA - HEC Montr?al
jonathan.gheyssens at hec.ca
514-581-8009
514-678-5231


From h.wickham at gmail.com  Tue Aug 29 23:47:37 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 29 Aug 2006 16:47:37 -0500
Subject: [R] lattice and several groups
In-Reply-To: <44F4A0FF.2010708@free.fr>
References: <44F4A0FF.2010708@free.fr>
Message-ID: <f8e6ff050608291447s771ed4aana347c05f3650e960@mail.gmail.com>

>      I would like to use the lattice library to show several groups on
> the same graph. Here's my example :
>
> ## the data
> f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> f1 <- rep(f1,3)
> f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)

It's pretty easy to do this with ggplot:

install.packages("ggplot", repos="http://ggobi.org/r/")
library(ggplot)
qplot(x, val, data=df, shape=f2, colour=f1)

Hadley


From hodgess at gator.dt.uh.edu  Wed Aug 30 00:25:26 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 29 Aug 2006 17:25:26 -0500
Subject: [R] Substring and strsplit
Message-ID: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>

Dear R People:

I am trying to split a character vector into a set of individual
letters:

Ideal:
x3 <- c("dog")
"d" "o" "g"

I tried the following:
> strsplit(x3)
Error in strsplit(x3) : argument "split" is missing, with no default
> strsplit(x3,1)
[[1]]
[1] "dog"

I know that this is incredibly simple, but what am I doing wrong?

Either Windows or Linux 2.3.1

Thanks in advance!


Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From jholtman at gmail.com  Wed Aug 30 00:29:51 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 29 Aug 2006 18:29:51 -0400
Subject: [R] Substring and strsplit
In-Reply-To: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
References: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
Message-ID: <644e1f320608291529g4359b648r3776fea2f4cd1214@mail.gmail.com>

Use '' as parameter to strsplit

> x3 <- 'dog'
> strsplit(x3, '')
[[1]]
[1] "d" "o" "g"

>


On 8/29/06, Erin Hodgess <hodgess at gator.dt.uh.edu> wrote:
> Dear R People:
>
> I am trying to split a character vector into a set of individual
> letters:
>
> Ideal:
> x3 <- c("dog")
> "d" "o" "g"
>
> I tried the following:
> > strsplit(x3)
> Error in strsplit(x3) : argument "split" is missing, with no default
> > strsplit(x3,1)
> [[1]]
> [1] "dog"
>
> I know that this is incredibly simple, but what am I doing wrong?
>
> Either Windows or Linux 2.3.1
>
> Thanks in advance!
>
>
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From tlumley at u.washington.edu  Wed Aug 30 00:29:50 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 29 Aug 2006 15:29:50 -0700 (PDT)
Subject: [R] Substring and strsplit
In-Reply-To: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
References: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.64.0608291528510.20094@homer23.u.washington.edu>

On Tue, 29 Aug 2006, Erin Hodgess wrote:

> Dear R People:
>
> I am trying to split a character vector into a set of individual
> letters:
>
> Ideal:
> x3 <- c("dog")
> "d" "o" "g"
>
> I tried the following:
>> strsplit(x3)
> Error in strsplit(x3) : argument "split" is missing, with no default
>> strsplit(x3,1)
> [[1]]
> [1] "dog"
>
> I know that this is incredibly simple, but what am I doing wrong?
>

This is the first example on the help page for strsplit.

 	-thomas


From deepayan.sarkar at gmail.com  Wed Aug 30 00:54:30 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 29 Aug 2006 15:54:30 -0700
Subject: [R] lattice/xyplot: plotting 4 variables in two panels - can
	this be done?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC038783D1@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038783D1@DJFPOST01.djf.agrsci.dk>
Message-ID: <eb555e660608291554y42a50e01rc7bde68a7fa3fe51@mail.gmail.com>

On 8/29/06, S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Hi,
> I would like to create a plot of y1,y2,y3,y4 against x for several subjects such that y1 and y2 are plotted against x in one panel and y3 and y4 against x in another panel. Thus if there are 3 subjects I should end up with 6 panels. Is there a simple way of doing so (i.e. without calling xyplot() several times, and then padding the results together)??


You need to transform your data into a suitable form. See ?reshape and
?make.groups.

Deepayan


From cmbarker at ucdavis.edu  Wed Aug 30 02:27:22 2006
From: cmbarker at ucdavis.edu (Chris Barker)
Date: Tue, 29 Aug 2006 17:27:22 -0700
Subject: [R] Handling realisations in geoRglm
Message-ID: <44F4DB6A.60400@ucdavis.edu>

Dear R users:

I want to model mosquito count data based on landcover attributes and 
meteorological variables using a Poisson GLSM in the geoRglm package.  I 
have monthly mosquito counts over more than 20 years with repeated 
observations from individual trap sites over time.  I have used 
as.geodata() to successfully read my dataset into the geodata format 
utilized by geoR and geoRglm, including specification of the 
realisations.  (I know the realisations have been specified correctly 
because I can get data summaries by realisation from the geodata 
object.)  However, after reading the packages' documentation and 
searching the mailing lists and other sources, it seems that the 
functions in geoRglm do not acknowledge the existence of multiple 
realisations per site.  I see that the likfit() function in geoR has an 
argument for realisations, but I cannot find anything similar in 
geoRglm.  Is it possible to model data with repeated realisations from 
the same sites using geoRglm?  If so, how can this be done and is there 
a way to model dependence among the realisations over time?

Thank you,
Chris Barker
Ph.D. Candidate
University of California, Davis
cmbarker at ucdavis.edu

From anu1978 at hotmail.com  Wed Aug 30 02:24:54 2006
From: anu1978 at hotmail.com (Anusha)
Date: Tue, 29 Aug 2006 20:24:54 -0400
Subject: [R] Help on apply() function
Message-ID: <BAY105-DAV1AE5A6CB01759A250C078B23E0@phx.gbl>

Respected Sir/Madam,

I have a problem with apply function. I have to two matrices of dimension of
one column but n rows. I have to check whether one matrix is greater than
other by going thru each row (ie) using if condition to check one matrix
with another matrix. 

I like to use apply() function to this approach. That is apply function
between two matrices. I searched for examples online but I couldn't find
any.

I don't know how to loop thru the matrices.

Please help with this problem.

Any help is appreciated.

My mailid is monkponu at yahoo.com.

Thanks,
Anusha.


From ggrothendieck at gmail.com  Wed Aug 30 02:28:11 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 20:28:11 -0400
Subject: [R] lattice and several groups
In-Reply-To: <44F4A0FF.2010708@free.fr>
References: <44F4A0FF.2010708@free.fr>
Message-ID: <971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>

Try this:

xyplot(val ~ x, data = df, type = "p",
	col = as.numeric(df$f1), pch = as.numeric(df$f2))

key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
	points = list(pch = 1:nlevels(df$f1))
)

key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
	points = list(pch = 20, col = 1:nlevels(df$f2))
)

trellis.focus("panel", 1, 1)
draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
trellis.unfocus()


On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> Dear R-list,
>
>     I would like to use the lattice library to show several groups on
> the same graph. Here's my example :
>
> ## the data
> f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> f1 <- rep(f1,3)
> f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
> #############################################################
> library(lattice)
>
> para.liste <- trellis.par.get()
> superpose.symbol <- para.liste$superpose.symbol
> superpose.symbol$pch <- c(1,2,3)
> trellis.par.set("superpose.symbol",superpose.symbol)
>
> # Now I can see the group according to the f1 factor (with a different
> symbol for every modality)
> xyplot( val~x,
>        data=df,
>        group=f1,
>        auto.key=list(space="right")
>       )
>
> # or I can see the group according to the f2 factor
> xyplot( val~x,
>        data=df,
>        type="l",
>        group=f2,
>        auto.key=list(space="right",points=FALSE,lines=TRUE)
>       )
>
> How can I do to highlight both the f1 and f2 factors on one panel with
> the legends, using the lattice function ?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Aug 30 02:36:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 20:36:47 -0400
Subject: [R] Producing R demos
In-Reply-To: <44F4AB9F.6070105@bfro.uni-lj.si>
References: <44F4AB9F.6070105@bfro.uni-lj.si>
Message-ID: <971536df0608291736n53d99536j87fd401e6746528a@mail.gmail.com>

I have not used this but wink is a free demo-maker for Windows
available at:
   http://www.debugmode.com/wink/
Click the red on green arrow on that page to a (very short) demo.

On 8/29/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Hello!
>
> I have found terrific demo or R package functionality at
>
> http://had.co.nz/reshape/french-fries-demo.html
>
> Author has told me off-list that he is using SnapzProX (on mac), and
> ghostwriter (http://had.co.nz/ghostwriter/) to automate the typing.
> Unfortunatelly, I do not have mac ;) Can anyone on the list suggest, how
> such a demo (video + automated typing of a script) could be produced
> under either Windows or Linux OS? I would like to create such a demo for
> presentation as I will not be able to install R on the machine, but I
> could play a movie.
>
> Thanks!
>
> --
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gregor.gorjanc at bfro.uni-lj.si  Wed Aug 30 02:44:16 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Wed, 30 Aug 2006 02:44:16 +0200
Subject: [R] Producing R demos
In-Reply-To: <971536df0608291736n53d99536j87fd401e6746528a@mail.gmail.com>
References: <44F4AB9F.6070105@bfro.uni-lj.si>
	<971536df0608291736n53d99536j87fd401e6746528a@mail.gmail.com>
Message-ID: <44F4DF60.50005@bfro.uni-lj.si>

Gabor Grothendieck wrote:
> I have not used this but wink is a free demo-maker for Windows
> available at:
>   http://www.debugmode.com/wink/
> Click the red on green arrow on that page to a (very short) demo.

Thanks! Do you perhaps also know equivalent of "ghostwriter" (link is
bellow) for windows/linux?

> On 8/29/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
>> Hello!
>>
>> I have found terrific demo or R package functionality at
>>
>> http://had.co.nz/reshape/french-fries-demo.html
>>
>> Author has told me off-list that he is using SnapzProX (on mac), and
>> ghostwriter (http://had.co.nz/ghostwriter/) to automate the typing.
>> Unfortunatelly, I do not have mac ;) Can anyone on the list suggest, how
>> such a demo (video + automated typing of a script) could be produced
>> under either Windows or Linux OS? I would like to create such a demo for
>> presentation as I will not be able to install R on the machine, but I
>> could play a movie.
>>
>> Thanks!


-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From ggrothendieck at gmail.com  Wed Aug 30 02:49:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 20:49:46 -0400
Subject: [R] Producing R demos
In-Reply-To: <44F4DF60.50005@bfro.uni-lj.si>
References: <44F4AB9F.6070105@bfro.uni-lj.si>
	<971536df0608291736n53d99536j87fd401e6746528a@mail.gmail.com>
	<44F4DF60.50005@bfro.uni-lj.si>
Message-ID: <971536df0608291749s52c80cecicb29c172957df7a4@mail.gmail.com>

Don't know.  I would check whether the functionality is included in wink.

On 8/29/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Gabor Grothendieck wrote:
> > I have not used this but wink is a free demo-maker for Windows
> > available at:
> >   http://www.debugmode.com/wink/
> > Click the red on green arrow on that page to a (very short) demo.
>
> Thanks! Do you perhaps also know equivalent of "ghostwriter" (link is
> bellow) for windows/linux?
>
> > On 8/29/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> >> Hello!
> >>
> >> I have found terrific demo or R package functionality at
> >>
> >> http://had.co.nz/reshape/french-fries-demo.html
> >>
> >> Author has told me off-list that he is using SnapzProX (on mac), and
> >> ghostwriter (http://had.co.nz/ghostwriter/) to automate the typing.
> >> Unfortunatelly, I do not have mac ;) Can anyone on the list suggest, how
> >> such a demo (video + automated typing of a script) could be produced
> >> under either Windows or Linux OS? I would like to create such a demo for
> >> presentation as I will not be able to install R on the machine, but I
> >> could play a movie.
> >>
> >> Thanks!
>
>
> --
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> ----------------------------------------------------------------------
>


From ggrothendieck at gmail.com  Wed Aug 30 02:56:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 20:56:33 -0400
Subject: [R] lattice/xyplot: plotting 4 variables in two panels - can
	this be done?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC038783D1@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038783D1@DJFPOST01.djf.agrsci.dk>
Message-ID: <971536df0608291756m2b936f6an395308d6428ff1d1@mail.gmail.com>

Try xyplot.zoo in the zoo package (be sure you have
the latest zoo version). One caveat is that xyplot.zoo is
under development and could change.  Here we use the
builtin data set anscombe as an example:

# display the anscombe data set
anscombe

library(lattice)
library(zoo)
# create zoo object using column 1 as x/times and columns 5:8 as the y's
z <- zoo(data.matrix(anscombe[,5:8]), anscombe[,1])
xyplot(z, screens = c(1, 1, 2, 2), col = 1:4, pch = 1:4, type = "b")


On 8/29/06, S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Hi,
> I would like to create a plot of y1,y2,y3,y4 against x for several subjects such that y1 and y2 are plotted against x in one panel and y3 and y4 against x in another panel. Thus if there are 3 subjects I should end up with 6 panels. Is there a simple way of doing so (i.e. without calling xyplot() several times, and then padding the results together)??
> Regards
> S?ren
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Aug 30 03:01:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 29 Aug 2006 21:01:15 -0400
Subject: [R] lattice and several groups
In-Reply-To: <971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
Message-ID: <971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>

Note that before entering this you need:

library(lattice)
library(grid) # to access the viewport function

On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> xyplot(val ~ x, data = df, type = "p",
>        col = as.numeric(df$f1), pch = as.numeric(df$f2))
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>        points = list(pch = 1:nlevels(df$f1))
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>        points = list(pch = 20, col = 1:nlevels(df$f2))
> )
>
> trellis.focus("panel", 1, 1)
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> trellis.unfocus()
>
>
> On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> > Dear R-list,
> >
> >     I would like to use the lattice library to show several groups on
> > the same graph. Here's my example :
> >
> > ## the data
> > f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> > f1 <- rep(f1,3)
> > f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> > df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
> > #############################################################
> > library(lattice)
> >
> > para.liste <- trellis.par.get()
> > superpose.symbol <- para.liste$superpose.symbol
> > superpose.symbol$pch <- c(1,2,3)
> > trellis.par.set("superpose.symbol",superpose.symbol)
> >
> > # Now I can see the group according to the f1 factor (with a different
> > symbol for every modality)
> > xyplot( val~x,
> >        data=df,
> >        group=f1,
> >        auto.key=list(space="right")
> >       )
> >
> > # or I can see the group according to the f2 factor
> > xyplot( val~x,
> >        data=df,
> >        type="l",
> >        group=f2,
> >        auto.key=list(space="right",points=FALSE,lines=TRUE)
> >       )
> >
> > How can I do to highlight both the f1 and f2 factors on one panel with
> > the legends, using the lattice function ?
> >
> > Thanks
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From edd at debian.org  Wed Aug 30 03:01:39 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 29 Aug 2006 20:01:39 -0500
Subject: [R] C compile problem on Ubuntu linux
In-Reply-To: <44E8C60E.9060602@vanderbilt.edu>
References: <44E8AC6A.1040605@vanderbilt.edu> <44E8B8F6.7060401@unileon.es>
	<44E8C60E.9060602@vanderbilt.edu>
Message-ID: <17652.58227.623096.796550@basebud.nulle.part>


On 20 August 2006 at 15:29, Frank E Harrell Jr wrote:
| Manuel Castej?n Limas wrote:
| > Hello,
| > I've just compiled Hmisc ok under dapper.
| > I think you need to further install some packages.
| > Have you installed libc6-dev?
| > I would start installing the build-essential package.
| > Best wishes
| > Manuel
| 
| Thanks Manuel, apt-get install build-essential solved the problem.

FWIW r-base-dev also depends on build-essential, so problem-solving tip
number one ("if on Debian and building things for R, install r-base-dev")
still applies.

Cheers, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From edd at debian.org  Wed Aug 30 03:03:21 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 29 Aug 2006 20:03:21 -0500
Subject: [R] problem in install on ubuntu
In-Reply-To: <44EDC75E.70407@botany.utoronto.ca>
References: <Pine.LNX.4.63.0608241204460.2109@est.ufpr.br>
	<44EDC75E.70407@botany.utoronto.ca>
Message-ID: <17652.58329.276284.180463@basebud.nulle.part>


On 24 August 2006 at 11:35, Ryan Austin wrote:
| Hi Bruno,
| 
| Your missing the Basic Linear Algebra Subroutines 3.0 package.
| 
| apt-get install refblas3
| 
| should fix the problem. (At least in Debian, should be the same for Ubuntu)

The 'empty' package r-base-dev depends on 'refblas3-dev | atlas3-base-dev' so
installing r-base-dev on Debian or Ubuntu provides what most if not all
packages require.

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From btyner at gmail.com  Wed Aug 30 04:19:10 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 29 Aug 2006 22:19:10 -0400
Subject: [R]  Problem plotting with xyplot
References: 75.1f781c7d.2d1cb376@aol.com
Message-ID: <44F4F59E.7010706@stat.purdue.edu>

This is in regard to Dan Norlund's post from December 2003 (available at 
https://stat.ethz.ch/pipermail/r-help/attachments/20031225/1bce02b8/attachment.pl 
)

While I agree with Prof. Ripley's warning against fitting so few 
observations per panel, this is not actually what causes the error. 
Instead, the culprit is the fact that panel.loess uses 
family='symmetric' by default; since the first iteration of fitting 
interpolates the data, the residuals are zero, hence the robustness 
weights are not defined for the next iteration. In the example Dan gave, 
a change to

xyplot(tolerance~age | factor(id), data=tolerance.pp, 
type=c("p","smooth"), family="gaussian")

avoids this. As family='gaussian' is the default outside of panel.loess, 
it is easy to forget this detail.

Ben


From mark.lyman at gmail.com  Wed Aug 30 06:42:08 2006
From: mark.lyman at gmail.com (Mark Lyman)
Date: Wed, 30 Aug 2006 04:42:08 +0000 (UTC)
Subject: [R] Help on apply() function
References: <BAY105-DAV1AE5A6CB01759A250C078B23E0@phx.gbl>
Message-ID: <loom.20060830T063351-600@post.gmane.org>

Anusha <anu1978 <at> hotmail.com> writes:

> 
> Respected Sir/Madam,
> 
> I have a problem with apply function. I have to two matrices of dimension of
> one column but n rows. I have to check whether one matrix is greater than
> other by going thru each row (ie) using if condition to check one matrix
> with another matrix. 
> 
> I like to use apply() function to this approach. That is apply function
> between two matrices. I searched for examples online but I couldn't find
> any.
> 
> I don't know how to loop thru the matrices.
> 
> Please help with this problem.
> 
> Any help is appreciated.
> 
> My mailid is monkponu <at> yahoo.com.
> 
> Thanks,
> Anusha.
>

You can use the functions all.equal with the function isTRUE (see ?all.equal) to
check if two objects are nearly equal (within a certain tolerance). Or you can
use identical (see ?identical) to check if they are exactly the same. See the
examples in the help for identical.

Mark Lyman


From arun.kumar.saha at gmail.com  Wed Aug 30 07:47:23 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Wed, 30 Aug 2006 11:17:23 +0530
Subject: [R] Need help to estimate the Coef matrices in mAr
Message-ID: <d4c57560608292247h2f18d279r75061f0a4eb6cebd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060830/de67868e/attachment.pl 

From joshi_juni at hotmail.com  Wed Aug 30 07:47:49 2006
From: joshi_juni at hotmail.com (Juni Joshi)
Date: Wed, 30 Aug 2006 15:47:49 +1000
Subject: [R] Cross-correlation between two time series data
Message-ID: <BAY108-F295F0A4099FCED365A82C2FF3E0@phx.gbl>


   Hi all,

   I  have  two  time  series  data  (say  x  and  y). I am interested to
   calculate the correlation between them and its confidence interval (or
   to  test  no  correlation). Function cor.test(x,y) does the test of no
   correlation. But this test probably is wrong because of autocorrelated
   data.

   ccf()  calculates the correlation between two series data. But it does
   not  provide  the  confidence intervals of cross correlation. Is there
   any  function  that  calculates the confidence interval of correlation
   between  two  time  series data or performs the test of no correlation
   between two time series data.

   Thanks.

   Jun

From ccatj at web.de  Wed Aug 30 08:37:55 2006
From: ccatj at web.de (Christian Jones)
Date: Wed, 30 Aug 2006 08:37:55 +0200
Subject: [R] fitting an interaction term
Message-ID: <700084723@web.de>

Hello!
I?m fitting a model with glm(family binomial). The best model counts 9 Variables and includes an interaction term that was generated by the product of to continuous variables (a*b). All variables are correlated under a value of 0.7 (Spearman rank order) While the estimates of both main effects are negativ, the resulting interaction term is positiv. This change of sign makes it difficult to interpret the model and above all, is this perhaps due to a bad variable choice ?
Thanks a lot for helping
Christian


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug 30 08:59:41 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 30 Aug 2006 08:59:41 +0200
Subject: [R] Substring and strsplit
References: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
Message-ID: <00a901c6cc01$ddfc2240$0540210a@www.domain>

you can also use substring(), e.g.,

substring(x3, 1:nchar(x3), 1:nchar(x3))


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Erin Hodgess" <hodgess at gator.dt.uh.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 30, 2006 12:25 AM
Subject: [R] Substring and strsplit


> Dear R People:
>
> I am trying to split a character vector into a set of individual
> letters:
>
> Ideal:
> x3 <- c("dog")
> "d" "o" "g"
>
> I tried the following:
>> strsplit(x3)
> Error in strsplit(x3) : argument "split" is missing, with no default
>> strsplit(x3,1)
> [[1]]
> [1] "dog"
>
> I know that this is incredibly simple, but what am I doing wrong?
>
> Either Windows or Linux 2.3.1
>
> Thanks in advance!
>
>
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From a_devan at hotmail.com  Wed Aug 30 02:18:28 2006
From: a_devan at hotmail.com (DEVAN)
Date: Tue, 29 Aug 2006 20:18:28 -0400
Subject: [R] Help on apply() function
Message-ID: <BAY108-DAV538F1D8CB68068C78443C893E0@phx.gbl>

Respected Sir/Madam,

I have a problem with apply function. I have to two matrices of dimension of
one column but n rows. I have to check whether one matrix is greater than
other by going thru each row (ie) using if condition to check one matrix
with another matrix. 

I like to use apply() function to this approach. That is apply function
between two matrices. I searched for examples online but I couldn't find
any.

I don't know how to loop thru the matrices.

Please help with this problem.

Any help is appreciated.

My mailid is monkponu at yahoo.com.

Thanks,
Anusha.


From gordon.morrison at hsbcib.com  Wed Aug 30 09:35:22 2006
From: gordon.morrison at hsbcib.com (gordon.morrison at hsbcib.com)
Date: Wed, 30 Aug 2006 08:35:22 +0100
Subject: [R] Installation of SrcStatConnectorSrv on Windows
Message-ID: <OF8C341E11.ABC31430-ON802571DA.0027629C-802571DA.0029B10F@hsbcib.com>


I am trying to install SrcStatConnectorSrv(2) and rcom from local zip
files.

I have successfully downloaded both files (from CRAN for rcom CRAN Other
for SrcStatConnectorSrv) and installed rcom. However, I get the following
error message when I try to install SrcStatConnectorSrv (the version dated
21-Aug-2006:

> utils:::menuInstallLocal()
Error in gzfile(file, "r") : unable to open connection
In addition: Warning message:
cannot open compressed file 'SrcStatConnectorSrv(2)/DESCRIPTION'

I have also run

 RSiteSearch("SrcStatConnectorSrv")

but this failed to yield any results.

I am running Windows XP Professional and

> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)

Could anyone shed any light on what I am doing wrong?

Many thanks in anticipation

Gordon

************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************


************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************


-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group "HSBC"
for the information of the addressee only and should not be reproduced
and/or distributed to any other person. Each page attached hereto must
be read in conjunction with any disclaimer which forms part of it.
Unless otherwise stated, this transmission is neither an offer nor the
solicitation of an offer to sell or purchase any investment. Its
contents are based on information obtained from sources believed to be
reliable but HSBC makes no representation and accepts no responsibility
or liability as to its completeness or accuracy.


From bibiko at eva.mpg.de  Wed Aug 30 10:07:48 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 30 Aug 2006 10:07:48 +0200
Subject: [R] Substring and strsplit
In-Reply-To: <00a901c6cc01$ddfc2240$0540210a@www.domain>
References: <200608292225.k7TMPQrJ029520@gator.dt.uh.edu>
	<00a901c6cc01$ddfc2240$0540210a@www.domain>
Message-ID: <67A99605-58BF-4263-BB44-48D6BA94C54D@eva.mpg.de>

If you are using 'only' English then

str <- "dog"
strsplit(str,NULL)[[1]]

works perfectly and it is fast.

But if you also dealing with Unicode character have a look at

http://wiki.r-project.org/rwiki/doku.php?id=tips:data- 
strings:decomposestring

Cheers,

Hans



> you can also use substring(), e.g.,
>
> substring(x3, 1:nchar(x3), 1:nchar(x3))
>
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "Erin Hodgess" <hodgess at gator.dt.uh.edu>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, August 30, 2006 12:25 AM
> Subject: [R] Substring and strsplit
>
>
>
>> Dear R People:
>>
>> I am trying to split a character vector into a set of individual
>> letters:
>>
>> Ideal:
>> x3 <- c("dog")
>> "d" "o" "g"
>>
>> I tried the following:
>>
>>> strsplit(x3)
>>>
>> Error in strsplit(x3) : argument "split" is missing, with no default
>>
>>> strsplit(x3,1)
>>>
>> [[1]]
>> [1] "dog"
>>
>> I know that this is incredibly simple, but what am I doing wrong?
>>
>> Either Windows or Linux 2.3.1
>>
>> Thanks in advance!
>>
>>
>> Sincerely,
>> Erin Hodgess
>> Associate Professor
>> Department of Computer and Mathematical Sciences
>> University of Houston - Downtown
>> mailto: hodgess at gator.uhd.edu
>>
>>
>>


From andras.treszl at gmail.com  Wed Aug 30 10:42:46 2006
From: andras.treszl at gmail.com (Andras Treszl)
Date: Wed, 30 Aug 2006 10:42:46 +0200
Subject: [R] MCMClogit
Message-ID: <653559300608300142u28725872q34d6573a27d58f5@mail.gmail.com>

Hi,

I am using MCMCpack and the MCMClogit function to create logistic
regression models in a medical (adverse event) study. My question is,
is there a way where I can directly create the estimated probabilities
of the adverse outcome, moreover the confidence interval for the
estimated probabilities?

Thank you for your help!

Andras


From stat700004 at yahoo.co.in  Wed Aug 30 10:55:57 2006
From: stat700004 at yahoo.co.in (stat stat)
Date: Wed, 30 Aug 2006 09:55:57 +0100 (BST)
Subject: [R] How to put title Vertically
Message-ID: <20060830085557.7100.qmail@web7612.mail.in.yahoo.com>

Dear all R users,

Suppose,


Dear all R users,

Suppose,


pauto.cor = pacf(lh, plot=F)
max.lag = max(pauto.cor$lag)
min.lag = min(pauto.cor$lag)
centre = (max.lag - min.lag)/2
pauto.cor = pauto.cor$acf
pauto.cor = pauto.cor[-1]

par(mar=c(3,0,1,1))
barplot(pauto.cor, axes=F,xlim=c(max(pauto.cor),
min(pauto.cor)), space=0,
col="green4",border="green",horiz=T) 


#This plots PACF vertically

Now I want to put a title of above plot but NOT
horizontally rather Vertically.

Can anyone please tell me how to do that?

Thanks and regards,
stat

thanks in advance


From msubianto at gmail.com  Wed Aug 30 11:43:23 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 30 Aug 2006 11:43:23 +0200
Subject: [R] Barplot
Message-ID: <44F55DBB.7010305@gmail.com>

Dear all,
I have a dataset. I want to make barplot from this data.
Zero1 <- "
   V1 V2 V3 V4 V5 V6 V7 V8       V9
1   1  0  0  0  1  0  0  0 Positive
2   0  0  1  0  1  0  1  1 Negative
3   0  0  1  0  0  0  1  1 Positive
4   0  1  0  1  1  1  0  1 Negative
5   0  0  1  0  1  1  0  0 Positive
6   0  1  0  0  1  1  1  1 Negative
7   1  0  1  1  1  1  1  1 Negative
8   0  0  0  0  1  0  0  1 Negative
9   0  1  1  1  1  0  0  1 Negative
10  0  0  0  1  1  0  1  0 Positive
11  0  0  0  0  1  0  0  1 Negative
12  0  0  1  1  1  1  1  0 Positive
13  0  1  1  0  1  1  1  1 Negative"

z1 <- read.table(textConnection(Zero1), header=TRUE)
z1
str(z1)

A simple way I can use mosaic plot
mosaicplot(table(z1))
library(vcd)
mosaic(table(z1))

I have tried to learn ?xtabs ?table and ?ftable but I can't figure out.
I need a barplot for all variables and the result maybe like

|   |                              |   |
|   |   | |   |   |            |   |   |
|pos|neg| |pos|neg|            |pos|neg|
|   |   | |   |   |            |   |   |
--------- ---------            ---------
    v1        v2    v3 .... v7     v8

Thanks you for any helps.
Regards, Muhammad Subianto


From ripley at stats.ox.ac.uk  Wed Aug 30 11:50:20 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Aug 2006 10:50:20 +0100 (BST)
Subject: [R] Installation of SrcStatConnectorSrv on Windows
In-Reply-To: <OF8C341E11.ABC31430-ON802571DA.0027629C-802571DA.0029B10F@hsbcib.com>
References: <OF8C341E11.ABC31430-ON802571DA.0027629C-802571DA.0029B10F@hsbcib.com>
Message-ID: <Pine.LNX.4.64.0608301044080.27066@gannet.stats.ox.ac.uk>

That is not an R package.

It looks like the sources of the server described on 

http://cran.r-project.org/contrib/extra/dcom/RSrv200.html

which tells you about the approriate mailing list to ask about it.
You probably want to use RSrv200.exe to install the server (and other 
tools and examples).


On Wed, 30 Aug 2006, gordon.morrison at hsbcib.com wrote:

> 
> I am trying to install SrcStatConnectorSrv(2) and rcom from local zip
> files.
> 
> I have successfully downloaded both files (from CRAN for rcom CRAN Other
> for SrcStatConnectorSrv) and installed rcom. However, I get the following
> error message when I try to install SrcStatConnectorSrv (the version dated
> 21-Aug-2006:
> 
> > utils:::menuInstallLocal()
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'SrcStatConnectorSrv(2)/DESCRIPTION'
> 
> I have also run
> 
>  RSiteSearch("SrcStatConnectorSrv")
> 
> but this failed to yield any results.
> 
> I am running Windows XP Professional and
> 
> > version
>                _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> Could anyone shed any light on what I am doing wrong?
> 
> Many thanks in anticipation
> 
> Gordon
> 
> ************************************************************
> HSBC Bank plc may be solicited in the course of its placement efforts for a
> new issue, by investment clients of the firm for whom the Bank as a firm
> already provides other services. It may equally decide to allocate to its
> own proprietary book or with an associate of HSBC Group. This represents a
> potential conflict of interest. HSBC Bank plc has internal arrangements
> designed to ensure that the firm would give unbiased and full advice to the
> corporate finance client about the valuation and pricing of the offering as
> well as internal systems, controls and procedures to identify and manage
> conflicts of interest.
> 
> HSBC Bank plc
> Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
> Registered in England - Number 14259
> Authorised and regulated by the Financial Services Authority.
> ************************************************************
> 
> 
> ************************************************************
> HSBC Bank plc may be solicited in the course of its placement efforts for a
> new issue, by investment clients of the firm for whom the Bank as a firm
> already provides other services. It may equally decide to allocate to its
> own proprietary book or with an associate of HSBC Group. This represents a
> potential conflict of interest. HSBC Bank plc has internal arrangements
> designed to ensure that the firm would give unbiased and full advice to the
> corporate finance client about the valuation and pricing of the offering as
> well as internal systems, controls and procedures to identify and manage
> conflicts of interest.
> 
> HSBC Bank plc
> Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
> Registered in England - Number 14259
> Authorised and regulated by the Financial Services Authority.
> ************************************************************
> 
> 
> -----------------------------------------
> SAVE PAPER - THINK BEFORE YOU PRINT!
> 
> This transmission has been issued by a member of the HSBC Group "HSBC"
> for the information of the addressee only and should not be reproduced
> and/or distributed to any other person. Each page attached hereto must
> be read in conjunction with any disclaimer which forms part of it.
> Unless otherwise stated, this transmission is neither an offer nor the
> solicitation of an offer to sell or purchase any investment. Its
> contents are based on information obtained from sources believed to be
> reliable but HSBC makes no representation and accepts no responsibility
> or liability as to its completeness or accuracy.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jacques.veslot at good.ibl.fr  Wed Aug 30 12:09:42 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 30 Aug 2006 12:09:42 +0200
Subject: [R] Barplot
In-Reply-To: <44F55DBB.7010305@gmail.com>
References: <44F55DBB.7010305@gmail.com>
Message-ID: <44F563E6.7030103@good.ibl.fr>

barplot(t(sapply(split(z1[,1:8], z1$V9),colSums)), beside=T)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Muhammad Subianto a ?crit :
> Dear all,
> I have a dataset. I want to make barplot from this data.
> Zero1 <- "
>    V1 V2 V3 V4 V5 V6 V7 V8       V9
> 1   1  0  0  0  1  0  0  0 Positive
> 2   0  0  1  0  1  0  1  1 Negative
> 3   0  0  1  0  0  0  1  1 Positive
> 4   0  1  0  1  1  1  0  1 Negative
> 5   0  0  1  0  1  1  0  0 Positive
> 6   0  1  0  0  1  1  1  1 Negative
> 7   1  0  1  1  1  1  1  1 Negative
> 8   0  0  0  0  1  0  0  1 Negative
> 9   0  1  1  1  1  0  0  1 Negative
> 10  0  0  0  1  1  0  1  0 Positive
> 11  0  0  0  0  1  0  0  1 Negative
> 12  0  0  1  1  1  1  1  0 Positive
> 13  0  1  1  0  1  1  1  1 Negative"
> 
> z1 <- read.table(textConnection(Zero1), header=TRUE)
> z1
> str(z1)
> 
> A simple way I can use mosaic plot
> mosaicplot(table(z1))
> library(vcd)
> mosaic(table(z1))
> 
> I have tried to learn ?xtabs ?table and ?ftable but I can't figure out.
> I need a barplot for all variables and the result maybe like
> 
> |   |                              |   |
> |   |   | |   |   |            |   |   |
> |pos|neg| |pos|neg|            |pos|neg|
> |   |   | |   |   |            |   |   |
> --------- ---------            ---------
>     v1        v2    v3 .... v7     v8
> 
> Thanks you for any helps.
> Regards, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andras.treszl at gmail.com  Wed Aug 30 12:10:51 2006
From: andras.treszl at gmail.com (Andras Treszl)
Date: Wed, 30 Aug 2006 12:10:51 +0200
Subject: [R] MCMClogit
Message-ID: <653559300608300310m31886d11o85b9101fb8e47be7@mail.gmail.com>

Hi,

I am using MCMCpack and the MCMClogit function to create logistic
regression models in a medical (adverse event) study. My question is,
is there a way where I can directly create the estimated probabilities
of the adverse outcome, and the confidence interval for the
estimated probabilities? Or is there another package I should use instead?

Thank you for your help!

Andras


From phhs80 at gmail.com  Wed Aug 30 12:14:52 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 30 Aug 2006 11:14:52 +0100
Subject: [R] Help on apply() function
In-Reply-To: <loom.20060830T063351-600@post.gmane.org>
References: <BAY105-DAV1AE5A6CB01759A250C078B23E0@phx.gbl>
	<loom.20060830T063351-600@post.gmane.org>
Message-ID: <6ade6f6c0608300314h5ec7d6eawa4881f6046775334@mail.gmail.com>

On 8/30/06, Mark Lyman <mark.lyman at gmail.com> wrote:
> > I have a problem with apply function. I have to two matrices of dimension of
> > one column but n rows. I have to check whether one matrix is greater than
> > other by going thru each row (ie) using if condition to check one matrix
> > with another matrix.
> >
> > I like to use apply() function to this approach. That is apply function
> > between two matrices. I searched for examples online but I couldn't find
> > any.
> >
> > I don't know how to loop thru the matrices.
>
> You can use the functions all.equal with the function isTRUE (see ?all.equal) to
> check if two objects are nearly equal (within a certain tolerance). Or you can
> use identical (see ?identical) to check if they are exactly the same. See the
> examples in the help for identical.

A possible solution could be:

> x <- 1:5
> x
[1] 1 2 3 4 5
> y <- 2:6
> y
[1] 2 3 4 5 6
> all((y-x)>0)
[1] TRUE

The result TRUE means that each element of y is greater than the
homologous element in x.

Paul


From rkrug at sun.ac.za  Wed Aug 30 12:27:13 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 30 Aug 2006 12:27:13 +0200
Subject: [R] density() with from, to or cut and comparrison of density()
Message-ID: <44F56801.9000807@sun.ac.za>

Hi

the function density() does normally integrate to one - I've checked it
and it works and I also read the previous threads.
But I realised that it does not integrate to one if I use from, to or cut.

My scenario: I simulated densities of a plants originating from an sseed
source at distance zero. Therefore the density of the plants will be
highest close to zero. Is there anything I can do to have this pattern?
If I use 'from' or 'cut', the resulting densities do not integrate to
one which I need as I want to compare different density curves.

Ny second question is concerning the bandwidth. An I correct in saying
that if I want to compare different density estimates that the bandwidth
should be the same for all of them?

Thanks in advance for your help,

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From jim at bitwrit.com.au  Thu Aug 31 02:27:12 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 30 Aug 2006 20:27:12 -0400
Subject: [R] Barplot
In-Reply-To: <44F55DBB.7010305@gmail.com>
References: <44F55DBB.7010305@gmail.com>
Message-ID: <44F62CE0.2020005@bitwrit.com.au>

Muhammad Subianto wrote:
> ... 
> I have tried to learn ?xtabs ?table and ?ftable but I can't figure out.
> I need a barplot for all variables and the result maybe like
> 
> |   |                              |   |
> |   |   | |   |   |            |   |   |
> |pos|neg| |pos|neg|            |pos|neg|
> |   |   | |   |   |            |   |   |
> --------- ---------            ---------
>     v1        v2    v3 .... v7     v8
> 
barplot(sapply(z1[1:8],by,z1[9],sum),beside=TRUE)

Jim


From robert-mcfadden at o2.pl  Wed Aug 30 12:47:32 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Wed, 30 Aug 2006 12:47:32 +0200
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <44F28BE0.3050807@mango-solutions.com>
Message-ID: <001001c6cc21$b2761cf0$1191680a@robert>

May be it's not a bug, but I tried to search for the package rpanel and I
was not find. At the r-project's site that package is available.
How to explain it?
Rob


From rfrancois at mango-solutions.com  Wed Aug 30 12:55:16 2006
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Wed, 30 Aug 2006 11:55:16 +0100
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <001001c6cc21$b2761cf0$1191680a@robert>
References: <001001c6cc21$b2761cf0$1191680a@robert>
Message-ID: <44F56E94.1080109@mango-solutions.com>

Robert Mcfadden a ?crit :
> May be it's not a bug, but I tried to search for the package rpanel and I
> was not find. At the r-project's site that package is available.
> How to explain it?
> Rob   
>   
Hi Rob,

If it's not there : http://finzi.psych.upenn.edu/R/library/
it's not on the extension

Cheers,

Romain


From ligges at statistik.uni-dortmund.de  Wed Aug 30 12:55:43 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 30 Aug 2006 12:55:43 +0200
Subject: [R] How to put title Vertically
In-Reply-To: <20060830085557.7100.qmail@web7612.mail.in.yahoo.com>
References: <20060830085557.7100.qmail@web7612.mail.in.yahoo.com>
Message-ID: <44F56EAF.6040203@statistik.uni-dortmund.de>



stat stat wrote:
> Dear all R users,
> 
> Suppose,
> 
> 
> Dear all R users,
> 
> Suppose,
> 
> 
> pauto.cor = pacf(lh, plot=F)
> max.lag = max(pauto.cor$lag)
> min.lag = min(pauto.cor$lag)
> centre = (max.lag - min.lag)/2
> pauto.cor = pauto.cor$acf
> pauto.cor = pauto.cor[-1]
> 
> par(mar=c(3,0,1,1))
> barplot(pauto.cor, axes=F,xlim=c(max(pauto.cor),
> min(pauto.cor)), space=0,
> col="green4",border="green",horiz=T) 
> 
> 
> #This plots PACF vertically
> 
> Now I want to put a title of above plot but NOT
> horizontally rather Vertically.
> 
> Can anyone please tell me how to do that?


Use mtext()

Uwe Ligges


> Thanks and regards,
> stat
> 
> thanks in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Aug 30 13:01:52 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 30 Aug 2006 13:01:52 +0200
Subject: [R] density() with from, to or cut and comparrison of density()
In-Reply-To: <44F56801.9000807@sun.ac.za>
References: <44F56801.9000807@sun.ac.za>
Message-ID: <44F57020.50908@statistik.uni-dortmund.de>



Rainer M Krug wrote:
> Hi
> 
> the function density() does normally integrate to one - I've checked it
> and it works and I also read the previous threads.
> But I realised that it does not integrate to one if I use from, to or cut.
> 
> My scenario: I simulated densities of a plants originating from an sseed
> source at distance zero. Therefore the density of the plants will be
> highest close to zero. Is there anything I can do to have this pattern?
> If I use 'from' or 'cut', the resulting densities do not integrate to
> one which I need as I want to compare different density curves.

The kernel chosen might be not the ideal one for such a restriction. If 
the density outside the "cut" range is extremely small, you might want 
to do a dirty transformation so that the values sum up to 1 again.

> Ny second question is concerning the bandwidth. An I correct in saying
> that if I want to compare different density estimates that the bandwidth
> should be the same for all of them?

Yes.

Uwe Ligges


> Thanks in advance for your help,
> 
> Rainer
>


From ligges at statistik.uni-dortmund.de  Wed Aug 30 13:07:36 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 30 Aug 2006 13:07:36 +0200
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <44F56E94.1080109@mango-solutions.com>
References: <001001c6cc21$b2761cf0$1191680a@robert>
	<44F56E94.1080109@mango-solutions.com>
Message-ID: <44F57178.1090407@statistik.uni-dortmund.de>



Romain Francois wrote:
> Robert Mcfadden a ?crit :
>> May be it's not a bug, but I tried to search for the package rpanel and I
>> was not find. At the r-project's site that package is available.
>> How to explain it?
>> Rob   
>>   
> Hi Rob,
> 
> If it's not there : http://finzi.psych.upenn.edu/R/library/
> it's not on the extension


... because that side is update once a month, hence simply wait for a 
couple of days until it will show up.

Uwe Ligges


> Cheers,
> 
> Romain
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From msubianto at gmail.com  Wed Aug 30 13:11:01 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 30 Aug 2006 13:11:01 +0200
Subject: [R] Barplot - thanks
In-Reply-To: <44F55DBB.7010305@gmail.com>
References: <44F55DBB.7010305@gmail.com>
Message-ID: <44F57245.1090703@gmail.com>

Dear all,
Many Thanks to Jacques VESLOT and Jim Lemon for their helps.

Best, Muhammad Subianto


#Jacques VESLOT
barplot(t(sapply(split(z1[,1:8], z1$V9),colSums)), beside=T)

#Jim Lemon
barplot(sapply(z1[1:8],by,z1[9],sum),beside=TRUE)




On this day 30/08/2006 11:43, Muhammad Subianto wrote:
> Dear all,
> I have a dataset. I want to make barplot from this data.
> Zero1 <- "
>    V1 V2 V3 V4 V5 V6 V7 V8       V9
> 1   1  0  0  0  1  0  0  0 Positive
> 2   0  0  1  0  1  0  1  1 Negative
> 3   0  0  1  0  0  0  1  1 Positive
> 4   0  1  0  1  1  1  0  1 Negative
> 5   0  0  1  0  1  1  0  0 Positive
> 6   0  1  0  0  1  1  1  1 Negative
> 7   1  0  1  1  1  1  1  1 Negative
> 8   0  0  0  0  1  0  0  1 Negative
> 9   0  1  1  1  1  0  0  1 Negative
> 10  0  0  0  1  1  0  1  0 Positive
> 11  0  0  0  0  1  0  0  1 Negative
> 12  0  0  1  1  1  1  1  0 Positive
> 13  0  1  1  0  1  1  1  1 Negative"
> 
> z1 <- read.table(textConnection(Zero1), header=TRUE)
> z1
> str(z1)
> 
> A simple way I can use mosaic plot
> mosaicplot(table(z1))
> library(vcd)
> mosaic(table(z1))
> 
> I have tried to learn ?xtabs ?table and ?ftable but I can't figure out.
> I need a barplot for all variables and the result maybe like
> 
> |   |                              |   |
> |   |   | |   |   |            |   |   |
> |pos|neg| |pos|neg|            |pos|neg|
> |   |   | |   |   |            |   |   |
> --------- ---------            ---------
>     v1        v2    v3 .... v7     v8
> 
> Thanks you for any helps.
> Regards, Muhammad Subianto
>


From baron at psych.upenn.edu  Wed Aug 30 13:13:56 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 30 Aug 2006 07:13:56 -0400
Subject: [R] Firefox extension fo "R Site Search"
In-Reply-To: <44F56E94.1080109@mango-solutions.com>
References: <001001c6cc21$b2761cf0$1191680a@robert>
	<44F56E94.1080109@mango-solutions.com>
Message-ID: <20060830111356.GA19947@psych.upenn.edu>

On 08/30/06 11:55, Romain Francois wrote:
> Robert Mcfadden a ?crit :
> > May be it's not a bug, but I tried to search for the package rpanel and I
> > was not find. At the r-project's site that package is available.
> > How to explain it?
> > Rob
> >
> Hi Rob,
> 
> If it's not there : http://finzi.psych.upenn.edu/R/library/
> it's not on the extension

The list is updated monthly, and rpanel is new.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From jfox at mcmaster.ca  Wed Aug 30 13:37:36 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 30 Aug 2006 07:37:36 -0400
Subject: [R] [R-pkgs] Version 1.2-0 of the Rcmdr package
Message-ID: <20060830113736.EBRT24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>

I've submitted a new, and substantially enhanced, version (1.2-0) of the
Rcmdr package to CRAN. Some highlights (from the CHANGES) file:

   o  Added ability to import from Excel, Access or dBase files (contributed
by Samir Messad, Renaud Lancelot and Matthieu Lesnoff).

   o  Added ability to read data from the clipboard (suggested by Graham
Smith).

   o  Added "Data -> Active data set -> Stack variables in active data set"
(suggested by Richard Heiberger).
   
   o  Added many probability distributions, courtesy of Miroslav Ristic and
G. Jay Kerns, Andy Chang, and Theophilius Boye.
   
   o  Added dialogs to sample from probability distributions; divided
distributions menus into Continuous and Discrete.
   
   o  Added totPercents function. Added total percentages and components of
chi-square to two-way tables (suggested by Richard Heiberger).
      
   o  Added "Statistics -> Summaries -> Count missing observations".
   
   o  Added "Statistics -> Summaries -> Correlation test", contributed by
Stefano Calza.
      
   o  The numerical summaries, table of statistics, frequency distributions,
numeric to factor, and recode dialogs will now process multiple variables in
parallel (suggested by Bo Sommerlund).
   
   o  Added function numSummaries for neater output from "Statistics ->
Summaries -> Numerical summaries" (motivated by comments from Bo
Sommerlund).
   
   o  XY conditioning plots dialog added, courtesy of Richard Heiberger.
      
   o  Enhancements to 3D scatterplots.

   o  Small interface improvements (e.g., pressing letter key in a list box
moves to next entry starting with that letter).

   o  New Rcmdr options (etc and etcMenus) determine where the Commander
will look for its configuration and menu files (motivated by a suggestion by
Richard Heiberger).
      
Rcmdr version 1.2-0 includes revised Catalan and Japanese translations
(courtesy of Manel Salamero and Takaharu Araki); as other revised
translations become available, I'll submit updated versions of the package
to CRAN. Version 1.2-1 will also have a revised introductory manual.

As usual, bug reports, comments, and suggestions are welcome.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From g.comte at alliance-ir.net  Wed Aug 30 14:30:09 2006
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Wed, 30 Aug 2006 14:30:09 +0200
Subject: [R] Datetime
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3415ED1C@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060830/e4255b6a/attachment.pl 

From g.comte at alliance-ir.net  Wed Aug 30 14:30:09 2006
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Wed, 30 Aug 2006 14:30:09 +0200
Subject: [R] Datetime
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3415ED1C@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060830/e4255b6a/attachment-0001.pl 

From rkrug at sun.ac.za  Wed Aug 30 13:45:14 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Wed, 30 Aug 2006 13:45:14 +0200
Subject: [R] density() with from, to or cut and comparrison of density()
In-Reply-To: <44F57020.50908@statistik.uni-dortmund.de>
References: <44F56801.9000807@sun.ac.za>
	<44F57020.50908@statistik.uni-dortmund.de>
Message-ID: <44F57A4A.7050306@sun.ac.za>

Uwe Ligges wrote:
> 
> 
> Rainer M Krug wrote:

SNIP

>> My scenario: I simulated densities of a plants originating from an sseed
>> source at distance zero. Therefore the density of the plants will be
>> highest close to zero. Is there anything I can do to have this pattern?
>> If I use 'from' or 'cut', the resulting densities do not integrate to
>> one which I need as I want to compare different density curves.
> 
> The kernel chosen might be not the ideal one for such a restriction. If
> the density outside the "cut" range is extremely small, you might want
> to do a dirty transformation so that the values sum up to 1 again.
> 

The density on the left of the "cut" range (let's say the left is at 0)
is high - it is as high or even higher then in the left in the estimated
range. But I don't want to include it in any calculations, as I am only
concerned about the decline of the density in the range 0 < x < 125. On
the right "cut" range, the density is nearly zero and in relation to the
estimated range very small.

Which kernel would be a better choice for this kind of scenario?


>> Ny second question is concerning the bandwidth. An I correct in saying
>> that if I want to compare different density estimates that the bandwidth
>> should be the same for all of them?
> 
> Yes.

Thanks.

> 
> Uwe Ligges
> 
> 
>> Thanks in advance for your help,
>>
>> Rainer
>>


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From p.dalgaard at biostat.ku.dk  Wed Aug 30 15:02:45 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Aug 2006 15:02:45 +0200
Subject: [R] Datetime
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415ED1C@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415ED1C@allexch01.alliance.com>
Message-ID: <x2u03uqsui.fsf@viggo.kubism.ku.dk>

"COMTE Guillaume" <g.comte at alliance-ir.net> writes:

> Hi all,
> 
> I'm getting confused by date handling.
> 
>  
> 
> I wish to read a date from a file wich is a number of seconds since 1970
> (POSIXct).
> 
> Then i wish to convert this date to a human readable form (POSIXlt)
> 
>  
> 
> By example :
> 
>  
> 
> ctDate<-1132963200 #"Wed Aug 30 14:24:37 2006"
> 
> is(ctDate)
> 
> [1] "numeric" "vector"
> 
>  
> 
> ctDate isn't a numeric vector, it is a POSIXct... none of the clues
> inside the docs has given me the answer, i can't cast it, i can't
> converti t to POSIXlt because it is wrong class (as.POSIX** don't want
> to work...)
> 
>  
> 
> What is the trick ??

It *is* a numeric vector. You need to add it to the epoch time or do
an explicit conversion

> ISOdatetime(1970,1,1,1,0,0)+ctDate # epoch is 01:00 in CET
[1] "2005-11-26 01:00:00 CET"

> structure(ctDate,class="POSIXct")
[1] "2005-11-26 01:00:00 CET"

(Your date seems out of whack by some 24 million seconds...)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bolker at zoo.ufl.edu  Wed Aug 30 15:23:47 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 30 Aug 2006 13:23:47 +0000 (UTC)
Subject: [R] Handling realisations in geoRglm
References: <44F4DB6A.60400@ucdavis.edu>
Message-ID: <loom.20060830T151720-96@post.gmane.org>

Chris Barker <cmbarker <at> ucdavis.edu> writes:

> 
However, after reading the packages' documentation and 
> searching the mailing lists and other sources, it seems that the 
> functions in geoRglm do not acknowledge the existence of multiple 
> realisations per site.  I see that the likfit() function in geoR has an 
> argument for realisations, but I cannot find anything similar in 
> geoRglm.  Is it possible to model data with repeated realisations from 
> the same sites using geoRglm?  If so, how can this be done and is there 
> a way to model dependence among the realisations over time?
> 
  I don't know; I would be interested in the conclusions you reach.
I, too, found the support for multiple realisations to be relatively
skimpy in geoR (only likfit, and not any of the variogram-based
methods, support realisations); the code for likfit.glsm in the
geoRglm packages also reveals no hint of realisation code.

  Even so, what there is in geoR says quite strongly that realisations
are expected to be independent.  My guess is that for complex space/time
dependence, and wanting an answer to a practical problem, you would be
better off with a GEE approach ...

Writing the package maintainer directly
would probably be more useful.

  Ben Bolker


From admartin at wustl.edu  Wed Aug 30 15:47:38 2006
From: admartin at wustl.edu (Andrew D. Martin)
Date: Wed, 30 Aug 2006 08:47:38 -0500
Subject: [R] MCMClogit
In-Reply-To: <653559300608300310m31886d11o85b9101fb8e47be7@mail.gmail.com>
References: <653559300608300310m31886d11o85b9101fb8e47be7@mail.gmail.com>
Message-ID: <9A641CE7-B869-4CD2-9191-FC67763E8D96@wustl.edu>


Andras,

At this point you need to write your own function to take the  
posterior density sample (stored in a coda mcmc object) and  
covariates of interest to get a Monte Carlo estimate of these  
probabilities.

Best,
ADM

On Aug 30, 2006, at 5:10 AM, Andras Treszl wrote:

> Hi,
>
> I am using MCMCpack and the MCMClogit function to create logistic
> regression models in a medical (adverse event) study. My question is,
> is there a way where I can directly create the estimated probabilities
> of the adverse outcome, and the confidence interval for the
> estimated probabilities? Or is there another package I should use  
> instead?
>
> Thank you for your help!
>
> Andras
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Andrew D. Martin, Ph.D.
Professor and CERL Director, School of Law
Professor, Department of Political Science
Washington University in St. Louis

(314) 935-5863 (Office)
(314) 935-5150 (Fax)

Office: Anheuser-Busch 470
Email: admartin at wustl.edu
WWW: http://adm.wustl.edu


From HDoran at air.org  Wed Aug 30 15:49:33 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 30 Aug 2006 09:49:33 -0400
Subject: [R] Create a vector from another vector
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060830/e045a8db/attachment.pl 

From r.hankin at noc.soton.ac.uk  Wed Aug 30 16:01:34 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 30 Aug 2006 15:01:34 +0100
Subject: [R] Create a vector from another vector
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>
Message-ID: <6BA454D5-8AAA-4BD3-BBD1-70EA2192727A@soc.soton.ac.uk>

Dear Harold

package "partitions" does almost this:



 > library(partitions)
 > x <- 1+restrictedparts(15,5)
 > x[,1:10]
      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]   16   15   14   13   12   11   10
[2,]    1    2    3    4    5    6    7
[3,]    1    1    1    1    1    1    1
[4,]    1    1    1    1    1    1    1
[5,]    1    1    1    1    1    1    1
      [,8] [,9] [,10]
[1,]    9   14    13
[2,]    8    2     3
[3,]    1    2     2
[4,]    1    1     1
[5,]    1    1     1
 >

HTH

Robin


On 30 Aug 2006, at 14:49, Doran, Harold wrote:

> Dear list
>
> Suppose I have the following vector:
>
> x <-  c(3,4,2,5,6)
>
> Obviously, this sums to 20. Now, I want to have a second vector,  
> call it
> x2, that sums to x where 5 <= x <= 20, but there are constraints.
>
> 1) The new vector must be same length as x
> 2) No element of the new vector can be 0
> 3) Element x2[i] of the new vector cannot be larger than element x 
> [i] of
> the original vector
> 4) Ordering is not important
>
> The following would be an example of what I would want if the user
> wanted the vector x2 to sum to 19
>
> x2 <- c(2,4,2,5,6)
>
> Or, because ordering is not important, this is acceptable
>
> x2 <- c(3,3,2,5,6)
>
> Whereas this would not be appropriate
>
> x3 <- c(4, 2,2,5,6)
>
> Because element x3[1] is larger than x[1] even though it sums to 19.
>
> Ideally, the function would take as input the original vector, x, and
> the number that the new vector would sum to. In this example, the  
> vector
> could sum to any number 5 through 20.
>
> For example,
>
> myFun <- function(x, sumto) ... details ...
>
> Is there a preexisiting function that would already do this? I have
> spent too much (unsuccessful) time trying to write a function of my  
> own
> but can't seem to get this to work properly.
>
> Any hints would be greatly appreciated.
>
> Harold
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From svincenz at nemo.unipr.it  Wed Aug 30 15:56:50 2006
From: svincenz at nemo.unipr.it (Simone Vincenzi)
Date: Wed, 30 Aug 2006 15:56:50 +0200
Subject: [R] Optimization
Message-ID: <200608301357.k7UDuodl032724@smtp.unipr.it>

Dear R-list,
I'm trying to estimate the relative importance of 6 environmental variables
in determining clam yield. To estimate clam yield a previous work used the
function Yield = (A^a*B^b*C^c...)^1/(a+b+c+...) where A,B,C... are the
values of the environmental variables and the weights a,b,c... have not been
calibrated on data but taken from literature. Now I'd like to estimate the
weights a,b,c... by using a dataset with 110 observations of yield and
values of the environmental variables. I'm wondering if it is feasible or if
the number of observation is too low, if some data transformation is needed
and which R function is the most appropriate to try to estimate the weights.
Any help would be greatly appreciated.

Simone Vincenzi 

_________________________________________
Simone Vincenzi, PhD Student 
Department of Environmental Sciences
University of Parma
Parco Area delle Scienze, 33/A, 43100 Parma, Italy
Phone: +39 0521 905696
Fax: +39 0521 906611
e.mail: svincenz at nemo.unipr.it 



--


From romain.lorrilliere at ese.u-psud.fr  Wed Aug 30 16:04:13 2006
From: romain.lorrilliere at ese.u-psud.fr (Romain Lorrilliere)
Date: Wed, 30 Aug 2006 14:04:13 -0000
Subject: [R] converting decimal - hexadecimal
Message-ID: <451E7762.8000307@ese.u-psud.fr>

Hi,

do you know, a method to convert an decimal value (integer) to the 
corresponding hexadecimal value ?

thinks for help.

Romain

-- 

Lorrilli?re Romain

 

UMR 8079 Laboratoire Ecologie, Syst?matique et Evolution

B?t. 362

Universit? Paris-Sud

91405 Orsay cedex

France

 

tel : 01 69 15 56 85

fax : 01 69 15 56 96

mobile : 06 81 70 90 70

 

email : romain.lorrilliere at ese.u-psud.fr

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html




-- 

Romain Lorrilli?re

 

UMR 8079 Laboratoire Ecologie, Syst?matique et Evolution

B?t. 362

Universit? Paris-Sud

91405 Orsay cedex

France

 

tel : 01 69 15 56 85

fax : 01 69 15 56 96

mobile : 06 81 70 90 70

 

email : romain.lorrilliere at ese.u-psud.fr


From murdoch at stats.uwo.ca  Wed Aug 30 16:11:59 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 30 Aug 2006 10:11:59 -0400
Subject: [R] Create a vector from another vector
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>
Message-ID: <44F59CAF.7090201@stats.uwo.ca>

On 8/30/2006 9:49 AM, Doran, Harold wrote:
> Dear list
> 
> Suppose I have the following vector:
> 
> x <-  c(3,4,2,5,6)
> 
> Obviously, this sums to 20. Now, I want to have a second vector, call it
> x2, that sums to x where 5 <= x <= 20, but there are constraints.
> 
> 1) The new vector must be same length as x
> 2) No element of the new vector can be 0
> 3) Element x2[i] of the new vector cannot be larger than element x[i] of
> the original vector
> 4) Ordering is not important
> 
> The following would be an example of what I would want if the user
> wanted the vector x2 to sum to 19
> 
> x2 <- c(2,4,2,5,6) 
> 
> Or, because ordering is not important, this is acceptable
> 
> x2 <- c(3,3,2,5,6) 
> 
> Whereas this would not be appropriate
> 
> x3 <- c(4, 2,2,5,6)
> 
> Because element x3[1] is larger than x[1] even though it sums to 19.

I don't think it's really clear what you mean by "ordering is not 
important".  Would

x2 <- c(6,5,2,4,2)

be acceptable (a re-ordering of your first two examples), even though 
x2[1] > x1[1]?

It's also not clear what result you want in the usual case where there 
are multiple possible answers.  Do you want a randomly chosen one (from 
what distribution?), or would any value satisfying the constraints be 
okay?  (The latter would be easiest:  just start with x1, and decrement 
entries until the desired sum is achieved.  Whether a random value is 
easy or not really depends on the desired distribution.)  Do entries 
need to be integer valued?

> 
> Ideally, the function would take as input the original vector, x, and
> the number that the new vector would sum to. In this example, the vector
> could sum to any number 5 through 20.
> 
> For example,
> 
> myFun <- function(x, sumto) ... details ...
> 
> Is there a preexisiting function that would already do this? I have
> spent too much (unsuccessful) time trying to write a function of my own
> but can't seem to get this to work properly.

I doubt it, because this doesn't look like a standard problem.

Duncan Murdoch

> 
> Any hints would be greatly appreciated.
> 
> Harold
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.hankin at noc.soton.ac.uk  Wed Aug 30 16:13:16 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 30 Aug 2006 15:13:16 +0100
Subject: [R] Create a vector from another vector
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E3B7BAE@dc1ex01.air.org>
Message-ID: <58A5A5F8-0568-4713-B191-9D606FD8D8FA@soc.soton.ac.uk>

I think I've got it now.

If I understand your question,  try:


 > x <- do.call("cbind",lapply(5:7,function(i){restrictedparts(i, 
5,include.zero=FALSE)}))
 > acceptable <- function(x){all(x<=c(3,4,5,2,6))}
 > x[,apply(x,2,acceptable)]
      [,1] [,2] [,3] [,4]
[1,]    1    2    3    2
[2,]    1    1    1    2
[3,]    1    1    1    1
[4,]    1    1    1    1
[5,]    1    1    1    1
 >


rksh



On 30 Aug 2006, at 14:49, Doran, Harold wrote:

> Dear list
>
> Suppose I have the following vector:
>
> x <-  c(3,4,2,5,6)
>
> Obviously, this sums to 20. Now, I want to have a second vector,  
> call it
> x2, that sums to x where 5 <= x <= 20, but there are constraints.
>
> 1) The new vector must be same length as x
> 2) No element of the new vector can be 0
> 3) Element x2[i] of the new vector cannot be larger than element x 
> [i] of
> the original vector
> 4) Ordering is not important
>
> The following would be an example of what I would want if the user
> wanted the vector x2 to sum to 19
>
> x2 <- c(2,4,2,5,6)
>
> Or, because ordering is not important, this is acceptable
>
> x2 <- c(3,3,2,5,6)
>
> Whereas this would not be appropriate
>
> x3 <- c(4, 2,2,5,6)
>
> Because element x3[1] is larger than x[1] even though it sums to 19.
>
> Ideally, the function would take as input the original vector, x, and
> the number that the new vector would sum to. In this example, the  
> vector
> could sum to any number 5 through 20.
>
> For example,
>
> myFun <- function(x, sumto) ... details ...
>
> Is there a preexisiting function that would already do this? I have
> spent too much (unsuccessful) time trying to write a function of my  
> own
> but can't seem to get this to work properly.
>
> Any hints would be greatly appreciated.
>
> Harold
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ggrothendieck at gmail.com  Wed Aug 30 16:18:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Aug 2006 10:18:36 -0400
Subject: [R] Barplot
In-Reply-To: <44F55DBB.7010305@gmail.com>
References: <44F55DBB.7010305@gmail.com>
Message-ID: <971536df0608300718r564081f6v361df9e6aa36151@mail.gmail.com>

Try this.  First we reduce the data to a frequency matrix and
then plot it using classic and then lattice graphics:

zm <- as.matrix(rowsum(z1[-9], z1[,9]))

barplot(zm, beside = TRUE, col = grey.colors(2))
legend("topleft", legend = levels(z1[,9]), fill = grey.colors(2))

library(lattice)
barchart(Freq ~ Var2, as.data.frame.table(zm),
  groups = Var1, origin = 0, auto.key = TRUE)

On 8/30/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> I have a dataset. I want to make barplot from this data.
> Zero1 <- "
>   V1 V2 V3 V4 V5 V6 V7 V8       V9
> 1   1  0  0  0  1  0  0  0 Positive
> 2   0  0  1  0  1  0  1  1 Negative
> 3   0  0  1  0  0  0  1  1 Positive
> 4   0  1  0  1  1  1  0  1 Negative
> 5   0  0  1  0  1  1  0  0 Positive
> 6   0  1  0  0  1  1  1  1 Negative
> 7   1  0  1  1  1  1  1  1 Negative
> 8   0  0  0  0  1  0  0  1 Negative
> 9   0  1  1  1  1  0  0  1 Negative
> 10  0  0  0  1  1  0  1  0 Positive
> 11  0  0  0  0  1  0  0  1 Negative
> 12  0  0  1  1  1  1  1  0 Positive
> 13  0  1  1  0  1  1  1  1 Negative"
>
> z1 <- read.table(textConnection(Zero1), header=TRUE)
> z1
> str(z1)
>
> A simple way I can use mosaic plot
> mosaicplot(table(z1))
> library(vcd)
> mosaic(table(z1))
>
> I have tried to learn ?xtabs ?table and ?ftable but I can't figure out.
> I need a barplot for all variables and the result maybe like
>
> |   |                              |   |
> |   |   | |   |   |            |   |   |
> |pos|neg| |pos|neg|            |pos|neg|
> |   |   | |   |   |            |   |   |
> --------- ---------            ---------
>    v1        v2    v3 .... v7     v8
>
> Thanks you for any helps.
> Regards, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Wed Aug 30 16:22:57 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 30 Aug 2006 10:22:57 -0400
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <644e1f320608300722hbb909e6u7c587ae7d252b292@mail.gmail.com>

use sprintf

> sprintf("%x",123)
[1] "7b"
>



On 9/30/06, Romain Lorrilliere <romain.lorrilliere at ese.u-psud.fr> wrote:
> Hi,
>
> do you know, a method to convert an decimal value (integer) to the
> corresponding hexadecimal value ?
>
> thinks for help.
>
> Romain
>
> --
>
> Lorrilli?re Romain
>
>
>
> UMR 8079 Laboratoire Ecologie, Syst?matique et Evolution
>
> B?t. 362
>
> Universit? Paris-Sud
>
> 91405 Orsay cedex
>
> France
>
>
>
> tel : 01 69 15 56 85
>
> fax : 01 69 15 56 96
>
> mobile : 06 81 70 90 70
>
>
>
> email : romain.lorrilliere at ese.u-psud.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>
> --
>
> Romain Lorrilli?re
>
>
>
> UMR 8079 Laboratoire Ecologie, Syst?matique et Evolution
>
> B?t. 362
>
> Universit? Paris-Sud
>
> 91405 Orsay cedex
>
> France
>
>
>
> tel : 01 69 15 56 85
>
> fax : 01 69 15 56 96
>
> mobile : 06 81 70 90 70
>
>
>
> email : romain.lorrilliere at ese.u-psud.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Wed Aug 30 16:24:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Aug 2006 10:24:29 -0400
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <971536df0608300724r151eb8ccs423d2ac483794445@mail.gmail.com>

Try:

sprintf("%x", 109)


On 9/30/06, Romain Lorrilliere <romain.lorrilliere at ese.u-psud.fr> wrote:
> Hi,
>
> do you know, a method to convert an decimal value (integer) to the
> corresponding hexadecimal value ?
>
> thinks for help.
>
> Romain
>
> --
>
> Lorrilli?re Romain
>
>
>
> UMR 8079 Laboratoire Ecologie, Syst?matique et Evolution
>
> B?t. 362
>
> Universit? Paris-Sud
>
> 91405 Orsay cedex
>
> France
>
>
>
> tel : 01 69 15 56 85
>
> fax : 01 69 15 56 96
>
> mobile : 06 81 70 90 70
>
>
>
> email : romain.lorrilliere at ese.u-psud.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>
> --
>
> Romain Lorrilli?re
>
>
>
> UMR 8079 Laboratoire Ecologie, Syst?matique et Evolution
>
> B?t. 362
>
> Universit? Paris-Sud
>
> 91405 Orsay cedex
>
> France
>
>
>
> tel : 01 69 15 56 85
>
> fax : 01 69 15 56 96
>
> mobile : 06 81 70 90 70
>
>
>
> email : romain.lorrilliere at ese.u-psud.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mel at altk.com  Wed Aug 30 16:26:08 2006
From: mel at altk.com (mel)
Date: Wed, 30 Aug 2006 16:26:08 +0200
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <44F5A000.5050504@altk.com>

?sprintf

ex :
 > sprintf('%X',10)
[1] "A"

hih


From olivesecret at gmail.com  Wed Aug 30 16:26:46 2006
From: olivesecret at gmail.com (Olive Yang)
Date: Wed, 30 Aug 2006 10:26:46 -0400
Subject: [R] function which gives the hessian matrix of the log-likelihood
	of a nonlinear mixed model?
Message-ID: <88e12c7c0608300726h3c5f78b1kdc3cf2d9d0df3c46@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060830/cef92771/attachment.pl 

From singularitaet at gmx.net  Wed Aug 30 16:26:44 2006
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 30 Aug 2006 16:26:44 +0200
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <451E7762.8000307@ese.u-psud.fr>
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <44F5A024.10009@gmx.net>

how about learning to use help.search? (In may you already asked a
similiar question...)

help.search("hexadecimal")

which would lead you to format.hexmode

?format.hexmode



Romain Lorrilliere schrieb:
> Hi,
>
> do you know, a method to convert an decimal value (integer) to the 
> corresponding hexadecimal value ?
>
> thinks for help.
>
> Romain
>
>


From rdbisch at gmail.com  Wed Aug 30 16:27:58 2006
From: rdbisch at gmail.com (Rick Bischoff)
Date: Wed, 30 Aug 2006 10:27:58 -0400
Subject: [R] working with summarized data
Message-ID: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>

The data sets I am working with all have a weight variable--e.g.,  
each row doesn't mean 1 observation.

With that in mind, nearly all of the graphs and summary statistics  
are incorrect for my data, because they don't take into account the  
weight.

****
For example "median" is incorrect, as the quantiles aren't calculated  
with weights:

sum( weights[X < median(X)] ) / sum(weights)

This should be 0.5... of course it's not.
****

Unfortunately, it seems that most(all?) of R's graphics and summary  
statistic functions don't take a weight or frequency argument.    
(Fortunately the models do...)

Am I completely missing how to do this?  One way would be to  
replicate each row proportional to the weight (e.g. if the weight was  
4, we would 3 additional copies) but this will get prohibitive pretty  
quickly as the dataset grows.


Thanks in advance!


From bibiko at eva.mpg.de  Wed Aug 30 16:35:38 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 30 Aug 2006 16:35:38 +0200
Subject: [R] converting decimal - hexadecimal
In-Reply-To: <44F5A000.5050504@altk.com>
References: <451E7762.8000307@ese.u-psud.fr> <44F5A000.5050504@altk.com>
Message-ID: <5E1B281F-CFE0-4BF0-8E2F-D196AF42C7FD@eva.mpg.de>

An other way would be:

a <- 123
class(a) <- "hexmode"
a
[1] "7b"

On 30 Aug 2006, at 16:26, mel wrote:

> ?sprintf
>
> ex :
>> sprintf('%X',10)
> [1] "A"
>
> hih
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

**********************************************************
Hans-Joerg Bibiko
Max Planck Institute for Evolutionary Anthropology
Department of Linguistics
Deutscher Platz 6     phone:   +49 (0) 341 3550 341
D-04103 Leipzig       fax:     +49 (0) 341 3550 333
Germany               e-mail:  bibiko at eva.mpg.de


From efg at stowers-institute.org  Wed Aug 30 16:40:25 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 30 Aug 2006 09:40:25 -0500
Subject: [R] converting decimal - hexadecimal
References: <451E7762.8000307@ese.u-psud.fr>
Message-ID: <ed4812$o7f$1@sea.gmane.org>

"Romain Lorrilliere" <romain.lorrilliere at ese.u-psud.fr> wrote in message 
news:451E7762.8000307 at ese.u-psud.fr...
Hi,

do you know, a method to convert an decimal value (integer) to the
corresponding hexadecimal value ?

Starting in R 2.1.0, sprintf can be used:

> x <- c(0, 65535, 65536, 305419896, 2^31-1)
> y <- sprintf("0x%X", x)
> y
[1] "0x0"        "0xFFFF"     "0x10000"    "0x12345678" "0x7FFFFFFF"
> as.numeric(y)
[1]          0      65535      65536  305419896 2147483647

efg

Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research


From ggrothendieck at gmail.com  Wed Aug 30 16:50:24 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Aug 2006 10:50:24 -0400
Subject: [R] working with summarized data
In-Reply-To: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
References: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
Message-ID: <971536df0608300750w1695f0b1l2748bad9be58820@mail.gmail.com>

In each case, look around (help.search,
RSiteSearch) to see if you can find a function
that handles weights.  For the case you mention,
medians, it can be done via quantile regression:

	x <- w <- 1:5
	library(quantreg)
	coef(rq(x ~ 1, weight = w))

On 8/30/06, Rick Bischoff <rdbisch at gmail.com> wrote:
> The data sets I am working with all have a weight variable--e.g.,
> each row doesn't mean 1 observation.
>
> With that in mind, nearly all of the graphs and summary statistics
> are incorrect for my data, because they don't take into account the
> weight.
>
> ****
> For example "median" is incorrect, as the quantiles aren't calculated
> with weights:
>
> sum( weights[X < median(X)] ) / sum(weights)
>
> This should be 0.5... of course it's not.
> ****
>
> Unfortunately, it seems that most(all?) of R's graphics and summary
> statistic functions don't take a weight or frequency argument.
> (Fortunately the models do...)
>
> Am I completely missing how to do this?  One way would be to
> replicate each row proportional to the weight (e.g. if the weight was
> 4, we would 3 additional copies) but this will get prohibitive pretty
> quickly as the dataset grows.
>
>
> Thanks in advance!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From HDoran at air.org  Wed Aug 30 16:49:56 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 30 Aug 2006 10:49:56 -0400
Subject: [R] Create a vector from another vector
Message-ID: <2323A6D37908A847A7C32F1E3662C80E3B7BCC@dc1ex01.air.org>

Hi Duncan

Here is a bit more detail, this is a bit tough to explain, sorry for not
being clear. Ordering is not important because the vector I am creating
is used as a sufficient statistic in an optimization routine to get some
MLEs. So, any combination of the vector that sums to X is OK. But, the
condition that x2[i] <= x[i] must be maintained. So, the example below
would not work because x2[1] > x[1] as you note below.

> I don't think it's really clear what you mean by "ordering is 
> not important".  Would
> 
> x2 <- c(6,5,2,4,2)
> be acceptable (a re-ordering of your first two examples), 
> even though x2[1] > x1[1]?

To be concrete, the following is the optimization function. This is a
psychometric problem where the goal is to get the MLE for a test taker
conditional on their response pattern (i.e., number of points on the
test) and the item parameters.

pcm.max3 <- function(score, d){
    pcm <- function(theta, d, score)
exp(sum(theta-d[1:score]))/sum(exp(cumsum(theta-d)))
    opt <- function(theta) -sum(log(mapply(pcm, d, theta = theta, score=
score )))
    start_val <- log(sum(score-1)/(length(score-1)/sum(score-1)))
    out <- optim(start_val, opt, method = "BFGS", hessian = TRUE)
    cat('theta is about', round(out$par, 2), ', se',
1/sqrt(out$hes),'\n')
  } 

Suppose we have a three item test. I store the item parameters in a list
as

items <- list(c(0,.5,1), c(0,1), c(0, -1, .5, 1))

We can get the total possible number correct as

(x <- sapply(items, length))
[1] 3 2 4

But, you cannot actually get the MLE for this because the likelihood is
unbounded in this case. 

So, let's say the student scored in the following categories for each
item:

x2 <- c(3,1,4)

By x2[i] <= x[i], I mean that there are 3 possible categories for item 1
above. So, a student can only score in categories 1,2 or 3. He cannot
score in category 4. This is why the condition that x2[i] <= x[i] is
critical. 

But, because total score is a sufficient statistic, (i.e., "ordering is
not important") we could either vector in the function pcm. 

x3 <- c(3,2,3)

Using the function 

pcm.max3(x2, items)
pcm.max3(x3, items)

Gives the same MLE.

But, the vector 

X_bad <- c(4,1,3)

Would not work. You can see that the elements of this vector actually
serve as indices denoting which category a test taker scored in for each
item in the list "items"

I hope this is helpful and appreciate your time.

Harold


> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
>


From r.hankin at noc.soton.ac.uk  Wed Aug 30 17:10:26 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 30 Aug 2006 16:10:26 +0100
Subject: [R] Create a vector from another vector
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E3B7BCC@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E3B7BCC@dc1ex01.air.org>
Message-ID: <421FBE56-688A-4B6D-896B-71A49E68CF4E@soc.soton.ac.uk>

OK,

With this extra detail, a
third solution follows, which may be closer in spirit
to your application.
It may or may not be faster than the other two,
depending on the exact parameters used:


library(partitions)
1+blockparts(n=15,y=c(3,4,2,5,6)-1,include.fewer=T)

(720 distinct solutions)


rksh

On 30 Aug 2006, at 15:49, Doran, Harold wrote:

> Hi Duncan
>
> Here is a bit more detail, this is a bit tough to explain, sorry  
> for not
> being clear. Ordering is not important because the vector I am  
> creating
> is used as a sufficient statistic in an optimization routine to get  
> some
> MLEs. So, any combination of the vector that sums to X is OK. But, the
> condition that x2[i] <= x[i] must be maintained. So, the example below
> would not work because x2[1] > x[1] as you note below.
>
>> I don't think it's really clear what you mean by "ordering is
>> not important".  Would
>>
>> x2 <- c(6,5,2,4,2)
>> be acceptable (a re-ordering of your first two examples),
>> even though x2[1] > x1[1]?
>
> To be concrete, the following is the optimization function. This is a
> psychometric problem where the goal is to get the MLE for a test taker
> conditional on their response pattern (i.e., number of points on the
> test) and the item parameters.
>
> pcm.max3 <- function(score, d){
>     pcm <- function(theta, d, score)
> exp(sum(theta-d[1:score]))/sum(exp(cumsum(theta-d)))
>     opt <- function(theta) -sum(log(mapply(pcm, d, theta = theta,  
> score=
> score )))
>     start_val <- log(sum(score-1)/(length(score-1)/sum(score-1)))
>     out <- optim(start_val, opt, method = "BFGS", hessian = TRUE)
>     cat('theta is about', round(out$par, 2), ', se',
> 1/sqrt(out$hes),'\n')
>   }
>
> Suppose we have a three item test. I store the item parameters in a  
> list
> as
>
> items <- list(c(0,.5,1), c(0,1), c(0, -1, .5, 1))
>
> We can get the total possible number correct as
>
> (x <- sapply(items, length))
> [1] 3 2 4
>
> But, you cannot actually get the MLE for this because the  
> likelihood is
> unbounded in this case.
>
> So, let's say the student scored in the following categories for each
> item:
>
> x2 <- c(3,1,4)
>
> By x2[i] <= x[i], I mean that there are 3 possible categories for  
> item 1
> above. So, a student can only score in categories 1,2 or 3. He cannot
> score in category 4. This is why the condition that x2[i] <= x[i] is
> critical.
>
> But, because total score is a sufficient statistic, (i.e.,  
> "ordering is
> not important") we could either vector in the function pcm.
>
> x3 <- c(3,2,3)
>
> Using the function
>
> pcm.max3(x2, items)
> pcm.max3(x3, items)
>
> Gives the same MLE.
>
> But, the vector
>
> X_bad <- c(4,1,3)
>
> Would not work. You can see that the elements of this vector actually
> serve as indices denoting which category a test taker scored in for  
> each
> item in the list "items"
>
> I hope this is helpful and appreciate your time.
>
> Harold
>
>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From dimitris.rizopoulos at med.kuleuven.be  Wed Aug 30 17:11:06 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 30 Aug 2006 17:11:06 +0200
Subject: [R] Create a vector from another vector
References: <2323A6D37908A847A7C32F1E3662C80E3B7BCC@dc1ex01.air.org>
Message-ID: <010701c6cc46$840f9ab0$0540210a@www.domain>

maybe something like this could be of help:

max.score <- c(3,4,3) # max score for each item
all.pats <- as.matrix(expand.grid(lapply(max.score, ":", 1)))
all.pats[rowSums(all.pats) == 5, ]



Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: "Duncan Murdoch" <murdoch at stats.uwo.ca>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 30, 2006 4:49 PM
Subject: Re: [R] Create a vector from another vector


> Hi Duncan
>
> Here is a bit more detail, this is a bit tough to explain, sorry for 
> not
> being clear. Ordering is not important because the vector I am 
> creating
> is used as a sufficient statistic in an optimization routine to get 
> some
> MLEs. So, any combination of the vector that sums to X is OK. But, 
> the
> condition that x2[i] <= x[i] must be maintained. So, the example 
> below
> would not work because x2[1] > x[1] as you note below.
>
>> I don't think it's really clear what you mean by "ordering is
>> not important".  Would
>>
>> x2 <- c(6,5,2,4,2)
>> be acceptable (a re-ordering of your first two examples),
>> even though x2[1] > x1[1]?
>
> To be concrete, the following is the optimization function. This is 
> a
> psychometric problem where the goal is to get the MLE for a test 
> taker
> conditional on their response pattern (i.e., number of points on the
> test) and the item parameters.
>
> pcm.max3 <- function(score, d){
>    pcm <- function(theta, d, score)
> exp(sum(theta-d[1:score]))/sum(exp(cumsum(theta-d)))
>    opt <- function(theta) -sum(log(mapply(pcm, d, theta = theta, 
> score=
> score )))
>    start_val <- log(sum(score-1)/(length(score-1)/sum(score-1)))
>    out <- optim(start_val, opt, method = "BFGS", hessian = TRUE)
>    cat('theta is about', round(out$par, 2), ', se',
> 1/sqrt(out$hes),'\n')
>  }
>
> Suppose we have a three item test. I store the item parameters in a 
> list
> as
>
> items <- list(c(0,.5,1), c(0,1), c(0, -1, .5, 1))
>
> We can get the total possible number correct as
>
> (x <- sapply(items, length))
> [1] 3 2 4
>
> But, you cannot actually get the MLE for this because the likelihood 
> is
> unbounded in this case.
>
> So, let's say the student scored in the following categories for 
> each
> item:
>
> x2 <- c(3,1,4)
>
> By x2[i] <= x[i], I mean that there are 3 possible categories for 
> item 1
> above. So, a student can only score in categories 1,2 or 3. He 
> cannot
> score in category 4. This is why the condition that x2[i] <= x[i] is
> critical.
>
> But, because total score is a sufficient statistic, (i.e., "ordering 
> is
> not important") we could either vector in the function pcm.
>
> x3 <- c(3,2,3)
>
> Using the function
>
> pcm.max3(x2, items)
> pcm.max3(x3, items)
>
> Gives the same MLE.
>
> But, the vector
>
> X_bad <- c(4,1,3)
>
> Would not work. You can see that the elements of this vector 
> actually
> serve as indices denoting which category a test taker scored in for 
> each
> item in the list "items"
>
> I hope this is helpful and appreciate your time.
>
> Harold
>
>
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible 
>> > code.
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rdbisch at gmail.com  Wed Aug 30 17:30:36 2006
From: rdbisch at gmail.com (Rick Bischoff)
Date: Wed, 30 Aug 2006 11:30:36 -0400
Subject: [R] working with summarized data
In-Reply-To: <f8e6ff050608300800v1ecf59fq6f233676181474ac@mail.gmail.com>
References: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
	<f8e6ff050608300800v1ecf59fq6f233676181474ac@mail.gmail.com>
Message-ID: <F5867361-7417-4020-B3C0-C89DE185617E@gmail.com>

>> Unfortunately, it seems that most(all?) of R's graphics and summary
>> statistic functions don't take a weight or frequency argument.
>> (Fortunately the models do...)
>
> I have been been meaning to add this functionality to my graphics
> package ggplot (http://had.co.nz/ggplot), but unfortunately haven't
> had time yet.  I'm guessing you want something like:
>
> * scatterplot: scale size of point according to weight (can do)
> * bar chart: bars should have height proportional to weight (can do)
> * histogram: area proportion to weighting variable (have some half
> finished code to do)
> * smoothers: should automatically use weights
> * boxplot: use weighted quantiles/letter statistics (is there a
> function for that?)
>
> What else is there?

densityplot is the only other one I can think of at the moment...  
With the rest of those, I could certainly live without it though!

Thanks!


From tlumley at u.washington.edu  Wed Aug 30 19:00:51 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 30 Aug 2006 10:00:51 -0700 (PDT)
Subject: [R] working with summarized data
In-Reply-To: <F5867361-7417-4020-B3C0-C89DE185617E@gmail.com>
References: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
	<f8e6ff050608300800v1ecf59fq6f233676181474ac@mail.gmail.com>
	<F5867361-7417-4020-B3C0-C89DE185617E@gmail.com>
Message-ID: <Pine.LNX.4.64.0608300958090.17647@homer23.u.washington.edu>

On Wed, 30 Aug 2006, Rick Bischoff wrote:

>>> Unfortunately, it seems that most(all?) of R's graphics and summary
>>> statistic functions don't take a weight or frequency argument.
>>> (Fortunately the models do...)
>>
>> I have been been meaning to add this functionality to my graphics
>> package ggplot (http://had.co.nz/ggplot), but unfortunately haven't
>> had time yet.  I'm guessing you want something like:
>>
>> * scatterplot: scale size of point according to weight (can do)
>> * bar chart: bars should have height proportional to weight (can do)
>> * histogram: area proportion to weighting variable (have some half
>> finished code to do)
>> * smoothers: should automatically use weights
>> * boxplot: use weighted quantiles/letter statistics (is there a
>> function for that?)
>>
>> What else is there?
>
> densityplot is the only other one I can think of at the moment...
> With the rest of those, I could certainly live without it though!
>

Density plots, scatterplot smoothers, hexbin plots, bubble plots, 
histograms, and boxplots are available in the survey package. These are 
probability-weighted rather than frequency-weighted but it doesn't matter 
for graphics.  You could use them as is (which requires setting up a 
survey design object) or rip the internals out of them.


 	-thomas


From henrik.parn at bio.ntnu.no  Wed Aug 30 19:02:07 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Wed, 30 Aug 2006 19:02:07 +0200
Subject: [R] lmer applied to a wellknown (?) example
Message-ID: <44F5C48F.9010309@bio.ntnu.no>

Dear all,

During my pre-R era I tried (yes, tried) to understand mixed models by 
working through the 'rat example' in Sokal and Rohlfs Biometry (2000) 
3ed p 288-292. The same example was later used by Crawley (2002) in his 
Statistical Computing p 363-373 and I have seen the same data being used 
elsewhere in the litterature.

Because this example is so thoroughly described, I thought it would be 
very interesting to analyse it also using lmer and to see how the 
different approaches and outputs differs - from the more or less manual 
old-school (?) approach in Sokal, aov in Crawley, and to mixed models by 
lmer.

In the example, three treatments (Treatment) with two rats (Rat) each 
(i.e six unique rats in total). Three liver preparations (Liver) are 
taken from each rat (i.e 18 unique liver preparations), and two glycogen 
readings (Glycogen) are taken from each liver preparation (36 readings).

We want to test if treatments has affected the glycogen levels. The 
readings are nested in preparation and the preparations nested in rats.

The data can be found here (or on p. 289 in Sokal):
http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/rats.txt
//
I was hoping to use the rat example as some kind of reference on my way 
to understand mixed models and using lmer. However, first I wish someone 
could check my suggested models!

My suggestions:

attach(rats)
rats$Glycogen <- as.numeric(Glycogen)
rats$Treatment <- as.factor(Treatment)
rats$Rat <- as.factor(Rat)
rats$Liver <- as.factor(Liver)
str(rats)

model1 <- lmer(Glycogen ~ Treatment + (1|Liver) + (1|Rat), data=rats)
summary(model1)

Was that it?

I also tried to make the 'liver-in-rat' nesting explicit  (as suggested 
in 'Examples from...')
 
model2 <- lmer(Glycogen ~ Treatment + (1|Rat:Liver) + (1|Rat), data=rats)
summary(model2)

but then the random effects differs from model1.

Does the non-unique coding of rats and preparations in the original data 
set mess things up? Do I need to recode the ids to unique levels...

rats$rat2 <- as.factor(rep(1:6, each=6))
rats$liver2 <- as.factor(rep(1:18, each=2))
str(rats)

...and then:

model3 <- lmer(Glycogen ~ Treatment + (1|liver2) + (1|rat2), data=rats)
# or maybe
model3 <- lmer(Glycogen ~ Treatment + (1|rat2:liver2) + (1|rat2), data=rats)
 

Can anyone help me to get this right! Thanks in advance!

P.S.
Thanks to all contributors to lme/lmer topic on the list (yes, I have 
searched for 'lmer nested'...) and also the examples provided by John 
Fox' 'Linear mixed models, Appendix to An R and S-PLUS companion...' and 
Douglas Bates' 'Examples from Multilevel Software...' and R-news 5/1. 
Very helpful, but as usually I bet I missed something...Sorry.

Regards,

Henrik 

-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)


From miltinho_astronauta at yahoo.com.br  Wed Aug 30 19:16:20 2006
From: miltinho_astronauta at yahoo.com.br (Milton Cezar)
Date: Wed, 30 Aug 2006 14:16:20 -0300 (ART)
Subject: [R] bootstrap for group and subgroup
Message-ID: <20060830171620.57607.qmail@web53405.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060830/fce39a6b/attachment.pl 

From stubben at lanl.gov  Wed Aug 30 19:17:56 2006
From: stubben at lanl.gov (Chris Stubben)
Date: Wed, 30 Aug 2006 17:17:56 +0000 (UTC)
Subject: [R] Bootstraping for groups and subgroups and joing with other
	table
References: <20060829011359.3965.qmail@web53405.mail.yahoo.com>
Message-ID: <loom.20060830T191500-949@post.gmane.org>

>   I have a table with following collumns: State, SamplePlot, Species and
BodySize. I sampled bird species at
> 34 SamplePlots and 5 States (regions) monthly during two years. On each bird
record I measured bodysize
> and identified the species. So I have many records of each species (about 150
species) at each SamplePlot
> and each Region (State). 
> 
>   Now I would like bootstrap these data, selecting 50 records for each
State/SamplePlot combinations and
> count how many species (richness) were sampled at bootstrap. I need to do this
1.000 times. 
> 
>   After that and need join the number of species [obtained at each bootstrap
and for each State/SamplePlot
> combination] with a dataframe that have other attributes for SamplePlot (like
Area, Perimeter etc). 



I asked a similar question earlier... IF you have data frame birds and
bird.plots, maybe something like this.

#initialize empty variable 
boot<-NULL

for(i in 1:100)
{
   ## split on state and site and create list a
   a <-split(birds, paste(birds$State,birds$SampleSite), drop=T)

   # sample 50 rows each OR by number of observations (better?)
   # b<-lapply( a, function(x) x[sample(nrow(x), 50, replace=T),])
     b<-lapply( a, function(x) x[sample(nrow(x), replace=T),])

   ## count number of unique species or other statistic? 
   ###  and add row to boot matrix
   boot<-rbind(boot, unlist( lapply(b, function (x) length(unique(x$Species)) ) ))
}

## mean

y<-apply(boot, 2, mean)


## convert to data frame for merge

y<-data.frame(y)

names(y)<-"boot.count"


## add row names to bird.plots for easy join
rownames(bird.plots)<-paste(bird.plots$State,bird.plots$SampleSite)

merge(bird.plots,y, by=0)

          Row.names      State SampleSite Area boot.count
1       Bahia Site1      Bahia      Site1   10       1.00
2       Bahia Site2      Bahia      Site2   25       1.96
3       Bahia Site3      Bahia      Site3   70       1.72
4       Bahia Site4      Bahia      Site4   15       1.73
5       Bahia Site5      Bahia      Site5    5       1.42
6  RioJaneiro Site1 RioJaneiro      Site1   32       2.49
7  RioJaneiro Site2 RioJaneiro      Site2   45       1.63
8  RioJaneiro Site3 RioJaneiro      Site3   10       2.37
9    SaoPaulo Site1   SaoPaulo      Site1   23       2.41
10   SaoPaulo Site2   SaoPaulo      Site2   45       2.57


Chris Stubben


From msubianto at gmail.com  Wed Aug 30 19:43:06 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 30 Aug 2006 19:43:06 +0200
Subject: [R] Barplot
In-Reply-To: <971536df0608300718r564081f6v361df9e6aa36151@mail.gmail.com>
References: <44F55DBB.7010305@gmail.com>
	<971536df0608300718r564081f6v361df9e6aa36151@mail.gmail.com>
Message-ID: <44F5CE2A.6090509@gmail.com>

Dear all,

To Gabor Grothendieck,  (again) thanks you very much for your help.
Now, I can play around with lattice package.

Best, Muhammad Subianto

#Gabor
#reduce the data to a frequency matrix and
#then plot it using classic and then lattice graphics:

zm <- as.matrix(rowsum(z1[-9], z1[,9]))

barplot(zm, beside = TRUE, col = grey.colors(2))
legend("topleft", legend = levels(z1[,9]), fill = grey.colors(2))

library(lattice)
barchart(Freq ~ Var2, as.data.frame.table(zm),
   groups = Var1, origin = 0, auto.key = TRUE)


On this day 30/08/2006 16:18, Gabor Grothendieck wrote:
> Try this.  First we reduce the data to a frequency matrix and
> then plot it using classic and then lattice graphics:
> 
> zm <- as.matrix(rowsum(z1[-9], z1[,9]))
> 
> barplot(zm, beside = TRUE, col = grey.colors(2))
> legend("topleft", legend = levels(z1[,9]), fill = grey.colors(2))
> 
> library(lattice)
> barchart(Freq ~ Var2, as.data.frame.table(zm),
>   groups = Var1, origin = 0, auto.key = TRUE)
> 
> On 8/30/06, Muhammad Subianto <msubianto at gmail.com> wrote:
>> Dear all,
>> I have a dataset. I want to make barplot from this data.
>> Zero1 <- "
>>   V1 V2 V3 V4 V5 V6 V7 V8       V9
>> 1   1  0  0  0  1  0  0  0 Positive
>> 2   0  0  1  0  1  0  1  1 Negative
>> 3   0  0  1  0  0  0  1  1 Positive
>> 4   0  1  0  1  1  1  0  1 Negative
>> 5   0  0  1  0  1  1  0  0 Positive
>> 6   0  1  0  0  1  1  1  1 Negative
>> 7   1  0  1  1  1  1  1  1 Negative
>> 8   0  0  0  0  1  0  0  1 Negative
>> 9   0  1  1  1  1  0  0  1 Negative
>> 10  0  0  0  1  1  0  1  0 Positive
>> 11  0  0  0  0  1  0  0  1 Negative
>> 12  0  0  1  1  1  1  1  0 Positive
>> 13  0  1  1  0  1  1  1  1 Negative"
>>
>> z1 <- read.table(textConnection(Zero1), header=TRUE)
>> z1
>> str(z1)
>>
>> A simple way I can use mosaic plot
>> mosaicplot(table(z1))
>> library(vcd)
>> mosaic(table(z1))
>>
>> I have tried to learn ?xtabs ?table and ?ftable but I can't figure out.
>> I need a barplot for all variables and the result maybe like
>>
>> |   |                              |   |
>> |   |   | |   |   |            |   |   |
>> |pos|neg| |pos|neg|            |pos|neg|
>> |   |   | |   |   |            |   |   |
>> --------- ---------            ---------
>>    v1        v2    v3 .... v7     v8
>>
>> Thanks you for any helps.
>> Regards, Muhammad Subianto


From prasannaprakash at gmail.com  Wed Aug 30 19:58:14 2006
From: prasannaprakash at gmail.com (Prasanna)
Date: Wed, 30 Aug 2006 19:58:14 +0200
Subject: [R] Ranking and selection statistical procedure
Message-ID: <fc5b8ae70608301058n50691504h79e97798a66fc57a@mail.gmail.com>

Dear R helpers

I would like to know if the "Ranking and Selection" statistical
procedure has been implemented in R. I made a quick search in the R
packages list but I could not find it.

Thanks in advance
Prasanna


From Greg.Snow at intermountainmail.org  Wed Aug 30 17:35:23 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 30 Aug 2006 09:35:23 -0600
Subject: [R] Producing R demos
Message-ID: <07E228A5BE53C24CAD490193A7381BBB590ED5@LP-EXCHVS07.CO.IHC.COM>

One option is to use VNC along with vncrec to do the recording (see the
website:  http://www.sodan.org/~penny/vncrec/).  I think there are some
other recorders also available for vnc, so you might try a google
search.


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gregor Gorjanc
Sent: Tuesday, August 29, 2006 3:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Producing R demos

Hello!

I have found terrific demo or R package functionality at

http://had.co.nz/reshape/french-fries-demo.html

Author has told me off-list that he is using SnapzProX (on mac), and
ghostwriter (http://had.co.nz/ghostwriter/) to automate the typing.
Unfortunatelly, I do not have mac ;) Can anyone on the list suggest, how
such a demo (video + automated typing of a script) could be produced
under either Windows or Linux OS? I would like to create such a demo for
presentation as I will not be able to install R on the machine, but I
could play a movie.

Thanks!

--
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
you have no certainty until you try." Sophocles ~ 450 B.C.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Greg.Snow at intermountainmail.org  Wed Aug 30 18:08:07 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 30 Aug 2006 10:08:07 -0600
Subject: [R] density() with from, to or cut and comparrison of density()
Message-ID: <07E228A5BE53C24CAD490193A7381BBB590EF2@LP-EXCHVS07.CO.IHC.COM>

You may want to look at the logspline package, it uses a different
technique than density does, but it estimates densities and allows you
to tell the routine that there is a minimum value and that the density
does not extend beyond there.

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rainer M Krug
Sent: Wednesday, August 30, 2006 4:27 AM
To: R help list
Subject: [R] density() with from, to or cut and comparrison of density()

Hi

the function density() does normally integrate to one - I've checked it
and it works and I also read the previous threads.
But I realised that it does not integrate to one if I use from, to or
cut.

My scenario: I simulated densities of a plants originating from an sseed
source at distance zero. Therefore the density of the plants will be
highest close to zero. Is there anything I can do to have this pattern?
If I use 'from' or 'cut', the resulting densities do not integrate to
one which I need as I want to compare different density curves.

Ny second question is concerning the bandwidth. An I correct in saying
that if I want to compare different density estimates that the bandwidth
should be the same for all of them?

Thanks in advance for your help,

Rainer

--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology and Entomology University of
Stellenbosch Matieland 7602 South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Greg.Snow at intermountainmail.org  Wed Aug 30 18:28:18 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 30 Aug 2006 10:28:18 -0600
Subject: [R] working with summarized data
Message-ID: <07E228A5BE53C24CAD490193A7381BBB590F04@LP-EXCHVS07.CO.IHC.COM>

There are functions to do weighted summary statistics in the Hmisc
package (wtd.quantile, ...).

For more complicated analyses (but not plots yet) the biglm package has
a bigglm function that expects the data in chunks, you could write a
function that expand parts of the dataset at a time.

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rick Bischoff
Sent: Wednesday, August 30, 2006 8:28 AM
To: r-help at stat.math.ethz.ch
Subject: [R] working with summarized data

The data sets I am working with all have a weight variable--e.g., each
row doesn't mean 1 observation.

With that in mind, nearly all of the graphs and summary statistics are
incorrect for my data, because they don't take into account the weight.

****
For example "median" is incorrect, as the quantiles aren't calculated
with weights:

sum( weights[X < median(X)] ) / sum(weights)

This should be 0.5... of course it's not.
****

Unfortunately, it seems that most(all?) of R's graphics and summary  
statistic functions don't take a weight or frequency argument.    
(Fortunately the models do...)

Am I completely missing how to do this?  One way would be to replicate
each row proportional to the weight (e.g. if the weight was 4, we would
3 additional copies) but this will get prohibitive pretty quickly as the
dataset grows.


Thanks in advance!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Han.Lee at GeodeCapital.com  Wed Aug 30 22:45:12 2006
From: Han.Lee at GeodeCapital.com (Lee, Han)
Date: Wed, 30 Aug 2006 16:45:12 -0400
Subject: [R] Debugging with  gdb
Message-ID: <E876A7185C93844A8663E9B5DCF070B701DD8676@MSGMROCLQ2WIN.DMN1.FMR.COM>

I tried to run gdb in linux with emacs
But could not even run a simple example in the writing extensions
tutorial.
The execution history is as follows.
Gdb worked fine for other debugging such as C++ codes.

Thanks 
Han



I started R at echo of emacs by typing 
(also tried other methods mentioned in the tutorial both in emacs and
xterm.)
M-x gdb    ==> R -d gdb

(gdb) run
Starting program: /home/gcmio/local.20060808/lib/R/bin/exec/R -cd
/home/a409791/R/R-Test/ -fullname
[Thread debugging using libthread_db enabled]
[New Thread -1218514176 (LWP 11086)]
WARNING: unknown option '-cd'

ARGUMENT '/home/a409791/R/R-Test/' __ignored__

WARNING: unknown option '-fullname'


R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.3.1 (2006-06-01)
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

>
Program received signal SIGINT, Interrupt.
[Switching to Thread -1218514176 (LWP 11086)]
0x00366c58 in ___newselect_nocancel () from /lib/tls/libc.so.6
(gdb) b do_get
Breakpoint 1 at 0x80ca4f5: file envir.c, line 1615.
(gdb) signal 0
Continuing with no signal.

> x <- 1
> get("x")

Breakpoint 1, do_get (call=0x9443878, op=0x934bd54, args=0x9416408,
rho=0x9417a54)
    at envir.c:1615
1615	    checkArity(op, args);
(gdb) p $1
History has not yet reached $1.
(gdb) p R_PV(x)
No symbol "x" in current context.
(gdb)


From laurentRhelp at free.fr  Wed Aug 30 23:50:45 2006
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Wed, 30 Aug 2006 23:50:45 +0200
Subject: [R] lattice and several groups
In-Reply-To: <f8e6ff050608291447s771ed4aana347c05f3650e960@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>
	<f8e6ff050608291447s771ed4aana347c05f3650e960@mail.gmail.com>
Message-ID: <44F60835.7040008@free.fr>

hadley wickham a ?crit :

>>      I would like to use the lattice library to show several groups on
>> the same graph. Here's my example :
>>
>> ## the data
>> f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
>> f1 <- rep(f1,3)
>> f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
>> df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), 
>> x=rep(c(1,2,3),3),f1=f1,f2=f2)
>
>
> It's pretty easy to do this with ggplot:
>
> install.packages("ggplot", repos="http://ggobi.org/r/")
> library(ggplot)
> qplot(x, val, data=df, shape=f2, colour=f1)
>
> Hadley
>
>
Great, Hadley, but the code was not exactly the good one. The code below 
works fine for me :

install.packages("ggplot", repos="http://ggobi.org/r/")
library(ggplot)
qplot(x, val, data=df, glyph=f1, col=f2)

In fact, my problem is to fit the data for every level of the f2 factor, 
showing the levels of the f1 factor and that for several surveys . 
Here's an example closer to my actual data :

## the data

n <- 18
x1 <- seq(1,n)
val1 <- -2*x1+50
val2 <- (-2*(x1-8)^2)+100
val3 <- (-2*(x1-8)^2)+50
y <- c(val1,val2,val3)
x <- rep(x1,3)
f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
f1 <- rep(f1,3)
f2 <- rep(c("g1","g2","g3"),each=n)
df <- data.frame(x=x,y=y,f1=f1,f2=f2)

surveys <- 
factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
df <- rbind(df,df,df)
df <- data.frame(df,surveys=surveys)
#######################################################################
library(lattice)

para.liste <- trellis.par.get()
superpose.symbol <- para.liste$superpose.symbol
superpose.symbol$pch <- c(1,2,3)
trellis.par.set("superpose.symbol",superpose.symbol)

xyplot( y~x | surveys,  
        data=df,
        group=f1,
        auto.key=list(space="right")
       )

xyplot( y~x | surveys  ,
        data=df,
        type="l",
        group=f2,
        auto.key=list(space="right",points=FALSE,lines=TRUE)
       )

Can I use the ggplot library  ?


From rolf at erdos.math.unb.ca  Wed Aug 30 23:49:47 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Wed, 30 Aug 2006 18:49:47 -0300 (ADT)
Subject: [R] .Rprofile under Windoze.
Message-ID: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>

I am (for my sins) having to do some work using R under Windoze.  I
wanted to set up a .Rprofile to control my set-up.  The docs on
.Rprofile say that it can/should be placed in ``the user's home
directory''.  ``An Introduction to R'' observes lucidly that this
concept needs to be clarified under Windoze.

Following the suggestions in An Introduction to R, I tried
putting a .Rprofile in

	"C:\Documents and Settings\rolf\My Documents"

When that didn't work, I tried putting it in the starting directory
(and confirmed that I'd got that right by checking with getwd() and
list.files(all.files=TRUE) ).

The last invocation indicated that the name of the file was *really*
``.Rprofile.txt'' --- although I'd tried to save it as (simply)
``.Rprofile''.  Is that the problem?  If so, how can I persuade
Windoze NOT to stick that damned .txt tag on the end?  (Gawd, but I
***hate*** Windoze!!!)  If that's not the problem, can you suggest
what *is* the problem?

All that .Rprofile(.txt) has in it at the moment is

	options(prompt="Wheee! ")

so that I can easily tell whether it's working.  If I
execute

	> source(".Rprofile.txt")

the prompt does indeed get changed to ``Wheee! '' as it should.

I would appreciate enlightenment.  Ta.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From laurentRhelp at free.fr  Thu Aug 31 00:05:04 2006
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Thu, 31 Aug 2006 00:05:04 +0200
Subject: [R] lattice and several groups
In-Reply-To: <971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
Message-ID: <44F60B90.6070608@free.fr>

Gabor Grothendieck a ?crit :

> Try this:
>
> xyplot(val ~ x, data = df, type = "p",
>     col = as.numeric(df$f1), pch = as.numeric(df$f2))
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>     points = list(pch = 1:nlevels(df$f1))
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>     points = list(pch = 20, col = 1:nlevels(df$f2))
> )
>
> trellis.focus("panel", 1, 1)
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> trellis.unfocus()
>
>
> On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>
>> Dear R-list,
>>
>>     I would like to use the lattice library to show several groups on
>> the same graph. Here's my example :
>>
>> ## the data
>> f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
>> f1 <- rep(f1,3)
>> f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
>> df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), 
>> x=rep(c(1,2,3),3),f1=f1,f2=f2)
>> #############################################################
>> library(lattice)
>>
>> para.liste <- trellis.par.get()
>> superpose.symbol <- para.liste$superpose.symbol
>> superpose.symbol$pch <- c(1,2,3)
>> trellis.par.set("superpose.symbol",superpose.symbol)
>>
>> # Now I can see the group according to the f1 factor (with a different
>> symbol for every modality)
>> xyplot( val~x,
>>        data=df,
>>        group=f1,
>>        auto.key=list(space="right")
>>       )
>>
>> # or I can see the group according to the f2 factor
>> xyplot( val~x,
>>        data=df,
>>        type="l",
>>        group=f2,
>>        auto.key=list(space="right",points=FALSE,lines=TRUE)
>>       )
>>
>> How can I do to highlight both the f1 and f2 factors on one panel with
>> the legends, using the lattice function ?
>>
>> Thanks
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
Thank you, Gabor. The way to put the two legends is very interesting. 
For the graphs, in fact, my problem is to fit the data for every level 
of the f2 factor, showing the levels of the f1 factor in each panel and 
that for several surveys . Here's an example closer to my actual data :

## the data

n <- 18
x1 <- seq(1,n)
val1 <- -2*x1+50
val2 <- (-2*(x1-8)^2)+100
val3 <- (-2*(x1-8)^2)+50
y <- c(val1,val2,val3)
x <- rep(x1,3)
f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
f1 <- rep(f1,3)
f2 <- rep(c("g1","g2","g3"),each=n)
df <- data.frame(x=x,y=y,f1=f1,f2=f2)

surveys <- 
factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
df <- rbind(df,df,df)
df <- data.frame(df,surveys=surveys)
#######################################################################
library(lattice)

para.liste <- trellis.par.get()
superpose.symbol <- para.liste$superpose.symbol
superpose.symbol$pch <- c(1,2,3)
trellis.par.set("superpose.symbol",superpose.symbol)

xyplot( y~x | surveys,  
        data=df,
        group=f1,
        auto.key=list(space="right")
       )

xyplot( y~x | surveys  ,
        data=df,
        type="l",
        group=f2,
        auto.key=list(space="right",points=FALSE,lines=TRUE)
       )

Certainly, I have to use the panel function but I don't know how to mark 
the f1 factor in each panel !


From laurentRhelp at free.fr  Thu Aug 31 00:11:56 2006
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Thu, 31 Aug 2006 00:11:56 +0200
Subject: [R] lattice and several groups
In-Reply-To: <971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>
Message-ID: <44F60D2C.5050201@free.fr>

Gabor Grothendieck a ?crit :

>Note that before entering this you need:
>
>library(lattice)
>library(grid) # to access the viewport function
>
>On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>  
>
>>Try this:
>>
>>xyplot(val ~ x, data = df, type = "p",
>>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
>>
>>key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>>       points = list(pch = 1:nlevels(df$f1))
>>)
>>
>>key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>>       points = list(pch = 20, col = 1:nlevels(df$f2))
>>)
>>
>>trellis.focus("panel", 1, 1)
>>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
>>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>>trellis.unfocus()
>>
>>
>>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
>>    
>>
>>>Dear R-list,
>>>
>>>    I would like to use the lattice library to show several groups on
>>>the same graph. Here's my example :
>>>
>>>## the data
>>>f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
>>>f1 <- rep(f1,3)
>>>f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
>>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
>>>#############################################################
>>>library(lattice)
>>>
>>>para.liste <- trellis.par.get()
>>>superpose.symbol <- para.liste$superpose.symbol
>>>superpose.symbol$pch <- c(1,2,3)
>>>trellis.par.set("superpose.symbol",superpose.symbol)
>>>
>>># Now I can see the group according to the f1 factor (with a different
>>>symbol for every modality)
>>>xyplot( val~x,
>>>       data=df,
>>>       group=f1,
>>>       auto.key=list(space="right")
>>>      )
>>>
>>># or I can see the group according to the f2 factor
>>>xyplot( val~x,
>>>       data=df,
>>>       type="l",
>>>       group=f2,
>>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
>>>      )
>>>
>>>How can I do to highlight both the f1 and f2 factors on one panel with
>>>the legends, using the lattice function ?
>>>
>>>Thanks
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>>
>>>      
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>
Thank you, Gabor. The way to put the two legends is very interesting. 
For the graphs, in fact, my problem is to fit the data for every level 
of the f2 factor, showing the levels of the f1 factor in each panel and 
that for several surveys . Here's an example closer to my actual data :

## the data

n <- 18
x1 <- seq(1,n)
val1 <- -2*x1+50
val2 <- (-2*(x1-8)2)+100
val3 <- (-2*(x1-8)2)+50
y <- c(val1,val2,val3)
x <- rep(x1,3)
f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
f1 <- rep(f1,3)
f2 <- rep(c("g1","g2","g3"),each=n)
df <- data.frame(x=x,y=y,f1=f1,f2=f2)

surveys <- 
factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
df <- rbind(df,df,df)
df <- data.frame(df,surveys=surveys)
#######################################################################
library(lattice)

para.liste <- trellis.par.get()
superpose.symbol <- para.liste$superpose.symbol
superpose.symbol$pch <- c(1,2,3)
trellis.par.set("superpose.symbol",superpose.symbol)

xyplot( y~x | surveys,         data=df,
       group=f1,
       auto.key=list(space="right")
      )

xyplot( y~x | surveys  ,
       data=df,
       type="l",
       group=f2,
       auto.key=list(space="right",points=FALSE,lines=TRUE)
      )

Certainly, I have to use the panel function but I don't know how to mark 
the f1 factor in each panel (I want to fit the values according to the 
f2 factor) !


From bbands at gmail.com  Thu Aug 31 00:04:54 2006
From: bbands at gmail.com (BBands)
Date: Wed, 30 Aug 2006 15:04:54 -0700
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>
References: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>
Message-ID: <6e8360ad0608301504h7fac0314r9e64816d0159ded2@mail.gmail.com>

On 8/30/06, Rolf Turner <rolf at erdos.math.unb.ca> wrote:
> I am (for my sins) having to do some work using R under Windoze.  I
> wanted to set up a .Rprofile to control my set-up.  The docs on
> .Rprofile say that it can/should be placed in ``the user's home
> directory''.  ``An Introduction to R'' observes lucidly that this
> concept needs to be clarified under Windoze.

Saving ".Rprofile" to the R root directory works for me.

On my system that is "C:\Program Files\R\R-2.3.1\"

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From Charles.Annis at StatisticalEngineering.com  Thu Aug 31 00:06:43 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 30 Aug 2006 18:06:43 -0400
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>
Message-ID: <02a201c6cc80$9551bd00$6400a8c0@DD4XFW31>

Under Windows mine is located here

C:\Program Files\R\R-2.3.1\library\base\R

The file name, however is not .Rprofile, but rather Rprofile


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rolf Turner
Sent: Wednesday, August 30, 2006 5:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] .Rprofile under Windoze.

I am (for my sins) having to do some work using R under Windoze.  I
wanted to set up a .Rprofile to control my set-up.  The docs on
.Rprofile say that it can/should be placed in ``the user's home
directory''.  ``An Introduction to R'' observes lucidly that this
concept needs to be clarified under Windoze.

Following the suggestions in An Introduction to R, I tried
putting a .Rprofile in

	"C:\Documents and Settings\rolf\My Documents"

When that didn't work, I tried putting it in the starting directory
(and confirmed that I'd got that right by checking with getwd() and
list.files(all.files=TRUE) ).

The last invocation indicated that the name of the file was *really*
``.Rprofile.txt'' --- although I'd tried to save it as (simply)
``.Rprofile''.  Is that the problem?  If so, how can I persuade
Windoze NOT to stick that damned .txt tag on the end?  (Gawd, but I
***hate*** Windoze!!!)  If that's not the problem, can you suggest
what *is* the problem?

All that .Rprofile(.txt) has in it at the moment is

	options(prompt="Wheee! ")

so that I can easily tell whether it's working.  If I
execute

	> source(".Rprofile.txt")

the prompt does indeed get changed to ``Wheee! '' as it should.

I would appreciate enlightenment.  Ta.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Thu Aug 31 00:07:06 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 30 Aug 2006 17:07:06 -0500
Subject: [R] lattice and several groups
In-Reply-To: <44F60835.7040008@free.fr>
References: <44F4A0FF.2010708@free.fr>
	<f8e6ff050608291447s771ed4aana347c05f3650e960@mail.gmail.com>
	<44F60835.7040008@free.fr>
Message-ID: <f8e6ff050608301507x8f277e5vf60ff99ede55c1a1@mail.gmail.com>

> In fact, my problem is to fit the data for every level of the f2 factor,
> showing the levels of the f1 factor and that for several surveys .
> Here's an example closer to my actual data :

Then maybe you want:
qplot(x, y, . ~ surveys, data=df, type="line", colour=f1, id=f2, size=f2)

(which doesn't produce a very nice legend)

Or to build up piece by piece
p <- ggplot(df, . ~ surveys, aes=list(x=x, y=y, colour=f1, id=f2))
p <- ggline(p)
ggpoint(p, aes=list(shape=f2))

(you might want to flip shape and colour around to get what you want)

Hadley


From bbands at gmail.com  Thu Aug 31 00:11:35 2006
From: bbands at gmail.com (BBands)
Date: Wed, 30 Aug 2006 15:11:35 -0700
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <6e8360ad0608301504h7fac0314r9e64816d0159ded2@mail.gmail.com>
References: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>
	<6e8360ad0608301504h7fac0314r9e64816d0159ded2@mail.gmail.com>
Message-ID: <6e8360ad0608301511j3375b3a1m4b080352923412fa@mail.gmail.com>

And... If you have/use shortcuts to R, you may also save an
".Rprofile" to whatever directory you name in the "Start in:" field of
the shortcut. This allows one to have many profiles.

       jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From mr.spoon21 at gmail.com  Thu Aug 31 00:28:15 2006
From: mr.spoon21 at gmail.com (Carlo Trimarchi)
Date: Thu, 31 Aug 2006 00:28:15 +0200
Subject: [R] how to read just a column
Message-ID: <8f67b6f80608301528l36c802c9ie4e32afc26c43e5@mail.gmail.com>

Hi,
how can I read, using for example read.table() or scan(), just one
column from a text file that has more columns without any header?

Thanks, bye.


From NordlDJ at dshs.wa.gov  Thu Aug 31 01:35:04 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS))
Date: Wed, 30 Aug 2006 16:35:04 -0700
Subject: [R] .Rprofile under Windoze.
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED979@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Rolf Turner
> Sent: Wednesday, August 30, 2006 2:50 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] .Rprofile under Windoze.
> 
> I am (for my sins) having to do some work using R under Windoze.  I
> wanted to set up a .Rprofile to control my set-up.  The docs on
> .Rprofile say that it can/should be placed in ``the user's home
> directory''.  ``An Introduction to R'' observes lucidly that this
> concept needs to be clarified under Windoze.
> 
> Following the suggestions in An Introduction to R, I tried
> putting a .Rprofile in
> 
> 	"C:\Documents and Settings\rolf\My Documents"
> 
> When that didn't work, I tried putting it in the starting directory
> (and confirmed that I'd got that right by checking with getwd() and
> list.files(all.files=TRUE) ).
> 
> The last invocation indicated that the name of the file was *really*
> ``.Rprofile.txt'' --- although I'd tried to save it as (simply)
> ``.Rprofile''.  Is that the problem?  If so, how can I persuade
> Windoze NOT to stick that damned .txt tag on the end?  (Gawd, but I
> ***hate*** Windoze!!!)  If that's not the problem, can you suggest
> what *is* the problem?
> 
<<<snip>>>

Rolf,

Whether an extension is automagically added (and if so what) is usually a
function of the program writing the file out.  In MS Windows programs, there
is usually an option in the Save/SaveAs menu called something like "Save As
Type".  To save without an extension you want to make sure that the value is
'All Files (*.*)', otherwise the program will usually tag on a default
extension.

Hope this is helpful,

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From murdoch at stats.uwo.ca  Thu Aug 31 02:01:38 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 30 Aug 2006 20:01:38 -0400
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>
References: <200608302149.k7ULnlfM002496@erdos.math.unb.ca>
Message-ID: <44F626E2.5000204@stats.uwo.ca>

On 8/30/2006 5:49 PM, Rolf Turner wrote:
> I am (for my sins) having to do some work using R under Windoze.  I
> wanted to set up a .Rprofile to control my set-up.  The docs on
> .Rprofile say that it can/should be placed in ``the user's home
> directory''.  ``An Introduction to R'' observes lucidly that this
> concept needs to be clarified under Windoze.
> 
> Following the suggestions in An Introduction to R, I tried
> putting a .Rprofile in
> 
> 	"C:\Documents and Settings\rolf\My Documents"

That's probably the right place.  You can confirm in the R Edit|GUI 
preferences dialog:  it defaults to trying to save or load from the same 
place it would look for .Rprofile.

> 
> When that didn't work, I tried putting it in the starting directory
> (and confirmed that I'd got that right by checking with getwd() and
> list.files(all.files=TRUE) ).
> 
> The last invocation indicated that the name of the file was *really*
> ``.Rprofile.txt'' --- although I'd tried to save it as (simply)
> ``.Rprofile''.  Is that the problem?  If so, how can I persuade
> Windoze NOT to stick that damned .txt tag on the end?  

You can put it in double quotes.  It's really your editor doing this; 
most reasonable editors (but not the ones that come with Windows) are 
configurable to never add the extension.

Another change to Windows defaults that's essential to maintain sanity 
is to tell it to display full filenames, not to hide the .txt in the 
first place.  You do this in an Explorer window by clicking on 
"Tools|Folder options...|View|Hide extensions for known file types" 
(where each of the 4 components in that path is a different kind of 
interface element.  I love the rich Windows user interface!)

Duncan Murdoch

(Gawd, but I
> ***hate*** Windoze!!!)  If that's not the problem, can you suggest
> what *is* the problem?
> 
> All that .Rprofile(.txt) has in it at the moment is
> 
> 	options(prompt="Wheee! ")
> 
> so that I can easily tell whether it's working.  If I
> execute
> 
> 	> source(".Rprofile.txt")
> 
> the prompt does indeed get changed to ``Wheee! '' as it should.
> 
> I would appreciate enlightenment.  Ta.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Thu Aug 31 02:27:10 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 30 Aug 2006 20:27:10 -0400
Subject: [R] how to read just a column
In-Reply-To: <8f67b6f80608301528l36c802c9ie4e32afc26c43e5@mail.gmail.com>
References: <8f67b6f80608301528l36c802c9ie4e32afc26c43e5@mail.gmail.com>
Message-ID: <644e1f320608301727n7d6f277q998a4f1ad2c2667b@mail.gmail.com>

you can read them all in and delete the ones you don't want.  Or check
out 'what' on 'scan' or colClasses on 'read.table'

On 8/30/06, Carlo Trimarchi <mr.spoon21 at gmail.com> wrote:
> Hi,
> how can I read, using for example read.table() or scan(), just one
> column from a text file that has more columns without any header?
>
> Thanks, bye.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Thu Aug 31 03:44:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Aug 2006 21:44:22 -0400
Subject: [R] lattice and several groups
In-Reply-To: <44F60D2C.5050201@free.fr>
References: <44F4A0FF.2010708@free.fr>
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>
	<44F60D2C.5050201@free.fr>
Message-ID: <971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>

To handle conditioning on survey we provide a panel function
that subsets col and pch:

# define test data - df

# note that your val2 and val3 lines had a syntax
# so we have commented them out and
# replaced them as shown.
n <- 18
x1 <- seq(1,n)
val1 <- -2*x1+50
# val2 <- (-2*(x1-8)2)+100
val2 <- (-2*(x1-8))+100
# val3 <- (-2*(x1-8)2)+50
val3 <- (-2*(x1-8))+50
y <- c(val1,val2,val3)
x <- rep(x1,3)
f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
f1 <- rep(f1,3)
f2 <- rep(c("g1","g2","g3"),each=n)
df <- data.frame(x=x,y=y,f1=f1,f2=f2)
surveys <-
factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
df <- rbind(df,df,df)
df <- data.frame(df,surveys=surveys)

# create xyplot

library(lattice)
library(grid)

pnl <- function(x, y, groups, subscripts, col, pch, ...)
	panel.xyplot(x, y, col = col[subscripts], pch = pch[subscripts], ...)

xyplot(y ~ x | surveys, data = df,
	col = as.numeric(df$f1), pch = as.numeric(df$f2), panel = pnl)


key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
       points = list(pch = 1:nlevels(df$f1))
)

key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
       points = list(pch = 20, col = 1:nlevels(df$f2))
)

# add legend

draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
draw.key(key2, draw = TRUE, vp = viewport(.75, .9))


On 8/30/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> Gabor Grothendieck a ?crit :
>
> >Note that before entering this you need:
> >
> >library(lattice)
> >library(grid) # to access the viewport function
> >
> >On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >
> >>Try this:
> >>
> >>xyplot(val ~ x, data = df, type = "p",
> >>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
> >>
> >>key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> >>       points = list(pch = 1:nlevels(df$f1))
> >>)
> >>
> >>key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> >>       points = list(pch = 20, col = 1:nlevels(df$f2))
> >>)
> >>
> >>trellis.focus("panel", 1, 1)
> >>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> >>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> >>trellis.unfocus()
> >>
> >>
> >>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> >>
> >>
> >>>Dear R-list,
> >>>
> >>>    I would like to use the lattice library to show several groups on
> >>>the same graph. Here's my example :
> >>>
> >>>## the data
> >>>f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> >>>f1 <- rep(f1,3)
> >>>f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> >>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
> >>>#############################################################
> >>>library(lattice)
> >>>
> >>>para.liste <- trellis.par.get()
> >>>superpose.symbol <- para.liste$superpose.symbol
> >>>superpose.symbol$pch <- c(1,2,3)
> >>>trellis.par.set("superpose.symbol",superpose.symbol)
> >>>
> >>># Now I can see the group according to the f1 factor (with a different
> >>>symbol for every modality)
> >>>xyplot( val~x,
> >>>       data=df,
> >>>       group=f1,
> >>>       auto.key=list(space="right")
> >>>      )
> >>>
> >>># or I can see the group according to the f2 factor
> >>>xyplot( val~x,
> >>>       data=df,
> >>>       type="l",
> >>>       group=f2,
> >>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
> >>>      )
> >>>
> >>>How can I do to highlight both the f1 and f2 factors on one panel with
> >>>the legends, using the lattice function ?
> >>>
> >>>Thanks
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> Thank you, Gabor. The way to put the two legends is very interesting.
> For the graphs, in fact, my problem is to fit the data for every level
> of the f2 factor, showing the levels of the f1 factor in each panel and
> that for several surveys . Here's an example closer to my actual data :
>
> ## the data
>
> n <- 18
> x1 <- seq(1,n)
> val1 <- -2*x1+50
> val2 <- (-2*(x1-8)2)+100
> val3 <- (-2*(x1-8)2)+50
> y <- c(val1,val2,val3)
> x <- rep(x1,3)
> f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> f1 <- rep(f1,3)
> f2 <- rep(c("g1","g2","g3"),each=n)
> df <- data.frame(x=x,y=y,f1=f1,f2=f2)
>
> surveys <-
> factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> df <- rbind(df,df,df)
> df <- data.frame(df,surveys=surveys)
> #######################################################################
> library(lattice)
>
> para.liste <- trellis.par.get()
> superpose.symbol <- para.liste$superpose.symbol
> superpose.symbol$pch <- c(1,2,3)
> trellis.par.set("superpose.symbol",superpose.symbol)
>
> xyplot( y~x | surveys,         data=df,
>       group=f1,
>       auto.key=list(space="right")
>      )
>
> xyplot( y~x | surveys  ,
>       data=df,
>       type="l",
>       group=f2,
>       auto.key=list(space="right",points=FALSE,lines=TRUE)
>      )
>
> Certainly, I have to use the panel function but I don't know how to mark
> the f1 factor in each panel (I want to fit the values according to the
> f2 factor) !
>
>
>


From rmh at temple.edu  Thu Aug 31 04:03:23 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 30 Aug 2006 22:03:23 -0400 (EDT)
Subject: [R] .Rprofile under Windoze.
Message-ID: <20060830220323.BGZ61145@po-d.temple.edu>

Rolf

> The last invocation indicated that the name of the file was *really*
> ``.Rprofile.txt'' --- although I'd tried to save it as (simply)
> ``.Rprofile''.  Is that the problem?  If so, how can I persuade
> Windoze NOT to stick that damned .txt tag on the end? 

The easiest way is to use a smarter editor.  Emacs is perfectly
happy to name a file .Rprofile and wouldn't dream of appending
an extension to the filename that you specify.

Rich


From dunn at usq.edu.au  Thu Aug 31 04:18:52 2006
From: dunn at usq.edu.au (Peter Dunn)
Date: Thu, 31 Aug 2006 12:18:52 +1000
Subject: [R] NaN when using dffits, stemming from  lm.influence  call
Message-ID: <44F6470C.1020200@usq.edu.au>

Hi all

I'm getting a NaN returned on using dffits, as explained
below.  To me, there seems no obvious (or non-obvious reason
for that matter) reason why a  NaN  appears.

Before I start digging further, can anyone see why  dffits
might be failing?  Is there a problem with the data?


Consider:

# Load data
dep <- 
read.table("http://www.sci.usq.edu.au/staff/dunn/Datasets/Books/Hand/Hand-R/factor1-R.dat",
    header=TRUE)
attach(dep)
dep

# Fit Poisson glm
dep.glm2 <- glm( Counts ~ factor(Depression) + factor(SLE) + 
factor(Children) + factor(Depression):factor(SLE),
    family=poisson( link=log) )

# Compute dffits
dffits( dep.glm2 )


This produces the output:
          1          2          3          4          5          6 
      1.4207746 -0.1513808        NaN  0.9079142 -0.1032664 -1.0860289
          7          8
0.4853797  3.8560863

NaN exists for Observation 3.  I cannot understand why: there's
nothing grossly unusual or bad about it.  I look a bit closer,
and it falls over in lm.influence when computing the deletion
statistic sigma:



 > lm.influence(dep.glm2)$sigma
        1        2        3        4        5        6        7        8
0.914829 2.134279      NaN 2.186707 2.224885 1.934539 2.225115 1.957111

The rest of the results from lm.influence are OK; for example:

 > lm.influence(dep.glm2)$wt.res
           1           2           3           4           5           6
  2.62840627 -0.88476903 -1.09492912  0.20247856 -0.23114458 -0.95123387
           7           8
  0.07521515  0.30208051


Use of debug( lm.influence ) indicates the NaN appears in this line
of lm.influence:



res <- .Fortran("lminfl", model$qr$qr, n, n, k, as.integer(do.coef),
     model$qr$qraux, wt.res = e, hat = double(n), coefficients = if 
(do.coef) matrix(0,
         n, k) else double(0), sigma = double(n), tol = 10 * 
.Machine$double.eps,
     DUP = FALSE, PACKAGE = "base")[c("hat", "coefficients", "sigma",
     "wt.res")]


I don't particularly wish to dig around in the Fortran if someone
else can look at it and see my problem easily.  But if I must...



The appearance of the  NaN  seems odd, since (as I understand it)
   lm.influence(dep.glm2)$sigma  computes  sigma  when each observation
is removed in turn.  So if I remove Observation 3 and try fitting the
model, there are no problems or complaints:

dep.glm3 <- glm( Counts ~ factor(Depression) + factor(SLE) +
    factor(Children) + factor(Depression):factor(SLE),
    family=poisson( link=log), subset=(-3) )


This produces:


 > dep.glm3

Call:  glm(formula = Counts ~ factor(Depression) + factor(SLE) + 
factor(Children) +      factor(Depression):factor(SLE), family = 
poisson(link = log),      subset = (-3))

Coefficients:
                      (Intercept)               factor(Depression)1
                           5.4389                           -4.1392
                     factor(SLE)1                 factor(Children)1
                          -0.6503                           -2.4036
factor(Depression)1:factor(SLE)1
                           3.9513

Degrees of Freedom: 6 Total (i.e. Null);  2 Residual
Null Deviance:      695.9
Residual Deviance: 0.8535       AIC: 41.25


No problems, errors, or any signs of potential problems.


In the changes to R 2.3.0 (in the NEWS file,
eg http://mirror.aarnet.edu.au/pub/CRAN/src/base/NEWS)
I find this:

    o	Influence measures such as rstandard() and cooks.distance()
	could return infinite values rather than NaN for a case which
	was fitted exactly.  Similarly, plot.lm() could fail on such
	examples.  plot.lm(which = 5)  had to be modified to only plot
	cases with hat < 1.  (PR#8367)

	lm.influence() was incorrectly reporting 'coefficients' and
	'sigma' as NaN for cases with hat = 1, and on some platforms
	not detecting hat = 1 correctly.

The last sentence identifies NaN being reported for sigma, as I
find with my data.  But my data do not have hat = 1, but the hat
diagonals are large.  The troublesome Observation 3 does not have the
largest hat value in the data though:

 > hatvalues(dep.glm2)
         1         2         3         4         5         6         7 
        8
0.1689061 0.1064651 0.9098542 0.9030814 0.3799079 0.6382790 0.9327408 
0.9607654

And besides, I am using the most recent version of R (see below).  BTW,
the NaNs appear in the previous version of R also.

So I'm flummoxed.

As always, help appreciated.

P.



 > version
                _
platform       i386-pc-linux-gnu
arch           i386
os             linux-gnu
system         i386, linux-gnu
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)


-- 
Dr Peter Dunn  |  Email:  dunn <at> usq.edu.au
Faculty of Sciences, University of Southern Queensland
and the Australian Centre for Sustainable Catchments
CRICOS:  QLD 00244B |  NSW 02225M |  VIC 02387D |  WA 02521C

From ggrothendieck at gmail.com  Thu Aug 31 05:04:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Aug 2006 23:04:14 -0400
Subject: [R] lattice and several groups
In-Reply-To: <971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>
	<44F60D2C.5050201@free.fr>
	<971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>
Message-ID: <971536df0608302004q1f100729ha4b9b73859d2e529@mail.gmail.com>

Or maybe this is what you are looking for where pnl below was
created by modifying source to the panel.plot.default in the zoo
package (there might be a simpler way):


pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
    for (g in levels(groups)) {
        idx <- g == groups[subscripts]
        if (any(idx))
            panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
                pch = pch[subscripts][idx], type = type)
    }
}

xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
	col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)


key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
       points = list(pch = 1:nlevels(df$f1))
)

key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
       points = list(pch = 20, col = 1:nlevels(df$f2))
)

draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
draw.key(key2, draw = TRUE, vp = viewport(.75, .9))




On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> To handle conditioning on survey we provide a panel function
> that subsets col and pch:
>
> # define test data - df
>
> # note that your val2 and val3 lines had a syntax
> # so we have commented them out and
> # replaced them as shown.
> n <- 18
> x1 <- seq(1,n)
> val1 <- -2*x1+50
> # val2 <- (-2*(x1-8)2)+100
> val2 <- (-2*(x1-8))+100
> # val3 <- (-2*(x1-8)2)+50
> val3 <- (-2*(x1-8))+50
> y <- c(val1,val2,val3)
> x <- rep(x1,3)
> f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> f1 <- rep(f1,3)
> f2 <- rep(c("g1","g2","g3"),each=n)
> df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> surveys <-
> factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> df <- rbind(df,df,df)
> df <- data.frame(df,surveys=surveys)
>
> # create xyplot
>
> library(lattice)
> library(grid)
>
> pnl <- function(x, y, groups, subscripts, col, pch, ...)
>        panel.xyplot(x, y, col = col[subscripts], pch = pch[subscripts], ...)
>
> xyplot(y ~ x | surveys, data = df,
>        col = as.numeric(df$f1), pch = as.numeric(df$f2), panel = pnl)
>
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>       points = list(pch = 1:nlevels(df$f1))
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>       points = list(pch = 20, col = 1:nlevels(df$f2))
> )
>
> # add legend
>
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>
>
> On 8/30/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> > Gabor Grothendieck a ?crit :
> >
> > >Note that before entering this you need:
> > >
> > >library(lattice)
> > >library(grid) # to access the viewport function
> > >
> > >On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > >
> > >
> > >>Try this:
> > >>
> > >>xyplot(val ~ x, data = df, type = "p",
> > >>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
> > >>
> > >>key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> > >>       points = list(pch = 1:nlevels(df$f1))
> > >>)
> > >>
> > >>key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> > >>       points = list(pch = 20, col = 1:nlevels(df$f2))
> > >>)
> > >>
> > >>trellis.focus("panel", 1, 1)
> > >>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> > >>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> > >>trellis.unfocus()
> > >>
> > >>
> > >>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> > >>
> > >>
> > >>>Dear R-list,
> > >>>
> > >>>    I would like to use the lattice library to show several groups on
> > >>>the same graph. Here's my example :
> > >>>
> > >>>## the data
> > >>>f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> > >>>f1 <- rep(f1,3)
> > >>>f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> > >>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
> > >>>#############################################################
> > >>>library(lattice)
> > >>>
> > >>>para.liste <- trellis.par.get()
> > >>>superpose.symbol <- para.liste$superpose.symbol
> > >>>superpose.symbol$pch <- c(1,2,3)
> > >>>trellis.par.set("superpose.symbol",superpose.symbol)
> > >>>
> > >>># Now I can see the group according to the f1 factor (with a different
> > >>>symbol for every modality)
> > >>>xyplot( val~x,
> > >>>       data=df,
> > >>>       group=f1,
> > >>>       auto.key=list(space="right")
> > >>>      )
> > >>>
> > >>># or I can see the group according to the f2 factor
> > >>>xyplot( val~x,
> > >>>       data=df,
> > >>>       type="l",
> > >>>       group=f2,
> > >>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
> > >>>      )
> > >>>
> > >>>How can I do to highlight both the f1 and f2 factors on one panel with
> > >>>the legends, using the lattice function ?
> > >>>
> > >>>Thanks
> > >>>
> > >>>______________________________________________
> > >>>R-help at stat.math.ethz.ch mailing list
> > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>>and provide commented, minimal, self-contained, reproducible code.
> > >>>
> > >>>
> > >>>
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> > >
> > Thank you, Gabor. The way to put the two legends is very interesting.
> > For the graphs, in fact, my problem is to fit the data for every level
> > of the f2 factor, showing the levels of the f1 factor in each panel and
> > that for several surveys . Here's an example closer to my actual data :
> >
> > ## the data
> >
> > n <- 18
> > x1 <- seq(1,n)
> > val1 <- -2*x1+50
> > val2 <- (-2*(x1-8)2)+100
> > val3 <- (-2*(x1-8)2)+50
> > y <- c(val1,val2,val3)
> > x <- rep(x1,3)
> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> > f1 <- rep(f1,3)
> > f2 <- rep(c("g1","g2","g3"),each=n)
> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> >
> > surveys <-
> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> > df <- rbind(df,df,df)
> > df <- data.frame(df,surveys=surveys)
> > #######################################################################
> > library(lattice)
> >
> > para.liste <- trellis.par.get()
> > superpose.symbol <- para.liste$superpose.symbol
> > superpose.symbol$pch <- c(1,2,3)
> > trellis.par.set("superpose.symbol",superpose.symbol)
> >
> > xyplot( y~x | surveys,         data=df,
> >       group=f1,
> >       auto.key=list(space="right")
> >      )
> >
> > xyplot( y~x | surveys  ,
> >       data=df,
> >       type="l",
> >       group=f2,
> >       auto.key=list(space="right",points=FALSE,lines=TRUE)
> >      )
> >
> > Certainly, I have to use the panel function but I don't know how to mark
> > the f1 factor in each panel (I want to fit the values according to the
> > f2 factor) !
> >
> >
> >
>


From edd at debian.org  Thu Aug 31 05:14:26 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 30 Aug 2006 22:14:26 -0500
Subject: [R] [R-pkgs] New package 'random' for non-deterministic random
	number
Message-ID: <17654.21522.980572.948491@basebud.nulle.part>


Dear useRs,

A few days ago, the initial version 0.1.0 of a new package 'random' was
uploaded to CRAN.

The random packages provides convenient access to the non-deterministic
random numbers provided by the random.org site created by Mads Haahr
(http://www.random.org).  

While certain hardware and software solutions that provide access to
non-deterministic random-numbers exist, few if any are portable across all
the hardware platforms R supports. Retrieving non-deterministic random
numbers may be beneficial to seed parallel simulations with independent
draws, to obtain portable initializations for other RNGs, to validate
simulation with non-deterministic RNGs, or simply for fun and
experimentations.

The package contains five simple functions 
     randomNumber	(random integeres between min, max w/ duplicates)
     randomSequence	(random sequences between min, max w/o duplicates)
     randomBytes	(in hex, dec, oct or bin)
     randomBufferStatus (to query the server status)
     sufficientBits	(boolean test of randomBufferStats vs rec'ed value)

Also included are two vignettes that can be accessed via
     vignette("random-intro", package = "random")
     vignette("random-essay", package = "random")

'random-intro' provides some background on the package as well as initial
test results using the dieharder suite by Robert G. Brown. I hope to expand
on these tests in the near future. 'random-essay' is a transcript of the web
essay at http://www.random.org/essay.html and provides some background on the
random.org service.

Some information is / will be at 
     http://dirk.eddelbuettel.com/code/random.html will 

Comments or suggestions are more than welcome!

Regards, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ggrothendieck at gmail.com  Thu Aug 31 06:13:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 31 Aug 2006 00:13:23 -0400
Subject: [R] lattice and several groups
In-Reply-To: <971536df0608302004q1f100729ha4b9b73859d2e529@mail.gmail.com>
References: <44F4A0FF.2010708@free.fr>
	<971536df0608291728o7435696ew121a6b3843b51c94@mail.gmail.com>
	<971536df0608291801i69a47e5bscd5157db05507cb9@mail.gmail.com>
	<44F60D2C.5050201@free.fr>
	<971536df0608301844h5ff2305cjb689baaa0895f75d@mail.gmail.com>
	<971536df0608302004q1f100729ha4b9b73859d2e529@mail.gmail.com>
Message-ID: <971536df0608302113v60d56659lf7c57f86ee2a81c4@mail.gmail.com>

In thinking about this a bit more we can use
panel.superpose/panel.groups to shorten it:

# define data -- df

# note that your val2 and val3 lines had a syntax
# so we have commented them out and
# replaced them as shown.
n <- 18
x1 <- seq(1,n)
val1 <- -2*x1+50
# val2 <- (-2*(x1-8)2)+100
val2 <- (-2*(x1-8))+100
# val3 <- (-2*(x1-8)2)+50
val3 <- (-2*(x1-8))+50
y <- c(val1,val2,val3)
x <- rep(x1,3)
f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
f1 <- rep(f1,3)
f2 <- rep(c("g1","g2","g3"),each=n)
df <- data.frame(x=x,y=y,f1=f1,f2=f2)
surveys <-
factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
df <- rbind(df,df,df)
df <- data.frame(df,surveys=surveys)

# create xyplot

library(lattice)
library(grid)

# set custom col and pch here
my.col <- 1:nlevels(df$f2)
my.pch <- 1:nlevels(df$f1)

pnl <- function(x, y, subscripts, pch, type, ...)
   panel.xyplot(x, y, type = type, pch = my.pch[df[subscripts, "f1"]], ...)
	
xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
        panel = panel.superpose,
        panel.groups = pnl,
        par.settings = list(superpose.line = list(col = my.col),
           superpose.symbol = list(col = my.col))
)


key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
       points = list(pch = my.pch)
)

key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
       lines = list(col = my.col)
)

draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
draw.key(key2, draw = TRUE, vp = viewport(.75, .9))



On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Or maybe this is what you are looking for where pnl below was
> created by modifying source to the panel.plot.default in the zoo
> package (there might be a simpler way):
>
>
> pnl <- function (x, y, subscripts, groups, col, pch, type, ...) {
>    for (g in levels(groups)) {
>        idx <- g == groups[subscripts]
>        if (any(idx))
>            panel.xyplot(x[idx], y[idx], ..., col = col[subscripts][idx],
>                pch = pch[subscripts][idx], type = type)
>    }
> }
>
> xyplot(y ~ x | surveys, data = df, groups = df$f2, type = "b",
>        col = as.numeric(df$f2), pch = as.numeric(df$f1), panel = pnl)
>
>
> key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
>       points = list(pch = 1:nlevels(df$f1))
> )
>
> key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
>       points = list(pch = 20, col = 1:nlevels(df$f2))
> )
>
> draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
>
>
>
>
> On 8/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > To handle conditioning on survey we provide a panel function
> > that subsets col and pch:
> >
> > # define test data - df
> >
> > # note that your val2 and val3 lines had a syntax
> > # so we have commented them out and
> > # replaced them as shown.
> > n <- 18
> > x1 <- seq(1,n)
> > val1 <- -2*x1+50
> > # val2 <- (-2*(x1-8)2)+100
> > val2 <- (-2*(x1-8))+100
> > # val3 <- (-2*(x1-8)2)+50
> > val3 <- (-2*(x1-8))+50
> > y <- c(val1,val2,val3)
> > x <- rep(x1,3)
> > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> > f1 <- rep(f1,3)
> > f2 <- rep(c("g1","g2","g3"),each=n)
> > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> > surveys <-
> > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> > df <- rbind(df,df,df)
> > df <- data.frame(df,surveys=surveys)
> >
> > # create xyplot
> >
> > library(lattice)
> > library(grid)
> >
> > pnl <- function(x, y, groups, subscripts, col, pch, ...)
> >        panel.xyplot(x, y, col = col[subscripts], pch = pch[subscripts], ...)
> >
> > xyplot(y ~ x | surveys, data = df,
> >        col = as.numeric(df$f1), pch = as.numeric(df$f2), panel = pnl)
> >
> >
> > key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> >       points = list(pch = 1:nlevels(df$f1))
> > )
> >
> > key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> >       points = list(pch = 20, col = 1:nlevels(df$f2))
> > )
> >
> > # add legend
> >
> > draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> > draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> >
> >
> > On 8/30/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> > > Gabor Grothendieck a ?crit :
> > >
> > > >Note that before entering this you need:
> > > >
> > > >library(lattice)
> > > >library(grid) # to access the viewport function
> > > >
> > > >On 8/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > >
> > > >
> > > >>Try this:
> > > >>
> > > >>xyplot(val ~ x, data = df, type = "p",
> > > >>       col = as.numeric(df$f1), pch = as.numeric(df$f2))
> > > >>
> > > >>key1 <- list(border = TRUE, colums = 2, text = list(levels(df$f1)),
> > > >>       points = list(pch = 1:nlevels(df$f1))
> > > >>)
> > > >>
> > > >>key2 <- list(border = TRUE, colums = 2, text = list(levels(df$f2)),
> > > >>       points = list(pch = 20, col = 1:nlevels(df$f2))
> > > >>)
> > > >>
> > > >>trellis.focus("panel", 1, 1)
> > > >>draw.key(key1, draw = TRUE, vp = viewport(.9, .9))
> > > >>draw.key(key2, draw = TRUE, vp = viewport(.75, .9))
> > > >>trellis.unfocus()
> > > >>
> > > >>
> > > >>On 8/29/06, Laurent Rhelp <laurentRhelp at free.fr> wrote:
> > > >>
> > > >>
> > > >>>Dear R-list,
> > > >>>
> > > >>>    I would like to use the lattice library to show several groups on
> > > >>>the same graph. Here's my example :
> > > >>>
> > > >>>## the data
> > > >>>f1 <- factor(c("mod1","mod2","mod3"),levels=c("mod1","mod2","mod3"))
> > > >>>f1 <- rep(f1,3)
> > > >>>f2 <- factor(rep(c("g1","g2","g3"),each=3),levels=c("g1","g2","g3"))
> > > >>>df <- data.frame(val=c(4,3,2,5,4,3,6,5,4), x=rep(c(1,2,3),3),f1=f1,f2=f2)
> > > >>>#############################################################
> > > >>>library(lattice)
> > > >>>
> > > >>>para.liste <- trellis.par.get()
> > > >>>superpose.symbol <- para.liste$superpose.symbol
> > > >>>superpose.symbol$pch <- c(1,2,3)
> > > >>>trellis.par.set("superpose.symbol",superpose.symbol)
> > > >>>
> > > >>># Now I can see the group according to the f1 factor (with a different
> > > >>>symbol for every modality)
> > > >>>xyplot( val~x,
> > > >>>       data=df,
> > > >>>       group=f1,
> > > >>>       auto.key=list(space="right")
> > > >>>      )
> > > >>>
> > > >>># or I can see the group according to the f2 factor
> > > >>>xyplot( val~x,
> > > >>>       data=df,
> > > >>>       type="l",
> > > >>>       group=f2,
> > > >>>       auto.key=list(space="right",points=FALSE,lines=TRUE)
> > > >>>      )
> > > >>>
> > > >>>How can I do to highlight both the f1 and f2 factors on one panel with
> > > >>>the legends, using the lattice function ?
> > > >>>
> > > >>>Thanks
> > > >>>
> > > >>>______________________________________________
> > > >>>R-help at stat.math.ethz.ch mailing list
> > > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > >>>and provide commented, minimal, self-contained, reproducible code.
> > > >>>
> > > >>>
> > > >>>
> > > >
> > > >______________________________________________
> > > >R-help at stat.math.ethz.ch mailing list
> > > >https://stat.ethz.ch/mailman/listinfo/r-help
> > > >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > >and provide commented, minimal, self-contained, reproducible code.
> > > >
> > > >
> > > >
> > > >
> > > Thank you, Gabor. The way to put the two legends is very interesting.
> > > For the graphs, in fact, my problem is to fit the data for every level
> > > of the f2 factor, showing the levels of the f1 factor in each panel and
> > > that for several surveys . Here's an example closer to my actual data :
> > >
> > > ## the data
> > >
> > > n <- 18
> > > x1 <- seq(1,n)
> > > val1 <- -2*x1+50
> > > val2 <- (-2*(x1-8)2)+100
> > > val3 <- (-2*(x1-8)2)+50
> > > y <- c(val1,val2,val3)
> > > x <- rep(x1,3)
> > > f1 <- rep(c("mod1","mod2","mod3"),each=n/3)
> > > f1 <- rep(f1,3)
> > > f2 <- rep(c("g1","g2","g3"),each=n)
> > > df <- data.frame(x=x,y=y,f1=f1,f2=f2)
> > >
> > > surveys <-
> > > factor(c(rep("survey1",n*3),rep("survey2",n*3),rep("survey3",n*3)))
> > > df <- rbind(df,df,df)
> > > df <- data.frame(df,surveys=surveys)
> > > #######################################################################
> > > library(lattice)
> > >
> > > para.liste <- trellis.par.get()
> > > superpose.symbol <- para.liste$superpose.symbol
> > > superpose.symbol$pch <- c(1,2,3)
> > > trellis.par.set("superpose.symbol",superpose.symbol)
> > >
> > > xyplot( y~x | surveys,         data=df,
> > >       group=f1,
> > >       auto.key=list(space="right")
> > >      )
> > >
> > > xyplot( y~x | surveys  ,
> > >       data=df,
> > >       type="l",
> > >       group=f2,
> > >       auto.key=list(space="right",points=FALSE,lines=TRUE)
> > >      )
> > >
> > > Certainly, I have to use the panel function but I don't know how to mark
> > > the f1 factor in each panel (I want to fit the values according to the
> > > f2 factor) !
> > >
> > >
> > >
> >
>


From jhowison at syr.edu  Thu Aug 31 06:36:16 2006
From: jhowison at syr.edu (James Howison)
Date: Thu, 31 Aug 2006 00:36:16 -0400
Subject: [R] Combine 'overlapping' dataframes, respecting row names
Message-ID: <AD567844-719D-4FED-92A6-4C06A9CBBFAE@syr.edu>

Hi,

I've examined the archives and found quite a few questions on  
concatenating dataframes, but none that really addressed my issue,  
I'm afraid.  I've also examined the cbind and rbind documentation but  
nonetheless here I am writing to r-help ;)

This is what I have (the row names are dates used for conversion to  
an irregular time series with the its package):

 > cvsFrame
            cvsactions
2002-11-15          4
2002-12-15          9
2003-01-15          5
2003-02-15          5

 > downloadsFrame
             downloads
2002-09-15          1
2002-10-15          2
2002-11-15         12
2002-12-15          8

(notice how the dates are overlapping?)

The output I'd like is:

            cvsaction  downloads
2002-09-15       NA          1
2002-10-15       NA          2
2002-11-15       4          12
2002-12-15       9           8
2003-01-15       5          NA
2003-02-15       5          NA

ie. merge the data.frames, respecting the row.names and inserting NAs  
where a frame didn't contain info for a row in the final frame.

This is the closest I gotten (I'm sure cbind is doing what it's meant  
to do but it's obviously not what I need)

 > cbind(downloadsFrame,cvsFrame)
             downloads cvsactions
2002-09-15          1          4
2002-10-15          2          9
2002-11-15         12          5
2002-12-15          8          5

It takes the row.names from the first frame given and then just adds  
the data in rows 1 through 4, regardless of their row.name. And it  
doesn't work at all if the column lengths are different. (Yes, it  
would be nice if the 'its' class had a way to merge 'its' objects,  
but the question seemed general enough to ask on list.)

Thanks,
James


From ggrothendieck at gmail.com  Thu Aug 31 06:51:44 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 31 Aug 2006 00:51:44 -0400
Subject: [R] Combine 'overlapping' dataframes, respecting row names
In-Reply-To: <AD567844-719D-4FED-92A6-4C06A9CBBFAE@syr.edu>
References: <AD567844-719D-4FED-92A6-4C06A9CBBFAE@syr.edu>
Message-ID: <971536df0608302151l4ad3b6cdjabec84cd7eb545b4@mail.gmail.com>

If you are converting them to 'its' anyways then after the
conversion to 'its' use the 'its' union command.

On 8/31/06, James Howison <jhowison at syr.edu> wrote:
> Hi,
>
> I've examined the archives and found quite a few questions on
> concatenating dataframes, but none that really addressed my issue,
> I'm afraid.  I've also examined the cbind and rbind documentation but
> nonetheless here I am writing to r-help ;)
>
> This is what I have (the row names are dates used for conversion to
> an irregular time series with the its package):
>
>  > cvsFrame
>            cvsactions
> 2002-11-15          4
> 2002-12-15          9
> 2003-01-15          5
> 2003-02-15          5
>
>  > downloadsFrame
>             downloads
> 2002-09-15          1
> 2002-10-15          2
> 2002-11-15         12
> 2002-12-15          8
>
> (notice how the dates are overlapping?)
>
> The output I'd like is:
>
>            cvsaction  downloads
> 2002-09-15       NA          1
> 2002-10-15       NA          2
> 2002-11-15       4          12
> 2002-12-15       9           8
> 2003-01-15       5          NA
> 2003-02-15       5          NA
>
> ie. merge the data.frames, respecting the row.names and inserting NAs
> where a frame didn't contain info for a row in the final frame.
>
> This is the closest I gotten (I'm sure cbind is doing what it's meant
> to do but it's obviously not what I need)
>
>  > cbind(downloadsFrame,cvsFrame)
>             downloads cvsactions
> 2002-09-15          1          4
> 2002-10-15          2          9
> 2002-11-15         12          5
> 2002-12-15          8          5
>
> It takes the row.names from the first frame given and then just adds
> the data in rows 1 through 4, regardless of their row.name. And it
> doesn't work at all if the column lengths are different. (Yes, it
> would be nice if the 'its' class had a way to merge 'its' objects,
> but the question seemed general enough to ask on list.)
>
> Thanks,
> James
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Thu Aug 31 06:54:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 05:54:48 +0100 (BST)
Subject: [R] Combine 'overlapping' dataframes, respecting row names
In-Reply-To: <AD567844-719D-4FED-92A6-4C06A9CBBFAE@syr.edu>
References: <AD567844-719D-4FED-92A6-4C06A9CBBFAE@syr.edu>
Message-ID: <Pine.LNX.4.64.0608310546230.31396@gannet.stats.ox.ac.uk>

'merge' is the key here.  You say you want to merge, but it seems did not 
try merge()

> (res <- merge(cvsFrame, downloadsFrame, by="row.names", all=TRUE))
   Row.names cvsactions downloads
1 2002-11-15          4        12
2 2002-12-15          9         8
3 2003-01-15          5        NA
4 2003-02-15          5        NA
5 2002-09-15         NA         1
6 2002-10-15         NA         2

You can sort on Row.names later: say

res[order(as.character(res$Row.names)), ]


On Thu, 31 Aug 2006, James Howison wrote:

> Hi,
> 
> I've examined the archives and found quite a few questions on  
> concatenating dataframes, but none that really addressed my issue,  
> I'm afraid.  I've also examined the cbind and rbind documentation but  
> nonetheless here I am writing to r-help ;)
> 
> This is what I have (the row names are dates used for conversion to  
> an irregular time series with the its package):
> 
>  > cvsFrame
>             cvsactions
> 2002-11-15          4
> 2002-12-15          9
> 2003-01-15          5
> 2003-02-15          5
> 
>  > downloadsFrame
>              downloads
> 2002-09-15          1
> 2002-10-15          2
> 2002-11-15         12
> 2002-12-15          8
> 
> (notice how the dates are overlapping?)
> 
> The output I'd like is:
> 
>             cvsaction  downloads
> 2002-09-15       NA          1
> 2002-10-15       NA          2
> 2002-11-15       4          12
> 2002-12-15       9           8
> 2003-01-15       5          NA
> 2003-02-15       5          NA
> 
> ie. merge the data.frames, respecting the row.names and inserting NAs  
> where a frame didn't contain info for a row in the final frame.
> 
> This is the closest I gotten (I'm sure cbind is doing what it's meant  
> to do but it's obviously not what I need)
> 
>  > cbind(downloadsFrame,cvsFrame)
>              downloads cvsactions
> 2002-09-15          1          4
> 2002-10-15          2          9
> 2002-11-15         12          5
> 2002-12-15          8          5
> 
> It takes the row.names from the first frame given and then just adds  
> the data in rows 1 through 4, regardless of their row.name. And it  
> doesn't work at all if the column lengths are different. (Yes, it  
> would be nice if the 'its' class had a way to merge 'its' objects,  
> but the question seemed general enough to ask on list.)
> 
> Thanks,
> James
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mel at altk.com  Thu Aug 31 07:48:57 2006
From: mel at altk.com (mel)
Date: Thu, 31 Aug 2006 07:48:57 +0200
Subject: [R] how to read just a column
In-Reply-To: <8f67b6f80608301528l36c802c9ie4e32afc26c43e5@mail.gmail.com>
References: <8f67b6f80608301528l36c802c9ie4e32afc26c43e5@mail.gmail.com>
Message-ID: <44F67849.3090906@altk.com>

Carlo Trimarchi a ?crit :

> Hi,
> how can I read, using for example read.table() or scan(), just one
> column from a text file that has more columns without any header?
> Thanks, bye.

afaik, you have to read all the table
and then you select the column you want.
eg read.table(blabla)[3] to get the 3rd column.

You can read partially rows (see nrows) but not columns.
Please somebody correct me if I am wrong.

(Of course a trick could be to transpose your table
before writing it, etc)

hih


From petr.pikal at precheza.cz  Thu Aug 31 08:09:05 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 31 Aug 2006 08:09:05 +0200
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <6e8360ad0608301511j3375b3a1m4b080352923412fa@mail.gmail.com>
References: <6e8360ad0608301504h7fac0314r9e64816d0159ded2@mail.gmail.com>
Message-ID: <44F69921.27943.124619@localhost>

Hi

or you can change

Rprofile.site in etc directory to whatever startup commands you want 
to execute to have the same profile in all sessions.

HTH
Petr




On 30 Aug 2006 at 15:11, BBands wrote:

Date sent:      	Wed, 30 Aug 2006 15:11:35 -0700
From:           	BBands <bbands at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	Re: [R] .Rprofile under Windoze.

> And... If you have/use shortcuts to R, you may also save an
> ".Rprofile" to whatever directory you name in the "Start in:" field of
> the shortcut. This allows one to have many profiles.
> 
>        jab
> -- 
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the beginning.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From sumantabasak at mail.com  Thu Aug 31 08:12:50 2006
From: sumantabasak at mail.com (Sumanta Basak)
Date: Thu, 31 Aug 2006 01:12:50 -0500
Subject: [R] Data Download Probelm from Yahoo
Message-ID: <20060831061250.EDEC21CE304@ws1-6.us4.outblaze.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/aab520cb/attachment.pl 

From marcov98 at gmail.com  Thu Aug 31 08:30:06 2006
From: marcov98 at gmail.com (Marco Vinisio Martinez)
Date: Thu, 31 Aug 2006 01:30:06 -0500
Subject: [R] Question about the test of hartley and box
Message-ID: <56a44fbc0608302330q1329d54fr5af029668645ded5@mail.gmail.com>

Hello

I have 2 questions:

1. How I can do the test of hartley for homogeneity of variances?
2. How I can do the test of Box for homogeneity of variances?

Thanks in advance
-- 
Marco Vinisio Martinez
Profesor Departamento de Matematicas y Departamento de Biologia
Pontificia Universidad Javeriana


From ripley at stats.ox.ac.uk  Thu Aug 31 08:50:44 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 07:50:44 +0100 (BST)
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <44F69921.27943.124619@localhost>
References: <6e8360ad0608301504h7fac0314r9e64816d0159ded2@mail.gmail.com>
	<44F69921.27943.124619@localhost>
Message-ID: <Pine.LNX.4.64.0608310746160.12590@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, Petr Pikal wrote:

> Hi
> 
> or you can change
> 
> Rprofile.site in etc directory to whatever startup commands you want 
> to execute to have the same profile in all sessions.

Please see the warning about this in ?Startup: it is not the same thing as 
putting .Rprofile in your home directory.

Also, many users will not have write permission on R_HOME/etc.


> On 30 Aug 2006 at 15:11, BBands wrote:
> 
> > And... If you have/use shortcuts to R, you may also save an
> > ".Rprofile" to whatever directory you name in the "Start in:" field of
> > the shortcut. This allows one to have many profiles.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From geoffrey.russell at gmail.com  Thu Aug 31 03:28:18 2006
From: geoffrey.russell at gmail.com (Geoff Russell)
Date: Thu, 31 Aug 2006 10:58:18 +0930
Subject: [R] Frequency tables without underlying data
Message-ID: <93c3eada0608301828l173b93b4q23a0bd7f8edc58b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/8aa0e466/attachment.pl 

From n.nguyen at garvan.org.au  Thu Aug 31 01:47:31 2006
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Thu, 31 Aug 2006 09:47:31 +1000
Subject: [R] Update new version
In-Reply-To: <mailman.13.1156932004.4788.r-help@stat.math.ethz.ch>
Message-ID: <001f01c6cc8e$a9ad7f60$0fe05e81@D145LD1S>

Dear R helpers,
I would like to update R 2.2.1 to the latest version, 
How can I do it without loosing all my installed packages?
Many thanks
Nguyen


From Nigel.Findlater at partnerre.com  Thu Aug 31 09:15:37 2006
From: Nigel.Findlater at partnerre.com (Nigel.Findlater at partnerre.com)
Date: Thu, 31 Aug 2006 09:15:37 +0200
Subject: [R] DCOM 1.3.5 Exception from HRESULT: 0x80040013 on iR.Init("R")
Message-ID: <OFDB850F80.E0932062-ONC12571DB.0026BE98-C12571DB.00280C3D@partnerre.com>


Hallo Everyone,

I have a problem getting R DCom to work on a MS Server 2003. I am using
Visual Studio 2005 to use some statistical functions from R. I have
installed DCOM 1.3.5. but as soon as the statement iR.Init("R") is executed
I get the following error:

Exception from HRESULT: 0x80040013

I have checked the path and the environmental variables and there are none
associated with the RDCom, which is what I would expect.

I searched the registry for differences between my laptop (Windows XP SP2)
where it works but have not found anything useful

I have also tried to install Version 2.0.0 from
http://cran.r-project.org/contrib/extra/dcom/RSrv200.html. But this did not
resolve the prblem

Any ideas?

Thanks...

Nigel...


DISCLAIMER: This e-mail contains information solely intended...{{dropped}}


From petr.pikal at precheza.cz  Thu Aug 31 09:26:23 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 31 Aug 2006 09:26:23 +0200
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <Pine.LNX.4.64.0608310746160.12590@gannet.stats.ox.ac.uk>
References: <44F69921.27943.124619@localhost>
Message-ID: <44F6AB3F.1713.590156@localhost>

Hi

Well I see your point, but let say I would like to load some of my 
functions, libraries I use, some dataset, and set the lattice theme 
to my preferred version.

As I work on different problems and each problem has its separate 
directory I used to copy .Rprofile from one directory to another, but 
recently I find more convenient just to copy my Rprofile.site from 
one version of R to newly installed. Then I am sure I have always the 
same startup regardless of problem I am working on. Am I wrong? Is 
there some other recommended way?

Although I do not have write permission to registry and to some of 
root directories, thank goodness (and Rcore development team :-), I 
am able to install R elsewhere and use it without some sophisticated 
hacking.

Best regards
Petr Pikal


On 31 Aug 2006 at 7:50, Prof Brian Ripley wrote:

Date sent:      	Thu, 31 Aug 2006 07:50:44 +0100 (BST)
From:           	Prof Brian Ripley <ripley at stats.ox.ac.uk>
To:             	Petr Pikal <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] .Rprofile under Windoze.

> On Thu, 31 Aug 2006, Petr Pikal wrote:
> 
> > Hi
> > 
> > or you can change
> > 
> > Rprofile.site in etc directory to whatever startup commands you want
> > to execute to have the same profile in all sessions.
> 
> Please see the warning about this in ?Startup: it is not the same
> thing as putting .Rprofile in your home directory.
> 
> Also, many users will not have write permission on R_HOME/etc.
> 
> 
> > On 30 Aug 2006 at 15:11, BBands wrote:
> > 
> > > And... If you have/use shortcuts to R, you may also save an
> > > ".Rprofile" to whatever directory you name in the "Start in:"
> > > field of the shortcut. This allows one to have many profiles.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From babaktei at yahoo.com  Thu Aug 31 09:26:41 2006
From: babaktei at yahoo.com (Bab Tei)
Date: Thu, 31 Aug 2006 00:26:41 -0700 (PDT)
Subject: [R] Weigthed mixed variable distance function needed for clustering
Message-ID: <20060831072641.18923.qmail@web50412.mail.yahoo.com>

Hi
I need to use weigthed mixed variable (Numerical and
categorical) distance function for clustering.
But daisy does not accept weigthings. Is there any
other function?
Regards


From ripley at stats.ox.ac.uk  Thu Aug 31 09:32:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 08:32:09 +0100 (BST)
Subject: [R] .Rprofile under Windoze.
In-Reply-To: <44F6AB3F.1713.590156@localhost>
References: <44F69921.27943.124619@localhost> <44F6AB3F.1713.590156@localhost>
Message-ID: <Pine.LNX.4.64.0608310830030.17081@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, Petr Pikal wrote:

> Hi
> 
> Well I see your point, but let say I would like to load some of my 
> functions, libraries I use, some dataset, and set the lattice theme 
> to my preferred version.
> 
> As I work on different problems and each problem has its separate 
> directory I used to copy .Rprofile from one directory to another, but 
> recently I find more convenient just to copy my Rprofile.site from 
> one version of R to newly installed. Then I am sure I have always the 
> same startup regardless of problem I am working on. Am I wrong? Is 
> there some other recommended way?

I use a .Rprofile in home directory (only): works in all startup 
directories and all versions of R (which matters to me as I am forever 
rebuilding R from scratch).

> Although I do not have write permission to registry and to some of 
> root directories, thank goodness (and Rcore development team :-), I 
> am able to install R elsewhere and use it without some sophisticated 
> hacking.
> 
> Best regards
> Petr Pikal
> 
> 
> On 31 Aug 2006 at 7:50, Prof Brian Ripley wrote:
> 
> Date sent:      	Thu, 31 Aug 2006 07:50:44 +0100 (BST)
> From:           	Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To:             	Petr Pikal <petr.pikal at precheza.cz>
> Copies to:      	r-help at stat.math.ethz.ch
> Subject:        	Re: [R] .Rprofile under Windoze.
> 
> > On Thu, 31 Aug 2006, Petr Pikal wrote:
> > 
> > > Hi
> > > 
> > > or you can change
> > > 
> > > Rprofile.site in etc directory to whatever startup commands you want
> > > to execute to have the same profile in all sessions.
> > 
> > Please see the warning about this in ?Startup: it is not the same
> > thing as putting .Rprofile in your home directory.
> > 
> > Also, many users will not have write permission on R_HOME/etc.
> > 
> > 
> > > On 30 Aug 2006 at 15:11, BBands wrote:
> > > 
> > > > And... If you have/use shortcuts to R, you may also save an
> > > > ".Rprofile" to whatever directory you name in the "Start in:"
> > > > field of the shortcut. This allows one to have many profiles.
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> > Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> > UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Aug 31 09:46:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 08:46:30 +0100 (BST)
Subject: [R] Update new version
In-Reply-To: <001f01c6cc8e$a9ad7f60$0fe05e81@D145LD1S>
References: <001f01c6cc8e$a9ad7f60$0fe05e81@D145LD1S>
Message-ID: <Pine.LNX.4.64.0608310845370.17209@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, Nguyen Dinh Nguyen wrote:

> Dear R helpers,
> I would like to update R 2.2.1 to the latest version, 
> How can I do it without loosing all my installed packages?

What OS?

If Windows, this is discussed in the rw-FAQ, and the advice there is 
useful on all OSes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Aug 31 09:48:38 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 08:48:38 +0100 (BST)
Subject: [R] DCOM 1.3.5 Exception from HRESULT: 0x80040013 on
	iR.Init("R")
In-Reply-To: <OFDB850F80.E0932062-ONC12571DB.0026BE98-C12571DB.00280C3D@partnerre.com>
References: <OFDB850F80.E0932062-ONC12571DB.0026BE98-C12571DB.00280C3D@partnerre.com>
Message-ID: <Pine.LNX.4.64.0608310846530.17209@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, Nigel.Findlater at partnerre.com wrote:

> 
> Hallo Everyone,
> 
> I have a problem getting R DCom to work on a MS Server 2003. I am using
> Visual Studio 2005 to use some statistical functions from R. I have
> installed DCOM 1.3.5. but as soon as the statement iR.Init("R") is executed
> I get the following error:
> 
> Exception from HRESULT: 0x80040013
> 
> I have checked the path and the environmental variables and there are none
> associated with the RDCom, which is what I would expect.
> 
> I searched the registry for differences between my laptop (Windows XP SP2)
> where it works but have not found anything useful
> 
> I have also tried to install Version 2.0.0 from
> http://cran.r-project.org/contrib/extra/dcom/RSrv200.html. But this did not
> resolve the prblem
> 
> Any ideas?

That page tells you the appropriate mailing list for questions about that
software, or you can do as the R posting guide asks and contact the 
authors/maintainers.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Thu Aug 31 09:59:37 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 31 Aug 2006 09:59:37 +0200
Subject: [R] Frequency tables without underlying data
In-Reply-To: <93c3eada0608301828l173b93b4q23a0bd7f8edc58b@mail.gmail.com>
References: <93c3eada0608301828l173b93b4q23a0bd7f8edc58b@mail.gmail.com>
Message-ID: <44F696E9.40604@statistik.uni-dortmund.de>



Geoff Russell wrote:
> Hi from a new useR,
> 
> I know how to build a table() with 2 factors(), but I want to build a
> table() when
> I only know the frequencies:
> 
> e.g. I know that
> 
>                useR  useStatA
>        rich     100      200
>        poor   200      5


  X <- as.table(matrix(c(100, 200, 100, 5), ncol=2))
  dimnames(X) <-
     list(wealth = c("rich", "poor"), use = c("useR", "useStatA"))

Uwe Ligges


> but i don't have the underlying data to set up factors. Can I still make a
> frequency
> table?
> 
> Cheers,
> Geoff Russell
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Aug 31 10:13:50 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 31 Aug 2006 10:13:50 +0200
Subject: [R] Dendrogram troubles
In-Reply-To: <c2f237040608291240y5dd32d36le699d9f536508075@mail.gmail.com>
References: <c2f237040608291240y5dd32d36le699d9f536508075@mail.gmail.com>
Message-ID: <44F69A3E.90502@statistik.uni-dortmund.de>



Davendra Sohal wrote:
> Hi,
> I am making a dendrogram with 180 terminal values. Whether I keep it
> horizontal or vertical, it gives a 'squished' graph that refuses to be
> stretched beyond the window size and I cannot read the labels.
> (I'm using hclust and then plot to make the tree.)
> Is there a way to stretch the graph in R, or by exporting it somehow, so
> that I can read the 180 values and see what the results are?

Maybe export it as a huge PDF and zoom into it with your PDF viewer.
For 180 labels, you will obviously need roughly 1800 pixels to display 
them with an 8 point font and 1 pixel between each line of text.
Most current 17" and 19" TFT displays have 1280 pixels in width ....

Uwe Ligges


> Please help.
> Thank you very much.
> -DS.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Aug 31 10:15:21 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 31 Aug 2006 10:15:21 +0200
Subject: [R] forestplot fucntion in rmeta package
In-Reply-To: <6c583e880608291155s4e847962tb6af26da92041ea6@mail.gmail.com>
References: <6c583e880608291155s4e847962tb6af26da92041ea6@mail.gmail.com>
Message-ID: <44F69A99.8020207@statistik.uni-dortmund.de>



zhe zhang wrote:
> Dear R users,
> 
> I would like to adjust the x axis the way I wanted using
> "forestplot(labeltext, mean, lower, upper, align = NULL, xlab = "", zero =
> 0, graphwidth = unit(2, "inches"), col = meta.colors(), xlog = FALSE)". I
> tried using xaxt="n" and then redefine the axis using axis(1, at=c(0,0.5,1.0
> ,1.5,2.0,2.5),label=c(0,0.5,1.0,1.5,2.0,2.5)), but it doesn't work at all.
> Seems the forestplot function itself does not allow some basic plot option
> like xaxt="n"? Anybody porvides some hints to solve this problem? Many
> thanks!

Either adapt the function yourself and provide a patch to the package 
maintainer, who certainly will be happy if people are contributing 
features and bugfixes, or send a feature request to the package maintainer.

Uwe Ligges


> 
> Shirley
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From alok at igidr.ac.in  Thu Aug 31 10:08:34 2006
From: alok at igidr.ac.in (Alok kumar)
Date: Thu, 31 Aug 2006 13:38:34 +0530 (IST)
Subject: [R] Moving Window regressions with corrections for
 Heteroscedasticity and Autocorrelations(HAC)
Message-ID: <16574177.1157011714550.JavaMail.root@webmail.igidr.ac.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/e0b04bba/attachment.pl 

From g.russell at eos-finance.com  Wed Aug 30 14:47:34 2006
From: g.russell at eos-finance.com (g.russell at eos-finance.com)
Date: Wed, 30 Aug 2006 14:47:34 +0200
Subject: [R] Antwort: Buying more computer for GLM
In-Reply-To: <OFDB88C6A7.984B7C03-ONC12571B2.004B2CB8-C12571B2.004BCD68@LocalDomain>
Message-ID: <OF277EC43C.AA7C36F4-ONC12571DA.00458E22-C12571DA.004645E8@DFD-Hamburg>

Hello,

at the moment I am doing quite a lot of regression, especially 
logistic regression, on 20000 or more records with 30 or more 
factors, using the "step" function to search for the model with the 
smallest AIC.   This takes a lot of time on this 1.8 GHZ Pentium 
box.   Memory does not seem to be such a big problem; not much 
swapping is going on and CPU usage is at or close to 100%.    What 
would be the most cost-effective way to speed this up?    The 
obvious way would be to get a machine with a faster processor (3GHz 
plus) but I wonder whether it might instead be better to run a dual-
processor machine or something like that; this looks at least like a
problem R should be able to parallelise, though I don't know whether it 
does.

Thanks for your help,
 
George Russell


From prasannaprakash at gmail.com  Thu Aug 31 12:20:59 2006
From: prasannaprakash at gmail.com (Prasanna BALAPRAKASH)
Date: Thu, 31 Aug 2006 12:20:59 +0200
Subject: [R] Ranking and selection statistical procedure
Message-ID: <07AD12B9-5B0A-4D1E-8C38-2204184C1226@gmail.com>

Dear R helpers

I would like to know if the "Ranking and Selection" statistical
procedure has been implemented in R. I made a quick search in the R
packages list but I could not find it.

Thanks in advance
Prasanna


From buser at stat.math.ethz.ch  Thu Aug 31 12:28:05 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 31 Aug 2006 12:28:05 +0200
Subject: [R] lmer applied to a wellknown (?) example
In-Reply-To: <44F5C48F.9010309@bio.ntnu.no>
References: <44F5C48F.9010309@bio.ntnu.no>
Message-ID: <17654.47541.76798.205703@stat.math.ethz.ch>

Dear Henrik

There is an article in the R-News "Fitting linear mixed models
in R" in which you can find some examples for the syntax of
nested and non-nested design.

http://cran.r-project.org/doc/Rnews/Rnews_2005-1.pdf

Hope this helps

Christoph 

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Henrik Parn writes:
 > Dear all,
 > 
 > During my pre-R era I tried (yes, tried) to understand mixed models by 
 > working through the 'rat example' in Sokal and Rohlfs Biometry (2000) 
 > 3ed p 288-292. The same example was later used by Crawley (2002) in his 
 > Statistical Computing p 363-373 and I have seen the same data being used 
 > elsewhere in the litterature.
 > 
 > Because this example is so thoroughly described, I thought it would be 
 > very interesting to analyse it also using lmer and to see how the 
 > different approaches and outputs differs - from the more or less manual 
 > old-school (?) approach in Sokal, aov in Crawley, and to mixed models by 
 > lmer.
 > 
 > In the example, three treatments (Treatment) with two rats (Rat) each 
 > (i.e six unique rats in total). Three liver preparations (Liver) are 
 > taken from each rat (i.e 18 unique liver preparations), and two glycogen 
 > readings (Glycogen) are taken from each liver preparation (36 readings).
 > 
 > We want to test if treatments has affected the glycogen levels. The 
 > readings are nested in preparation and the preparations nested in rats.
 > 
 > The data can be found here (or on p. 289 in Sokal):
 > http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/rats.txt
 > //
 > I was hoping to use the rat example as some kind of reference on my way 
 > to understand mixed models and using lmer. However, first I wish someone 
 > could check my suggested models!
 > 
 > My suggestions:
 > 
 > attach(rats)
 > rats$Glycogen <- as.numeric(Glycogen)
 > rats$Treatment <- as.factor(Treatment)
 > rats$Rat <- as.factor(Rat)
 > rats$Liver <- as.factor(Liver)
 > str(rats)
 > 
 > model1 <- lmer(Glycogen ~ Treatment + (1|Liver) + (1|Rat), data=rats)
 > summary(model1)
 > 
 > Was that it?
 > 
 > I also tried to make the 'liver-in-rat' nesting explicit  (as suggested 
 > in 'Examples from...')
 >  
 > model2 <- lmer(Glycogen ~ Treatment + (1|Rat:Liver) + (1|Rat), data=rats)
 > summary(model2)
 > 
 > but then the random effects differs from model1.
 > 
 > Does the non-unique coding of rats and preparations in the original data 
 > set mess things up? Do I need to recode the ids to unique levels...
 > 
 > rats$rat2 <- as.factor(rep(1:6, each=6))
 > rats$liver2 <- as.factor(rep(1:18, each=2))
 > str(rats)
 > 
 > ...and then:
 > 
 > model3 <- lmer(Glycogen ~ Treatment + (1|liver2) + (1|rat2), data=rats)
 > # or maybe
 > model3 <- lmer(Glycogen ~ Treatment + (1|rat2:liver2) + (1|rat2), data=rats)
 >  
 > 
 > Can anyone help me to get this right! Thanks in advance!
 > 
 > P.S.
 > Thanks to all contributors to lme/lmer topic on the list (yes, I have 
 > searched for 'lmer nested'...) and also the examples provided by John 
 > Fox' 'Linear mixed models, Appendix to An R and S-PLUS companion...' and 
 > Douglas Bates' 'Examples from Multilevel Software...' and R-news 5/1. 
 > Very helpful, but as usually I bet I missed something...Sorry.
 > 
 > Regards,
 > 
 > Henrik 
 > 
 > -- 
 > ************************
 > Henrik P?rn
 > Department of Biology
 > NTNU
 > 7491 Trondheim
 > Norway
 > 
 > +47 735 96282 (office)
 > +47 909 89 255 (mobile)
 > +47 735 96100 (fax)
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Thu Aug 31 12:49:33 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 31 Aug 2006 12:49:33 +0200
Subject: [R] plot(augPred) and abline help
Message-ID: <44F6DADD.17269.1130FB5@localhost>

Dear all

I try to add a line, (vertical in my case) but at present time I am 
only partially successful.

fm1 <- lme(Orthodont)

# standard plot
plot(augPred(fm1, level = 0:1, length.out = 2))

#plot with vertical but without points and fitted lines
plot(augPred(fm1, level = 0:1, length.out = 2),
panel=function(v,...) {
panel.abline(v=10)}
)

# plot with vertical but without fitted lines
plot(augPred(fm1, level = 0:1, length.out=2),
panel=function(x,y,...) {
panel.xyplot(x,y,...)
panel.abline(v=10)}
)

# plot with vertical and with all points (fitted lines are drawn as 
points)
plot(augPred(fm1, level = 0:1),
panel=function(x,y,...) {
panel.xyplot(x,y,...)
panel.abline(v=10)}
)

I am probably somewhere close but I have no clue, which parameter I 
shall modify to get measured points, fitted lines and vertical lines 
in panels together.

Please help

Thank you
Best regards.

Petr Pikal
petr.pikal at precheza.cz


From Nigel.Findlater at partnerre.com  Thu Aug 31 13:15:29 2006
From: Nigel.Findlater at partnerre.com (Nigel.Findlater at partnerre.com)
Date: Thu, 31 Aug 2006 13:15:29 +0200
Subject: [R] DCOM 1.3.5 Exception from HRESULT: 0x80040013 on
	iR.Init("R")
In-Reply-To: <Pine.LNX.4.64.0608310846530.17209@gannet.stats.ox.ac.uk>
Message-ID: <OF75FF6D4A.65A43581-ONC12571DB.003DAC59-C12571DB.003E0227@partnerre.com>

Thanks for the comment, I have re-sent the question to
rcom-l at mailman.csd.univie.ac.at




                                                                           
             Prof Brian Ripley                                             
             <ripley at stats.ox.                                             
             ac.uk>                                                     To 
                                       Nigel.Findlater at partnerre.com       
             31/08/2006 09:48                                           cc 
                                       r-help at stat.math.ethz.ch            
                                                                   Subject 
                                       Re: [R] DCOM 1.3.5 Exception from   
                                       HRESULT: 0x80040013 on iR.Init("R") 
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




On Thu, 31 Aug 2006, Nigel.Findlater at partnerre.com wrote:

>
> Hallo Everyone,
>
> I have a problem getting R DCom to work on a MS Server 2003. I am using
> Visual Studio 2005 to use some statistical functions from R. I have
> installed DCOM 1.3.5. but as soon as the statement iR.Init("R") is
executed
> I get the following error:
>
> Exception from HRESULT: 0x80040013
>
> I have checked the path and the environmental variables and there are
none
> associated with the RDCom, which is what I would expect.
>
> I searched the registry for differences between my laptop (Windows XP
SP2)
> where it works but have not found anything useful
>
> I have also tried to install Version 2.0.0 from
> http://cran.r-project.org/contrib/extra/dcom/RSrv200.html. But this did
not
> resolve the prblem
>
> Any ideas?

That page tells you the appropriate mailing list for questions about that
software, or you can do as the R posting guide asks and contact the
authors/maintainers.

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595




DISCLAIMER: This e-mail contains information solely intended...{{dropped}}


From p.dalgaard at biostat.ku.dk  Thu Aug 31 13:44:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Aug 2006 13:44:28 +0200
Subject: [R] Antwort: Buying more computer for GLM
In-Reply-To: <OF277EC43C.AA7C36F4-ONC12571DA.00458E22-C12571DA.004645E8@DFD-Hamburg>
References: <OF277EC43C.AA7C36F4-ONC12571DA.00458E22-C12571DA.004645E8@DFD-Hamburg>
Message-ID: <x2mz9lf7tv.fsf@viggo.kubism.ku.dk>

g.russell at eos-finance.com writes:

> Hello,
> 
> at the moment I am doing quite a lot of regression, especially 
> logistic regression, on 20000 or more records with 30 or more 
> factors, using the "step" function to search for the model with the 
> smallest AIC.   This takes a lot of time on this 1.8 GHZ Pentium 
> box.   Memory does not seem to be such a big problem; not much 
> swapping is going on and CPU usage is at or close to 100%.    What 
> would be the most cost-effective way to speed this up?    The 
> obvious way would be to get a machine with a faster processor (3GHz 
> plus) but I wonder whether it might instead be better to run a dual-
> processor machine or something like that; this looks at least like a
> problem R should be able to parallelise, though I don't know whether it 
> does.

Is this floating point bound? (When you say 30 factors does that mean
30 parameters or factors representing a much larger number of groups).
If it is integer bound, I don't think you can do much better than
increase CPU speed and - note - memory bandwidth (look for large-cache
systems and fast front-side bus). To increase floating point
performance, you might consider the option of using optimized BLAS
(see the Windows FAQ 8.2 and/or the "R Installation and
Administration" manual) like ATLAS; this in turn may be multithreaded
and make use of multiple CPUs or multi-core CPUs.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From vito_ricci at yahoo.com  Thu Aug 31 13:47:17 2006
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 31 Aug 2006 13:47:17 +0200 (CEST)
Subject: [R] Error in memory allocation
Message-ID: <20060831114717.28609.qmail@web36101.mail.mud.yahoo.com>

Dear UseRs,

I need a litle help. 
I'm trying to read a .dat file of about 71 MB using
read.fwf (as data recorded are in fixed width
formatted data). R starts in reading data, but after
some time, about 1/2 hour, I get this error:

Error in .signalSimpleWarning("Reached total
allocation of 510: see help(memory.size)"

memory.size()
529880586

R is running under windows:

sysname                       release 
"Windows"                      "NT 5.1" 
version                      nodename 
"(build 2600) Service Pack 2"  "PC-TULIP" 
machine                         login 
"x86"                   "Utente-01" 
user                   "Utente-01" 

Can anyone help me? I need to read those data!

Thanks in advance.

Regards.

Vito Ricci




Se non ora, quando?
Se non qui, dove?
Se non tu, chi?


From AnupTyagi at yahoo.com  Thu Aug 31 13:47:43 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Thu, 31 Aug 2006 11:47:43 +0000 (UTC)
Subject: [R] Antwort: Buying more computer for GLM
References: <OFDB88C6A7.984B7C03-ONC12571B2.004B2CB8-C12571B2.004BCD68@LocalDomain>
	<OF277EC43C.AA7C36F4-ONC12571DA.00458E22-C12571DA.004645E8@DFD-Hamburg>
Message-ID: <loom.20060831T134017-579@post.gmane.org>

Please look at http://boinc.berkeley.edu/

Your problem seems to be similar to the ones for which BOINC is used. I am not
sure how to do this with R, though. May be other people in this can help.

Anupam.


From roger at ysidro.econ.uiuc.edu  Thu Aug 31 13:50:22 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Thu, 31 Aug 2006 06:50:22 -0500
Subject: [R] Ranking and selection statistical procedure
In-Reply-To: <07AD12B9-5B0A-4D1E-8C38-2204184C1226@gmail.com>
References: <07AD12B9-5B0A-4D1E-8C38-2204184C1226@gmail.com>
Message-ID: <A68D671D-2DE0-48BB-8C78-983400F3207B@ysidro.econ.uiuc.edu>

Look at ?rank ?order and ?quantile  assuming that you are using
these terms as in cs.


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Aug 31, 2006, at 5:20 AM, Prasanna BALAPRAKASH wrote:

> Dear R helpers
>
> I would like to know if the "Ranking and Selection" statistical
> procedure has been implemented in R. I made a quick search in the R
> packages list but I could not find it.
>
> Thanks in advance
> Prasanna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ccatj at web.de  Thu Aug 31 13:58:38 2006
From: ccatj at web.de (Christian Jones)
Date: Thu, 31 Aug 2006 13:58:38 +0200
Subject: [R] need help with an interaction term
Message-ID: <702009194@web.de>

Hello!
I?m fitting a model with glm(family binomial). The best model counts 9 Variables and includes an interaction term that was generated by the product of to continuous variables (a*b). All variables are correlated under a value of 0.7 (Spearman rank order) While the estimates of both main effects are negativ, the resulting interaction term is positiv. This change of sign makes it difficult to interpret the model and above all, is this perhaps due to a bad variable choice ?
Thanks a lot for helping
Christian

_______________________________________________________________________
Viren-Scan f?r Ihren PC! Jetzt f?r jeden. Sofort, online und kostenlos.
Gleich testen! http://www.pc-sicherheit.web.de/freescan/?mc=022222


From ccleland at optonline.net  Thu Aug 31 14:15:23 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 31 Aug 2006 08:15:23 -0400
Subject: [R] need help with an interaction term
In-Reply-To: <702009194@web.de>
References: <702009194@web.de>
Message-ID: <44F6D2DB.4070705@optonline.net>

Christian Jones wrote:
> Hello!
> I?m fitting a model with glm(family binomial). The best model counts 9 Variables and includes an interaction term that was generated by the product of to continuous variables (a*b). All variables are correlated under a value of 0.7 (Spearman rank order) While the estimates of both main effects are negativ, the resulting interaction term is positiv. This change of sign makes it difficult to interpret the model and above all, is this perhaps due to a bad variable choice ?
> Thanks a lot for helping

  Rather than trying to interpret the model coefficients directly, you
might visualize the a*b interaction effect.  The effects package by John
Fox is very useful for this:

http://cran.r-project.org/doc/packages/effects.pdf

> Christian
> 
> _______________________________________________________________________
> Viren-Scan f?r Ihren PC! Jetzt f?r jeden. Sofort, online und kostenlos.
> Gleich testen! http://www.pc-sicherheit.web.de/freescan/?mc=022222
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From rolf at erdos.math.unb.ca  Thu Aug 31 14:32:59 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Thu, 31 Aug 2006 09:32:59 -0300 (ADT)
Subject: [R] Summary and thanks: .Rprofile under Windoze.
Message-ID: <200608311232.k7VCWxFX009214@erdos.math.unb.ca>


Many thanks to all who responded to my cri de coeur: Charles Annis,
John Bollinger, Petr Pikal, Brian Ripley, Duncan Murdoch, Dan
Nordlund, and Richard Heiberger.  (I don't think --- I hope --- I
haven't missed anyone.)

The essential solution to my problem was indeed to eliminate that
@#@$#@$^#@ ``.txt'' extension from the filename.  I.e.  the file must
be called ``.Rprofile'' and NOT ``.Rprofile.txt''.  (No shit,
Sherlock!)

So how to effect this?  Several people suggested using a better
editor than Notepad (Good Idea!) but even Notepad can be coerced into
cooperating by clicking on ``All Files'' in the ``Save as type''
wrecked-angle, rather than leaving in the default option
``Text Documents (*.txt)''.

In retrospect this seems rather obvious ... sigh.

It was also pointed out to me that one can keep Windoze from hiding
the file extension (which is its confusing default behaviour and which
substantially contributed to my frustration).

I achieved this by clicking on

    Start --> Control Panel --> Tools --> Folder Options --> View

and then unchecking ``Hide extensions for known file types'' and
clicking on ``Apply to All Folders''.

Thanks again to everyone.
				cheers,

					Rolf Turner
					rolf at math.unb.ca


From pmccask at cyllene.uwa.edu.au  Thu Aug 31 14:33:41 2006
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Thu, 31 Aug 2006 20:33:41 +0800
Subject: [R] predict.lm within a function
Message-ID: <44F6D725.1020301@cyllene.uwa.edu.au>

Hi
I'm trying to wrap predict.lm within a function, but I'm having problems 
passing arguments into it in this way.

Basically I want to create a lm object, then pass it into the predict.lm 
function and be able to tell predict.lm which variable I want to predict 
for, by passing the variable name as an argument of the wrapper 
function. This variable will always be a factor with 3 levels, so I want 
to predict the response within each of these levels evaluated at the 
mean value of all other variables in the model (as indicated below by 
replicating the means for the data 3 times in the data.frame part of 
predict.lm).

The simplest of all my attempts looks like this:

my.fun <- function(formula, mydata, predict_variable){
        my.lm <- lm(formula=formula, data=mydata)
        my.predict <- 
predict.lm(my.lm,(data.frame(predict_variable=y,rbind(mean(mydata, 
na.rm=T),mean(mydata, na.rm=T), mean(mydata,na.rm=T)), 
row.names=as.numeric(names(table(mydata$predict_variable))))))
        }

outside of the function, the following commands do exactly what I want:

my.lm <- lm(LDL ~ AGE + SEX + factor(SNP1_add), data=testdata)
predict.lm(my.lm,(data.frame(SNP1_add=y,rbind(mean(testdata, 
na.rm=T),mean(testdata, na.rm=T), mean(testdata,na.rm=T)), 
row.names=as.numeric(names(table(testdata$SNP1_add))))))

where SNP1_add is the variable to predict for, but when I run

my.fun(formula=LDL ~ AGE + SEX + factor(SNP1_add), mydata=testdata, 
predict_variable="SNP1_add")

I get: Error in sort(unique.default(x), na.last = TRUE) :
        'x' must be atomic

I figure I should be using eval() or substitute() somewhere to but none 
of my attempts have been successful. Any help would be appreciated.

-- 
Pamela A McCaskie
BSc(Mathematical Sciences)(Hons)

Laboratory for Genetic Epidemiology
Western Australian Institute for Medical Research
University of Western Australia
Ground Floor, B Block
Hospital Avenue, Nedlands
Western Australia  6009
AUSTRALIA
Email:        pmccask at cyllene.uwa.edu.au
Phone:        +61 (0)8 9346 1612
Mob:          +61 (0)417 926 607

This e-mail and all attachments contain information which is confidential and/or subject to copyright. It may not be used or disclosed unless authorised by the sender.


From ripley at stats.ox.ac.uk  Thu Aug 31 14:51:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 13:51:40 +0100 (BST)
Subject: [R] NaN when using dffits, stemming from  lm.influence  call
In-Reply-To: <44F6470C.1020200@usq.edu.au>
References: <44F6470C.1020200@usq.edu.au>
Message-ID: <Pine.LNX.4.64.0608311304240.21681@gannet.stats.ox.ac.uk>

When applying dffits to a GLM, you are presumably intending to apply it to 
the working regression.  However, that is not what lm.influence has long 
been set up to do (it uses the deviance residuals).

In your example the influence is high and so the approximations of 
applying the standard formulae to the working regression are unrealistic, 
hence the NaN (it is predicting a negative variance on deleting that 
case, which is a failure of the approximation used).

If you look at the reference (Williams) you will find discussion of the 
approxmations and indeed what is possibly a better approximation in his 
'likelihood residuals'.


On Thu, 31 Aug 2006, Peter Dunn wrote:

> Hi all
> 
> I'm getting a NaN returned on using dffits, as explained
> below.  To me, there seems no obvious (or non-obvious reason
> for that matter) reason why a  NaN  appears.
> 
> Before I start digging further, can anyone see why  dffits
> might be failing?  Is there a problem with the data?
> 
> 
> Consider:
> 
> # Load data
> dep <-
> read.table("http://www.sci.usq.edu.au/staff/dunn/Datasets/Books/Hand/Hand-R/factor1-R.dat",
>    header=TRUE)
> attach(dep)
> dep
> 
> # Fit Poisson glm
> dep.glm2 <- glm( Counts ~ factor(Depression) + factor(SLE) + factor(Children)
> + factor(Depression):factor(SLE),
>    family=poisson( link=log) )
> 
> # Compute dffits
> dffits( dep.glm2 )
> 
> 
> This produces the output:
>          1          2          3          4          5          6 1.4207746
>      -0.1513808        NaN  0.9079142 -0.1032664 -1.0860289
>          7          8
> 0.4853797  3.8560863
> 
> NaN exists for Observation 3.  I cannot understand why: there's
> nothing grossly unusual or bad about it.  I look a bit closer,
> and it falls over in lm.influence when computing the deletion
> statistic sigma:
> 
> 
> 
> > lm.influence(dep.glm2)$sigma
>        1        2        3        4        5        6        7        8
> 0.914829 2.134279      NaN 2.186707 2.224885 1.934539 2.225115 1.957111
> 
> The rest of the results from lm.influence are OK; for example:
> 
> > lm.influence(dep.glm2)$wt.res
>           1           2           3           4           5           6
>  2.62840627 -0.88476903 -1.09492912  0.20247856 -0.23114458 -0.95123387
>           7           8
>  0.07521515  0.30208051
> 
> 
> Use of debug( lm.influence ) indicates the NaN appears in this line
> of lm.influence:
> 
> 
> 
> res <- .Fortran("lminfl", model$qr$qr, n, n, k, as.integer(do.coef),
>     model$qr$qraux, wt.res = e, hat = double(n), coefficients = if (do.coef)
> matrix(0,
>         n, k) else double(0), sigma = double(n), tol = 10 *
> .Machine$double.eps,
>     DUP = FALSE, PACKAGE = "base")[c("hat", "coefficients", "sigma",
>     "wt.res")]
> 
> 
> I don't particularly wish to dig around in the Fortran if someone
> else can look at it and see my problem easily.  But if I must...
> 
> 
> 
> The appearance of the  NaN  seems odd, since (as I understand it)
>   lm.influence(dep.glm2)$sigma  computes  sigma  when each observation
> is removed in turn.  So if I remove Observation 3 and try fitting the
> model, there are no problems or complaints:
> 
> dep.glm3 <- glm( Counts ~ factor(Depression) + factor(SLE) +
>    factor(Children) + factor(Depression):factor(SLE),
>    family=poisson( link=log), subset=(-3) )
> 
> 
> This produces:
> 
> 
> > dep.glm3
> 
> Call:  glm(formula = Counts ~ factor(Depression) + factor(SLE) +
> factor(Children) +      factor(Depression):factor(SLE), family = poisson(link
> = log),      subset = (-3))
> 
> Coefficients:
>                      (Intercept)               factor(Depression)1
>                           5.4389                           -4.1392
>                     factor(SLE)1                 factor(Children)1
>                          -0.6503                           -2.4036
> factor(Depression)1:factor(SLE)1
>                           3.9513
> 
> Degrees of Freedom: 6 Total (i.e. Null);  2 Residual
> Null Deviance:      695.9
> Residual Deviance: 0.8535       AIC: 41.25
> 
> 
> No problems, errors, or any signs of potential problems.
> 
> 
> In the changes to R 2.3.0 (in the NEWS file,
> eg http://mirror.aarnet.edu.au/pub/CRAN/src/base/NEWS)
> I find this:
> 
>    o	Influence measures such as rstandard() and cooks.distance()
> 	could return infinite values rather than NaN for a case which
> 	was fitted exactly.  Similarly, plot.lm() could fail on such
> 	examples.  plot.lm(which = 5)  had to be modified to only plot
> 	cases with hat < 1.  (PR#8367)
> 
> 	lm.influence() was incorrectly reporting 'coefficients' and
> 	'sigma' as NaN for cases with hat = 1, and on some platforms
> 	not detecting hat = 1 correctly.
> 
> The last sentence identifies NaN being reported for sigma, as I
> find with my data.  But my data do not have hat = 1, but the hat
> diagonals are large.  The troublesome Observation 3 does not have the
> largest hat value in the data though:
> 
> > hatvalues(dep.glm2)
>        1         2         3         4         5         6         7 
>        8
> 0.1689061 0.1064651 0.9098542 0.9030814 0.3799079 0.6382790 0.9327408
> 0.9607654
> 
> And besides, I am using the most recent version of R (see below).  BTW,
> the NaNs appear in the previous version of R also.
> 
> So I'm flummoxed.
> 
> As always, help appreciated.
> 
> P.
> 
> 
> 
> > version
>                _
> platform       i386-pc-linux-gnu
> arch           i386
> os             linux-gnu
> system         i386, linux-gnu
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From clcarter at umd.edu  Thu Aug 31 14:52:35 2006
From: clcarter at umd.edu (Catherine Carter)
Date: Thu, 31 Aug 2006 08:52:35 -0400
Subject: [R] Problems with OS X R
Message-ID: <DC9588C8-7F2B-427A-8BCF-E3482F3D610C@umd.edu>

Hi everyone,
I hope I'm not repeating someone else, but I am having trouble with  
my R installation on my MacBook with OS X Universal. I get this error  
when R starts up:

2006-08-30 10:53:50.222 R[1021] *** -[NSBundle load]: Error loading  
code /Users/clc/Library/InputManagers/Smart Crash Reports/Smart Crash  
Reports.bundle/Contents/MacOS/Smart Crash Reports for bundle /Users/ 
clc/Library/InputManagers/Smart Crash Reports/Smart Crash  
Reports.bundle, error code 2 (link edit error code 0, error number 0 ())

and this one when I try to update packages.

Error in update.packages(lib = ""/Library/Frameworks/R.framework/ 
Resources/library/"",  :
	object "Library" not found
 >

I have tried deleting all the pieces I could find and re-installing,  
but that didn't seem to work.
Any suggestions?

Thanks,
Catherine

---------------------------------
Dr. Catherine Carter
University of Maryland
Department of Geography
1111A Lefrak
301.405.4620u


From Patrik.Ohagen at mpa.se  Thu Aug 31 15:06:48 2006
From: Patrik.Ohagen at mpa.se (=?iso-8859-1?Q?=D6hagen_Patrik?=)
Date: Thu, 31 Aug 2006 15:06:48 +0200
Subject: [R]  Plotting a stepwise increasing function
In-Reply-To: <44F6D725.1020301@cyllene.uwa.edu.au>
Message-ID: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>


Dear All, Sorry for spamming you with something trivial. I would like to plot a "stepwise increasing function" (if that makes sense? See crude ASCHII art below). I think there is some clever paramter for solving the problem but I just cannot find it.


        _______________|
        |
        |
________|
|
|
|


Thank you in advance!


Cheers, Patrik


From p.dalgaard at biostat.ku.dk  Thu Aug 31 15:13:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Aug 2006 15:13:28 +0200
Subject: [R] Plotting a stepwise increasing function
In-Reply-To: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>
References: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>
Message-ID: <x2hcztf3pj.fsf@viggo.kubism.ku.dk>

?hagen Patrik <Patrik.Ohagen at mpa.se> writes:

> Dear All, Sorry for spamming you with something trivial. I would
> like to plot a "stepwise increasing function" (if that makes sense?
> See crude ASCII art below). I think there is some clever paramter
> for solving the problem but I just cannot find it.
> 
> 
>         _______________|
>         |
>         |
> ________|
> |
> |
> |

You may be looking for

type="s" or "S"

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From joris.dewolf at cropdesign.com  Thu Aug 31 15:13:10 2006
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Thu, 31 Aug 2006 15:13:10 +0200
Subject: [R] Plotting a stepwise increasing function
In-Reply-To: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>
References: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>
Message-ID: <44F6E066.10603@cropdesign.com>

see
plot(y~x,type="s")

?hagen Patrik wrote:
> Dear All, Sorry for spamming you with something trivial. I would like to plot a "stepwise increasing function" (if that makes sense? See crude ASCHII art below). I think there is some clever paramter for solving the problem but I just cannot find it.
> 
> 
>         _______________|
>         |
>         |
> ________|
> |
> |
> |
> 
> 
> Thank you in advance!
> 
> 
> Cheers, Patrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}


From dimitris.rizopoulos at med.kuleuven.be  Thu Aug 31 15:17:26 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 31 Aug 2006 15:17:26 +0200
Subject: [R] Plotting a stepwise increasing function
References: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>
Message-ID: <015901c6ccff$cdb51000$0540210a@www.domain>

probably you're looking for:

plot(rnorm(10), type = "s")
# or
plot(rnorm(10), type = "S")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "?hagen Patrik" <Patrik.Ohagen at mpa.se>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 31, 2006 3:06 PM
Subject: [R] Plotting a stepwise increasing function


>
> Dear All, Sorry for spamming you with something trivial. I would 
> like to plot a "stepwise increasing function" (if that makes sense? 
> See crude ASCHII art below). I think there is some clever paramter 
> for solving the problem but I just cannot find it.
>
>
>        _______________|
>        |
>        |
> ________|
> |
> |
> |
>
>
> Thank you in advance!
>
>
> Cheers, Patrik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jrkrideau at yahoo.ca  Thu Aug 31 15:38:53 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 31 Aug 2006 09:38:53 -0400 (EDT)
Subject: [R] Plotting a stepwise increasing function
In-Reply-To: <2BAF2D3C41D1274E9228E63287F19B7E06B05D@mailsrv2.loginmpa.mpa.se>
Message-ID: <20060831133853.70832.qmail@web32807.mail.mud.yahoo.com>


--- ?hagen Patrik <Patrik.Ohagen at mpa.se> wrote:

> 
> Dear All, Sorry for spamming you with something
> trivial. I would like to plot a "stepwise increasing
> function" (if that makes sense? See crude ASCHII art
> below). I think there is some clever paramter for
> solving the problem but I just cannot find it.
> 
> 
>         _______________|
>         |
>         |
> ________|
> |
> |
> |
> 

methods(plot)
?plot.stepfun

This looks like what you want


From epistat at gmail.com  Thu Aug 31 16:12:51 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 31 Aug 2006 22:12:51 +0800
Subject: [R] what's wrong with my simulation programs on logistic regression
Message-ID: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/48dc61ea/attachment.pl 

From epistat at gmail.com  Thu Aug 31 16:21:55 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 31 Aug 2006 22:21:55 +0800
Subject: [R] what's wrong with my simulation programs on logistic
	regression
In-Reply-To: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>
References: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>
Message-ID: <2fc17e30608310721x7f0e6c60ie69dce0c5f05b224@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/05d423b2/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Aug 31 16:25:18 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 31 Aug 2006 16:25:18 +0200
Subject: [R] what's wrong with my simulation programs on logistic
	regression
References: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>
Message-ID: <018001c6cd09$48fe8f80$0540210a@www.domain>

In this way you don't simulate response data; it'd also be a good idea 
to consider a bigger sample, e.g.,

dat <- matrix(rnorm(4000), ncol = 8)
dat <- data.frame(dat)
names(dat) <- paste("x", 1:8, sep = "")
dat$y <- rbinom(nrow(dat), 1, plogis(rowSums(dat)))

fit <- glm(y ~ ., family = binomial, data = dat)
fit

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "zhijie zhang" <epistat at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, August 31, 2006 4:12 PM
Subject: [R] what's wrong with my simulation programs on logistic 
regression


> Dear friends,
> I'm doing a simulation on logistic regression model, but the 
> programs can't
> work well,please help me to correct it and give some suggestions.
> My programs:
> data<-matrix(rnorm(400),ncol=8)  #sample size is 50
> data<-data.frame(data)
> names(data)<-c(paste("x",1:8,sep=""))  #8 independent 
> variables,x1-x8;
> #logistic regression model is logit(y)=x1+x2+x3+x4+x5+x6+x7+x8
> data$y<-exp(data$x1+data$x2+data$x3+data$x4+data$x5+data$x6+data$x7+data$x8)/(1+(data$x1+data$x2+data$x3+data$x4+data$x5+data$x6+data$x7+data$x8))
>
> logist<-glm(y~.,family=binomial(),data=simdata)
> *Warning messages:*
> 1: algorithm can't converge in: glm.fit(x = X, y = Y, weights = 
> weights,
> start = start, etastart = etastart,
> 2: the probability is 0 or 1 in: glm.fit (x = X, y = Y, weights = 
> weights,
> start = start, etastart = etastart,
> -- 
> With Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Thu Aug 31 16:55:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 15:55:40 +0100 (BST)
Subject: [R] what's wrong with my simulation programs on logistic
	regression
In-Reply-To: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>
References: <2fc17e30608310712t6be1024bvcbbad215c504e098@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0608311548520.5222@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, zhijie zhang wrote:

> Dear friends,
>  I'm doing a simulation on logistic regression model, but the programs can't
> work well,please help me to correct it and give some suggestions.
> My programs:
> data<-matrix(rnorm(400),ncol=8)  #sample size is 50
> data<-data.frame(data)
> names(data)<-c(paste("x",1:8,sep=""))  #8 independent variables,x1-x8;
> #logistic regression model is logit(y)=x1+x2+x3+x4+x5+x6+x7+x8

Rather it is logit(p) = ...,  and y ~ binomial(1, p)

There is a different sort of 'logistic regression' with 

y = exp(eta)/(1+exp(eta)) + epsilon

but you fit that by nls, not glm.

> data$y<-exp(data$x1+data$x2+data$x3+data$x4+data$x5+data$x6+data$x7+data$x8)/(1+(data$x1+data$x2+data$x3+data$x4+data$x5+data$x6+data$x7+data$x8))

You need exp()/(1+exp()), and the second exp is missing.

Once you have p, you can use data$y <- rbinom(length(p), 1, p)

> logist<-glm(y~.,family=binomial(),data=simdata)
> *Warning messages:*
> 1: algorithm can't converge in: glm.fit(x = X, y = Y, weights = weights,
> start = start, etastart = etastart,
> 2: the probability is 0 or 1 in: glm.fit (x = X, y = Y, weights = weights,
> start = start, etastart = etastart,

You do not have a Bernoulli response: it often helps to look at your 
simulated data to see if it makes sense (just as you would look at real 
data, I hope).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ffmanova at mevik.net  Thu Aug 31 15:32:14 2006
From: ffmanova at mevik.net (
	=?iso-8859-1?q?Bj=F8rn-Helge_Mevik_and_=D8yvind_Langsrud?=)
Date: Thu, 31 Aug 2006 15:32:14 +0200
Subject: [R] [R-pkgs] New package ffmanova for 50-50 MANOVA released
Message-ID: <m0wt8pjajl.fsf@bar.nemo-project.org>

Version 0.1-0 of a new package `ffmanova' is now available on CRAN.

Comments, suggestions, etc. are welcome.  Please use the email address
ffmanova (at) mevik.net.

The package implements 50-50 MANOVA (Langsrud, 2002) with p-value
adjustment based on rotation testing (Langsrud, 2005).

The 50-50 MANOVA method is a modified variant of classical MANOVA made
to handle several highly correlated responses.  Classical MANOVA
performs poorly in such cases and it collapses when the number of
responses exceeds the number of observations.  The 50-50 MANOVA method
is suggested as a general method that will handle all types of data.
Principal component analysis is an integrated part of the algorithm.

The single response special case is ordinary general linear modeling.
Type II sums of squares are used to handle unbalanced designs
(Langsrud, 2003).  Furthermore, the Type II philosophy is extended to
continuous design variables.  This means that the method is invariant
to scale changes.  Centering of design variables is not needed.  The
Type II approach ensures that common pitfalls are avoided.

A univariate F-test p-value for each response can be reported when
several responses are present.  However, with a large number of
response variables, these results are questionable since we will
expect a lot of type I errors ("incorrect significance").  Therefore
the p-values need to be adjusted.

By using rotation testing it is possible to adjust the single response
p-values according to the familywise error rate criterion in an exact
and non-conservative (unlike Bonferroni) way.

It is also possible to adjust p-values according to a false discovery
rate criterion.  Our method is based on rotation testing and allows
any kind of dependence among the responses (Moen et al., 2005).

Note that rotation testing is closely related to permutation testing.
One difference is that rotation testing relies on the multinormal
assumption.  All the classical tests (t-test, F-test, Hotelling T^2
test, ...) can be viewed as special cases of rotation testing.


REFERENCES 

Langsrud, ?. (2002), 50-50 Multivariate Analysis of Variance for
Collinear Responses, Journal of the Royal Statistical Society SERIES D
- The Statistician, 51, 305-317.

Langsrud, ?. (2003), ANOVA for Unbalanced Data: Use Type II Instead of
Type III Sums of Squares, Statistics and Computing, 13, 163-167.

Langsrud, ?. (2005), Rotation Tests, Statistics and Computing, 15, 53-60. 

Moen, B., Oust, A., Langsrud, ?., Dorrell, N., Gemma, L., Marsden,
G.L., Hinds, J., Kohler, A., Wren, B.W. and Rudi, K. (2005), An
explorative multifactor approach for investigating global survival
mechanisms of Campylobacter jejuni under environmental conditions,
Applied and Environmental Microbiology, 71, 2086-2094.


-- 
Bj?rn-Helge Mevik and ?yvind Langsrud

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From mbabyak at nc.rr.com  Thu Aug 31 17:14:59 2006
From: mbabyak at nc.rr.com (Mike)
Date: Thu, 31 Aug 2006 11:14:59 -0400
Subject: [R] sample size for recurrent events
Message-ID: <44F6FCF3.1020609@nc.rr.com>

Dear All,

I'm a relative novice in R, with some experience with S-plus and quite a 
bit in SAS.  In R and S-Plus, I've primarily worked with Frank Harrell's 
Design and Hmisc libraries.  I have R 2.3.1 for windows (XP) installed.

I have a project coming up in which the outcome will be recurrent 
events.  There will be right censoring and likely to be a good stack of 
0s on the left.  In searching the R archives, I came across the gcmrec 
package, which looks like it will estimate the models I might be 
interested in. 

My question is whether anyone is aware of work done on sample size 
planning for these models?  I came across a meeting abstract for a paper 
that apparently presented some SAS code for power estimation, but I 
haven't had any luck contacting the authors.

Does anyone have any experience with this issue or pointers to where I 
might do some reading on this topic?  Of course, any thoughts on the 
approriate model would also be more than welcome.

Thanks,

Mike Babyak


From mkimpel at iupui.edu  Thu Aug 31 17:27:39 2006
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Thu, 31 Aug 2006 11:27:39 -0400
Subject: [R] problem with postscript output of R-devel on Windows
Message-ID: <836F00680EECD340A96AD34ECFF3B53461B1A3@iu-mssg-mbx106.ads.iu.edu>

I have developed a problem with the postscript output of plot on Windows. My code still works properly with R 2.3 but, with R 2.4, the white text on red background does not show up. It does, however, show up when output is sent to the screen. Below is my code and sessionInfo.

R version 2.4.0 Under development (unstable) (2006-08-29 r39012) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "splines"   "tools"     "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[9] "base"     

other attached packages:
  Rgraphviz geneplotter         XML     GOstats    Category    hgu95av2        KEGG    multtest      xtable 
   "1.11.9"    "1.11.8"    "0.99-8"     "1.6.0"     "1.4.1"    "1.12.0"     "1.8.1"    "1.11.2"     "1.3-2" 
       RBGL    annotate          GO       graph       Ruuid       limma  genefilter    survival     rat2302 
    "1.8.1"    "1.11.5"     "1.6.5"   "1.11.13"    "1.11.2"     "2.7.9"    "1.11.8"      "2.28"    "1.12.0" 
       affy      affyio     Biobase 
   "1.11.6"     "1.1.8"   "1.11.29"


fileName<-paste(experiment, contrast, "FDR", FDR, "Graph", "ps", sep=".")
    postscript(file=fileName, paper="special",width=width, height=height) #set up graphics device
    plot(result.gN, layout.param, nodeAttrs = nAttrs, edgeAttrs = eAttrs,
        main=paste(paste("Experiment:", experiment, ";  Contrast:", contrast,";  FDR:", FDR, sep=""), paste("Min. connections ==", min.edges, "Min. citations per connection ==", min.cites, "Additional search criteria:",
            termAdditional, sep=" "), sep="    "))

Thanks,

Mark

Mark W. Kimpel MD 

?


From tlumley at u.washington.edu  Thu Aug 31 17:39:42 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 31 Aug 2006 08:39:42 -0700 (PDT)
Subject: [R] forestplot fucntion in rmeta package
In-Reply-To: <44F69A99.8020207@statistik.uni-dortmund.de>
References: <6c583e880608291155s4e847962tb6af26da92041ea6@mail.gmail.com>
	<44F69A99.8020207@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0608310831320.25565@homer23.u.washington.edu>

On Thu, 31 Aug 2006, Uwe Ligges wrote:
>
> zhe zhang wrote:
>> Dear R users,
>>
>> I would like to adjust the x axis the way I wanted using
>> "forestplot(labeltext, mean, lower, upper, align = NULL, xlab = "", zero =
>> 0, graphwidth = unit(2, "inches"), col = meta.colors(), xlog = FALSE)". I
>> tried using xaxt="n" and then redefine the axis using axis(1, at=c(0,0.5,1.0
>> ,1.5,2.0,2.5),label=c(0,0.5,1.0,1.5,2.0,2.5)), but it doesn't work at all.
>> Seems the forestplot function itself does not allow some basic plot option
>> like xaxt="n"? Anybody porvides some hints to solve this problem? Many
>> thanks!
>
> Either adapt the function yourself and provide a patch to the package
> maintainer, who certainly will be happy if people are contributing
> features and bugfixes, or send a feature request to the package maintainer.
>

The forestplot() function uses grid graphics, for which xaxt="n" is not 
"some basic plot option". More importantly, although it would be easy to 
not draw the x axis, it wouldn't help at all, since axis() still wouldn't 
work.


        -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From cberry at tajo.ucsd.edu  Thu Aug 31 18:13:10 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 31 Aug 2006 09:13:10 -0700
Subject: [R] Antwort: Buying more computer for GLM
In-Reply-To: <OF277EC43C.AA7C36F4-ONC12571DA.00458E22-C12571DA.004645E8@DFD-Hamburg>
References: <OF277EC43C.AA7C36F4-ONC12571DA.00458E22-C12571DA.004645E8@DFD-Hamburg>
Message-ID: <Pine.LNX.4.64.0608310851350.4560@tajo.ucsd.edu>


George,

Logistic regression with ONLY factors?

In principle this can be solved by casting this as a log-linear model of 
counts and using iterative proportional fitting.

For sparse data like yours (i.e. a table with 20000 counts and >= 2^31 
cells), it will be necessary to use a method that does not explicitly 
operate on the table of counts as loglin() does. I would guess that 
rake() in the survey package would handle this, but I've not looked at 
the code it uses.

If you are only using a fraction of the factors then loglm() (in MASS) or 
loglin() may suffice.

HTH,

Chuck

On Wed, 30 Aug 2006, g.russell at eos-finance.com wrote:

> Hello,
>
> at the moment I am doing quite a lot of regression, especially
> logistic regression, on 20000 or more records with 30 or more
> factors, using the "step" function to search for the model with the
> smallest AIC.   This takes a lot of time on this 1.8 GHZ Pentium
> box.   Memory does not seem to be such a big problem; not much
> swapping is going on and CPU usage is at or close to 100%.    What
> would be the most cost-effective way to speed this up?    The
> obvious way would be to get a machine with a faster processor (3GHz
> plus) but I wonder whether it might instead be better to run a dual-
> processor machine or something like that; this looks at least like a
> problem R should be able to parallelise, though I don't know whether it
> does.
>
> Thanks for your help,
>
> George Russell
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From pisicandru at hotmail.com  Thu Aug 31 18:30:14 2006
From: pisicandru at hotmail.com (Monica Pisica)
Date: Thu, 31 Aug 2006 16:30:14 +0000
Subject: [R] problems with plot.data.frame
Message-ID: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>

Hi list,

I have a question about 'plot'. I am trying to plot values registered every 
month - or every other month. If i build a data.frame called mydata like 
this (as an example)

jan   3   1   7
mar  2   4   2
may 1   3   2
jul    3   7   4
sep  5   2   3
nov  3   1   5

and use the command line:

plot(mydata[c(1,3)])

I get a graph that has on the x axis my months in alphabetical order - which 
i don't want, and instead of points i have thick horizontal lines. I've 
tried everything i could and understood from the R help files to give me 
points and on x axis the month in my order instead of alpha order. No 
success. What is the trick?

I fixed the month order by using numerals in front of them like 01, 03, ... 
etc, but this is not an elegant solution.

Any help will be much appreciated.

Monica


From thomas.koenig at epigenomics.com  Thu Aug 31 18:36:57 2006
From: thomas.koenig at epigenomics.com (Thomas Koenig)
Date: 31 Aug 2006 18:36:57 +0200
Subject: [R] S4 Method Dispatch for Sealed Classes
Message-ID: <44F71029.404@epigenomics.com>

Hi,

I encounter a problem with method dispatch with S4 classes, using the
'sealed' parameter in setClass.

See that example below:

setClass("X",representation(x="numeric"),sealed=TRUE)
setGeneric("foo",function(x) standardGeneric("foo"))
setMethod("foo",signature("X"),function(x) print("foo(X)"))
x <- new("X")
foo(x) ## works fine

setClass("Y",representation("X"))
y <- new("Y")
foo(y) ## works fine

setClass("Z",representation("X"),sealed=TRUE)
z <- new("Z")
extends("Z")
## [1] "Z" "X"

foo(z) ## error
Error in foo(z) : no direct or inherited method for function 'foo' for
this call

My question is: why does sealed=TRUE impact on method dispatch?

Please, can you point me to the relevant documentation?
Reading ?setMethod, ?Methods, I failed to find an answer.

Thank you.

Best.

Thomas

Tried on R 2.3.1 and R 2.4.0-devel (2006-08-29 r39012): same result.



-- 
Thomas Koenig                                        Biostatistician
Epigenomics AG    Kleine Praesidentenstr. 1    10178 Berlin, Germany
phone:+49-30-24345-376                          fax:+49-30-24345-555
http://www.epigenomics.com             thomas.koenig at epigenomics.com


From gunter.berton at gene.com  Thu Aug 31 18:46:01 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 31 Aug 2006 09:46:01 -0700
Subject: [R] problems with plot.data.frame
In-Reply-To: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>
Message-ID: <002e01c6cd1c$f15adcc0$711f210a@gne.windows.gene.com>

You need to read R docs. Month is a factor. ?factor
As a result you are getting a bar plot. 
You can make month numeric and control axis labelling via ?axis
See also ?plot.default and ?par

There are also undoubtedly functions available either in base R or through
packages that would do this directly. You might check the hmisc and zoo
packages for starters.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monica Pisica
> Sent: Thursday, August 31, 2006 9:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problems with plot.data.frame
> Importance: High
> 
> Hi list,
> 
> I have a question about 'plot'. I am trying to plot values 
> registered every 
> month - or every other month. If i build a data.frame called 
> mydata like 
> this (as an example)
> 
> jan   3   1   7
> mar  2   4   2
> may 1   3   2
> jul    3   7   4
> sep  5   2   3
> nov  3   1   5
> 
> and use the command line:
> 
> plot(mydata[c(1,3)])
> 
> I get a graph that has on the x axis my months in 
> alphabetical order - which 
> i don't want, and instead of points i have thick horizontal 
> lines. I've 
> tried everything i could and understood from the R help files 
> to give me 
> points and on x axis the month in my order instead of alpha order. No 
> success. What is the trick?
> 
> I fixed the month order by using numerals in front of them 
> like 01, 03, ... 
> etc, but this is not an elegant solution.
> 
> Any help will be much appreciated.
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Thu Aug 31 18:51:28 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 31 Aug 2006 11:51:28 -0500
Subject: [R] Weighed 2D kernel density estimator
Message-ID: <f8e6ff050608310951u4c8ee83flc84ce25bca789d99@mail.gmail.com>

Is there a function/package that performs weighted 2D density
estimates?  I haven't found anything searching with those keywords,
but perhaps there is a more general algorithm that I should be looking
for.

Any hints appreciated.  Thanks,

Hadley


From gunter.berton at gene.com  Thu Aug 31 18:52:09 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 31 Aug 2006 09:52:09 -0700
Subject: [R] problems with plot.data.frame
In-Reply-To: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>
Message-ID: <003801c6cd1d$ccc2ec30$711f210a@gne.windows.gene.com>

I perhaps should have added:

?plot.data.frame, ?plot.factor and ?UseMethod for S3 generic dispatch
(plot.factor is the plotting method actually being called and you are
actually getting a boxplot for a single value.)

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monica Pisica
> Sent: Thursday, August 31, 2006 9:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problems with plot.data.frame
> Importance: High
> 
> Hi list,
> 
> I have a question about 'plot'. I am trying to plot values 
> registered every 
> month - or every other month. If i build a data.frame called 
> mydata like 
> this (as an example)
> 
> jan   3   1   7
> mar  2   4   2
> may 1   3   2
> jul    3   7   4
> sep  5   2   3
> nov  3   1   5
> 
> and use the command line:
> 
> plot(mydata[c(1,3)])
> 
> I get a graph that has on the x axis my months in 
> alphabetical order - which 
> i don't want, and instead of points i have thick horizontal 
> lines. I've 
> tried everything i could and understood from the R help files 
> to give me 
> points and on x axis the month in my order instead of alpha order. No 
> success. What is the trick?
> 
> I fixed the month order by using numerals in front of them 
> like 01, 03, ... 
> etc, but this is not an elegant solution.
> 
> Any help will be much appreciated.
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Aug 31 18:52:34 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 31 Aug 2006 12:52:34 -0400
Subject: [R] problems with plot.data.frame
In-Reply-To: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>
References: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>
Message-ID: <644e1f320608310952p238c0cd0q64f3b4e21840de0e@mail.gmail.com>

Convert the column with the months to an actual date.  I am going to
assume this is all the same year, so try this:

> x <- read.table('clipboard', as.is=T)
> x
   V1 V2 V3 V4
1 jan  3  1  7
2 mar  2  4  2
3 may  1  3  2
4 jul  3  7  4
5 sep  5  2  3
6 nov  3  1  5
> x$Date <- as.POSIXct(strptime(paste(x$V1, '1 2006'), format='%b %d %Y'))
> x
   V1 V2 V3 V4       Date
1 jan  3  1  7 2006-01-01
2 mar  2  4  2 2006-03-01
3 may  1  3  2 2006-05-01
4 jul  3  7  4 2006-07-01
5 sep  5  2  3 2006-09-01
6 nov  3  1  5 2006-11-01
> plot(x$Date, x$V2)
>


On 8/31/06, Monica Pisica <pisicandru at hotmail.com> wrote:
> Hi list,
>
> I have a question about 'plot'. I am trying to plot values registered every
> month - or every other month. If i build a data.frame called mydata like
> this (as an example)
>
> jan   3   1   7
> mar  2   4   2
> may 1   3   2
> jul    3   7   4
> sep  5   2   3
> nov  3   1   5
>
> and use the command line:
>
> plot(mydata[c(1,3)])
>
> I get a graph that has on the x axis my months in alphabetical order - which
> i don't want, and instead of points i have thick horizontal lines. I've
> tried everything i could and understood from the R help files to give me
> points and on x axis the month in my order instead of alpha order. No
> success. What is the trick?
>
> I fixed the month order by using numerals in front of them like 01, 03, ...
> etc, but this is not an elegant solution.
>
> Any help will be much appreciated.
>
> Monica
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From murdoch at stats.uwo.ca  Thu Aug 31 18:51:45 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 31 Aug 2006 12:51:45 -0400
Subject: [R] problem with postscript output of R-devel on Windows
In-Reply-To: <836F00680EECD340A96AD34ECFF3B53461B1A3@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B53461B1A3@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <44F713A1.1080407@stats.uwo.ca>

On 8/31/2006 11:27 AM, Kimpel, Mark William wrote:
> I have developed a problem with the postscript output of plot on Windows. My code still works properly with R 2.3 but, with R 2.4, the white text on red background does not show up. It does, however, show up when output is sent to the screen. Below is my code and sessionInfo.
> 
> R version 2.4.0 Under development (unstable) (2006-08-29 r39012) 
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] "splines"   "tools"     "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
> [9] "base"     
> 
> other attached packages:
>   Rgraphviz geneplotter         XML     GOstats    Category    hgu95av2        KEGG    multtest      xtable 
>    "1.11.9"    "1.11.8"    "0.99-8"     "1.6.0"     "1.4.1"    "1.12.0"     "1.8.1"    "1.11.2"     "1.3-2" 
>        RBGL    annotate          GO       graph       Ruuid       limma  genefilter    survival     rat2302 
>     "1.8.1"    "1.11.5"     "1.6.5"   "1.11.13"    "1.11.2"     "2.7.9"    "1.11.8"      "2.28"    "1.12.0" 
>        affy      affyio     Biobase 
>    "1.11.6"     "1.1.8"   "1.11.29"
> 
> 
> fileName<-paste(experiment, contrast, "FDR", FDR, "Graph", "ps", sep=".")
>     postscript(file=fileName, paper="special",width=width, height=height) #set up graphics device
>     plot(result.gN, layout.param, nodeAttrs = nAttrs, edgeAttrs = eAttrs,
>         main=paste(paste("Experiment:", experiment, ";  Contrast:", contrast,";  FDR:", FDR, sep=""), paste("Min. connections ==", min.edges, "Min. citations per connection ==", min.cites, "Additional search criteria:",
>             termAdditional, sep=" "), sep="    "))


Could you put together a reproducible example to illustrate the problem? 
  We don't have all the variables used in that example.  I think you 
should be able to do it with just base packages attached; if not, it's 
likely a problem with one of the contributed packages, rather than with R.

Duncan Murdoch


From sfalcon at fhcrc.org  Thu Aug 31 19:00:16 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 31 Aug 2006 10:00:16 -0700
Subject: [R] ANN: R/BioC Intro Course Oct 9th-11th in Seattle
Message-ID: <m2ejuwu9gf.fsf@ziti.fhcrc.org>

Hello all,

We will be holding an R/BioC intro course in Seattle.  The dates are
October 9th-11th.

For more information, please visit:

  https://secure.bioconductor.org/biocintro/


Best Wishes,

+ seth


PS:

    At the moment we are using a self-signed SSL certificate (this is
    just as secure encryption-wise, but you have to trust you are
    getting to the right site).  You can verify the fingerprint by
    asking your browser to show you details about the certificate.
    
    MD5: C8 62 A0 E1 55 0F 9F 46 FE 9B 7C B8 53 49 5D D5
    
    SHA1: 61 6D 13 72 FD 12 91 B3 F7 9F DC EB 9F 38 D1 95 E0 DB 7F 25


From ripley at stats.ox.ac.uk  Thu Aug 31 19:06:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Aug 2006 18:06:12 +0100 (BST)
Subject: [R] problems with plot.data.frame
In-Reply-To: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>
References: <BAY108-F1081FE6196DB43E99DE37EC33F0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0608311759030.6673@gannet.stats.ox.ac.uk>

On Thu, 31 Aug 2006, Monica Pisica wrote:

> Hi list,
> 
> I have a question about 'plot'. I am trying to plot values registered every 
> month - or every other month. If i build a data.frame called mydata like 
> this (as an example)
> 
> jan   3   1   7
> mar  2   4   2
> may 1   3   2
> jul    3   7   4
> sep  5   2   3
> nov  3   1   5

This has no column names, so it looks like this is 3 columns with row 
names.  I think it is actually four columns, but it would never print like 
that.

Quite likely then the first column is a factor with levels in alphabetical 
order.  If I copy your layout and do

> read.table("clipboard") -> A
> A
   V1 V2 V3 V4
1 jan  3  1  7
2 mar  2  4  2
3 may  1  3  2
4 jul  3  7  4
5 sep  5  2  3
6 nov  3  1  5

> levels(A[[1]])
[1] "jan" "jul" "mar" "may" "nov" "sep"

and you seem to want

a <- as.character(A[[1]])
A[[1]] <- factor(a, levels=a)

and then

plot.default(A[c(1,3)])

(You are showing a boxplot, since you are plotting a numeric variable 
against a factor.)

> 
> and use the command line:
> 
> plot(mydata[c(1,3)])
> 
> I get a graph that has on the x axis my months in alphabetical order - which 
> i don't want, and instead of points i have thick horizontal lines. I've 
> tried everything i could and understood from the R help files to give me 
> points and on x axis the month in my order instead of alpha order. No 
> success. What is the trick?
> 
> I fixed the month order by using numerals in front of them like 01, 03, ... 
> etc, but this is not an elegant solution.
> 
> Any help will be much appreciated.
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ssj1364 at gmail.com  Thu Aug 31 19:19:39 2006
From: ssj1364 at gmail.com (Spencer Jones)
Date: Thu, 31 Aug 2006 11:19:39 -0600
Subject: [R]  periodic spline in glm
Message-ID: <1c6126db0608311019j285a8316sfbfd27b3f6600e2d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/eb69b51a/attachment.pl 

From lu.yuefeng at gmail.com  Thu Aug 31 19:24:04 2006
From: lu.yuefeng at gmail.com (Lu Yuefeng)
Date: Thu, 31 Aug 2006 13:24:04 -0400
Subject: [R] java R interface
Message-ID: <bc808e8d0608311024s18df6101had2c57de34e51967@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/8c556bd7/attachment.pl 

From jasoncbarnhart at msn.com  Thu Aug 31 19:40:04 2006
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Thu, 31 Aug 2006 10:40:04 -0700
Subject: [R] Error in memory allocation
References: <20060831114717.28609.qmail@web36101.mail.mud.yahoo.com>
Message-ID: <BAY116-DAV14F763097FA70225F98F5CCF3F0@phx.gbl>

My guess is memory is not the issue here.  It appears that you 500+MB of 
system memory and that should suffice for a 71MB dataset.  Additionally, a 
dataset of that size should load in much less time than 30 minutes.

You should read-up on the read.fwf options to ensure that the file is being 
read correctly.  Submitting your read.fwf code fragment and a fragment of 
the data file (like the top 10 lines) will help us diagnose the situation.

Also, if you reduce your data file to something like 10 lines, it's easier 
and quicker to iterate and trouble shoot the loading process.

----- Original Message ----- 
From: "Vito Ricci" <vito_ricci at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 31, 2006 4:47 AM
Subject: [R] Error in memory allocation


> Dear UseRs,
>
> I need a litle help.
> I'm trying to read a .dat file of about 71 MB using
> read.fwf (as data recorded are in fixed width
> formatted data). R starts in reading data, but after
> some time, about 1/2 hour, I get this error:
>
> Error in .signalSimpleWarning("Reached total
> allocation of 510: see help(memory.size)"
>
> memory.size()
> 529880586
>
> R is running under windows:
>
> sysname                       release
> "Windows"                      "NT 5.1"
> version                      nodename
> "(build 2600) Service Pack 2"  "PC-TULIP"
> machine                         login
> "x86"                   "Utente-01"
> user                   "Utente-01"
>
> Can anyone help me? I need to read those data!
>
> Thanks in advance.
>
> Regards.
>
> Vito Ricci
>
>
>
>
> Se non ora, quando?
> Se non qui, dove?
> Se non tu, chi?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pisicandru at hotmail.com  Thu Aug 31 19:57:18 2006
From: pisicandru at hotmail.com (Monica Pisica)
Date: Thu, 31 Aug 2006 17:57:18 +0000
Subject: [R] problems with plot.data.frame
In-Reply-To: <Pine.LNX.4.64.0608311759030.6673@gannet.stats.ox.ac.uk>
Message-ID: <BAY108-F1F7FAE5A5EC316283ABB8C33F0@phx.gbl>

Hi again,

OK i came up with this after i got few good sugegstions.

First my data.frame actually looks like that (Thanks for clarifications to 
Prof. Brian Ripley)

   V1 V2 V3 V4
1  jan  3  1   7
2  mar 2  4  2
3  may 1  3  2
4  jul   3  7  4
5  sep  5  2  3
6  nov  3  1  5

What i want: 1. On x axis i want the ticks with labels column V1 in that 
order and not alpha order.
2. i want points to represent the data, not horizoltal bars as a box-plot 
with only one value as it will plot if i use plot.data.frame

Note. In my table i already have the order i want, but if i wouldn't have it 
prof. Ripley's sugegstion is very welcome.

What i've done in the end:

>plot (mydata$V3, xlab="month")
>axis (side=1, at=c(1:6), labels=c(1:6), ticks=TRUE, col.axis="white")
>month.label <- as.character(mydata[[1]])
>axis(side=1, at=c(1:6), labels = month.label, ticks=TRUE)

This is not an elegant solution but i get the graph i wanted. If anybody has 
a better solution certainly i would like to see it.

thanks you again for all your help,

Monica


From matthew_wiener at merck.com  Thu Aug 31 20:14:08 2006
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 31 Aug 2006 14:14:08 -0400
Subject: [R] java R interface  [Broadcast]
Message-ID: <4E9A692D8755DF478B56A2892388EE1FF57057@usctmx1118.merck.com>

We have successfully used Rserve:  http://stats.math.uni-augsburg.de/Rserve/

Hope this helps,

Matt Wiener 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lu Yuefeng
Sent: Thursday, August 31, 2006 1:24 PM
To: r-help at stat.math.ethz.ch
Subject: [R] java R interface [Broadcast]

Hello all,

    I am wondering if you have experience or information about calling R
routines within JAVA? Is it feasible and how to pass the data/results? TXIA!

Yuefeng

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From valdar at well.ox.ac.uk  Thu Aug 31 20:22:20 2006
From: valdar at well.ox.ac.uk (William Valdar)
Date: Thu, 31 Aug 2006 19:22:20 +0100 (BST)
Subject: [R] get ylim of current plot()
In-Reply-To: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
References: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
Message-ID: <Pine.LNX.4.63.0608311902550.13169@zeon.well.ox.ac.uk>

Dear All,

How can I query the parameters of the current plot if it was produced by 
plot()? For example, if I do not specify ylim explicitly, eg,

  plot(rnorm(100), rnorm(100))

then how do find out what ylim was chosen? Note that I realize this is 
possible with lattice's xyplot():

  (obj <- xyplot(rnorm(100), rnorm(100)))
  print(obj$y.limits)

Thanks in advance,

Will

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 589
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar


From mr.spoon21 at gmail.com  Thu Aug 31 20:23:56 2006
From: mr.spoon21 at gmail.com (Carlo Trimarchi)
Date: Thu, 31 Aug 2006 20:23:56 +0200
Subject: [R]  alpha in Holt-Winters method
Message-ID: <8f67b6f80608311123u18ed37afs1a69da6fd8567c11@mail.gmail.com>

Hi,
I'd like to know if the Holt-Winters function in R can modify the
alpha paremeter in an "intelligent" way. I know it can vary from 0 to
1.

Wich mathematical formula does it use to calculate the correct value of alpha?

Thanks, bye.
Carlo


From ligges at statistik.uni-dortmund.de  Thu Aug 31 20:32:32 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 31 Aug 2006 20:32:32 +0200
Subject: [R] get ylim of current plot()
In-Reply-To: <Pine.LNX.4.63.0608311902550.13169@zeon.well.ox.ac.uk>
References: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
	<Pine.LNX.4.63.0608311902550.13169@zeon.well.ox.ac.uk>
Message-ID: <44F72B40.2050509@statistik.uni-dortmund.de>



William Valdar wrote:
> Dear All,
> 
> How can I query the parameters of the current plot if it was produced by 
> plot()? For example, if I do not specify ylim explicitly, eg,
> 
>   plot(rnorm(100), rnorm(100))
> 
> then how do find out what ylim was chosen? Note that I realize this is 
> possible with lattice's xyplot():
> 
>   (obj <- xyplot(rnorm(100), rnorm(100)))
>   print(obj$y.limits)

par("usr") tells you the size in user coordinates.

Uwe Ligges


> Thanks in advance,
> 
> Will
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> Dr William Valdar               ++44 (0)1865 287 589
> Wellcome Trust Centre           valdar at well.ox.ac.uk
> for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Dimitris.Rizopoulos at med.kuleuven.be  Thu Aug 31 20:43:08 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Thu, 31 Aug 2006 20:43:08 +0200
Subject: [R] get ylim of current plot()
In-Reply-To: <Pine.LNX.4.63.0608311902550.13169@zeon.well.ox.ac.uk>
References: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
	<Pine.LNX.4.63.0608311902550.13169@zeon.well.ox.ac.uk>
Message-ID: <20060831204308.ul1zpipnh5psow0g@webmail3.kuleuven.be>

I don't think you can obtain these values directly since  
plot.default() does not return something.

'ylim' is in fact calculated from xy.coords() as the range of the  
finite 'y' abscissa values; check the code of plot.default() for more  
info.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting William Valdar <valdar at well.ox.ac.uk>:

> Dear All,
>
> How can I query the parameters of the current plot if it was produced by
> plot()? For example, if I do not specify ylim explicitly, eg,
>
>   plot(rnorm(100), rnorm(100))
>
> then how do find out what ylim was chosen? Note that I realize this is
> possible with lattice's xyplot():
>
>   (obj <- xyplot(rnorm(100), rnorm(100)))
>   print(obj$y.limits)
>
> Thanks in advance,
>
> Will
>
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> Dr William Valdar               ++44 (0)1865 287 589
> Wellcome Trust Centre           valdar at well.ox.ac.uk
> for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From rguha at indiana.edu  Thu Aug 31 20:48:43 2006
From: rguha at indiana.edu (Rajarshi Guha)
Date: Thu, 31 Aug 2006 14:48:43 -0400
Subject: [R] java R interface
In-Reply-To: <bc808e8d0608311024s18df6101had2c57de34e51967@mail.gmail.com>
References: <bc808e8d0608311024s18df6101had2c57de34e51967@mail.gmail.com>
Message-ID: <1157050124.7493.32.camel@localhost>

On Thu, 2006-08-31 at 13:24 -0400, Lu Yuefeng wrote:
> Hello all,
> 
>     I am wondering if you have experience or information about calling R
> routines within JAVA? Is it feasible and how to pass the data/results? TXIA!

SJava and JRI both allow this

-------------------------------------------------------------------
Rajarshi Guha <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Say it with flowers - give her a triffid


From markleeds at verizon.net  Thu Aug 31 21:09:20 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Thu, 31 Aug 2006 15:09:20 -0400
Subject: [R] alpha in Holt-Winters method
References: <8f67b6f80608311123u18ed37afs1a69da6fd8567c11@mail.gmail.com>
Message-ID: <000901c6cd30$f74d7250$2e01a8c0@m8d4477f3de884>

you would have to take your series, y_1, ... y_t and run it over various 
values of alpha ( say zero to 1 insteps of .01 ) and see which one gives say 
the least MSE.
but that will be the optimal alpha for the window you are looking at. it 
doesn't mean it will be the optimal apha for the data you deal with
at y_t+1,..... y_t+n.

it's been a while, but , in holt winters, i think there are 3 smoothing 
constants so i would think you would want to optimize over all 3 of them ?

someone else can give the following  in more detail there is probably a way 
to use nlmin or one of those optimization functions to find the optimal 
smoothing constants but i
don't think it's worth the hassle because they are in a small enough range 
that you can do it by brute force as above.


----- Original Message ----- 
From: "Carlo Trimarchi" <mr.spoon21 at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 31, 2006 2:23 PM
Subject: [R] alpha in Holt-Winters method


> Hi,
> I'd like to know if the Holt-Winters function in R can modify the
> alpha paremeter in an "intelligent" way. I know it can vary from 0 to
> 1.
>
> Wich mathematical formula does it use to calculate the correct value of 
> alpha?
>
> Thanks, bye.
> Carlo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Thu Aug 31 21:41:10 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 31 Aug 2006 12:41:10 -0700 (PDT)
Subject: [R] alpha in Holt-Winters method
In-Reply-To: <000901c6cd30$f74d7250$2e01a8c0@m8d4477f3de884>
References: <8f67b6f80608311123u18ed37afs1a69da6fd8567c11@mail.gmail.com>
	<000901c6cd30$f74d7250$2e01a8c0@m8d4477f3de884>
Message-ID: <Pine.LNX.4.64.0608311239010.25565@homer23.u.washington.edu>

On Thu, 31 Aug 2006, MARK LEEDS wrote:

> you would have to take your series, y_1, ... y_t and run it over various
> values of alpha ( say zero to 1 insteps of .01 ) and see which one gives say
> the least MSE.
> but that will be the optimal alpha for the window you are looking at. it
> doesn't mean it will be the optimal apha for the data you deal with
> at y_t+1,..... y_t+n.
>
> it's been a while, but , in holt winters, i think there are 3 smoothing
> constants so i would think you would want to optimize over all 3 of them ?
>
> someone else can give the following  in more detail there is probably a way
> to use nlmin or one of those optimization functions to find the optimal
> smoothing constants but i
> don't think it's worth the hassle because they are in a small enough range
> that you can do it by brute force as above.
>

Or you could read the documentation for HoltWinters

      The function tries to find the optimal values of alpha and/or beta
      and/or gamma by minimizing the squared one-step prediction error
      if they are omitted.

If you want to know how the optimization is done, looking at the code will 
tell you.


 	-thomas


From mkimpel at iupui.edu  Thu Aug 31 21:51:41 2006
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Thu, 31 Aug 2006 15:51:41 -0400
Subject: [R] problem with postscript output of R-devel on Windows
In-Reply-To: <44F713A1.1080407@stats.uwo.ca>
Message-ID: <836F00680EECD340A96AD34ECFF3B53461B1D4@iu-mssg-mbx106.ads.iu.edu>

I apologize for my previous confusing example. Below is some sample
code, taken directly from the "image" help file, that reproduces a
postscript problem. This now happens with both R 2.3.1 and R 2.4

What I get appears to be output of only certain postscript "objects", to
use an Adobe term. When I use the R GUI menu to "save as", jpeg and pdf
files save correctly, but the postscript file does not. I am not getting
any axis labels or topo labels. This is true whether I import the PS
file into either Photoshop or Illustrator.

Thanks, Mark

x <- 10*(1:nrow(volcano))
     y <- 10*(1:ncol(volcano))
     image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
     contour(x, y, volcano, levels = seq(90, 200, by = 5),
             add = TRUE, col = "peru")
     axis(1, at = seq(100, 800, by = 100))
     axis(2, at = seq(100, 600, by = 100))
     box()
     title(main = "Maunga Whau Volcano", font.main = 4)

> sessionInfo()
Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets" 
[7] "base"  

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: Thursday, August 31, 2006 12:52 PM
To: Kimpel, Mark William
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] problem with postscript output of R-devel on Windows

On 8/31/2006 11:27 AM, Kimpel, Mark William wrote:
> I have developed a problem with the postscript output of plot on
Windows. My code still works properly with R 2.3 but, with R 2.4, the
white text on red background does not show up. It does, however, show up
when output is sent to the screen. Below is my code and sessionInfo.
> 
> R version 2.4.0 Under development (unstable) (2006-08-29 r39012) 
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] "splines"   "tools"     "methods"   "stats"     "graphics"
"grDevices" "utils"     "datasets" 
> [9] "base"     
> 
> other attached packages:
>   Rgraphviz geneplotter         XML     GOstats    Category
hgu95av2        KEGG    multtest      xtable 
>    "1.11.9"    "1.11.8"    "0.99-8"     "1.6.0"     "1.4.1"
"1.12.0"     "1.8.1"    "1.11.2"     "1.3-2" 
>        RBGL    annotate          GO       graph       Ruuid
limma  genefilter    survival     rat2302 
>     "1.8.1"    "1.11.5"     "1.6.5"   "1.11.13"    "1.11.2"
"2.7.9"    "1.11.8"      "2.28"    "1.12.0" 
>        affy      affyio     Biobase 
>    "1.11.6"     "1.1.8"   "1.11.29"
> 
> 
> fileName<-paste(experiment, contrast, "FDR", FDR, "Graph", "ps",
sep=".")
>     postscript(file=fileName, paper="special",width=width,
height=height) #set up graphics device
>     plot(result.gN, layout.param, nodeAttrs = nAttrs, edgeAttrs =
eAttrs,
>         main=paste(paste("Experiment:", experiment, ";  Contrast:",
contrast,";  FDR:", FDR, sep=""), paste("Min. connections ==",
min.edges, "Min. citations per connection ==", min.cites, "Additional
search criteria:",
>             termAdditional, sep=" "), sep="    "))


Could you put together a reproducible example to illustrate the problem?

  We don't have all the variables used in that example.  I think you 
should be able to do it with just base packages attached; if not, it's 
likely a problem with one of the contributed packages, rather than with
R.

Duncan Murdoch


From brantc at stanford.edu  Thu Aug 31 22:03:13 2006
From: brantc at stanford.edu (Brant Carlson)
Date: Thu, 31 Aug 2006 13:03:13 -0700
Subject: [R] alpha-channel transparency in filled.contour
Message-ID: <20060831200313.GA22227@bayern.int.stanford-vlf.org>

Greetings

I am currently using R to produce contour plots of a given variable via
"contour", together with a hacked* version of filled.contour to shade
several regions of the plot.  As the regions overlap, I am using
colors with alpha-channel values of ~0.1 together with the pdf device
(version="1.4").

The transparency works well.  However, a white grid is superimposed over
all the drawn regions.  Simplified code to reproduce the grid effect is
below.

An earlier attempt to draw the regions with "rect" had a similar
problem, which was fixed by setting the density parameter to -1 to
disable shading.  The filled.contour function has no such parameter, nor
does the .Internal call to "filledcontour" used to actually draw the contours in
my hacked version.  

Is there an easy way to fix this (like some other way to set the density
parameter), or another way to draw a filled region between two levels in
a contour plot that would allow for alpha-channel transparency?

Much Thanks.

   Brant Carlson

*: the "hacked" version merely deletes the code that changes the margins
and draws the key.

#code to reproduce grid effect:
pdf("deleteme.pdf",version="1.4")
y <- matrix(1:100,nrow=10,ncol=10)
filled.contour(y,col=c(0,rgb(1,0,0,0.1)))
dev.off()


From ccatj at web.de  Thu Aug 31 22:15:04 2006
From: ccatj at web.de (Christian Jones)
Date: Thu, 31 Aug 2006 22:15:04 +0200
Subject: [R] need help with an interaction term
Message-ID: <702716241@web.de>

Hello Chuck,
many thanks for your advice.
I?ll go through the package and see if I?ll manage to visualize the interaction effect this time as I already tried it with a special tool for displaying models with interaction terms in 3D. 
http://www.uni-oldenburg.de/landeco/Download/Software/LR_Mesh/LR_Mesh.htm
but could not yield any plausible results. Hopefully this time...
best regards
Christian

> -----Urspr?ngliche Nachricht-----
> Von: Chuck Cleland <ccleland at optonline.net>
> Gesendet: 31.08.06 14:16:12
> An: Christian Jones <ccatj at web.de>
> CC: r-help at stat.math.ethz.ch
> Betreff: Re: [R] need help with an interaction term


> Christian Jones wrote:
> > Hello!
> > I???m fitting a model with glm(family binomial). The best model counts 9 Variables and includes an interaction term that was generated by the product of to continuous variables (a*b). All variables are correlated under a value of 0.7 (Spearman rank order) While the estimates of both main effects are negativ, the resulting interaction term is positiv. This change of sign makes it difficult to interpret the model and above all, is this perhaps due to a bad variable choice ?
> > Thanks a lot for helping
> 
>   Rather than trying to interpret the model coefficients directly, you
> might visualize the a*b interaction effect.  The effects package by John
> Fox is very useful for this:
> 
> http://cran.r-project.org/doc/packages/effects.pdf
> 
> > Christian
> > 
> > _______________________________________________________________________
> > Viren-Scan f?r Ihren PC! Jetzt f?r jeden. Sofort, online und kostenlos.
> > Gleich testen! http://www.pc-sicherheit.web.de/freescan/?mc=022222
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>


From toby_marks at americancentury.com  Thu Aug 31 22:22:30 2006
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Thu, 31 Aug 2006 15:22:30 -0500
Subject: [R] cumulative growth rates indexed to a common starting point over
 n series of observations
Message-ID: <OF75466F68.0199D2DA-ON862571DB.006CFE77-862571DB.006FEC8F@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/26b07f93/attachment.pl 

From murdoch at stats.uwo.ca  Thu Aug 31 22:26:59 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 31 Aug 2006 16:26:59 -0400
Subject: [R] problem with postscript output of R-devel on Windows
In-Reply-To: <836F00680EECD340A96AD34ECFF3B53461B1D4@iu-mssg-mbx106.ads.iu.edu>
References: <836F00680EECD340A96AD34ECFF3B53461B1D4@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <44F74613.3040200@stats.uwo.ca>

On 8/31/2006 3:51 PM, Kimpel, Mark William wrote:
> I apologize for my previous confusing example. Below is some sample
> code, taken directly from the "image" help file, that reproduces a
> postscript problem. This now happens with both R 2.3.1 and R 2.4
> 
> What I get appears to be output of only certain postscript "objects", to
> use an Adobe term. When I use the R GUI menu to "save as", jpeg and pdf
> files save correctly, but the postscript file does not. I am not getting
> any axis labels or topo labels. This is true whether I import the PS
> file into either Photoshop or Illustrator.

I don't see a problem using GSView.  Maybe this is an Adobe bug?

Duncan

> 
> Thanks, Mark
> 
> x <- 10*(1:nrow(volcano))
>      y <- 10*(1:ncol(volcano))
>      image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
>      contour(x, y, volcano, levels = seq(90, 200, by = 5),
>              add = TRUE, col = "peru")
>      axis(1, at = seq(100, 800, by = 100))
>      axis(2, at = seq(100, 600, by = 100))
>      box()
>      title(main = "Maunga Whau Volcano", font.main = 4)
> 
>> sessionInfo()
> Version 2.3.1 (2006-06-01) 
> i386-pc-mingw32 
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets" 
> [7] "base"  
> 
> Mark W. Kimpel MD 
> 
>  
> 
> (317) 490-5129 Work, & Mobile
> 
>  
> 
> (317) 663-0513 Home (no voice mail please)
> 
> 1-(317)-536-2730 FAX
> 
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: Thursday, August 31, 2006 12:52 PM
> To: Kimpel, Mark William
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] problem with postscript output of R-devel on Windows
> 
> On 8/31/2006 11:27 AM, Kimpel, Mark William wrote:
>> I have developed a problem with the postscript output of plot on
> Windows. My code still works properly with R 2.3 but, with R 2.4, the
> white text on red background does not show up. It does, however, show up
> when output is sent to the screen. Below is my code and sessionInfo.
>> 
>> R version 2.4.0 Under development (unstable) (2006-08-29 r39012) 
>> i386-pc-mingw32 
>> 
>> locale:
>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>> 
>> attached base packages:
>> [1] "splines"   "tools"     "methods"   "stats"     "graphics"
> "grDevices" "utils"     "datasets" 
>> [9] "base"     
>> 
>> other attached packages:
>>   Rgraphviz geneplotter         XML     GOstats    Category
> hgu95av2        KEGG    multtest      xtable 
>>    "1.11.9"    "1.11.8"    "0.99-8"     "1.6.0"     "1.4.1"
> "1.12.0"     "1.8.1"    "1.11.2"     "1.3-2" 
>>        RBGL    annotate          GO       graph       Ruuid
> limma  genefilter    survival     rat2302 
>>     "1.8.1"    "1.11.5"     "1.6.5"   "1.11.13"    "1.11.2"
> "2.7.9"    "1.11.8"      "2.28"    "1.12.0" 
>>        affy      affyio     Biobase 
>>    "1.11.6"     "1.1.8"   "1.11.29"
>> 
>> 
>> fileName<-paste(experiment, contrast, "FDR", FDR, "Graph", "ps",
> sep=".")
>>     postscript(file=fileName, paper="special",width=width,
> height=height) #set up graphics device
>>     plot(result.gN, layout.param, nodeAttrs = nAttrs, edgeAttrs =
> eAttrs,
>>         main=paste(paste("Experiment:", experiment, ";  Contrast:",
> contrast,";  FDR:", FDR, sep=""), paste("Min. connections ==",
> min.edges, "Min. citations per connection ==", min.cites, "Additional
> search criteria:",
>>             termAdditional, sep=" "), sep="    "))
> 
> 
> Could you put together a reproducible example to illustrate the problem?
> 
>   We don't have all the variables used in that example.  I think you 
> should be able to do it with just base packages attached; if not, it's 
> likely a problem with one of the contributed packages, rather than with
> R.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgreen at dyson.brisnet.org.au  Thu Aug 31 23:05:40 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Fri, 01 Sep 2006 07:05:40 +1000
Subject: [R] grep question
In-Reply-To: <mailman.15.1156845603.16465.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060901070041.00c5e630@pop3.brisnet.org.au>


I am hoping for some advice as to how to modify the following syntax, so 
that instead of saving all records which refer to Farrah, I select all 
instances that do not include Farrah, or the word Coolum.


test <- read.csv("c:\\newdat.csv", as.is=TRUE, header=T)
sure <- test[grep('Farrah', paste(test$V3.HD, test$V3.LP, test$V3.TD)),]
write.csv(sure,"c:/farrah4.csv")


Any assistance is appreciated,

regards

Bob  Green


From lizzylaws at yahoo.com  Thu Aug 31 23:24:06 2006
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Thu, 31 Aug 2006 14:24:06 -0700 (PDT)
Subject: [R] differnce between lme and proc mixed
Message-ID: <20060831212406.99584.qmail@web32110.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/24fad351/attachment.pl 

From markleeds at verizon.net  Thu Aug 31 23:27:59 2006
From: markleeds at verizon.net (MARK LEEDS)
Date: Thu, 31 Aug 2006 17:27:59 -0400
Subject: [R] cumulative growth rates indexed to a common starting point
 over n series of observations
References: <OF75466F68.0199D2DA-ON862571DB.006CFE77-862571DB.006FEC8F@americancentury.com>
Message-ID: <001001c6cd44$559b9e50$2e01a8c0@m8d4477f3de884>

i think you can add 1 to whatever series you are considering and then use 
cumprod() but check that that works.
also do ?cumprod


----- Original Message ----- 
From: <toby_marks at americancentury.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 31, 2006 4:22 PM
Subject: [R] cumulative growth rates indexed to a common starting point over 
n series of observations


> What is the R way of computing cumulative growth rates given a series of
> discrete values indexed .
>
> For instance, given a matrix of 20 observations for each of 5 series (zz),
> what is the most straight forward technique in R for computing cumulative
> growth (zzcum) ?
> It seems for the solution I'm after might be imbedding the following cum
> growth rate calc as a function into a function call to apply, mapply, ...?
>
>
>
> zz = rnorm(100)
> dim(zz) = c(20,5)
> zz
> zzcum=matrix(nrow=20,ncol=5)
> zzcum
> zzcum[1,]=100*(1+zz[1,]/100)
> zzcum
> for(i in 2:20){zzcum[i,] = zzcum[i-1,]*(1+zz[i,]/100)}
> zzcum
>
>
>
>
>
> ------------------------------------------------------------
> CONFIDENTIALITY NOTICE: This electronic mail transmission (i...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Aug 31 23:59:12 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 31 Aug 2006 17:59:12 -0400
Subject: [R] grep question
In-Reply-To: <5.1.0.14.0.20060901070041.00c5e630@pop3.brisnet.org.au>
References: <mailman.15.1156845603.16465.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060901070041.00c5e630@pop3.brisnet.org.au>
Message-ID: <644e1f320608311459l47f5e276gd013f1bc3e3a038a@mail.gmail.com>

This finds the matching indices of Farrah and Common and then create a
set that does not include them:

> x <- c('Farrah', 'more', 'Common', 'last')
> got.F <- grep('Farrah',x)
> got.C <- grep('Common', x)
> not.ForC <- setdiff(seq(along=x), c(got.F, got.C))
> x[not.ForC]
[1] "more" "last"
>


On 8/31/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
>
> I am hoping for some advice as to how to modify the following syntax, so
> that instead of saving all records which refer to Farrah, I select all
> instances that do not include Farrah, or the word Coolum.
>
>
> test <- read.csv("c:\\newdat.csv", as.is=TRUE, header=T)
> sure <- test[grep('Farrah', paste(test$V3.HD, test$V3.LP, test$V3.TD)),]
> write.csv(sure,"c:/farrah4.csv")
>
>
> Any assistance is appreciated,
>
> regards
>
> Bob  Green
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ajayshah at mayin.org  Thu Aug 31 20:43:31 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 1 Sep 2006 00:13:31 +0530
Subject: [R] Pretty-printing multiple regression models
Message-ID: <20060831184331.GA24719@lubyanka.local>

A few days ago, I had asked this question. Consider this situation:
> x1 <- runif(100); x2 <- runif(100); y <- 2 + 3*x1 - 4*x2 + rnorm(100)
> m1 <- summary(lm(y ~ x1))
> m2 <- summary(lm(y ~ x2))
> m3 <- summary(lm(y ~ x1 + x2))

You have estimated 3 different "competing" models, and suppose you
want to present the set of models in one table. xtable(m1) is cool,
but that would give us 3 different tables.

What I want is this table:

-----------------------------------------------------------
                    M1             M2              M3
-----------------------------------------------------------
Intercept         0.0816         3.6292         2.2272
                 (0.5533)       (0.2316)***    (0.2385)***

x1                2.8151                        2.7606
                 (0.5533)***                   (0.3193)***

x2                              -4.2899        -4.2580
                                (0.401)***     (0.3031)***

$\sigma_e$        1.538          1.175          0.8873
$R^2$             0.2089         0.5385         0.7393
-----------------------------------------------------------

I was given feedback from this mailing list that this is a specialised
display and requires custom code. So I wrote some code. I will be very
happy if you could look at this code, and give me ideas on how to do
it better, and how to generalise it. I am most unhappy with the fact
that right now, I'm tied to the fact that summary.lm() gives you
something which has a $coefficients. Ideally this kind of display
should be general for all kinds of models.

My code produces two results:

    > numbers
                           M1      M2       M3
    Intercept          0.0691  3.1110   1.7831
    t                  0.2471 12.1860   6.8888
    x1                 2.6595      NA   2.7772
    t                  5.3837      NA   8.0206
    x2                     NA -3.2788  -3.3683
    t                      NA -7.6922 -10.1331
    Residual std. dev. 1.3567  1.2195   0.9505
    $R^2$              0.2283  0.3765   0.6251
    Adjusted $R^2$     0.2204  0.3701   0.6174

and:

    > specialised.latex.generation(numbers)
    \hline
     &  M1 &  M2 &  M3\\
    \hline
    Intercept &  0.0691 &  3.111 &  1.7831\\
     &  (0.2471) &  (12.186)$^\mbox{***} &  (6.8888)$^\mbox{***}\\[1mm]
    x1 &  2.6595 &  &  2.7772\\
     &  (5.3837)$^\mbox{***} &  &  (8.0206)$^\mbox{***}\\[1mm]
    x2 &  &  -3.2788 &  -3.3683\\
     &  &  (-7.6922)$^\mbox{***} &  (-10.1331)$^\mbox{***}\\[1mm]
    Residual std. dev. &  1.3567 &  1.2195 &  0.9505\\
    $R^2$ &  0.2283 &  0.3765 &  0.6251\\
    Adjusted $R^2$ &  0.2204 &  0.3701 &  0.6174\\
    \hline

mmp <- function(regressors, bottom.matter, models.names, allmodels) {
  numbers <- matrix(NA, nrow=(2*length(regressors))+length(bottom.matter),
                    ncol=length(models.names))
  colnames(numbers) <- models.names
  rownames(numbers) <- rep("t", nrow(numbers))

  baserow <- 1
  for (i in 1:length(regressors)) {
    if (regressors[i] == "Intercept") {
      regex <- "^\\(Intercept\\)$"
    } else {
      regex <- paste("^", regressors[i], "$", sep="")
    }
    rownames(numbers)[baserow] <- regressors[i]
    for (j in 1:length(allmodels)) {
      m <- allmodels[[j]]
      if (any(locations <- grep(regex, rownames(m$coefficients)))) {
        if (length(locations) > 1) {
          cat("Regex ", regex, " has multiple matches at model ", j, "\n")
          return(NULL)
        }
        numbers[baserow,j] <- as.numeric(sprintf("%.4f",
                                                 m$coefficients[locations,1]))
        numbers[baserow+1,j] <- as.numeric(sprintf("%.4f",
                                                   m$coefficients[locations,3]))
      }
    }
    baserow <- baserow + 2
  }

                                        # Now process the bottom.matter
  for (i in 1:length(bottom.matter)) {
    if (bottom.matter[i] == "sigma") {
      for (j in 1:length(allmodels)) {
        m <- allmodels[[j]]
        numbers[baserow,j] <- as.numeric(sprintf("%.4f", m$sigma))
      }
      rownames(numbers)[baserow] <- "Residual std. dev."
      baserow <- baserow + 1
    }
    
    if (bottom.matter[i] == "r.squared") {
      for (j in 1:length(allmodels)) {
        m <- allmodels[[j]]
        numbers[baserow,j] <- as.numeric(sprintf("%.4f", m$r.squared))
      }
      rownames(numbers)[baserow] <- "$R^2$"
      baserow <- baserow + 1
    }

    if (bottom.matter[i] == "adj.r.squared") {
      for (j in 1:length(allmodels)) {
        m <- allmodels[[j]]
        numbers[baserow,j] <- as.numeric(sprintf("%.4f", m$adj.r.squared))
      }
      rownames(numbers)[baserow] <- "Adjusted $R^2$"
      baserow <- baserow + 1
    }
  }
  numbers
}

# Given a 't' statistic, give me a string with
# '*' or '**' or '***' based on the 90%, 95% and 99% cutoffs on N(0,1)
stars <- function(t) {
  t <- abs(t)
  n <- -1 + as.numeric(cut(t,breaks=c(-0.01,-qnorm(c(0.05, 0.025, 0.005)),Inf)))
  if (n == 0) {
    return("")
  } else {
    return(paste("$^\\mbox{",
                 paste(rep("*", n), sep="", collapse=""),
                 "}", sep=""))
  }
}

specialised.latex.generation <- function(numbers) {
  cat("\\hline\n")
  for (j in 1:ncol(numbers)) {
    cat(" & ", colnames(numbers)[j])
  }
  cat("\\\\\n\\hline\n")
  for (i in 1:nrow(numbers)) {
    if (rownames(numbers)[i] == "t") {
      for (j in 1:ncol(numbers)) {
        if (is.na(numbers[i,j])) {
          cat(" & ")
        } else {
          cat(" & ", sprintf("(%s)%s", numbers[i,j], stars(numbers[i,j])))
        }
      }
      cat("\\\\[1mm]\n")
    } else {
      cat(rownames(numbers)[i])
      for (j in 1:ncol(numbers)) {
        if (is.na(numbers[i,j])) {
          cat(" & ")
        } else {
          cat(" & ", numbers[i, j])
        }
      }
      cat("\\\\\n")
    }
  }
  cat("\\hline\n")
}

x1 <- runif(100); x2 <- runif(100); y <- 2 + 3*x1 - 4*x2 + rnorm(100)
m1 <- summary(lm(y ~ x1))
m2 <- summary(lm(y ~ x2))
m3 <- summary(lm(y ~ x1 + x2))
numbers <- mmp(regressors=c("Intercept", "x1", "x2"),
               bottom.matter=c("sigma", "r.squared", "adj.r.squared"),
               models.names=c("M1", "M2", "M3"),
               allmodels=list(m1, m2, m3))
numbers
specialised.latex.generation(numbers)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From ajayshah at mayin.org  Thu Aug 31 20:49:54 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 1 Sep 2006 00:19:54 +0530
Subject: [R] Ooops, small mistake fixed (pretty printing multiple models)
Message-ID: <20060831184954.GA24788@lubyanka.local>

The R code I just mailed out had a small error in it. This one
works. Now what one needs is a way to get decimal alignment in LaTeX
tabular objects.

x1 <- runif(100); x2 <- runif(100); y <- 2 + 3*x1 - 4*x2 + rnorm(100)
m1 <- summary(lm(y ~ x1))
m2 <- summary(lm(y ~ x2))
m3 <- summary(lm(y ~ x1 + x2))

# What I want is this table:
# 
# -----------------------------------------------------------
#                     M1             M2              M3
# -----------------------------------------------------------
# Intercept         0.0816         3.6292         2.2272
#                  (0.5533)       (0.2316)***    (0.2385)***
# 
# x1                2.8151                        2.7606
#                  (0.5533)***                   (0.3193)***
# 
# x2                              -4.2899        -4.2580
#                                 (0.401)***     (0.3031)***
# 
# $\sigma_e$        1.538          1.175          0.8873
# $R^2$             0.2089         0.5385         0.7393
# -----------------------------------------------------------

mmp <- function(regressors, bottom.matter, models.names, allmodels) {
  numbers <- matrix(NA, nrow=(2*length(regressors))+length(bottom.matter),
                    ncol=length(models.names))
  colnames(numbers) <- models.names
  rownames(numbers) <- rep("t", nrow(numbers))

  baserow <- 1
  for (i in 1:length(regressors)) {
    if (regressors[i] == "Intercept") {
      regex <- "^\\(Intercept\\)$"
    } else {
      regex <- paste("^", regressors[i], "$", sep="")
    }
    rownames(numbers)[baserow] <- regressors[i]
    for (j in 1:length(allmodels)) {
      m <- allmodels[[j]]
      if (any(locations <- grep(regex, rownames(m$coefficients)))) {
        if (length(locations) > 1) {
          cat("Regex ", regex, " has multiple matches at model ", j, "\n")
          return(NULL)
        }
        numbers[baserow,j] <- as.numeric(sprintf("%.4f",
                                                 m$coefficients[locations,1]))
        numbers[baserow+1,j] <- as.numeric(sprintf("%.4f",
                                                   m$coefficients[locations,3]))
      }
    }
    baserow <- baserow + 2
  }

                                        # Now process the bottom.matter
  for (i in 1:length(bottom.matter)) {
    if (bottom.matter[i] == "sigma") {
      for (j in 1:length(allmodels)) {
        m <- allmodels[[j]]
        numbers[baserow,j] <- as.numeric(sprintf("%.4f", m$sigma))
      }
      rownames(numbers)[baserow] <- "Residual std. dev."
      baserow <- baserow + 1
    }
    
    if (bottom.matter[i] == "r.squared") {
      for (j in 1:length(allmodels)) {
        m <- allmodels[[j]]
        numbers[baserow,j] <- as.numeric(sprintf("%.4f", m$r.squared))
      }
      rownames(numbers)[baserow] <- "$R^2$"
      baserow <- baserow + 1
    }

    if (bottom.matter[i] == "adj.r.squared") {
      for (j in 1:length(allmodels)) {
        m <- allmodels[[j]]
        numbers[baserow,j] <- as.numeric(sprintf("%.4f", m$adj.r.squared))
      }
      rownames(numbers)[baserow] <- "Adjusted $R^2$"
      baserow <- baserow + 1
    }
  }
  numbers
}

# Given a 't' statistic, give me a string with
# '*' or '**' or '***' based on the 90%, 95% and 99% cutoffs on N(0,1)
stars <- function(t) {
  t <- abs(t)
  n <- -1 + as.numeric(cut(t,breaks=c(-0.01,-qnorm(c(0.05, 0.025, 0.005)),Inf)))
  if (n == 0) {
    return("")
  } else {
    return(paste("$^\\textrm{",
                 paste(rep("*", n), sep="", collapse=""),
                 "}$", sep=""))
  }
}

specialised.latex.generation <- function(numbers) {
  cat("\\hline\n")
  for (j in 1:ncol(numbers)) {
    cat(" & ", colnames(numbers)[j])
  }
  cat("\\\\\n\\hline\n")
  for (i in 1:nrow(numbers)) {
    if (rownames(numbers)[i] == "t") {
      for (j in 1:ncol(numbers)) {
        if (is.na(numbers[i,j])) {
          cat(" & ")
        } else {
          cat(" & ", sprintf("(%s)%s", numbers[i,j], stars(numbers[i,j])))
        }
      }
      cat("\\\\[1mm]\n")
    } else {
      cat(rownames(numbers)[i])
      for (j in 1:ncol(numbers)) {
        if (is.na(numbers[i,j])) {
          cat(" & ")
        } else {
          cat(" & ", numbers[i, j])
        }
      }
      cat("\\\\\n")
    }
  }
  cat("\\hline")
}

numbers <- mmp(regressors=c("Intercept", "x1", "x2"),
               bottom.matter=c("sigma", "r.squared", "adj.r.squared"),
               models.names=c("M1", "M2", "M3"),
               allmodels=list(m1, m2, m3))
numbers
specialised.latex.generation(numbers)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From AnupTyagi at yahoo.com  Thu Aug 31 07:12:04 2006
From: AnupTyagi at yahoo.com (Anupam Tyagi)
Date: Thu, 31 Aug 2006 10:42:04 +0530
Subject: [R] working with summarized data
In-Reply-To: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
References: <F417F47A-2030-4135-BDB9-25913E7747AB@gmail.com>
Message-ID: <ed5r33$pem$2@sea.gmane.org>

One solution is to simulate the population by repeating each row
"weight" number of times. This is inefficient. It may create a very
large dataset for a large sample survey. But some of graphs and other
things may turn out to your liking, depending upon how the functions are
written.

Anupam.

Rick Bischoff wrote the following on 8/30/2006 7:57 PM:
> The data sets I am working with all have a weight variable--e.g.,  
> each row doesn't mean 1 observation.
> 
> With that in mind, nearly all of the graphs and summary statistics  
> are incorrect for my data, because they don't take into account the  
> weight.
> 
> ****
> For example "median" is incorrect, as the quantiles aren't calculated  
> with weights:
> 
> sum( weights[X < median(X)] ) / sum(weights)
> 
> This should be 0.5... of course it's not.
> ****
> 
> Unfortunately, it seems that most(all?) of R's graphics and summary  
> statistic functions don't take a weight or frequency argument.    
> (Fortunately the models do...)
> 
> Am I completely missing how to do this?  One way would be to  
> replicate each row proportional to the weight (e.g. if the weight was  
> 4, we would 3 additional copies) but this will get prohibitive pretty  
> quickly as the dataset grows.
> 
> 
> Thanks in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mcorrales at cpdata.cl  Thu Aug 31 18:04:16 2006
From: mcorrales at cpdata.cl (Marcela Corrales)
Date: Thu, 31 Aug 2006 12:04:16 -0400
Subject: [R] Consulta sobre bases en R
Message-ID: <0J4V003IODYJGP00@frontend1.telmexchile.cl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/f7264bf9/attachment.ksh 

From flyhyena at yahoo.com.cn  Thu Aug 31 16:41:27 2006
From: flyhyena at yahoo.com.cn (z s)
Date: Thu, 31 Aug 2006 22:41:27 +0800 (CST)
Subject: [R] newbie question about index
In-Reply-To: <mailman.13.1156932004.4788.r-help@stat.math.ethz.ch>
Message-ID: <20060831144127.10844.qmail@web15610.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060831/fc21028b/attachment.pl 

