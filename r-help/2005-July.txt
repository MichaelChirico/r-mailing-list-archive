From gunter.berton at gene.com  Fri Jul  1 00:09:29 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 30 Jun 2005 15:09:29 -0700
Subject: [R] ts.plot data labels
Message-ID: <200506302209.j5UM9TFZ023712@compton.gene.com>

 
 "xy.labels" is not a valid parameter for ts.plot;it is for plot.ts (the
plot method for ts objects). Please re-read the Help files and note the
difference between them.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Beth Wilmot
> Sent: Thursday, June 30, 2005 2:25 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] ts.plot data labels
> 
> Dear R users,
> I am trying to put labels on the data points on a ts.plot.
>  
> I have tried:
> ts.plot(df.ts, gpars=list(xy.labels=colnames(df.ts), 
> xlab="group", ylab="level"))
> which plots fine but gives no labels
>  
> xy.labels=TRUE 
> also plots without labels
>  
> I also tried using text() but got an error 
>  
> Error in xy.coords(x, y, recycle = TRUE) : 
>         Argument "x" is missing, with no default
> 
> 
> Thanks,
> Beth
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Jul  1 00:18:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 30 Jun 2005 18:18:15 -0400
Subject: [R] ts.plot data labels
In-Reply-To: <s2c400ae.071@ohsu.edu>
References: <s2c400ae.071@ohsu.edu>
Message-ID: <971536df05063015185b6b23e3@mail.gmail.com>

On 6/30/05, Beth Wilmot <wilmotb at ohsu.edu> wrote:
> Dear R users,
> I am trying to put labels on the data points on a ts.plot.
> 
> I have tried:
> ts.plot(df.ts, gpars=list(xy.labels=colnames(df.ts), xlab="group", ylab="level"))
> which plots fine but gives no labels
> 
> xy.labels=TRUE
> also plots without labels
> 
> I also tried using text() but got an error
> 
> Error in xy.coords(x, y, recycle = TRUE) :
>        Argument "x" is missing, with no default


Try using text.  See ?ts.plot and ?text for more info:

ts.plot(ts(1:26), type = "p", gpars = list(pch = 20))  # points
text(1:26, 1:26, letters, offset = 0.2, cex = .5, pos = 1) # letters
below points



From reid_huntsinger at merck.com  Fri Jul  1 00:24:18 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 30 Jun 2005 18:24:18 -0400
Subject: [R] R CMD INSTALL typo?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A94E9@uswpmx00.merck.com>

I recently installed R-2.1.1 and R CMD INSTALL gives me the message

mkdir: cannot create directory `': No such file or directory

before doing whatever it is supposed to do. It looks to me like the script
template INSTALL.in has a typo, in line 97: (excerpt follows starting with
line 93)

startdir=`${GETWD}`
: ${TMPDIR=/tmp}
{ tmpdir=`(mktemp -d -q "${TMPDIR}/R.INSTALL.XXXXXX") 2>/dev/null` \
    && test -n "${tmpdir}" && test -d "${tmpdir}" ; } ||
  { test -n "${RANDOM}" && tmp=${TMPDIR}/R.INSTALL$$-${RANDOM} \
      && (${MKDIR_P} "${tmpdir}") ; } ||
  { tmpdir=${TMPDIR}/R.INSTALL.$$ && (${MKDIR_P} "${tmpdir}") ; } ||
  (error "cannot create temporary directory" && exit 1)
tmpdir=`cd "${tmpdir}" && ${GETWD}`

where the "tmp=" midway through should I think be "tmpdir=". Making that
change solved the problem. I didn't see the problem mentioned anywhere so
thought I should point it out and make sure I'm not breaking something.

Reid Huntsinger



From gerifalte28 at hotmail.com  Fri Jul  1 00:25:53 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 30 Jun 2005 22:25:53 +0000
Subject: [R] How to rotate the axisnames in a BARPLOT
In-Reply-To: <12806.1120165515@www8.gmx.net>
Message-ID: <BAY103-F396EE34E37828F08B15090A6E30@phx.gbl>

Try the following.  I made this based on the eaxmple from FAQ 7.27

tab=table(rpois(100, 2))  #Creates table to make a barplot
par(mar = c(6, 4, 4, 2) + 0.1)#Prepares margin to fit x axis labels
pts=barplot(tab, xaxt = "n", xlab = "", col="yellow2")#Barplot of tab 
without x axis or label.
#Also stores the middle points of the bars

axis(side=1,at=pts, labels=F, tick=T)#Creates x axis with tickmarks exactly 
at the middle of the bars
nam=paste("Text",names(tab))#Names of each category. To be used on labels

text(pts, par("usr")[3] - 1.5, srt = 45, adj = 1, labels = nam, xpd = TRUE) 
#Adds the tickmark labels
# adj = 1 will place the text at the end ot the tick marks
# xpd = TRUE will "clip" text outside the plot region

mtext(1, text = "My Axis Label", line = 4) #Adds x axis label at line 2

Second hit of  RSiteSearch("logarithmic axis barplot") takes you to barplot2 
{gplots}


Cheers

Francisco

>From: hulubu at gmx.de
>To: r-help at stat.math.ethz.ch
>Subject: [R] How to rotate the axisnames in a BARPLOT
>Date: Thu, 30 Jun 2005 23:05:15 +0200 (MEST)
>
>Hi all,
>
>- how can I do a barplot with rotated axis labels? I've seen the example 
>for
>just a plot in the FAQ, but I'll missing the coordinates to plot my text at
>the right position beneath the bars.
>Is there any (easy?) solution?
>
>- how can I set the y-axis in a barplot to logarithmic scale?
>
>Many thanks in advance!
>
>Best Regards
>  Tom
>
>--
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From sghosh at lexgen.com  Fri Jul  1 00:55:32 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 30 Jun 2005 17:55:32 -0500
Subject: [R] FW: plot legend outside the grid
Message-ID: <2B47B68F97330841AC8C670749084A7D06C50B@wdexchmb01.lexicon.lexgen.com>



-----Original Message-----
From: Ghosh, Sandeep 
Sent: Thursday, June 30, 2005 5:43 PM
To: 'Berton Gunter'
Subject: plot legend outside the grid 


Thanks for the pointers... I managed to get everything to look and feel the way I want except for the legend to plot outside the grid... Thanks for the note on the par, but I'm not able to it to plot outside the plot grid..

dataFrame <- as.data.frame(t(structure(c(
64,'wt',
62,'wt',
66,'wt',
65,'wt',
60,'wt',
61,'wt',
65,'wt',
66,'wt',
65,'wt',
63,'wt',
67,'wt',
65,'wt',
62,'wt',
65,'wt',
68,'wt',
65,'wt',
63,'wt',
65,'wt',
62,'wt',
65,'wt',
66,'wt',
62,'wt',
65,'wt',
65,'wt',
66,'wt',
65,'wt',
62,'wt',
65,'wt',
66,'wt',
65,'wt',
61,'wt',
65,'wt',
66,'wt',
65,'wt',
62,'wt',
63,'het',
67,'het',
60,'het',
67,'het',
66,'het',
62,'het',
65,'het',
62,'het',
61,'het',
62,'het',
66,'het',
60,'het',
65,'het',
65,'het',
61,'het',
64,'het',
68,'het',
64,'het',
63,'het',
62,'het',
64,'het',
62,'het',
64,'het',
65,'het',
60,'het',
65,'het',
70,'het',
63,'het',
67,'het',
66,'het',
65,'hom',
62,'hom',
68,'hom',
67,'hom',
67,'hom',
63,'hom',
67,'hom',
66,'hom',
63,'hom',
72,'hom',
62,'hom',
61,'hom',
66,'hom',
64,'hom',
60,'hom',
61,'hom',
66,'hom',
66,'hom',
66,'hom',
62,'hom',
70,'hom',
65,'hom',
64,'hom',
63,'hom',
65,'hom',
69,'hom',
61,'hom',
66,'hom',
65,'hom',
61,'hom',
63,'hom',
64,'hom',
67,'hom'), .Dim=c(2,98))));

colnames(dataFrame) <- c('marbles_buried', 'genotype');

png('mb.png', width=400, height=400, pointsize=8);

dataFrame[c("marbles_buried")] <- lapply(dataFrame[c("marbles_buried")], function(x) as.numeric(levels(x)[x]));

par(xpd=FALSE)

with (dataFrame, stripchart(marbles_buried ~ genotype, method="jitter", vertical=TRUE,  col = c('blue', 'red', 'green'), xlab='Genotype', ylab = "Marbles Buried", main='MBA WTs Vs HOMs', pch=c(1,4,2), jitter=1/3.5, cex=1))

meds <- as.vector(with(dataFrame, by(marbles_buried, genotype, mean)))
segments((1:3)-0.25, meds, (1:3)+0.25, meds, col = c('blue', 'red', 'green'));

dataWt <- subset(dataFrame, genotype=='wt', select=c(marbles_buried,genotype));
dataHet <- subset(dataFrame, genotype=='het', select=c(marbles_buried,genotype));
dataHom <- subset(dataFrame, genotype=='hom', select=c(marbles_buried,genotype));

wtCount <- length(dataWt$marbles_buried);
hetCount <- length(dataHet$marbles_buried);
homCount <- length(dataHom$marbles_buried);
wtLegend <- paste ("wt, (n=", wtCount, ")");
hetLegend <- paste ("het, (n=", hetCount, ")");
homLegend <- paste ("hom, (n=", homCount, ")");

par(xpd=TRUE)
legend(1, max(as.vector(dataFrame$marbles_buried)), c(wtLegend, hetLegend, homLegend), col=c('blue', 'red', 'green'), pch=c(1,4,2));

-Thanks
Sandeep.

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com]
Sent: Thursday, June 30, 2005 2:55 PM
To: Ghosh, Sandeep
Subject: RE: [R] Help with stripplot


Of course!

stripchart() is a base graphics function and tehrefore has available to it
the base graphics functionality, like (the base graphics function, **not**
the lattice argument) legend(). See ?legend in the graphics package. Note
the use of locator() for positioning the legend.

Note: By default the legend will be clipped to the plot region. If you wish
to have a legend outside the plot region set the xpd parameter of par to
TRUE or NA prior to plotting.

-- Bert 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ghosh, Sandeep
> Sent: Thursday, June 30, 2005 12:22 PM
> To: Deepayan Sarkar; r-help at stat.math.ethz.ch
> Subject: Re: [R] Help with stripplot
> 
> Another question, in stripchart is there a way to draw a 
> legends. I need legends that gives the mice count for each 
> genotype wt/het/hom, something like the xyplot plot support 
> for key/auto.key.
> 
> -Sandeep



From ggrothendieck at gmail.com  Fri Jul  1 01:38:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 30 Jun 2005 19:38:38 -0400
Subject: [R] request/suggestion: modified names
In-Reply-To: <156CDC8CCFD1894295D2907F16337A48B2AF06@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A48B2AF06@bru-s-006.europe.shell.com>
Message-ID: <971536df0506301638a51cc2c@mail.gmail.com>

On 6/30/05, Ritter, Christian C GSMCIL-GSTMS/2
<christian.ritter at shell.com> wrote:
> For some time now I use a modified version of names (extract direction) of the following type:
> Names<-
> function (x,filter="^")
> {
>        grep(filter,names(x),value=TRUE)
> }
> 
> Request:
> Has anyone already written a version which goes the other way (that is, which allows assignment of the type Names(x,filter)<-...). Naive versions are obvious, but I think something would have to be done to check validity. So if anyone has already done it for me, I would be very happy if you could share it. Such a function would be quite useful when working with large data frames (as I typically do) and need to edit names in a convenient way.
> 
> Suggestion:
> Wouldn't the filter argument make sense in general, that is in "names" and not only in a custom version "Names"?
> 

This is not a direct answer to your question, as stated, but
if what is desired is really a method to do ad hoc editing 
(as opposed to renaming columns on a programmatic basis) then 
one can use 'fix':

	irish <- head(iris) # test data
	fix(irish)

At this point a spreadsheet pops up and one can edit the 
header directly in a GUI.  (I am on Windows XP -- not sure if this
works on other OSes).  Exit the spreadsheet and its done.

This can alternately be done in JGR, a front end to R.  Press ctrl-B to 
access the object browser.  Pick out your data frame and a spreadsheet 
will popup.  Right click the column name and choose Rename.



From Nongluck.Klibbua at newcastle.edu.au  Fri Jul  1 02:17:26 2005
From: Nongluck.Klibbua at newcastle.edu.au (Nongluck Klibbua)
Date: Fri, 01 Jul 2005 10:17:26 +1000
Subject: [R] how to code garch-t(1,1),egarch(1,1) and gjr(1,1)
Message-ID: <s2c51843.067@MC-GWDOM2.newcastle.edu.au>

hi,
I try to code garch-t(1,1),egach(1,1) and gjr(1,1) to estimate my data.
How I can code these model with my data (e.g. garch code is
y<-garch(x,order=c(1,1))
best regards,
luck



From pberming at research.ryerson.ca  Fri Jul  1 03:29:57 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Thu, 30 Jun 2005 21:29:57 -0400
Subject: [R] compiling under windows
Message-ID: <EC3EEED49432A54990181E8E8B4707275494AA@mail2.arts.ryerson.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/3b48828d/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jul  1 08:52:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Jul 2005 07:52:40 +0100 (BST)
Subject: [R] compiling under windows
In-Reply-To: <EC3EEED49432A54990181E8E8B4707275494AA@mail2.arts.ryerson.ca>
References: <EC3EEED49432A54990181E8E8B4707275494AA@mail2.arts.ryerson.ca>
Message-ID: <Pine.LNX.4.61.0507010728430.1419@gannet.stats>

On Thu, 30 Jun 2005, Philip Bermingham wrote:

> What is the best way to set up a project in visual studio, work on R and
> re compile?  Is it better to use a different compiler or programming
> environment?  I specifically want to work on C and Fortran extensions.

See the `R Installation and Administration' manual.

It is possible to use Visual C++ (there is no Fortran in Visual Studio, 
although a third-party* extension has been available), but it is easier 
and more reliable to use the compilers used to build R itself. Information 
on using VC++ is in the file README.packages, in the top-level directory 
of a binary installatiion and in R_HOME/src/gnuwin32 in the sources.


* Originally from DEC then Compaq and now apparently from Intel.



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From steve.roberts at manchester.ac.uk  Fri Jul  1 10:44:49 2005
From: steve.roberts at manchester.ac.uk (Steve Roberts)
Date: Fri, 01 Jul 2005 09:44:49 +0100
Subject: [R] LME correlation structures: user defined
In-Reply-To: <a06110401be665f5792dd@[150.203.51.113]>
References: <s23fe350.047@ohsu.edu>
Message-ID: <42C5108C.20079.2E6A6261@fs1.ser.man.ac.uk>

Hi,

I noticed your old posts whilst tidying up my mailbox. I am struggling 
trying to understand the code to define new structures - part of the real 
work is hidden away in C code and I don't speak C very well. Have 
either of you successfully implemented any alternative structures? (At 
one stage I ws interested in trying Toeplitz forms, and now have a 
need for compound symmetry forms where the rho depends on a 
factor.) Have you anything beyond the source that might help? - for 
example a more richly commented example, or notes on what the C 
(and R)  functions actually do.

Regards,

Steve.




Date sent:      	Wed, 23 Mar 2005 10:56:46 +1100
To:             	"Michael Jerosch-Herold" <jeroschh at ohsu.edu>, R-help at stat.math.ethz.ch
From:           	Simon Blomberg <Simon.Blomberg at anu.edu.au>
Subject:        	Re: [R] LME correlation structures: user defined
Copies to:      	
> >Let me modify my question about user-defined covariance structures
> >for LME models: Can somebody tell me how I can see the code for the
> >definition of the correlation structures that come with the NLME
> >package. Specifically I like to see the code for the functions coef,
> >corMatrix, and intialize for any of the pre-defined correlation
> >structures, and use this as a template to define a new correlation
> >structure. So how do I see e.g. the code for the method initialize
> >for the correlation structure corExp or corARMA?
> 
> I'm interested in this too. Go to CRAN and download the source code
> for the nlme package. ungzip and untar it. Got to the R subdirectory.
> Inside that is a file called corStruct.R with all the method
> definitions for the built-in corStruct classes. Reading those should
> help.
> 
> Cheers,
> 
> Simon.
> 
> 
> >
> >thank you in advance!
> >
> >Michael Jerosch-Herold
> >
> >PS: Oh, and if somebody could still send me example code for a user
> >defined correlation structure I would much appreciate it, as my
> >previous requests for help have not yielded any response.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
> Visiting Fellow
> School of Botany & Zoology
> The Australian National University
> Canberra ACT 0200
> Australia
> 
> T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
> F: +61 2 6125 5573
> 
> CRICOS Provider # 00120C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


  Dr Steve Roberts 
  steve.roberts at manchester.ac.uk

Senior Lecturer in Medical Statistics,
CMMCH NHS Trust and University of Manchester Biostatistics Group,
0161 275 5192/5764 / 0161 276 5785



From alex.bach at irta.es  Fri Jul  1 01:09:28 2005
From: alex.bach at irta.es (Alex Bach)
Date: Fri, 1 Jul 2005 01:09:28 +0200
Subject: [R] Nolinear mixed-effects models (nlme)
Message-ID: <0C769598-FAE7-4032-943A-8FBCE62E4BD0@irta.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/cd879f38/attachment.pl

From Arne.Muller at sanofi-aventis.com  Fri Jul  1 12:14:20 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Fri, 1 Jul 2005 12:14:20 +0200
Subject: [R] p-values for classification
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3BC@CRBSMXSUSR04>

Dear All,

I'm classifying some data with various methods (binary classification). I'm interpreting the results via a confusion matrix from which I calculate the sensitifity and the fdr. The classifiers are trained on 575 data points and my test set has 50 data points.

I'd like to calculate p-values for obtaining <=fdr and >=sensitifity for each classifier. I was thinking about shuffling/bootstrap the lables of the test set, classify them and calculating the p-value from the obtained normal distributed random fdr and sensitifity.

The problem is that it's rather slow when running many rounds of shuffling/classification (I'd like to do this for many classifiers and parameter combinations). In addition classification of the 50 test data points with shuffled lables realistically produces only a  very limited number of possible fdr's and sensitivities, and I'm wondering if I can realy believe these values to be normal.

Basically I'm looking for a way to calculate the p-values analytically. I'd be happy  for any suggestions, web-addresses or references.

	kind regads,

	Arne



From ecoinformatics at gmail.com  Fri Jul  1 12:18:11 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Fri, 1 Jul 2005 12:18:11 +0200
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <82659b3c050630091798f97af@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<82659b3c05063008295b71d0e4@mail.gmail.com>
	<82659b3c050630091798f97af@mail.gmail.com>
Message-ID: <15f8e67d05070103181402f327@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/1e206aab/attachment.pl

From f.calboli at imperial.ac.uk  Fri Jul  1 13:31:48 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 01 Jul 2005 12:31:48 +0100
Subject: [R] loop over large dataset
Message-ID: <1120217508.14695.153.camel@localhost.localdomain>

Hi All,

I'd like to ask for a few clarifications. I am doing some calculations
over some biggish datasets. One has ~ 23000 rows, and 6 columns, the
other has ~620000 rows and 6 columns.

I am using these datasets to perform a simulation of of haplotype
coalescence over a pedigree (the datestes themselves are pedigree
information). I created a new dataset (same number of rows as the
pedigree dataset, 2 colums) and I use a looping functions to assign
haplotypes according to a standrd biological reprodictive process (i.e.
meiosis, sexual reproduction).

My code is someting like:

  off = function(sire, dam){ # simulation of reproduction, two inds
  sch.toll = round(runif(1, min = 1, max = 2))
  dch.toll = round(runif(1, min = 1, max = 2))
  s.gam = sire[,sch.toll]
  d.gam = dam[,dch.toll]
  offspring = cbind(s.gam,d.gam)
# offspring
}

for (i in 1:dim(new)[1]){
if(ped[i,3] != 0 & ped[i,5] != 0){
zz = off(as.matrix(t(new[ped[i,3],])),as.matrix(t(new[ped[i,5],])))
new[i,1] = zz[1]
new[i,2] = zz[2]
}
}

I am also attribution a generation index to each row with a trivial
calulation:

for(i in atres){
  genz[i] = (genz[ped[i,3]] + genz[ped[i,5]])/2 + 1
  #print(genz[i])
}

My question then. On the 23000 rows dataset the calculations take about
5 minutes. On the 620000 rows one I kill the process after ~24 hours,
and the the job is not finished. Why such immense discrepancy in
execution times (the code is the same, the datasets are stored in two
separate .RData files)?

Any light would be appreciated.

Federico

PS I am running R 2.1.0 on Debian Sarge, on a Dual 3 GHz Xeon machine
with 2 gig RAM. The R process uses 99% of the CPU, but hardly any RAM
for what I gather from top.



-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From 0034058 at fudan.edu.cn  Fri Jul  1 13:40:44 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 01 Jul 2005 19:40:44 +0800
Subject: [R] the format of the result
Message-ID: <20050701194044.6ad19006@localhost.localdomain>

I write a function to get the frequency and prop of a variable.

freq<-function(x,digits=3)
{naa<-is.na(x)
nas<-sum(naa)
if (any(naa))
x<-x[!naa]
n<-length(x)
ta<-table(x)
prop<-prop.table(ta)*100
res<-rbind(ta,prop)
rownames(res)<-c("Freq","Prop")
cat("Missing value(s) are",nas,".\n")
cat("Valid case(s) are",n,".\n")
cat("Total case(s) are",(n+nas),".\n\n")
print(res,digits=(digits+2))
cat("\n")
}

> freq(sample(letters[1:3],48,T),2)
Missing value(s) are 0 .
Valid case(s) are 48 .
Total case(s) are 48 .

         a     b     c
Freq 11.00 20.00 17.00
Prop 22.92 41.67 35.42

and i want the result to be like
         a      b     c
Freq 11.00  20.00  17.00
Prop 22.92% 41.67% 35.42%

how should i change my function to get what i want?
-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From HStevens at MUOhio.edu  Fri Jul  1 13:54:54 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Fri, 1 Jul 2005 07:54:54 -0400
Subject: [R] Simple indexing conundrum
Message-ID: <3c447e0bd3483d73234b7e7f78897921@MUOhio.edu>

My apologies in advance for my thickness but I can't seem to solve the 
following, seemingly simple, data manipulation problem:

I have a data frame that contains multiple factors and multiple 
continuous response variables, but duplicates of some factor 
combinations. The duplicates contain bad data, so I would like to 
eliminate the duplicates. I would like to retain the entire rows 
identified by the maximum value of one particular continuous response 
variable.

For instance,

 >data(airquality)

 > str(airquality)
`data.frame':	153 obs. of  6 variables:
  $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
  $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
  $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
  $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
  $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
  $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...

I would like to subset airquality, retaining only the rows, containing 
the maximum Solar.R for each month.

Any solution would be greatly appreciated.

Regards,
Hank



Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From ripley at stats.ox.ac.uk  Fri Jul  1 14:01:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Jul 2005 13:01:05 +0100 (BST)
Subject: [R] p-values for classification
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3BC@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3BC@CRBSMXSUSR04>
Message-ID: <Pine.LNX.4.61.0507011254390.8409@gannet.stats>

Not really an R question.

Most classifiers will produce predicted probabilities, and you can check 
their accuracy.  There are lots of details in my PRNN book, and some 
examples in MASS4.

I suggest you adjust your training and test sets to be more nearly equal, 
or use cross-validation.

I don't see how shuffling the labels will help: you want to know how well 
a classifier does when there is a real relationship between the 
explanatory variables and the class.  To take a simple example, suppose 
the classes are clearly linearly separable.  Then a logistic discriminant 
will have nigh-perfect performance on the actual data, but very poor 
performance on permuted labels.  You would do a lot better to simulate 
from a good fitted model, the so-called parametric bootstrapping.

On Fri, 1 Jul 2005 Arne.Muller at sanofi-aventis.com wrote:

> Dear All,
>
> I'm classifying some data with various methods (binary classification). 
> I'm interpreting the results via a confusion matrix from which I 
> calculate the sensitifity and the fdr. The classifiers are trained on 
> 575 data points and my test set has 50 data points.
>
> I'd like to calculate p-values for obtaining <=fdr and >=sensitifity for 
> each classifier. I was thinking about shuffling/bootstrap the lables of 
> the test set, classify them and calculating the p-value from the 
> obtained normal distributed random fdr and sensitifity.
>
> The problem is that it's rather slow when running many rounds of 
> shuffling/classification (I'd like to do this for many classifiers and 
> parameter combinations). In addition classification of the 50 test data 
> points with shuffled lables realistically produces only a very limited 
> number of possible fdr's and sensitivities, and I'm wondering if I can 
> realy believe these values to be normal.
>
> Basically I'm looking for a way to calculate the p-values analytically. 
> I'd be happy for any suggestions, web-addresses or references.
>
> 	kind regads,
>
> 	Arne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jsorkin at grecc.umaryland.edu  Fri Jul  1 14:01:07 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 01 Jul 2005 08:01:07 -0400
Subject: [R] R integration with Microsoft Powerpoint
Message-ID: <s2c4f859.021@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/8657460c/attachment.pl

From pwolf at wiwi.uni-bielefeld.de  Fri Jul  1 14:02:36 2005
From: pwolf at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 01 Jul 2005 14:02:36 +0200
Subject: [R] [R-pkgs] New CRAN package "relax": R Editor for Literate
 Analysis and lateX
Message-ID: <42C530DC.8000106@wiwi.uni-bielefeld.de>

Now package relax is on CRAN.

The name relax is short for
 
    R Editor for Literate Analysis and lateX

The main element of package relax is the function relax() which starts an
all-in-one editor for data analysis and easy creation of LaTeX based 
documents
with R.

After calling relax() it creates a tcl/tk widget with a report field. 
 The report field
enables you to enter R expressions as well as pieces of text to document 
your ideas.
Computations and plots can be included quickly. After finishing your work
the sequence of text chunks, code chunks and integrated graphics and/or 
R-output
will constitute the basis of your work. To achieve a higher quality 
relax integrates
LaTeX compilation for professional formatting and pretty printing.

Dependencies:

* R (>= 2.1.0), tcltk
* relax runs on windows systems, LaTeX / ghostscript has to be installed
* on Linux systems you have to install the img-package for tcltk

For further info see: 
http://www.wiwi.uni-bielefeld.de/~wolf/software/relax/relax.html

maintainer:

Hans Peter Wolf
Department of Economics
University of Bielefeld
pwolf at wiwi.uni-bielefeld.de



    R Editor for Literate Analysis and lateX

    --- the all-in-one editor for data analysis
    and easy creation of LaTeX based documents with R



From navarre_sabine at yahoo.fr  Fri Jul  1 14:04:56 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Fri, 1 Jul 2005 14:04:56 +0200 (CEST)
Subject: [R] barplot legend
Message-ID: <20050701120456.31070.qmail@web26608.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/412f55bc/attachment.pl

From andy_liaw at merck.com  Fri Jul  1 14:12:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Jul 2005 08:12:12 -0400
Subject: [R] loop over large dataset
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA5B@usctmx1106.Merck.com>

My suggestion is that you try to vectorize the computation as much as you
can.

>From what you've shown, `new' and `ped' need to have the same number of
rows, right?

Your `off' function seems to be randomly choosing between columns 1 and 2
from its two input matrices (one row each?).  You may want to do the
sampling all at once instead of looping over the rows.  E.g.,

> (m <- matrix(1:10, ncol=2))
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> (colSample <- sample(1:2, nrow(m), replace=TRUE))
[1] 1 1 2 1 1
> (x <- m[cbind(1:nrow(m), colSample)])
[1] 1 2 8 4 5

So you might want to do something like (obviously untested):

todo <- ped[,3] * ped[,5] != 0  ## indicator of which rows to work on
n.todo <- sum(todo)  ## how many are there?
sire <- new[ped[todo, 3], ]
dam <- new[ped[todo, 5], ]
s.gam <- sire[1:nrow(sire), sample(1:2, nrow(sire), replace=TRUE)]
d.gam <- dam[1:nrow(dam), sample(1:2, nrow(dam), replace=TRUE)]
new[todo, 1:2] <- cbind(s.gam, d.gam)

Andy


> From: Federico Calboli
> 
> Hi All,
> 
> I'd like to ask for a few clarifications. I am doing some calculations
> over some biggish datasets. One has ~ 23000 rows, and 6 columns, the
> other has ~620000 rows and 6 columns.
> 
> I am using these datasets to perform a simulation of of haplotype
> coalescence over a pedigree (the datestes themselves are pedigree
> information). I created a new dataset (same number of rows as the
> pedigree dataset, 2 colums) and I use a looping functions to assign
> haplotypes according to a standrd biological reprodictive 
> process (i.e.
> meiosis, sexual reproduction).
> 
> My code is someting like:
> 
>   off = function(sire, dam){ # simulation of reproduction, two inds
>   sch.toll = round(runif(1, min = 1, max = 2))
>   dch.toll = round(runif(1, min = 1, max = 2))
>   s.gam = sire[,sch.toll]
>   d.gam = dam[,dch.toll]
>   offspring = cbind(s.gam,d.gam)
> # offspring
> }
> 
> for (i in 1:dim(new)[1]){
> if(ped[i,3] != 0 & ped[i,5] != 0){
> zz = off(as.matrix(t(new[ped[i,3],])),as.matrix(t(new[ped[i,5],])))
> new[i,1] = zz[1]
> new[i,2] = zz[2]
> }
> }
> 
> I am also attribution a generation index to each row with a trivial
> calulation:
> 
> for(i in atres){
>   genz[i] = (genz[ped[i,3]] + genz[ped[i,5]])/2 + 1
>   #print(genz[i])
> }
> 
> My question then. On the 23000 rows dataset the calculations 
> take about
> 5 minutes. On the 620000 rows one I kill the process after ~24 hours,
> and the the job is not finished. Why such immense discrepancy in
> execution times (the code is the same, the datasets are stored in two
> separate .RData files)?
> 
> Any light would be appreciated.
> 
> Federico
> 
> PS I am running R 2.1.0 on Debian Sarge, on a Dual 3 GHz Xeon machine
> with 2 gig RAM. The R process uses 99% of the CPU, but hardly any RAM
> for what I gather from top.
> 
> 
> 
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jsorkin at grecc.umaryland.edu  Fri Jul  1 14:14:47 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 01 Jul 2005 08:14:47 -0400
Subject: [R] It is time to say thank you.
Message-ID: <s2c4fb91.024@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/9c5ed030/attachment.pl

From andy_liaw at merck.com  Fri Jul  1 14:31:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Jul 2005 08:31:03 -0400
Subject: [R] Simple indexing conundrum
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA5D@usctmx1106.Merck.com>

Is this close to what you want?

> air.sub <- do.call("rbind", lapply(split(airquality, airquality$Month), 
+                                    function(d) d[which.max(d$Solar.R),]))
> air.sub
  Ozone Solar.R Wind Temp Month Day
5    14     334 11.5   64     5  16
6    NA     332 13.8   80     6  14
7    40     314 10.9   83     7   6
8    28     273 11.5   82     8  13
9    24     259  9.7   73     9  10

Andy

> From: Martin Henry H. Stevens
> 
> My apologies in advance for my thickness but I can't seem to 
> solve the 
> following, seemingly simple, data manipulation problem:
> 
> I have a data frame that contains multiple factors and multiple 
> continuous response variables, but duplicates of some factor 
> combinations. The duplicates contain bad data, so I would like to 
> eliminate the duplicates. I would like to retain the entire rows 
> identified by the maximum value of one particular continuous response 
> variable.
> 
> For instance,
> 
>  >data(airquality)
> 
>  > str(airquality)
> `data.frame':	153 obs. of  6 variables:
>   $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
>   $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
>   $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
>   $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
>   $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
>   $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
> 
> I would like to subset airquality, retaining only the rows, 
> containing 
> the maximum Solar.R for each month.
> 
> Any solution would be greatly appreciated.
> 
> Regards,
> Hank
> 
> 
> 
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From MSchwartz at mn.rr.com  Fri Jul  1 14:41:39 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 01 Jul 2005 07:41:39 -0500
Subject: [R] barplot legend
In-Reply-To: <20050701120456.31070.qmail@web26608.mail.ukl.yahoo.com>
References: <20050701120456.31070.qmail@web26608.mail.ukl.yahoo.com>
Message-ID: <1120221700.3766.12.camel@localhost.localdomain>

On Fri, 2005-07-01 at 14:04 +0200, Navarre Sabine wrote:
> Hi,
>  
> Is it possible ti put the legend out of a barplot?
>  
> tanks
>  
> Sabine


I presume that you mean outside the plot region?

If so, you can use something like the following:

# Adjust the plot margins to make room for the 
# legend on the right side. See ?par
par(mar = c(5, 4, 4, 10) + 0.1)

barplot(1:10)
box()

# Set xpd to allow legend placement outside
# plot region. See ?par
par(xpd = TRUE)

# Left click on the right side of the window where you want
# the legend. See ?locator
l <- locator(1)

# Now put the legend where you clicked
# See ?legend
legend(l$x, l$y, legend = "Legend Here")

HTH,

Marc Schwartz



From jfbrennan at rogers.com  Fri Jul  1 14:45:04 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 1 Jul 2005 08:45:04 -0400
Subject: [R] Simple indexing conundrum
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA5D@usctmx1106.Merck.com>
Message-ID: <200507011245.j61Cj4si003549@hypatia.math.ethz.ch>

Here is a different approach I only send since the result is slightly
different in that two rows are returned for Month 9 and the original row
number is retained.

> max2<-function(x){max(x,na.rm=T)}
> MonthMax<-ave(Solar.R,Month,FUN=max2)
> new<-subset(airquality,Solar.R==MonthMax)
> new<-subset(airquality,Solar.R==MonthMax)
> new
    Ozone Solar.R Wind Temp Month Day
16     14     334 11.5   64     5  16
45     NA     332 13.8   80     6  14
67     40     314 10.9   83     7   6
105    28     273 11.5   82     8  13
133    24     259  9.7   73     9  10
135    21     259 15.5   76     9  12

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
Sent: July 1, 2005 8:31 AM
To: 'Martin Henry H. Stevens'; R-Help
Subject: Re: [R] Simple indexing conundrum

Is this close to what you want?

> air.sub <- do.call("rbind", lapply(split(airquality, airquality$Month), 
+                                    function(d) d[which.max(d$Solar.R),]))
> air.sub
  Ozone Solar.R Wind Temp Month Day
5    14     334 11.5   64     5  16
6    NA     332 13.8   80     6  14
7    40     314 10.9   83     7   6
8    28     273 11.5   82     8  13
9    24     259  9.7   73     9  10

Andy

> From: Martin Henry H. Stevens
> 
> My apologies in advance for my thickness but I can't seem to 
> solve the 
> following, seemingly simple, data manipulation problem:
> 
> I have a data frame that contains multiple factors and multiple 
> continuous response variables, but duplicates of some factor 
> combinations. The duplicates contain bad data, so I would like to 
> eliminate the duplicates. I would like to retain the entire rows 
> identified by the maximum value of one particular continuous response 
> variable.
> 
> For instance,
> 
>  >data(airquality)
> 
>  > str(airquality)
> `data.frame':	153 obs. of  6 variables:
>   $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
>   $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
>   $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
>   $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
>   $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
>   $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
> 
> I would like to subset airquality, retaining only the rows, 
> containing 
> the maximum Solar.R for each month.
> 
> Any solution would be greatly appreciated.
> 
> Regards,
> Hank
> 
> 
> 
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Jul  1 14:57:42 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 1 Jul 2005 14:57:42 +0200
Subject: [R] "10^k" axis labels {was ".. (log scale on y-axis)"}
In-Reply-To: <971536df0506300428cf8191a@mail.gmail.com>
References: <f8b847f0506291320ff15932@mail.gmail.com>
	<971536df0506300428cf8191a@mail.gmail.com>
Message-ID: <17093.15814.727897.518826@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Thu, 30 Jun 2005 07:28:30 -0400 writes:

    Gabor> On 6/29/05, Jing Shen <jshen6 at gmail.com> wrote:

    >> I am planning to plot my data on log scale (y-axis). There is a
    >> parameter in plot function, which is
    >> plot( ..., log="y", ...)
    >> While, the problem is that it is with base of e. Is there a way to let
    >> me change it to 10 instead of e?
    >> 

    Gabor> Is your question how to get the axis labels to be powers of 10?
    Gabor> In that case,

    Gabor> plot(1:100, log = "y", yaxt = "n")  # do not show y axis 
    Gabor> axis(2, c(1,10,100))  # draw y axis with required labels

and if you're there, you might be interested in the following
which provides a somewhat automated way to show 
"a * 10 ^ k" tick-labels instead of the scientific "a e k" ones.
{ For some time, I had wanted that something like this could
  become an easy option for builtin axis(*), but then I also know
  that we should rather strive to build future-proof tools, which
  "hence" should we applicable to 'grid' as well as to old-style
  'graphics'  and all this got me stuck in the process ...
}

Martin Maechler, ETH Zurich

---------------

axTexpr <- function(side, at = axTicks(side, axp=axp, usr=usr, log=log),
                    axp = NULL, usr = NULL, log = NULL)
{
    ## Purpose: Do "a 10^k" labeling instead of "a e<k>"
    ##	      this auxiliary should return 'at' and 'label' (expression)
    ## ----------------------------------------------------------------------
    ## Arguments: as for axTicks()
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date:  7 May 2004, 18:01
    eT <- floor(log10(abs(at)))# at == 0 case is dealt with below
    mT <- at / 10^eT
    ss <- lapply(seq(along = at),
                 function(i) if(at[i] == 0) quote(0) else
                 substitute(A %*% 10^E, list(A=mT[i], E=eT[i])))
    do.call("expression", ss)
}

par(mar=.1+c(5,5,4,1))## << For the horizontal y-axis labels, need more space
plot(x,y, axes= FALSE, frame=TRUE)
aX <- axTicks(1); axis(1, at=aX, label= axTexpr(1, aX))
if(FALSE) # rather the next one
{ aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY))}
## or rather (horizontal labels on y-axis):
aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY), las=2)



From ligges at statistik.uni-dortmund.de  Fri Jul  1 15:04:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Jul 2005 15:04:10 +0200
Subject: [R] FW: plot legend outside the grid
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C50B@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C50B@wdexchmb01.lexicon.lexgen.com>
Message-ID: <42C53F4A.7080207@statistik.uni-dortmund.de>

In principle you are there, just after opening the device set par(mar) 
appropriate (large margin to place the legend in) before starting with 
plotting.
In the legend, specify points which virtually are in the margin you have 
already expanded...


Uwe Ligges



Ghosh, Sandeep wrote:

> 
> -----Original Message-----
> From: Ghosh, Sandeep 
> Sent: Thursday, June 30, 2005 5:43 PM
> To: 'Berton Gunter'
> Subject: plot legend outside the grid 
> 
> 
> Thanks for the pointers... I managed to get everything to look and feel the way I want except for the legend to plot outside the grid... Thanks for the note on the par, but I'm not able to it to plot outside the plot grid..
> 
> dataFrame <- as.data.frame(t(structure(c(
> 64,'wt',
> 62,'wt',
> 66,'wt',

[SNIP]


> 63,'hom',
> 64,'hom',
> 67,'hom'), .Dim=c(2,98))));
> 
> colnames(dataFrame) <- c('marbles_buried', 'genotype');
> 
> png('mb.png', width=400, height=400, pointsize=8);
> 
> dataFrame[c("marbles_buried")] <- lapply(dataFrame[c("marbles_buried")], function(x) as.numeric(levels(x)[x]));
> 
> par(xpd=FALSE)
> 
> with (dataFrame, stripchart(marbles_buried ~ genotype, method="jitter", vertical=TRUE,  col = c('blue', 'red', 'green'), xlab='Genotype', ylab = "Marbles Buried", main='MBA WTs Vs HOMs', pch=c(1,4,2), jitter=1/3.5, cex=1))
> 
> meds <- as.vector(with(dataFrame, by(marbles_buried, genotype, mean)))
> segments((1:3)-0.25, meds, (1:3)+0.25, meds, col = c('blue', 'red', 'green'));
> 
> dataWt <- subset(dataFrame, genotype=='wt', select=c(marbles_buried,genotype));
> dataHet <- subset(dataFrame, genotype=='het', select=c(marbles_buried,genotype));
> dataHom <- subset(dataFrame, genotype=='hom', select=c(marbles_buried,genotype));
> 
> wtCount <- length(dataWt$marbles_buried);
> hetCount <- length(dataHet$marbles_buried);
> homCount <- length(dataHom$marbles_buried);
> wtLegend <- paste ("wt, (n=", wtCount, ")");
> hetLegend <- paste ("het, (n=", hetCount, ")");
> homLegend <- paste ("hom, (n=", homCount, ")");
> 
> par(xpd=TRUE)
> legend(1, max(as.vector(dataFrame$marbles_buried)), c(wtLegend, hetLegend, homLegend), col=c('blue', 'red', 'green'), pch=c(1,4,2));
> 
> -Thanks
> Sandeep.
> 
> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com]
> Sent: Thursday, June 30, 2005 2:55 PM
> To: Ghosh, Sandeep
> Subject: RE: [R] Help with stripplot
> 
> 
> Of course!
> 
> stripchart() is a base graphics function and tehrefore has available to it
> the base graphics functionality, like (the base graphics function, **not**
> the lattice argument) legend(). See ?legend in the graphics package. Note
> the use of locator() for positioning the legend.
> 
> Note: By default the legend will be clipped to the plot region. If you wish
> to have a legend outside the plot region set the xpd parameter of par to
> TRUE or NA prior to plotting.
> 
> -- Bert 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ghosh, Sandeep
>>Sent: Thursday, June 30, 2005 12:22 PM
>>To: Deepayan Sarkar; r-help at stat.math.ethz.ch
>>Subject: Re: [R] Help with stripplot
>>
>>Another question, in stripchart is there a way to draw a 
>>legends. I need legends that gives the mice count for each 
>>genotype wt/het/hom, something like the xyplot plot support 
>>for key/auto.key.
>>
>>-Sandeep
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From HStevens at MUOhio.edu  Fri Jul  1 15:13:10 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Fri, 1 Jul 2005 09:13:10 -0400
Subject: [R] Simple indexing conundrum
In-Reply-To: <200507011245.j61Cj3T8082292@mcsaix06.mcs.muohio.edu>
References: <200507011245.j61Cj3T8082292@mcsaix06.mcs.muohio.edu>
Message-ID: <2d840479c63d09b15e1f68b2195e5724@MUOhio.edu>

Thank guys! Both solutions do what I need. Thanks.
Hank
On Jul 1, 2005, at 8:45 AM, Jim Brennan wrote:

> Here is a different approach I only send since the result is slightly
> different in that two rows are returned for Month 9 and the original 
> row
> number is retained.
>
>> max2<-function(x){max(x,na.rm=T)}
>> MonthMax<-ave(Solar.R,Month,FUN=max2)
>> new<-subset(airquality,Solar.R==MonthMax)
>> new<-subset(airquality,Solar.R==MonthMax)
>> new
>     Ozone Solar.R Wind Temp Month Day
> 16     14     334 11.5   64     5  16
> 45     NA     332 13.8   80     6  14
> 67     40     314 10.9   83     7   6
> 105    28     273 11.5   82     8  13
> 133    24     259  9.7   73     9  10
> 135    21     259 15.5   76     9  12
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: July 1, 2005 8:31 AM
> To: 'Martin Henry H. Stevens'; R-Help
> Subject: Re: [R] Simple indexing conundrum
>
> Is this close to what you want?
>
>> air.sub <- do.call("rbind", lapply(split(airquality, 
>> airquality$Month),
> +                                    function(d) 
> d[which.max(d$Solar.R),]))
>> air.sub
>   Ozone Solar.R Wind Temp Month Day
> 5    14     334 11.5   64     5  16
> 6    NA     332 13.8   80     6  14
> 7    40     314 10.9   83     7   6
> 8    28     273 11.5   82     8  13
> 9    24     259  9.7   73     9  10
>
> Andy
>
>> From: Martin Henry H. Stevens
>>
>> My apologies in advance for my thickness but I can't seem to
>> solve the
>> following, seemingly simple, data manipulation problem:
>>
>> I have a data frame that contains multiple factors and multiple
>> continuous response variables, but duplicates of some factor
>> combinations. The duplicates contain bad data, so I would like to
>> eliminate the duplicates. I would like to retain the entire rows
>> identified by the maximum value of one particular continuous response
>> variable.
>>
>> For instance,
>>
>>> data(airquality)
>>
>>> str(airquality)
>> `data.frame':	153 obs. of  6 variables:
>>   $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
>>   $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
>>   $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
>>   $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
>>   $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
>>   $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
>>
>> I would like to subset airquality, retaining only the rows,
>> containing
>> the maximum Solar.R for each month.
>>
>> Any solution would be greatly appreciated.
>>
>> Regards,
>> Hank
>>
>>
>>
>> Dr. Martin Henry H. Stevens, Assistant Professor
>> 338 Pearson Hall
>> Botany Department
>> Miami University
>> Oxford, OH 45056
>>
>> Office: (513) 529-4206
>> Lab: (513) 529-4262
>> FAX: (513) 529-4243
>> http://www.cas.muohio.edu/botany/bot/henry.html
>> http://www.muohio.edu/ecology/
>> http://www.muohio.edu/botany/
>> "E Pluribus Unum"
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From ggrothendieck at gmail.com  Fri Jul  1 15:23:45 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Jul 2005 09:23:45 -0400
Subject: [R] "10^k" axis labels {was ".. (log scale on y-axis)"}
In-Reply-To: <17093.15814.727897.518826@stat.math.ethz.ch>
References: <f8b847f0506291320ff15932@mail.gmail.com>
	<971536df0506300428cf8191a@mail.gmail.com>
	<17093.15814.727897.518826@stat.math.ethz.ch>
Message-ID: <971536df05070106233c7e27d9@mail.gmail.com>

On 7/1/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
> >>>>>     on Thu, 30 Jun 2005 07:28:30 -0400 writes:
> 
>    Gabor> On 6/29/05, Jing Shen <jshen6 at gmail.com> wrote:
> 
>    >> I am planning to plot my data on log scale (y-axis). There is a
>    >> parameter in plot function, which is
>    >> plot( ..., log="y", ...)
>    >> While, the problem is that it is with base of e. Is there a way to let
>    >> me change it to 10 instead of e?
>    >>
> 
>    Gabor> Is your question how to get the axis labels to be powers of 10?
>    Gabor> In that case,
> 
>    Gabor> plot(1:100, log = "y", yaxt = "n")  # do not show y axis
>    Gabor> axis(2, c(1,10,100))  # draw y axis with required labels
> 
> and if you're there, you might be interested in the following
> which provides a somewhat automated way to show
> "a * 10 ^ k" tick-labels instead of the scientific "a e k" ones.
> { For some time, I had wanted that something like this could
>  become an easy option for builtin axis(*), but then I also know
>  that we should rather strive to build future-proof tools, which
>  "hence" should we applicable to 'grid' as well as to old-style
>  'graphics'  and all this got me stuck in the process ...
> }
> 
> Martin Maechler, ETH Zurich
> 
> ---------------
> 
> axTexpr <- function(side, at = axTicks(side, axp=axp, usr=usr, log=log),
>                    axp = NULL, usr = NULL, log = NULL)
> {
>    ## Purpose: Do "a 10^k" labeling instead of "a e<k>"
>    ##        this auxiliary should return 'at' and 'label' (expression)
>    ## ----------------------------------------------------------------------
>    ## Arguments: as for axTicks()
>    ## ----------------------------------------------------------------------
>    ## Author: Martin Maechler, Date:  7 May 2004, 18:01
>    eT <- floor(log10(abs(at)))# at == 0 case is dealt with below
>    mT <- at / 10^eT
>    ss <- lapply(seq(along = at),
>                 function(i) if(at[i] == 0) quote(0) else
>                 substitute(A %*% 10^E, list(A=mT[i], E=eT[i])))
>    do.call("expression", ss)
> }
> 
> par(mar=.1+c(5,5,4,1))## << For the horizontal y-axis labels, need more space
> plot(x,y, axes= FALSE, frame=TRUE)
> aX <- axTicks(1); axis(1, at=aX, label= axTexpr(1, aX))
> if(FALSE) # rather the next one
> { aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY))}
> ## or rather (horizontal labels on y-axis):
> aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY), las=2)

This may not be as good as what you have (although its arguably
prettier in the specific example below) and may suffice in many,
though possibly not all, cases -- I mention it since its very simple
and, in fact, requires no auxilliary routines.  It uses your idea of 
employing axTicks.  The key trick is to use axTicks twice in axis:

 x <- 10 ^ seq(-2,10) # test data

plot(x, log = "y", yaxt = "n")
axis(2, axTicks(2), axTicks(2))



From usenet at s-boehringer.de  Fri Jul  1 15:29:26 2005
From: usenet at s-boehringer.de (usenet@s-boehringer.de)
Date: Fri, 1 Jul 2005 15:29:26 +0200 (CEST)
Subject: [R] Reconstructing LD function
Message-ID: <38102.132.252.149.100.1120224566.squirrel@webmail.loomes.de>

On Mon, 2005-06-27 at 13:18, Prof Brian Ripley wrote:
> On Mon, 27 Jun 2005 usenet at s-boehringer.de wrote:
>
> > in an LDA analysis with n groups n-1 LD functions result. Implicitly this
> > defines an LD fucntion for the last group. Does there exist code already
> > to explictly construct this LD function?

Thank you for the quick reply.

> What `LDA analysis' are our discussing here?  (LDA is usually
> `linear discriminant analysis', so what did you mean and what R function
> are you nor referring to?)

> R has lda in package MASS, and that works with n LD functions.  To reduce
> it to n-1, subtract the last one from the others, in which case LD_n == 0.
Indeed I have been using the MASS::lda package.

> Anything you do in LD analysis only depends on differences in LD
> functions, and there really are n of them.  With two groups one is
> conventionally taken to be zero (the first, usually, not the last).
How is the classifcation decision reached from the LD functions? Are
those what is known as "linear Fisherian discriminant functions"? If so,
I'm not positive about why one of these functions can be set to 0.

Thank you in advance for the clarification.

Best wishes,

	Stefan



From ripley at stats.ox.ac.uk  Fri Jul  1 15:44:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Jul 2005 14:44:40 +0100 (BST)
Subject: [R] Reconstructing LD function
In-Reply-To: <38102.132.252.149.100.1120224566.squirrel@webmail.loomes.de>
References: <38102.132.252.149.100.1120224566.squirrel@webmail.loomes.de>
Message-ID: <Pine.LNX.4.61.0507011439360.9415@gannet.stats>

On Fri, 1 Jul 2005 usenet at s-boehringer.de wrote:

> On Mon, 2005-06-27 at 13:18, Prof Brian Ripley wrote:
>> On Mon, 27 Jun 2005 usenet at s-boehringer.de wrote:
>>
>>> in an LDA analysis with n groups n-1 LD functions result. Implicitly this
>>> defines an LD fucntion for the last group. Does there exist code already
>>> to explictly construct this LD function?
>
> Thank you for the quick reply.
>
>> What `LDA analysis' are our discussing here?  (LDA is usually
>> `linear discriminant analysis', so what did you mean and what R function
>> are you nor referring to?)
>
>> R has lda in package MASS, and that works with n LD functions.  To reduce
>> it to n-1, subtract the last one from the others, in which case LD_n == 0.
> Indeed I have been using the MASS::lda package.
>
>> Anything you do in LD analysis only depends on differences in LD
>> functions, and there really are n of them.  With two groups one is
>> conventionally taken to be zero (the first, usually, not the last).

> How is the classifcation decision reached from the LD functions? Are
> those what is known as "linear Fisherian discriminant functions"? If so,
> I'm not positive about why one of these functions can be set to 0.

What I did say:

`Anything you do in LD analysis only depends on differences in LD 
functions'

So subtracting any one function from the others does not change the 
differences.

LD is not about classification, and Fisher did not do classification, nor 
did he use more than 2 classes.  I suspect your difficulty is going to be 
clearing your preconceptions. lda() is support software for a book which 
does explain the relationship between Fisher's discrimination and 
classification: please consult it for the background.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmbates at gmail.com  Fri Jul  1 15:46:35 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 1 Jul 2005 08:46:35 -0500
Subject: [R] Nolinear mixed-effects models (nlme)
In-Reply-To: <0C769598-FAE7-4032-943A-8FBCE62E4BD0@irta.es>
References: <0C769598-FAE7-4032-943A-8FBCE62E4BD0@irta.es>
Message-ID: <40e66e0b05070106462db2dfba@mail.gmail.com>

On 6/30/05, Alex Bach <alex.bach at irta.es> wrote:
> Hello,
> 
> I am trying to fit a nonlinear model of the form of:
> 
> A*x^b*exp(-c*x)
> 
> This represents a lactation curve. I have a bunch of cows, so I want
> COW to be a random effect.

You need to decide which of the model parameters (i.e. A, B and C)
should have a random effect grouped by COW and to specify this in your
call to nlme.

> 
> I have been trying the following code with very littel success:
> 
>  > fm1 <- nlme(yield ~ A*(DIM^B)*(exp(-C*DIM)),
> +             data = group,
> +             fixed = A + B + C ~ 1,
> +             start = c(A = 20, B = 0.3, C = 0.03))
> 
> Does anyone know how to add the random effect of the cow? I have used
> the command groupedData to have Cow as subject (i.e., yield~DIM |
> cow). Is this a valid and sufficient approach? I have the feeling it
> is not sufficient.
> 
> Also, does anyone know whether the formulation of the fixed effects
> is correct?.
> 
> Thank you very much,
> 
> Alex
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramos.beatriz at gmail.com  Fri Jul  1 16:01:05 2005
From: ramos.beatriz at gmail.com (bgmail)
Date: Fri, 01 Jul 2005 16:01:05 +0200
Subject: [R] plot svm
Message-ID: <42C54CA1.9070907@gmail.com>

Hello
I'm working with DNA microarrays and want to classify them with SVM.  I 
want to plot the results and it's imposible for me. I found others 
tutorials and examples (with iris and cats data) where you can plot the 
results with plot.svm, but you need to write a formula and I don't know 
how to do this with golubEsets data, for example .

plot ( svm1,  golubTrain,  formula)

For example, Iris Data:  
   Sepal.Length   Sepal.Width   Petal.Length   Petal.Width    Species
1            5.1               3.5                1.4                
0.2            setosa
2            4.9               3.0                1.4                
0.2            setosa
3            4.7               3.2                1.3                
0.2            setosa
4            4.6               3.1                1.5                
0.2            setosa
5            5.0               3.6                1.4                
0.2            setosa
6            5.4               3.9                1.7                
0.4            setosa
7            4.6               3.4                1.4                
0.3            setosa
8            5.0               3.4                1.5                
0.2            setosa

m2 <- svm(Species~., data = iris)
plot(m2, iris, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, 
Sepal.Length = 4))

I should be grateful if you would send me information about how to plot 
the golubEsets data (for example the formula, because I have tested 
several options but neither of them work). My data are very similar 
(expression values with several conditions), so I could plot my results 
if I knew how to plot golub data.

Thanks a lot

Beatriz



From ripley at stats.ox.ac.uk  Fri Jul  1 16:15:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Jul 2005 15:15:08 +0100 (BST)
Subject: [R] R integration with Microsoft Powerpoint
In-Reply-To: <s2c4f859.021@grecc.umaryland.edu>
References: <s2c4f859.021@grecc.umaryland.edu>
Message-ID: <Pine.LNX.4.61.0507011513200.9661@gannet.stats>

On Fri, 1 Jul 2005, John Sorkin wrote:

> Please allow me an unusual question.
>
> Is there any way that R can be closely integrated with a Microsoft
> Powerpoint presentation? I would like to embed R calculations in
> Powerpoint so that I will start Powerpoint, be prompted to enter some
> parameters, and an R function will run and return values and graphs.

R can be driven by COM, so if Powerpoint supports COM (possibly via VBA)
this would be possible.  It is likely, as other MS Office applications do.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ssk2031 at columbia.edu  Fri Jul  1 16:17:08 2005
From: ssk2031 at columbia.edu (Suresh Krishna)
Date: Fri, 01 Jul 2005 10:17:08 -0400
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <eb555e6605063010075617c608@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<eb555e6605063010075617c608@mail.gmail.com>
Message-ID: <42C55064.6030204@columbia.edu>


i dont use gmail, but this method *may* run into problems if people are 
replying to a message and "r-help" is on the cc: line.

thunderbird has a "to: or cc:" option for this... is gmail's to: field a 
default for "to: or cc:" ?

-s.

Deepayan Sarkar wrote:
> On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote:
> 
>>This is slightly off-topic but I would be interested in whether anyone
>>has succeeded in creating a filter expression for Google's gmail
>>system that will select messages sent through the R-help and R-devel
>>lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
>>in the subject line but I haven't been able to work out the exact
>>syntax that would do this and not select messages that have an 'R'
>>anywhere in the subject.
> 
> 
> I filter on the To field, which mostly works:
> 
> Matches: to:(r-help at stat.math.ethz.ch)
> Do this: Skip Inbox, Apply label "r-help"
> 
> Matches: to:(r-help at r-project.org)
> Do this: Skip Inbox, Apply label "r-help"
> 
> Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From khobson at fd9ns01.okladot.state.ok.us  Fri Jul  1 16:23:23 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 1 Jul 2005 09:23:23 -0500
Subject: [R]  the format of the result
Message-ID: <OFE1F9241E.C351421A-ON86257031.004EEDA2-86257031.004EEE1C@fd9ns01.okladot.state.ok.us>





See ?sprintf

#e.g. Replace your prop line with:
prop<-sprintf("%.2f%%", prop.table(ta)*100)

_____________
mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From ecoinformatics at gmail.com  Fri Jul  1 16:26:18 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Fri, 1 Jul 2005 16:26:18 +0200
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <42C55064.6030204@columbia.edu>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<eb555e6605063010075617c608@mail.gmail.com>
	<42C55064.6030204@columbia.edu>
Message-ID: <15f8e67d05070107261c814e31@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/aedd0a66/attachment.pl

From khobson at fd9ns01.okladot.state.ok.us  Fri Jul  1 16:40:07 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 1 Jul 2005 09:40:07 -0500
Subject: [R]  R integration with Microsoft Powerpoint
Message-ID: <OFCB62644C.67F4E4CA-ON86257031.0050024C-86257031.00507632@fd9ns01.okladot.state.ok.us>





Sure.  Just run R in a BAT file.  You just reference the BAT file in
PowerPoint like any other EXE application via an OLE link.   Of course you
can always use VBA code in Powerpoint to Shell() to the BAT program.

In R, type ?BATCH to see how the BAT file's content line should be coded to
run the R program.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From d.firth at warwick.ac.uk  Fri Jul  1 17:36:14 2005
From: d.firth at warwick.ac.uk (David Firth)
Date: Fri, 1 Jul 2005 16:36:14 +0100
Subject: [R] ranking predictive features in logsitic regression
In-Reply-To: <006d01c57db1$1bbcd250$9701a8c0@Tablet>
References: <006d01c57db1$1bbcd250$9701a8c0@Tablet>
Message-ID: <44b1c257e78f266bcb5896206c50c052@warwick.ac.uk>

On 30 Jun, 2005, at 21:20, Stephen Choularton wrote:

> Hi
>
> Is there some function R that multiplies each coefficient by the
> standard deviation of the corresponding variable and produces a 
> ranking?
>
>

Possibly you meant un-signed coefficients?  In which case something like

   function(model) rank(abs(coef(model)) * apply(model.matrix(model), 2, 
sd))

should do what you asked about.

The "relimp" package provides approximate inference for comparisons of 
this kind.

I should say that I don't think that such a ranking will often be very 
useful, though.  Some coefficients will be determined with greater 
precision than others, and there may be correlations to worry about, or 
variables may only make sense when considered in groups (eg factor 
effects, or interactions with corresponding main effects, etc.)

David



From liuwensui at gmail.com  Fri Jul  1 17:52:57 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 1 Jul 2005 11:52:57 -0400
Subject: [R] OT: How to instaill gcc in cygwin?
Message-ID: <1115a2b0050701085226cb004b@mail.gmail.com>

Dear Listers,

I know it is far off topic. But I do know there must be some people
here who know it very well.

Sorry for bothering others.

Thanks.

-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From jared_stabach at hotmail.com  Fri Jul  1 19:12:21 2005
From: jared_stabach at hotmail.com (Jared Stabach)
Date: Fri, 01 Jul 2005 13:12:21 -0400
Subject: [R] Calculate 3D Fixed Kernel Home Range
Message-ID: <BAY102-F37ADE82887F2E95DDD319386E20@phx.gbl>

I have x,y data on three animals (~150 data points each).  I have calculated 
the fixed kernel home range using the 'adehabitat' library and the LSCV 
smoothing factor.  Can anyone provide me with some help on how to display 
the density estimate of the Utilization Distribution 3-dimensionally?

Thanks in advance,

Jared



From mschwartz at mn.rr.com  Fri Jul  1 19:42:20 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 01 Jul 2005 12:42:20 -0500
Subject: [R] the format of the result
In-Reply-To: <20050701194044.6ad19006@localhost.localdomain>
References: <20050701194044.6ad19006@localhost.localdomain>
Message-ID: <1120239741.3740.29.camel@localhost.localdomain>

On Fri, 2005-07-01 at 19:40 +0800, ronggui wrote:
> I write a function to get the frequency and prop of a variable.
> 
> freq<-function(x,digits=3)
> {naa<-is.na(x)
> nas<-sum(naa)
> if (any(naa))
> x<-x[!naa]
> n<-length(x)
> ta<-table(x)
> prop<-prop.table(ta)*100
> res<-rbind(ta,prop)
> rownames(res)<-c("Freq","Prop")
> cat("Missing value(s) are",nas,".\n")
> cat("Valid case(s) are",n,".\n")
> cat("Total case(s) are",(n+nas),".\n\n")
> print(res,digits=(digits+2))
> cat("\n")
> }
> 
> > freq(sample(letters[1:3],48,T),2)
> Missing value(s) are 0 .
> Valid case(s) are 48 .
> Total case(s) are 48 .
> 
>          a     b     c
> Freq 11.00 20.00 17.00
> Prop 22.92 41.67 35.42
> 
> and i want the result to be like
>          a      b     c
> Freq 11.00  20.00  17.00
> Prop 22.92% 41.67% 35.42%
> 
> how should i change my function to get what i want?


Here is a modification of the function that I think should work. Note
that part of the output formatting process has to take into account the
a priori unknowns involving your 'digits' argument, the lengths of the
dimnames resulting from the table and the lengths of the frequency
counts in the table. Thus, a fair amount of the code is establishing the
'width' argument, which is then used in formatC() so that the output can
be column aligned properly.

Note that by default, table() will exclude "NA", so you do not need to
subset 'x' before using table().

Also, note that I change "Prop" to "Pct".


freq <- function(x, digits = 3)
{
  n <- length(x)
  missing <- sum(is.na(x))
  ta <- table(x)
  pct <- prop.table(ta) * 100

  width <- max(nchar(unlist(dimnames(ta))) + 1,
               nchar(ta) + digits + 1,
               5 + digits)
  
  Vals <- paste(formatC(unlist(dimnames(ta)), format = "s",
                        width = width),
                collapse = "  ")

  Freq <- paste(formatC(ta, format = "f", digits = digits,
                        width = width),
                collapse = "  ")

  Pct <- paste(formatC(pct, format = "f", digits = digits,
                       width = width),
               "%", sep = "", collapse = " ")

  cat("Missing value(s) are", missing, ".\n")
  cat("Valid case(s) are", n - missing,".\n")
  cat("Total case(s) are", n, ".\n\n")

  cat("    ", Vals, "\n")
  cat("Freq", Freq, "\n")
  cat("Pct ", Pct, "\n")
  cat("\n")
}


Thus:


> freq(sample(letters[1:3], 48, TRUE), 2)
Missing value(s) are 0 .
Valid case(s) are 48 .
Total case(s) are 48 .

           a        b        c 
Freq   28.00     8.00    12.00 
Pct    58.33%   16.67%   25.00% 


> freq(sample(c(letters[1:3], NA), 1000, TRUE), 2)
Missing value(s) are 257 .
Valid case(s) are 743 .
Total case(s) are 1000 .

           a        b        c 
Freq  250.00   218.00   275.00 
Pct    33.65%   29.34%   37.01% 


> freq(iris$Species)
Missing value(s) are 0 .
Valid case(s) are 150 .
Total case(s) are 150 .

          setosa   versicolor    virginica 
Freq      50.000       50.000       50.000 
Pct       33.333%      33.333%      33.333% 


> freq(iris$Species, 0)
Missing value(s) are 0 .
Valid case(s) are 150 .
Total case(s) are 150 .

          setosa   versicolor    virginica 
Freq          50           50           50 
Pct           33%          33%          33% 


HTH,

Marc Schwartz



From HDoran at air.org  Fri Jul  1 19:43:56 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 1 Jul 2005 13:43:56 -0400
Subject: [R] Lines for plot (Sweave)
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409748844@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/35293ce7/attachment.pl

From ggrothendieck at gmail.com  Fri Jul  1 19:53:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Jul 2005 13:53:37 -0400
Subject: [R] Lines for plot (Sweave)
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409748844@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409748844@dc1ex2.air.org>
Message-ID: <971536df050701105331abfed2@mail.gmail.com>

On 7/1/05, Doran, Harold <HDoran at air.org> wrote:
> Dear List:
> 
> I am generating a series of plots iteratively using Sweave. In short, a
> dataframe is subsetted row by row and variable graphics are created
> conditional on the data in each row. In this particular case, this code
> ends up generating 17,000 individual plots.
> 
> In some cases, all student data (this is working with student
> achievement data) are available and my code below works very well in the
> sense that a line connects all points. However, in some cases there are
> missing data and I need to modify my code so that lines are connected
> through all points even when data are missing.
> 
> Here is a snip of relevant code. In the actual program, the data in
> stu.vector and avg.vector are obtained from the dataframe as the
> programs loops through each row.
> 
> stu.vector<-c(2500, 2510,   NA , 2600)
> avg.vector<-c(2635, 2589, 2628, 2685)
> x <- c(0,1,2,3)
> graph.min <- min(stu.vector,avg.vector ,na.rm=TRUE)-150
> graph.max <- max(stu.vector,avg.vector ,na.rm=TRUE)+150
> plot(x, stu.vector, ylim=c(graph.min,graph.max),  xlab=" ", ylab="Scaled
> Score", xaxt='n', pch=2, col='blue', main="Math Growth Rate")
> points(x, avg.vector, pch=1, col='red')
> lines(x, stu.vector, lty=1, col='blue')
> lines(x, avg.vector, lty=3, col='red')
> 
> If the NA did not exist in the object stu.vector then all points would
> be connected with lines. However, in some cases data are missing and I
> need to connect the data in stu.vector with lines. So in this case, the
> line would connect points 1 and 2, and then 2 and 4 even though point 3
> is missing.

Replace the first lines statement with:

lines(approx(x, stu.vector), lty=1, col='blue')



From shitao at hotmail.com  Fri Jul  1 20:00:43 2005
From: shitao at hotmail.com (Tao Shi)
Date: Fri, 01 Jul 2005 18:00:43 +0000
Subject: [R] zlim for levelplot
Message-ID: <BAY22-F362C62B814409CA49BD05FC7E20@phx.gbl>



From HDoran at air.org  Fri Jul  1 19:57:50 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 1 Jul 2005 13:57:50 -0400
Subject: [R] Lines for plot (Sweave)
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409748848@dc1ex2.air.org>

Fabulous, it works great. I didn't know about approx().

Thank you 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Friday, July 01, 2005 1:54 PM
To: Doran, Harold
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Lines for plot (Sweave)

On 7/1/05, Doran, Harold <HDoran at air.org> wrote:
> Dear List:
> 
> I am generating a series of plots iteratively using Sweave. In short, 
> a dataframe is subsetted row by row and variable graphics are created 
> conditional on the data in each row. In this particular case, this 
> code ends up generating 17,000 individual plots.
> 
> In some cases, all student data (this is working with student 
> achievement data) are available and my code below works very well in 
> the sense that a line connects all points. However, in some cases 
> there are missing data and I need to modify my code so that lines are 
> connected through all points even when data are missing.
> 
> Here is a snip of relevant code. In the actual program, the data in 
> stu.vector and avg.vector are obtained from the dataframe as the 
> programs loops through each row.
> 
> stu.vector<-c(2500, 2510,   NA , 2600)
> avg.vector<-c(2635, 2589, 2628, 2685)
> x <- c(0,1,2,3)
> graph.min <- min(stu.vector,avg.vector ,na.rm=TRUE)-150 graph.max <- 
> max(stu.vector,avg.vector ,na.rm=TRUE)+150 plot(x, stu.vector, 
> ylim=c(graph.min,graph.max),  xlab=" ", ylab="Scaled Score", xaxt='n',

> pch=2, col='blue', main="Math Growth Rate") points(x, avg.vector, 
> pch=1, col='red') lines(x, stu.vector, lty=1, col='blue') lines(x, 
> avg.vector, lty=3, col='red')
> 
> If the NA did not exist in the object stu.vector then all points would

> be connected with lines. However, in some cases data are missing and I

> need to connect the data in stu.vector with lines. So in this case, 
> the line would connect points 1 and 2, and then 2 and 4 even though 
> point 3 is missing.

Replace the first lines statement with:

lines(approx(x, stu.vector), lty=1, col='blue')



From khobson at fd9ns01.okladot.state.ok.us  Fri Jul  1 20:27:44 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 1 Jul 2005 13:27:44 -0500
Subject: [R] R integration with Microsoft Powerpoint
In-Reply-To: <OWA-1oySew3EqEDLOdY00004f0a@owa-1.sph.ad.jhsph.edu>
Message-ID: <OFCC2B49FF.F9C4EA9E-ON86257031.00646867-86257031.00654CFF@fd9ns01.okladot.state.ok.us>





Of course there are many ways to do it.

The user input could come from R dialogs via the tcltk package or the
Input() dialogs from VBA in Powerpoint.

I chose the output as PDF.  The R source code called cars.r, might go
something like:

pdf(file=paste(getwd(), "/", "cars.pdf", sep=""),
  width = 8.5, height = 11, onefile = TRUE, family = "Helvetica",
    title = "R Graphics Output", fonts = NULL, version = "1.1")
plot(cars)
lines(lowess(cars))
graphics.off()
shell(paste(getwd(), "/", "cars.pdf", sep=""),wait=FALSE) #veiw PDF
stop("all done")

The cars.bat file, might go something like:

"C:\Program Files\R\rw2010\bin\R.exe" CMD BATCH c:\myfiles\r\cars.r
#Change the drives and paths to R.exe and the cars.r files.

The cars.bat file was played from PowerPoint by creating the object and
doubleclicking in the slideshow.  The are other ways to do it of course.
In PowerPoint, click the menu item Insert | Object | Create and browse to
and select the cars.r file.  I set the object as an icon and used the R.exe
icon.

VBA scripting to play the cars.bat file is not all that involved either.
_______
mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From khobson at fd9ns01.okladot.state.ok.us  Fri Jul  1 21:11:31 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 1 Jul 2005 14:11:31 -0500
Subject: [R]  R integration with Microsoft Powerpoint
Message-ID: <OFE06ED26F.C049DB96-ON86257031.0068CE22-86257031.00694F32@fd9ns01.okladot.state.ok.us>





>...snip...In PowerPoint, click the menu item Insert | Object | Create and
browse to
and select the cars.r file. ...snip...
In the previous post snippet above, replace cars.r with cars.bat.

To run the cars.bat program via VBA, I would typically insert a button.  To
do so in PowerPoint, right click the toolbar, select Control Toolbar and
then click the button icon.  Right click and drag and draw the button onto
the slide.  Double click the button object and add code something like:

Private Sub CommandButton1_Click()
Shell ("c:\myfiles\r\cars.bat")
End Sub

When passing input to a program like R, I typically use VBA's Input() and
write the results to a TXT file.  This is then easily read into R.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From helprhelp at gmail.com  Fri Jul  1 21:22:35 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 1 Jul 2005 14:22:35 -0500
Subject: [R] lapply
Message-ID: <cdf8178305070112226267bef8@mail.gmail.com>

Hi, all:
I have a program here but it runs slow and I am wondering if there is
some place I can change to make it run faster.

Two lists, scd and c1, like this:
> scd[1:2]
[[1]]
[1] "54"  "241"

[[2]]
[1] "52" "53"
...
> c1[1:2]
[[1]]
 [1] "13"  "30"  "92"  "93"  "13"  "94"  "30"  "95"  "96"  "97"  "98"  "99"
[13] "8"   "19"  "31"  "100" "101" "29"

[[2]]
[1] "13" "55"

> length(scd)
[1] 2542
> length(c1)
[1] 31859

My target is 
for each in scd, I need to know how many times it (as the whole) occur in c1.

My code is
N <- length(scd) # num of word_comb
M <- length(c1) # num of class1
g1 <- lapply(1:N, function(i) lapply(1:M, function(j) all(scd[[i]]
%in% c1[[j]])))
a <- lapply(1:N, function(i) sum(g1[[i]]==T))

My questions:
1. g1's calc is very slow
2. how to do the following using apply:
tab <- array(as.integer(0), dim=c(2,2,N)
for (i in 1:N){
    tab[2,1,i] <- a[[i]]
}
tab[2,2,]=M-tab[2,1,]

Thanks,
-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From edd at debian.org  Fri Jul  1 21:19:45 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 1 Jul 2005 19:19:45 +0000 (UTC)
Subject: [R] OT: How to instaill gcc in cygwin?
References: <1115a2b0050701085226cb004b@mail.gmail.com>
Message-ID: <loom.20050701T211746-18@post.gmane.org>

Wensui Liu <liuwensui <at> gmail.com> writes:
> I know it is far off topic. But I do know there must be some people
> here who know it very well.

Click the 'install now' button at cygwin.org, and when the selection box 
appears in the install process, also select gcc.  There is a _lot_ of stuff
available for cygwin that the default install ignores.

That said, it won't help you for R as you cannot build R under Cygwin. The R
Extensions and R Admin manuals for details -- you'd want MinGW, a "cousin" of
Cygwin, on the PC.

Hth, Dirk



From macq at llnl.gov  Fri Jul  1 22:21:52 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 1 Jul 2005 13:21:52 -0700
Subject: [R] Lines for plot (Sweave)
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409748844@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409748844@dc1ex2.air.org>
Message-ID: <p06210211beeb52cb6dee@[128.115.153.6]>

You can use:

   lines(x[!is.na[stu.vector], stu.vector[!is.na(stu.vector)], lty=1, 
col='blue')

-Don

At 1:43 PM -0400 7/1/05, Doran, Harold wrote:
>Dear List:
>
>I am generating a series of plots iteratively using Sweave. In short, a
>dataframe is subsetted row by row and variable graphics are created
>conditional on the data in each row. In this particular case, this code
>ends up generating 17,000 individual plots.
>
>In some cases, all student data (this is working with student
>achievement data) are available and my code below works very well in the
>sense that a line connects all points. However, in some cases there are
>missing data and I need to modify my code so that lines are connected
>through all points even when data are missing.
>
>Here is a snip of relevant code. In the actual program, the data in
>stu.vector and avg.vector are obtained from the dataframe as the
>programs loops through each row.
>
>stu.vector<-c(2500, 2510,   NA , 2600)
>avg.vector<-c(2635, 2589, 2628, 2685)
>x <- c(0,1,2,3)
>graph.min <- min(stu.vector,avg.vector ,na.rm=TRUE)-150
>graph.max <- max(stu.vector,avg.vector ,na.rm=TRUE)+150
>plot(x, stu.vector, ylim=c(graph.min,graph.max),  xlab=" ", ylab="Scaled
>Score", xaxt='n', pch=2, col='blue', main="Math Growth Rate")
>points(x, avg.vector, pch=1, col='red')
>lines(x, stu.vector, lty=1, col='blue')
>lines(x, avg.vector, lty=3, col='red')
>
>If the NA did not exist in the object stu.vector then all points would
>be connected with lines. However, in some cases data are missing and I
>need to connect the data in stu.vector with lines. So in this case, the
>line would connect points 1 and 2, and then 2 and 4 even though point 3
>is missing.
>
>Can anyone suggest how I might do this?
>
>Thanks,
>Harold
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From malfonso at sinu.unicordoba.edu.co  Fri Jul  1 22:33:12 2005
From: malfonso at sinu.unicordoba.edu.co (Mario Alfonso Morales Rivera)
Date: Fri, 1 Jul 2005 15:33:12 -0500 (COT)
Subject: [R] Setting lattice boxplot's lines to black
Message-ID: <1091.172.16.1.31.1120249992.squirrel@sinu.unicordoba.edu.co>

Hi R users.

I'm using the lattice library and I need a print version for my graphics.

How Set I my boxplot's lines to black ????


tpl<-trellis.par.get("plot.line")

tpl$col<-"black"

trellis.par.set("plot.line", tpl)

Don't work. Boxplot's lines aren't black.


Thanks a lot.


This is my script.

library(lattice)

# I set background's color to white

tbg<-trellis.par.get("background")

tbg$col<-"white"

trellis.par.set("background", tbg)

# Set strip background's color to white

tsbg<-trellis.par.get("strip.background") tsbg$col<-"white"

trellis.par.set("strip.background", tsbg)

# I set symbol's color to black

tps<-trellis.par.get("plot.symbol")

tps$col<-"black"

trellis.par.set("plot.symbol", tps)


# Set line's color to black

tpl<-trellis.par.get("plot.line")

tpl$col<-"black"

trellis.par.set("plot.line", tpl)

print(bwplot(Sepal.Length~Species ,data=iris))


# This work whit xyplot but don't work whit bwplot, the boxplot's
# lines aren't black.


How Set I my boxplot's lines to black ????

Thanks a lot.




-- 
Mario Alfonso Morales Rivera
Profesor Auxiliar.
Departamento de Matem??ticas y Estadistica.
Universidad de C??doba.
Colombia



From menghui at gmail.com  Fri Jul  1 22:48:57 2005
From: menghui at gmail.com (Menghui Chen)
Date: Fri, 1 Jul 2005 15:48:57 -0500
Subject: [R] Generating correlated data from uniform distribution
Message-ID: <7d30e3c050701134826847db8@mail.gmail.com>

Dear R users,

I want to generate two random variables (X1, X2) from uniform
distribution (-0.5, 0.5) with a specified correlation coefficient r.
Does anyone know how to do it in R?

Many thanks!

Menghui



From deepayan.sarkar at gmail.com  Fri Jul  1 23:37:38 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 1 Jul 2005 16:37:38 -0500
Subject: [R] Setting lattice boxplot's lines to black
In-Reply-To: <1091.172.16.1.31.1120249992.squirrel@sinu.unicordoba.edu.co>
References: <1091.172.16.1.31.1120249992.squirrel@sinu.unicordoba.edu.co>
Message-ID: <eb555e66050701143725f17c71@mail.gmail.com>

On 7/1/05, Mario Alfonso Morales Rivera <malfonso at sinu.unicordoba.edu.co> wrote:
> Hi R users.
> 
> I'm using the lattice library and I need a print version for my graphics.
> 
> How Set I my boxplot's lines to black ????

(As I said in a private reply,) you seem to want a black and white plot, so use

trellis.device(color = FALSE)

to initialize the device.

Deepayan



From jfbrennan at rogers.com  Fri Jul  1 23:46:25 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 1 Jul 2005 17:46:25 -0400
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <7d30e3c050701134826847db8@mail.gmail.com>
Message-ID: <200507012147.j61LkOLH022709@hypatia.math.ethz.ch>

> dat<-matrix(runif(2000),2,1000)
> rho<-.77
> R<-matrix(c(1,rho,rho,1),2,2)
> dat2<-t(ch)%*%dat
> cor(dat2[1,],dat2[2,])
[1] 0.7513892
> dat<-matrix(runif(20000),2,10000)
> rho<-.28
> R<-matrix(c(1,rho,rho,1),2,2)
> ch<-chol(R)
> dat2<-t(ch)%*%dat
> cor(dat2[1,],dat2[2,])
[1] 0.2681669
> dat<-matrix(runif(200000),2,100000)
> rho<-.28
> R<-matrix(c(1,rho,rho,1),2,2)
> ch<-chol(R)
> dat2<-t(ch)%*%dat
> cor(dat2[1,],dat2[2,])
[1] 0.2814035
>
See  ?choleski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Menghui Chen
Sent: July 1, 2005 4:49 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Generating correlated data from uniform distribution

Dear R users,

I want to generate two random variables (X1, X2) from uniform
distribution (-0.5, 0.5) with a specified correlation coefficient r.
Does anyone know how to do it in R?

Many thanks!

Menghui

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tplate at acm.org  Sat Jul  2 00:00:51 2005
From: tplate at acm.org (Tony Plate)
Date: Fri, 01 Jul 2005 16:00:51 -0600
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <200507012147.j61LkOLH022709@hypatia.math.ethz.ch>
References: <200507012147.j61LkOLH022709@hypatia.math.ethz.ch>
Message-ID: <42C5BD13.3050104@acm.org>

Isn't this a little trickier with non-normal variables?  It sounds like 
Menghui Chen wants variables that have uniform marginal distribution, 
and a specified correlation.

When I look at histograms (or just the quantiles) of the rows of dat2 in 
your example, I see something for dat2[2,] that does not look much like 
it comes from a uniform distribution.

 > dat<-matrix(runif(2000),2,1000)
 > rho<-.77
 > R<-matrix(c(1,rho,rho,1),2,2)
 > ch<-chol(R)
 > dat2<-t(ch)%*%dat
 > cor(dat2[1,],dat2[2,])
[1] 0.7513892
 > hist(dat2[1,])
 > hist(dat2[2,])
 >
 > quantile(dat2[1,])
          0%         25%         50%         75%        100%
0.000655829 0.246216035 0.507075912 0.745158441 0.999916418
 > quantile(dat2[2,])
        0%       25%       50%       75%      100%
0.0393046 0.4980066 0.7150426 0.9208855 1.3864704
 >

-- Tony Plate

Jim Brennan wrote:
> dat<-matrix(runif(2000),2,1000)
> rho<-.77
> R<-matrix(c(1,rho,rho,1),2,2)
> ch<-chol(R)
> dat2<-t(ch)%*%dat
> cor(dat2[1,],dat2[2,])
[1] 0.7513892
> 
>>dat<-matrix(runif(20000),2,10000)
>>rho<-.28
>>R<-matrix(c(1,rho,rho,1),2,2)
>>ch<-chol(R)
>>dat2<-t(ch)%*%dat
>>cor(dat2[1,],dat2[2,])
> 
> [1] 0.2681669
> 
>>dat<-matrix(runif(200000),2,100000)
>>rho<-.28
>>R<-matrix(c(1,rho,rho,1),2,2)
>>ch<-chol(R)
>>dat2<-t(ch)%*%dat
>>cor(dat2[1,],dat2[2,])
> 
> [1] 0.2814035
> 
> See  ?choleski
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Menghui Chen
> Sent: July 1, 2005 4:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Generating correlated data from uniform distribution
> 
> Dear R users,
> 
> I want to generate two random variables (X1, X2) from uniform
> distribution (-0.5, 0.5) with a specified correlation coefficient r.
> Does anyone know how to do it in R?
> 
> Many thanks!
> 
> Menghui
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Jul  2 00:26:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 01 Jul 2005 15:26:02 -0700
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <42C5BD13.3050104@acm.org>
References: <200507012147.j61LkOLH022709@hypatia.math.ethz.ch>
	<42C5BD13.3050104@acm.org>
Message-ID: <42C5C2FA.7020101@pdf.com>

	  How about tetrachoric correlations?  Generate correlated normal 
observations, then convert to uniform using pnorm:

rho <- 0.9
Cor <- array(c(1, rho, rho, 1), dim=c(2,2))

library(mvtnorm)

set.seed(1)
Y <- rmvnorm(10000, sigma=Cor)

X <- pnorm(Y)-0.5
plot(X)
hist(X[,1])
hist(X[,2])
cor(X)

	  Enjoy.
	  spencer graves

Tony Plate wrote:

> Isn't this a little trickier with non-normal variables?  It sounds like 
> Menghui Chen wants variables that have uniform marginal distribution, 
> and a specified correlation.
> 
> When I look at histograms (or just the quantiles) of the rows of dat2 in 
> your example, I see something for dat2[2,] that does not look much like 
> it comes from a uniform distribution.
> 
>  > dat<-matrix(runif(2000),2,1000)
>  > rho<-.77
>  > R<-matrix(c(1,rho,rho,1),2,2)
>  > ch<-chol(R)
>  > dat2<-t(ch)%*%dat
>  > cor(dat2[1,],dat2[2,])
> [1] 0.7513892
>  > hist(dat2[1,])
>  > hist(dat2[2,])
>  >
>  > quantile(dat2[1,])
>           0%         25%         50%         75%        100%
> 0.000655829 0.246216035 0.507075912 0.745158441 0.999916418
>  > quantile(dat2[2,])
>         0%       25%       50%       75%      100%
> 0.0393046 0.4980066 0.7150426 0.9208855 1.3864704
>  >
> 
> -- Tony Plate
> 
> Jim Brennan wrote:
> 
>>dat<-matrix(runif(2000),2,1000)
>>rho<-.77
>>R<-matrix(c(1,rho,rho,1),2,2)
>>ch<-chol(R)
>>dat2<-t(ch)%*%dat
>>cor(dat2[1,],dat2[2,])
> 
> [1] 0.7513892
> 
>>>dat<-matrix(runif(20000),2,10000)
>>>rho<-.28
>>>R<-matrix(c(1,rho,rho,1),2,2)
>>>ch<-chol(R)
>>>dat2<-t(ch)%*%dat
>>>cor(dat2[1,],dat2[2,])
>>
>>[1] 0.2681669
>>
>>
>>>dat<-matrix(runif(200000),2,100000)
>>>rho<-.28
>>>R<-matrix(c(1,rho,rho,1),2,2)
>>>ch<-chol(R)
>>>dat2<-t(ch)%*%dat
>>>cor(dat2[1,],dat2[2,])
>>
>>[1] 0.2814035
>>
>>See  ?choleski
>>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Menghui Chen
>>Sent: July 1, 2005 4:49 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Generating correlated data from uniform distribution
>>
>>Dear R users,
>>
>>I want to generate two random variables (X1, X2) from uniform
>>distribution (-0.5, 0.5) with a specified correlation coefficient r.
>>Does anyone know how to do it in R?
>>
>>Many thanks!
>>
>>Menghui
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jfbrennan at rogers.com  Sat Jul  2 00:32:59 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 1 Jul 2005 18:32:59 -0400
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <42C5BD13.3050104@acm.org>
Message-ID: <200507012233.j61MWxiu020026@hypatia.math.ethz.ch>

Yes you are right I guess this works only for normal data. Free advice
sometimes comes with too little consideration :-)
Sorry about that and thanks to Spencer for the correct way.
-----Original Message-----
From: Tony Plate [mailto:tplate at acm.org] 
Sent: July 1, 2005 6:01 PM
To: Jim Brennan
Cc: 'Menghui Chen'; r-help at stat.math.ethz.ch
Subject: Re: [R] Generating correlated data from uniform distribution

Isn't this a little trickier with non-normal variables?  It sounds like 
Menghui Chen wants variables that have uniform marginal distribution, 
and a specified correlation.

When I look at histograms (or just the quantiles) of the rows of dat2 in 
your example, I see something for dat2[2,] that does not look much like 
it comes from a uniform distribution.

 > dat<-matrix(runif(2000),2,1000)
 > rho<-.77
 > R<-matrix(c(1,rho,rho,1),2,2)
 > ch<-chol(R)
 > dat2<-t(ch)%*%dat
 > cor(dat2[1,],dat2[2,])
[1] 0.7513892
 > hist(dat2[1,])
 > hist(dat2[2,])
 >
 > quantile(dat2[1,])
          0%         25%         50%         75%        100%
0.000655829 0.246216035 0.507075912 0.745158441 0.999916418
 > quantile(dat2[2,])
        0%       25%       50%       75%      100%
0.0393046 0.4980066 0.7150426 0.9208855 1.3864704
 >

-- Tony Plate

Jim Brennan wrote:
> dat<-matrix(runif(2000),2,1000)
> rho<-.77
> R<-matrix(c(1,rho,rho,1),2,2)
> ch<-chol(R)
> dat2<-t(ch)%*%dat
> cor(dat2[1,],dat2[2,])
[1] 0.7513892
> 
>>dat<-matrix(runif(20000),2,10000)
>>rho<-.28
>>R<-matrix(c(1,rho,rho,1),2,2)
>>ch<-chol(R)
>>dat2<-t(ch)%*%dat
>>cor(dat2[1,],dat2[2,])
> 
> [1] 0.2681669
> 
>>dat<-matrix(runif(200000),2,100000)
>>rho<-.28
>>R<-matrix(c(1,rho,rho,1),2,2)
>>ch<-chol(R)
>>dat2<-t(ch)%*%dat
>>cor(dat2[1,],dat2[2,])
> 
> [1] 0.2814035
> 
> See  ?choleski
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Menghui Chen
> Sent: July 1, 2005 4:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Generating correlated data from uniform distribution
> 
> Dear R users,
> 
> I want to generate two random variables (X1, X2) from uniform
> distribution (-0.5, 0.5) with a specified correlation coefficient r.
> Does anyone know how to do it in R?
> 
> Many thanks!
> 
> Menghui
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Sat Jul  2 00:59:13 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jul 2005 00:59:13 +0200
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <200507012233.j61MWxiu020026@hypatia.math.ethz.ch>
References: <200507012233.j61MWxiu020026@hypatia.math.ethz.ch>
Message-ID: <x27jga16bi.fsf@turmalin.kubism.ku.dk>

"Jim Brennan" <jfbrennan at rogers.com> writes:

> Yes you are right I guess this works only for normal data. Free advice
> sometimes comes with too little consideration :-)

Worth every cent...

> Sorry about that and thanks to Spencer for the correct way.

Hmm, but is it? Or rather, what is the relation between the
correlation of the normals  and that of the transformed variables? 
Looks nontrivial to me.

Incidentally, here's a way that satisfies the criteria, but in a
rather weird way:

N <- 10000
rho <- .6
x <- runif(N, -.5,.5)
y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From iidn01 at yahoo.com  Sat Jul  2 01:22:18 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Fri, 1 Jul 2005 16:22:18 -0700 (PDT)
Subject: [R] scope argument in step function
Message-ID: <20050701232218.13539.qmail@web31104.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/3a23407d/attachment.pl

From jfbrennan at rogers.com  Sat Jul  2 01:25:37 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 1 Jul 2005 19:25:37 -0400
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <x27jga16bi.fsf@turmalin.kubism.ku.dk>
Message-ID: <200507012325.j61NPbHi030545@hypatia.math.ethz.ch>

OK now I am skeptical especially when you say in a weird way:-)
This may be OK but look at plot(x,y) and I am suspicious. Is it still
alright with this kind of relationship?

For large N it appears Spencer's method is returning slightly lower
correlation for the uniforms as compared to the normals so maybe there is a
problem!?!

Hope we are all learning something and Menghui gets/has what he wants . :-)

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: July 1, 2005 6:59 PM
To: Jim Brennan
Cc: 'Tony Plate'; 'Menghui Chen'; r-help at stat.math.ethz.ch
Subject: Re: [R] Generating correlated data from uniform distribution

"Jim Brennan" <jfbrennan at rogers.com> writes:

> Yes you are right I guess this works only for normal data. Free advice
> sometimes comes with too little consideration :-)

Worth every cent...

> Sorry about that and thanks to Spencer for the correct way.

Hmm, but is it? Or rather, what is the relation between the
correlation of the normals  and that of the transformed variables? 
Looks nontrivial to me.

Incidentally, here's a way that satisfies the criteria, but in a
rather weird way:

N <- 10000
rho <- .6
x <- runif(N, -.5,.5)
y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From spencer.graves at pdf.com  Sat Jul  2 01:43:11 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 01 Jul 2005 16:43:11 -0700
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <x27jga16bi.fsf@turmalin.kubism.ku.dk>
References: <200507012233.j61MWxiu020026@hypatia.math.ethz.ch>
	<x27jga16bi.fsf@turmalin.kubism.ku.dk>
Message-ID: <42C5D50F.5030007@pdf.com>

	  Peter is absolutely correct:  The "correlation" I used was for a 
hidden normal process, not for the resultant correlated uniforms.  This 
is similar to but different from "tetrachoric corrrelations", about 
which there is a substantial literature (including an R package 
"polycor").

	  Why do you want correlated uniforms?  What do they represent 
physically?  Does it matter if you can match exactly a particular 
correlation coefficient, or is it enough to say that they are uniformily 
distributed random variables such that their normal scores have a 
specified correlation coefficient?  There is so much known about the 
multivariate normal distribution and so little about correlated uniforms 
that it might be more useful to know the correlations of latent normals, 
for which your uniforms are what are measured.

	  spencer graves 	

Peter Dalgaard wrote:

> "Jim Brennan" <jfbrennan at rogers.com> writes:
> 
> 
>>Yes you are right I guess this works only for normal data. Free advice
>>sometimes comes with too little consideration :-)
> 
> 
> Worth every cent...
> 
> 
>>Sorry about that and thanks to Spencer for the correct way.
> 
> 
> Hmm, but is it? Or rather, what is the relation between the
> correlation of the normals  and that of the transformed variables? 
> Looks nontrivial to me.
> 
> Incidentally, here's a way that satisfies the criteria, but in a
> rather weird way:
> 
> N <- 10000
> rho <- .6
> x <- runif(N, -.5,.5)
> y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From guerinche at gmail.com  Sat Jul  2 02:11:41 2005
From: guerinche at gmail.com (alejandro munoz)
Date: Fri, 1 Jul 2005 19:11:41 -0500
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <7d30e3c050701134826847db8@mail.gmail.com>
References: <7d30e3c050701134826847db8@mail.gmail.com>
Message-ID: <98c62e110507011711227c1d19@mail.gmail.com>

Dear Menghui,

You may consider looking in Luc Devroye's "Non-uniform Random Number
Generation". Despite its title, section XI.3.2 describes how to
generate bivariate uniforms. The book is out of print but Devroye
himself urges you to print it from his scanned PDFs(!):

http://cgm.cs.mcgill.ca/~luc/rnbookindex.html

Hope this helps,

alejandro

On 7/1/05, Menghui Chen <menghui at gmail.com> wrote:
> Dear R users,
> 
> I want to generate two random variables (X1, X2) from uniform
> distribution (-0.5, 0.5) with a specified correlation coefficient r.
> Does anyone know how to do it in R?
> 
> Many thanks!
> 
> Menghui
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From uofiowa at gmail.com  Sat Jul  2 02:24:16 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 1 Jul 2005 20:24:16 -0400
Subject: [R] as.Date < today ?
Message-ID: <3f87cc6d05070117247ab4191f@mail.gmail.com>

I have a Date variable that I constructed with as.Date()
How ca I compare it to today (<,>,==)   ?



From spencer.graves at pdf.com  Sat Jul  2 03:28:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 01 Jul 2005 18:28:47 -0700
Subject: [R] as.Date < today ?
In-Reply-To: <3f87cc6d05070117247ab4191f@mail.gmail.com>
References: <3f87cc6d05070117247ab4191f@mail.gmail.com>
Message-ID: <42C5EDCF.4060108@pdf.com>

	  The following is a minor modification of examples in the help for 
"as.Date":

 > x <- c("1jan1960", "2jan1960", "31mar1960", "30jul2006")
 > z <- as.Date(x, "%d%b%Y")
 > z< Sys.Date()
[1]  TRUE  TRUE  TRUE FALSE

	  How's this?
	  spencer graves

Omar Lakkis wrote:

> I have a Date variable that I constructed with as.Date()
> How ca I compare it to today (<,>,==)   ?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kerryrekky at yahoo.com  Sat Jul  2 04:43:34 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Fri, 1 Jul 2005 19:43:34 -0700 (PDT)
Subject: [R] Is it possible to use glm() with 30 observations?
Message-ID: <20050702024334.6656.qmail@web51807.mail.yahoo.com>

I have a very simple problem. When using glm to fit
binary logistic regression model, sometimes I receive
the following warning:

Warning messages:
1: fitted probabilities numerically 0 or 1 occurred
in: glm.fit(x = X, y = Y, weights = weights, start =
start, etastart = etastart,  
2: fitted probabilities numerically 0 or 1 occurred
in: glm.fit(x = X, y = Y, weights = weights, start =
start, etastart = etastart,  

What does this output tell me? Since I only have 30
observations, i assume this is a small sample problem.
Is it possible to fit this model in R with only 30
observations? Could any expert provide suggestions to
avoid the warning?



From ggrothendieck at gmail.com  Sat Jul  2 05:46:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Jul 2005 23:46:30 -0400
Subject: [R] Lines for plot (Sweave)
In-Reply-To: <p06210211beeb52cb6dee@128.115.153.6>
References: <88EAF3512A55DF46B06B1954AEF73F7409748844@dc1ex2.air.org>
	<p06210211beeb52cb6dee@128.115.153.6>
Message-ID: <971536df0507012046393f9f5c@mail.gmail.com>

A variation on your idea might be:

fo <- stu.vector ~ x
lines(fo, model.frame(fo), lty=1, col='blue')


On 7/1/05, Don MacQueen <macq at llnl.gov> wrote:
> You can use:
> 
>   lines(x[!is.na[stu.vector], stu.vector[!is.na(stu.vector)], lty=1,
> col='blue')
> 
> -Don
> 
> At 1:43 PM -0400 7/1/05, Doran, Harold wrote:
> >Dear List:
> >
> >I am generating a series of plots iteratively using Sweave. In short, a
> >dataframe is subsetted row by row and variable graphics are created
> >conditional on the data in each row. In this particular case, this code
> >ends up generating 17,000 individual plots.
> >
> >In some cases, all student data (this is working with student
> >achievement data) are available and my code below works very well in the
> >sense that a line connects all points. However, in some cases there are
> >missing data and I need to modify my code so that lines are connected
> >through all points even when data are missing.
> >
> >Here is a snip of relevant code. In the actual program, the data in
> >stu.vector and avg.vector are obtained from the dataframe as the
> >programs loops through each row.
> >
> >stu.vector<-c(2500, 2510,   NA , 2600)
> >avg.vector<-c(2635, 2589, 2628, 2685)
> >x <- c(0,1,2,3)
> >graph.min <- min(stu.vector,avg.vector ,na.rm=TRUE)-150
> >graph.max <- max(stu.vector,avg.vector ,na.rm=TRUE)+150
> >plot(x, stu.vector, ylim=c(graph.min,graph.max),  xlab=" ", ylab="Scaled
> >Score", xaxt='n', pch=2, col='blue', main="Math Growth Rate")
> >points(x, avg.vector, pch=1, col='red')
> >lines(x, stu.vector, lty=1, col='blue')
> >lines(x, avg.vector, lty=3, col='red')
> >
> >If the NA did not exist in the object stu.vector then all points would
> >be connected with lines. However, in some cases data are missing and I
> >need to connect the data in stu.vector with lines. So in this case, the
> >line would connect points 1 and 2, and then 2 and 4 even though point 3
> >is missing.
> >
> >Can anyone suggest how I might do this?
> >
> >Thanks,
> >Harold
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> --
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Jul  2 07:01:12 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 01 Jul 2005 22:01:12 -0700
Subject: [R] Is it possible to use glm() with 30 observations?
In-Reply-To: <20050702024334.6656.qmail@web51807.mail.yahoo.com>
References: <20050702024334.6656.qmail@web51807.mail.yahoo.com>
Message-ID: <42C61F98.2090806@pdf.com>

	  The issue is not 30 observations but whether it is possible to 
perfectly separate the two possible outcomes.  Consider the following:

tst.glm <- data.frame(x=1:3, y=c(0, 1, 0))
glm(y~x, family=binomial, data=tst.glm)

tst2.glm <- data.frame(x=1:1000,
                      y=rep(0:1, each=500))
glm(y~x, family=binomial, data=tst2.glm)

	  The algorithm fits y~x to tst.glm without complaining for tst.glm, 
but issues warnings for tst2.glm.  This is called the Hauck-Donner 
effect, and RSiteSearch("Hauck-Donner") just now produced 8 hits.  For 
more information, look for "Hauck-Donnner" in the index of Venables, W. 
N. and Ripley, B. D. (2002) _Modern Applied Statistics with S._ New 
York: Springer.  (If you don't already have this book, I recommend you 
give serious consideration to purchasing a copy.  It is excellent on 
many issues relating to statistical analysis and R.

	  Spencer Graves

Kerry Bush wrote:

> I have a very simple problem. When using glm to fit
> binary logistic regression model, sometimes I receive
> the following warning:
> 
> Warning messages:
> 1: fitted probabilities numerically 0 or 1 occurred
> in: glm.fit(x = X, y = Y, weights = weights, start =
> start, etastart = etastart,  
> 2: fitted probabilities numerically 0 or 1 occurred
> in: glm.fit(x = X, y = Y, weights = weights, start =
> start, etastart = etastart,  
> 
> What does this output tell me? Since I only have 30
> observations, i assume this is a small sample problem.
> Is it possible to fit this model in R with only 30
> observations? Could any expert provide suggestions to
> avoid the warning?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From admin at perfectharmony-ms.org  Sat Jul  2 08:03:52 2005
From: admin at perfectharmony-ms.org (Perfect Harmony)
Date: Sat, 2 Jul 2005 01:03:52 -0500
Subject: [R] Can you help?
Message-ID: <200507020556.j625uPa2000748@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050702/433dffc2/attachment.pl

From ripley at stats.ox.ac.uk  Sat Jul  2 08:41:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Jul 2005 07:41:44 +0100 (BST)
Subject: [R] scope argument in step function
In-Reply-To: <20050701232218.13539.qmail@web31104.mail.mud.yahoo.com>
References: <20050701232218.13539.qmail@web31104.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0507020733140.19558@gannet.stats>

>  What I wondered is if I pass the 'upper=~.' ,
> it seems step() thinks the full model is current one. Not adding
> anymore. If this is the right answer, is there a better way than
> creating fmla argument in the above?

Yes, that is exactly what the help page for step says it means. (So why 
are you unsure and asking?)

upper = terms(Response ~ ., data=ds3))

would appear to be a simpler way to get your intention.


On Fri, 1 Jul 2005, Young Cho wrote:

> Thanks a lot for help in advance. I am switching from matlab to R and I 
> guess I need some time to get rolling. I was wondering why this code :
>
>> fit.0 <- lm( Response ~ 1, data = ds3)
>> step(fit.0,scope=list(upper=~.,lower=~1),data=ds3)
> Start:  AIC= -32.66
> Response ~ 1
>
> Call:
> lm(formula = Response ~ 1, data = ds3)
> Coefficients:
> (Intercept)
>      1.301
>
>
> is not working different from the following:
>
>>
>> cnames <- dimnames(ds3)[[2]]
>> cnames <- cnames[-444]        # last col is Response
>>
>> fmla <- as.formula(paste(" ~ ",paste(cnames,collapse="+")))
>> step(fit.0,scope=list(upper=fmla,lower=~1),data=ds3)
> Start:  AIC= -32.66
> Response ~ 1
>> fmla <- as.formula(paste(" ~ ",paste(cnames,collapse="+")))
>> fit.s <- step(fit.0,scope=list(upper=fmla,lower=~1),data=ds3)
>
> Step:  AIC= -Inf
> Response ~ ENTP9324 + CH1W0281
>           Df Sum of Sq     RSS  AIC
> <none>                        0 -Inf
> - CH1W0281  3   0.00381 0.00381 -115
> - ENTP9324  9         1       1  -34
>
> The dataframe ds3 is 17 by 444 and I understand it is not smart thing to 
> run stepwise regression. What I wondered is if I pass the 'upper=~.' , 
> it seems step() thinks the full model is current one. Not adding 
> anymore. If this is the right answer, is there a better way than 
> creating fmla argument in the above?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From knoblauch at lyon.inserm.fr  Sat Jul  2 09:06:48 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sat,  2 Jul 2005 09:06:48 +0200
Subject: [R]  Generating correlated data from uniform distribution
Message-ID: <1120288008.42c63d08686ae@webmail.lyon.inserm.fr>

While you are looking at weird distributions, here is one that 
 we have used in experiments on noise masking to explore the 
 bandwidth of visual mechanisms 

D'Zmura, M., & Knoblauch, K. (1998). Spectral bandwidths for the detection of 
color. 
Vision Research, 20, 3117-28 and
G. Monaci, G. Menegaz, S. Susstrunk and K. Knoblauch Chromatic Contrast 
Detection in Spatial 
Chromatic Noise Visual Neuroscience, Vol. 21, No 3, pp. 291-294, 2004

N <- 10000
x <- runif(N, -.5,.5)
y <- runif(N, -abs(x), abs(x))
plot(x,y)

y is not uniform but it is conditional on x.  The plot reveals
why we called this "sectored noise".

HTH

ken

--------------------------------------------------------
"Jim Brennan" <jfbrennan at rogers.com> writes:

> Yes you are right I guess this works only for normal data. Free advice
> sometimes comes with too little consideration :-)

Worth every cent...

> Sorry about that and thanks to Spencer for the correct way.

Hmm, but is it? Or rather, what is the relation between the
correlation of the normals  and that of the transformed variables? 
Looks nontrivial to me.

Incidentally, here's a way that satisfies the criteria, but in a
rather weird way:

N <- 10000
rho <- .6
x <- runif(N, -.5,.5)
y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
(+45) 35327907

____________________
Ken Knoblauch
Inserm U371, Cerveau et Vision
Department of Cognitive Neurosciences
18 avenue du Doyen Lepine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10
http://www.lyon.inserm.fr/371/



From Nongluck.Klibbua at newcastle.edu.au  Sat Jul  2 10:14:15 2005
From: Nongluck.Klibbua at newcastle.edu.au (Nongluck Klibbua)
Date: Sat, 02 Jul 2005 18:14:15 +1000
Subject: [R] how to call sas in R
Message-ID: <s2c6d984.094@MC-GWDOM2.newcastle.edu.au>

Hello all,
I would like to know how to call sas code in R. Since I simulate data in
R and I need to use sas code (garch-t,egarch and gjr) to estimate it. I
need to simulate 500 times with 2000 obs. How I can call that code in
R.Also, how I can keep the parameters from the estimate.

j=1:500
i=1:2000
sas code
keep parameters.

Best Appreciate,
Luck



From p.dalgaard at biostat.ku.dk  Sat Jul  2 10:52:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jul 2005 10:52:33 +0200
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <20050701232543.2C829A1E3@slim.kubism.ku.dk>
References: <20050701232543.2C829A1E3@slim.kubism.ku.dk>
Message-ID: <x24qbdr3n2.fsf@turmalin.kubism.ku.dk>

"Jim Brennan" <jfbrennan at rogers.com> writes:

> OK now I am skeptical especially when you say in a weird way:-)
> This may be OK but look at plot(x,y) and I am suspicious. Is it still
> alright with this kind of relationship?
...
> N <- 10000
> rho <- .6
> x <- runif(N, -.5,.5)
> y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))

Well, the covariance is (everything has mean zero, of course)

E(XY) = (1+rho)/2*EX^2 + (1-rho)/2*E(X*-X) = rho*EX^2 

The marginal distribution of Y is a mixture of two identical uniforms
(X and -X) so is uniform and in particular has the same variance as X.

In summary,  EXY/sqrt(EX^2EY^2) == rho

So as I said, it satisfies the formal requirements. X and Y are
uniformly distributed and their correlation is rho. 

If for nothing else, I suppose that this example is good for
demonstrating that independence and uncorrelatedness is not the same
thing. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From d.firth at warwick.ac.uk  Sat Jul  2 11:01:06 2005
From: d.firth at warwick.ac.uk (David Firth)
Date: Sat, 2 Jul 2005 10:01:06 +0100
Subject: [R] Is it possible to use glm() with 30 observations?
In-Reply-To: <42C61F98.2090806@pdf.com>
References: <20050702024334.6656.qmail@web51807.mail.yahoo.com>
	<42C61F98.2090806@pdf.com>
Message-ID: <405bbfcb6d02ed3b29775314c3577eb2@warwick.ac.uk>

On 2 Jul 2005, at 06:01, Spencer Graves wrote:

> 	  The issue is not 30 observations but whether it is possible to
> perfectly separate the two possible outcomes.  Consider the following:
>
> tst.glm <- data.frame(x=1:3, y=c(0, 1, 0))
> glm(y~x, family=binomial, data=tst.glm)
>
> tst2.glm <- data.frame(x=1:1000,
>                       y=rep(0:1, each=500))
> glm(y~x, family=binomial, data=tst2.glm)
>
> 	  The algorithm fits y~x to tst.glm without complaining for tst.glm,
> but issues warnings for tst2.glm.  This is called the Hauck-Donner
> effect, and RSiteSearch("Hauck-Donner") just now produced 8 hits.  For
> more information, look for "Hauck-Donnner" in the index of Venables, W.
> N. and Ripley, B. D. (2002) _Modern Applied Statistics with S._ New
> York: Springer.

Not exactly.  The phenomenon that causes the warning for tst2.glm above 
is more commonly known as "complete separation".  For some comments on 
its implications you might look at another work by B D Ripley, the 1996 
book "Pattern Recognition and Neural Networks".  There are some further 
references in the help files of the "brlr" package on CRAN.

The problem noted by Hauck and Donner (1997, JASA) is slightly related, 
but not the same.  See the aforementioned book by Venables and Ripley, 
for example.  The glm function does not routinely warn us about the 
"Hauck-Donner effect", afaik.

The original poster did not say what was the purpose of the logistic 
regression was, so it is hard to advise.  Depending on the purpose, the 
separation that was detected may or may not be a problem.

Regards,
David

> (If you don't already have this book, I recommend you
> give serious consideration to purchasing a copy.  It is excellent on
> many issues relating to statistical analysis and R.
>
> 	  Spencer Graves
>
> Kerry Bush wrote:
>
>> I have a very simple problem. When using glm to fit
>> binary logistic regression model, sometimes I receive
>> the following warning:
>>
>> Warning messages:
>> 1: fitted probabilities numerically 0 or 1 occurred
>> in: glm.fit(x = X, y = Y, weights = weights, start =
>> start, etastart = etastart,
>> 2: fitted probabilities numerically 0 or 1 occurred
>> in: glm.fit(x = X, y = Y, weights = weights, start =
>> start, etastart = etastart,
>>
>> What does this output tell me? Since I only have 30
>> observations, i assume this is a small sample problem.
>> Is it possible to fit this model in R with only 30
>> observations? Could any expert provide suggestions to
>> avoid the warning?
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul  2 12:00:56 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 02 Jul 2005 11:00:56 +0100 (BST)
Subject: [R] Is it possible to use glm() with 30 observations?
In-Reply-To: <20050702024334.6656.qmail@web51807.mail.yahoo.com>
Message-ID: <XFMail.050702110018.Ted.Harding@nessie.mcc.ac.uk>

On 02-Jul-05 Kerry Bush wrote:
> I have a very simple problem. When using glm to fit
> binary logistic regression model, sometimes I receive
> the following warning:
> 
> Warning messages:
> 1: fitted probabilities numerically 0 or 1 occurred
> in: glm.fit(x = X, y = Y, weights = weights, start =
> start, etastart = etastart,  
> 2: fitted probabilities numerically 0 or 1 occurred
> in: glm.fit(x = X, y = Y, weights = weights, start =
> start, etastart = etastart,  
> 
> What does this output tell me? Since I only have 30
> observations, i assume this is a small sample problem.

It isn't. Spencer Graves has shown clearly with two examples
that you can get a fit with no warnings with 3 observations,
and a fit with warnings for 1000 observations. As he says,
it arises when you get "perfect separation" with respect to
the linear model.

However, it may be worth expanding Spencer's explanation.
With a single explanatory variable x (as in Spencer's examples),
"perfect separation" occurs when y = 0 for all x <= some x0,
and y = 1 for all x > x0.

One of the parameters in the linear model is the "scale parameter"
(i.e. the recipsorcal of the "slope"). If you express the model
in the form

  logit(P(Y=1;x)) = (x - mu)/sigma

then sigma is the scale parameter in question.

As sigma -> 0, P(Y=1;x) -> 0 for x < mu, and -> 1 for x > mu.

Therefore, for any value of mu between x0 (at and below which
all y=0 in your data) and x1 (the next larger value of x, at
and above which all y=1), letting sigma -> 0 gives a fit
which perfectly predicts your y-values: it predicts P(Y=1) = 0,
i.e. P(Y=0) = 1, for x < mu, and predicts P(Y=1) = 1 for x > mu;
and this is exactly what you have observed in the data.

So it's not a disaster -- in model-fitting terms, it is a
resounding success!

However, in real life one does not expect to be dealing with
a situation where the outcomes are so perfectly predictable,
and therefore one views such a result with due mistrust.
One attributes the "perfect separation" not to perfect
predictability, but to the possibility that, by chance,
all the "y=0" occur at lower values of x, and all the "y=1"
at higher values of x.

> Is it possible to fit this model in R with only 30
> observations? Could any expert provide suggestions to
> avoid the warning?

Yes! As Spencer showed, it is possible with 3 -- but of
course it depends on the outcomes y.

As to a suggestion to "avoid the warning" -- what you really
need to avoid is data where the x-values are so sparse in
the neighbourhood of the "P(Y=1;x) = 0.5" area that it becomes
likely that you will get y-values showing perfect separation.

What that means in practice is that, over the range of x-values
such that P(Y=1;x) rises from (say) 0.2 to 0.8 (chosen for
illustration), you should have several x values in your data.
Then the phenomonon of "perfect separation" becomes unlikely.

But what that means in real life is that you need enough data,
over the relevant range of x-values, to enable you to obtain
(with high probability) a non-zero estimate of sigma (i.e. an
estimate of slope 1/sigma which is not infinite) -- i.e. that
you have enough data, and in the right places, to estimate the
rate at which the probability increases from low to high values.

(Theoretically, it is possible that you get "perfect separation"
even with well-distributed x-values and sigma > 0; but with
well-distributed x-values the chance that this would occur is so
small that it can be neglected).

So, to come back to your particular case, the really meaningful
suggestion for "avoiding the warning" is that you need better
data. If your study is such that you have to take the x-values
as they come (as opposed to a designed experiement where you
can decide what they are to be), then this suggestion boils
down to "get more data".

What that would mean depends on having information about the
smallest value of sigma (largest value of slope) that is
*plausible* in your context. Your data are not particularly
useful, since they positively encourage adopting sigma=0.
So objective information about this could only come from
independent knowledge.

However, as a rule of thumb, in such a situation I would try
to get more data until I had, say, 10 x-values roughly evenly
distributed between the largest for which y=0 and the smallest
for which y=1. If that didn't work first time, then repeat
using the extended data as starting-point.

Or simply sample more data until the phenomenonof perfect
separation was well avoided, and the S.D. of the x-coefficient
was distinctly smaller than the value of the x-coefficient.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Jul-05                                       Time: 10:45:04
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul  2 13:22:19 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 02 Jul 2005 12:22:19 +0100 (BST)
Subject: [R] Generating correlated data from uniform distribution
In-Reply-To: <x24qbdr3n2.fsf@turmalin.kubism.ku.dk>
Message-ID: <XFMail.050702122219.Ted.Harding@nessie.mcc.ac.uk>

On 02-Jul-05 Peter Dalgaard wrote:
> "Jim Brennan" <jfbrennan at rogers.com> writes:
> 
>> OK now I am skeptical especially when you say in a weird way:-)
>> This may be OK but look at plot(x,y) and I am suspicious. Is it still
>> alright with this kind of relationship?
> ...
>> N <- 10000
>> rho <- .6
>> x <- runif(N, -.5,.5)
>> y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))
> 
> Well, the covariance is (everything has mean zero, of course)
> 
> E(XY) = (1+rho)/2*EX^2 + (1-rho)/2*E(X*-X) = rho*EX^2 
> 
> The marginal distribution of Y is a mixture of two identical uniforms
> (X and -X) so is uniform and in particular has the same variance as X.
> 
> In summary,  EXY/sqrt(EX^2EY^2) == rho
> 
> So as I said, it satisfies the formal requirements. X and Y are
> uniformly distributed and their correlation is rho. 
> 
> If for nothing else, I suppose that this example is good for
> demonstrating that independence and uncorrelatedness is not the same
> thing.

That was a nice sneaky solution! I was toying with something similar,
but less sneaky, until I saw Peter's, on the lines of

  x<-runif(2N, -0.5,0.5); ix<-(N-k):(N+k); y<-x; y[ix]<-(-y[ix])

(which makes the same point about independence and correlation).
The larger k as a fraction of N, the more you swing from rho = 1
to rho = -1, but you cannot achieve, as Peter did, an arbitrary
correlation coefficient rho since the value depends on k which
can only take discrete values.

Another approach which leads to a less "special" joint distribution
is

  x<-sort(runif(N, -0.5,0.5)); y<-sort(runif(N, -0.5,0.5))

followed by a rho-dependent permutation of y. I'm still pondering
a way of choosing the permutation so as to get a desired rho.

The extremes are the identity, which for a given sample will
give as close as you can get to rho = +1, and reversal, which
gives as close as you can get to rho = -1.

However, the maximum theoretical rho which you can get (as opposed
to what is possible for particular samples, which may get arbitrarily
close to +1) depends on N. For instance, with N=3, it looks as
though the theoretical rho is about 0.9 with the "identity"
permutation (for N=1000, however, just about all samples give
rho > 0.99).

I smell a source of interesting exam questions ...

Over to you!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Jul-05                                       Time: 12:22:09
------------------------------ XFMail ------------------------------



From drewbrewit at yahoo.com  Sat Jul  2 15:09:33 2005
From: drewbrewit at yahoo.com (Nick Drew)
Date: Sat, 2 Jul 2005 06:09:33 -0700 (PDT)
Subject: [R] interrupted Y axis
Message-ID: <20050702130933.11655.qmail@web50902.mail.yahoo.com>

I did not find an answer to my question after a quick
search using the R
search engine so thought I'd ask away:

Does any know if there's a function exists to create
an interrupted Y axis?
What I mean by interrupted Y axis is that part of the
Y axis has been
removed or excised to permit one to see parts of the
data in more detail.

Perhaps an example will make this clear. Please go to
http://www.jbc.org/cgi/reprint/274/41/28950 and open
the PDF document
located there. Go to page 4, figure 2c provides a
crude example of what I
mean by interrupted Y axis. Part of the Y axis between
800 and 4500 has been
removed to permit easy inspection of the upper end of
the range of data.
(This is not my work but simply an example of what I'm
trying to describe.)
One finds these interrupted Y axis graphs in
newspapers or other
periodicals, more often than not as a bar chart.

Does a function in R exist to permit on to this easily
to a graph? If not,
would such a function be useful? If yes, would the
grid package be the right
tool for me to try and implement this?
~Nick



From joseclaudio.faria at terra.com.br  Sat Jul  2 15:15:36 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sat, 02 Jul 2005 10:15:36 -0300
Subject: [R] Cluster of iris data set from Mahalanobis distances
Message-ID: <42C69378.6010603@terra.com.br>

Dear R list,

My question:

I'm trying to calculate Mahalanobis distances for 'Species' from the iris data 
set as below:

cat('\n')
cat('Cluster analyse of iris data\n')
cat('from Mahalanobis distance obtained of SAS\n')
cat('\n')

n   = 3
dat = c(        0,
  	       89.86419, 	       0,
  	      179.38471, 	17.20107, 	0)

D = matrix(0, n, n)

nam = c('Set', 'Ver', 'Vir')
rownames(D) = nam
colnames(D) = nam

k = 0
for (i in 1:n) {
   for (j in 1:i) {
      k      = k+1
      D[i,j] = dat[k]
      D[j,i] = dat[k]
   }
}

D=sqrt(D) #D2  -> D

dendroS = hclust(as.dist(D), method='single')
dendroC = hclust(as.dist(D), method='complete')

win.graph(w = 3.5, h = 6)
split.screen(c(2, 1))
screen(1)
plot(dendroS, main='Single', sub='', xlab='', ylab='', col='blue')

screen(2)
plot(dendroC, main='Complete', sub='', xlab='', col='red')


I'm not fouding how to make it in the CRAN documentation (Archives, packages: 
mclust, cluster, fpc and mva).

Could help me?

Regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br



From ggrothendieck at gmail.com  Sat Jul  2 15:21:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 2 Jul 2005 09:21:35 -0400
Subject: [R] interrupted Y axis
In-Reply-To: <20050702130933.11655.qmail@web50902.mail.yahoo.com>
References: <20050702130933.11655.qmail@web50902.mail.yahoo.com>
Message-ID: <971536df0507020621b73fa1d@mail.gmail.com>

On 7/2/05, Nick Drew <drewbrewit at yahoo.com> wrote:
> I did not find an answer to my question after a quick
> search using the R
> search engine so thought I'd ask away:
> 
> Does any know if there's a function exists to create
> an interrupted Y axis?
> What I mean by interrupted Y axis is that part of the
> Y axis has been
> removed or excised to permit one to see parts of the
> data in more detail.
> 
> Perhaps an example will make this clear. Please go to
> http://www.jbc.org/cgi/reprint/274/41/28950 and open
> the PDF document
> located there. Go to page 4, figure 2c provides a
> crude example of what I
> mean by interrupted Y axis. Part of the Y axis between
> 800 and 4500 has been
> removed to permit easy inspection of the upper end of
> the range of data.
> (This is not my work but simply an example of what I'm
> trying to describe.)
> One finds these interrupted Y axis graphs in
> newspapers or other
> periodicals, more often than not as a bar chart.
> 
> Does a function in R exist to permit on to this easily
> to a graph? If not,
> would such a function be useful? If yes, would the
> grid package be the right
> tool for me to try and implement this?


See axis.break in the plotrix package.



From MSchwartz at mn.rr.com  Sat Jul  2 15:32:04 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 02 Jul 2005 08:32:04 -0500
Subject: [R] interrupted Y axis
In-Reply-To: <20050702130933.11655.qmail@web50902.mail.yahoo.com>
References: <20050702130933.11655.qmail@web50902.mail.yahoo.com>
Message-ID: <1120311125.9639.14.camel@localhost.localdomain>

On Sat, 2005-07-02 at 06:09 -0700, Nick Drew wrote:
> I did not find an answer to my question after a quick
> search using the R
> search engine so thought I'd ask away:
> 
> Does any know if there's a function exists to create
> an interrupted Y axis?
> What I mean by interrupted Y axis is that part of the
> Y axis has been
> removed or excised to permit one to see parts of the
> data in more detail.
> 
> Perhaps an example will make this clear. Please go to
> http://www.jbc.org/cgi/reprint/274/41/28950 and open
> the PDF document
> located there. Go to page 4, figure 2c provides a
> crude example of what I
> mean by interrupted Y axis. Part of the Y axis between
> 800 and 4500 has been
> removed to permit easy inspection of the upper end of
> the range of data.
> (This is not my work but simply an example of what I'm
> trying to describe.)
> One finds these interrupted Y axis graphs in
> newspapers or other
> periodicals, more often than not as a bar chart.
> 
> Does a function in R exist to permit on to this easily
> to a graph? If not,
> would such a function be useful? If yes, would the
> grid package be the right
> tool for me to try and implement this?
> ~Nick

Using:

RSiteSearch("axis break")

comes up with 78 hits. You might want to consider some of the approaches
taken, including the axis.break() function in the 'plotrix' package on
CRAN.

Note however that the examples (ie. newspapers) you site are called "Pop
Charts" by Bill Cleveland in his book The Elements of Graphing Data and
are highly criticized.

If you have such disparate ranges you might want to consider separate
plots and/or log scaling. Be careful however, that expanding the
vertical axis to isolate specific data can result in the visual
perception of significant difference where none exists. This notion is
also explored by Cleveland and Tufte ("The Visual Display of
Quantitative Information").

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Sat Jul  2 18:00:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Jul 2005 09:00:19 -0700
Subject: [R] Is it possible to use glm() with 30 observations?
In-Reply-To: <XFMail.050702110018.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050702110018.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42C6BA13.3070003@pdf.com>

	  I agree with Ted:  "in model-fitting terms, it is a
resounding success!"  With any data set having at least one point with a 
binomial yield of 0 or 100%, you can get this phenomenon by adding 
series of random numbers sequentially to a model.  Eventually, you will 
add enough variables that some linear combination of the "predictors" 
will provide perfect prediction for that case.  In such cases, the usual 
asymptotic normality of the parameter estimates breaks down, but you can 
still test using 2*log(likelihood ratio) being approximately chi-square 
with anova(fit1, fit2), as explained in Venables and Ripley and many 
other sources, e.g., ?anova.glm:

 > tst2.glm <- data.frame(x=1:1000,
+                      y=rep(0:1, each=500))
 > fit0 <- glm(y~1, family=binomial, data=tst2.glm)
 > fit1 <- glm(y~x, family=binomial, data=tst2.glm)
Warning messages:
1: algorithm did not converge in: glm.fit(x = X, y = Y, weights = 
weights, start = start, etastart = etastart,
2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y 
= Y, weights = weights, start = start, etastart = etastart,
 > anova(fit0, fit1, test="Chisq")
Analysis of Deviance Table

Model 1: y ~ 1
Model 2: y ~ x
   Resid. Df Resid. Dev  Df Deviance  P(>|Chi|)
1       999     1386.3
2       998  6.764e-05   1   1386.3 1.999e-303
 >
	  This effect exposes a limit to the traditional advice for 
experimental design to test only at two levels for each experimental 
factor, and put them as far apart as possible without jeopardizing 
linearity.  Here, you want to estimate a priori the range over which the 
probability of "success" goes from, say, 20% to 80%, then double that 
range and make all sets of test conditions unique and spaced roughly 
evenly over that range.  Designing experiments with binary response is 
an extremely difficult problem.  This crude procedure should get you 
something moderately close to an optimal design.

	  Best Wishes,
	  spencer graves

(Ted Harding) wrote:
> On 02-Jul-05 Kerry Bush wrote:
> 
>>I have a very simple problem. When using glm to fit
>>binary logistic regression model, sometimes I receive
>>the following warning:
>>
>>Warning messages:
>>1: fitted probabilities numerically 0 or 1 occurred
>>in: glm.fit(x = X, y = Y, weights = weights, start =
>>start, etastart = etastart,  
>>2: fitted probabilities numerically 0 or 1 occurred
>>in: glm.fit(x = X, y = Y, weights = weights, start =
>>start, etastart = etastart,  
>>
>>What does this output tell me? Since I only have 30
>>observations, i assume this is a small sample problem.
> 
> 
> It isn't. Spencer Graves has shown clearly with two examples
> that you can get a fit with no warnings with 3 observations,
> and a fit with warnings for 1000 observations. As he says,
> it arises when you get "perfect separation" with respect to
> the linear model.
> 
> However, it may be worth expanding Spencer's explanation.
> With a single explanatory variable x (as in Spencer's examples),
> "perfect separation" occurs when y = 0 for all x <= some x0,
> and y = 1 for all x > x0.
> 
> One of the parameters in the linear model is the "scale parameter"
> (i.e. the recipsorcal of the "slope"). If you express the model
> in the form
> 
>   logit(P(Y=1;x)) = (x - mu)/sigma
> 
> then sigma is the scale parameter in question.
> 
> As sigma -> 0, P(Y=1;x) -> 0 for x < mu, and -> 1 for x > mu.
> 
> Therefore, for any value of mu between x0 (at and below which
> all y=0 in your data) and x1 (the next larger value of x, at
> and above which all y=1), letting sigma -> 0 gives a fit
> which perfectly predicts your y-values: it predicts P(Y=1) = 0,
> i.e. P(Y=0) = 1, for x < mu, and predicts P(Y=1) = 1 for x > mu;
> and this is exactly what you have observed in the data.
> 
> So it's not a disaster -- in model-fitting terms, it is a
> resounding success!
> 
> However, in real life one does not expect to be dealing with
> a situation where the outcomes are so perfectly predictable,
> and therefore one views such a result with due mistrust.
> One attributes the "perfect separation" not to perfect
> predictability, but to the possibility that, by chance,
> all the "y=0" occur at lower values of x, and all the "y=1"
> at higher values of x.
> 
> 
>>Is it possible to fit this model in R with only 30
>>observations? Could any expert provide suggestions to
>>avoid the warning?
> 
> 
> Yes! As Spencer showed, it is possible with 3 -- but of
> course it depends on the outcomes y.
> 
> As to a suggestion to "avoid the warning" -- what you really
> need to avoid is data where the x-values are so sparse in
> the neighbourhood of the "P(Y=1;x) = 0.5" area that it becomes
> likely that you will get y-values showing perfect separation.
> 
> What that means in practice is that, over the range of x-values
> such that P(Y=1;x) rises from (say) 0.2 to 0.8 (chosen for
> illustration), you should have several x values in your data.
> Then the phenomonon of "perfect separation" becomes unlikely.
> 
> But what that means in real life is that you need enough data,
> over the relevant range of x-values, to enable you to obtain
> (with high probability) a non-zero estimate of sigma (i.e. an
> estimate of slope 1/sigma which is not infinite) -- i.e. that
> you have enough data, and in the right places, to estimate the
> rate at which the probability increases from low to high values.
> 
> (Theoretically, it is possible that you get "perfect separation"
> even with well-distributed x-values and sigma > 0; but with
> well-distributed x-values the chance that this would occur is so
> small that it can be neglected).
> 
> So, to come back to your particular case, the really meaningful
> suggestion for "avoiding the warning" is that you need better
> data. If your study is such that you have to take the x-values
> as they come (as opposed to a designed experiement where you
> can decide what they are to be), then this suggestion boils
> down to "get more data".
> 
> What that would mean depends on having information about the
> smallest value of sigma (largest value of slope) that is
> *plausible* in your context. Your data are not particularly
> useful, since they positively encourage adopting sigma=0.
> So objective information about this could only come from
> independent knowledge.
> 
> However, as a rule of thumb, in such a situation I would try
> to get more data until I had, say, 10 x-values roughly evenly
> distributed between the largest for which y=0 and the smallest
> for which y=1. If that didn't work first time, then repeat
> using the extended data as starting-point.
> 
> Or simply sample more data until the phenomenonof perfect
> separation was well avoided, and the S.D. of the x-coefficient
> was distinctly smaller than the value of the x-coefficient.
> 
> Hoping this helps,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 02-Jul-05                                       Time: 10:45:04
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From yuleih at umich.edu  Sat Jul  2 21:41:29 2005
From: yuleih at umich.edu (Yulei He)
Date: Sat, 2 Jul 2005 15:41:29 -0400 (EDT)
Subject: [R] probability-probability plot
Message-ID: <Pine.LNX.4.61.0507021540070.14144@rtype.gpcc.itd.umich.edu>

Hi, there.

Is there any function in R to plot the probability-probability plot (PP 
plot)? Suppose I am testing some data against normal.

Thanks.

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From celebridades at megamail.pt  Sat Jul  2 22:06:54 2005
From: celebridades at megamail.pt (alex diaz)
Date: Sat, 02 Jul 2005 21:06:54 +0100
Subject: [R] plot question
Message-ID: <1120334814.42c6f3dee78c8@paris-hme1>

dear list:

in the following plot:

plot(rnorm(10),rnorm(10),xlab="year",ylab=expression
(paste('M x'*10^{3},)),font.lab=2)

font.lab=2, but xlab and ylab are different. I want 
both labels in the same way. help?

a.d.

-------------------------------------------------
Email Enviado utilizando o servio MegaMail



From baron at psych.upenn.edu  Sat Jul  2 22:07:43 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 2 Jul 2005 16:07:43 -0400
Subject: [R] probability-probability plot
In-Reply-To: <Pine.LNX.4.61.0507021540070.14144@rtype.gpcc.itd.umich.edu>
References: <Pine.LNX.4.61.0507021540070.14144@rtype.gpcc.itd.umich.edu>
Message-ID: <20050702200743.GC27857@psych>

On 07/02/05 15:41, Yulei He wrote:
 Hi, there.
 
 Is there any function in R to plot the probability-probability plot (PP
 plot)? Suppose I am testing some data against normal.

qqnorm might be what you want, or lead to it.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From tolga at coubros.com  Sat Jul  2 22:39:09 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 02 Jul 2005 21:39:09 +0100
Subject: [R] Specifying the minimum and maximum of x and y-axis in plot
Message-ID: <42C6FB6D.6080804@coubros.com>

Hi,

How do I specify the minimum and maximum of the x and y-axis when using 
plot ?

Thanks,
Tolga



From alexbri at netcabo.pt  Sat Jul  2 22:56:16 2005
From: alexbri at netcabo.pt (Alexandre Brito)
Date: Sat, 2 Jul 2005 21:56:16 +0100
Subject: [R] Specifying the minimum and maximum of x and y-axis in plot
References: <42C6FB6D.6080804@coubros.com>
Message-ID: <001301c57f48$7f260c60$a25d8453@cc7w5mza3u10k3>

see ?plot.default

for example:
plot(1:10,1:10,ylim=c(0,10),xlim=c(0,20))


----- Original Message ----- 
From: "Tolga Uzuner" <tolga at coubros.com>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, July 02, 2005 9:39 PM
Subject: [R] Specifying the minimum and maximum of x and y-axis in plot


> Hi,
>
> How do I specify the minimum and maximum of the x and y-axis when using
> plot ?
>
> Thanks,
> Tolga
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From r.shengzhe at gmail.com  Sun Jul  3 01:07:01 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Sun, 3 Jul 2005 01:07:01 +0200
Subject: [R] how to set the position and size of the select.list window
Message-ID: <ea57975b05070216074cb61414@mail.gmail.com>

Hello,

I use "select.list" to obtain a window of select items, but how can I
set the position and size of this window?

Thank you!
shengzhe



From tolga at coubros.com  Sun Jul  3 01:41:05 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 03 Jul 2005 00:41:05 +0100
Subject: [R] jagged array
Message-ID: <42C72611.7050601@coubros.com>

Hi,

I have a "jagged array" which looks like this:

 > res
...
[[996]]
[1] 1.375 3.375 4.125 4.625 4.875 4.875 6.625 7.125 8.875

[[997]]
[1] 1.875 5.125 6.875 7.875 9.875

[[998]]
[1] 1.875 5.375 6.625 6.875 8.125 9.375 9.625

[[999]]
[1] 1.875 6.875 9.875

[[1000]]
[1] 1.875 6.125 6.875 9.375 9.625


Now, I want to use the first row,so I say res[1]

 > res[1]
[[1]]
 [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375

How do I access, an element within this ? I try

 > res[1][1]
[[1]]
 [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375

which returns the same vector.. ??? How do I "get to" 3.125 ?

Thanks



From gunter.berton at gene.com  Sun Jul  3 01:52:07 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Sat, 2 Jul 2005 16:52:07 -0700
Subject: [R] jagged array
In-Reply-To: <42C72611.7050601@coubros.com>
Message-ID: <200507022352.j62Nq6La025338@ohm.gene.com>

Please read "An Introduction to R" and the help file for "[", where this is
explained. If you want to use R, you need to expend effort to learn it.

-- Bert Gunter 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tolga Uzuner
Sent: Saturday, July 02, 2005 4:41 PM
To: r-help at stat.math.ethz.ch
Subject: [R] jagged array

Hi,

I have a "jagged array" which looks like this:

 > res
...
[[996]]
[1] 1.375 3.375 4.125 4.625 4.875 4.875 6.625 7.125 8.875

[[997]]
[1] 1.875 5.125 6.875 7.875 9.875

[[998]]
[1] 1.875 5.375 6.625 6.875 8.125 9.375 9.625

[[999]]
[1] 1.875 6.875 9.875

[[1000]]
[1] 1.875 6.125 6.875 9.375 9.625


Now, I want to use the first row,so I say res[1]

 > res[1]
[[1]]
 [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375

How do I access, an element within this ? I try

 > res[1][1]
[[1]]
 [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375

which returns the same vector.. ??? How do I "get to" 3.125 ?

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tolga at coubros.com  Sun Jul  3 02:00:50 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 03 Jul 2005 01:00:50 +0100
Subject: [R] jagged array
In-Reply-To: <200507022352.j62Nq6La025338@ohm.gene.com>
References: <200507022352.j62Nq6La025338@ohm.gene.com>
Message-ID: <42C72AB2.6090901@coubros.com>

Berton Gunter wrote:

>Please read "An Introduction to R" and the help file for "[", where this is
>explained. If you want to use R, you need to expend effort to learn it.
>
>-- Bert Gunter 
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tolga Uzuner
>Sent: Saturday, July 02, 2005 4:41 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] jagged array
>
>Hi,
>
>I have a "jagged array" which looks like this:
>
> > res
>...
>[[996]]
>[1] 1.375 3.375 4.125 4.625 4.875 4.875 6.625 7.125 8.875
>
>[[997]]
>[1] 1.875 5.125 6.875 7.875 9.875
>
>[[998]]
>[1] 1.875 5.375 6.625 6.875 8.125 9.375 9.625
>
>[[999]]
>[1] 1.875 6.875 9.875
>
>[[1000]]
>[1] 1.875 6.125 6.875 9.375 9.625
>
>
>Now, I want to use the first row,so I say res[1]
>
> > res[1]
>[[1]]
> [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375
>
>How do I access, an element within this ? I try
>
> > res[1][1]
>[[1]]
> [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375
>
>which returns the same vector.. ??? How do I "get to" 3.125 ?
>
>Thanks
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>  
>
tx



From ggrothendieck at gmail.com  Sun Jul  3 04:12:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 2 Jul 2005 22:12:28 -0400
Subject: [R] plot question
In-Reply-To: <1120334814.42c6f3dee78c8@paris-hme1>
References: <1120334814.42c6f3dee78c8@paris-hme1>
Message-ID: <971536df05070219126b4ad167@mail.gmail.com>

Trying characters and expressions variously it seems that font.lab applies
to character strings but not to expressions so if you want to use an expression
just use bold (or whatever) explicitly on the expression.  One gotcha is that
bold will not work as one might have expected on numbers so they must 
be represented as character strings -- which is why we have used "3" rather 
than 3 below.

plot(rnorm(10),rnorm(10),xlab=quote(bold(year)),ylab=quote(bold("Mx10"^"3")))

On 7/2/05, alex diaz <celebridades at megamail.pt> wrote:
> dear list:
> 
> in the following plot:
> 
> plot(rnorm(10),rnorm(10),xlab="year",ylab=expression
> (paste('M x'*10^{3},)),font.lab=2)
> 
> font.lab=2, but xlab and ylab are different. I want
> both labels in the same way. help?
> 
> a.d.
> 
> -------------------------------------------------
> Email Enviado utilizando o servi??o MegaMail
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Sun Jul  3 05:24:50 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Jul 2005 20:24:50 -0700
Subject: [R] jagged array
In-Reply-To: <42C72AB2.6090901@coubros.com>
References: <200507022352.j62Nq6La025338@ohm.gene.com>
	<42C72AB2.6090901@coubros.com>
Message-ID: <42C75A82.1010705@pdf.com>

	  Access list items using "[[".  "res[1]" is a list with only one 
attribute.  "res[1][1]" is the same list with only one attributre. 
"res[[1]]" is the first attribute of that list, so res[[1]][1] in your 
example should be 3.125.

	  spencer graves

Tolga Uzuner wrote:

> Berton Gunter wrote:
> 
> 
>>Please read "An Introduction to R" and the help file for "[", where this is
>>explained. If you want to use R, you need to expend effort to learn it.
>>
>>-- Bert Gunter 
>>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tolga Uzuner
>>Sent: Saturday, July 02, 2005 4:41 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] jagged array
>>
>>Hi,
>>
>>I have a "jagged array" which looks like this:
>>
>>
>>>res
>>
>>...
>>[[996]]
>>[1] 1.375 3.375 4.125 4.625 4.875 4.875 6.625 7.125 8.875
>>
>>[[997]]
>>[1] 1.875 5.125 6.875 7.875 9.875
>>
>>[[998]]
>>[1] 1.875 5.375 6.625 6.875 8.125 9.375 9.625
>>
>>[[999]]
>>[1] 1.875 6.875 9.875
>>
>>[[1000]]
>>[1] 1.875 6.125 6.875 9.375 9.625
>>
>>
>>Now, I want to use the first row,so I say res[1]
>>
>>
>>>res[1]
>>
>>[[1]]
>>[1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375
>>
>>How do I access, an element within this ? I try
>>
>>
>>>res[1][1]
>>
>>[[1]]
>>[1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 9.375 9.375
>>
>>which returns the same vector.. ??? How do I "get to" 3.125 ?
>>
>>Thanks
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
>> 
>>
> 
> tx
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul  3 05:33:20 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Jul 2005 20:33:20 -0700
Subject: [R] probability-probability plot
In-Reply-To: <20050702200743.GC27857@psych>
References: <Pine.LNX.4.61.0507021540070.14144@rtype.gpcc.itd.umich.edu>
	<20050702200743.GC27857@psych>
Message-ID: <42C75C80.3020806@pdf.com>

	  There are PP plots and QQ plots.  I've tried PP plots and didn't get 
much from them.  The normal QQ plot [qqnorm, in R], however, I use for 
all kinds of things.  In a data mining situation, I'll get, e.g., 500 
p.values, all uniformily distributed under the null hypothesis.  I'll 
transform that null distribution to N(0, 1) with qnorm(p.values) and make

	  qqnorm(qnorm(p.values), datax=TRUE).

	  I may be chastised severely by Tukeyites for "datax=TRUE", but the 
human eye can decode a 45 degree angle easier than any other angle other 
than horizontal or vertical.  If you have a couple of mild outliers with 
a typical aspect ratio of the plot, the "datax=TRUE" option will make 
the line moderately close to 45 degrees.

	  spencer graves

Jonathan Baron wrote:

> On 07/02/05 15:41, Yulei He wrote:
>  Hi, there.
>  
>  Is there any function in R to plot the probability-probability plot (PP
>  plot)? Suppose I am testing some data against normal.
> 
> qqnorm might be what you want, or lead to it.
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From snuggleduck at freemail.hu  Sun Jul  3 11:14:45 2005
From: snuggleduck at freemail.hu (=?ISO-8859-2?Q?B=F3kony_Veronika?=)
Date: Sun, 3 Jul 2005 11:14:45 +0200 (CEST)
Subject: [R] code for model-averaging by Akaike weights
Message-ID: <freemail.20050603111445.77404@fm7.freemail.hu>

Dear all,

does anyone have r code to perform model-averaging of regression 
parameters by Akaike weights, 
and/or to do all-possible-subsets lm modelling that reports parameter 
estimates, AICc and number of parameters for each model? 

I have been looking for these in the archive but found none.

(I am aware that many of you would warn me against these methods 
advocated by Burnham and Anderson 2002, please do not.)

Thank you,
VB




-------------------------------------------------------------------------------
[freemail] extra 1GB-os postafi??kkal, ??nnek m??r van? http://freemail.hu



From bitwrit at ozemail.com.au  Sun Jul  3 21:24:46 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sun, 03 Jul 2005 19:24:46 +0000
Subject: [R] interrupted Y axis
In-Reply-To: <20050702130933.11655.qmail@web50902.mail.yahoo.com>
References: <20050702130933.11655.qmail@web50902.mail.yahoo.com>
Message-ID: <42C83B7E.5030501@ozemail.com.au>

Nick Drew wrote:
> I did not find an answer to my question after a quick
> search using the R
> search engine so thought I'd ask away:
> 
> Does any know if there's a function exists to create
> an interrupted Y axis?
> What I mean by interrupted Y axis is that part of the
> Y axis has been
> removed or excised to permit one to see parts of the
> data in more detail.
> 
> Perhaps an example will make this clear. Please go to
> http://www.jbc.org/cgi/reprint/274/41/28950 and open
> the PDF document
> located there. Go to page 4, figure 2c provides a
> crude example of what I
> mean by interrupted Y axis. Part of the Y axis between
> 800 and 4500 has been
> removed to permit easy inspection of the upper end of
> the range of data.
> (This is not my work but simply an example of what I'm
> trying to describe.)
> One finds these interrupted Y axis graphs in
> newspapers or other
> periodicals, more often than not as a bar chart.
> 
> Does a function in R exist to permit on to this easily
> to a graph? If not,
> would such a function be useful? If yes, would the
> grid package be the right
> tool for me to try and implement this?
> ~Nick
> 
As Gabor and Mark have already pointed out, axis.break() in the plotrix 
package will do the axis break. You will also have to create custom axis 
labels using axis(). However, the plotting function used in the paper 
you cite also breaks the bars that extend to the upper range (nice 
touch, wish I had thought of that). I'm not sure if any of the barplot* 
functions will do this out of the box. I would probably fake the bottom 
and top sections with homemade bars using rect() if I had this problem. 
Anybody have a better idea?

Jim



From tolga at coubros.com  Sun Jul  3 11:38:15 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 03 Jul 2005 10:38:15 +0100
Subject: [R] jagged array
In-Reply-To: <42C75A82.1010705@pdf.com>
References: <200507022352.j62Nq6La025338@ohm.gene.com>
	<42C72AB2.6090901@coubros.com> <42C75A82.1010705@pdf.com>
Message-ID: <42C7B207.7090003@coubros.com>

Spencer Graves wrote:

>       Access list items using "[[".  "res[1]" is a list with only one 
> attribute.  "res[1][1]" is the same list with only one attributre. 
> "res[[1]]" is the first attribute of that list, so res[[1]][1] in your 
> example should be 3.125.
>
>       spencer graves
>
> Tolga Uzuner wrote:
>
>> Berton Gunter wrote:
>>
>>
>>> Please read "An Introduction to R" and the help file for "[", where 
>>> this is
>>> explained. If you want to use R, you need to expend effort to learn it.
>>>
>>> -- Bert Gunter
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tolga Uzuner
>>> Sent: Saturday, July 02, 2005 4:41 PM
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] jagged array
>>>
>>> Hi,
>>>
>>> I have a "jagged array" which looks like this:
>>>
>>>
>>>> res
>>>
>>>
>>> ...
>>> [[996]]
>>> [1] 1.375 3.375 4.125 4.625 4.875 4.875 6.625 7.125 8.875
>>>
>>> [[997]]
>>> [1] 1.875 5.125 6.875 7.875 9.875
>>>
>>> [[998]]
>>> [1] 1.875 5.375 6.625 6.875 8.125 9.375 9.625
>>>
>>> [[999]]
>>> [1] 1.875 6.875 9.875
>>>
>>> [[1000]]
>>> [1] 1.875 6.125 6.875 9.375 9.625
>>>
>>>
>>> Now, I want to use the first row,so I say res[1]
>>>
>>>
>>>> res[1]
>>>
>>>
>>> [[1]]
>>> [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 
>>> 9.375 9.375
>>>
>>> How do I access, an element within this ? I try
>>>
>>>
>>>> res[1][1]
>>>
>>>
>>> [[1]]
>>> [1] 3.125 4.375 4.625 5.125 5.375 6.375 6.875 7.875 9.125 9.125 
>>> 9.375 9.375
>>>
>>> which returns the same vector.. ??? How do I "get to" 3.125 ?
>>>
>>> Thanks
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>
>>>
>>
>> tx
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
Many thanks, Tolga



From i.visser at uva.nl  Sun Jul  3 12:19:51 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Sun, 03 Jul 2005 12:19:51 +0200
Subject: [R] code for model-averaging by Akaike weights
In-Reply-To: <freemail.20050603111445.77404@fm7.freemail.hu>
Message-ID: <BEED8867.77E9%i.visser@uva.nl>

the leaps package does all-subsets regression, hth, ingmar


On 7/3/05 11:14 AM, "B??kony Veronika" <snuggleduck at freemail.hu> wrote:

> Dear all,
> 
> does anyone have r code to perform model-averaging of regression
> parameters by Akaike weights,
> and/or to do all-possible-subsets lm modelling that reports parameter
> estimates, AICc and number of parameters for each model?
> 
> I have been looking for these in the archive but found none.
> 
> (I am aware that many of you would warn me against these methods
> advocated by Burnham and Anderson 2002, please do not.)
> 
> Thank you,
> VB
> 
> 
> 
> 
> 
------------------------------------------------------------------------------>
-
> [freemail] extra 1GB-os postafi??kkal, ??nnek m??r van? http://freemail.hu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From ndsoares at mail.telepac.pt  Sun Jul  3 12:56:59 2005
From: ndsoares at mail.telepac.pt (Nuno Soares)
Date: Sun, 3 Jul 2005 11:56:59 +0100
Subject: [R] Pearson and Spearman correlation coeffcients matrix
Message-ID: <200507031057.j63Av4hE014691@hypatia.math.ethz.ch>

Hi everyone,

I've been trying to find a function that outputs the Pearson and/or Spearman
correlation coefficients for several variables with the associated
statistics in one single table/matrix. For what I've been able to understand
the Stats package is only able to compute these coeficients/statistics only
in defined pairs. This becomes time consuming when we want to determine
these measures with, say, 10 variables or more. Does anyone knows a solution
to this?

Kind regards,

Nuno



From HDoran at air.org  Sun Jul  3 13:49:00 2005
From: HDoran at air.org (Doran, Harold)
Date: Sun, 3 Jul 2005 07:49:00 -0400
Subject: [R] Symbolic Maximum Likelihood in R
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41BB9@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050703/c6ea24a4/attachment.pl

From dmbates at gmail.com  Sun Jul  3 15:12:29 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 3 Jul 2005 08:12:29 -0500
Subject: [R] Pearson and Spearman correlation coeffcients matrix
In-Reply-To: <200507031057.j63Av4hE014691@hypatia.math.ethz.ch>
References: <200507031057.j63Av4hE014691@hypatia.math.ethz.ch>
Message-ID: <40e66e0b05070306125c1f89de@mail.gmail.com>

On 7/3/05, Nuno Soares <ndsoares at mail.telepac.pt> wrote:
> Hi everyone,
> 
> I've been trying to find a function that outputs the Pearson and/or Spearman
> correlation coefficients for several variables with the associated
> statistics in one single table/matrix. For what I've been able to understand
> the Stats package is only able to compute these coeficients/statistics only
> in defined pairs. This becomes time consuming when we want to determine
> these measures with, say, 10 variables or more. Does anyone knows a solution
> to this?

I believe that the cor function already does what you want.  Check

?cor

and 

example(cor)

If the first argument to cor is a matrix, it returns the correlation
matrix of all the columns.



From ivar.herfindal at bio.ntnu.no  Sun Jul  3 15:28:33 2005
From: ivar.herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Sun, 03 Jul 2005 15:28:33 +0200
Subject: [R] Pearson and Spearman correlation coeffcients matrix
In-Reply-To: <40e66e0b05070306125c1f89de@mail.gmail.com>
References: <200507031057.j63Av4hE014691@hypatia.math.ethz.ch>
	<40e66e0b05070306125c1f89de@mail.gmail.com>
Message-ID: <42C7E801.3010103@bio.ntnu.no>



Douglas Bates wrote:

> On 7/3/05, Nuno Soares <ndsoares at mail.telepac.pt> wrote:
> 
>>Hi everyone,
>>
>>I've been trying to find a function that outputs the Pearson and/or Spearman
>>correlation coefficients for several variables with the associated
>>statistics in one single table/matrix. For what I've been able to understand
>>the Stats package is only able to compute these coeficients/statistics only
>>in defined pairs. This becomes time consuming when we want to determine
>>these measures with, say, 10 variables or more. Does anyone knows a solution
>>to this?
> 
> 
> I believe that the cor function already does what you want.  Check
> 
> ?cor
> 
> and 
> 
> example(cor)
> 
> If the first argument to cor is a matrix, it returns the correlation
> matrix of all the columns.
> 
> 
You can also try the rcorr function in the Hmisc package (or bundle or 
whatever it is called). It returns one matrix with correlation 
coefficients and one matrix with p-values.

Ivar



From baron at psych.upenn.edu  Sun Jul  3 15:33:15 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 3 Jul 2005 09:33:15 -0400
Subject: [R] Pearson and Spearman correlation coeffcients matrix
In-Reply-To: <40e66e0b05070306125c1f89de@mail.gmail.com>
References: <200507031057.j63Av4hE014691@hypatia.math.ethz.ch>
	<40e66e0b05070306125c1f89de@mail.gmail.com>
Message-ID: <20050703133315.GA21127@psych>

I think "associated statistics" means p-levels.

This post
http://finzi.psych.upenn.edu/R/Rhelp01/archive/5380.html
tells how to do it and gives and example.

But I think that nobody wants to make a function for this because 
it would encourage bad behavior, i.e., snooping through a
correlation matrix looking for "significant" correlations and
then making up a story about how the 5% of them that were
significant at p<.05 were really planned tests.

Jon

On 07/03/05 08:12, Douglas Bates wrote:
 On 7/3/05, Nuno Soares <ndsoares at mail.telepac.pt> wrote:
 > Hi everyone,
 >
 > I've been trying to find a function that outputs the Pearson and/or Spearman
 > correlation coefficients for several variables with the associated
 > statistics in one single table/matrix. For what I've been able to understand
 > the Stats package is only able to compute these coeficients/statistics only
 > in defined pairs. This becomes time consuming when we want to determine
 > these measures with, say, 10 variables or more. Does anyone knows a solution
 > to this?
 
 I believe that the cor function already does what you want.  Check
 
 ?cor
 
 and
 
 example(cor)
 
 If the first argument to cor is a matrix, it returns the correlation
 matrix of all the columns.
 
 ______________________________________________
 R-help at stat.math.ethz.ch mailing list
 https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From wilks at dial.pipex.com  Sun Jul  3 19:51:06 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Sun, 3 Jul 2005 18:51:06 +0100
Subject: [R] plot question
Message-ID: <JCEIJNOHMNBPLMGFDHNDCEKPCAAA.wilks@dial.pipex.com>


a.d.

I refer you to ?title and its given examples.

try this --

plot(rnorm(10),rnorm(10),xlab=" ",ylab=" ")
title(xlab="year",
ylab=expression(paste('M x'*10^{3},)),font=2)

note that 'title()' will alos accept a list for x and y labs,
for additional parameters,e.g., 'col' and 'cex'


John

a.d wrote---

dear list:

in the following plot:

plot(rnorm(10),rnorm(10),xlab="year",ylab=expression
(paste('M x'*10^{3},)),font.lab=2)

font.lab=2, but xlab and ylab are different. I want 
both labels in the same way. help?

a.d.



From 0034058 at fudan.edu.cn  Sun Jul  3 20:08:12 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 04 Jul 2005 02:08:12 +0800
Subject: [R] demo(scoping)
Message-ID: <20050704020812.5162f251@localhost.localdomain>

entercount an error with demo(scoping).

> demo(scoping)


        demo(scoping)
        ---- ~~~~~~~
___snip_____


> ross$balance()
Your balance is 120


> try(ross$withdraw(500))
Error in ross$withdraw(500) : You don't have that much money!


> version
         _
platform i486-pc-linux-gnu
arch     i486
os       linux-gnu
system   i486, linux-gnu
status   beta
major    2
minor    1.1
year     2005
month    06
day      13
language R


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From ggrothendieck at gmail.com  Sun Jul  3 20:59:34 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Jul 2005 14:59:34 -0400
Subject: [R] demo(scoping)
In-Reply-To: <20050704020812.5162f251@localhost.localdomain>
References: <20050704020812.5162f251@localhost.localdomain>
Message-ID: <971536df05070311596c7f6ee5@mail.gmail.com>

On 7/3/05, ronggui <0034058 at fudan.edu.cn> wrote:
> entercount an error with demo(scoping).
> 
> > demo(scoping)
> 
> 
>        demo(scoping)
>        ---- ~~~~~~~
> ___snip_____
> 
> 
> > ross$balance()
> Your balance is 120
> 
> 
> > try(ross$withdraw(500))
> Error in ross$withdraw(500) : You don't have that much money!
> 

Its supposed to give that behavior since its trying to withdraw 500
from an account that only has 120 in it.



From wilks at dial.pipex.com  Sun Jul  3 21:15:58 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Sun, 3 Jul 2005 20:15:58 +0100
Subject: [R] plot question
Message-ID: <JCEIJNOHMNBPLMGFDHNDKEKPCAAA.wilks@dial.pipex.com>


Gabor,

I thought that I had worked around the 'expression' format problem,
but if the x-y labels are to be bold, then using,say, cex.lab=1.25    in the
title(), appears to simulate 'bold' font very well, both for the ylab maths
expression and xlab text.

Your solution is the rigorous one!

John

for example --

plot(rnorm(10),rnorm(10),xlab=" ",ylab=" ")
title(xlab="year",
ylab=expression(paste("M x"*10^{3})),font.lab=1,col.lab=4,
cex.lab=1.25)


Gabor Grothendieck wrote ---

Trying characters and expressions variously it seems that font.lab applies
to character strings but not to expressions so if you want to use an
expression
just use bold (or whatever) explicitly on the expression.  One gotcha is
that
bold will not work as one might have expected on numbers so they must
be represented as character strings -- which is why we have used "3" rather
than 3 below.

plot(rnorm(10),rnorm(10),xlab=quote(bold(year)),ylab=quote(bold("Mx10"^"3"))
)

On 7/2/05, alex diaz <celebridades at megamail.pt> wrote:
> dear list:
>
> in the following plot:
>
> plot(rnorm(10),rnorm(10),xlab="year",ylab=expression
> (paste('M x'*10^{3},)),font.lab=2)
>
> font.lab=2, but xlab and ylab are different. I want
> both labels in the same way. help?
>
> a.d.
>



From ggrothendieck at gmail.com  Sun Jul  3 21:30:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Jul 2005 15:30:54 -0400
Subject: [R] plot question
In-Reply-To: <JCEIJNOHMNBPLMGFDHNDKEKPCAAA.wilks@dial.pipex.com>
References: <JCEIJNOHMNBPLMGFDHNDKEKPCAAA.wilks@dial.pipex.com>
Message-ID: <971536df0507031230a1fc823@mail.gmail.com>

At least on Windows XP R 2.1.1 it does seem like there is quite a 
difference to me, at least when done side by side, which can be 
seen like this:

plot(rnorm(10),rnorm(10),xlab="",ylab="")
title(xlab="x",ylab=quote(bold(3*"4")),font.lab=1,cex.lab=1.25,col.lab="blue")

since the cex.lab will act on both 3 and "4" but bold will only act on
the "4" since numbers such as 3 in the example are ignored by bold.
Maybe in isolation it would suffice.

On 7/3/05, John Wilkinson (pipex) <wilks at dial.pipex.com> wrote:
> 
> Gabor,
> 
> I thought that I had worked around the 'expression' format problem,
> but if the x-y labels are to be bold, then using,say, cex.lab=1.25    in the
> title(), appears to simulate 'bold' font very well, both for the ylab maths
> expression and xlab text.
> 
> Your solution is the rigorous one!
> 
> John
> 
> for example --
> 
> plot(rnorm(10),rnorm(10),xlab=" ",ylab=" ")
> title(xlab="year",
> ylab=expression(paste("M x"*10^{3})),font.lab=1,col.lab=4,
> cex.lab=1.25)
> 
> 
> Gabor Grothendieck wrote ---
> 
> Trying characters and expressions variously it seems that font.lab applies
> to character strings but not to expressions so if you want to use an
> expression
> just use bold (or whatever) explicitly on the expression.  One gotcha is
> that
> bold will not work as one might have expected on numbers so they must
> be represented as character strings -- which is why we have used "3" rather
> than 3 below.
> 
> plot(rnorm(10),rnorm(10),xlab=quote(bold(year)),ylab=quote(bold("Mx10"^"3"))
> )
> 
> On 7/2/05, alex diaz <celebridades at megamail.pt> wrote:
> > dear list:
> >
> > in the following plot:
> >
> > plot(rnorm(10),rnorm(10),xlab="year",ylab=expression
> > (paste('M x'*10^{3},)),font.lab=2)
> >
> > font.lab=2, but xlab and ylab are different. I want
> > both labels in the same way. help?
> >
> > a.d.
> >
> 
> 
>



From wasquith at austin.rr.com  Sun Jul  3 22:02:25 2005
From: wasquith at austin.rr.com (William H. Asquith)
Date: Sun, 3 Jul 2005 15:02:25 -0500
Subject: [R] over/under flow
Message-ID: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>

I am porting some FORTRAN to R in which an Inf triggers an if().  The
trigger is infinite on exp(lgamma(OVER)).  What is the canonical R 
style of determining OVER when exp(OVER)== Inf?  The code structure 
that I am
porting is best left intact--so I need to query R somehow to the value
of OVER that causes exp(lgamma(OVER)) to equal Inf.

On my system,
exp(lgamma(171)) is about first to equal Inf.

I asked similar question a few weeks ago on exp(OVER) and got the 
answer back as log(.Machine$double.xmax).  I now have the lgamma 
involved.  I think that answer is what is OVER such the

.Machine$double.xmax = lgamma(OVER),

but I am not sure how to invert or solve for OVER

Thanks.



From p.dalgaard at biostat.ku.dk  Sun Jul  3 22:43:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jul 2005 22:43:12 +0200
Subject: [R] over/under flow
In-Reply-To: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>
References: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>
Message-ID: <x2d5pzipsv.fsf@turmalin.kubism.ku.dk>

"William H. Asquith" <wasquith at austin.rr.com> writes:

> I am porting some FORTRAN to R in which an Inf triggers an if().  The
> trigger is infinite on exp(lgamma(OVER)).  What is the canonical R 
> style of determining OVER when exp(OVER)== Inf?  The code structure 
> that I am
> porting is best left intact--so I need to query R somehow to the value
> of OVER that causes exp(lgamma(OVER)) to equal Inf.
> 
> On my system,
> exp(lgamma(171)) is about first to equal Inf.
> 
> I asked similar question a few weeks ago on exp(OVER) and got the 
> answer back as log(.Machine$double.xmax).  I now have the lgamma 
> involved.  I think that answer is what is OVER such the
> 
> .Machine$double.xmax = lgamma(OVER),

Not quite... (see below)
 
> but I am not sure how to invert or solve for OVER


> uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), c(171,172))
$root
[1] 171.6244

$f.root
[1] -1.462051e-07

$iter
[1] 3

$estim.prec
[1] 6.103516e-05


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Sun Jul  3 22:58:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 3 Jul 2005 16:58:33 -0400
Subject: [R] over/under flow
In-Reply-To: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>
References: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>
Message-ID: <971536df050703135856b92cba@mail.gmail.com>

On 7/3/05, William H. Asquith <wasquith at austin.rr.com> wrote:
> I am porting some FORTRAN to R in which an Inf triggers an if().  The
> trigger is infinite on exp(lgamma(OVER)).  What is the canonical R
> style of determining OVER when exp(OVER)== Inf?  The code structure
> that I am
> porting is best left intact--so I need to query R somehow to the value
> of OVER that causes exp(lgamma(OVER)) to equal Inf.
> 
> On my system,
> exp(lgamma(171)) is about first to equal Inf.
> 
> I asked similar question a few weeks ago on exp(OVER) and got the
> answer back as log(.Machine$double.xmax).  I now have the lgamma
> involved.  I think that answer is what is OVER such the
> 
> .Machine$double.xmax = lgamma(OVER),
> 
> but I am not sure how to invert or solve for OVER
> 

You could just test the result, e.g.  
   result <- exp(lgamma(x))
   if (is.finite(result)) ...



From wasquith at austin.rr.com  Sun Jul  3 23:00:40 2005
From: wasquith at austin.rr.com (William H. Asquith)
Date: Sun, 3 Jul 2005 16:00:40 -0500
Subject: [R] over/under flow
In-Reply-To: <x2d5pzipsv.fsf@turmalin.kubism.ku.dk>
References: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>
	<x2d5pzipsv.fsf@turmalin.kubism.ku.dk>
Message-ID: <366f191fef2b1ea7ada087f41fad0d50@austin.rr.com>

Great, but to followup, how do I select the bounds (B,E) on the root 
for an arbitrary machine?

OVER = uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), 
c(B,E))$root

If uniroot() is fast enough, is it appropriate for me to set B at say 1 
and E at log(.Machine$double.max)?  Suggestions on do this the proper 
"R way"?  Perhaps this . . .

OVER = uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), 
c(1,log(.Machine$double.xmax)))$root

I am working on a package so different machines will be involved thus 
simple 171,172 might not be the best idea for the root?

THANKS!

-wha


On Jul 3, 2005, at 3:43 PM, Peter Dalgaard wrote:

> "William H. Asquith" <wasquith at austin.rr.com> writes:
>
>> I am porting some FORTRAN to R in which an Inf triggers an if().  The
>> trigger is infinite on exp(lgamma(OVER)).  What is the canonical R
>> style of determining OVER when exp(OVER)== Inf?  The code structure
>> that I am
>> porting is best left intact--so I need to query R somehow to the value
>> of OVER that causes exp(lgamma(OVER)) to equal Inf.
>>
>> On my system,
>> exp(lgamma(171)) is about first to equal Inf.
>>
>> I asked similar question a few weeks ago on exp(OVER) and got the
>> answer back as log(.Machine$double.xmax).  I now have the lgamma
>> involved.  I think that answer is what is OVER such the
>>
>> .Machine$double.xmax = lgamma(OVER),
>
> Not quite... (see below)
>
>> but I am not sure how to invert or solve for OVER
>
>
>> uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), c(171,172))
> $root
> [1] 171.6244
>
> $f.root
> [1] -1.462051e-07
>
> $iter
> [1] 3
>
> $estim.prec
> [1] 6.103516e-05
>
>
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 
> 35327907
>



From pberming at research.ryerson.ca  Mon Jul  4 02:58:39 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Sun, 3 Jul 2005 20:58:39 -0400
Subject: [R] Windows compile
Message-ID: <EC3EEED49432A54990181E8E8B47072754957F@mail2.arts.ryerson.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050703/de25659f/attachment.pl

From murdoch at stats.uwo.ca  Mon Jul  4 03:13:44 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 03 Jul 2005 21:13:44 -0400
Subject: [R] Windows compile
In-Reply-To: <EC3EEED49432A54990181E8E8B47072754957F@mail2.arts.ryerson.ca>
References: <EC3EEED49432A54990181E8E8B47072754957F@mail2.arts.ryerson.ca>
Message-ID: <42C88D48.1080003@stats.uwo.ca>

Philip Bermingham wrote:
> I'm trying to compile R on Windows 2003 Server and following the
> instruction laid out in R inst and admin manual I continue to get this
> error:
> 
>  
> 
> make: ./Rpwd.exe: Command not found
> 
> make[1]: ./Rpwd.exe: Command not found
> 
> /cygdrive/d/rp/tools/bin/make --no-print-directory -C front-ends Rpwd
> 
> /cygdrive/d/rp/tools/bin/make -C ../../include -f Makefile.win version
> 
> make[3]: sh.exe: Command not found

sh.exe is one of the programs in the tools collection, so it looks as 
though you don't have that on your path.  Getting the path right is 
important.  A description of what you need in the path is in the R 
Installation and Administration manual.

Duncan Murdoch



From Massimiliano.Talarico at xelion.unicredit.it  Mon Jul  4 09:15:48 2005
From: Massimiliano.Talarico at xelion.unicredit.it (Talarico Massimiliano (Xelion))
Date: Mon, 4 Jul 2005 09:15:48 +0200
Subject: [R] RESIDUALS IN THE AR TIME SERIES FUNCTION
Message-ID: <1048FBEFA7413B42911B866B0A567CFD0B889958@USEBW103.mailasp.unicredit.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050704/60434cf6/attachment.pl

From maechler at stat.math.ethz.ch  Mon Jul  4 09:33:31 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 4 Jul 2005 09:33:31 +0200
Subject: [R] over/under flow
In-Reply-To: <366f191fef2b1ea7ada087f41fad0d50@austin.rr.com>
References: <713ad6faf89e4d440bf59c59aebd088d@austin.rr.com>
	<x2d5pzipsv.fsf@turmalin.kubism.ku.dk>
	<366f191fef2b1ea7ada087f41fad0d50@austin.rr.com>
Message-ID: <17096.58955.566233.302716@stat.math.ethz.ch>

>>>>> "William" == William H Asquith <wasquith at austin.rr.com>
>>>>>     on Sun, 3 Jul 2005 16:00:40 -0500 writes:

    William> Great, but to followup, how do I select the bounds (B,E) on the root 
    William> for an arbitrary machine?

    William> OVER = uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), 
    William> c(B,E))$root

    William> If uniroot() is fast enough, is it appropriate for me to set B at say 1 
    William> and E at log(.Machine$double.max)?  Suggestions on do this the proper 
    William> "R way"?  Perhaps this . . .

    William> OVER = uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), 
    William> c(1,log(.Machine$double.xmax)))$root

    William> I am working on a package so different machines will be involved thus 
    William> simple 171,172 might not be the best idea for the root?

Well,

1) We nowadays *require* compliance with
the usual ``IEEE arithmetic standard''  {put a bit too simplistically}
and I'd bet that those compiler / runtime library combinations
that can build R properly, would also give cutoffs in this same interval

2) lgamma(x) is very linear ``out there'' -- try
      curve(lgamma(x) - log(.Machine$double.xmax), 170, 180)
   --    so you can easily enlarge the interval a bit, e.g. to
   (170,175) or even to (100,200)

Martin


    William> On Jul 3, 2005, at 3:43 PM, Peter Dalgaard wrote:

    >> "William H. Asquith" <wasquith at austin.rr.com> writes:
    >> 
    >>> I am porting some FORTRAN to R in which an Inf triggers an if().  The
    >>> trigger is infinite on exp(lgamma(OVER)).  What is the canonical R
    >>> style of determining OVER when exp(OVER)== Inf?  The code structure
    >>> that I am
    >>> porting is best left intact--so I need to query R somehow to the value
    >>> of OVER that causes exp(lgamma(OVER)) to equal Inf.
    >>> 
    >>> On my system,
    >>> exp(lgamma(171)) is about first to equal Inf.
    >>> 
    >>> I asked similar question a few weeks ago on exp(OVER) and got the
    >>> answer back as log(.Machine$double.xmax).  I now have the lgamma
    >>> involved.  I think that answer is what is OVER such the
    >>> 
    >>> .Machine$double.xmax = lgamma(OVER),
    >> 
    >> Not quite... (see below)
    >> 
    >>> but I am not sure how to invert or solve for OVER
    >> 
    >> 
    >>> uniroot(function(x) lgamma(x)-log(.Machine$double.xmax), c(171,172))
    >> $root
    >> [1] 171.6244
    >> 
    >> $f.root
    >> [1] -1.462051e-07
    >> 
    >> $iter
    >> [1] 3
    >> 
    >> $estim.prec
    >> [1] 6.103516e-05
    >> 
    >> 
    >> -- 
    >> O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
    >> c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
    >> (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 
    >> 35327918
    >> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 
    >> 35327907
    >> 

    William> ______________________________________________
    William> R-help at stat.math.ethz.ch mailing list
    William> https://stat.ethz.ch/mailman/listinfo/r-help
    William> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


    William> !DSPAM:42c852c3295961260279199!



From ggrothendieck at gmail.com  Mon Jul  4 10:43:44 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Jul 2005 04:43:44 -0400
Subject: [R] RESIDUALS IN THE AR TIME SERIES FUNCTION
In-Reply-To: <1048FBEFA7413B42911B866B0A567CFD0B889958@USEBW103.mailasp.unicredit.it>
References: <1048FBEFA7413B42911B866B0A567CFD0B889958@USEBW103.mailasp.unicredit.it>
Message-ID: <971536df05070401436566fe0@mail.gmail.com>

On 7/4/05, Talarico Massimiliano (Xelion)
<Massimiliano.Talarico at xelion.unicredit.it> wrote:
> Dear all,
> 
> It's possible to obtain the residuals with AR time series function.

There is no residuals method for the "ar" class but you could
try this using the lh dataset as an example.  Note that the
zoo package is required in order to use zoo's lag function,
which has been enhanced to take a vector of lags, and dyn 
of the dyn package is used to add dynamic regression 
capabilities to lm.  Also, this example uses features in
the most recent versions of the dyn and zoo packages
so be sure you have the latest ones from CRAN.

ar(lh)
library(dyn) # this also brings in zoo
lh.zoo <- as.zoo(lh)
lh.lm <- dyn$lm(lh.zoo ~ lag(lh.zoo, -seq(3)))
lh.lm # seems close to the coefficients calculated by ar
resid(lh.lm)



From Achim.Zeileis at wu-wien.ac.at  Mon Jul  4 10:56:13 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 4 Jul 2005 10:56:13 +0200 (CEST)
Subject: [R] RESIDUALS IN THE AR TIME SERIES FUNCTION
In-Reply-To: <1048FBEFA7413B42911B866B0A567CFD0B889958@USEBW103.mailasp.unicredit.it>
References: <1048FBEFA7413B42911B866B0A567CFD0B889958@USEBW103.mailasp.unicredit.it>
Message-ID: <Pine.LNX.4.58.0507041051230.14354@thorin.ci.tuwien.ac.at>

On Mon, 4 Jul 2005, Talarico Massimiliano (Xelion) wrote:

> Dear all,
>
> It's possible to obtain the residuals with AR time series function.

Yes, that's true if you refer to the function ar().
Or was it a question? If so, look at the `value' section of
  help(ar)
which gives you the answer.
Z

>
>
> Thanks in advance,
>
> Massimiliano Talarico
>
> Pianificazione Commerciale - Area Crm
>
> Unicredit Xelion Banca S.p.A.
>
> Via Pirelli 32 - 20124 Milano
>
> Tel.: 02 67360 525
>
> Fax: 02 67738 525
>
> www.xelion.it
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Mon Jul  4 11:30:39 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Jul 2005 05:30:39 -0400
Subject: [R] Symbolic Maximum Likelihood in R
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407E41BB9@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407E41BB9@dc1ex2.air.org>
Message-ID: <971536df05070402306d9689ee@mail.gmail.com>

On 7/3/05, Doran, Harold <HDoran at air.org> wrote:
> Dear List:
> 
> Is any one aware of a package that would extend the D() function and allow for one to maximize a likelihood function symbolically? Something akin to Solve[x==0, parameter] function in Mathematica?
> 
> Clearly R has the capacity to _compute_ MLEs given a set of data. But, I'm looking for a package that would allow for me to define the likelihood function, find the 1st order partial derivative of this function (which can already be handled in D()), and then symbolically maximize this function?
> 

There are a number of free symbolic mathematics packages such as
Maxima and Yacas.  Here is a simple example in Yacas.  (Also
see http://finzi.psych.upenn.edu/R/Rhelp02a/archive/31418.html
for transferring functions from yacas to R).

C:\usr\yacas> yacas
[...various startup messages...]
In> f(x) := Exp(-(x-mu)^2)/Sqrt(2*Pi)
Out> True;
In> logf(x) := Ln(f(x));
Out> True;
In> loglik := logf(x1) + logf(x2) + logf(x3);
Out> Ln(Exp(-(x1-mu)^2)/Sqrt(2*Pi))+Ln(Exp(-(x2-mu)^2)/Sqrt(2*Pi))+Ln(Exp(-(x3-m
u)^2)/Sqrt(2*Pi));
In> score := D(mu) loglik;
Out> (-(-2)*(x2-mu)*Exp(-(x2-mu)^2)*2*Pi)/(2*Pi*Exp(-(x2-mu)^2))-((-2)*(x1-mu)*E
xp(-(x1-mu)^2)*2*Pi)/(2*Pi*Exp(-(x1-mu)^2))-((-2)*(x3-mu)*Exp(-(x3-mu)^2)*2*Pi)/
(2*Pi*Exp(-(x3-mu)^2));
In> score := Simplify(score);
Out> 2*(x2+(-3)*mu+x1+x3);
In> Solve(score == 0, mu);
Out> (x2+x1+x3)/3;



From dieter.menne at menne-biomed.de  Mon Jul  4 11:45:27 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 4 Jul 2005 11:45:27 +0200
Subject: [R] A faster way to aggregate?
Message-ID: <INEGIMHGODBGKFPOJBBMGEEJCEAA.dieter.menne@menne-biomed.de>

Dear List,

I have  a logical data frame with NA's and a grouping factor, and I want to
calculate
the % TRUE per column and group. With an indexed database, result are mainly
limited by printout time, but my R-solution below let's me wait (there are
about 10* cases in the real
data set).
Any suggestions to speed this up? Yes, I could wait for the result in real
life, but just curious if I did something wrong. In real life, data set is
ordered by groups, but how can I use this with a data frame?

Dieter Menne


# Generate test data
ncol = 20
nrow = 20000
ngroup=nrow %/% 20
colrow=ncol*nrow
group = factor(floor(runif(nrow)*ngroup))
sc = data.frame(group,matrix(ifelse(runif(colrow) >
0.1,runif(colrow)>0.3,NA),
     nrow=nrow))

# aggregate
system.time ({
 s = aggregate(sc[2:(ncol+1)],list(group = group),
    function(x) {
       xt=table(x)
       as.integer(100*xt[2]/(xt[1]+xt[2]))
    }
  )
})
# 26.09  0.03 26.95    NA    NA

# by and apply
system.time ({
  s = by (sc[2:(ncol+1)],group,function(x) {
     apply(x,2,function(x) {
         xt=table(x)
         as.integer(100*xt[2]/(xt[1]+xt[2]))
       }
     )
    })
  s=do.call("rbind",s)
})

# 82.89  0.18 85.16    NA    NA



From navarre_sabine at yahoo.fr  Mon Jul  4 11:45:39 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Mon, 4 Jul 2005 11:45:39 +0200 (CEST)
Subject: [R] compare two lists with differents levels
Message-ID: <20050704094539.7682.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050704/ce788ef6/attachment.pl

From alex at transitive.com  Mon Jul  4 12:19:16 2005
From: alex at transitive.com (Alex Brown)
Date: Mon, 4 Jul 2005 11:19:16 +0100
Subject: [R] Rotate legends or other approaches to nice legend placement?
Message-ID: <72ee23b6349ea315f918ccbd570e79f2@transitive.com>

I'm sure this general sort of question has been asked many times before 
- I would _like_ automatic and sensible legend placement in barplots so 
data is not overwritten... but since there doesn't seem to be one, one 
of the following would be useful:

One approach for this would be to place the legend to the right of the 
graph, and rotate it by 90 degrees.

Is there a sensible way to do this?

alternatively, is there a function to

1) estimate legend size
2) adjust nrows so that the full width of the drawing device is used, 
minimising height
3) use layout() so that enough space is allocated beneath the graph for 
the legend
4) draw legend
5) allow user to call plot, correctly drawing the plot in the remaining 
frame?

I have taken a look at this, but I am confused by the different units 
used by par(mar), legend(plot=F), and layout.

-Alex Brown



From f.calboli at imperial.ac.uk  Mon Jul  4 12:23:12 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 4 Jul 2005 11:23:12 +0100
Subject: [R]  loop over large dataset
References: <9EC93E2D-4248-427A-A8BA-898B9E22FDD2@imperial.ac.uk>
Message-ID: <CE4702D9-87B3-443C-BE6B-A7DF56EF1FA0@imperial.ac.uk>

In my absentmindedness I'd forgotten to CC this to the list... and  
BTW, using gc() in the loop increases the runtime...


>> My suggestion is that you try to vectorize the computation as much  
>> as you
>> can.
>>
>> From what you've shown, `new' and `ped' need to have the same  
>> number of
>> rows, right?
>>
>> Your `off' function seems to be randomly choosing between columns  
>> 1 and 2
>> from its two input matrices (one row each?).  You may want to do the
>> sampling all at once instead of looping over the rows.  E.g.,
>>
>>
>>
>>> (m <- matrix(1:10, ncol=2))
>>>
>>>
>>      [,1] [,2]
>> [1,]    1    6
>> [2,]    2    7
>> [3,]    3    8
>> [4,]    4    9
>> [5,]    5   10
>>
>>
>>> (colSample <- sample(1:2, nrow(m), replace=TRUE))
>>>
>>>
>> [1] 1 1 2 1 1
>>
>>
>>> (x <- m[cbind(1:nrow(m), colSample)])
>>>
>>>
>> [1] 1 2 8 4 5
>>
>> So you might want to do something like (obviously untested):
>>
>> todo <- ped[,3] * ped[,5] != 0  ## indicator of which rows to work on
>> n.todo <- sum(todo)  ## how many are there?
>> sire <- new[ped[todo, 3], ]
>> dam <- new[ped[todo, 5], ]
>> s.gam <- sire[1:nrow(sire), sample(1:2, nrow(sire), replace=TRUE)]
>> d.gam <- dam[1:nrow(dam), sample(1:2, nrow(dam), replace=TRUE)]
>> new[todo, 1:2] <- cbind(s.gam, d.gam)
>>
>>
>
> Improving the efficiency of the code is abviously a plus, but the  
> real thing I am mesmerised by is the sheer increase in runtime...  
> how come not a linear increase with dataset size?
>
> Cheers,
>
> Federico
>
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St. Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
>



From ggrothendieck at gmail.com  Mon Jul  4 13:12:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Jul 2005 07:12:58 -0400
Subject: [R] A faster way to aggregate?
In-Reply-To: <INEGIMHGODBGKFPOJBBMGEEJCEAA.dieter.menne@menne-biomed.de>
References: <INEGIMHGODBGKFPOJBBMGEEJCEAA.dieter.menne@menne-biomed.de>
Message-ID: <971536df0507040412272352c4@mail.gmail.com>

On 7/4/05, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Dear List,
> 
> I have  a logical data frame with NA's and a grouping factor, and I want to
> calculate
> the % TRUE per column and group. With an indexed database, result are mainly
> limited by printout time, but my R-solution below let's me wait (there are
> about 10* cases in the real
> data set).
> Any suggestions to speed this up? Yes, I could wait for the result in real
> life, but just curious if I did something wrong. In real life, data set is
> ordered by groups, but how can I use this with a data frame?
> 
> Dieter Menne
> 
> 
> # Generate test data
> ncol = 20
> nrow = 20000
> ngroup=nrow %/% 20
> colrow=ncol*nrow
> group = factor(floor(runif(nrow)*ngroup))
> sc = data.frame(group,matrix(ifelse(runif(colrow) >
> 0.1,runif(colrow)>0.3,NA),
>     nrow=nrow))
> 
> # aggregate
> system.time ({
>  s = aggregate(sc[2:(ncol+1)],list(group = group),
>    function(x) {
>       xt=table(x)
>       as.integer(100*xt[2]/(xt[1]+xt[2]))
>    }
>  )
> })
> # 26.09  0.03 26.95    NA    NA
> 
> # by and apply
> system.time ({
>  s = by (sc[2:(ncol+1)],group,function(x) {
>     apply(x,2,function(x) {
>         xt=table(x)
>         as.integer(100*xt[2]/(xt[1]+xt[2]))
>       }
>     )
>    })
>  s=do.call("rbind",s)
> })
> 
> # 82.89  0.18 85.16    NA    NA
> 

Look at ?rowsum



From ligges at statistik.uni-dortmund.de  Mon Jul  4 13:41:04 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Jul 2005 13:41:04 +0200
Subject: [R] loop over large dataset
In-Reply-To: <CE4702D9-87B3-443C-BE6B-A7DF56EF1FA0@imperial.ac.uk>
References: <9EC93E2D-4248-427A-A8BA-898B9E22FDD2@imperial.ac.uk>
	<CE4702D9-87B3-443C-BE6B-A7DF56EF1FA0@imperial.ac.uk>
Message-ID: <42C92050.5040108@statistik.uni-dortmund.de>

Federico Calboli wrote:

> In my absentmindedness I'd forgotten to CC this to the list... and  
> BTW, using gc() in the loop increases the runtime...

If the data size increases, you cannot expect linear run time behaviour, 
e.g. because gc() is called more frequently. And of course, gc() needs 
some time, hence you get the expected increase in runtime. This answers 
you other question as well.

Uwe Ligges


>>>My suggestion is that you try to vectorize the computation as much  
>>>as you
>>>can.
>>>
>>>From what you've shown, `new' and `ped' need to have the same  
>>>number of
>>>rows, right?
>>>
>>>Your `off' function seems to be randomly choosing between columns  
>>>1 and 2
>>>from its two input matrices (one row each?).  You may want to do the
>>>sampling all at once instead of looping over the rows.  E.g.,
>>>
>>>
>>>
>>>
>>>>(m <- matrix(1:10, ncol=2))
>>>>
>>>>
>>>
>>>     [,1] [,2]
>>>[1,]    1    6
>>>[2,]    2    7
>>>[3,]    3    8
>>>[4,]    4    9
>>>[5,]    5   10
>>>
>>>
>>>
>>>>(colSample <- sample(1:2, nrow(m), replace=TRUE))
>>>>
>>>>
>>>
>>>[1] 1 1 2 1 1
>>>
>>>
>>>
>>>>(x <- m[cbind(1:nrow(m), colSample)])
>>>>
>>>>
>>>
>>>[1] 1 2 8 4 5
>>>
>>>So you might want to do something like (obviously untested):
>>>
>>>todo <- ped[,3] * ped[,5] != 0  ## indicator of which rows to work on
>>>n.todo <- sum(todo)  ## how many are there?
>>>sire <- new[ped[todo, 3], ]
>>>dam <- new[ped[todo, 5], ]
>>>s.gam <- sire[1:nrow(sire), sample(1:2, nrow(sire), replace=TRUE)]
>>>d.gam <- dam[1:nrow(dam), sample(1:2, nrow(dam), replace=TRUE)]
>>>new[todo, 1:2] <- cbind(s.gam, d.gam)
>>>
>>>
>>
>>Improving the efficiency of the code is abviously a plus, but the  
>>real thing I am mesmerised by is the sheer increase in runtime...  
>>how come not a linear increase with dataset size?
>>
>>Cheers,
>>
>>Federico
>>
>>--
>>Federico C. F. Calboli
>>Department of Epidemiology and Public Health
>>Imperial College, St. Mary's Campus
>>Norfolk Place, London W2 1PG
>>
>>Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>>
>>f.calboli [.a.t] imperial.ac.uk
>>f.calboli [.a.t] gmail.com
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Massimiliano.Talarico at xelion.unicredit.it  Mon Jul  4 13:40:45 2005
From: Massimiliano.Talarico at xelion.unicredit.it (Talarico Massimiliano (Xelion))
Date: Mon, 4 Jul 2005 13:40:45 +0200
Subject: [R] To Predict for a vector
Message-ID: <1048FBEFA7413B42911B866B0A567CFD0B8E86D4@USEBW103.mailasp.unicredit.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050704/51efe67b/attachment.pl

From dieter.menne at menne-biomed.de  Mon Jul  4 14:49:22 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 4 Jul 2005 12:49:22 +0000 (UTC)
Subject: [R]  A faster way to aggregate?
References: <INEGIMHGODBGKFPOJBBMGEEJCEAA.dieter.menne@menne-biomed.de>
	<971536df0507040412272352c4@mail.gmail.com>
Message-ID: <loom.20050704T144557-259@post.gmane.org>

My Original question (edited)

> I have  a logical data frame with NA's and a grouping factor, and I want to
> calculate
> the % TRUE per column and group. With an indexed database, result are mainly
> limited by printout time, but my R-solution below lets me wait.
> Any suggestions to speed this up? 

Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:

> Look at ?rowsum

Nearby colMeans works, but why so slow?

Dieter Menne

# Generate test data
ncol = 20
nrow = 20000
ngroup=nrow %/% 20
colrow=ncol*nrow
group = factor(floor(runif(nrow)*ngroup))
sc = data.frame(group,matrix(ifelse(runif(colrow) > 0.1,runif(colrow)>0.3,NA),
     nrow=nrow))

# aggregate (still best)
system.time ({
 s = aggregate(sc[2:(ncol+1)],list(group = group),
    function(x) {
       xt=table(x)
       as.integer(100*xt[2]/(xt[1]+xt[2]))
    }
  )
})
# 26.09  0.03 26.95    NA    NA

# by and apply
system.time ({
  s1 = by (sc[2:(ncol+1)],group,function(x) {
     as.integer(100*colMeans(x,na.rm=T))

    })
  s1=as.data.frame(do.call("rbind",s))
})

#  51.49  0.93 52.60    NA    NA



From f.calboli at imperial.ac.uk  Mon Jul  4 15:29:38 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 4 Jul 2005 14:29:38 +0100
Subject: [R] loop over large dataset
In-Reply-To: <42C92050.5040108@statistik.uni-dortmund.de>
References: <9EC93E2D-4248-427A-A8BA-898B9E22FDD2@imperial.ac.uk>
	<CE4702D9-87B3-443C-BE6B-A7DF56EF1FA0@imperial.ac.uk>
	<42C92050.5040108@statistik.uni-dortmund.de>
Message-ID: <75450FD4-0EB6-4831-BA87-6BF5EB590960@imperial.ac.uk>


On 4 Jul 2005, at 12:41, Uwe Ligges wrote:

> Federico Calboli wrote:
>
>
>> In my absentmindedness I'd forgotten to CC this to the list...  
>> and  BTW, using gc() in the loop increases the runtime...
>>
>
> If the data size increases, you cannot expect linear run time  
> behaviour, e.g. because gc() is called more frequently. And of  
> course, gc() needs some time, hence you get the expected increase  
> in runtime. This answers you other question as well.

Is then internal gc() calls that increase the runtime from 5 minutes  
to more then 24 hours for a 27x increase in data (given that the code  
is exactely the same)?

Federico

--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From Dubravko.Dolic at komdat.com  Mon Jul  4 15:50:34 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Mon, 4 Jul 2005 15:50:34 +0200
Subject: [R] RMySQL typing Problem (bigint unsigned)
Message-ID: <52D1AC81378E9342947189B04176014728F963@agentsmith.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050704/fa70de22/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Jul  4 16:15:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jul 2005 16:15:27 +0200
Subject: [R] loop over large dataset
In-Reply-To: <75450FD4-0EB6-4831-BA87-6BF5EB590960@imperial.ac.uk>
References: <9EC93E2D-4248-427A-A8BA-898B9E22FDD2@imperial.ac.uk>
	<CE4702D9-87B3-443C-BE6B-A7DF56EF1FA0@imperial.ac.uk>
	<42C92050.5040108@statistik.uni-dortmund.de>
	<75450FD4-0EB6-4831-BA87-6BF5EB590960@imperial.ac.uk>
Message-ID: <x2wto63beo.fsf@turmalin.kubism.ku.dk>

Federico Calboli <f.calboli at imperial.ac.uk> writes:

> > behaviour, e.g. because gc() is called more frequently. And of  
> > course, gc() needs some time, hence you get the expected increase  
> > in runtime. This answers you other question as well.
> 
> Is then internal gc() calls that increase the runtime from 5 minutes  
> to more then 24 hours for a 27x increase in data (given that the code  
> is exactely the same)?

Your original code got lost in the threading, but that order of
magnitude suggests that you have N^2/2 behaviour somewhere. The typical
culprit is code like

x <- numeric(0)
for (i in 1:N){
  newx <- <<....>>
  x <- c(x, newx)
} 

in which the extension of x causes the whole thing to be reallocated
and copied. Same thing with cbind and rbind constructs of course.



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From f.calboli at imperial.ac.uk  Mon Jul  4 16:22:37 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 4 Jul 2005 15:22:37 +0100
Subject: [R] loop over large dataset
In-Reply-To: <x2wto63beo.fsf@turmalin.kubism.ku.dk>
References: <9EC93E2D-4248-427A-A8BA-898B9E22FDD2@imperial.ac.uk>
	<CE4702D9-87B3-443C-BE6B-A7DF56EF1FA0@imperial.ac.uk>
	<42C92050.5040108@statistik.uni-dortmund.de>
	<75450FD4-0EB6-4831-BA87-6BF5EB590960@imperial.ac.uk>
	<x2wto63beo.fsf@turmalin.kubism.ku.dk>
Message-ID: <BD733DD8-8232-4C7E-BAA9-0F91D01783E7@imperial.ac.uk>


On 4 Jul 2005, at 15:15, Peter Dalgaard wrote:
>
> Your original code got lost in the threading, but that order of
> magnitude suggests that you have N^2/2 behaviour somewhere. The  
> typical
> culprit is code like
>
> x <- numeric(0)
> for (i in 1:N){
>   newx <- <<....>>
>   x <- c(x, newx)
> }
>
> in which the extension of x causes the whole thing to be reallocated
> and copied. Same thing with cbind and rbind constructs of course.


I changed my code a bit, and now the runtime is dow to less than a  
minute (from more than 24 hours). I was copying a large dataset many  
times over, when I extracted the columns I need as independet vectors  
runtime dropped like a stone.

Cheers,

Federico

--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From sekemp at glam.ac.uk  Mon Jul  4 16:45:35 2005
From: sekemp at glam.ac.uk (Samuel E. Kemp)
Date: Mon, 4 Jul 2005 15:45:35 +0100
Subject: [R] partial autoregression matrix function
Message-ID: <c296af57cb4d0978f8f73fab0e227afe@glam.ac.uk>

Hi,

Does anyone know of a function in R that is capable of calculating the 
partial autoregression matrix function for vector autoregressive moving 
average (VARMA) models?

The dse package has functions capable of simulating and estimating 
VARMA models, but I did not notice a function for model identification.

Any help would be greatly appreciated.

Kind regards,

Sam.



From murdoch at stats.uwo.ca  Mon Jul  4 16:53:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 04 Jul 2005 10:53:58 -0400
Subject: [R] RMySQL typing Problem (bigint unsigned)
In-Reply-To: <52D1AC81378E9342947189B04176014728F963@agentsmith.komdat.intern>
References: <52D1AC81378E9342947189B04176014728F963@agentsmith.komdat.intern>
Message-ID: <42C94D86.9000703@stats.uwo.ca>

On 7/4/2005 9:50 AM, Dubravko Dolic wrote:
> Dear Group,
> 
>  
> 
> if anyone has experience with the RMySQL Package maybe this behaviour is know:
> 
>  
> 
> Reading data from a table into R the fields with datatype bigint(20) unsigned are transformed in some way: e.g. the query "select * from orders where userid = 14929859848712890325" selects the correct case but in R the userid is changed to 14929859848712890368. What happened here? This transformation is true for all fields of that type...

R doesn't have a bigint type, so I imagine these are being changed to 
doubles.  In double precision those are the same number.

I don't know the best way to handle this, but one way would be to do SQL 
calculations to extract the lower 10 digits separately from the upper 10 
digits.  R doubles can represent 10 digit integers exactly.

Duncan Murdoch



From ggrothendieck at gmail.com  Mon Jul  4 17:22:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Jul 2005 11:22:02 -0400
Subject: [R] A faster way to aggregate?
In-Reply-To: <loom.20050704T144557-259@post.gmane.org>
References: <INEGIMHGODBGKFPOJBBMGEEJCEAA.dieter.menne@menne-biomed.de>
	<971536df0507040412272352c4@mail.gmail.com>
	<loom.20050704T144557-259@post.gmane.org>
Message-ID: <971536df050704082274de15e8@mail.gmail.com>

On 7/4/05, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> My Original question (edited)
> 
> > I have  a logical data frame with NA's and a grouping factor, and I want to
> > calculate
> > the % TRUE per column and group. With an indexed database, result are mainly
> > limited by printout time, but my R-solution below lets me wait.
> > Any suggestions to speed this up?
> 
> Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:
> 
> > Look at ?rowsum
> 
> Nearby colMeans works, but why so slow?
> 
> Dieter Menne
> 
> # Generate test data
> ncol = 20
> nrow = 20000
> ngroup=nrow %/% 20
> colrow=ncol*nrow
> group = factor(floor(runif(nrow)*ngroup))
> sc = data.frame(group,matrix(ifelse(runif(colrow) > 0.1,runif(colrow)>0.3,NA),
>     nrow=nrow))
> 
> # aggregate (still best)
> system.time ({
>  s = aggregate(sc[2:(ncol+1)],list(group = group),
>    function(x) {
>       xt=table(x)
>       as.integer(100*xt[2]/(xt[1]+xt[2]))
>    }
>  )
> })
> # 26.09  0.03 26.95    NA    NA
> 
> # by and apply
> system.time ({
>  s1 = by (sc[2:(ncol+1)],group,function(x) {
>     as.integer(100*colMeans(x,na.rm=T))
> 
>    })
>  s1=as.data.frame(do.call("rbind",s))
> })
> 
> #  51.49  0.93 52.60    NA    NA
> 

Note that you did not actually try my suggestion which was rowsum,
not colMeans.

The following solution based on rowsum is more than
an order of magnitude faster than any of the solutions in your
posts:

	sc1 <- as.matrix(sc[,-1])
	is.na.sc1 <- is.na(sc1)
	x1 <- rowsum(ifelse(is.na.sc1, 0, sc1), group)
	xx <- rowsum(1-is.na.sc1, group)
	res <- floor(100*x1/xx)



From MSchwartz at mn.rr.com  Mon Jul  4 17:25:14 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 04 Jul 2005 10:25:14 -0500
Subject: [R] Rotate legends or other approaches to nice legend	placement?
In-Reply-To: <72ee23b6349ea315f918ccbd570e79f2@transitive.com>
References: <72ee23b6349ea315f918ccbd570e79f2@transitive.com>
Message-ID: <1120490715.8580.23.camel@localhost.localdomain>

On Mon, 2005-07-04 at 11:19 +0100, Alex Brown wrote:
> I'm sure this general sort of question has been asked many times before 
> - I would _like_ automatic and sensible legend placement in barplots so 
> data is not overwritten... but since there doesn't seem to be one, one 
> of the following would be useful:
> 
> One approach for this would be to place the legend to the right of the 
> graph, and rotate it by 90 degrees.
> 
> Is there a sensible way to do this?
> 
> alternatively, is there a function to
> 
> 1) estimate legend size
> 2) adjust nrows so that the full width of the drawing device is used, 
> minimising height
> 3) use layout() so that enough space is allocated beneath the graph for 
> the legend
> 4) draw legend
> 5) allow user to call plot, correctly drawing the plot in the remaining 
> frame?
> 
> I have taken a look at this, but I am confused by the different units 
> used by par(mar), legend(plot=F), and layout.
> 
> -Alex Brown

For placing the legend outside the plot region, see this post from just
a few days ago:

https://stat.ethz.ch/pipermail/r-help/2005-July/073207.html


In terms of automating the process, that usually means making some
assumptions and then writing code to fit the assumptions, while
providing options to handle the cases that don't.

Briefly, one approach to automating legend placement within the plot
region might be something like this. Create a new function we'll call
barplotL() (not feeling overly creative this morning....). 

Basically, it figures out the maximum y value from the height argument
and multiplies that by 1.5 to provide extra room at the top of the plot
region for the legend. You can of course adjust this factor as required
(ie. less room is needed for a horizontal legend).

For the upper left hand corner (ULHC) of the legend, it takes the range
of the x and y axes and then places the UHLC at 5%/95% of the respective
ranges from the UHLC of the plot region. See ?par (specifically 'usr').

It provides options for coloring the legend boxes (in lieu of the
default grey) and for making the legend horizontal instead of vertical. 

You can add other options as well, but this should get you started.

BTW, this approach presumes that 'height' will be a matrix, since I am
not sure that a legend makes sense otherwise...


barplotL <- function(height, beside = FALSE, 
                     legend = NULL, col = NULL,
                     leg.horiz = FALSE)
{
  ylim <- ifelse(beside, 
                 max(height) * 1.5, 
                 max(colSums(height) * 1.5)) 

  barplot(height = height, ylim = c(0, ylim), 
          beside = beside, col = col)

  x.pos <- par("usr")[1] + ((par("usr")[2] - par("usr")[1]) * .05)
  y.pos <- par("usr")[4] - ((par("usr")[4] - par("usr")[3]) * .05)

  if(is.null(col))
    col <- grey.colors(nrow(height))

  if (is.null(legend))
    legend <- rownames(height) 

  legend(x.pos, y.pos, legend = legend, fill = col,
         horiz = leg.horiz)
}



So, let's try it:


barplotL(VADeaths, beside = FALSE)

barplotL(VADeaths, beside = FALSE, leg.horiz = TRUE)

barplotL(VADeaths, beside = TRUE, 
         col = c("red", "yellow", "orange"))

barplotL(VADeaths, beside = TRUE, leg.horiz = TRUE, 
         col = c("red", "yellow", "orange"))


You can also look at the smartlegend() function in the gplots package on
CRAN, but you still need to adjust the y axis ranges as above to make
room for the legend itself.

HTH,

Marc Schwartz



From pigood at verizon.net  Mon Jul  4 17:28:45 2005
From: pigood at verizon.net (Phillip Good)
Date: Mon, 04 Jul 2005 08:28:45 -0700
Subject: [R] Lack of independence in anova()
Message-ID: <GHEKKACNLEADPKCNEEDFOEGMCEAA.pigood@verizon.net>

 If the observations are normally distributed and the 2xk design is
balanced,  theory requires that the tests for interaction and row effects be
independent.  In my program, appended below, this would translate to cntT
(approx)= cntR*cntI/N if all R routines were functioning correctly.  They
aren't.

sim2=function(size,N,p){
  cntR=0
  cntC=0
  cntI=0
  cntT=0
  cntP=0
  for(i in 1:N){
    #generate data
     v=gendata(size)
    #analyze after build(ing) design containing data
     lm.out=lm(yield~c*r,build(size,v))
     av.out=anova(lm.out)
    #if column effect is significant, increment cntC
     if (av.out[[5]][1]<=p)cntC=cntC+1
   #if row effect is significant, increment cntR
     if (av.out[[5]][2]<=p){
           cntR=cntR+1
	   tmp = 1
	   }
     else tmp =0 		
     if (av.out[[5]][3]<=p){
           #if interaction is significant, increment cntI
            cntI=cntI+1
	#if both interaction and row effect are significant, increment cntT
            cntT=cntT + tmp
            } 
     }
    list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
}

build=function(size,v){
#size is a vector containing the sample sizes
col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
+size[7]+size[8]))
return(data.frame(c=factor(col), r=factor(row),yield=v))
}

gendata=function(size){
  ssize=sum(size);
  return (rnorm(ssize))
}

#Example
 size=c(3,3,3,0,3,3,3,0)
 sim2(size,10000,10,.16)



Phillip Good
Huntington Beach CA


From dieter.menne at menne-biomed.de  Mon Jul  4 17:43:11 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 4 Jul 2005 15:43:11 +0000 (UTC)
Subject: [R] A faster way to aggregate?
References: <INEGIMHGODBGKFPOJBBMGEEJCEAA.dieter.menne@menne-biomed.de>
	<971536df0507040412272352c4@mail.gmail.com>
	<loom.20050704T144557-259@post.gmane.org>
	<971536df050704082274de15e8@mail.gmail.com>
Message-ID: <loom.20050704T174025-516@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> gmail.com> writes:

> Note that you did not actually try my suggestion which was rowsum,
> not colMeans.

Mea culpa, I was hooked by rowSums, and so I was did not get aware of the 
grouping facility of rowsum().

 
> The following solution based on rowsum is more than
> an order of magnitude faster than any of the solutions in your
> posts:
> 
> 	sc1 <- as.matrix(sc[,-1])
> 	is.na.sc1 <- is.na(sc1)
> 	x1 <- rowsum(ifelse(is.na.sc1, 0, sc1), group)
> 	xx <- rowsum(1-is.na.sc1, group)
> 	res <- floor(100*x1/xx)

Thanks a lot.

Dieter



From pberming at research.ryerson.ca  Mon Jul  4 17:52:13 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Mon, 04 Jul 2005 11:52:13 -0400
Subject: [R] Windows compile
In-Reply-To: <42C93307.4070506@stats.uwo.ca>
References: <EC3EEED49432A54990181E8E8B47072754957F@mail2.arts.ryerson.ca>
	<42C88D48.1080003@stats.uwo.ca> <42C930DE.3090705@csca.ryerson.ca>
	<42C93307.4070506@stats.uwo.ca>
Message-ID: <42C95B2D.2070907@csca.ryerson.ca>

That worked great, thank you.  but it seems I have a new error occurring 
after the zipping of the help files:

hhc: not found
cp: cannot stat `C:/R/R-2.1.1/src/library/base/chm/base.chm': No such 
file or di
rectory
make[3]: *** [chm-base] Error 1
make[2]: *** [pkg-base] Error 2
make[1]: *** [rpackage] Error 2
make: *** [all] Error 2

Any advise?
Philip Bermingham

Duncan Murdoch wrote:

> On 7/4/2005 8:51 AM, Philip Bermingham wrote:
>
>> Where would I specify the path to this file? I checked the mkrules 
>> file but it doesn't mention it there.  
>
>
> The path is the system path.  You specify it in your shell, before 
> starting make, or in one of the Control Panel settings (if you want to 
> make the change system wide).
>
> I'd suggest writing a batch file that sets the path, and run it before 
> a build.  Then you don't need to worry about side effects of changes 
> on other programs.
>
> It will look something like this:
>
> path=.;f:\r\tools\bin;f:\perl\bin;f:\minGW\bin;f:\cygwin\bin;c:\util\misc;c:\windows\command;F:\texmf\miktex\bin;c:\progra~1\htmlhe~1 
>
>
> but this is from a very old batch file (I don't use the Windows shell 
> any more), and of course the paths to the programs will likely be 
> different on your system than on mine.
>
>> Also I did a search for Rpwd.exe and it seems I don't have that 
>> file.  I have Rpwd.c.  It seems I'm missing an important step in the 
>> build processes, but I don't know what it is.
>
>
> Rpwd.exe is made in one of the first steps of the build process.  What 
> you missed is setting up the path so that the build can proceed.
>
> Duncan Murdoch
>
>>
>> Philip.
>>
>> Duncan Murdoch wrote:
>>
>>> Philip Bermingham wrote:
>>>
>>>> I'm trying to compile R on Windows 2003 Server and following the
>>>> instruction laid out in R inst and admin manual I continue to get this
>>>> error:
>>>>
>>>>  
>>>>
>>>> make: ./Rpwd.exe: Command not found
>>>>
>>>> make[1]: ./Rpwd.exe: Command not found
>>>>
>>>> /cygdrive/d/rp/tools/bin/make --no-print-directory -C front-ends Rpwd
>>>>
>>>> /cygdrive/d/rp/tools/bin/make -C ../../include -f Makefile.win version
>>>>
>>>> make[3]: sh.exe: Command not found
>>>
>>>
>>>
>>> sh.exe is one of the programs in the tools collection, so it looks 
>>> as though you don't have that on your path.  Getting the path right 
>>> is important.  A description of what you need in the path is in the 
>>> R Installation and Administration manual.
>>>
>>> Duncan Murdoch
>>>
>>>
>
>



From dmbates at gmail.com  Mon Jul  4 18:12:46 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 4 Jul 2005 11:12:46 -0500
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFOEGMCEAA.pigood@verizon.net>
References: <GHEKKACNLEADPKCNEEDFOEGMCEAA.pigood@verizon.net>
Message-ID: <40e66e0b05070409122d9d1de6@mail.gmail.com>

I have already had email exchanges off-list with Phillip Good pointing
out that the independence property that he wishes to establish by
simulation is a consequence of orthogonality of the column span of the
row contrasts and the interaction contrasts.  If you set the contrasts
option to a set of orthogonal contrasts such as contr.helmert or
contr.sum, which has no effect on the results of the anova, this is
easily established.

> build
function(size, v = rnorm(sum(size))) {
    col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
    rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
    row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
    +size[7]+size[8]))
    return(data.frame(c=factor(col), r=factor(row),yield=v))
}
> size
[1] 3 3 3 0 3 3 3 0
> set.seed(1234321)
> vv <- build(size)
> vv
   c r      yield
1  0 0  1.2369081
2  0 0  1.5616230
3  0 0  1.8396185
4  1 0  0.3173245
5  1 0  1.0715115
6  1 0 -1.1459955
7  2 0  0.2488894
8  2 0  0.1158625
9  2 0  2.6200816
10 0 1  1.2624048
11 0 1 -0.9862578
12 0 1 -0.3235653
13 1 1  0.2039706
14 1 1 -1.4574576
15 1 1  1.9158713
16 2 1 -2.0333909
17 2 1  1.0050272
18 2 1  0.6789184
> options(contrasts = c('contr.helmert', 'contr.poly'))
> crossprod(model.matrix(~c*r, vv))
            (Intercept) c1 c2 r1 c1:r1 c2:r1
(Intercept)          18  0  0  0     0     0
c1                    0 12  0  0     0     0
c2                    0  0 36  0     0     0
r1                    0  0  0 18     0     0
c1:r1                 0  0  0  0    12     0
c2:r1                 0  0  0  0     0    36


On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>  If the observations are normally distributed and the 2xk design is
> balanced,  theory requires that the tests for interaction and row effects be
> independent.  In my program, appended below, this would translate to cntT
> (approx)= cntR*cntI/N if all R routines were functioning correctly.  They
> aren't.
> 
> sim2=function(size,N,p){
>   cntR=0
>   cntC=0
>   cntI=0
>   cntT=0
>   cntP=0
>   for(i in 1:N){
>     #generate data
>      v=gendata(size)
>     #analyze after build(ing) design containing data
>      lm.out=lm(yield~c*r,build(size,v))
>      av.out=anova(lm.out)
>     #if column effect is significant, increment cntC
>      if (av.out[[5]][1]<=p)cntC=cntC+1
>    #if row effect is significant, increment cntR
>      if (av.out[[5]][2]<=p){
>            cntR=cntR+1
>            tmp = 1
>            }
>      else tmp =0
>      if (av.out[[5]][3]<=p){
>            #if interaction is significant, increment cntI
>             cntI=cntI+1
>         #if both interaction and row effect are significant, increment cntT
>             cntT=cntT + tmp
>             }
>      }
>     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
> }
> 
> build=function(size,v){
> #size is a vector containing the sample sizes
> col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
> rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
> row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
> +size[7]+size[8]))
> return(data.frame(c=factor(col), r=factor(row),yield=v))
> }
> 
> gendata=function(size){
>   ssize=sum(size);
>   return (rnorm(ssize))
> }
> 
> #Example
>  size=c(3,3,3,0,3,3,3,0)
>  sim2(size,10000,10,.16)
> 
> 
> 
> Phillip Good
> Huntington Beach CA
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From dmbates at gmail.com  Mon Jul  4 18:24:26 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 4 Jul 2005 11:24:26 -0500
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFOEGMCEAA.pigood@verizon.net>
References: <GHEKKACNLEADPKCNEEDFOEGMCEAA.pigood@verizon.net>
Message-ID: <40e66e0b05070409248e2ea14@mail.gmail.com>

A couple more comments on this simulation.  Notice that the sim2
function is defined with arguments size, N and p but is being called
with four arguments.  It appears as if the value of p will be 10 in
that call.

If you decide to do such a simulation yourself you can save a lot of
time by just building the model matrix once and using lm.fit in
subsequent calls.

Also, there is no need to do the counting in the body of the sim2
function.  Just save the 3 p-values from each replication.  The test
of independence is equivalent to showing that the distribution of the
p-values is uniform over the unit cube.


On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>  If the observations are normally distributed and the 2xk design is
> balanced,  theory requires that the tests for interaction and row effects be
> independent.  In my program, appended below, this would translate to cntT
> (approx)= cntR*cntI/N if all R routines were functioning correctly.  They
> aren't.
> 
> sim2=function(size,N,p){
>   cntR=0
>   cntC=0
>   cntI=0
>   cntT=0
>   cntP=0
>   for(i in 1:N){
>     #generate data
>      v=gendata(size)
>     #analyze after build(ing) design containing data
>      lm.out=lm(yield~c*r,build(size,v))
>      av.out=anova(lm.out)
>     #if column effect is significant, increment cntC
>      if (av.out[[5]][1]<=p)cntC=cntC+1
>    #if row effect is significant, increment cntR
>      if (av.out[[5]][2]<=p){
>            cntR=cntR+1
>            tmp = 1
>            }
>      else tmp =0
>      if (av.out[[5]][3]<=p){
>            #if interaction is significant, increment cntI
>             cntI=cntI+1
>         #if both interaction and row effect are significant, increment cntT
>             cntT=cntT + tmp
>             }
>      }
>     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
> }
> 
> build=function(size,v){
> #size is a vector containing the sample sizes
> col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
> rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
> row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
> +size[7]+size[8]))
> return(data.frame(c=factor(col), r=factor(row),yield=v))
> }
> 
> gendata=function(size){
>   ssize=sum(size);
>   return (rnorm(ssize))
> }
> 
> #Example
>  size=c(3,3,3,0,3,3,3,0)
>  sim2(size,10000,10,.16)
> 
> 
> 
> Phillip Good
> Huntington Beach CA
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From pigood at verizon.net  Mon Jul  4 18:45:54 2005
From: pigood at verizon.net (Phillip Good)
Date: Mon, 04 Jul 2005 09:45:54 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <40e66e0b05070409122d9d1de6@mail.gmail.com>
Message-ID: <GHEKKACNLEADPKCNEEDFCEGOCEAA.pigood@verizon.net>

To my surprise, the R functions I employed did NOT verify the independence
property.  I've no quarrel with the theory--it's the R functions that worry
me.

pG

-----Original Message-----
From: Douglas Bates [mailto:dmbates at gmail.com]
Sent: Monday, July 04, 2005 9:13 AM
To: pigood at verizon.net; r-help
Subject: Re: [R] Lack of independence in anova()


I have already had email exchanges off-list with Phillip Good pointing
out that the independence property that he wishes to establish by
simulation is a consequence of orthogonality of the column span of the
row contrasts and the interaction contrasts.  If you set the contrasts
option to a set of orthogonal contrasts such as contr.helmert or
contr.sum, which has no effect on the results of the anova, this is
easily established.

> build
function(size, v = rnorm(sum(size))) {
    col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
    rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
    row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
    +size[7]+size[8]))
    return(data.frame(c=factor(col), r=factor(row),yield=v))
}
> size
[1] 3 3 3 0 3 3 3 0
> set.seed(1234321)
> vv <- build(size)
> vv
   c r      yield
1  0 0  1.2369081
2  0 0  1.5616230
3  0 0  1.8396185
4  1 0  0.3173245
5  1 0  1.0715115
6  1 0 -1.1459955
7  2 0  0.2488894
8  2 0  0.1158625
9  2 0  2.6200816
10 0 1  1.2624048
11 0 1 -0.9862578
12 0 1 -0.3235653
13 1 1  0.2039706
14 1 1 -1.4574576
15 1 1  1.9158713
16 2 1 -2.0333909
17 2 1  1.0050272
18 2 1  0.6789184
> options(contrasts = c('contr.helmert', 'contr.poly'))
> crossprod(model.matrix(~c*r, vv))
            (Intercept) c1 c2 r1 c1:r1 c2:r1
(Intercept)          18  0  0  0     0     0
c1                    0 12  0  0     0     0
c2                    0  0 36  0     0     0
r1                    0  0  0 18     0     0
c1:r1                 0  0  0  0    12     0
c2:r1                 0  0  0  0     0    36


On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>  If the observations are normally distributed and the 2xk design is
> balanced,  theory requires that the tests for interaction and row effects
be
> independent.  In my program, appended below, this would translate to cntT
> (approx)= cntR*cntI/N if all R routines were functioning correctly.  They
> aren't.
>
> sim2=function(size,N,p){
>   cntR=0
>   cntC=0
>   cntI=0
>   cntT=0
>   cntP=0
>   for(i in 1:N){
>     #generate data
>      v=gendata(size)
>     #analyze after build(ing) design containing data
>      lm.out=lm(yield~c*r,build(size,v))
>      av.out=anova(lm.out)
>     #if column effect is significant, increment cntC
>      if (av.out[[5]][1]<=p)cntC=cntC+1
>    #if row effect is significant, increment cntR
>      if (av.out[[5]][2]<=p){
>            cntR=cntR+1
>            tmp = 1
>            }
>      else tmp =0
>      if (av.out[[5]][3]<=p){
>            #if interaction is significant, increment cntI
>             cntI=cntI+1
>         #if both interaction and row effect are significant, increment
cntT
>             cntT=cntT + tmp
>             }
>      }
>     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
> }
>
> build=function(size,v){
> #size is a vector containing the sample sizes
> col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
> rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
> row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
> +size[7]+size[8]))
> return(data.frame(c=factor(col), r=factor(row),yield=v))
> }
>
> gendata=function(size){
>   ssize=sum(size);
>   return (rnorm(ssize))
> }
>
> #Example
>  size=c(3,3,3,0,3,3,3,0)
>  sim2(size,10000,10,.16)
>
>
>
> Phillip Good
> Huntington Beach CA
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From pigood at verizon.net  Mon Jul  4 18:58:15 2005
From: pigood at verizon.net (Phillip Good)
Date: Mon, 04 Jul 2005 09:58:15 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <40e66e0b05070409248e2ea14@mail.gmail.com>
Message-ID: <GHEKKACNLEADPKCNEEDFMEGOCEAA.pigood@verizon.net>

My bad.  The example of a call should read, sim2(size,10000,.16).
Originally, the intent of the program was to compare ANOV (the gold
standard) with synchronized permutation tests when data is from contaminated
normal distributions or the design is unbalanced.  The tests for main
effects and interactions are always independent with synchronized
permutations and ought to be for ANOV with normal data and balanced designs.

-----Original Message-----
From: Douglas Bates [mailto:dmbates at gmail.com]
Sent: Monday, July 04, 2005 9:24 AM
To: pigood at verizon.net
Cc: r-help at r-project.org
Subject: Re: [R] Lack of independence in anova()


A couple more comments on this simulation.  Notice that the sim2
function is defined with arguments size, N and p but is being called
with four arguments.  It appears as if the value of p will be 10 in
that call.

If you decide to do such a simulation yourself you can save a lot of
time by just building the model matrix once and using lm.fit in
subsequent calls.

Also, there is no need to do the counting in the body of the sim2
function.  Just save the 3 p-values from each replication.  The test
of independence is equivalent to showing that the distribution of the
p-values is uniform over the unit cube.


On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>  If the observations are normally distributed and the 2xk design is
> balanced,  theory requires that the tests for interaction and row effects
be
> independent.  In my program, appended below, this would translate to cntT
> (approx)= cntR*cntI/N if all R routines were functioning correctly.  They
> aren't.
>
> sim2=function(size,N,p){
>   cntR=0
>   cntC=0
>   cntI=0
>   cntT=0
>   cntP=0
>   for(i in 1:N){
>     #generate data
>      v=gendata(size)
>     #analyze after build(ing) design containing data
>      lm.out=lm(yield~c*r,build(size,v))
>      av.out=anova(lm.out)
>     #if column effect is significant, increment cntC
>      if (av.out[[5]][1]<=p)cntC=cntC+1
>    #if row effect is significant, increment cntR
>      if (av.out[[5]][2]<=p){
>            cntR=cntR+1
>            tmp = 1
>            }
>      else tmp =0
>      if (av.out[[5]][3]<=p){
>            #if interaction is significant, increment cntI
>             cntI=cntI+1
>         #if both interaction and row effect are significant, increment
cntT
>             cntT=cntT + tmp
>             }
>      }
>     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
> }
>
> build=function(size,v){
> #size is a vector containing the sample sizes
> col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
> rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
> row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
> +size[7]+size[8]))
> return(data.frame(c=factor(col), r=factor(row),yield=v))
> }
>
> gendata=function(size){
>   ssize=sum(size);
>   return (rnorm(ssize))
> }
>
> #Example
>  size=c(3,3,3,0,3,3,3,0)
>  sim2(size,10000,10,.16)
>
>
>
> Phillip Good
> Huntington Beach CA
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From christof.bigler at colorado.edu  Mon Jul  4 19:11:23 2005
From: christof.bigler at colorado.edu (Christof Bigler)
Date: Mon, 4 Jul 2005 11:11:23 -0600
Subject: [R] Colors in mtext
Message-ID: <b57927aae809106d5181dffc5684282c@colorado.edu>

I want to assign different colors to different strings using 
mtext(...). However, when I use something like

mtext(text=c("string1","string2","string3"), 
col=c("black","blue","red"), side=3, line=0)

string2 and string3 are printed over string1. When I use 
paste("string1","string2","string3"), the series of strings are printed 
one over each other, but I still don't get different colors for 
different strings.

Any solutions?

Christof



From zhliur at yahoo.com  Mon Jul  4 20:35:38 2005
From: zhliur at yahoo.com (yyan liu)
Date: Mon, 4 Jul 2005 11:35:38 -0700 (PDT)
Subject: [R] question about boxplot axis
Message-ID: <20050704183538.36156.qmail@web53104.mail.yahoo.com>

Hi:
  I have a question making side by side boxplot. 
  My response is numeric and I want to make a side by
side boxplot of it accroding to a factor vector. So,
there are several boxplots on the same plot. Each
boxplot is with respect to one level for a factor. The
levels of
the factor are some characters. When I make the plot,
the boxplots are arranged according to the alphabetic
order of the levels. My question is that how I can
change the order the boxplots are arranged. For
example, 
 
   fac<-c("a","b","c","d")
   boxplot(time.vec~fac,xlab="Events",ylab="Time In
Days",col="yellow")

Then the boxplots are arranged in the order of
"a","b","c","d". But for some reason, I want it to be
"c","a","d","b". Any suggestion about this question?

thx a lot!

liu zhong



From lbagua at uga.edu  Mon Jul  4 20:42:48 2005
From: lbagua at uga.edu (Luis Borda de Agua)
Date: Mon, 4 Jul 2005 14:42:48 -0400
Subject: [R] r version 2.1.0 and graphics in mac os 10.3.9
Message-ID: <423979b69fd7d67420a664c749025088@uga.edu>

I use mac os 10.3.9 and I've installed in my computer R 2.1.0 (I  
believe this is the latest R version).

Although it works alright when I open R by clicking in the R icon, I  
cannot use the graphics facility when I open R from a X-terminal window  
(or x-11 window).

In fact, when I try to open R I get the message that I pasted below.
Is this a R bug?
Is R assuming that I should have the latest Mac OS Tiger installed?
Or is it a problem related to my computer only?

I did not have this problem when I used a previous version of R (I  
don't know which one was).

Thank you in advance for your help.

Luis BA.

___________________________________________


% R

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.0 Patched (2005-05-12), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/ 
grDevices.so':
   dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/exec/R  
version mismatch for library: /usr/local/lib/libxml2.2.dylib  
(compatibility version of user: 9.0.0 greater than library's version:  
8.0.0)
Loading required package: grDevices
Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/ 
grDevices.so':
   dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/exec/R  
version mismatch for library: /usr/local/lib/libxml2.2.dylib  
(compatibility version of user: 9.0.0 greater than library's version:  
8.0.0)
In addition: Warning message:
package grDevices in options("defaultPackages") was not found
Error: package 'grDevices' could not be loaded


________________________________________________________
Luis Borda de Agua
2502 Department of Plant Biology
University of Georgia
Athens GA 30602
USA
Phone: (706) 583-0943
Fax: (706) 542-1805



From MSchwartz at mn.rr.com  Mon Jul  4 21:00:08 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 04 Jul 2005 14:00:08 -0500
Subject: [R] question about boxplot axis
In-Reply-To: <20050704183538.36156.qmail@web53104.mail.yahoo.com>
References: <20050704183538.36156.qmail@web53104.mail.yahoo.com>
Message-ID: <1120503608.8580.43.camel@localhost.localdomain>

On Mon, 2005-07-04 at 11:35 -0700, yyan liu wrote:
> Hi:
>   I have a question making side by side boxplot. 
>   My response is numeric and I want to make a side by
> side boxplot of it accroding to a factor vector. So,
> there are several boxplots on the same plot. Each
> boxplot is with respect to one level for a factor. The
> levels of
> the factor are some characters. When I make the plot,
> the boxplots are arranged according to the alphabetic
> order of the levels. My question is that how I can
> change the order the boxplots are arranged. For
> example, 
>  
>    fac<-c("a","b","c","d")
>    boxplot(time.vec~fac,xlab="Events",ylab="Time In
> Days",col="yellow")
> 
> Then the boxplots are arranged in the order of
> "a","b","c","d". But for some reason, I want it to be
> "c","a","d","b". Any suggestion about this question?
> 
> thx a lot!
> 
> liu zhong



time.vec <- rnorm(40)
fac <- factor(sample(letters[1:4], 40, replace = TRUE))

# Default ordering
boxplot(time.vec ~ fac, xlab="Events", 
        ylab="Time In Days", col="yellow")


# Now adjust factor levels as you desire
fac <- factor(fac, levels = c("c", "a", "d", "b"))

# Redo boxplot with new ordering
boxplot(time.vec ~ fac, xlab="Events", 
        ylab="Time In Days", col="yellow")


See ?factor for more information. boxplot() sequences the boxes based
upon the factor levels.

HTH,

Marc Schwartz



From pgilbert at bank-banque-canada.ca  Mon Jul  4 20:59:08 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 04 Jul 2005 14:59:08 -0400
Subject: [R] partial autoregression matrix function
In-Reply-To: <c296af57cb4d0978f8f73fab0e227afe@glam.ac.uk>
References: <c296af57cb4d0978f8f73fab0e227afe@glam.ac.uk>
Message-ID: <42C986FC.20807@bank-banque-canada.ca>



Samuel E. Kemp wrote:
> Hi,
> 
> Does anyone know of a function in R that is capable of calculating the 
> partial autoregression matrix function for vector autoregressive moving 
> average (VARMA) models?
?pacf

> 
> The dse package has functions capable of simulating and estimating 
> VARMA models, but I did not notice a function for model identification.

In dse you might also look at
?checkResiduals
?informationTests

Paul Gilbert
> 
> Any help would be greatly appreciated.
> 
> Kind regards,
> 
> Sam.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From shaikhoj at univmail.cis.mcmaster.ca  Mon Jul  4 21:26:39 2005
From: shaikhoj at univmail.cis.mcmaster.ca (O.J. Shaikh)
Date: Mon, 04 Jul 2005 15:26:39 -0400
Subject: [R] R emacs and ess
Message-ID: <web-97487630@cgpsrv2.cis.mcmaster.ca>

Hi all,
 I want to apologise first for being a beginner with linux/emacs/ess;
I am pretty good with R ;).

I have R-1.9.1 installed on my directory of a linux server, I also have
ess 5.2.8 installed there as well.  When I call R using (C-u M-x R) in
emacs, i receive the following error [no match].

I was wondering if emacs is not looking for ess in the right place (if
there is something i have to edit into the .emacs file) and if someone
could explain to me how i should do that since i am not good with
linux.  

I was also wondering if emacs cant find R and if someone could let me
know how i should go about doing that.  Currently to execute R I use
(sh R) from the R bin directory.

Thanks for all your help
Omar



From spencer.graves at pdf.com  Mon Jul  4 21:41:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 04 Jul 2005 12:41:08 -0700
Subject: [R] Colors in mtext
In-Reply-To: <b57927aae809106d5181dffc5684282c@colorado.edu>
References: <b57927aae809106d5181dffc5684282c@colorado.edu>
Message-ID: <42C990D4.4040207@pdf.com>

	  The documentation "?mtext" says that "adj" is "adjustment for each 
string in reading direction. For strings parallel to the axes, 'adj=0' 
means left or bottom alignment, and 'adj=1' means right or top 
alignment."  With this information, I tried the following, which looks 
like it may be close to what you are requesting:

mtext(text=c("string1","string2","string3"),
       adj=c(0, .5, 1), col=c("black","blue","red"), side=3, line=0)

	  spencer graves

Christof Bigler wrote:
> I want to assign different colors to different strings using 
> mtext(...). However, when I use something like
> 
> mtext(text=c("string1","string2","string3"), 
> col=c("black","blue","red"), side=3, line=0)
> 
> string2 and string3 are printed over string1. When I use 
> paste("string1","string2","string3"), the series of strings are printed 
> one over each other, but I still don't get different colors for 
> different strings.
> 
> Any solutions?
> 
> Christof
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From foster at pop.psu.edu  Mon Jul  4 22:31:50 2005
From: foster at pop.psu.edu (E. Michael Foster)
Date: Mon, 4 Jul 2005 16:31:50 -0400 (EDT)
Subject: [R] using index of a loop as a macro variable
Message-ID: <Pine.GSO.4.10.10507041626250.11648-100000@japan.pop.psu.edu>

Hi,

I'm a long-time STATA user and a R newbie. I'm doing ok, but I'm addicted
to STATA macro variables.  Is there something like a macro variable in R?

Specifically, I'd like to be able to do something like

for (i in 1:3) {
	.....
	x`i' <- ...
}

where R would resolve x`i' to the objects named x1, x2 and x3 as I move
through the loop.  I guess I could create these in advance of the loop and
fill them in, but I'd rather not.  

Is there a way to use an index of a loop in this manner? 


thanks,
michael

E. Michael Foster
Professor of Maternal and Child Health
School of Public Health
University of North Carolina



From Charles.Annis at StatisticalEngineering.com  Mon Jul  4 22:39:13 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 4 Jul 2005 16:39:13 -0400
Subject: [R] using index of a loop as a macro variable
In-Reply-To: <Pine.GSO.4.10.10507041626250.11648-100000@japan.pop.psu.edu>
Message-ID: <200507042039.j64Kd7Tx017068@hypatia.math.ethz.ch>

x <- rep(NA, 3)
for (i in 1:length(x)){
x[i] <- ...
}



will do the job, but you may be able to take advantage of R's vectorization
and do what you want with no loop at all.




Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of E. Michael Foster
Sent: Monday, July 04, 2005 4:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] using index of a loop as a macro variable

Hi,

I'm a long-time STATA user and a R newbie. I'm doing ok, but I'm addicted
to STATA macro variables.  Is there something like a macro variable in R?

Specifically, I'd like to be able to do something like

for (i in 1:3) {
	.....
	x`i' <- ...
}

where R would resolve x`i' to the objects named x1, x2 and x3 as I move
through the loop.  I guess I could create these in advance of the loop and
fill them in, but I'd rather not.  

Is there a way to use an index of a loop in this manner? 


thanks,
michael

E. Michael Foster
Professor of Maternal and Child Health
School of Public Health
University of North Carolina

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Mon Jul  4 22:44:19 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 4 Jul 2005 15:44:19 -0500
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFCEGOCEAA.pigood@verizon.net>
References: <40e66e0b05070409122d9d1de6@mail.gmail.com>
	<GHEKKACNLEADPKCNEEDFCEGOCEAA.pigood@verizon.net>
Message-ID: <40e66e0b0507041344596b2a22@mail.gmail.com>

I wrote up a note on how to do a more efficient simulation of the
p-values from anova then discovered to my surprise that the chi-square
test for independence of the significance of the F-tests indicated
that they were not independent.  I was stumped by this but fortunately
Thomas Lumley came to my rescue with an explanation.  There is no
reason why the results of the F tests should be independent.  The
numerators are independent but the denominator is the same for both
tests.  When, due to random variation, the denominator is small, then
the p-values for both tests will tend to be small.  If, instead of
F-tests you use chi-square tests then you do see independence.

Here is the note on the simulation.

There are several things that could be done to speed the simulation of
the p-values of an anova like this under the null distribution.

If you examine the structure of a fitted lm object (use the function
str()) you will see that there are components called `qr', `effects'
and `assign'.  You can verify by experimentation that `qr' and
`assign' depend only on the experimental design.  Furthermore, the
`effects' vector can be reproduced as qr.qty(qrstr, y).

The sums of squares for the different terms in the model are the sums
of squares of elements of the effects vector as indexed by the assign
vector.  The residual sum of squares is the sum of squares of the
remaining elements of the assign vector.  You can generate 10000
replications of the calculations of the relevant sums of squares as

> set.seed(1234321)
> vv <- data.frame(c = gl(3,3,18), r = gl(2,9,18))
> vv
   c r
1  1 1
2  1 1
3  1 1
4  2 1
5  2 1
6  2 1
7  3 1
8  3 1
9  3 1
10 1 2
11 1 2
12 1 2
13 2 2
14 2 2
15 2 2
16 3 2
17 3 2
18 3 2
> fm1 <- lm(rnorm(18) ~ c*r, vv)
> fm1$assign
[1] 0 1 1 2 3 3
> asgn <- c(fm1$assign, rep(4, 12))
> system.time(res <- replicate(10000, tapply(qr.qty(fm1$qr, rnorm(18))^2, asgn, sum)))
[1] 20.61  0.01 20.61  0.00  0.00
> res[,1:6]
       [,1]      [,2]     [,3]      [,4]       [,5]        [,6]
0 0.4783121 0.3048634 0.713689 0.6937838 0.03649023  2.63392426
1 0.5825213 1.4756395 1.127018 0.5209751 1.18697199  3.32972093
2 0.2612723 3.6396106 0.547506 1.1641910 0.37843963  0.03411672
3 2.6259806 3.5504584 1.645215 0.1197238 0.85361018  4.53895212
4 9.1942755 8.2122693 4.863392 5.4413571 2.03715439 22.94815118

The rows of that array correspond to the sum of squares for the
Intercept (which we generally ignore), the factor 'c', the factor 'r',
their interaction and the residuals.

As you can see that took about 21 seconds on my system, which I expect
is a bit faster than your simulation ran.

Because I set the seed to a known value I can reproduce the results
for the first few simulations to check that the sums of squares are
correct.  Remember that the original fit (fm1) is not included in the
table.

> set.seed(1234321)
> fm1 <- lm(rnorm(18) ~ c*r, vv)
> anova(fm2 <- lm(rnorm(18) ~ c*r, vv))
Analysis of Variance Table

Response: rnorm(18)
          Df Sum Sq Mean Sq F value Pr(>F)
c          2 0.5825  0.2913  0.3801 0.6917
r          1 0.2613  0.2613  0.3410 0.5701
c:r        2 2.6260  1.3130  1.7137 0.2215
Residuals 12 9.1943  0.7662               

You can continue this process if you wish further verification that
the results correspond to the fitted models.

You can get the p-values for the F-tests as

> pvalsF <- data.frame(pc = pf((res[2,]/2)/(res[5,]/12), 2, 12, low = FALSE),
+                      pr = pf((res[3,]/1)/(res[5,]/12), 1, 12, low = FALSE),
+                      pint = pf((res[4,]/2)/(res[5,]/12), 2, 12, low = FALSE))

Again you can check this for the first few by hand.

> pvalsF[1:5,]
          pc         pr      pint
1 0.69171238 0.57006574 0.2214847
2 0.37102129 0.03975286 0.1158059
3 0.28634939 0.26771167 0.1740633
4 0.57775850 0.13506561 0.8775828
5 0.06363138 0.16124100 0.1224806

If you wish you could then check marginal distributions using
techniques like an empirical density plot.

> library(lattice)
> densityplot(~ pc, pvals)

At this point I would recommend checking the joint distribution but if
you want to choose a specific level and check the contingency table
that could be done as 

> xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF)
            I(pint < 0.16)
I(pr < 0.16) FALSE TRUE
       FALSE  7204 1240
       TRUE   1215  341

The summary method for an xtabs object provides a test of independence

> summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF))
Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsF)
Number of cases in table: 10000 
Number of factors: 2 
Test for independence of all factors:
	Chisq = 51.6, df = 1, p-value = 6.798e-13

for which you can see the puzzling result.  However, if you use the
chisquared test based on the known residual variance of 1, you get

> pvalsC <- data.frame(pc = pchisq(res[2,], 2, low = FALSE),
+                      pr = pchisq(res[3,], 1, low = FALSE),
+                      pint = pchisq(res[4,], 2, low = FALSE))
> pvalsC[1:5,]
         pc         pr      pint
1 0.7473209 0.60924741 0.2690144
2 0.4781553 0.05642013 0.1694446
3 0.5692081 0.45933855 0.4392846
4 0.7706757 0.28059805 0.9418946
5 0.5523983 0.53843951 0.6525907
> xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC)
            I(pint < 0.16)
I(pr < 0.16) FALSE TRUE
       FALSE  7121 1319
       TRUE   1324  236
> summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC))
Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsC)
Number of cases in table: 10000 
Number of factors: 2 
Test for independence of all factors:
	Chisq = 0.25041, df = 1, p-value = 0.6168

 

On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
> To my surprise, the R functions I employed did NOT verify the independence
> property.  I've no quarrel with the theory--it's the R functions that worry
> me.
> 
> pG
> 
> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: Monday, July 04, 2005 9:13 AM
> To: pigood at verizon.net; r-help
> Subject: Re: [R] Lack of independence in anova()
> 
> 
> I have already had email exchanges off-list with Phillip Good pointing
> out that the independence property that he wishes to establish by
> simulation is a consequence of orthogonality of the column span of the
> row contrasts and the interaction contrasts.  If you set the contrasts
> option to a set of orthogonal contrasts such as contr.helmert or
> contr.sum, which has no effect on the results of the anova, this is
> easily established.
> 
> > build
> function(size, v = rnorm(sum(size))) {
>     col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
>     rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
>     row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
>     +size[7]+size[8]))
>     return(data.frame(c=factor(col), r=factor(row),yield=v))
> }
> > size
> [1] 3 3 3 0 3 3 3 0
> > set.seed(1234321)
> > vv <- build(size)
> > vv
>    c r      yield
> 1  0 0  1.2369081
> 2  0 0  1.5616230
> 3  0 0  1.8396185
> 4  1 0  0.3173245
> 5  1 0  1.0715115
> 6  1 0 -1.1459955
> 7  2 0  0.2488894
> 8  2 0  0.1158625
> 9  2 0  2.6200816
> 10 0 1  1.2624048
> 11 0 1 -0.9862578
> 12 0 1 -0.3235653
> 13 1 1  0.2039706
> 14 1 1 -1.4574576
> 15 1 1  1.9158713
> 16 2 1 -2.0333909
> 17 2 1  1.0050272
> 18 2 1  0.6789184
> > options(contrasts = c('contr.helmert', 'contr.poly'))
> > crossprod(model.matrix(~c*r, vv))
>             (Intercept) c1 c2 r1 c1:r1 c2:r1
> (Intercept)          18  0  0  0     0     0
> c1                    0 12  0  0     0     0
> c2                    0  0 36  0     0     0
> r1                    0  0  0 18     0     0
> c1:r1                 0  0  0  0    12     0
> c2:r1                 0  0  0  0     0    36
> 
> 
> On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
> >  If the observations are normally distributed and the 2xk design is
> > balanced,  theory requires that the tests for interaction and row effects
> be
> > independent.  In my program, appended below, this would translate to cntT
> > (approx)= cntR*cntI/N if all R routines were functioning correctly.  They
> > aren't.
> >
> > sim2=function(size,N,p){
> >   cntR=0
> >   cntC=0
> >   cntI=0
> >   cntT=0
> >   cntP=0
> >   for(i in 1:N){
> >     #generate data
> >      v=gendata(size)
> >     #analyze after build(ing) design containing data
> >      lm.out=lm(yield~c*r,build(size,v))
> >      av.out=anova(lm.out)
> >     #if column effect is significant, increment cntC
> >      if (av.out[[5]][1]<=p)cntC=cntC+1
> >    #if row effect is significant, increment cntR
> >      if (av.out[[5]][2]<=p){
> >            cntR=cntR+1
> >            tmp = 1
> >            }
> >      else tmp =0
> >      if (av.out[[5]][3]<=p){
> >            #if interaction is significant, increment cntI
> >             cntI=cntI+1
> >         #if both interaction and row effect are significant, increment
> cntT
> >             cntT=cntT + tmp
> >             }
> >      }
> >     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
> > }
> >
> > build=function(size,v){
> > #size is a vector containing the sample sizes
> > col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
> > rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
> > row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
> > +size[7]+size[8]))
> > return(data.frame(c=factor(col), r=factor(row),yield=v))
> > }
> >
> > gendata=function(size){
> >   ssize=sum(size);
> >   return (rnorm(ssize))
> > }
> >
> > #Example
> >  size=c(3,3,3,0,3,3,3,0)
> >  sim2(size,10000,10,.16)
> >
> >
> >
> > Phillip Good
> > Huntington Beach CA
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Mon Jul  4 22:47:40 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Jul 2005 13:47:40 -0700 (PDT)
Subject: [R] using index of a loop as a macro variable
In-Reply-To: <Pine.GSO.4.10.10507041626250.11648-100000@japan.pop.psu.edu>
References: <Pine.GSO.4.10.10507041626250.11648-100000@japan.pop.psu.edu>
Message-ID: <Pine.A41.4.61b.0507041336290.296604@homer08.u.washington.edu>

On Mon, 4 Jul 2005, E. Michael Foster wrote:
> I'm a long-time STATA user and a R newbie. I'm doing ok, but I'm addicted
> to STATA macro variables.  Is there something like a macro variable in R?
>
> Specifically, I'd like to be able to do something like
>
> for (i in 1:3) {
> 	.....
> 	x`i' <- ...
> }
>
> where R would resolve x`i' to the objects named x1, x2 and x3 as I move
> through the loop.  I guess I could create these in advance of the loop and
> fill them in, but I'd rather not.
>
> Is there a way to use an index of a loop in this manner?

No. Well, actually, yes, but you don't want to. Stata macros rarely 
translate word-for-word into R.  There is a FAQ describing how to do this 
sort of thing, but the most important paragraph is the last one, where it 
says not to do this.


What you want is a list.

for(i in 1:3){
     .....
     x[[i]]<-...
}

Now, x needs to exist before the loop. You can use
    x<-NULL
to create it, or if you know how long it will be you can use
    x<-vector("list",3)



 	-thomas



From dmbates at gmail.com  Mon Jul  4 22:48:51 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 4 Jul 2005 15:48:51 -0500
Subject: [R] Lack of independence in anova()
In-Reply-To: <40e66e0b0507041344596b2a22@mail.gmail.com>
References: <40e66e0b05070409122d9d1de6@mail.gmail.com>
	<GHEKKACNLEADPKCNEEDFCEGOCEAA.pigood@verizon.net>
	<40e66e0b0507041344596b2a22@mail.gmail.com>
Message-ID: <40e66e0b05070413488f7eaa@mail.gmail.com>

On 7/4/05, Douglas Bates <dmbates at gmail.com> wrote:
...
> The sums of squares for the different terms in the model are the sums
> of squares of elements of the effects vector as indexed by the assign
> vector.  The residual sum of squares is the sum of squares of the
> remaining elements of the assign vector. 

I meant to write "effects vector" not "assign vector" in that last sentence.



From p.dalgaard at biostat.ku.dk  Mon Jul  4 22:56:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jul 2005 22:56:17 +0200
Subject: [R] R emacs and ess
In-Reply-To: <web-97487630@cgpsrv2.cis.mcmaster.ca>
References: <web-97487630@cgpsrv2.cis.mcmaster.ca>
Message-ID: <x2r7eethn2.fsf@turmalin.kubism.ku.dk>

"O.J. Shaikh" <shaikhoj at univmail.cis.mcmaster.ca> writes:

> Hi all,
>  I want to apologise first for being a beginner with linux/emacs/ess;
> I am pretty good with R ;).

Not at upgrading it, it seems ... ;-)

> I have R-1.9.1 installed on my directory of a linux server, I also have
> ess 5.2.8 installed there as well.  When I call R using (C-u M-x R) in
> emacs, i receive the following error [no match].
> 
> I was wondering if emacs is not looking for ess in the right place (if
> there is something i have to edit into the .emacs file) and if someone
> could explain to me how i should do that since i am not good with
> linux.  
> 
> I was also wondering if emacs cant find R and if someone could let me
> know how i should go about doing that.  Currently to execute R I use
> (sh R) from the R bin directory.

There's an ESS list too, but you would seem to have omitted to put
something in you emacs startup file. It's usually (require 'ess-site),
but likely different if you install in a private dir. Something like
(load "/wherever/you/put/it/ess-site"), I guess. Check the install
documentation.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From karen at biology.biol.wits.ac.za  Tue Jul  5 00:42:47 2005
From: karen at biology.biol.wits.ac.za (Mrs Karen Kotschy)
Date: Tue, 5 Jul 2005 00:42:47 +0200 (SAST)
Subject: [R] function for cumulative occurrence of elements
In-Reply-To: <E1DnFcQ-0004aC-8u@sys02.mail.msu.edu>
References: <E1DnFcQ-0004aC-8u@sys02.mail.msu.edu>
Message-ID: <Pine.LNX.4.58.0507050034040.19635@euclea.sevenc.private>

Hi Steven

Are you aware of the package "vegan" for community ecology? There is a
function in this package called specaccum, which calculates species
accumulation curves for you. Various methods can be specified, including
"random".

I must admit I have not used this particular function (yet!) but it seems
like it could be useful to you.

Regards
Karen

-------
Karen Kotschy
Centre for Water in the Environment
University of the Witwatersrand
Johannesburg
South Africa


On Tue, 28 Jun 2005, Steven K Friedman wrote:

>
> Hello,
>
> I have a data set with 9700 records, and 7 parameters.
>
> The data were collected for a survey of forest communities.  Sample plots
> (1009) and species (139) are included in this data set. I need to determine
> how species are accumulated as new plots are considered. Basically, I want
> to develop a species area curve.
>
> I've included the first 20 records from the data set.  Point represents the
> plot id. The other parameters are parts of the information statistic H'.
>
> Using "Table", I can construct a data set that lists the occurrence of a
> species at any Point (it produces a binary 0/1 data table). From there it
> get confusing, regarding the most efficient approach to determining the
> addition of new and or repeated species occurrences.
>
> ptcount <-  table(sppoint.freq$species, sppoint.freq$Point)
>
>  From here I've played around with colSums to calculate the number of species
> at each Point.  The difficulty is determining if a species is new or
> repeated.  Also since there are 1009 points a function is needed to screen
> every Point.
>
> Two goals are of interest: 1) the species accumulation curve, and 2) an
> accumulation curve when random Points are considered.
>
> Any help would be greatly appreciated.
>
> Thank you
> Steve Friedman
>
>
>  Point        species frequency point.list point.prop   log.prop
> point.hprime
> 1      7   American elm         7         27 0.25925926 -1.3499267
> 0.3499810
> 2      7          apple         2         27 0.07407407 -2.6026897
> 0.1927918
> 3      7   black cherry         8         27 0.29629630 -1.2163953
> 0.3604134
> 4      7      black oak         1         27 0.03703704 -3.2958369
> 0.1220680
> 5      7    chokecherry         1         27 0.03703704 -3.2958369
> 0.1220680
> 6      7         oak sp         1         27 0.03703704 -3.2958369
> 0.1220680
> 7      7 pignut hickory         1         27 0.03703704 -3.2958369
> 0.1220680
> 8      7      red maple         1         27 0.03703704 -3.2958369
> 0.1220680
> 9      7      white oak         5         27 0.18518519 -1.6863990
> 0.3122961
> 10     9   black spruce         2         27 0.07407407 -2.6026897
> 0.1927918
> 11     9    blue spruce         2         27 0.07407407 -2.6026897
> 0.1927918
> 12     9        missing        12         27 0.44444444 -0.8109302
> 0.3604134
> 13     9  Norway spruce         8         27 0.29629630 -1.2163953
> 0.3604134
> 14     9   white spruce         3         27 0.11111111 -2.1972246
> 0.2441361
> 15    12          apple         2         27 0.07407407 -2.6026897
> 0.1927918
> 16    12   black cherry         1         27 0.03703704 -3.2958369
> 0.1220680
> 17    12   black locust         1         27 0.03703704 -3.2958369
> 0.1220680
> 18    12   black walnut         1         27 0.03703704 -3.2958369
> 0.1220680
> 19    12          lilac         3         27 0.11111111 -2.1972246
> 0.2441361
> 20    12        missing         2         27 0.07407407 -2.6026897
> 0.1927918
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Soren.Hojsgaard at agrsci.dk  Tue Jul  5 01:51:28 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 5 Jul 2005 01:51:28 +0200
Subject: [R] Problems with eval() in connection with match.call()
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A18@DJFPOST01.djf.agrsci.dk>

Dear all, I have a problem when passing parms from one function to another when the argument list is just '...'. Consider this example:
 
foo<-function(){
 xx <- 111222
 bar(x=xx)
}
bar <- function(...){
  cl <- match.call(expand.dots=TRUE)
  print(cl)
  x <- eval(cl$x)
  print(x)
}
foo()

> bar(x = xx)
> Error in eval(expr, envir, enclos) : Object "xx" not found

My expectation was, that xx would be evaluated to 111222 in foo before being passed on to bar, but obviously it is not so. Should I do something explicitely in foo() to 'evaluate' xx or need I do something special in bar()?? 
 
Thanks in advance, S??ren



From dmbates at gmail.com  Tue Jul  5 02:33:22 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 4 Jul 2005 19:33:22 -0500
Subject: [R] Problems with eval() in connection with match.call()
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A18@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A18@DJFPOST01.djf.agrsci.dk>
Message-ID: <40e66e0b050704173324491c98@mail.gmail.com>

On 7/4/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Dear all, I have a problem when passing parms from one function to another when the argument list is just '...'. Consider this example:
> 
> foo<-function(){
>  xx <- 111222
>  bar(x=xx)
> }
> bar <- function(...){
>   cl <- match.call(expand.dots=TRUE)
>   print(cl)
>   x <- eval(cl$x)
>   print(x)
> }
> foo()
> 
> > bar(x = xx)
> > Error in eval(expr, envir, enclos) : Object "xx" not found
> 
> My expectation was, that xx would be evaluated to 111222 in foo before being passed on to bar, but obviously it is not so. 

Welcome to the world of lazy evaluation.  Arguments are evaluated only
when needed (i.e. when first referenced) not at the time of building
the function call.   That's why the default value of an argument can
depend on previously calculated results within the function.

> Should I do something explicitely in foo() to 'evaluate' xx or need I do something special in bar()??

Change the eval(cl$x) to eval(cl$x, parent.frame())

> bar
function(...){
    cl <- match.call(expand.dots=TRUE)
    print(cl)
    x <- eval(cl$x, parent.frame())
    print(x)
}
> foo()
bar(x = xx)
[1] 111222

> Thanks in advance, S??ren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Tue Jul  5 02:37:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 04 Jul 2005 20:37:41 -0400
Subject: [R] Problems with eval() in connection with match.call()
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A18@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A18@DJFPOST01.djf.agrsci.dk>
Message-ID: <42C9D655.2030305@stats.uwo.ca>

S??ren H??jsgaard wrote:
> Dear all, I have a problem when passing parms from one function to another when the argument list is just '...'. Consider this example:
>  
> foo<-function(){
>  xx <- 111222
>  bar(x=xx)
> }
> bar <- function(...){
>   cl <- match.call(expand.dots=TRUE)
>   print(cl)
>   x <- eval(cl$x)
>   print(x)
> }
> foo()
> 
> 
>>bar(x = xx)
>>Error in eval(expr, envir, enclos) : Object "xx" not found
> 
> 
> My expectation was, that xx would be evaluated to 111222 in foo before being passed on to bar, but obviously it is not so. Should I do something explicitely in foo() to 'evaluate' xx or need I do something special in bar()?? 

Hi S??ren.

You need to use eval.parent(cl$x) (which is the same as eval(cl$x, 
envir=parent.frame())) to get what you want.  You want the evaluation to 
happen in the caller's frame of reference, not in bar's frame.

Generally when you eval() an expression, evaluation takes place in the 
current frame:  the function's local frame when it's in a function, the 
global frame when you do it at a command line.  You can use the envir= 
argument to change where it takes place.

Things look different when you have named parameters, e.g.

bar <- function(x) {
print(x)
}

because in this case, the argument gets turned into a "promise" at the 
time of the call, and a promise knows its own evaluation frame.  There's 
nothing really special about named parameters though, e.g.

 > bar <- function(...) {
+  cl <- list(...)
+  print(cl$x)
+ }
 > foo()
[1] 111222


By using match.call, you ignore the environment, and only see the 
expression.

I imagine you could put together an example where eval.parent() gets the 
evaluation wrong, e.g. because you want to evaluate in the parent's 
parent.  I was a little surprised that it didn't happen here:

 > foo
function(){
  xx <- 111222
  bar2(x=xx)
}
 > bar2
function(...) { bar(...) }
 > bar
function(...){
   cl <- match.call(expand.dots=TRUE)
   print(cl)
   x <- eval.parent(cl$x)
   print(x)
}
 > foo()
bar(x = ..1)
[1] 111222



I guess match.call() works hard to be helpful.

Duncan Murdoch



From r.shengzhe at gmail.com  Tue Jul  5 03:08:29 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 5 Jul 2005 03:08:29 +0200
Subject: [R] how to set the position and size of the select.list window
In-Reply-To: <ea57975b05070216074cb61414@mail.gmail.com>
References: <ea57975b05070216074cb61414@mail.gmail.com>
Message-ID: <ea57975b050704180826607390@mail.gmail.com>

Hello,

I use "select.list" to obtain a window of select items, but how can I
set the position and size of this window?

Are there any functions which are used to maximize and minimize the
window of R Console?

Thank you!
shengzhe



From jsorkin at grecc.umaryland.edu  Tue Jul  5 04:34:52 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 04 Jul 2005 22:34:52 -0400
Subject: [R] help eliminating for loops
Message-ID: <s2c9b9a9.052@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050704/ad25a254/attachment.pl

From andy_liaw at merck.com  Tue Jul  5 04:41:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Jul 2005 22:41:25 -0400
Subject: [R] help eliminating for loops
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA63@usctmx1106.Merck.com>

Would this suffice:

n <- 10
NaCon <- matrix(rnorm(n * n, mean=10.77, sd=0.02), 10, 10)
MeanNaCon <- rowMeans(NaCon)
[...]

??

Andy

> From: John Sorkin
> 
> I am having trouble with apply. Could someone suggest changes to the
> code below that will eliminate the for loops?
> R 2.1.1 patched
> Windows 2k
> Thanks,
> John
>  
> function () 
> {
> NaCon<-array(dim=c(10,10))
> for (i in 1:10) {NaCon[i,]<-rnorm(10,10.77,0.02)}
> MeanNaCon<-vector(mode="numeric",length=10)
> for (j in 1:10) {MeanNaCon[j]<-mean(NaCon[j,1:10])}
> print(MeanNaCon)
> #assign("MakeNaCon",MakeNaCon)
> hist(MeanNaCon)
> }
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From gramarga at carpa.ciagri.usp.br  Tue Jul  5 04:47:56 2005
From: gramarga at carpa.ciagri.usp.br (Gabriel Rodrigues Alves Margarido)
Date: Mon, 04 Jul 2005 23:47:56 -0300
Subject: [R] Derivative of a function
Message-ID: <200507050253.j652rliS018621@hypatia.math.ethz.ch>

Suppose I have a simple function that returns a matrix, such as:

test <- function(x){ return(matrix(c(x,x^2,x^3,x^4),2,2)) }

so that test returns:
[ x      x^3 ]
[ x^2    x^4 ]

Is it possible for me to get the derivative of an expression such as:

c(1,0) %*% test() %*% c(0,1)

The vectors are used just to "index" the matrix.
I don't want a value, but the expression to work with (in that case,
the expected expression would be 3*x^2)...

Tried functions D and deriv in many ways, but no success.
I will be grateful if anyone can help.



From gramarga at carpa.ciagri.usp.br  Tue Jul  5 04:47:56 2005
From: gramarga at carpa.ciagri.usp.br (Gabriel Rodrigues Alves Margarido)
Date: Mon, 04 Jul 2005 23:47:56 -0300
Subject: [R] Derivative of a function
Message-ID: <200507050253.j652rl6B018620@hypatia.math.ethz.ch>

Suppose I have a simple function that returns a matrix, such as:

test <- function(x){ return(matrix(c(x,x^2,x^3,x^4),2,2)) }

so that test returns:
[ x      x^3 ]
[ x^2    x^4 ]

Is it possible for me to get the derivative of an expression such as:

c(1,0) %*% test() %*% c(0,1)

The vectors are used just to "index" the matrix.
I don't want a value, but the expression to work with (in that case,
the expected expression would be 3*x^2)...

Tried functions D and deriv in many ways, but no success.
I will be grateful if anyone can help.



From david.meyer at wu-wien.ac.at  Tue Jul  5 00:30:39 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 5 Jul 2005 00:30:39 +0200
Subject: [R] [R-pkgs] New version of "vcd" package
Message-ID: <20050705003039.563e6e9c.david.meyer@wu-wien.ac.at>

Dear useRs,

a completely revised version of the `vcd' ("Visualizing Categorical
Data") package is now
available from CRAN. This major revision includes the following
enhancements:

* grid-based:

The package is now entirely based on `grid', the new R graphics system,
thus exploiting
its unique functionalities. Powered by grid, it is now possible,
e.g., to simply compose complex plots of available components, or to
modify plot elements 
after they have been drawn.

* new flexible framework for mosaic and association plots

Extended mosaic and association plots are now integrated in a completely
new
generic framework for the visualization of contingency tables (so-called
`strucplots'). 
The new design modularizes labeling, shading, spacing, and drawing of
legends, and
also the cells' content by the use of panel functions.

Powerful labeling functions offer much more flexibility for adding
labels
(e.g., no restrictions on the number of dimensions, flexible positioning
of labels,
cell labeling, etc.)

The framework in particular includes many predefinded functions for the
creation of
residual-based shadings.

Convenience interfaces for various `flavors' of mosaic displays are
available, e.g., 
doubledecker plots, or visualizations of "loglm" objects with
residual-based shading.

* misc:

In addition, the package features several new data sets, and an
inference
function for (conditional) independence of margins in a contingency
table.


Happy drawing!

David, Achim, Kurt

-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ssaha at ncsu.edu  Tue Jul  5 09:55:26 2005
From: ssaha at ncsu.edu (Soma Saha)
Date: Tue, 5 Jul 2005 03:55:26 -0400 (EDT)
Subject: [R] Getting runtime error in stepclass
Message-ID: <33290.152.14.66.93.1120550126.squirrel@webmail.ncsu.edu>

Hi!

I got the following runtime error when I tried to use svm method with
stepclass.

Error in "colnames<-"(`*tmp*`, value = c("0", "1")) :
        attempt to set colnames on object with less than two dimensions

I repeated the same sequence of statements but this time I used the
classification function used in the example, i.e., "lda" and it worked
fine but I got the same error when I tried randomForest.

As the same script worked with a different classification function, I am
wondering if stepclass works with svm and randomForest.

I using R version 2.0.1 on a Linux machine. I am including the R script I
used below.

I would be grateful for any suggestion on how to make stepclass work with
these classification functions.

I would also like to make stepclass work with knn but knn does not have a
'predict' method, which is a requirement for a classification method to
work with stepclass. Is there any way to get around this?

Thanks,
Soma



library("e1071")
library("randomForest")
library("klaR")

td <- read.table("dgdata1.txt", header=TRUE, sep=",")
dgenes <- subset(td, dg == 1, select = dg:eg)
ndgenes <- subset(td, dg == 0, select = dg:eg)
n1 <- nrow(dgenes)
n2 <- nrow(ndgenes)
ndgrows <- 1:n2
selrows <- sample(ndgrows)
sndgenes <- ndgenes[selrows[1:n1],]

train <- rbind(dgenes, sndgenes)
attach(train)
traind <- subset(train, select = -dg)
trainr <- factor(dg)
detach(train)

sc_res <- stepclass(traind, trainr, "svm", direction = "forward",
criterion = "AC", fold = 10)



From ligges at statistik.uni-dortmund.de  Tue Jul  5 10:29:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 10:29:10 +0200
Subject: [R] how to set the position and size of the select.list window
In-Reply-To: <ea57975b050704180826607390@mail.gmail.com>
References: <ea57975b05070216074cb61414@mail.gmail.com>
	<ea57975b050704180826607390@mail.gmail.com>
Message-ID: <42CA44D6.1090700@statistik.uni-dortmund.de>

wu sz wrote:

> Hello,
> 
> I use "select.list" to obtain a window of select items, but how can I
> set the position and size of this window?
> 
> Are there any functions which are used to maximize and minimize the
> window of R Console?

You cannot fo windows from select.list and R console, AFAIK, but you can 
do for windows() devices using windows() and brintToTop(), for example.

Uwe Ligges


> Thank you!
> shengzhe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jul  5 10:41:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 10:41:18 +0200
Subject: [R] Getting runtime error in stepclass
In-Reply-To: <33290.152.14.66.93.1120550126.squirrel@webmail.ncsu.edu>
References: <33290.152.14.66.93.1120550126.squirrel@webmail.ncsu.edu>
Message-ID: <42CA47AE.6020002@statistik.uni-dortmund.de>

Soma Saha wrote:

> Hi!
> 
> I got the following runtime error when I tried to use svm method with
> stepclass.
> 
> Error in "colnames<-"(`*tmp*`, value = c("0", "1")) :
>         attempt to set colnames on object with less than two dimensions
> 
> I repeated the same sequence of statements but this time I used the
> classification function used in the example, i.e., "lda" and it worked
> fine but I got the same error when I tried randomForest.
> 
> As the same script worked with a different classification function, I am
> wondering if stepclass works with svm and randomForest.
> 
> I using R version 2.0.1 on a Linux machine. I am including the R script I
> used below.
> 
> I would be grateful for any suggestion on how to make stepclass work with
> these classification functions.
> 
> I would also like to make stepclass work with knn but knn does not have a
> 'predict' method, which is a requirement for a classification method to
> work with stepclass. Is there any way to get around this?
> 
> Thanks,
> Soma
> 
> 
> 
> library("e1071")
> library("randomForest")
> library("klaR")
> 
> td <- read.table("dgdata1.txt", header=TRUE, sep=",")
> dgenes <- subset(td, dg == 1, select = dg:eg)
> ndgenes <- subset(td, dg == 0, select = dg:eg)
> n1 <- nrow(dgenes)
> n2 <- nrow(ndgenes)
> ndgrows <- 1:n2
> selrows <- sample(ndgrows)
> sndgenes <- ndgenes[selrows[1:n1],]
> 
> train <- rbind(dgenes, sndgenes)
> attach(train)
> traind <- subset(train, select = -dg)
> trainr <- factor(dg)
> detach(train)
> 
> sc_res <- stepclass(traind, trainr, "svm", direction = "forward",
> criterion = "AC", fold = 10)

Looks like predict.svm does not always return "probabilities" in all 
cases, hence cannot be used in your case, I guess.
svmlight (interface in in packages klaR) will do, though. You also might 
want to use sknn() (also package klaR), since it has a predict method 
rather than knn().

For randomForest, we will enhance stepclass() to support it in future.

Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mkeller at fam.tuwien.ac.at  Tue Jul  5 11:55:20 2005
From: mkeller at fam.tuwien.ac.at (Martin Keller-Ressel)
Date: Tue, 05 Jul 2005 09:55:20 -0000
Subject: [R] timezone problems
Message-ID: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>

Hi,

Im using R 2.1.1 and running Code that previously worked (on R 2.1.0 I  
believe) using the 'timeDate' function from the fCalendar package. The  
code now throws an error:

Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")

However I have read the documentation of the fCalendar package and I have  
set my system variable TZ to GMT.
I tracked the error down to the function Sys.timezone() which returns NA  
in spite of what Sys.time() returns.

> Sys.timezone()
[1] NA

> Sys.time()
[1] "2005-07-05 08:41:53 GMT"

My version:

> version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.1
year     2005
month    06
day      20
language R

Any help is appreciated,

Martin Keller-Ressel


---
Martin Keller-Ressel
Research Unit of Financial and Actuarial Mathematics
TU Vienna



From gchappi at gmail.com  Tue Jul  5 11:03:24 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 5 Jul 2005 11:03:24 +0200
Subject: [R] Kind of 2 dim histogram - levelplot
Message-ID: <47fce06505070502034866236c@mail.gmail.com>

Dear R-List,

I've written some code to put measurement values at a position x and y
in bins (xb and yb). It works, but I wonder if there isn't a function
that would do what I do by hand in "# fill data in bins"?

Here is the code:

  # data
x <- c( 1.1, 1.5, 2.3, 2.5, 2.6, 2.9, 3.3, 3.5 )
y <- c( 6.3, 6.2, 5.9, 5.3, 5.4, 4.2, 4.8, 4.6 )
val <- c( 50,  58,  32,  14,  12,  17,  36,  52 )
  # bins
xb <- 1:4
yb <- 4:7
xble <- length( xb ) - 1
yble <- length( yb ) - 1
  # fill data in bins
g <- expand.grid( x=xb[1:xble], y=yb[1:yble] )
g$cnt <- numeric( dim( g )[1] )
g$avg <- numeric( dim( g )[1] )
g$proz <- numeric( dim( g )[1] )

idx <- 1
for (myy in 1:yble) {
  for (myx in 1:xble) {
    xIdx <- which( ( (x >= xb[myx]) & (x < xb[myx + 1]) ) )
    yIdx <- which( ( (y >= yb[myy]) & (y < yb[myy + 1]) ) )
    bIdx <- intersect( xIdx, yIdx )
    g[idx,3] <- length( bIdx )
    g[idx,4] <- sum( val[bIdx] )/g[idx,3]
    g[idx,5] <- sum(val[bIdx]>0)/length(bIdx)*100
    idx <- idx + 1
  }
}
  # show data and plot
g
levelplot(cnt ~ x*y, g, main = "Count", region = TRUE)


Best regards,
Hans-Peter



From Saghir.Bashir at UCB-Group.com  Tue Jul  5 12:04:01 2005
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Tue, 5 Jul 2005 12:04:01 +0200 
Subject: [R] by (tapply) and for loop differences
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F0A7E22@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050705/f5c82c94/attachment.pl

From dfiaschi at ec.unipi.it  Tue Jul  5 12:07:41 2005
From: dfiaschi at ec.unipi.it (Davide Fiaschi)
Date: Tue, 05 Jul 2005 12:07:41 +0200
Subject: [R] Code of Hansen's (2000) Econometrica paper on threshold
	estimation
Message-ID: <42CA5BED.7020101@ec.unipi.it>

I am searching for an R version of the code written in GAUSS by Bruce 
Hansen for his paper on Econometrica, 2000, "Sample Splitting and 
Threshold Estimation".
Someone can help me?
Davide

-- 
Davide Fiaschi
Dipartimento di Scienze Economiche
University of Pisa
Via Ridolfi 10
56100 Pisa (PI)
Italy
Phone/Fax: ++39.050.2216208/++39.050.598040
E-mail: dfiaschi at ec.unipi.it
Homepage: http://www-dse.ec.unipi.it/fiaschi/



From rainer.grohmann at gmx.net  Tue Jul  5 12:22:24 2005
From: rainer.grohmann at gmx.net (rainer grohmann)
Date: Tue, 5 Jul 2005 12:22:24 +0200 (MEST)
Subject: [R] =?iso-8859-1?q?PLS=3A_problem_transforming_scores_to_variable?=
	=?iso-8859-1?q?_space?=
Message-ID: <30877.1120558944@www59.gmx.net>

Dear List!

I am trying to calculate the distance between original data points and their
position in the PLS model. In order to do this, I tried to predict the
scores using the predict.mvr function and calculate the corresponding
positions in variable space.

The prediction of scores works perfectly:

------
data(trees)
# build model
t<-plsr(Volume~.,data=trees)

# predict scores for training data and compare: 
#  column 1: scores from model building
#  column 2: scores obtained with predict.mvr
#  column 3: scores from the corresponding matrix-multiplication
# only the first dimension is compared here. 
#  However, results the other component agrees as well
cbind(t$scores[,1],
      predict(t,trees,type="scores")[,1],
      (scale(as.matrix(trees[,-3]),scale=FALSE,center=TRUE)%*%
       t$projection)[,1])
------

However, when I try to map the scores back to variable space, I ran into
problems:

My guess was to use the loadings matrix. 
In order to check the results, I took the scores from the model, mapped them
to variable space and mapped them back to scores using the predict function.
The result of this procedure should again be the scores. 
I know the last step - prediting scores - works (see above). Therefore, if I
ended up with the original scores   after my transformations, I knew my
first step - mapping of scores to variable space - was correct too, I would
indeed obtain the original scores.
Obviously, I am mistaken (otherwise, I wouldn't ask you for help).

Here is, what I did (based on the code above)
------
# map scores to variable space and back to scores and compare: 
#  column 1: original scores from model building
#  column 2: incorrectly mapped scores
# only the first dimension is compared here. 
#  However, results the other component disagrees as well
cbind(t$scores[,1],(t$scores%*%(t$loadings)%*%t$projection)[,1])
------

Any help or hint would be appreciated!
   Rainer

> R.version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R                

--



From p.dalgaard at biostat.ku.dk  Tue Jul  5 12:37:11 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jul 2005 12:37:11 +0200
Subject: [R] by (tapply) and for loop differences
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F0A7E22@ntbraexc104.dir.ucb-group.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F0A7E22@ntbraexc104.dir.ucb-group.com>
Message-ID: <x2mzp17d48.fsf@turmalin.kubism.ku.dk>

"Bashir Saghir (Aztek Global)" <Saghir.Bashir at ucb-group.com> writes:

> I am getting a difference in results when running some analysis using by and
> tapply compare to using a for loop. I've tried searching the web but had no
> luck with the keywords I used.
>  
> I've attached a simple example below to illustrates my problem. I get a
> difference in the mean of yvar, diff and the p-value using tapply & by
> compared to a for loop. I cannot see what I am doing wrong. Can anyone help?
> 
> > # Simulate some data - I'll do 2 simulations...
> > 
> > xvar = rnorm(40, 20, 5)
> > yvar = rnorm(40, 22, 2)
> > num = factor(rep(1:2, each=20))
> > sdat = data.frame(cbind(num, xvar, yvar))
> > 
> > # Define a function to do a simple t test and return some values...
> > 
> > kindtest = function(varx, vary){
> +    res = t.test(varx, vary)
> +    x.mn = res$estimate[1]
> +    y.mn = res$estimate[2]
> +    diff = y.mn-x.mn
> +    pval = res$p.value
> +    cat("Mean xvar =", x.mn, " Mean yvar =", y.mn)
> +    cat(" diff =", diff, "  p-value=", pval, "\n\n")
> +    list(x.mn=x.mn, y.mn=y.mn, diff=diff, pval=pval)
> + }
> 
> ## Results from by and tapply
> 
> > attach(sdat)
> >   bres = by(xvar, num, kindtest, yvar)  
> Mean xvar = 19.8904  Mean yvar = 21.97729 diff = 2.086891   p-value=
> 0.06222805 
> Mean xvar = 19.88329  Mean yvar = 21.97729 diff = 2.093996   p-value=
> 0.05245329 
> 
> >   tres = tapply(xvar, num, kindtest, yvar)
> Mean xvar = 19.8904  Mean yvar = 21.97729 diff = 2.086891   p-value=
> 0.06222805 
> Mean xvar = 19.88329  Mean yvar = 21.97729 diff = 2.093996   p-value=
> 0.05245329 
> 
> > detach(sdat,1)
> 
> ## Results from for
> 
> > for(i in 1:2) {
> +   subdat= subset(sdat, num==i)
> +   kindtest(subdat$xvar, subdat$yvar)
> + }
> Mean xvar = 19.8904  Mean yvar = 21.98615 diff = 2.095746   p-value=
> 0.07319223 
> Mean xvar = 19.88329  Mean yvar = 21.96843 diff = 2.085141   p-value=
> 0.05850057 
> 

The fact that the by/tapply approach is giving you the same Mean yvar
for both groups should be a dead giveaway....

Stick print(varx) and print(vary) into kindtest, and you'll see the
point. You are passing yvar *without* subsetting (and since the t.test
isn't paired, it can hardly be expected to complain that x and y
differ in length...).

This is probably closer to the mark:

  by(sdat, num, with, kindtest(xvar, yvar))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Tue Jul  5 12:46:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 05 Jul 2005 06:46:42 -0400
Subject: [R] how to set the position and size of the select.list window
In-Reply-To: <42CA44D6.1090700@statistik.uni-dortmund.de>
References: <ea57975b05070216074cb61414@mail.gmail.com>	<ea57975b050704180826607390@mail.gmail.com>
	<42CA44D6.1090700@statistik.uni-dortmund.de>
Message-ID: <42CA6512.90709@stats.uwo.ca>

Uwe Ligges wrote:
> wu sz wrote:
> 
> 
>>Hello,
>>
>>I use "select.list" to obtain a window of select items, but how can I
>>set the position and size of this window?
>>
>>Are there any functions which are used to maximize and minimize the
>>window of R Console?
> 
> 
> You cannot fo windows from select.list and R console, AFAIK, but you can 
> do for windows() devices using windows() and brintToTop(), for example.

That's a typo:  it's bringToTop().  Another function which could be used 
to do this is getWindowsHandle, along with C level programming to the 
Windows API.  I haven't looked into this, so it's possible the UI 
library we use will get confused, but it should be possible to send 
Windows messages to those handles.

We do have longstanding bugs with the maximized/minimized status being 
overridden by various reset commands.  I tracked down some of these 
once, but didn't get all of them (and did introduce some new bugs).  I 
don't have the energy to fix the rest.

Duncan Murdoch



From elise.buisson at univ.u-3mrs.fr  Tue Jul  5 13:29:42 2005
From: elise.buisson at univ.u-3mrs.fr (Elise BUISSON)
Date: Tue, 5 Jul 2005 13:29:42 +0200 (CEST)
Subject: [R] split-split plot design with missing data
Message-ID: <3840.62.173.234.146.1120562982.squirrel@webmail.u-3mrs.fr>

Dear all,

I measured the survival and biomass of plants under various combinations
of treatments. There are three treatments (S, T, N) with two levels each
(S, s; T, t; N, n).
The 8 possible combinations
STN
STn
StN
Stn
sTN
sTn
stN
stn
are organized in a split-split plot design and replicated 12 times.

I have analyzed the survival data (%) using the aov function, which seemed
to work fine.
I then tried to analyze the biomass data, but since many plants died,
there is quite a bit of missing data. The aov function does not seem to
allow the analyses of such data.

Can anybody help?
Yours sincerely,
Elise Buisson.



From ligges at statistik.uni-dortmund.de  Tue Jul  5 13:57:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 13:57:01 +0200
Subject: [R] Derivative of a function
In-Reply-To: <200507050253.j652rl6B018620@hypatia.math.ethz.ch>
References: <200507050253.j652rl6B018620@hypatia.math.ethz.ch>
Message-ID: <42CA758D.3060004@statistik.uni-dortmund.de>

Gabriel Rodrigues Alves Margarido wrote:

> Suppose I have a simple function that returns a matrix, such as:
> 
> test <- function(x){ return(matrix(c(x,x^2,x^3,x^4),2,2)) }
> 
> so that test returns:
> [ x      x^3 ]
> [ x^2    x^4 ]
> 
> Is it possible for me to get the derivative of an expression such as:
> 
> c(1,0) %*% test() %*% c(0,1)
> 
> The vectors are used just to "index" the matrix.
> I don't want a value, but the expression to work with (in that case,
> the expected expression would be 3*x^2)...
> 
> Tried functions D and deriv in many ways, but no success.
> I will be grateful if anyone can help.
>

I think you cannot easily handle expressions like this.
Perhaps you better look for some algebra system such as Mathematica and 
friends.

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jul  5 14:10:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 14:10:44 +0200
Subject: [R] Windows compile
In-Reply-To: <42C95B2D.2070907@csca.ryerson.ca>
References: <EC3EEED49432A54990181E8E8B47072754957F@mail2.arts.ryerson.ca>	<42C88D48.1080003@stats.uwo.ca>
	<42C930DE.3090705@csca.ryerson.ca>	<42C93307.4070506@stats.uwo.ca>
	<42C95B2D.2070907@csca.ryerson.ca>
Message-ID: <42CA78C4.5050608@statistik.uni-dortmund.de>

Philip Bermingham wrote:

> That worked great, thank you.  but it seems I have a new error occurring 
> after the zipping of the help files:
> 
> hhc: not found
> cp: cannot stat `C:/R/R-2.1.1/src/library/base/chm/base.chm': No such 
> file or di
> rectory
> make[3]: *** [chm-base] Error 1
> make[2]: *** [pkg-base] Error 2
> make[1]: *** [rpackage] Error 2
> make: *** [all] Error 2
> 
> Any advise?

hhc.exe is Microsoft's html help compiler. Have you configured MkRules 
correctly (you have to chnage the path therein, as the manual suggests), 
and hhc.exe in your path?

Uwe Ligges


> Philip Bermingham
> 
> Duncan Murdoch wrote:
> 
> 
>>On 7/4/2005 8:51 AM, Philip Bermingham wrote:
>>
>>
>>>Where would I specify the path to this file? I checked the mkrules 
>>>file but it doesn't mention it there.  
>>
>>
>>The path is the system path.  You specify it in your shell, before 
>>starting make, or in one of the Control Panel settings (if you want to 
>>make the change system wide).
>>
>>I'd suggest writing a batch file that sets the path, and run it before 
>>a build.  Then you don't need to worry about side effects of changes 
>>on other programs.
>>
>>It will look something like this:
>>
>>path=.;f:\r\tools\bin;f:\perl\bin;f:\minGW\bin;f:\cygwin\bin;c:\util\misc;c:\windows\command;F:\texmf\miktex\bin;c:\progra~1\htmlhe~1 
>>
>>
>>but this is from a very old batch file (I don't use the Windows shell 
>>any more), and of course the paths to the programs will likely be 
>>different on your system than on mine.
>>
>>
>>>Also I did a search for Rpwd.exe and it seems I don't have that 
>>>file.  I have Rpwd.c.  It seems I'm missing an important step in the 
>>>build processes, but I don't know what it is.
>>
>>
>>Rpwd.exe is made in one of the first steps of the build process.  What 
>>you missed is setting up the path so that the build can proceed.
>>
>>Duncan Murdoch
>>
>>
>>>Philip.
>>>
>>>Duncan Murdoch wrote:
>>>
>>>
>>>>Philip Bermingham wrote:
>>>>
>>>>
>>>>>I'm trying to compile R on Windows 2003 Server and following the
>>>>>instruction laid out in R inst and admin manual I continue to get this
>>>>>error:
>>>>>
>>>>> 
>>>>>
>>>>>make: ./Rpwd.exe: Command not found
>>>>>
>>>>>make[1]: ./Rpwd.exe: Command not found
>>>>>
>>>>>/cygdrive/d/rp/tools/bin/make --no-print-directory -C front-ends Rpwd
>>>>>
>>>>>/cygdrive/d/rp/tools/bin/make -C ../../include -f Makefile.win version
>>>>>
>>>>>make[3]: sh.exe: Command not found
>>>>
>>>>
>>>>
>>>>sh.exe is one of the programs in the tools collection, so it looks 
>>>>as though you don't have that on your path.  Getting the path right 
>>>>is important.  A description of what you need in the path is in the 
>>>>R Installation and Administration manual.
>>>>
>>>>Duncan Murdoch
>>>>
>>>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jul  5 14:12:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 14:12:44 +0200
Subject: [R] To Predict for a vector
In-Reply-To: <1048FBEFA7413B42911B866B0A567CFD0B8E86D4@USEBW103.mailasp.unicredit.it>
References: <1048FBEFA7413B42911B866B0A567CFD0B8E86D4@USEBW103.mailasp.unicredit.it>
Message-ID: <42CA793C.50001@statistik.uni-dortmund.de>

Talarico Massimiliano (Xelion) wrote:

> Dear all,
> 
> It's possible in R to predict the new values for a vector ?
> 
> For example I have these vectors:
> 
>  
> 
> V1 at time 1 is [1,3,6,3,8,5,3]
> 
> V2 at time 2 is [5,3,7,8,9,5,4]
> 
> V3 at time 3 is [7,5,3,2,1,7,5]
> 
> ........
> 
> ........
> 
> Vn at time n is [5,4,3,7,8,9,3]
> 
>  
> 
> I want to estimate the vector at time n+1, with some package in R.

Well, which *method* do you want to use?
Do you want to model cross correlation, for example?

Uwe Ligges

>  
> 
> Thanks in advance.
> 
> MT
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jul  5 14:15:35 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 14:15:35 +0200
Subject: [R] compare two lists with differents levels
In-Reply-To: <20050704094539.7682.qmail@web26604.mail.ukl.yahoo.com>
References: <20050704094539.7682.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42CA79E7.8070209@statistik.uni-dortmund.de>

Navarre Sabine wrote:


Make all factor objects to include *all* levels, I guess (since the huge 
exmple stuff below is not reproducible).

Uwe Ligges

> Hi,
> I would like to compare 2 lists resulted from a sql query! bu there are different levels, so when I want to do:
>  
> 
> release1<-sqlQuery(channel,paste("select distinct c.ID,c.Title TitleCrit from category cat, category_criteria cc, criteria c, question_criteria qc, question q, form_question fq, form f, release_form rf, release r, product_release pr, product p where cat.ID=cc.category and cc.criteria=c.ID and c.ID=qc.criteria and qc.question=q.ID and fq.question=q.ID and fq.form=f.ID and f.ID=rf.form and rf.release=r.ID and r.ID=pr.release and pr.product=p.ID and r.ID='",param1,"';",sep=""))
> 
> release2<-sqlQuery(channel,paste("select distinct c.ID,c.Title TitleCrit from category cat, category_criteria cc, criteria c, question_criteria qc, question q, form_question fq, form f, release_form rf, release r, product_release pr, product p where cat.ID=cc.category and cc.criteria=c.ID and c.ID=qc.criteria and qc.question=q.ID and fq.question=q.ID and fq.form=f.ID and f.ID=rf.form and rf.release=r.ID and r.ID=pr.release and pr.product=p.ID and r.ID='",param2,"';",sep=""))
> 
> data_NA<-matrix(data=0,nrow=length(release$Title),ncol=length(formv$TitleCrit),byrow=TRUE, dimnames=list(as.factor(release$Title),paste(as.factor(formv$TitleCrit),"(",formv$first_letter,")",sep="")))
> 
> for (i in 1:length(formv$TitleCrit))
> {
>     for(k in 1: length(release1$TitleCrit))
>     {
>          if(formv$TitleCrit[i]==release1$TitleCrit[k])
>          {
>             data_NA[1,formv$TitleCrit[[i]]]<-1
>          }
>          else{data_NA[1,formv$TitleCrit[[i]]]<-NA}
>    }
> }
> 
> for (i in 1:length(formv$TitleCrit))
> {
>        for(k in 1: length(release2$TitleCrit))
>        {
>             if(formv$TitleCrit[i]==release2$TitleCrit[k])
>             {
>                 data_NA[2,formv$TitleCrit[[i]]]<-1
>             }
>             else{data_NA[2,formv$TitleCrit[[i]]]<-NA}
>        }
> }
> 
>  
> 
> On R: Error in Ops.factor(formv$TitleCrit[i], release2$TitleCrit[k]) : level sets of factors are different
> 
> How can I compare these 2 lists?
> 
> Thanks
> 
>  
> 
> SABINE
> 
>  
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Jul  5 15:14:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Jul 2005 09:14:20 -0400
Subject: [R] Derivative of a function
In-Reply-To: <200507050253.j652rliS018621@hypatia.math.ethz.ch>
References: <200507050253.j652rliS018621@hypatia.math.ethz.ch>
Message-ID: <971536df05070506141c4685e9@mail.gmail.com>

On 7/4/05, Gabriel Rodrigues Alves Margarido
<gramarga at carpa.ciagri.usp.br> wrote:
> Suppose I have a simple function that returns a matrix, such as:
> 
> test <- function(x){ return(matrix(c(x,x^2,x^3,x^4),2,2)) }
> 
> so that test returns:
> [ x      x^3 ]
> [ x^2    x^4 ]
> 
> Is it possible for me to get the derivative of an expression such as:
> 
> c(1,0) %*% test() %*% c(0,1)
> 
> The vectors are used just to "index" the matrix.
> I don't want a value, but the expression to work with (in that case,
> the expected expression would be 3*x^2)...
> 
> Tried functions D and deriv in many ways, but no success.
> I will be grateful if anyone can help.
> 

Note sure if this is good enough but in the following you can
replace 1,2 in the third line with other indices to extract out any
matrix component and differentiate it.  e is a list that contains the
4 expressions and idx[i,j] gives the index in e that contains
the i,j-th expression:

e <- lapply(1:4, function(i) bquote(x^.(i)))
idx <- matrix(1:4, 2)
e12 <- e[idx[1,2]][[1]]
D(e12, "x")



From ggrothendieck at gmail.com  Tue Jul  5 15:35:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Jul 2005 09:35:01 -0400
Subject: [R] Derivative of a function
In-Reply-To: <971536df05070506141c4685e9@mail.gmail.com>
References: <200507050253.j652rliS018621@hypatia.math.ethz.ch>
	<971536df05070506141c4685e9@mail.gmail.com>
Message-ID: <971536df0507050635a7ef099@mail.gmail.com>

On 7/5/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/4/05, Gabriel Rodrigues Alves Margarido
> <gramarga at carpa.ciagri.usp.br> wrote:
> > Suppose I have a simple function that returns a matrix, such as:
> >
> > test <- function(x){ return(matrix(c(x,x^2,x^3,x^4),2,2)) }
> >
> > so that test returns:
> > [ x      x^3 ]
> > [ x^2    x^4 ]
> >
> > Is it possible for me to get the derivative of an expression such as:
> >
> > c(1,0) %*% test() %*% c(0,1)
> >
> > The vectors are used just to "index" the matrix.
> > I don't want a value, but the expression to work with (in that case,
> > the expected expression would be 3*x^2)...
> >
> > Tried functions D and deriv in many ways, but no success.
> > I will be grateful if anyone can help.
> >
> 
> Note sure if this is good enough but in the following you can
> replace 1,2 in the third line with other indices to extract out any
> matrix component and differentiate it.  e is a list that contains the
> 4 expressions and idx[i,j] gives the index in e that contains
> the i,j-th expression:
> 


Here is an improvement that gets rid of idx:

> e <- lapply(1:4, function(i) bquote(x^.(i)))
> dim(e) <- c(2,2)
> D(e[1,2][[1]],"x")
3 * x^2



From maechler at stat.math.ethz.ch  Tue Jul  5 15:37:14 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 5 Jul 2005 15:37:14 +0200
Subject: [R] Derivative of a function
In-Reply-To: <971536df05070506141c4685e9@mail.gmail.com>
References: <200507050253.j652rliS018621@hypatia.math.ethz.ch>
	<971536df05070506141c4685e9@mail.gmail.com>
Message-ID: <17098.36106.576864.855523@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Tue, 5 Jul 2005 09:14:20 -0400 writes:

    Gabor> On 7/4/05, Gabriel Rodrigues Alves Margarido
    Gabor> <gramarga at carpa.ciagri.usp.br> wrote:
    >> Suppose I have a simple function that returns a matrix, such as:
    >> 
    >> test <- function(x){ return(matrix(c(x,x^2,x^3,x^4),2,2)) }
    >> 
    >> so that test returns:
    >> [ x      x^3 ]
    >> [ x^2    x^4 ]
    >> 
    >> Is it possible for me to get the derivative of an expression such as:
    >> 
    >> c(1,0) %*% test() %*% c(0,1)
    >> 
    >> The vectors are used just to "index" the matrix.
    >> I don't want a value, but the expression to work with (in that case,
    >> the expected expression would be 3*x^2)...
    >> 
    >> Tried functions D and deriv in many ways, but no success.
    >> I will be grateful if anyone can help.
    >> 

    Gabor> Note sure if this is good enough but in the following you can
    Gabor> replace 1,2 in the third line with other indices to extract out any
    Gabor> matrix component and differentiate it.  e is a list that contains the
    Gabor> 4 expressions and idx[i,j] gives the index in e that contains
    Gabor> the i,j-th expression:

    Gabor> e <- lapply(1:4, function(i) bquote(x^.(i)))
    Gabor> idx <- matrix(1:4, 2)
    Gabor> e12 <- e[idx[1,2]][[1]]
    Gabor> D(e12, "x")

neat, Gabor!

The following may also be useful {after the initial "e <- lapply(....)"}:

  De <- lapply(e, D, "x")
  dim(De) <- c(2,2)

  > De[[1,2]]
  3 * x^2
  > De[[2,2]]
  4 * x^3

--
Martin Maechler, ETH Zurich



From anubhavmanglick at gmail.com  Tue Jul  5 15:47:36 2005
From: anubhavmanglick at gmail.com (Anubhav Manglick)
Date: Tue, 5 Jul 2005 06:47:36 -0700
Subject: [R] generalized gamma random number generation
Message-ID: <d812a006050705064722d8ea52@mail.gmail.com>

hi
I am stuck with the generation of generalized gamma distributed random numbers

can u plese help me

anubhav



From sean.oriordain at gmail.com  Tue Jul  5 15:56:50 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Tue, 5 Jul 2005 13:56:50 +0000
Subject: [R] finding out more about an object, e.g. lm
Message-ID: <8ed68eed05070506566004d503@mail.gmail.com>

Hi!

I'm trying to use lm(y~x) amongst others in an automated way; I've
gone through the section on indexing in R-lang and I've looked MASS4. 
How do I find out more about the structure of the returned object?  In
perl I can look at object structure pretty-printed in the debugger -
is there an R equivalent?

I've used coef(lm(y~x))[[1]] and coef(lm(y~x))[[2]] to extract the
intercept;  but while summary(lm(y~x)) prints R-squared... it isn't
entirely obvious how to extract this value as a scalar.  ?lm doesn't
give me the information

by saying
unlist(summary(lm(y~x))) it prints

$r.squared
[1] 0.2673333

so now I can say
my.r.sq <- summary(lm(y~x))$r.squared

but is there a better way?  on my small lm model unlist(lm(y~x)) is a
pretty long list! :-)

Many thanks in advance,
Sean O'Riordain



From ligges at statistik.uni-dortmund.de  Tue Jul  5 15:58:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 15:58:29 +0200
Subject: [R] Getting runtime error in stepclass
In-Reply-To: <42CA47AE.6020002@statistik.uni-dortmund.de>
References: <33290.152.14.66.93.1120550126.squirrel@webmail.ncsu.edu>
	<42CA47AE.6020002@statistik.uni-dortmund.de>
Message-ID: <42CA9205.3040007@statistik.uni-dortmund.de>

Uwe Ligges wrote:

> Soma Saha wrote:
> 
>> Hi!
>>
>> I got the following runtime error when I tried to use svm method with
>> stepclass.
>>
>> Error in "colnames<-"(`*tmp*`, value = c("0", "1")) :
>>         attempt to set colnames on object with less than two dimensions
>>
>> I repeated the same sequence of statements but this time I used the
>> classification function used in the example, i.e., "lda" and it worked
>> fine but I got the same error when I tried randomForest.
>>
>> As the same script worked with a different classification function, I am
>> wondering if stepclass works with svm and randomForest.
>>
>> I using R version 2.0.1 on a Linux machine. I am including the R script I
>> used below.
>>
>> I would be grateful for any suggestion on how to make stepclass work with
>> these classification functions.
>>
>> I would also like to make stepclass work with knn but knn does not have a
>> 'predict' method, which is a requirement for a classification method to
>> work with stepclass. Is there any way to get around this?
>>
>> Thanks,
>> Soma
>>
>>
>>
>> library("e1071")
>> library("randomForest")
>> library("klaR")
>>
>> td <- read.table("dgdata1.txt", header=TRUE, sep=",")
>> dgenes <- subset(td, dg == 1, select = dg:eg)
>> ndgenes <- subset(td, dg == 0, select = dg:eg)
>> n1 <- nrow(dgenes)
>> n2 <- nrow(ndgenes)
>> ndgrows <- 1:n2
>> selrows <- sample(ndgrows)
>> sndgenes <- ndgenes[selrows[1:n1],]
>>
>> train <- rbind(dgenes, sndgenes)
>> attach(train)
>> traind <- subset(train, select = -dg)
>> trainr <- factor(dg)
>> detach(train)
>>
>> sc_res <- stepclass(traind, trainr, "svm", direction = "forward",
>> criterion = "AC", fold = 10)
> 
> 
> Looks like predict.svm does not always return "probabilities" in all 
> cases, hence cannot be used in your case, I guess.
> svmlight (interface in in packages klaR) will do, though. You also might 
> want to use sknn() (also package klaR), since it has a predict method 
> rather than knn().
> 
> For randomForest, we will enhance stepclass() to support it in future.

I just told nonsense, stepclass() does not make sense with 
randomForest(), obviously ... (wonder why nobody shouted?)

Uwe


> Uwe Ligges
> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
>



From dmbates at gmail.com  Tue Jul  5 16:02:24 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 5 Jul 2005 09:02:24 -0500
Subject: [R] Getting runtime error in stepclass
In-Reply-To: <42CA9205.3040007@statistik.uni-dortmund.de>
References: <33290.152.14.66.93.1120550126.squirrel@webmail.ncsu.edu>
	<42CA47AE.6020002@statistik.uni-dortmund.de>
	<42CA9205.3040007@statistik.uni-dortmund.de>
Message-ID: <40e66e0b05070507022a7a4186@mail.gmail.com>

On 7/5/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Uwe Ligges wrote:
> 
> > Soma Saha wrote:
> >
> >> Hi!
> >>
> >> I got the following runtime error when I tried to use svm method with
> >> stepclass.
> >>
> >> Error in "colnames<-"(`*tmp*`, value = c("0", "1")) :
> >>         attempt to set colnames on object with less than two dimensions
> >>
> >> I repeated the same sequence of statements but this time I used the
> >> classification function used in the example, i.e., "lda" and it worked
> >> fine but I got the same error when I tried randomForest.
> >>
> >> As the same script worked with a different classification function, I am
> >> wondering if stepclass works with svm and randomForest.
> >>
> >> I using R version 2.0.1 on a Linux machine. I am including the R script I
> >> used below.
> >>
> >> I would be grateful for any suggestion on how to make stepclass work with
> >> these classification functions.
> >>
> >> I would also like to make stepclass work with knn but knn does not have a
> >> 'predict' method, which is a requirement for a classification method to
> >> work with stepclass. Is there any way to get around this?
> >>
> >> Thanks,
> >> Soma
> >>
> >>
> >>
> >> library("e1071")
> >> library("randomForest")
> >> library("klaR")
> >>
> >> td <- read.table("dgdata1.txt", header=TRUE, sep=",")
> >> dgenes <- subset(td, dg == 1, select = dg:eg)
> >> ndgenes <- subset(td, dg == 0, select = dg:eg)
> >> n1 <- nrow(dgenes)
> >> n2 <- nrow(ndgenes)
> >> ndgrows <- 1:n2
> >> selrows <- sample(ndgrows)
> >> sndgenes <- ndgenes[selrows[1:n1],]
> >>
> >> train <- rbind(dgenes, sndgenes)
> >> attach(train)
> >> traind <- subset(train, select = -dg)
> >> trainr <- factor(dg)
> >> detach(train)
> >>
> >> sc_res <- stepclass(traind, trainr, "svm", direction = "forward",
> >> criterion = "AC", fold = 10)
> >
> >
> > Looks like predict.svm does not always return "probabilities" in all
> > cases, hence cannot be used in your case, I guess.
> > svmlight (interface in in packages klaR) will do, though. You also might
> > want to use sknn() (also package klaR), since it has a predict method
> > rather than knn().
> >
> > For randomForest, we will enhance stepclass() to support it in future.
> 
> I just told nonsense, stepclass() does not make sense with
> randomForest(), obviously ... (wonder why nobody shouted?)


Oh, we're just so used to you talking nonsense that we don't bother to
point it out any more :-)



From tlumley at u.washington.edu  Tue Jul  5 16:20:35 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Jul 2005 07:20:35 -0700 (PDT)
Subject: [R] finding out more about an object, e.g. lm
In-Reply-To: <8ed68eed05070506566004d503@mail.gmail.com>
References: <8ed68eed05070506566004d503@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0507050718460.228008@homer09.u.washington.edu>

On Tue, 5 Jul 2005, Sean O'Riordain wrote:

> Hi!
>
> I'm trying to use lm(y~x) amongst others in an automated way; I've
> gone through the section on indexing in R-lang and I've looked MASS4.
> How do I find out more about the structure of the returned object?  In
> perl I can look at object structure pretty-printed in the debugger -
> is there an R equivalent?

str(), or browseEnv(), or the improved object browser in the JGR GUI.

 	-thomas



From sghosh at lexgen.com  Tue Jul  5 16:21:10 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Tue, 5 Jul 2005 09:21:10 -0500
Subject: [R] plot legend outside the grid
Message-ID: <2B47B68F97330841AC8C670749084A7D06C514@wdexchmb01.lexicon.lexgen.com>

Thanks Bert for all the help. I got the legend figured out Friday but left early becoz of long weekend so didn't get a chance to reply.. I modified the plot margins a little bit and Here's what I finally had...
par(mar=c(c(10, 6, 6, 10) + 0.1));

par(xpd=FALSE);

with (dataFrame, stripchart(marbles_buried ~ genotype, method="jitter", vertical=TRUE,  col = c('blue', 'red', 'green'), xlab='Genotype', ylab = "Marbles Buried", main='MBA WTs Vs HOMs', pch=c(1,4,2), jitter=1/3.5, cex=1));

meds <- as.vector(with(dataFrame, by(marbles_buried, genotype, mean)));
segments((1:3)-0.25, meds, (1:3)+0.25, meds, col = c('blue', 'red', 'green'));

dataWt <- subset(dataFrame, genotype=='wt', select=c(marbles_buried,genotype));
dataHet <- subset(dataFrame, genotype=='het', select=c(marbles_buried,genotype));
dataHom <- subset(dataFrame, genotype=='hom', select=c(marbles_buried,genotype));

wtCount <- length(dataWt$marbles_buried);
hetCount <- length(dataHet$marbles_buried);
homCount <- length(dataHom$marbles_buried);
wtLegend <- paste ("wt, (n=", wtCount, ")");
hetLegend <- paste ("het, (n=", hetCount, ")");
homLegend <- paste ("hom, (n=", homCount, ")");

par(xpd=TRUE);
legend(3.8, max(as.vector(dataFrame$marbles_buried)), c(wtLegend, hetLegend, homLegend), col=c('blue', 'red', 'green'), pch=c(1,4,2));

Again thanks so much for all the pointers..

- Sandeep

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com]
Sent: Friday, July 01, 2005 7:37 PM
To: Ghosh, Sandeep
Subject: RE: plot legend outside the grid 


 You are not specifying the x,y position of the legend correctly. You do not
need to end each statement with a ";"; you should set par(xpd=TRUE) before
you do the plot. On a windows device, I used locator() to determine the
coordinates of where to put he legend. The following produced a clean legend
that does not get mixed up with the plotted points:

legend(2.5,72, c(wtLegend, hetLegend, homLegend), col=c('blue', 'red',
'green'), pch=c(1,4,2))

If you change the 72 to 75 the legend is plotted outside the plot frame.

Please read the help files carefully and also "An Introduction to R" to
learn R's syntax.

-- Bert

-----Original Message-----
From: Ghosh, Sandeep [mailto:sghosh at lexgen.com] 
Sent: Thursday, June 30, 2005 3:43 PM
To: Berton Gunter
Subject: plot legend outside the grid 

Thanks for the pointers... I managed to get everything to look and feel the
way I want except for the legend to plot outside the grid... Thanks for the
note on the par, but I'm not able to it to plot outside the plot grid..

dataFrame <- as.data.frame(t(structure(c(
64,'wt',
62,'wt',
66,'wt',
65,'wt',
60,'wt',
61,'wt',
65,'wt',
66,'wt',
65,'wt',
63,'wt',
67,'wt',
65,'wt',
62,'wt',
65,'wt',
68,'wt',
65,'wt',
63,'wt',
65,'wt',
62,'wt',
65,'wt',
66,'wt',
62,'wt',
65,'wt',
65,'wt',
66,'wt',
65,'wt',
62,'wt',
65,'wt',
66,'wt',
65,'wt',
61,'wt',
65,'wt',
66,'wt',
65,'wt',
62,'wt',
63,'het',
67,'het',
60,'het',
67,'het',
66,'het',
62,'het',
65,'het',
62,'het',
61,'het',
62,'het',
66,'het',
60,'het',
65,'het',
65,'het',
61,'het',
64,'het',
68,'het',
64,'het',
63,'het',
62,'het',
64,'het',
62,'het',
64,'het',
65,'het',
60,'het',
65,'het',
70,'het',
63,'het',
67,'het',
66,'het',
65,'hom',
62,'hom',
68,'hom',
67,'hom',
67,'hom',
63,'hom',
67,'hom',
66,'hom',
63,'hom',
72,'hom',
62,'hom',
61,'hom',
66,'hom',
64,'hom',
60,'hom',
61,'hom',
66,'hom',
66,'hom',
66,'hom',
62,'hom',
70,'hom',
65,'hom',
64,'hom',
63,'hom',
65,'hom',
69,'hom',
61,'hom',
66,'hom',
65,'hom',
61,'hom',
63,'hom',
64,'hom',
67,'hom'), .Dim=c(2,98))));

colnames(dataFrame) <- c('marbles_buried', 'genotype');

png('mb.png', width=400, height=400, pointsize=8);

dataFrame[c("marbles_buried")] <- lapply(dataFrame[c("marbles_buried")],
function(x) as.numeric(levels(x)[x]));

par(xpd=FALSE)

with (dataFrame, stripchart(marbles_buried ~ genotype, method="jitter",
vertical=TRUE,  col = c('blue', 'red', 'green'), xlab='Genotype', ylab =
"Marbles Buried", main='MBA WTs Vs HOMs', pch=c(1,4,2), jitter=1/3.5,
cex=1))

meds <- as.vector(with(dataFrame, by(marbles_buried, genotype, mean)))
segments((1:3)-0.25, meds, (1:3)+0.25, meds, col = c('blue', 'red',
'green'));

dataWt <- subset(dataFrame, genotype=='wt',
select=c(marbles_buried,genotype));
dataHet <- subset(dataFrame, genotype=='het',
select=c(marbles_buried,genotype));
dataHom <- subset(dataFrame, genotype=='hom',
select=c(marbles_buried,genotype));

wtCount <- length(dataWt$marbles_buried);
hetCount <- length(dataHet$marbles_buried);
homCount <- length(dataHom$marbles_buried);
wtLegend <- paste ("wt, (n=", wtCount, ")");
hetLegend <- paste ("het, (n=", hetCount, ")");
homLegend <- paste ("hom, (n=", homCount, ")");

par(xpd=TRUE)
legend(1, max(as.vector(dataFrame$marbles_buried)), c(wtLegend, hetLegend,
homLegend), col=c('blue', 'red', 'green'), pch=c(1,4,2));

-Thanks
Sandeep.

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com]
Sent: Thursday, June 30, 2005 2:55 PM
To: Ghosh, Sandeep
Subject: RE: [R] Help with stripplot


Of course!

stripchart() is a base graphics function and tehrefore has available to it
the base graphics functionality, like (the base graphics function, **not**
the lattice argument) legend(). See ?legend in the graphics package. Note
the use of locator() for positioning the legend.

Note: By default the legend will be clipped to the plot region. If you wish
to have a legend outside the plot region set the xpd parameter of par to
TRUE or NA prior to plotting.

-- Bert 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ghosh, Sandeep
> Sent: Thursday, June 30, 2005 12:22 PM
> To: Deepayan Sarkar; r-help at stat.math.ethz.ch
> Subject: Re: [R] Help with stripplot
> 
> Another question, in stripchart is there a way to draw a 
> legends. I need legends that gives the mice count for each 
> genotype wt/het/hom, something like the xyplot plot support 
> for key/auto.key.
> 
> -Sandeep



From michael.watson at bbsrc.ac.uk  Tue Jul  5 16:29:28 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 5 Jul 2005 15:29:28 +0100
Subject: [R] Discriminant Function Analysis
Message-ID: <8975119BCD0AC5419D61A9CF1A923E9502067A46@iahce2knas1.iah.bbsrc.reserved>

Dear All

This is more of a statistics question than a question about help for R,
so forgive me.

I am using lda from the MASS package to perform linear discriminant
function analysis.  I have 14 cases belonging to two groups and have
measured each of 37 variables.  I want to find those variables that best
discriminate between the two groups, and I want to visualise that and
create a classification function.  Please note at this stage it is a
proof of concept problem - I realise that I must follow this up with a
much more robust anaylsis involving cross-validation.

1) First problem, I got this error message:
> z <- lda(C0GRP_NA ~ ., dpi30)
Warning message: 
variables are collinear in: lda.default(x, grouping, ...) 

I guess this is not a good thing, however, I *did* get a result and it
discriminated perfectly between my groups.  Can anyone explain what this
means?  Does it invalidate my results?

2) My analysis came up with one discriminant variable.  How do I control
how many are produced?  I currently assume this is the only significant
discriminant variable found.  Can I insist it finds more?

3) More of a tip - when my analysis only finds one significant variable,
what is a good way to visualise this graphically?

4) Can I work out from the coefficients which sub groups of my variable
are better at discriminating than others?  I guess I could simply
perform a t-test first to select the best variables...?

5) How do I turn my discriminant function into a classification
function?  i.e. when I plot the scores for the groups I can see
graphically that all the values for one group are below 0.1 and all the
values for the other group are above 1.  But how do I turn my
discriminant function into a classification function?

Many thanks in advance for your help

Mick



From B.Rowlingson at lancaster.ac.uk  Tue Jul  5 16:38:41 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 05 Jul 2005 15:38:41 +0100
Subject: [R] finding out more about an object, e.g. lm
In-Reply-To: <8ed68eed05070506566004d503@mail.gmail.com>
References: <8ed68eed05070506566004d503@mail.gmail.com>
Message-ID: <42CA9B71.8060004@lancaster.ac.uk>

Sean O'Riordain wrote:
> ?lm doesn't
> give me the information

  That's because the information has come from 'summary' (specifically 
summary.lm).

?summary.lm:

r.squared: R^2, the "fraction of variance explained by the model",

  - it also lists the other things of interest you can get from an lm 
fitted object.

Baz



From br44114 at gmail.com  Tue Jul  5 17:08:25 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 5 Jul 2005 11:08:25 -0400
Subject: [R] how to call sas in R
Message-ID: <8d5a3635050705080875a138bf@mail.gmail.com>

Why don't you do the simulations in SAS? If you prefer otherwise,
setup the SAS code for running in batch mode (output and log
redirection), then call it from R with (on Windows, untested)
system("start ' ' C:\etc\sas.exe -sysin garch.sas") 
To keep the parameters from the estimate, have the SAS job output them
to a text file or data set.

hth,
b.


> -----Original Message-----
> From: Nongluck Klibbua [mailto:Nongluck.Klibbua at newcastle.edu.au] 
> Sent: Saturday, July 02, 2005 4:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to call sas in R
> 
> 
> Hello all,
> I would like to know how to call sas code in R. Since I 
> simulate data in
> R and I need to use sas code (garch-t,egarch and gjr) to 
> estimate it. I
> need to simulate 500 times with 2000 obs. How I can call that code in
> R.Also, how I can keep the parameters from the estimate.
> 
> j=1:500
> i=1:2000
> sas code
> keep parameters.
> 
> Best Appreciate,
> Luck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From einararn at hi.is  Tue Jul  5 17:54:07 2005
From: einararn at hi.is (Einar Arnason)
Date: Tue, 05 Jul 2005 15:54:07 +0000
Subject: [R] Problem installing RMySQL_0.5-5
Message-ID: <1120578848.11700.2.camel@assa.lif.hi.is>

Dear R users

I have a problem installing RMySQL_0.5-5 in that ld skips incompatible
libmysqlclient as shown below. Can someone help?

Thanks 
Einar Arnason


I am on a Red Hat Enterprise Linux ES (v. 3 for AMD64/Intel EM64T)
2.4.21-15.EL #1 SMP Thu Apr 22


running mysql Ver 14.7 Distrib 4.1.12, for pc-linux-gnu (i686) using
EditLine wrapper distributed under xampp (www.apachefriends.org)
including development package

and R compiled from source
> version
         _
platform x86_64-unknown-linux-gnu
arch     x86_64
os       linux-gnu
system   x86_64, linux-gnu
status
major    2
minor    1.1
year     2005
month    06
day      20
language R
R 2.1.1 (2005-06-20).

I export paths:

# export PKG_CPPFLAGS="-I/opt/lampp/include/mysql"
containing:

errmsg.h     my_dir.h         my_semaphore.h   my_sys.h       sslopt-
longopts.h
keycache.h   my_getopt.h      mysql_com.h      my_xml.h       sslopt-
vars.h
m_ctype.h    my_global.h      mysqld_error.h   raid.h         typelib.h
m_string.h   my_list.h        mysql_embed.h    readline.h
my_alloc.h   my_net.h         mysql.h          sql_common.h
my_config.h  my_no_pthread.h  mysql_time.h     sql_state.h
my_dbug.h    my_pthread.h     mysql_version.h  sslopt-case.h

# export PKG_LIBS="-L/opt/lampp/lib/mysql -lmysqlclient"
containing:

libdbug.a       libmysqlclient         libmysqlclient.so
libnisam.a
libheap.a       libmysqlclient.14      libmysqlclient.so.14
libvio.a
libmerge.a      libmysqlclient.14.0.0  libmysqlclient.so.14.0.0
libmyisam.a     libmysqlclient.a       libmystrings.a
libmyisammrg.a  libmysqlclient.la      libmysys.a

and get the following:

# R CMD INSTALL RMySQL_0.5-5.tar.gz
* Installing *source* package 'RMySQL' ...
creating cache ./config.cache
checking how to run the C preprocessor... cc -E
checking for compress in -lz... yes
checking for getopt_long in -lc... yes
checking for mysql_init in -lmysqlclient... no
checking for mysql.h... no
updating cache ./config.cache
creating ./config.status
creating src/Makevars
** libs
gcc -I/usr/local/lib/R/include -I/opt/lampp/include/mysql -
I/usr/local/include -fPIC  -g -O2 -c RS-DBI.c -o RS-DBI.o
gcc -I/usr/local/lib/R/include -I/opt/lampp/include/mysql -
I/usr/local/include -fPIC  -g -O2 -c RS-MySQL.c -o RS-MySQL.o
gcc -shared -L/usr/local/lib -o RMySQL.so RS-DBI.o RS-MySQL.o -
L/opt/lampp/lib/mysql -lmysqlclient -lz
/usr/bin/ld: skipping
incompatible /opt/lampp/lib/mysql/libmysqlclient.so when searching for -
lmysqlclient
/usr/bin/ld: skipping incompatible /opt/lampp/lib/mysql/libmysqlclient.a
when searching for -lmysqlclient
/usr/bin/ld: cannot find -lmysqlclient
collect2: ld returned 1 exit status
make: *** [RMySQL.so] Error 1
ERROR: compilation failed for package 'RMySQL'
** Removing '/usr/local/lib/R/library/RMySQL'
** Restoring previous '/usr/local/lib/R/library/RMySQL'



From spencer.graves at pdf.com  Tue Jul  5 17:58:37 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 05 Jul 2005 08:58:37 -0700
Subject: [R] finding out more about an object, e.g. lm
In-Reply-To: <42CA9B71.8060004@lancaster.ac.uk>
References: <8ed68eed05070506566004d503@mail.gmail.com>
	<42CA9B71.8060004@lancaster.ac.uk>
Message-ID: <42CAAE2D.4050005@pdf.com>

	  'methods(class="lm")' will identify all the "methods" (i.e., 
functions) that have been defined to do something with an object of 
class "lm".

	  'attributes(<and 'lm' object>)' may not be as good as "str" from some 
perspectives but can be useful from others.

	  spencer graves

Barry Rowlingson wrote:

> Sean O'Riordain wrote:
> 
>>?lm doesn't
>>give me the information
> 
> 
>   That's because the information has come from 'summary' (specifically 
> summary.lm).
> 
> ?summary.lm:
> 
> r.squared: R^2, the "fraction of variance explained by the model",
> 
>   - it also lists the other things of interest you can get from an lm 
> fitted object.
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From r.shengzhe at gmail.com  Tue Jul  5 18:12:50 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 5 Jul 2005 18:12:50 +0200
Subject: [R] bringToBottom? and tcltk document
In-Reply-To: <42CA6512.90709@stats.uwo.ca>
References: <ea57975b05070216074cb61414@mail.gmail.com>
	<ea57975b050704180826607390@mail.gmail.com>
	<42CA44D6.1090700@statistik.uni-dortmund.de>
	<42CA6512.90709@stats.uwo.ca>
Message-ID: <ea57975b05070509123910d25@mail.gmail.com>

Thanks for the answer from Uwe Ligges and Duncan Murdoch !

if any function could do the opposite action of bringToTop()?

Are there any introductory and detailed "tcltk" documents or help
files or books for using this package in R ?

shengzhe

2005/7/5, Duncan Murdoch <murdoch at stats.uwo.ca>:
> Uwe Ligges wrote:
> > wu sz wrote:
> >
> >
> >>Hello,
> >>
> >>I use "select.list" to obtain a window of select items, but how can I
> >>set the position and size of this window?
> >>
> >>Are there any functions which are used to maximize and minimize the
> >>window of R Console?
> >
> >
> > You cannot fo windows from select.list and R console, AFAIK, but you can
> > do for windows() devices using windows() and brintToTop(), for example.
> 
> That's a typo:  it's bringToTop().  Another function which could be used
> to do this is getWindowsHandle, along with C level programming to the
> Windows API.  I haven't looked into this, so it's possible the UI
> library we use will get confused, but it should be possible to send
> Windows messages to those handles.
> 
> We do have longstanding bugs with the maximized/minimized status being
> overridden by various reset commands.  I tracked down some of these
> once, but didn't get all of them (and did introduce some new bugs).  I
> don't have the energy to fix the rest.
> 
> Duncan Murdoch
>



From marfru at cartif.es  Tue Jul  5 18:24:15 2005
From: marfru at cartif.es (Mario de Frutos Dieguez)
Date: Tue, 05 Jul 2005 18:24:15 +0200
Subject: [R] Java and R help
Message-ID: <42CAB42F.4060200@cartif.es>

Im doing an aplication in Java and i have a program made in R what i 
want to launch with Java.

I have the following instructions:

Runtime r = Runtime.getRuntime();
       
try
{

            System.out.println ("Llamada a R...");
            p = r.exec(sRutaR);
        }
        catch (IOException e)
        {
            System.out.println ("Error lanzando R: " + e.getMessage());
            e.printStackTrace();
        }
        catch (Exception ex)
        {
            System.out.println ("Error lanzando R!!!! " + ex.toString());
            ex.printStackTrace();
        }
}

and after that i wait for a file that R must to make called 
terminado.dat. But when i launch the process the file doesn't create 
until i destroy the process.

can anyone explain what's happend with the process?

Thx in advance and sorry for my poor english



From greg.snow at ihc.com  Tue Jul  5 18:34:39 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 05 Jul 2005 10:34:39 -0600
Subject: [R] Generating correlated data from uniform distribution
Message-ID: <s2ca6256.088@lp-msg1.co.ihc.com>

Here is an approach using 'optim' and simulated annealing:

x <- sort(runif(1000))
y <- sort(runif(1000))

ord <- 1:1000
target <- function(ord){ ( cor(x, y[ord]) - 0.6 ) ^2 }
new.point <- function(ord){
	tmp <- sample(length(ord), 2)
	ord[tmp] <- ord[rev(tmp)]
	ord
}

new.point2 <- function(ord){
	tmp <- sample(length(ord) -100, 1)
	tmp2 <- sample(100, 1)
	ord[ c(tmp, tmp+tmp2) ] <- ord[ c(tmp+tmp2, tmp) ]
	ord
}

res <- optim(ord, target, new.point, method="SANN",
	control = list(maxit=6000, temp=2000, trace=TRUE))

res2 <- optim(ord, target, new.point2, method="SANN",
	control = list(maxit=60000, temp=200, trace=TRUE))

y <- y[res$par]

par(mfrow=c(2,2))
hist(x)
hist(y)
plot(x,y)
cor(x,y)


y <- sort(y)[res2$par]

par(mfrow=c(2,2))
hist(x)
hist(y)
plot(x,y)
cor(x,y)

Hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> "Jim Brennan" <jfbrennan at rogers.com> 07/01/05 05:25PM >>>
OK now I am skeptical especially when you say in a weird way:-)
This may be OK but look at plot(x,y) and I am suspicious. Is it still
alright with this kind of relationship?

For large N it appears Spencer's method is returning slightly lower
correlation for the uniforms as compared to the normals so maybe there is a
problem!?!

Hope we are all learning something and Menghui gets/has what he wants . :-)

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: July 1, 2005 6:59 PM
To: Jim Brennan
Cc: 'Tony Plate'; 'Menghui Chen'; r-help at stat.math.ethz.ch 
Subject: Re: [R] Generating correlated data from uniform distribution

"Jim Brennan" <jfbrennan at rogers.com> writes:

> Yes you are right I guess this works only for normal data. Free advice
> sometimes comes with too little consideration :-)

Worth every cent...

> Sorry about that and thanks to Spencer for the correct way.

Hmm, but is it? Or rather, what is the relation between the
correlation of the normals  and that of the transformed variables? 
Looks nontrivial to me.

Incidentally, here's a way that satisfies the criteria, but in a
rather weird way:

N <- 10000
rho <- .6
x <- runif(N, -.5,.5)
y <- x * sample(c(1,-1), N, replace=T, prob=c((1+rho)/2,(1-rho)/2))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hodgess at gator.dt.uh.edu  Tue Jul  5 18:46:17 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 5 Jul 2005 11:46:17 -0500
Subject: [R]  write.foreign and SPSS
Message-ID: <200507051646.j65GkH7w021671@gator.dt.uh.edu>

Dear R:

In the foreign library, there is a function "write.foreign".

When this is used to write to an SPSS file, there are 
actually 2 files as output: a data file and a code file.

How would you coordinate these to become an SPSS sav file, 
please?

Thanks so much!

R Version 2.1.0 Windows

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From murdoch at stats.uwo.ca  Tue Jul  5 18:47:56 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 05 Jul 2005 12:47:56 -0400
Subject: [R] bringToBottom? and tcltk document
In-Reply-To: <ea57975b05070509123910d25@mail.gmail.com>
References: <ea57975b05070216074cb61414@mail.gmail.com>	
	<ea57975b050704180826607390@mail.gmail.com>	
	<42CA44D6.1090700@statistik.uni-dortmund.de>	
	<42CA6512.90709@stats.uwo.ca>
	<ea57975b05070509123910d25@mail.gmail.com>
Message-ID: <42CAB9BC.9020803@stats.uwo.ca>

On 7/5/2005 12:12 PM, wu sz wrote:
> Thanks for the answer from Uwe Ligges and Duncan Murdoch !
> 
> if any function could do the opposite action of bringToTop()?

What is the opposite?  There isn't such a thing defined in the Windows 
API.  If you want to do anything in the API, then just grab the handle, 
and try it.

> Are there any introductory and detailed "tcltk" documents or help
> files or books for using this package in R ?

The tcltk documentation is (in Windows) available in R_HOME/Tcl/doc.  If 
you want something like a tutorial, you'll need to look on the web or in 
a library or bookstore.

Duncan Murdoch
> 
> shengzhe
> 
> 2005/7/5, Duncan Murdoch <murdoch at stats.uwo.ca>:
>> Uwe Ligges wrote:
>> > wu sz wrote:
>> >
>> >
>> >>Hello,
>> >>
>> >>I use "select.list" to obtain a window of select items, but how can I
>> >>set the position and size of this window?
>> >>
>> >>Are there any functions which are used to maximize and minimize the
>> >>window of R Console?
>> >
>> >
>> > You cannot fo windows from select.list and R console, AFAIK, but you can
>> > do for windows() devices using windows() and brintToTop(), for example.
>> 
>> That's a typo:  it's bringToTop().  Another function which could be used
>> to do this is getWindowsHandle, along with C level programming to the
>> Windows API.  I haven't looked into this, so it's possible the UI
>> library we use will get confused, but it should be possible to send
>> Windows messages to those handles.
>> 
>> We do have longstanding bugs with the maximized/minimized status being
>> overridden by various reset commands.  I tracked down some of these
>> once, but didn't get all of them (and did introduce some new bugs).  I
>> don't have the energy to fix the rest.
>> 
>> Duncan Murdoch
>>



From ligges at statistik.uni-dortmund.de  Tue Jul  5 20:42:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Jul 2005 20:42:37 +0200
Subject: [R] Discriminant Function Analysis
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E9502067A46@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E9502067A46@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <42CAD49D.6020305@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Dear All
> 
> This is more of a statistics question than a question about help for R,
> so forgive me.
> 
> I am using lda from the MASS package to perform linear discriminant
> function analysis.  I have 14 cases belonging to two groups and have
> measured each of 37 variables.  I want to find those variables that best
> discriminate between the two groups, and I want to visualise that and
> create a classification function.  Please note at this stage it is a
> proof of concept problem - I realise that I must follow this up with a
> much more robust anaylsis involving cross-validation.
> 
> 1) First problem, I got this error message:
> 
>>z <- lda(C0GRP_NA ~ ., dpi30)
> 
> Warning message: 
> variables are collinear in: lda.default(x, grouping, ...) 
> 
> I guess this is not a good thing, however, I *did* get a result and it
> discriminated perfectly between my groups.  Can anyone explain what this
> means?  Does it invalidate my results?

Well, 14 cases and 37 variables mean that not that many degrees of 
freedom are left.... ;-)
Of course, you get a perfect fit - with arbitrary data.

> 
> 2) My analysis came up with one discriminant variable.  How do I control
> how many are produced?  I currently assume this is the only significant
> discriminant variable found.  Can I insist it finds more?

Well, if projection into one dimension is already perfect, it's hard to 
find a second one that improves the result...


> 3) More of a tip - when my analysis only finds one significant variable,
> what is a good way to visualise this graphically?

Depends of the amount of data, either all data on one line, maybe 
jittered, or maybe even beter two boxplot, given there would be really 
perfect (and sensible) separation ....


> 4) Can I work out from the coefficients which sub groups of my variable
> are better at discriminating than others?  I guess I could simply
> perform a t-test first to select the best variables...?

No, because you ignore possible projections in this case.


> 5) How do I turn my discriminant function into a classification
> function?  i.e. when I plot the scores for the groups I can see
> graphically that all the values for one group are below 0.1 and all the
> values for the other group are above 1.  But how do I turn my
> discriminant function into a classification function?

What about looking for the point where it has the value 0.5 for the 
posterior?

Uwe LIgges



> Many thanks in advance for your help
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Tue Jul  5 21:07:07 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 05 Jul 2005 15:07:07 -0400
Subject: [R] write.foreign and SPSS
In-Reply-To: <200507051646.j65GkH7w021671@gator.dt.uh.edu>
References: <200507051646.j65GkH7w021671@gator.dt.uh.edu>
Message-ID: <42CADA5B.7070004@optonline.net>

Erin Hodgess wrote:
> Dear R:
> 
> In the foreign library, there is a function "write.foreign".
> 
> When this is used to write to an SPSS file, there are 
> actually 2 files as output: a data file and a code file.
> 
> How would you coordinate these to become an SPSS sav file, 
> please?

   Using the warpbreaks dataset as an example, do the following in R:

library(foreign)
write.foreign(warpbreaks, "c:/warpbreaks.dat", "c:/warpbreaks.sps", 
package="SPSS")

   In SPSS, open the syntax file "warpbreaks.sps".  This syntax will 
read in the data and apply variable and value labels to variables that 
were factors in R.  I find you often need to edit the syntax file 
created by write.foreign just a bit in SPSS.  Here is what write.foreign 
produces for me:

DATA LIST FILE= c:/warpbreaks.dat  free
/ breaks wool tension

VARIABLE LABELS
breaks "breaks"
  wool "wool"
  tension "tension"

VALUE LABELS
/
wool
1 "A"
  2 "B"
/
tension
1 "L"
  2 "M"
  3 "H"

EXECUTE.

   You need to add some periods and quotation marks in critical places 
for this to run properly in SPSS.

DATA LIST FILE= "c:/warpbreaks.dat"  free
/ breaks wool tension .

VARIABLE LABELS
breaks "breaks"
  wool "wool"
  tension "tension" .

VALUE LABELS
/
wool
1 "A"
  2 "B"
/
tension
1 "L"
  2 "M"
  3 "H" .

EXECUTE.

   Once the data has been read in by running the code file, it can be 
saved in the SPSS format.

> Thanks so much!
> 
> R Version 2.1.0 Windows
> 
> Sincerely,
> Erin Hodgess
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From tlumley at u.washington.edu  Tue Jul  5 21:16:13 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Jul 2005 12:16:13 -0700 (PDT)
Subject: [R] write.foreign and SPSS
In-Reply-To: <200507051646.j65GkH7w021671@gator.dt.uh.edu>
References: <200507051646.j65GkH7w021671@gator.dt.uh.edu>
Message-ID: <Pine.A41.4.61b.0507051215200.266518@homer08.u.washington.edu>

On Tue, 5 Jul 2005, Erin Hodgess wrote:

> Dear R:
>
> In the foreign library, there is a function "write.foreign".
>
> When this is used to write to an SPSS file, there are
> actually 2 files as output: a data file and a code file.
>
> How would you coordinate these to become an SPSS sav file,
> please?

Use SPSS to run the code file. It will read in the data file.  Then you 
can save from SPSS as a .sav.


 	-thomas



From r.shengzhe at gmail.com  Tue Jul  5 21:17:27 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 5 Jul 2005 21:17:27 +0200
Subject: [R] bringToBottom? and tcltk document
In-Reply-To: <42CAB9BC.9020803@stats.uwo.ca>
References: <ea57975b05070216074cb61414@mail.gmail.com>
	<ea57975b050704180826607390@mail.gmail.com>
	<42CA44D6.1090700@statistik.uni-dortmund.de>
	<42CA6512.90709@stats.uwo.ca>
	<ea57975b05070509123910d25@mail.gmail.com>
	<42CAB9BC.9020803@stats.uwo.ca>
Message-ID: <ea57975b05070512175e5a999c@mail.gmail.com>

Thanks for your quick reply!

the "opposite" I mean is to take a window to the bottom of the window
stack and stay at the bottom. This should be the reverse action of
bringToTop().

shengzhe

2005/7/5, Duncan Murdoch <murdoch at stats.uwo.ca>:
> On 7/5/2005 12:12 PM, wu sz wrote:
> > Thanks for the answer from Uwe Ligges and Duncan Murdoch !
> >
> > if any function could do the opposite action of bringToTop()?
> 
> What is the opposite?  There isn't such a thing defined in the Windows
> API.  If you want to do anything in the API, then just grab the handle,
> and try it.
> 
> > Are there any introductory and detailed "tcltk" documents or help
> > files or books for using this package in R ?
> 
> The tcltk documentation is (in Windows) available in R_HOME/Tcl/doc.  If
> you want something like a tutorial, you'll need to look on the web or in
> a library or bookstore.
> 
> Duncan Murdoch
> >
> > shengzhe
> >
> > 2005/7/5, Duncan Murdoch <murdoch at stats.uwo.ca>:
> >> Uwe Ligges wrote:
> >> > wu sz wrote:
> >> >
> >> >
> >> >>Hello,
> >> >>
> >> >>I use "select.list" to obtain a window of select items, but how can I
> >> >>set the position and size of this window?
> >> >>
> >> >>Are there any functions which are used to maximize and minimize the
> >> >>window of R Console?
> >> >
> >> >
> >> > You cannot fo windows from select.list and R console, AFAIK, but you can
> >> > do for windows() devices using windows() and brintToTop(), for example.
> >>
> >> That's a typo:  it's bringToTop().  Another function which could be used
> >> to do this is getWindowsHandle, along with C level programming to the
> >> Windows API.  I haven't looked into this, so it's possible the UI
> >> library we use will get confused, but it should be possible to send
> >> Windows messages to those handles.
> >>
> >> We do have longstanding bugs with the maximized/minimized status being
> >> overridden by various reset commands.  I tracked down some of these
> >> once, but didn't get all of them (and did introduce some new bugs).  I
> >> don't have the energy to fix the rest.
> >>
> >> Duncan Murdoch
> >>
> 
>



From sfalcon at fhcrc.org  Tue Jul  5 21:19:43 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 05 Jul 2005 12:19:43 -0700
Subject: [R] download.file() / install.packages() from a url with a
	username and password
In-Reply-To: <7D85A100-8B30-4E19-BFBC-4372E9B89AC0@wmgfunds.com> (John
	Marsland's message of "Thu, 30 Jun 2005 11:10:18 +0100")
References: <7D85A100-8B30-4E19-BFBC-4372E9B89AC0@wmgfunds.com>
Message-ID: <m2oe9h5acw.fsf@macaroni.local>

On 30 Jun 2005, john.marsland at wmgfunds.com wrote:

> I am trying to create a repository for my own packages as an easy
> way to auto install packages on a number of servers.
>
> Obviously, I am able to connect using install.packages() to CRAN  
> without problems but when I specify my own repos I get an error:
>
>> install.packages(pkgs, repos="http://some.site.com")
> Warnings message:
> cannot open: HTTP status was '401 Authorisation Required'
>
> I get a similar message if I try to use download.file() on the  
> PACKAGES file.
>
> I have tried the url in the form http://username:password at url.com.
>
> Any ideas?

Are you expecting your webserver to require authentication?  If not,
well, I think you have a server config issue.

Assuming you have configured authentication, I suspect it will be
difficult to get install.packages to play along.  

Perhaps there is another way you can protect the site.  For example,
only allow local connections and then use ssh port forwarding to
connect to the site from "outside".

+ seth



From tlumley at u.washington.edu  Tue Jul  5 21:39:36 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Jul 2005 12:39:36 -0700 (PDT)
Subject: [R] write.foreign and SPSS
In-Reply-To: <42CADA5B.7070004@optonline.net>
References: <200507051646.j65GkH7w021671@gator.dt.uh.edu>
	<42CADA5B.7070004@optonline.net>
Message-ID: <Pine.A41.4.61b.0507051217170.266518@homer08.u.washington.edu>

On Tue, 5 Jul 2005, Chuck Cleland wrote:
>
>   In SPSS, open the syntax file "warpbreaks.sps".  This syntax will
> read in the data and apply variable and value labels to variables that
> were factors in R.  I find you often need to edit the syntax file
> created by write.foreign just a bit in SPSS.

In that case there is a bug and it would have been helpful to have a bug 
report. This was tested on a relatively old AIX version of SPSS that may 
be less fussy. I've added the extra punctuation.

 	-thomas



From kemerson at darkwing.uoregon.edu  Tue Jul  5 22:08:29 2005
From: kemerson at darkwing.uoregon.edu (Kevin J Emerson)
Date: 05 Jul 2005 13:08:29 -0700
Subject: [R] logistic regression asymptote problem
Message-ID: <1120594109.4641.11.camel@d43-8.uoregon.edu>

R-helpers,

I have a question about logistic regressions.

Consider a case where you have binary data that reaches an asymptote
that is not 1, maybe its 0.5.  Can I still use a logistic regression to
fit a curve to this data?  If so, how can I do this in R.  As far as I
can figure out, using a logit link function assumes that the asymptote
is at y = 1.

An example.  Consider the following data:

"tmp" <-
structure(list(x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 
14), yes = c(0, 0, 0, 2, 1, 14, 24, 15, 23, 18, 22, 20, 14, 17
), no = c(94, 101, 95, 80, 81, 63, 51, 56, 30, 38, 31, 18, 21, 
20)), .Names = c("x", "yes", "no"), row.names = c("1", "2", "3", 
"4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14"), class =
"data.frame")

where x is the independent variable, and yes and no are counts of
events.  plotting the data you can see that the data seem to reach an
asymptote at around y=0.5.  using glm to fit a logistic regression it is
easily seen that it does not fit well.

tmp.glm <- glm(cbind(yes,no) ~ x, data = tmp, family = binomial(link =
logit))
plot(tmp.glm$fitted, type = "l", ylim = c(0,1))
par(new=T)
plot(tmp$yes / (tmp$yes + tmp$no), ylim = c(0,1))

Any suggestions would be greatly appreciated.

Cheers,
Kevin

-- 
------------------------------------
------------------------------------
Kevin J Emerson
Center for Ecology and Evolutionary Biology
1210 University of Oregon
University of Oregon
Eugene, OR 97403
kemerson at darkwing.uoregon.edu



From spencer.graves at pdf.com  Tue Jul  5 23:28:58 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 05 Jul 2005 14:28:58 -0700
Subject: [R] logistic regression asymptote problem
In-Reply-To: <1120594109.4641.11.camel@d43-8.uoregon.edu>
References: <1120594109.4641.11.camel@d43-8.uoregon.edu>
Message-ID: <42CAFB9A.7040009@pdf.com>

	  I saw a standard overdispersed binomial.  In particular, I saw NO 
evidence of saturation at 0.5 or anything below 1.  I did the following:


tmp$N <- tmp$yes+tmp$no

with(tmp, plot(x, yes))
with(tmp, plot(x, yes/N))

tmp.glm <- glm(cbind(yes,no) ~ x, data = tmp, family = binomial(link 
=logit))
tmp.glmq <- glm(cbind(yes,no) ~ x, data = tmp, family = 
quasibinomial(link =logit))
summary(tmp.glm)
summary(tmp.glmq)

plot(tmp.glm)
plot(tmp.glmq)

# Test the statistical significance of the "Dispersion" parameter
pchisq(summary(tmp.glmq)$dispersion*12, 12, lower=FALSE)

	  hope this helps.
	  spencer graves


Kevin J Emerson wrote:

> R-helpers,
> 
> I have a question about logistic regressions.
> 
> Consider a case where you have binary data that reaches an asymptote
> that is not 1, maybe its 0.5.  Can I still use a logistic regression to
> fit a curve to this data?  If so, how can I do this in R.  As far as I
> can figure out, using a logit link function assumes that the asymptote
> is at y = 1.
> 
> An example.  Consider the following data:
> 
> "tmp" <-
> structure(list(x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 
> 14), yes = c(0, 0, 0, 2, 1, 14, 24, 15, 23, 18, 22, 20, 14, 17
> ), no = c(94, 101, 95, 80, 81, 63, 51, 56, 30, 38, 31, 18, 21, 
> 20)), .Names = c("x", "yes", "no"), row.names = c("1", "2", "3", 
> "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14"), class =
> "data.frame")
> 
> where x is the independent variable, and yes and no are counts of
> events.  plotting the data you can see that the data seem to reach an
> asymptote at around y=0.5.  using glm to fit a logistic regression it is
> easily seen that it does not fit well.
> 
> tmp.glm <- glm(cbind(yes,no) ~ x, data = tmp, family = binomial(link =
> logit))
> plot(tmp.glm$fitted, type = "l", ylim = c(0,1))
> par(new=T)
> plot(tmp$yes / (tmp$yes + tmp$no), ylim = c(0,1))
> 
> Any suggestions would be greatly appreciated.
> 
> Cheers,
> Kevin
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From antoniou at central.ntua.gr  Tue Jul  5 23:34:17 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Wed, 6 Jul 2005 00:34:17 +0300
Subject: [R] multivariate lme?
Message-ID: <1C335732-08B8-43CE-8C91-60139D114521@central.ntua.gr>

Dear all,

is it possible to estimate a multivariate multilevel model in R (I  
guess the term in R is mixed-effects model).  I can estimate  
univariate models using either lme{nlme} or lmer{lme4}, but when I  
use a multivariate response, I get the same output as if I was trying  
to estimate a univariate model.

[For a reproducible example, please download http://mit.edu/costas/ 
www/parav.txt and run:

 >Parav<-read.table("parav.txt",header=T)
 >library(nlme)
 >attach(Parav)
]


 >myResponse <- cbind(accidents,Killed)

I get the same thing as if I used only accidents as the response.

For example:

 >parav7.lme <- lme(accidents~alcontrols,random=~1+alcontrols| 
department/prefecture,data=Parav)

yields the same output as:

 >parav8.lme <- lme(myResponse~alcontrols,random=~1+alcontrols| 
department/prefecture,data=Parav)


Thank you,

Costas

PS. I am working on R-2.1.0a on a Mac 10.4.1.




--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From ggrothendieck at gmail.com  Tue Jul  5 23:41:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Jul 2005 17:41:26 -0400
Subject: [R] bringToBottom? and tcltk document
In-Reply-To: <ea57975b05070509123910d25@mail.gmail.com>
References: <ea57975b05070216074cb61414@mail.gmail.com>
	<ea57975b050704180826607390@mail.gmail.com>
	<42CA44D6.1090700@statistik.uni-dortmund.de>
	<42CA6512.90709@stats.uwo.ca>
	<ea57975b05070509123910d25@mail.gmail.com>
Message-ID: <971536df05070514415bcae240@mail.gmail.com>

On 7/5/05, wu sz <r.shengzhe at gmail.com> wrote:

> Are there any introductory and detailed "tcltk" documents or help
> files or books for using this package in R ?

Do a google search for   tcltk examples   and look at the first
hit.  That page in turn links to many tcltk examples and the last
link is not an example but a link to 'Other sources...' of information.



From atp at piskorski.com  Tue Jul  5 23:44:49 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Tue, 5 Jul 2005 17:44:49 -0400
Subject: [R] build R source package in place from CVS?
Message-ID: <20050705214449.GA29864@piskorski.com>

I'm currently using R CMD INSTALL to build and install some of my own
custom R packages.  Basically, I use a script which first builds a
tarball of my R source code, and then calls R CMD INSTALL, which
builds and installs that source package from the tarball, including
re-compiling all my C code from scratch every single time, which is
both totally unneccessary and tediously slow.

What I would like to do instead (and what I do in fact do with S-Plus)
is simply cd to the directoy where the production copy of my R package
lives, do a CVS update, and then rebuild my R package right there in
that directory.  I do not want to generate any intermediary tarballs
nor copy the package files to any other locations.

Is there any current way to build R packages in place like that?  Or
if not, how would you suggest I go about accomplishing this?

Thanks!

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/



From deepayan.sarkar at gmail.com  Wed Jul  6 00:05:18 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 5 Jul 2005 17:05:18 -0500
Subject: [R] Kind of 2 dim histogram - levelplot
In-Reply-To: <47fce06505070502034866236c@mail.gmail.com>
References: <47fce06505070502034866236c@mail.gmail.com>
Message-ID: <eb555e6605070515057fe1fb23@mail.gmail.com>

On 7/5/05, Hans-Peter <gchappi at gmail.com> wrote:
> Dear R-List,
> 
> I've written some code to put measurement values at a position x and y
> in bins (xb and yb). It works, but I wonder if there isn't a function
> that would do what I do by hand in "# fill data in bins"?

Here's one way:

tab = table(cut(x, breaks = 1:4), cut(y, breaks = 4:7))
levelplot(tab, xlim = rownames(tab), ylim = colnames(tab))

You might also want to try the hexbin package, which does hexagonal
binning as opposed to rectangular binning here.

Deepayan



From OlsenN at pac.dfo-mpo.gc.ca  Wed Jul  6 00:16:38 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Tue, 5 Jul 2005 15:16:38 -0700 
Subject: [R] Invalid device number in savePlot
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECBF4D@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050705/5506da46/attachment.pl

From rvaradha at jhsph.edu  Wed Jul  6 00:26:41 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Tue, 5 Jul 2005 18:26:41 -0400
Subject: [R] generalized gamma random number generation
In-Reply-To: <d812a006050705064722d8ea52@mail.gmail.com>
Message-ID: <OWA-1fZ2DyYAnBfQnMK000056cf@owa-1.sph.ad.jhsph.edu>

Look at the function "rggamma" in J.K. Lindsey's package "rmutil".

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Anubhav Manglick
> Sent: Tuesday, July 05, 2005 9:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] generalized gamma random number generation
> 
> hi
> I am stuck with the generation of generalized gamma distributed random
> numbers
> 
> can u plese help me
> 
> anubhav
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From gunter.berton at gene.com  Wed Jul  6 00:33:19 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 5 Jul 2005 15:33:19 -0700
Subject: [R] Invalid device number in savePlot
In-Reply-To: <7CBBD627E4E688499349A5D11D07831602ECBF4D@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>
Message-ID: <200507052233.j65MXJuS017863@compton.gene.com>

I believe that this is probably related to a problem I sporadically
experience in the Windows GUI where after I page up through several graphs
in a windows() graphics window and page back down, the last graph is gone.
As you said, a minor annoyance that is probably more of a pain to fix than
it's worth. But you're not alone. :-)

FWIW:

OS       Windows 2000 V5 SP4
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R   

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> OlsenN at pac.dfo-mpo.gc.ca
> Sent: Tuesday, July 05, 2005 3:17 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Invalid device number in savePlot
> 
> Hi,
> I recently encountered an error using the command "savePlot" 
> when trying to
> save the third of 3 open graph windows.  After successfully saving and
> closing the first two windows I receive an "Invalid device number in
> savePlot" error.  The following is copied and pasted from an 
> example session
> to illustrate the behaviour:
>  
> > windows()
> > hist(rnorm(100))
> > windows()
> > hist(rnorm(100))
> > windows()
> > hist(rnorm(100))
> > savePlot("foo", "wmf")
> > dev.off()
> windows 
>       2 
> > savePlot("foo", "wmf")
> > dev.off()
> windows 
>       3 
> > savePlot("foo", "wmf")
> Error in savePlot("foo", "wmf") : invalid device number in savePlot
>  
> If I check the current active device and manually set this 
> parameter in
> savePlot, I still receive the error:
>  
> > dev.cur()
> windows 
>       3
> > savePlot("foo", "wmf", device=3)
> Error in savePlot("foo", "wmf", device = 3) : 
>         invalid device number in savePlot
>  
> This behaviour is certainly not a critical issue for me so 
> I'm not looking
> for a solution; I just thought I should report the issue in 
> case it's a bug
> ... or perhaps I'm simply doing something stupid without realising it.
> Norm
>  
> WinXP SP2
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.1            
> year     2005           
> month    06             
> day      20             
> language R
>  
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Jul  6 00:34:15 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 05 Jul 2005 18:34:15 -0400
Subject: [R] build R source package in place from CVS?
In-Reply-To: <20050705214449.GA29864@piskorski.com>
References: <20050705214449.GA29864@piskorski.com>
Message-ID: <42CB0AE7.4000009@stats.uwo.ca>

Andrew Piskorski wrote:
> I'm currently using R CMD INSTALL to build and install some of my own
> custom R packages.  Basically, I use a script which first builds a
> tarball of my R source code, and then calls R CMD INSTALL, which
> builds and installs that source package from the tarball, including
> re-compiling all my C code from scratch every single time, which is
> both totally unneccessary and tediously slow.
> 
> What I would like to do instead (and what I do in fact do with S-Plus)
> is simply cd to the directoy where the production copy of my R package
> lives, do a CVS update, and then rebuild my R package right there in
> that directory.  I do not want to generate any intermediary tarballs
> nor copy the package files to any other locations.
> 
> Is there any current way to build R packages in place like that?  Or
> if not, how would you suggest I go about accomplishing this?

You can use

  make pkg-foo

to build and install a package whose source is in R_HOME/src/library/foo, or

  R CMD INSTALL foo

to build and install one which is in the foo directory.  No tarballs.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Wed Jul  6 09:45:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Jul 2005 09:45:59 +0200
Subject: [R] build R source package in place from CVS?
In-Reply-To: <42CB0AE7.4000009@stats.uwo.ca>
References: <20050705214449.GA29864@piskorski.com>
	<42CB0AE7.4000009@stats.uwo.ca>
Message-ID: <42CB8C37.5060000@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> Andrew Piskorski wrote:
> 
>>I'm currently using R CMD INSTALL to build and install some of my own
>>custom R packages.  Basically, I use a script which first builds a
>>tarball of my R source code, and then calls R CMD INSTALL, which
>>builds and installs that source package from the tarball, including
>>re-compiling all my C code from scratch every single time, which is
>>both totally unneccessary and tediously slow.
>>
>>What I would like to do instead (and what I do in fact do with S-Plus)
>>is simply cd to the directoy where the production copy of my R package
>>lives, do a CVS update, and then rebuild my R package right there in
>>that directory.  I do not want to generate any intermediary tarballs
>>nor copy the package files to any other locations.
>>
>>Is there any current way to build R packages in place like that?  Or
>>if not, how would you suggest I go about accomplishing this?
> 
> 
> You can use
> 
>   make pkg-foo
> 
> to build and install a package whose source is in R_HOME/src/library/foo, or
> 
>   R CMD INSTALL foo
> 
> to build and install one which is in the foo directory.  No tarballs.
> 
> Duncan Murdoch

I think the question was how to avoid to pollute the source tree.
And the answer to this question is to write a makefile that provides a 
mechanism to clean up after installation - or at least a "make clean".

For myself: For packages I am using the mechanism Andrew descibed at 
first (building at first); for building R, I am copying the source (svn) 
tree and "make"ing from the copy.

Uwe Ligges



From roy.werkman at asml.com  Wed Jul  6 11:00:27 2005
From: roy.werkman at asml.com (Roy Werkman)
Date: Wed, 6 Jul 2005 11:00:27 +0200
Subject: [R] Question about statistics
Message-ID: <448071208107374B96ED90585EEBA9127EA7C9@NLVDHX84.sn-eu.asml.com>


Hi,

Although my question is not directly linked to R functionality, I hope
you can forgive me for posing it here. I have been looking for the
answer for a long time (~ 4 weeks) and have not been able to find it. My
question is:

Suppose I have an m*n matrix, with a random (normally distributed)
number per cell. Added to that I have a random number per column. I want
to determine the standard deviation of both distributions. For the
column-to-column (coco) sd I do following:

sd(coco) = sqrt(     sd(column averages)^2 - C * sd(cece)^2 /m     )

Where C = (m*n-1) / (m*n-n) , and sd(cece) is the sd over the matrix
with the column averages subtracted.

My question: how can I calculate the confidence interval on sd(coco)?
 

I would really appreciate your help,
Roy Werkman


-- 
The information contained in this communication and any atta...{{dropped}}



From ggrothendieck at gmail.com  Tue Jul  5 14:57:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Jul 2005 08:57:26 -0400
Subject: [R] [R-pkgs] batchfiles for Windows
Message-ID: <971536df05070505571023a56f@mail.gmail.com>

batchfiles, available on CRAN at

   http://cran.r-project.org/contrib/extra/batchfiles/

consists of a set of Windows XP batch files (they may run on 
NT/2000 too but that has not been tested) which facilitate:

1. starting R, automatically locating it in the registry
   each time they are run, so that when a new version of 
   R is installed its not necessary to update them all 
   - Rcmd.bat, Rgui.bat, R.bat

2. copying configuration files to new versions of R when R
   is updated and optionally updating MiKTeX to be consistent 
   with a new R version.  After a one-time configuration just 
   run the batch file after each new download and install of R 
   - Rrefresh.bat

3. use of .Rbuildignore when checking and installing 
   R packages automatically running a build first and then 
   'Rcmd CHECK' or 'Rcmd INSTALL' in one step.  Also sets R_LIBS
   to .../R/library if R_LIBS has not already been set.
   - makepkgs.bat

4. locating various R tools that are used in building R packages
   on your system by querying the registry.
   - Rfind.bat

Comments welcome.

Gabor Grothendieck

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From phdhwang at gmail.com  Wed Jul  6 14:14:08 2005
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Wed, 6 Jul 2005 21:14:08 +0900
Subject: [R] How to sample x-y coordinates from GIS files
Message-ID: <b040cbb005070605144aecf23d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/ab18efe0/attachment.pl

From halldor.bjornsson at gmail.com  Wed Jul  6 14:42:55 2005
From: halldor.bjornsson at gmail.com (halldor bjornsson)
Date: Wed, 6 Jul 2005 12:42:55 +0000
Subject: [R] How to sample x-y coordinates from GIS files
In-Reply-To: <b040cbb005070605144aecf23d@mail.gmail.com>
References: <b040cbb005070605144aecf23d@mail.gmail.com>
Message-ID: <6962245305070605425a994fd5@mail.gmail.com>

Check out the maptools package.
It allows you to read ESRI shapefiles.


On 7/6/05, Kum-Hoe Hwang <phdhwang at gmail.com> wrote:
> Hi Gurus!
>  I have a job that is to get randomly samples from point-based GIS data (sp
> called shape GIS files) under the total sum resctricted.
>  For example, I would like to take random smaples under the 1000 persons in
> each city.
> The randomly sampled persons should not be over 1000 any case.
>  Thank you in advance,
> 
> --
> Kum-Hoe Hwang, Ph.D.
> 
> Phone : 82-31-250-3283
> Email : phdhwang at gmail.com
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Wed Jul  6 14:53:00 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 6 Jul 2005 07:53:00 -0500
Subject: [R] multivariate lme?
In-Reply-To: <1C335732-08B8-43CE-8C91-60139D114521@central.ntua.gr>
References: <1C335732-08B8-43CE-8C91-60139D114521@central.ntua.gr>
Message-ID: <40e66e0b05070605531a15437@mail.gmail.com>

Neither lme nor lmer has provision for fitting multivariate models.

On 7/5/05, Constantinos Antoniou <antoniou at central.ntua.gr> wrote:
> Dear all,
> 
> is it possible to estimate a multivariate multilevel model in R (I
> guess the term in R is mixed-effects model).  I can estimate
> univariate models using either lme{nlme} or lmer{lme4}, but when I
> use a multivariate response, I get the same output as if I was trying
> to estimate a univariate model.
> 
> [For a reproducible example, please download http://mit.edu/costas/
> www/parav.txt and run:
> 
>  >Parav<-read.table("parav.txt",header=T)
>  >library(nlme)
>  >attach(Parav)
> ]
> 
> 
>  >myResponse <- cbind(accidents,Killed)
> 
> I get the same thing as if I used only accidents as the response.
> 
> For example:
> 
>  >parav7.lme <- lme(accidents~alcontrols,random=~1+alcontrols|
> department/prefecture,data=Parav)
> 
> yields the same output as:
> 
>  >parav8.lme <- lme(myResponse~alcontrols,random=~1+alcontrols|
> department/prefecture,data=Parav)
> 
> 
> Thank you,
> 
> Costas
> 
> PS. I am working on R-2.1.0a on a Mac 10.4.1.
> 
> 
> 
> 
> --
> Constantinos Antoniou, Ph.D.
> Department of Transportation Planning and Engineering
> National Technical University of Athens
> 5, Iroon Polytechniou str. GR-15773, Athens, Greece
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From marfru at cartif.es  Wed Jul  6 14:55:23 2005
From: marfru at cartif.es (Mario de Frutos Dieguez)
Date: Wed, 06 Jul 2005 14:55:23 +0200
Subject: [R] Java and R help
In-Reply-To: <42CAB42F.4060200@cartif.es>
References: <42CAB42F.4060200@cartif.es>
Message-ID: <42CBD4BB.5040404@cartif.es>

Mario de Frutos Dieguez escribi??:

>Im doing an aplication in Java and i have a program made in R what i 
>want to launch with Java.
>
>I have the following instructions:
>
>Runtime r = Runtime.getRuntime();
>       
>try
>{
>
>            System.out.println ("Llamada a R...");
>            p = r.exec(sRutaR);
>        }
>        catch (IOException e)
>        {
>            System.out.println ("Error lanzando R: " + e.getMessage());
>            e.printStackTrace();
>        }
>        catch (Exception ex)
>        {
>            System.out.println ("Error lanzando R!!!! " + ex.toString());
>            ex.printStackTrace();
>        }
>}
>
>and after that i wait for a file that R must to make called 
>terminado.dat. But when i launch the process the file doesn't create 
>until i destroy the process.
>
>can anyone explain what's happend with the process?
>
>Thx in advance and sorry for my poor english
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
I think i solve the problem but i dont solve anything hehehe. But i know
what's the problem. When i launch the R program like a process it enter
in deadlock with the parent thread and dont finish until i stop the main
thread.

Anybody has suffer the same nightmare like i? and can help me?

Thx in advance, and sorry for my poor english hehe



From ecoinformatics at gmail.com  Wed Jul  6 15:11:28 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Wed, 6 Jul 2005 15:11:28 +0200
Subject: [R] How to sample x-y coordinates from GIS files
In-Reply-To: <b040cbb005070605144aecf23d@mail.gmail.com>
References: <b040cbb005070605144aecf23d@mail.gmail.com>
Message-ID: <15f8e67d0507060611cceb7a7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/6026ea13/attachment.pl

From GEmukule at ke.cdc.gov  Wed Jul  6 15:29:49 2005
From: GEmukule at ke.cdc.gov (Emukule, Gideon)
Date: Wed, 6 Jul 2005 16:29:49 +0300
Subject: [R] sktest output
Message-ID: <97DAB59B6DDF9A4A96DD81C061FC829941D376@exp-kek1.ke.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/d286348f/attachment.pl

From gaurav at virtuaresearch.com  Wed Jul  6 15:52:15 2005
From: gaurav at virtuaresearch.com (Gaurav Gupta)
Date: Wed, 6 Jul 2005 09:52:15 -0400
Subject: [R] R COM classes
Message-ID: <200507061352.j66DqHlJ009056@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/4664bacf/attachment.pl

From ghislainv at gmail.com  Wed Jul  6 16:09:17 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Wed, 6 Jul 2005 16:09:17 +0200
Subject: [R] Plotting confidence intervals for lme
Message-ID: <ff51f022050706070971afca67@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/687ef85e/attachment.pl

From macq at llnl.gov  Wed Jul  6 16:45:25 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 6 Jul 2005 07:45:25 -0700
Subject: [R] timezone problems
In-Reply-To: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>
References: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <p06210203bef19c99bfe2@[128.115.153.6]>

How did you set the TZ system variable?
If you did not use Sys.putenv(), try using it instead.
Otherwise, I think you have to ask the package maintainer.

You may be misleading yourself by using Sys.time() to test whether TZ is set.
What does Sys.getenv() tell you?

I get a timezone code from Sys.time() even when TZ is not defined 
(see example below).
(but I do have a different OS)

>  Sys.timezone()
[1] ""
>  Sys.time()
[1] "2005-07-06 07:34:15 PDT"
>  Sys.getenv('TZ')
TZ
""
>  Sys.putenv(TZ='US/Pacific')
>  Sys.timezone()
[1] "US/Pacific"
>  Sys.getenv('TZ')
           TZ
"US/Pacific"
>  Sys.time()
[1] "2005-07-06 07:34:38 PDT"

>  Sys.putenv(TZ='GMT')
>  Sys.time()
[1] "2005-07-06 14:35:45 GMT"

>  version
          _                       
platform powerpc-apple-darwin7.9.0
arch     powerpc                 
os       darwin7.9.0             
system   powerpc, darwin7.9.0    
status                           
major    2                       
minor    1.1                     
year     2005                    
month    06                      
day      20                      
language R                       


At 9:55 AM +0000 7/5/05, Martin Keller-Ressel wrote:
>Hi,
>
>Im using R 2.1.1 and running Code that previously worked (on R 2.1.0 I 
>believe) using the 'timeDate' function from the fCalendar package. The 
>code now throws an error:
>
>Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")
>
>However I have read the documentation of the fCalendar package and I have 
>set my system variable TZ to GMT.
>I tracked the error down to the function Sys.timezone() which returns NA 
>in spite of what Sys.time() returns.
>
>>  Sys.timezone()
>[1] NA
>
>>  Sys.time()
>[1] "2005-07-05 08:41:53 GMT"
>
>My version:
>
>>  version
>           _
>platform i386-pc-mingw32
>arch     i386
>os       mingw32
>system   i386, mingw32
>status
>major    2
>minor    1.1
>year     2005
>month    06
>day      20
>language R
>
>Any help is appreciated,
>
>Martin Keller-Ressel
>
>
>---
>Martin Keller-Ressel
>Research Unit of Financial and Actuarial Mathematics
>TU Vienna
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From bill.shipley at usherbrooke.ca  Wed Jul  6 16:45:54 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 6 Jul 2005 10:45:54 -0400
Subject: [R] function for maximum entropy distributions
Message-ID: <004101c58239$69a33ea0$b01ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/8a5e2df6/attachment.pl

From r.shengzhe at gmail.com  Wed Jul  6 17:22:15 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Wed, 6 Jul 2005 17:22:15 +0200
Subject: [R] how to split several objects' names
In-Reply-To: <ea57975b0506240450621f0477@mail.gmail.com>
References: <ea57975b0506240450621f0477@mail.gmail.com>
Message-ID: <ea57975b050706082254dd922e@mail.gmail.com>

Hello,

I have a string like "O1 O2 O3", O1, O2 and O3 are the names of three
objects. How to split it into a vector or three new objects, and
obtain their content? I try to use strsplit to do that, but seems it
doesn't work like that.

And what is "<<-" for?   the same as "<-"?

Thank you,
Shengzhe



From kerrin at eden.rutgers.edu  Wed Jul  6 17:29:12 2005
From: kerrin at eden.rutgers.edu (Kerri-Ann Norton)
Date: Wed,  6 Jul 2005 11:29:12 -0400 (EDT)
Subject: [R] Error message NA/NaN/Inf in foreign function call (arg 6) when
 using knn()
Message-ID: <26683572.1120663752028.JavaMail.tomcat@roadmaster>

I am trying to use knn to do a nearest neighbor classification.  I tried using my dataset and got an error message so I used a simple example to try and understand what I was doing wrong and got the same message.  Here is what I typed into R:
 try
  [,1] [,2] [,3] [,4]
r "A"  "A"  "T"  "G" 
r "A"  "A"  "T"  "G" 
f "A"  "A"  "c"  "G" 
f "A"  "A"  "c"  "G" 
f "A"  "A"  "c"  "G" 
> cl2 <-factor(c(rep("1",2), rep("2",3)))
> cl2
[1] 1 1 2 2 2
Levels: 1 2
> knn(try, try, cl2, k = 2)
Error in knn(try, try, cl2, k = 2) : NA/NaN/Inf in foreign function call (arg 6)
In addition: Warning messages: 
1: NAs introduced by coercion 
2: NAs introduced by coercion 

I used try as test and train because I thought the error might be that the size of test and train data were different.  If someone could explain what the error means or how to fix it, I would greatly appreciate it.
Kerri-Ann Norton



From sundar.dorai-raj at pdf.com  Wed Jul  6 17:30:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 06 Jul 2005 10:30:37 -0500
Subject: [R] how to split several objects' names
In-Reply-To: <ea57975b050706082254dd922e@mail.gmail.com>
References: <ea57975b0506240450621f0477@mail.gmail.com>
	<ea57975b050706082254dd922e@mail.gmail.com>
Message-ID: <42CBF91D.6010207@pdf.com>



wu sz wrote:
> Hello,
> 
> I have a string like "O1 O2 O3", O1, O2 and O3 are the names of three
> objects. How to split it into a vector or three new objects, and
> obtain their content? I try to use strsplit to do that, but seems it
> doesn't work like that.

Please provide an example of what you tried or a complete example (see 
posting guide in signature for tips). I get:

 > strsplit("O1 O2 O3", " ")
[[1]]
[1] "O1" "O2" "O3"

And if O1, O2, and O3 are objects then use ?get, as in:

O1 <- rnorm(10)
O2 <- rnorm(10)
O3 <- rnorm(10)
O <- strsplit("O1 O2 O3", " ")[[1]]
lapply(O, get)

> 
> And what is "<<-" for?   the same as "<-"?
> 

Did you try help("<<-")?

HTH,

--sundar



From spencer.graves at pdf.com  Wed Jul  6 18:22:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 09:22:06 -0700
Subject: [R] Question about statistics
In-Reply-To: <448071208107374B96ED90585EEBA9127EA7C9@NLVDHX84.sn-eu.asml.com>
References: <448071208107374B96ED90585EEBA9127EA7C9@NLVDHX84.sn-eu.asml.com>
Message-ID: <42CC052E.6060607@pdf.com>

	  What do you think about the following:

library(nlme)
set.seed(1)
n <- 3;m <- 4
s.e <- 0
(X0 <- array(rep(rnorm(n), each=m)+s.e*rnorm(m*n),
            dim=c(m, n)))
s.e <- 1
(X1 <- array(rep(rnorm(n), each=m)+s.e*rnorm(m*n),
            dim=c(m, n)))

X. <- data.frame(Row=as.vector(row(X)),
           Col=as.vector(col(X)),x=as.vector(X1))
lme(x~Col, random=~1|Col, data=X.)

	  For more information on this, I highly recommend Pinheiro and Bates 
(2000) Mixed-Effects Models in S and S-Plus (Springer).

	  spencer graves

Roy Werkman wrote:
> Hi,
> 
> Although my question is not directly linked to R functionality, I hope
> you can forgive me for posing it here. I have been looking for the
> answer for a long time (~ 4 weeks) and have not been able to find it. My
> question is:
> 
> Suppose I have an m*n matrix, with a random (normally distributed)
> number per cell. Added to that I have a random number per column. I want
> to determine the standard deviation of both distributions. For the
> column-to-column (coco) sd I do following:
> 
> sd(coco) = sqrt(     sd(column averages)^2 - C * sd(cece)^2 /m     )
> 
> Where C = (m*n-1) / (m*n-n) , and sd(cece) is the sd over the matrix
> with the column averages subtracted.
> 
> My question: how can I calculate the confidence interval on sd(coco)?
>  
> 
> I would really appreciate your help,
> Roy Werkman
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From pigood at verizon.net  Wed Jul  6 18:35:12 2005
From: pigood at verizon.net (Phillip Good)
Date: Wed, 06 Jul 2005 09:35:12 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <40e66e0b0507041344596b2a22@mail.gmail.com>
Message-ID: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>

Do you or Lumley have a citation for this conclusion?   Most people go
forward with the ANOV on the basis that the various tests are independent.

Phillip Good
P.S.  Tests based on the method of synchronized permutations are
independent.

-----Original Message-----
From: Douglas Bates [mailto:dmbates at gmail.com]
Sent: Monday, July 04, 2005 1:44 PM
To: pigood at verizon.net
Cc: r-help
Subject: Re: [R] Lack of independence in anova()


I wrote up a note on how to do a more efficient simulation of the
p-values from anova then discovered to my surprise that the chi-square
test for independence of the significance of the F-tests indicated
that they were not independent.  I was stumped by this but fortunately
Thomas Lumley came to my rescue with an explanation.  There is no
reason why the results of the F tests should be independent.  The
numerators are independent but the denominator is the same for both
tests.  When, due to random variation, the denominator is small, then
the p-values for both tests will tend to be small.  If, instead of
F-tests you use chi-square tests then you do see independence.

Here is the note on the simulation.

There are several things that could be done to speed the simulation of
the p-values of an anova like this under the null distribution.

If you examine the structure of a fitted lm object (use the function
str()) you will see that there are components called `qr', `effects'
and `assign'.  You can verify by experimentation that `qr' and
`assign' depend only on the experimental design.  Furthermore, the
`effects' vector can be reproduced as qr.qty(qrstr, y).

The sums of squares for the different terms in the model are the sums
of squares of elements of the effects vector as indexed by the assign
vector.  The residual sum of squares is the sum of squares of the
remaining elements of the assign vector.  You can generate 10000
replications of the calculations of the relevant sums of squares as

> set.seed(1234321)
> vv <- data.frame(c = gl(3,3,18), r = gl(2,9,18))
> vv
   c r
1  1 1
2  1 1
3  1 1
4  2 1
5  2 1
6  2 1
7  3 1
8  3 1
9  3 1
10 1 2
11 1 2
12 1 2
13 2 2
14 2 2
15 2 2
16 3 2
17 3 2
18 3 2
> fm1 <- lm(rnorm(18) ~ c*r, vv)
> fm1$assign
[1] 0 1 1 2 3 3
> asgn <- c(fm1$assign, rep(4, 12))
> system.time(res <- replicate(10000, tapply(qr.qty(fm1$qr, rnorm(18))^2,
asgn, sum)))
[1] 20.61  0.01 20.61  0.00  0.00
> res[,1:6]
       [,1]      [,2]     [,3]      [,4]       [,5]        [,6]
0 0.4783121 0.3048634 0.713689 0.6937838 0.03649023  2.63392426
1 0.5825213 1.4756395 1.127018 0.5209751 1.18697199  3.32972093
2 0.2612723 3.6396106 0.547506 1.1641910 0.37843963  0.03411672
3 2.6259806 3.5504584 1.645215 0.1197238 0.85361018  4.53895212
4 9.1942755 8.2122693 4.863392 5.4413571 2.03715439 22.94815118

The rows of that array correspond to the sum of squares for the
Intercept (which we generally ignore), the factor 'c', the factor 'r',
their interaction and the residuals.

As you can see that took about 21 seconds on my system, which I expect
is a bit faster than your simulation ran.

Because I set the seed to a known value I can reproduce the results
for the first few simulations to check that the sums of squares are
correct.  Remember that the original fit (fm1) is not included in the
table.

> set.seed(1234321)
> fm1 <- lm(rnorm(18) ~ c*r, vv)
> anova(fm2 <- lm(rnorm(18) ~ c*r, vv))
Analysis of Variance Table

Response: rnorm(18)
          Df Sum Sq Mean Sq F value Pr(>F)
c          2 0.5825  0.2913  0.3801 0.6917
r          1 0.2613  0.2613  0.3410 0.5701
c:r        2 2.6260  1.3130  1.7137 0.2215
Residuals 12 9.1943  0.7662

You can continue this process if you wish further verification that
the results correspond to the fitted models.

You can get the p-values for the F-tests as

> pvalsF <- data.frame(pc = pf((res[2,]/2)/(res[5,]/12), 2, 12, low =
FALSE),
+                      pr = pf((res[3,]/1)/(res[5,]/12), 1, 12, low =
FALSE),
+                      pint = pf((res[4,]/2)/(res[5,]/12), 2, 12, low =
FALSE))

Again you can check this for the first few by hand.

> pvalsF[1:5,]
          pc         pr      pint
1 0.69171238 0.57006574 0.2214847
2 0.37102129 0.03975286 0.1158059
3 0.28634939 0.26771167 0.1740633
4 0.57775850 0.13506561 0.8775828
5 0.06363138 0.16124100 0.1224806

If you wish you could then check marginal distributions using
techniques like an empirical density plot.

> library(lattice)
> densityplot(~ pc, pvals)

At this point I would recommend checking the joint distribution but if
you want to choose a specific level and check the contingency table
that could be done as

> xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF)
            I(pint < 0.16)
I(pr < 0.16) FALSE TRUE
       FALSE  7204 1240
       TRUE   1215  341

The summary method for an xtabs object provides a test of independence

> summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF))
Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsF)
Number of cases in table: 10000
Number of factors: 2
Test for independence of all factors:
	Chisq = 51.6, df = 1, p-value = 6.798e-13

for which you can see the puzzling result.  However, if you use the
chisquared test based on the known residual variance of 1, you get

> pvalsC <- data.frame(pc = pchisq(res[2,], 2, low = FALSE),
+                      pr = pchisq(res[3,], 1, low = FALSE),
+                      pint = pchisq(res[4,], 2, low = FALSE))
> pvalsC[1:5,]
         pc         pr      pint
1 0.7473209 0.60924741 0.2690144
2 0.4781553 0.05642013 0.1694446
3 0.5692081 0.45933855 0.4392846
4 0.7706757 0.28059805 0.9418946
5 0.5523983 0.53843951 0.6525907
> xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC)
            I(pint < 0.16)
I(pr < 0.16) FALSE TRUE
       FALSE  7121 1319
       TRUE   1324  236
> summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC))
Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsC)
Number of cases in table: 10000
Number of factors: 2
Test for independence of all factors:
	Chisq = 0.25041, df = 1, p-value = 0.6168



On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
> To my surprise, the R functions I employed did NOT verify the independence
> property.  I've no quarrel with the theory--it's the R functions that
worry
> me.
>
> pG
>
> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: Monday, July 04, 2005 9:13 AM
> To: pigood at verizon.net; r-help
> Subject: Re: [R] Lack of independence in anova()
>
>
> I have already had email exchanges off-list with Phillip Good pointing
> out that the independence property that he wishes to establish by
> simulation is a consequence of orthogonality of the column span of the
> row contrasts and the interaction contrasts.  If you set the contrasts
> option to a set of orthogonal contrasts such as contr.helmert or
> contr.sum, which has no effect on the results of the anova, this is
> easily established.
>
> > build
> function(size, v = rnorm(sum(size))) {
>     col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
>     rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
>     row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
>     +size[7]+size[8]))
>     return(data.frame(c=factor(col), r=factor(row),yield=v))
> }
> > size
> [1] 3 3 3 0 3 3 3 0
> > set.seed(1234321)
> > vv <- build(size)
> > vv
>    c r      yield
> 1  0 0  1.2369081
> 2  0 0  1.5616230
> 3  0 0  1.8396185
> 4  1 0  0.3173245
> 5  1 0  1.0715115
> 6  1 0 -1.1459955
> 7  2 0  0.2488894
> 8  2 0  0.1158625
> 9  2 0  2.6200816
> 10 0 1  1.2624048
> 11 0 1 -0.9862578
> 12 0 1 -0.3235653
> 13 1 1  0.2039706
> 14 1 1 -1.4574576
> 15 1 1  1.9158713
> 16 2 1 -2.0333909
> 17 2 1  1.0050272
> 18 2 1  0.6789184
> > options(contrasts = c('contr.helmert', 'contr.poly'))
> > crossprod(model.matrix(~c*r, vv))
>             (Intercept) c1 c2 r1 c1:r1 c2:r1
> (Intercept)          18  0  0  0     0     0
> c1                    0 12  0  0     0     0
> c2                    0  0 36  0     0     0
> r1                    0  0  0 18     0     0
> c1:r1                 0  0  0  0    12     0
> c2:r1                 0  0  0  0     0    36
>
>
> On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
> >  If the observations are normally distributed and the 2xk design is
> > balanced,  theory requires that the tests for interaction and row
effects
> be
> > independent.  In my program, appended below, this would translate to
cntT
> > (approx)= cntR*cntI/N if all R routines were functioning correctly.
They
> > aren't.
> >
> > sim2=function(size,N,p){
> >   cntR=0
> >   cntC=0
> >   cntI=0
> >   cntT=0
> >   cntP=0
> >   for(i in 1:N){
> >     #generate data
> >      v=gendata(size)
> >     #analyze after build(ing) design containing data
> >      lm.out=lm(yield~c*r,build(size,v))
> >      av.out=anova(lm.out)
> >     #if column effect is significant, increment cntC
> >      if (av.out[[5]][1]<=p)cntC=cntC+1
> >    #if row effect is significant, increment cntR
> >      if (av.out[[5]][2]<=p){
> >            cntR=cntR+1
> >            tmp = 1
> >            }
> >      else tmp =0
> >      if (av.out[[5]][3]<=p){
> >            #if interaction is significant, increment cntI
> >             cntI=cntI+1
> >         #if both interaction and row effect are significant, increment
> cntT
> >             cntT=cntT + tmp
> >             }
> >      }
> >     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
> > }
> >
> > build=function(size,v){
> > #size is a vector containing the sample sizes
> > col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
> > rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
> > row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
> > +size[7]+size[8]))
> > return(data.frame(c=factor(col), r=factor(row),yield=v))
> > }
> >
> > gendata=function(size){
> >   ssize=sum(size);
> >   return (rnorm(ssize))
> > }
> >
> > #Example
> >  size=c(3,3,3,0,3,3,3,0)
> >  sim2(size,10000,10,.16)
> >
> >
> >
> > Phillip Good
> > Huntington Beach CA
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Wed Jul  6 18:44:29 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 09:44:29 -0700
Subject: [R] Plotting confidence intervals for lme
In-Reply-To: <ff51f022050706070971afca67@mail.gmail.com>
References: <ff51f022050706070971afca67@mail.gmail.com>
Message-ID: <42CC0A6D.2040604@pdf.com>

	  Consider the following extension of an example in "?lme":


fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
int <- intervals(fm2)
class(int$fixed)
kf <- dim(int$fixed)[1]
plot(int$fixed[,2], kf:1,
      xlab="x", ylab="", xlim=range(int$fixed),
      axes=FALSE)
axis(1)
axis(2, kf:1, dimnames(int$fixed)[[1]])
segments(int$fixed[,1], kf:1,
          int$fixed[,3], kf:1)
abline(v=0)

	  However, we are mixing units, i.e., the units for the intercept are 
"distance", while for "age" are "distance/time", and the interpretation 
of the coefficient of a factor like Sex depends on contrasts used. 
Thus, I don't know how much sense it makes to prepare plots like this.

	  For similar plots that make more sense, see Pinheiro and Bates (2000 
Mixed-Effects Models in S and S-PLUS (Springer).

	  spencer graves

Ghislain Vieilledent wrote:

> Hello and sorry to disturb.
> 
> I'm trying to plot the confidence intervals for the fixed effects of a lme.
> I want to obtain graphically, if it is possible, a bar with Estimate, upper 
> and lower CI for each level of the factors.
> 
> I know how to do for a lm model but for a lme one, I tried with 
> plot(intervals(...)) and plot(ci(...)) from the gmodels package but it 
> doesn't work well.
> 
> Thanks for you help and have a good day.
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tlumley at u.washington.edu  Wed Jul  6 19:06:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Jul 2005 10:06:45 -0700 (PDT)
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
References: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
Message-ID: <Pine.A41.4.61b.0507060945070.109146@homer05.u.washington.edu>

On Wed, 6 Jul 2005, Phillip Good wrote:
> Do you or Lumley have a citation for this conclusion?   Most people go
> forward with the ANOV on the basis that the various tests are independent.

I don't have a citation. I would have thought that the claim that they 
were independent was more in need of a citation.  I'd expect that books on 
the classical theory of linear models would show that the row, 
interaction, and residual mean squares are independent, and the lack of 
independence of the tests is then basic probability.  If X, Y, and Z are 
independent and Z takes on more than one value then X/Z and Y/Z can't be 
independent.

> P.S.  Tests based on the method of synchronized permutations are
> independent.

Quite possibly.  If so, they presumably condition on the observed residual 
mean square.

 	-thomas




>
> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: Monday, July 04, 2005 1:44 PM
> To: pigood at verizon.net
> Cc: r-help
> Subject: Re: [R] Lack of independence in anova()
>
>
> I wrote up a note on how to do a more efficient simulation of the
> p-values from anova then discovered to my surprise that the chi-square
> test for independence of the significance of the F-tests indicated
> that they were not independent.  I was stumped by this but fortunately
> Thomas Lumley came to my rescue with an explanation.  There is no
> reason why the results of the F tests should be independent.  The
> numerators are independent but the denominator is the same for both
> tests.  When, due to random variation, the denominator is small, then
> the p-values for both tests will tend to be small.  If, instead of
> F-tests you use chi-square tests then you do see independence.
>
> Here is the note on the simulation.
>
> There are several things that could be done to speed the simulation of
> the p-values of an anova like this under the null distribution.
>
> If you examine the structure of a fitted lm object (use the function
> str()) you will see that there are components called `qr', `effects'
> and `assign'.  You can verify by experimentation that `qr' and
> `assign' depend only on the experimental design.  Furthermore, the
> `effects' vector can be reproduced as qr.qty(qrstr, y).
>
> The sums of squares for the different terms in the model are the sums
> of squares of elements of the effects vector as indexed by the assign
> vector.  The residual sum of squares is the sum of squares of the
> remaining elements of the assign vector.  You can generate 10000
> replications of the calculations of the relevant sums of squares as
>
>> set.seed(1234321)
>> vv <- data.frame(c = gl(3,3,18), r = gl(2,9,18))
>> vv
>   c r
> 1  1 1
> 2  1 1
> 3  1 1
> 4  2 1
> 5  2 1
> 6  2 1
> 7  3 1
> 8  3 1
> 9  3 1
> 10 1 2
> 11 1 2
> 12 1 2
> 13 2 2
> 14 2 2
> 15 2 2
> 16 3 2
> 17 3 2
> 18 3 2
>> fm1 <- lm(rnorm(18) ~ c*r, vv)
>> fm1$assign
> [1] 0 1 1 2 3 3
>> asgn <- c(fm1$assign, rep(4, 12))
>> system.time(res <- replicate(10000, tapply(qr.qty(fm1$qr, rnorm(18))^2,
> asgn, sum)))
> [1] 20.61  0.01 20.61  0.00  0.00
>> res[,1:6]
>       [,1]      [,2]     [,3]      [,4]       [,5]        [,6]
> 0 0.4783121 0.3048634 0.713689 0.6937838 0.03649023  2.63392426
> 1 0.5825213 1.4756395 1.127018 0.5209751 1.18697199  3.32972093
> 2 0.2612723 3.6396106 0.547506 1.1641910 0.37843963  0.03411672
> 3 2.6259806 3.5504584 1.645215 0.1197238 0.85361018  4.53895212
> 4 9.1942755 8.2122693 4.863392 5.4413571 2.03715439 22.94815118
>
> The rows of that array correspond to the sum of squares for the
> Intercept (which we generally ignore), the factor 'c', the factor 'r',
> their interaction and the residuals.
>
> As you can see that took about 21 seconds on my system, which I expect
> is a bit faster than your simulation ran.
>
> Because I set the seed to a known value I can reproduce the results
> for the first few simulations to check that the sums of squares are
> correct.  Remember that the original fit (fm1) is not included in the
> table.
>
>> set.seed(1234321)
>> fm1 <- lm(rnorm(18) ~ c*r, vv)
>> anova(fm2 <- lm(rnorm(18) ~ c*r, vv))
> Analysis of Variance Table
>
> Response: rnorm(18)
>          Df Sum Sq Mean Sq F value Pr(>F)
> c          2 0.5825  0.2913  0.3801 0.6917
> r          1 0.2613  0.2613  0.3410 0.5701
> c:r        2 2.6260  1.3130  1.7137 0.2215
> Residuals 12 9.1943  0.7662
>
> You can continue this process if you wish further verification that
> the results correspond to the fitted models.
>
> You can get the p-values for the F-tests as
>
>> pvalsF <- data.frame(pc = pf((res[2,]/2)/(res[5,]/12), 2, 12, low =
> FALSE),
> +                      pr = pf((res[3,]/1)/(res[5,]/12), 1, 12, low =
> FALSE),
> +                      pint = pf((res[4,]/2)/(res[5,]/12), 2, 12, low =
> FALSE))
>
> Again you can check this for the first few by hand.
>
>> pvalsF[1:5,]
>          pc         pr      pint
> 1 0.69171238 0.57006574 0.2214847
> 2 0.37102129 0.03975286 0.1158059
> 3 0.28634939 0.26771167 0.1740633
> 4 0.57775850 0.13506561 0.8775828
> 5 0.06363138 0.16124100 0.1224806
>
> If you wish you could then check marginal distributions using
> techniques like an empirical density plot.
>
>> library(lattice)
>> densityplot(~ pc, pvals)
>
> At this point I would recommend checking the joint distribution but if
> you want to choose a specific level and check the contingency table
> that could be done as
>
>> xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF)
>            I(pint < 0.16)
> I(pr < 0.16) FALSE TRUE
>       FALSE  7204 1240
>       TRUE   1215  341
>
> The summary method for an xtabs object provides a test of independence
>
>> summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF))
> Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsF)
> Number of cases in table: 10000
> Number of factors: 2
> Test for independence of all factors:
> 	Chisq = 51.6, df = 1, p-value = 6.798e-13
>
> for which you can see the puzzling result.  However, if you use the
> chisquared test based on the known residual variance of 1, you get
>
>> pvalsC <- data.frame(pc = pchisq(res[2,], 2, low = FALSE),
> +                      pr = pchisq(res[3,], 1, low = FALSE),
> +                      pint = pchisq(res[4,], 2, low = FALSE))
>> pvalsC[1:5,]
>         pc         pr      pint
> 1 0.7473209 0.60924741 0.2690144
> 2 0.4781553 0.05642013 0.1694446
> 3 0.5692081 0.45933855 0.4392846
> 4 0.7706757 0.28059805 0.9418946
> 5 0.5523983 0.53843951 0.6525907
>> xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC)
>            I(pint < 0.16)
> I(pr < 0.16) FALSE TRUE
>       FALSE  7121 1319
>       TRUE   1324  236
>> summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC))
> Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsC)
> Number of cases in table: 10000
> Number of factors: 2
> Test for independence of all factors:
> 	Chisq = 0.25041, df = 1, p-value = 0.6168
>
>
>
> On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>> To my surprise, the R functions I employed did NOT verify the independence
>> property.  I've no quarrel with the theory--it's the R functions that
> worry
>> me.
>>
>> pG
>>
>> -----Original Message-----
>> From: Douglas Bates [mailto:dmbates at gmail.com]
>> Sent: Monday, July 04, 2005 9:13 AM
>> To: pigood at verizon.net; r-help
>> Subject: Re: [R] Lack of independence in anova()
>>
>>
>> I have already had email exchanges off-list with Phillip Good pointing
>> out that the independence property that he wishes to establish by
>> simulation is a consequence of orthogonality of the column span of the
>> row contrasts and the interaction contrasts.  If you set the contrasts
>> option to a set of orthogonal contrasts such as contr.helmert or
>> contr.sum, which has no effect on the results of the anova, this is
>> easily established.
>>
>>> build
>> function(size, v = rnorm(sum(size))) {
>>     col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
>>     rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
>>     row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
>>     +size[7]+size[8]))
>>     return(data.frame(c=factor(col), r=factor(row),yield=v))
>> }
>>> size
>> [1] 3 3 3 0 3 3 3 0
>>> set.seed(1234321)
>>> vv <- build(size)
>>> vv
>>    c r      yield
>> 1  0 0  1.2369081
>> 2  0 0  1.5616230
>> 3  0 0  1.8396185
>> 4  1 0  0.3173245
>> 5  1 0  1.0715115
>> 6  1 0 -1.1459955
>> 7  2 0  0.2488894
>> 8  2 0  0.1158625
>> 9  2 0  2.6200816
>> 10 0 1  1.2624048
>> 11 0 1 -0.9862578
>> 12 0 1 -0.3235653
>> 13 1 1  0.2039706
>> 14 1 1 -1.4574576
>> 15 1 1  1.9158713
>> 16 2 1 -2.0333909
>> 17 2 1  1.0050272
>> 18 2 1  0.6789184
>>> options(contrasts = c('contr.helmert', 'contr.poly'))
>>> crossprod(model.matrix(~c*r, vv))
>>             (Intercept) c1 c2 r1 c1:r1 c2:r1
>> (Intercept)          18  0  0  0     0     0
>> c1                    0 12  0  0     0     0
>> c2                    0  0 36  0     0     0
>> r1                    0  0  0 18     0     0
>> c1:r1                 0  0  0  0    12     0
>> c2:r1                 0  0  0  0     0    36
>>
>>
>> On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>>>  If the observations are normally distributed and the 2xk design is
>>> balanced,  theory requires that the tests for interaction and row
> effects
>> be
>>> independent.  In my program, appended below, this would translate to
> cntT
>>> (approx)= cntR*cntI/N if all R routines were functioning correctly.
> They
>>> aren't.
>>>
>>> sim2=function(size,N,p){
>>>   cntR=0
>>>   cntC=0
>>>   cntI=0
>>>   cntT=0
>>>   cntP=0
>>>   for(i in 1:N){
>>>     #generate data
>>>      v=gendata(size)
>>>     #analyze after build(ing) design containing data
>>>      lm.out=lm(yield~c*r,build(size,v))
>>>      av.out=anova(lm.out)
>>>     #if column effect is significant, increment cntC
>>>      if (av.out[[5]][1]<=p)cntC=cntC+1
>>>    #if row effect is significant, increment cntR
>>>      if (av.out[[5]][2]<=p){
>>>            cntR=cntR+1
>>>            tmp = 1
>>>            }
>>>      else tmp =0
>>>      if (av.out[[5]][3]<=p){
>>>            #if interaction is significant, increment cntI
>>>             cntI=cntI+1
>>>         #if both interaction and row effect are significant, increment
>> cntT
>>>             cntT=cntT + tmp
>>>             }
>>>      }
>>>     list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
>>> }
>>>
>>> build=function(size,v){
>>> #size is a vector containing the sample sizes
>>> col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
>>> rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
>>> row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
>>> +size[7]+size[8]))
>>> return(data.frame(c=factor(col), r=factor(row),yield=v))
>>> }
>>>
>>> gendata=function(size){
>>>   ssize=sum(size);
>>>   return (rnorm(ssize))
>>> }
>>>
>>> #Example
>>>  size=c(3,3,3,0,3,3,3,0)
>>>  sim2(size,10000,10,.16)
>>>
>>>
>>>
>>> Phillip Good
>>> Huntington Beach CA
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From spencer.graves at pdf.com  Wed Jul  6 19:08:29 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 10:08:29 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
References: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
Message-ID: <42CC100D.6070209@pdf.com>

	  I'm confused:  I understood Doug to be describing a traditional, 
normal theory ANOVA with k rows in the table for different effects plus 
a (k+1)st for residuals and the mean squares (MS) column are all 
indpendent chi-squares scaled by the same unknown sigma^2.  The k 
statistics in the F column are ratios of independent chi-squares with 
the same independent denominator chi-square.  How can they be indpendent?

	  spencer graves
p.s.  How is the method of synchronized permutations relevant to a 
traditional, normal theory ANOVA?

Phillip Good wrote:

> Do you or Lumley have a citation for this conclusion?   Most people go
> forward with the ANOV on the basis that the various tests are independent.
> 
> Phillip Good
> P.S.  Tests based on the method of synchronized permutations are
> independent.
> 
> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: Monday, July 04, 2005 1:44 PM
> To: pigood at verizon.net
> Cc: r-help
> Subject: Re: [R] Lack of independence in anova()
> 
> 
> I wrote up a note on how to do a more efficient simulation of the
> p-values from anova then discovered to my surprise that the chi-square
> test for independence of the significance of the F-tests indicated
> that they were not independent.  I was stumped by this but fortunately
> Thomas Lumley came to my rescue with an explanation.  There is no
> reason why the results of the F tests should be independent.  The
> numerators are independent but the denominator is the same for both
> tests.  When, due to random variation, the denominator is small, then
> the p-values for both tests will tend to be small.  If, instead of
> F-tests you use chi-square tests then you do see independence.
> 
> Here is the note on the simulation.
> 
> There are several things that could be done to speed the simulation of
> the p-values of an anova like this under the null distribution.
> 
> If you examine the structure of a fitted lm object (use the function
> str()) you will see that there are components called `qr', `effects'
> and `assign'.  You can verify by experimentation that `qr' and
> `assign' depend only on the experimental design.  Furthermore, the
> `effects' vector can be reproduced as qr.qty(qrstr, y).
> 
> The sums of squares for the different terms in the model are the sums
> of squares of elements of the effects vector as indexed by the assign
> vector.  The residual sum of squares is the sum of squares of the
> remaining elements of the assign vector.  You can generate 10000
> replications of the calculations of the relevant sums of squares as
> 
> 
>>set.seed(1234321)
>>vv <- data.frame(c = gl(3,3,18), r = gl(2,9,18))
>>vv
> 
>    c r
> 1  1 1
> 2  1 1
> 3  1 1
> 4  2 1
> 5  2 1
> 6  2 1
> 7  3 1
> 8  3 1
> 9  3 1
> 10 1 2
> 11 1 2
> 12 1 2
> 13 2 2
> 14 2 2
> 15 2 2
> 16 3 2
> 17 3 2
> 18 3 2
> 
>>fm1 <- lm(rnorm(18) ~ c*r, vv)
>>fm1$assign
> 
> [1] 0 1 1 2 3 3
> 
>>asgn <- c(fm1$assign, rep(4, 12))
>>system.time(res <- replicate(10000, tapply(qr.qty(fm1$qr, rnorm(18))^2,
> 
> asgn, sum)))
> [1] 20.61  0.01 20.61  0.00  0.00
> 
>>res[,1:6]
> 
>        [,1]      [,2]     [,3]      [,4]       [,5]        [,6]
> 0 0.4783121 0.3048634 0.713689 0.6937838 0.03649023  2.63392426
> 1 0.5825213 1.4756395 1.127018 0.5209751 1.18697199  3.32972093
> 2 0.2612723 3.6396106 0.547506 1.1641910 0.37843963  0.03411672
> 3 2.6259806 3.5504584 1.645215 0.1197238 0.85361018  4.53895212
> 4 9.1942755 8.2122693 4.863392 5.4413571 2.03715439 22.94815118
> 
> The rows of that array correspond to the sum of squares for the
> Intercept (which we generally ignore), the factor 'c', the factor 'r',
> their interaction and the residuals.
> 
> As you can see that took about 21 seconds on my system, which I expect
> is a bit faster than your simulation ran.
> 
> Because I set the seed to a known value I can reproduce the results
> for the first few simulations to check that the sums of squares are
> correct.  Remember that the original fit (fm1) is not included in the
> table.
> 
> 
>>set.seed(1234321)
>>fm1 <- lm(rnorm(18) ~ c*r, vv)
>>anova(fm2 <- lm(rnorm(18) ~ c*r, vv))
> 
> Analysis of Variance Table
> 
> Response: rnorm(18)
>           Df Sum Sq Mean Sq F value Pr(>F)
> c          2 0.5825  0.2913  0.3801 0.6917
> r          1 0.2613  0.2613  0.3410 0.5701
> c:r        2 2.6260  1.3130  1.7137 0.2215
> Residuals 12 9.1943  0.7662
> 
> You can continue this process if you wish further verification that
> the results correspond to the fitted models.
> 
> You can get the p-values for the F-tests as
> 
> 
>>pvalsF <- data.frame(pc = pf((res[2,]/2)/(res[5,]/12), 2, 12, low =
> 
> FALSE),
> +                      pr = pf((res[3,]/1)/(res[5,]/12), 1, 12, low =
> FALSE),
> +                      pint = pf((res[4,]/2)/(res[5,]/12), 2, 12, low =
> FALSE))
> 
> Again you can check this for the first few by hand.
> 
> 
>>pvalsF[1:5,]
> 
>           pc         pr      pint
> 1 0.69171238 0.57006574 0.2214847
> 2 0.37102129 0.03975286 0.1158059
> 3 0.28634939 0.26771167 0.1740633
> 4 0.57775850 0.13506561 0.8775828
> 5 0.06363138 0.16124100 0.1224806
> 
> If you wish you could then check marginal distributions using
> techniques like an empirical density plot.
> 
> 
>>library(lattice)
>>densityplot(~ pc, pvals)
> 
> 
> At this point I would recommend checking the joint distribution but if
> you want to choose a specific level and check the contingency table
> that could be done as
> 
> 
>>xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF)
> 
>             I(pint < 0.16)
> I(pr < 0.16) FALSE TRUE
>        FALSE  7204 1240
>        TRUE   1215  341
> 
> The summary method for an xtabs object provides a test of independence
> 
> 
>>summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsF))
> 
> Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsF)
> Number of cases in table: 10000
> Number of factors: 2
> Test for independence of all factors:
> 	Chisq = 51.6, df = 1, p-value = 6.798e-13
> 
> for which you can see the puzzling result.  However, if you use the
> chisquared test based on the known residual variance of 1, you get
> 
> 
>>pvalsC <- data.frame(pc = pchisq(res[2,], 2, low = FALSE),
> 
> +                      pr = pchisq(res[3,], 1, low = FALSE),
> +                      pint = pchisq(res[4,], 2, low = FALSE))
> 
>>pvalsC[1:5,]
> 
>          pc         pr      pint
> 1 0.7473209 0.60924741 0.2690144
> 2 0.4781553 0.05642013 0.1694446
> 3 0.5692081 0.45933855 0.4392846
> 4 0.7706757 0.28059805 0.9418946
> 5 0.5523983 0.53843951 0.6525907
> 
>>xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC)
> 
>             I(pint < 0.16)
> I(pr < 0.16) FALSE TRUE
>        FALSE  7121 1319
>        TRUE   1324  236
> 
>>summary(xtabs(~ I(pr < 0.16) + I(pint < 0.16), pvalsC))
> 
> Call: xtabs(formula = ~I(pr < 0.16) + I(pint < 0.16), data = pvalsC)
> Number of cases in table: 10000
> Number of factors: 2
> Test for independence of all factors:
> 	Chisq = 0.25041, df = 1, p-value = 0.6168
> 
> 
> 
> On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
> 
>>To my surprise, the R functions I employed did NOT verify the independence
>>property.  I've no quarrel with the theory--it's the R functions that
> 
> worry
> 
>>me.
>>
>>pG
>>
>>-----Original Message-----
>>From: Douglas Bates [mailto:dmbates at gmail.com]
>>Sent: Monday, July 04, 2005 9:13 AM
>>To: pigood at verizon.net; r-help
>>Subject: Re: [R] Lack of independence in anova()
>>
>>
>>I have already had email exchanges off-list with Phillip Good pointing
>>out that the independence property that he wishes to establish by
>>simulation is a consequence of orthogonality of the column span of the
>>row contrasts and the interaction contrasts.  If you set the contrasts
>>option to a set of orthogonal contrasts such as contr.helmert or
>>contr.sum, which has no effect on the results of the anova, this is
>>easily established.
>>
>>
>>>build
>>
>>function(size, v = rnorm(sum(size))) {
>>    col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
>>    rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
>>    row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
>>    +size[7]+size[8]))
>>    return(data.frame(c=factor(col), r=factor(row),yield=v))
>>}
>>
>>>size
>>
>>[1] 3 3 3 0 3 3 3 0
>>
>>>set.seed(1234321)
>>>vv <- build(size)
>>>vv
>>
>>   c r      yield
>>1  0 0  1.2369081
>>2  0 0  1.5616230
>>3  0 0  1.8396185
>>4  1 0  0.3173245
>>5  1 0  1.0715115
>>6  1 0 -1.1459955
>>7  2 0  0.2488894
>>8  2 0  0.1158625
>>9  2 0  2.6200816
>>10 0 1  1.2624048
>>11 0 1 -0.9862578
>>12 0 1 -0.3235653
>>13 1 1  0.2039706
>>14 1 1 -1.4574576
>>15 1 1  1.9158713
>>16 2 1 -2.0333909
>>17 2 1  1.0050272
>>18 2 1  0.6789184
>>
>>>options(contrasts = c('contr.helmert', 'contr.poly'))
>>>crossprod(model.matrix(~c*r, vv))
>>
>>            (Intercept) c1 c2 r1 c1:r1 c2:r1
>>(Intercept)          18  0  0  0     0     0
>>c1                    0 12  0  0     0     0
>>c2                    0  0 36  0     0     0
>>r1                    0  0  0 18     0     0
>>c1:r1                 0  0  0  0    12     0
>>c2:r1                 0  0  0  0     0    36
>>
>>
>>On 7/4/05, Phillip Good <pigood at verizon.net> wrote:
>>
>>> If the observations are normally distributed and the 2xk design is
>>>balanced,  theory requires that the tests for interaction and row
> 
> effects
> 
>>be
>>
>>>independent.  In my program, appended below, this would translate to
> 
> cntT
> 
>>>(approx)= cntR*cntI/N if all R routines were functioning correctly.
> 
> They
> 
>>>aren't.
>>>
>>>sim2=function(size,N,p){
>>>  cntR=0
>>>  cntC=0
>>>  cntI=0
>>>  cntT=0
>>>  cntP=0
>>>  for(i in 1:N){
>>>    #generate data
>>>     v=gendata(size)
>>>    #analyze after build(ing) design containing data
>>>     lm.out=lm(yield~c*r,build(size,v))
>>>     av.out=anova(lm.out)
>>>    #if column effect is significant, increment cntC
>>>     if (av.out[[5]][1]<=p)cntC=cntC+1
>>>   #if row effect is significant, increment cntR
>>>     if (av.out[[5]][2]<=p){
>>>           cntR=cntR+1
>>>           tmp = 1
>>>           }
>>>     else tmp =0
>>>     if (av.out[[5]][3]<=p){
>>>           #if interaction is significant, increment cntI
>>>            cntI=cntI+1
>>>        #if both interaction and row effect are significant, increment
>>
>>cntT
>>
>>>            cntT=cntT + tmp
>>>            }
>>>     }
>>>    list(cntC=cntC, cntR=cntR, cntI=cntI, cntT=cntT)
>>>}
>>>
>>>build=function(size,v){
>>>#size is a vector containing the sample sizes
>>>col=c(rep(0,size[1]),rep(1,size[2]),rep(2,size[3]),rep(3,size[4]),
>>>rep(0,size[5]),rep(1,size[6]),rep(2,size[7]),rep(3,size[8]))
>>>row=c(rep(0,size[1]+size[2]+size[3]+size[4]),rep(1,size[5]+size[6]
>>>+size[7]+size[8]))
>>>return(data.frame(c=factor(col), r=factor(row),yield=v))
>>>}
>>>
>>>gendata=function(size){
>>>  ssize=sum(size);
>>>  return (rnorm(ssize))
>>>}
>>>
>>>#Example
>>> size=c(3,3,3,0,3,3,3,0)
>>> sim2(size,10000,10,.16)
>>>
>>>
>>>
>>>Phillip Good
>>>Huntington Beach CA
>>>
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From magillb at *nomail*.sbcglobal.net  Wed Jul  6 19:13:47 2005
From: magillb at *nomail*.sbcglobal.net (Brett Magill)
Date: Wed, 06 Jul 2005 12:13:47 -0500
Subject: [R] Interactive Graphics
Message-ID: <200507061714.j66HEDhJ024261@hypatia.math.ethz.ch>

What is the status of interactive graphics in R?

I am thinking along the lines of what is provided by something like 
datadesk: 
http://www.datadesk.com/products/data_analysis/datadesk/index.shtml

I have tried RGGobi in the past, but found it clumsy (at least on 
Windows XP).  RGL and iplots both seem to have potential, but currently 
limited functionality.

Is this the state of the art in interactive graphical data analysis in R 
or are there other places to look?  A search of this list revealed few 
hits, mostly to those items mentioned above and mostly from 2-3 years ago.

Thanks,

Brett



From dmbates at gmail.com  Wed Jul  6 19:28:42 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 6 Jul 2005 12:28:42 -0500
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
References: <40e66e0b0507041344596b2a22@mail.gmail.com>
	<GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
Message-ID: <40e66e0b05070610282bac7432@mail.gmail.com>

On 7/6/05, Phillip Good <pigood at verizon.net> wrote:
> Do you or Lumley have a citation for this conclusion?   Most people go
> forward with the ANOV on the basis that the various tests are independent.
> 
> Phillip Good
> P.S.  Tests based on the method of synchronized permutations are
> independent.

Perhaps we could review the sequence of events here.  This exchange
began with your sending me a message claiming that there is a bug in
lm or anova in R because the results of your simulation were what you
expected.  I responded saying that it is unlikely that a serious bug
in such a fundamental part of R would remain undetected for such a
long time and then went further and took apart your simulation and
showed how it could be done much more effectively and also showed,
with help from Thomas Lumley and Peter Dalgaard, that your original
assumption is incorrect.

You have now made another blanket statement the "Most people go
forward with the ANOV on the basis that the various tests are
independent" and indicated that Thomas or I should provide a citation
to validate our claim.  Perhaps instead of claiming that it is
necessary for us to produce evidence in support of our claim that they
are not independent because they are based on the same denominator
mean square, you could produce a citation to back up your claim.  To
date none of your claims of the independence of the tests or the
supposed bugs in R have been substantiated.  If indeed "most people"
do as you claim then "most people" are suffering from a misconception,
a not-uncommon situation in statistics.



From dmbates at gmail.com  Wed Jul  6 19:30:21 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 6 Jul 2005 12:30:21 -0500
Subject: [R] Lack of independence in anova()
In-Reply-To: <40e66e0b05070610282bac7432@mail.gmail.com>
References: <40e66e0b0507041344596b2a22@mail.gmail.com>
	<GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
	<40e66e0b05070610282bac7432@mail.gmail.com>
Message-ID: <40e66e0b0507061030e59c585@mail.gmail.com>

On 7/6/05, Douglas Bates <dmbates at gmail.com> wrote:
...
> Perhaps we could review the sequence of events here.  This exchange
> began with your sending me a message claiming that there is a bug in
> lm or anova in R because the results of your simulation were what you
> expected.  

I meant to write "were not what you expected".  I've got to learn to
read the email messages before posting them.



From pigood at verizon.net  Wed Jul  6 19:43:24 2005
From: pigood at verizon.net (Phillip Good)
Date: Wed, 06 Jul 2005 10:43:24 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <42CC100D.6070209@pdf.com>
Message-ID: <GHEKKACNLEADPKCNEEDFGEHECEAA.pigood@verizon.net>

spencer graves asks:
How is the method of synchronized permutations relevant to a traditional,
normal theory ANOVA?

One ought now ask as I am doing currently whether  traditional, normal
theory ANOVA is applicable to the analysis of experimental designs.  If in
fact, the results of the various ANOV tests are not independent, then:
   i) shouldn't we be teaching this in courses on the analysis of
experimental designs? (Can anyone point to an FDA submission or aplied
statistics publication which conceeds this result?)
  ii) using tests whose results are independent?



From murdoch at stats.uwo.ca  Wed Jul  6 20:03:05 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 14:03:05 -0400
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFGEHECEAA.pigood@verizon.net>
References: <GHEKKACNLEADPKCNEEDFGEHECEAA.pigood@verizon.net>
Message-ID: <42CC1CD9.90304@stats.uwo.ca>

On 7/6/2005 1:43 PM, Phillip Good wrote:
> spencer graves asks:
> How is the method of synchronized permutations relevant to a traditional,
> normal theory ANOVA?
> 
> One ought now ask as I am doing currently whether  traditional, normal
> theory ANOVA is applicable to the analysis of experimental designs.  If in
> fact, the results of the various ANOV tests are not independent, then:
>    i) shouldn't we be teaching this in courses on the analysis of
> experimental designs? (Can anyone point to an FDA submission or aplied
> statistics publication which conceeds this result?)
>   ii) using tests whose results are independent?


I think it's relatively infrequent that we make inferences where 
independence matters, so it makes sense to use tests that are marginally 
well-behaved.  When do you care about the simultaneous behaviour of two 
tests?

The nice thing about ANOVA tests is that each one is valid regardless of 
the presence or absence of effects in other rows.  This doesn't require 
independence.

Where you might use independence is to try to construct a combined 
p-value looking for a violation of any of several hypotheses, but if 
that's the test you're interested in, you should just have pooled the 
terms in the numerator.

Duncan Murdoch



From chrish at stats.ucl.ac.uk  Wed Jul  6 20:35:06 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Wed, 6 Jul 2005 19:35:06 +0100 (BST)
Subject: [R] rlm/M/MM-estimator questions
Message-ID: <Pine.LNX.4.58.0507061928120.10149@egon.stats.ucl.ac.uk>

Hi list,

1) How can the MM-estimator method="MM" in function rlm be tuned to 85%
efficiency? It seems that there is a default tuning to 95%. I presume, but
am not sure, that the MM-estimator uses phi=phi.bisquare as default and
the tuning constant could be set by adding a parameter c=...
Is this true? Which value to use for 85%?
(In principle I should be able to figure that out theoretically, but it
would be much easier if somebody already knew the constant or a
straightforward way to compute it.)

2) The M-estimator with bisquare uses "rescaled MAD of the residuals" as
scale estimator according to the rlm help page. Does this mean that a
scale estimator is used which is computed from least squares residuals? Are
M-estimator and residual scale estimator iterated until convergence of
them both? (Does this converge?) Or what else? What does "rescaled" mean?

Thank you,
Christian


*** NEW ADDRESS! ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From friendly at yorku.ca  Wed Jul  6 21:36:03 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Wed, 06 Jul 2005 15:36:03 -0400
Subject: [R] plotting on a reverse log scale
Message-ID: <42CC32A3.9040605@yorku.ca>

I'd like to do some plots of historical event data on a reverse log 
scale, started, say at the year 2000 and going
backwards in time, with tick marks spaced according to log(2000-year).  
For example, see:

http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif

As an example, I'd like to create a density plot of such data with the 
horizontal axis reverse-logged,
a transformation of this image:
http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif

Some initial code to do a standard density plot looks like this:

mileyears <- read.csv("mileyears3.csv", skip=1, 
col.names=c("key","year","where","add","junk"))
mileyears <- mileyears[,2:4]

years <- mileyears$year
years1500 <- years[years>1500]
dens <- density(years1500, from=1500, to=1990)
plot(dens)
rug(years1500)

I could calculate log(2000-year), but I'm not sure how to do the 
plotting, do some minor tick marks
and label the major ones, say at 100 year intervals.

thanks,
-Michael


 

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From Ted.Harding at nessie.mcc.ac.uk  Wed Jul  6 20:29:24 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 06 Jul 2005 19:29:24 +0100 (BST)
Subject: [R] Lack of independence in anova()
In-Reply-To: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
Message-ID: <XFMail.050706192924.Ted.Harding@nessie.mcc.ac.uk>

On 06-Jul-05 Phillip Good wrote:
> Do you or Lumley have a citation for this conclusion?   Most people go
> forward with the ANOV on the basis that the various tests are
> independent.

This hardly needs a citation -- Thomas Lumley's explanation (excerpted
below from Doug Bates's mail) is justification enough, and it is
succinct and elementary.

However, a place one naturally looks for caveats of this kind is
in Kendall & Stuart, and I duly found it (Vol 3, section 35.46
of my 1983 edition). It is essentially exactly the same explanation:

  "However, the tests in the AV tables which we have considered
   are never independent tests, for although the various SS in a
   table may be independent of each other, all the tests we have
   derived use the Residual SS as denominator of the test statistic,
   and the various tests must therefore be statistically dependent,
   since, e.g., a Residual SS which is (by chance) large will
   depress all the values of the test staistics simultaneously."

(And K&S, thorough as they are with citations, do not cite any
primary reference for this either!)

However, if the "degrees of freedom" for Residual SS is large,
then the amount of random variation in the denominator will be
small and it will be effectively constant. Then, of course,
with independent numerators, the tests will be effectively
independent (and equivalent to chi-squared) and also, therefore,
the p-values.

The fact that "most people go forward with the ANOV on the basis
that the various tests are independent" possibly reflects the
wide-spread act of faith that one has "a large sample", whatever
the value of n may really be. One wonders how often people check
the p-value for their F on (n1:n2) d.f. against the p-value for
(n1:Inf) d.f.? The 5% point decreases quite perceptibly as n2
increases up to about 20, and more slowly thereafter; but still
the difference between F(n1:20) and F(n1:Inf) is substantial
for any n1 (being about 0.5 for n1 up to about 10, increasing
thereafter up to 0.84):

n1<-c(1+2*(0:50),5000);cbind(n1,qf(0.95,n1,20) - qf(0.95,n1,Inf))

F(Inf,Inf) = 1 ; F(20:Inf) = qf(0.95,Inf,20) = 1.841

Conversely (e.g.):

  > 1-pf(qf(0.95,20,20),20,20)
  [1] 0.05
  > 1-pf(qf(0.95,20,20),20,Inf)
  [1] 0.002391189

Such differences are related to the degree of non-independence of
several tests on the same data.

> [Douglas Bates]:
> Thomas Lumley came to my rescue with an explanation.  There is no
> reason why the results of the F tests should be independent.  The
> numerators are independent but the denominator is the same for both
> tests.  When, due to random variation, the denominator is small, then
> the p-values for both tests will tend to be small.  If, instead of
> F-tests you use chi-square tests then you do see independence.

But surely this amounts to assuming n2 = Inf? If that's an adequate
approximation, then fine; but if not (see e.g. above) then not!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Jul-05                                       Time: 19:29:18
------------------------------ XFMail ------------------------------



From loesljrg at verizon.net  Wed Jul  6 21:50:07 2005
From: loesljrg at verizon.net (JRG)
Date: Wed, 06 Jul 2005 15:50:07 -0400
Subject: [R] Lack of independence in anova()
In-Reply-To: <40e66e0b0507061030e59c585@mail.gmail.com>
References: <40e66e0b05070610282bac7432@mail.gmail.com>
Message-ID: <42CBFDAF.32254.1658655@localhost>

On 6 Jul 2005 at 12:30, Douglas Bates wrote:

> On 7/6/05, Douglas Bates <dmbates at gmail.com> wrote:
> ...
> > Perhaps we could review the sequence of events here.  This exchange
> > began with your sending me a message claiming that there is a bug in
> > lm or anova in R because the results of your simulation were what you
> > expected.  
> 

At the risk of further roiling the waters ...

As several have already pointed out, the "usual" F-tests in a balanced ANOVA have independent numerators but a common denominator, 
and hence the F-statistics cannot be independent.  Is this not the basis of Kimball's Inequality, which states that the effect of 
the common denominator is that the simultaneous error rate cannot exceed what it would be if the tests *were* in fact independent?

In other words, you should get a simultaneous error rate for the F-tests that is lower than that under independence of test 
statistics. Are you?

---JRG

John R. Gleason



> I meant to write "were not what you expected".  I've got to learn to
> read the email messages before posting them.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Jul  6 21:56:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 12:56:13 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <42CBFDAF.32254.1658655@localhost>
References: <40e66e0b05070610282bac7432@mail.gmail.com>
	<42CBFDAF.32254.1658655@localhost>
Message-ID: <42CC375D.50308@pdf.com>

	  As Duncan Murdoch indicated, the primary issue is not that the 
different tests are (not) statitically independent but that they are 
sensitive to different alternative hypotheses.

	  spencer graves

JRG wrote:

> On 6 Jul 2005 at 12:30, Douglas Bates wrote:
> 
> 
>>On 7/6/05, Douglas Bates <dmbates at gmail.com> wrote:
>>...
>>
>>>Perhaps we could review the sequence of events here.  This exchange
>>>began with your sending me a message claiming that there is a bug in
>>>lm or anova in R because the results of your simulation were what you
>>>expected.  
>>
> 
> At the risk of further roiling the waters ...
> 
> As several have already pointed out, the "usual" F-tests in a balanced ANOVA have independent numerators but a common denominator, 
> and hence the F-statistics cannot be independent.  Is this not the basis of Kimball's Inequality, which states that the effect of 
> the common denominator is that the simultaneous error rate cannot exceed what it would be if the tests *were* in fact independent?
> 
> In other words, you should get a simultaneous error rate for the F-tests that is lower than that under independence of test 
> statistics. Are you?
> 
> ---JRG
> 
> John R. Gleason
> 
> 
> 
> 
>>I meant to write "were not what you expected".  I've got to learn to
>>read the email messages before posting them.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Wed Jul  6 22:04:28 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 13:04:28 -0700
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CC32A3.9040605@yorku.ca>
References: <42CC32A3.9040605@yorku.ca>
Message-ID: <42CC394C.80706@pdf.com>

	  Do you want to move year 2000 to Inf?  How about a cube root 
transformation instead:

year <- seq(0, 4000, 100)
y2000.3 <- (sign(year-2000)*
             abs(year-2000)^(1/3))
plot(y2000.3, year, axes=FALSE)
axis(1, y2000.3, year)
axis(2)

	  Of course, one should package the transformation in a function  and 
more carefully select the tick marks, but a little study of the help 
pages for the functions in this example should suffice for that.

	  spencer graves

Michael Friendly wrote:

> I'd like to do some plots of historical event data on a reverse log 
> scale, started, say at the year 2000 and going
> backwards in time, with tick marks spaced according to log(2000-year).  
> For example, see:
> 
> http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
> 
> As an example, I'd like to create a density plot of such data with the 
> horizontal axis reverse-logged,
> a transformation of this image:
> http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
> 
> Some initial code to do a standard density plot looks like this:
> 
> mileyears <- read.csv("mileyears3.csv", skip=1, 
> col.names=c("key","year","where","add","junk"))
> mileyears <- mileyears[,2:4]
> 
> years <- mileyears$year
> years1500 <- years[years>1500]
> dens <- density(years1500, from=1500, to=1990)
> plot(dens)
> rug(years1500)
> 
> I could calculate log(2000-year), but I'm not sure how to do the 
> plotting, do some minor tick marks
> and label the major ones, say at 100 year intervals.
> 
> thanks,
> -Michael
> 
> 
>  
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From murdoch at stats.uwo.ca  Wed Jul  6 22:02:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 16:02:42 -0400
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CC32A3.9040605@yorku.ca>
References: <42CC32A3.9040605@yorku.ca>
Message-ID: <42CC38E2.20605@stats.uwo.ca>

On 7/6/2005 3:36 PM, Michael Friendly wrote:
> I'd like to do some plots of historical event data on a reverse log 
> scale, started, say at the year 2000 and going
> backwards in time, with tick marks spaced according to log(2000-year).  
> For example, see:
> 
> http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
> 
> As an example, I'd like to create a density plot of such data with the 
> horizontal axis reverse-logged,
> a transformation of this image:
> http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
> 
> Some initial code to do a standard density plot looks like this:
> 
> mileyears <- read.csv("mileyears3.csv", skip=1, 
> col.names=c("key","year","where","add","junk"))
> mileyears <- mileyears[,2:4]
> 
> years <- mileyears$year
> years1500 <- years[years>1500]
> dens <- density(years1500, from=1500, to=1990)
> plot(dens)
> rug(years1500)
> 
> I could calculate log(2000-year), but I'm not sure how to do the 
> plotting, do some minor tick marks
> and label the major ones, say at 100 year intervals.

I think you'll have to do everything explicitly.  That is, something 
like this:

years1500 <- runif(500, 1500, 1990)  # some fake data
x <- log(2000-years1500)
from <- log(2000-1990)
to <- log(2000-1500)
plot(density(x, from=from, to=to), axes=F)
rug(x)

labels <- pretty(years1500)
labels <- labels[labels<2000]
axis(1, labels, at=log(2000-labels))

minorticks <- pretty(years1500, n=20)
minorticks <- minorticks[minorticks<2000]
axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)

axis(2)
box()

Duncan Murdoch



From HDoran at air.org  Wed Jul  6 22:06:47 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 6 Jul 2005 16:06:47 -0400
Subject: [R] Tempfile error
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409748B59@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050706/e91c3842/attachment.pl

From David.Brahm at geodecapital.com  Wed Jul  6 22:20:51 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 6 Jul 2005 16:20:51 -0400
Subject: [R] Graphics: calling par(mar) after frame()
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBFD@MSGBOSCLF2WIN.DMN1.FMR.COM>

The following code produces 6 plots on a page, but the first is
distorted and different from the others:

par(mfrow=c(3,2), las=2)
for (i in 1:6) {
  frame()
  par(mar=c(7, 7, 1, 1))
  axis(2); box(); abline(h=seq(0,1,.5), col=2:4)
}

The first plot's axes are mis-aligned with the plotting area implied
by the box.  It seems to be a result of calling par(mar) after frame().
Is this expected behavior, or some kind of bug?

I'm using R-2.1.0 on Linux with X11; I see the same behavior in Windows.

-- David Brahm (brahm at alum.mit.edu)



From ligges at statistik.uni-dortmund.de  Wed Jul  6 22:42:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Jul 2005 22:42:36 +0200
Subject: [R] Interactive Graphics
In-Reply-To: <200507061714.j66HEDhJ024261@hypatia.math.ethz.ch>
References: <200507061714.j66HEDhJ024261@hypatia.math.ethz.ch>
Message-ID: <42CC423C.4050001@statistik.uni-dortmund.de>

Brett Magill wrote:
> What is the status of interactive graphics in R?
> 
> I am thinking along the lines of what is provided by something like 
> datadesk: 
> http://www.datadesk.com/products/data_analysis/datadesk/index.shtml
> 
> I have tried RGGobi in the past, but found it clumsy (at least on 
> Windows XP).  RGL and iplots both seem to have potential, but currently 
> limited functionality.
> 
> Is this the state of the art in interactive graphical data analysis in R 

Yes, given a certain definition of "interactive".

Uwe Ligges


> or are there other places to look?  A search of this list revealed few 
> hits, mostly to those items mentioned above and mostly from 2-3 years ago.
> 
> Thanks,
> 
> Brett
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From friendly at yorku.ca  Wed Jul  6 22:45:32 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Wed, 06 Jul 2005 16:45:32 -0400
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CC38E2.20605@stats.uwo.ca>
References: <42CC32A3.9040605@yorku.ca> <42CC38E2.20605@stats.uwo.ca>
Message-ID: <42CC42EC.4080101@yorku.ca>

Thanks Duncan,
That is almost exactly what I want, except I want time to
go in the normal order, not backwards, so:

# plot on reverse log scale
years1500 <- runif(500, 1500, 1990)  # some fake data
x <- -log(2000-years1500)
from <- -log(2000-1990)
to <- -log(2000-1500)
plot(density(x, from=from, to=to), axes=F)
rug(x)

labels <- pretty(years1500)
labels <- labels[labels<2000]
axis(1, labels, at=-log(2000-labels))

minorticks <- pretty(years1500, n=20)
minorticks <- minorticks[minorticks<2000]
axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)

axis(2)
box()

-Michael

Duncan Murdoch wrote:

> On 7/6/2005 3:36 PM, Michael Friendly wrote:
> 
>> I'd like to do some plots of historical event data on a reverse log 
>> scale, started, say at the year 2000 and going
>> backwards in time, with tick marks spaced according to 
>> log(2000-year).  For example, see:
>>
>> http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
>>
>> As an example, I'd like to create a density plot of such data with the 
>> horizontal axis reverse-logged,
>> a transformation of this image:
>> http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
>>
>> Some initial code to do a standard density plot looks like this:
>>
>> mileyears <- read.csv("mileyears3.csv", skip=1, 
>> col.names=c("key","year","where","add","junk"))
>> mileyears <- mileyears[,2:4]
>>
>> years <- mileyears$year
>> years1500 <- years[years>1500]
>> dens <- density(years1500, from=1500, to=1990)
>> plot(dens)
>> rug(years1500)
>>
>> I could calculate log(2000-year), but I'm not sure how to do the 
>> plotting, do some minor tick marks
>> and label the major ones, say at 100 year intervals.
> 
> 
> I think you'll have to do everything explicitly.  That is, something 
> like this:
> 
> years1500 <- runif(500, 1500, 1990)  # some fake data
> x <- log(2000-years1500)
> from <- log(2000-1990)
> to <- log(2000-1500)
> plot(density(x, from=from, to=to), axes=F)
> rug(x)
> 
> labels <- pretty(years1500)
> labels <- labels[labels<2000]
> axis(1, labels, at=log(2000-labels))
> 
> minorticks <- pretty(years1500, n=20)
> minorticks <- minorticks[minorticks<2000]
> axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)
> 
> axis(2)
> box()
> 
> Duncan Murdoch

-- 
Michael Friendly     Email: friendly at yorku.ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From ligges at statistik.uni-dortmund.de  Wed Jul  6 23:09:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Jul 2005 23:09:36 +0200
Subject: [R] Graphics: calling par(mar) after frame()
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBFD@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBFD@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <42CC4890.2000900@statistik.uni-dortmund.de>

Brahm, David wrote:

> The following code produces 6 plots on a page, but the first is
> distorted and different from the others:
> 
> par(mfrow=c(3,2), las=2)
> for (i in 1:6) {
>   frame()
>   par(mar=c(7, 7, 1, 1))
>   axis(2); box(); abline(h=seq(0,1,.5), col=2:4)
> }
> 
> The first plot's axes are mis-aligned with the plotting area implied
> by the box.  It seems to be a result of calling par(mar) after frame().
> Is this expected behavior, or some kind of bug?


Yes expected, at first yiou generate the plot, then you change the 
margins, and then you add stuff (axis).
For the second plot, par(mar) has already been called in the first 
iteration.

Why do you want to use it inside the loop?

Uwe Ligges

> I'm using R-2.1.0 on Linux with X11; I see the same behavior in Windows.
> 
> -- David Brahm (brahm at alum.mit.edu)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Wed Jul  6 23:22:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 17:22:38 -0400
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CC42EC.4080101@yorku.ca>
References: <42CC32A3.9040605@yorku.ca> <42CC38E2.20605@stats.uwo.ca>
	<42CC42EC.4080101@yorku.ca>
Message-ID: <42CC4B9E.5090106@stats.uwo.ca>

Michael Friendly wrote:
> Thanks Duncan,
> That is almost exactly what I want, except I want time to
> go in the normal order, not backwards, so:
> 
> # plot on reverse log scale
> years1500 <- runif(500, 1500, 1990)  # some fake data
> x <- -log(2000-years1500)
> from <- -log(2000-1990)
> to <- -log(2000-1500)
> plot(density(x, from=from, to=to), axes=F)
> rug(x)
> 
> labels <- pretty(years1500)
> labels <- labels[labels<2000]
> axis(1, labels, at=-log(2000-labels))
> 
> minorticks <- pretty(years1500, n=20)
> minorticks <- minorticks[minorticks<2000]
> axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> 
> axis(2)
> box()
> 
> -Michael
> 
> Duncan Murdoch wrote:
> 
> 
>>On 7/6/2005 3:36 PM, Michael Friendly wrote:
>>
>>
>>>I'd like to do some plots of historical event data on a reverse log 
>>>scale, started, say at the year 2000 and going
>>>backwards in time, with tick marks spaced according to 
>>>log(2000-year).  For example, see:
>>>
>>>http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
>>>
>>>As an example, I'd like to create a density plot of such data with the 
>>>horizontal axis reverse-logged,
>>>a transformation of this image:
>>>http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
>>>
>>>Some initial code to do a standard density plot looks like this:
>>>
>>>mileyears <- read.csv("mileyears3.csv", skip=1, 
>>>col.names=c("key","year","where","add","junk"))
>>>mileyears <- mileyears[,2:4]
>>>
>>>years <- mileyears$year
>>>years1500 <- years[years>1500]
>>>dens <- density(years1500, from=1500, to=1990)
>>>plot(dens)
>>>rug(years1500)
>>>
>>>I could calculate log(2000-year), but I'm not sure how to do the 
>>>plotting, do some minor tick marks
>>>and label the major ones, say at 100 year intervals.
>>
>>
>>I think you'll have to do everything explicitly.  That is, something 
>>like this:
>>
>>years1500 <- runif(500, 1500, 1990)  # some fake data
>>x <- log(2000-years1500)
>>from <- log(2000-1990)
>>to <- log(2000-1500)
>>plot(density(x, from=from, to=to), axes=F)

Careful there: from > to, and that seems to have messed up density. 
Since the code assumes from < to, we should have a test and an error 
message (or a swap).

Duncan Murdoch

>>rug(x)
>>
>>labels <- pretty(years1500)
>>labels <- labels[labels<2000]
>>axis(1, labels, at=log(2000-labels))
>>
>>minorticks <- pretty(years1500, n=20)
>>minorticks <- minorticks[minorticks<2000]
>>axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)
>>
>>axis(2)
>>box()
>>
>>Duncan Murdoch
> 
>



From murdoch at stats.uwo.ca  Wed Jul  6 23:25:43 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 17:25:43 -0400
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CC42EC.4080101@yorku.ca>
References: <42CC32A3.9040605@yorku.ca> <42CC38E2.20605@stats.uwo.ca>
	<42CC42EC.4080101@yorku.ca>
Message-ID: <42CC4C57.6050204@stats.uwo.ca>

Stupid me, I put my comment in the wrong place.  Ignore my last message, 
this one should be right:

Michael Friendly wrote:
> Thanks Duncan,
> That is almost exactly what I want, except I want time to
> go in the normal order, not backwards, so:
> 
> # plot on reverse log scale
> years1500 <- runif(500, 1500, 1990)  # some fake data
> x <- -log(2000-years1500)
> from <- -log(2000-1990)
> to <- -log(2000-1500)
> plot(density(x, from=from, to=to), axes=F)

Careful there: from > to, and that seems to have messed up density.
Since the code assumes from < to, we should have a test and an error
message (or a swap).

Duncan Murdoch

> rug(x)
> 
> labels <- pretty(years1500)
> labels <- labels[labels<2000]
> axis(1, labels, at=-log(2000-labels))
> 
> minorticks <- pretty(years1500, n=20)
> minorticks <- minorticks[minorticks<2000]
> axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> 
> axis(2)
> box()
> 
> -Michael
> 
> Duncan Murdoch wrote:
> 
> 
>>On 7/6/2005 3:36 PM, Michael Friendly wrote:
>>
>>
>>>I'd like to do some plots of historical event data on a reverse log 
>>>scale, started, say at the year 2000 and going
>>>backwards in time, with tick marks spaced according to 
>>>log(2000-year).  For example, see:
>>>
>>>http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
>>>
>>>As an example, I'd like to create a density plot of such data with the 
>>>horizontal axis reverse-logged,
>>>a transformation of this image:
>>>http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
>>>
>>>Some initial code to do a standard density plot looks like this:
>>>
>>>mileyears <- read.csv("mileyears3.csv", skip=1, 
>>>col.names=c("key","year","where","add","junk"))
>>>mileyears <- mileyears[,2:4]
>>>
>>>years <- mileyears$year
>>>years1500 <- years[years>1500]
>>>dens <- density(years1500, from=1500, to=1990)
>>>plot(dens)
>>>rug(years1500)
>>>
>>>I could calculate log(2000-year), but I'm not sure how to do the 
>>>plotting, do some minor tick marks
>>>and label the major ones, say at 100 year intervals.
>>
>>
>>I think you'll have to do everything explicitly.  That is, something 
>>like this:
>>
>>years1500 <- runif(500, 1500, 1990)  # some fake data
>>x <- log(2000-years1500)
>>from <- log(2000-1990)
>>to <- log(2000-1500)
>>plot(density(x, from=from, to=to), axes=F)
>>rug(x)
>>
>>labels <- pretty(years1500)
>>labels <- labels[labels<2000]
>>axis(1, labels, at=log(2000-labels))
>>
>>minorticks <- pretty(years1500, n=20)
>>minorticks <- minorticks[minorticks<2000]
>>axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)
>>
>>axis(2)
>>box()
>>
>>Duncan Murdoch
> 
>



From davidr at rhotrading.com  Wed Jul  6 23:28:31 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 6 Jul 2005 16:28:31 -0500
Subject: [R] pretty for date-time?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FBA95@rhosvr02.rhotrading.com>

pretty() works well for numbers and axTicks() can help for potting log axes, 
but has anyone written a pretty for chron objects (or other date or date-time classes)?
It would have natural units of years, months, .., days, hours, (minutes?), and
it would choose the appropriate unit based on the date(time) range.
I have searched the archives and documentation to no avail.
(I wrote one of these back in my Fortran days, and it was non-trivial, even just for dates.)

If no-one has done it, I would be willing to work with someone more expert on production quality R to produce it.

Thanks for any pointers,

David L. Reiner
??
Rho Trading
440 S. LaSalle St.
Chicago?? IL?? 60605
312-362-4963
??



From joseclaudio.faria at terra.com.br  Wed Jul  6 23:28:37 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Wed, 06 Jul 2005 18:28:37 -0300
Subject: [R] Help: Mahalanobis distances between 'Species' from iris
Message-ID: <42CC4D05.9060909@terra.com.br>

Dear R list,

I'm trying to calculate Mahalanobis distances for 'Species' of 'iris' data
as obtained below:

Squared Distance to Species From Species:

               Setosa Versicolor Virginica
Setosa 	           0   89.86419 179.38471
Versicolor  89.86419          0  17.20107
Virginica  179.38471   17.20107         0

This distances above were obtained with proc 'CANDISC' of SAS, please,
see Output 21.1.2: Iris Data: Squared Mahalanobis Distances from
http://www.id.unizh.ch/software/unix/statmath/sas/sasdoc/stat/chap21/sect19.htm

 From this distance my intention is to make a cluster analysis as below, using
the package 'mclust':

#
# --- Begin R script ---
#
# For units compatibility of 'iris' from R dataset and 'iris' data used in
# the SAS example:
Measures = iris[,1:4]*10
Species  = iris[,5]
irisSAS  = data.frame(Measures, Species)

n   = 3
Mah = c(        0,
          89.86419,        0,
         179.38471, 17.20107, 0)

# My Question is: how to obtain 'Mah' with R from 'irisSAS' data?

D = matrix(0, n, n)

nam = c('Set', 'Ver', 'Vir')
rownames(D) = nam
colnames(D) = nam

k = 0
for (i in 1:n) {
    for (j in 1:i) {
       k      = k+1
       D[i,j] = Mah[k]
       D[j,i] = Mah[k]
    }
}

D=sqrt(D) #D2 -> D

library(mclust)
dendroS = hclust(as.dist(D), method='single')
dendroC = hclust(as.dist(D), method='complete')

win.graph(w = 3.5, h = 6)
split.screen(c(2, 1))
screen(1)
plot(dendroS, main='Single', sub='', xlab='', ylab='', col='blue')

screen(2)
plot(dendroC, main='Complete', sub='', xlab='', col='red')
#
# --- End R script ---
#

I always need of this type of analysis and I'm not founding how to make it in 
the CRAN documentation (Archives, packages: mclust, cluster, fpc and mva).

Regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br



From ggrothendieck at gmail.com  Wed Jul  6 23:31:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Jul 2005 17:31:58 -0400
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CC42EC.4080101@yorku.ca>
References: <42CC32A3.9040605@yorku.ca> <42CC38E2.20605@stats.uwo.ca>
	<42CC42EC.4080101@yorku.ca>
Message-ID: <971536df05070614311b163415@mail.gmail.com>

Not sure if I am missing something essential here but it would
seem as simple as:

# data
set.seed(1)
x <- runif(500, 1500, 1990)  

# plot
d <- density(x, from = 1500, to = 1990)
plot(d$y ~ d$x, log = "x")
rug(x)
axis(1, seq(1500, 1990, 10), FALSE, tcl = -0.3)


On 7/6/05, Michael Friendly <friendly at yorku.ca> wrote:
> Thanks Duncan,
> That is almost exactly what I want, except I want time to
> go in the normal order, not backwards, so:
> 
> # plot on reverse log scale
> years1500 <- runif(500, 1500, 1990)  # some fake data
> x <- -log(2000-years1500)
> from <- -log(2000-1990)
> to <- -log(2000-1500)
> plot(density(x, from=from, to=to), axes=F)
> rug(x)
> 
> labels <- pretty(years1500)
> labels <- labels[labels<2000]
> axis(1, labels, at=-log(2000-labels))
> 
> minorticks <- pretty(years1500, n=20)
> minorticks <- minorticks[minorticks<2000]
> axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> 
> axis(2)
> box()
> 
> -Michael
> 
> Duncan Murdoch wrote:
> 
> > On 7/6/2005 3:36 PM, Michael Friendly wrote:
> >
> >> I'd like to do some plots of historical event data on a reverse log
> >> scale, started, say at the year 2000 and going
> >> backwards in time, with tick marks spaced according to
> >> log(2000-year).  For example, see:
> >>
> >> http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
> >>
> >> As an example, I'd like to create a density plot of such data with the
> >> horizontal axis reverse-logged,
> >> a transformation of this image:
> >> http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
> >>
> >> Some initial code to do a standard density plot looks like this:
> >>
> >> mileyears <- read.csv("mileyears3.csv", skip=1,
> >> col.names=c("key","year","where","add","junk"))
> >> mileyears <- mileyears[,2:4]
> >>
> >> years <- mileyears$year
> >> years1500 <- years[years>1500]
> >> dens <- density(years1500, from=1500, to=1990)
> >> plot(dens)
> >> rug(years1500)
> >>
> >> I could calculate log(2000-year), but I'm not sure how to do the
> >> plotting, do some minor tick marks
> >> and label the major ones, say at 100 year intervals.
> >
> >
> > I think you'll have to do everything explicitly.  That is, something
> > like this:
> >
> > years1500 <- runif(500, 1500, 1990)  # some fake data
> > x <- log(2000-years1500)
> > from <- log(2000-1990)
> > to <- log(2000-1500)
> > plot(density(x, from=from, to=to), axes=F)
> > rug(x)
> >
> > labels <- pretty(years1500)
> > labels <- labels[labels<2000]
> > axis(1, labels, at=log(2000-labels))
> >
> > minorticks <- pretty(years1500, n=20)
> > minorticks <- minorticks[minorticks<2000]
> > axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)
> >
> > axis(2)
> > box()
> >
> > Duncan Murdoch
> 
> --
> Michael Friendly     Email: friendly at yorku.ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Jul  6 23:44:47 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 17:44:47 -0400
Subject: [R] pretty for date-time?
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A5FBA95@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A5FBA95@rhosvr02.rhotrading.com>
Message-ID: <42CC50CF.50205@stats.uwo.ca>

davidr at rhotrading.com wrote:
> pretty() works well for numbers and axTicks() can help for potting log axes, 
> but has anyone written a pretty for chron objects (or other date or date-time classes)?
> It would have natural units of years, months, .., days, hours, (minutes?), and
> it would choose the appropriate unit based on the date(time) range.
> I have searched the archives and documentation to no avail.
> (I wrote one of these back in my Fortran days, and it was non-trivial, even just for dates.)

I don't think it exists (but you never know if it is in a contributed 
package on CRAN).  However, there are methods for the generic axis() 
function that work on Date and POSIXct objects.  (See ?axis.Date for 
details).

Do these do enough for you, or do you want some of the other options 
pretty() offers?

Duncan Murdoch

> 
> If no-one has done it, I would be willing to work with someone more expert on production quality R to produce it.
> 
> Thanks for any pointers,
> 
> David L. Reiner
>  
> Rho Trading
> 440 S. LaSalle St.
> Chicago  IL  60605
> 312-362-4963
>  
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From David.Brahm at geodecapital.com  Wed Jul  6 23:49:19 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 6 Jul 2005 17:49:19 -0400
Subject: [R] Graphics: calling par(mar) after frame()
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBFE@MSGBOSCLF2WIN.DMN1.FMR.COM>

I wrote:
> par(mfrow=c(3,2), las=2)
> for (i in 1:6) {
>   frame()
>   par(mar=c(7, 7, 1, 1))
>   axis(2); box(); abline(h=seq(0,1,.5), col=2:4)
> }
> The first plot's axes are mis-aligned with the plotting area...

Uwe Ligges <ligges at statistik.uni-dortmund.de> replied:
> Yes expected, at first you generate the plot, then you change the 
> margins, and then you add stuff (axis)...
> Why do you want to use it inside the loop?

Thanks, Uwe, for making it clearer.  My toy example puts these commands
inside a loop because my real-life problem has them inside a function,
which is called inside a loop.  My function sets par(mar, usr, mgp).
It seems I need to set par(mar) before frame(), par(usr) after frame(),
and par(mgp) either before or after.  I was not thinking of frame() as
"generating the plot" so much as "moving to the next location", so I
didn't understand why the order mattered.  Thanks again!

-- David Brahm (brahm at alum.mit.edu)



From gb at stat.umu.se  Thu Jul  7 00:00:52 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 7 Jul 2005 00:00:52 +0200
Subject: [R] Lack of independence in anova()
In-Reply-To: <Pine.A41.4.61b.0507060945070.109146@homer05.u.washington.edu>
References: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>
	<Pine.A41.4.61b.0507060945070.109146@homer05.u.washington.edu>
Message-ID: <20050706220052.GA3138@stat.umu.se>

On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
(...)
>  If X, Y, and Z are 
> independent and Z takes on more than one value then X/Z and Y/Z can't be 
> independent.

Not really true. I  can produce a counterexample on request (admittedly
quite trivial though).

G??ran Brostr??m



From ray.xiong at gmail.com  Thu Jul  7 00:42:02 2005
From: ray.xiong at gmail.com (Ray Xiong)
Date: Wed, 6 Jul 2005 17:42:02 -0500
Subject: [R] How to perform LSD or HSD tests with glm or lm
Message-ID: <d6d6fd440507061542222dc37f@mail.gmail.com>

Dear All,

I knew how to use glm or lm, but I do not know how to perform LSD or
HSD on treatments with them. If someone can help me out, I greatly
appreciate it.

Have a nice weekend.

Ray



From Ted.Harding at nessie.mcc.ac.uk  Thu Jul  7 01:51:54 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 07 Jul 2005 00:51:54 +0100 (BST)
Subject: [R] Lack of independence in anova()
In-Reply-To: <20050706220052.GA3138@stat.umu.se>
Message-ID: <XFMail.050707005154.Ted.Harding@nessie.mcc.ac.uk>

On 06-Jul-05 G??ran Brostr??m wrote:
> On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
> (...)
>>  If X, Y, and Z are independent and Z takes on more than one
>> value then X/Z and Y/Z can't be independent.
> 
> Not really true. I  can produce a counterexample on request
> (admittedly quite trivial though).
> 
> G??ran Brostr??m

But true if both X  and Y have positive probability of being
non-zero, n'est-pas?

Tut, tut, G??ran!

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Jul-05                                       Time: 00:51:24
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Thu Jul  7 02:02:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 17:02:06 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <20050706220052.GA3138@stat.umu.se>
References: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>	<Pine.A41.4.61b.0507060945070.109146@homer05.u.washington.edu>
	<20050706220052.GA3138@stat.umu.se>
Message-ID: <42CC70FE.1010500@pdf.com>

Hi, G??ran:  I'll bite:

	  (a) I'd like to see your counterexample.

	  (b) I'd like to know what is wrong with my the following, apparently 
defective, proof that they can't be independent:  First consider 
indicator functions of independent events A, B, and C.

	  P{(AC)&(BC)} = P{ABC} = PA*PB*PC.

	  But P(AC)*P(BC) = PA*PB*(PC)^2.  Thus, AC and BC can be independent 
only if PC = 0 or 1, i.e., the indicator of C is constant almost surely.

	  Is there a flaw in this?  If not, is there some reason this case 
cannot be extended the product of arbitrary random variables X, Y, and 
W=1/Z?

	  Thanks,
	  spencer graves

G??ran Brostr??m wrote:

> On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
> (...)
> 
>> If X, Y, and Z are 
>>independent and Z takes on more than one value then X/Z and Y/Z can't be 
>>independent.
> 
> 
> Not really true. I  can produce a counterexample on request (admittedly
> quite trivial though).
> 
> G??ran Brostr??m
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From pigood at verizon.net  Thu Jul  7 02:21:12 2005
From: pigood at verizon.net (Phillip Good)
Date: Wed, 06 Jul 2005 17:21:12 -0700
Subject: [R] Interpreting ANOV output
In-Reply-To: <42CC70FE.1010500@pdf.com>
Message-ID: <GHEKKACNLEADPKCNEEDFIEHKCEAA.pigood@verizon.net>

My thanks to Douglas Bates and others for pointing out ANOV's limitations.
Now the question is how to put my new won knowledge into practice.
Before my eyes were opened
   if row effect and interaction were both statistically significant
   I would report the row effects separately for each column.

Now
   if row effect and interaction are both statistically significant
   i'll realize this may simply be a matter of the test statistics being
interdependant (though I might still report the column effects separately
if, say, one column was different--in a practical sense--from the rest).
   if neither row nor column nor interaction effects are significant, I'll
attribute the result to too small a sample.

(As far as higher-order interactions go, I never really had a clue as to
what they were about anyway.)

Phillip Good



From murdoch at stats.uwo.ca  Thu Jul  7 02:51:08 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 20:51:08 -0400
Subject: [R] Lack of independence in anova()
In-Reply-To: <XFMail.050707005154.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050707005154.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42CC7C7C.4010004@stats.uwo.ca>

(Ted Harding) wrote:
> On 06-Jul-05 G??ran Brostr??m wrote:
> 
>>On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
>>(...)
>>
>>> If X, Y, and Z are independent and Z takes on more than one
>>>value then X/Z and Y/Z can't be independent.
>>
>>Not really true. I  can produce a counterexample on request
>>(admittedly quite trivial though).
>>
>>G??ran Brostr??m
> 
> 
> But true if both X  and Y have positive probability of being
> non-zero, n'est-pas?
> 
> Tut, tut, G??ran!

If X and Y are independent with symmetric distributions about zero, and 
Z is is supported on +/- A for some non-zero constant A, then X/Z and 
Y/Z are still independent.  There are probably other special cases too.

Duncan Murdoch



From murdoch at stats.uwo.ca  Thu Jul  7 03:10:29 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Jul 2005 21:10:29 -0400
Subject: [R] Lack of independence in anova()
In-Reply-To: <42CC70FE.1010500@pdf.com>
References: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>	<Pine.A41.4.61b.0507060945070.109146@homer05.u.washington.edu>	<20050706220052.GA3138@stat.umu.se>
	<42CC70FE.1010500@pdf.com>
Message-ID: <42CC8105.5060408@stats.uwo.ca>

Spencer Graves wrote:
> Hi, G??ran:  I'll bite:
> 
> 	  (a) I'd like to see your counterexample.
> 
> 	  (b) I'd like to know what is wrong with my the following, apparently 
> defective, proof that they can't be independent:  First consider 
> indicator functions of independent events A, B, and C.
> 
> 	  P{(AC)&(BC)} = P{ABC} = PA*PB*PC.
> 
> 	  But P(AC)*P(BC) = PA*PB*(PC)^2.  Thus, AC and BC can be independent 
> only if PC = 0 or 1, i.e., the indicator of C is constant almost surely.
> 
> 	  Is there a flaw in this?  

I don't see one.

 > If not, is there some reason this case
> cannot be extended the product of arbitrary random variables X, Y, and 
> W=1/Z?

Because you can't?  The situations are different?

If C is a non-trivial event independent of A, then AC is strictly a 
subset of A.  However, as the example I just posted shows (with constant 
1), you can have a non-trivial random variable W where XW has exactly 
the same distribution as X.

Duncan Murdoch



From minhuangr at gmail.com  Thu Jul  7 03:44:09 2005
From: minhuangr at gmail.com (huang min)
Date: Thu, 7 Jul 2005 09:44:09 +0800
Subject: [R] CDF plot
Message-ID: <bfc67668050706184428afbd43@mail.gmail.com>

Dear all,

I have define a discrete distribution P(y_i=x_i)=p_i, which I want to
plot a CDF plot. However, I can not find a function in R to draw it
for me after searching R and R-archive. I only find the one for the
sample CDF instead my theoretical one.

I find stepfun can do it for me, however, I want to plot some
different CDF with same support x in one plot. I can not manage how to
do it with stepfun.

Is there any existing function in R to draw the theoretical CDF plot?
Or how to abline other step functions in stepfun? Thanks.



From spencer.graves at pdf.com  Thu Jul  7 04:04:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 19:04:26 -0700
Subject: [R] Lack of independence in anova()
In-Reply-To: <42CC7C7C.4010004@stats.uwo.ca>
References: <XFMail.050707005154.Ted.Harding@nessie.mcc.ac.uk>
	<42CC7C7C.4010004@stats.uwo.ca>
Message-ID: <42CC8DAA.8070402@pdf.com>

Hi, Duncan & G??ran:

	  Consider the following:  X, Y, Z have symmetric distributions with 
the following restrictions:

	  P(X=1)=P(X=-1)=x with P(|X|<1)=0 so P(|X|>1)=1-2x.
	  P(Y=1)=P(Y=-1)=y with P(|Y|<1)=0 so P(|Y|>1)=1-2y.
	  P(Z=1)=P(Z=-1)=z with P(|Z|>1)=0 so P(|Z|<1)=1-2z.

	  Then

	  P(X/Z=1)=2xz, P(Y/Z=1)=2yz, and
	  P{(X/Z=1)&(Y/Z)=1}=2xyz.

	  Independence requires that this last probability is 4xyz^2.  This is 
true only if z=0.5.  If z<0.5, then X/Z and Y/Z are clearly dependent.
	
	  How's this?
	  spencer graves

Duncan Murdoch wrote:

> (Ted Harding) wrote:
> 
>>On 06-Jul-05 G??ran Brostr??m wrote:
>>
>>
>>>On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
>>>(...)
>>>
>>>
>>>>If X, Y, and Z are independent and Z takes on more than one
>>>>value then X/Z and Y/Z can't be independent.
>>>
>>>Not really true. I  can produce a counterexample on request
>>>(admittedly quite trivial though).
>>>
>>>G??ran Brostr??m
>>
>>
>>But true if both X  and Y have positive probability of being
>>non-zero, n'est-pas?
>>
>>Tut, tut, G??ran!
> 
> 
> If X and Y are independent with symmetric distributions about zero, and 
> Z is is supported on +/- A for some non-zero constant A, then X/Z and 
> Y/Z are still independent.  There are probably other special cases too.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From dmbates at gmail.com  Thu Jul  7 04:07:36 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 6 Jul 2005 21:07:36 -0500
Subject: [R] CDF plot
In-Reply-To: <bfc67668050706184428afbd43@mail.gmail.com>
References: <bfc67668050706184428afbd43@mail.gmail.com>
Message-ID: <40e66e0b050706190710ba44d3@mail.gmail.com>

On 7/6/05, huang min <minhuangr at gmail.com> wrote:
> Dear all,
> 
> I have define a discrete distribution P(y_i=x_i)=p_i, which I want to
> plot a CDF plot. However, I can not find a function in R to draw it
> for me after searching R and R-archive. I only find the one for the
> sample CDF instead my theoretical one.
> 
> I find stepfun can do it for me, however, I want to plot some
> different CDF with same support x in one plot. I can not manage how to
> do it with stepfun.
> 
> Is there any existing function in R to draw the theoretical CDF plot?
> Or how to abline other step functions in stepfun? Thanks.

The type = 's' option to the plot function is used to create the
"stairstep" plot of a cumulative distribution function.  Try

> x <- 0:10
> Pr <- pbinom(x, size = 10, prob = 0.314)
> data.frame(x = x, Pr = Pr)
    x         Pr
1   0 0.02308028
2   1 0.12872474
3   2 0.34632766
4   3 0.61193435
5   4 0.82469072
6   5 0.94155166
7   6 0.98612689
8   7 0.99778589
9   8 0.99978712
10  9 0.99999068
11 10 1.00000000
> plot(c(-0.3, x), c(0, Pr), type = "s", xlab = "x", ylab = "P", main = "Cumulative distribution function", las = 1)



From MSchwartz at mn.rr.com  Thu Jul  7 04:11:48 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 06 Jul 2005 21:11:48 -0500
Subject: [R] plotting on a reverse log scale
In-Reply-To: <971536df05070614311b163415@mail.gmail.com>
References: <42CC32A3.9040605@yorku.ca> <42CC38E2.20605@stats.uwo.ca>
	<42CC42EC.4080101@yorku.ca> <971536df05070614311b163415@mail.gmail.com>
Message-ID: <1120702309.3313.25.camel@localhost.localdomain>

I thought that I would take a stab at this. I should note however that
my solution is heavily biased by the first log scale plot that Michael
referenced below. To wit, my x axis has major ticks at powers of 10 and
minor ticks at 1/10 of each major tick interval.

Thus, here is my approach:

# Set the max range to an integer power of 10 as may be required
max.pow <- 5

# Set the data as others have done below
set.seed(1)
x <- runif(500, 1, 10^max.pow)
d <- density(x, from = 1, to = 10^max.pow)

# Now do the density plot, leaving the x axis blank
# Note that by doing a rev() on 'xlim' it reverses the min/max on the
# x axis, so you don't have to worry about adjusting things.
plot(d$y ~ d$x, xlim = rev(c(1, 10^max.pow)), log = "x", 
     xaxt = "n", type = "l",
     xlab = "2000 - Year (log scale)")

# Set the axis major tick marks
axis.at <- 10 ^ c(0:max.pow)

# Draw the major tick marks and label them using plotmath
axis(1, at = axis.at, tcl = -1,
     labels = parse(text = paste("10^", 0:max.pow, sep = "")))

# Now do the minor ticks, at 1/10 of each power of 10 interval
axis(1, at = 1:10 * rep(axis.at[-1] / 10, each = 10),
     tcl = -0.5, labels = FALSE)

# Do the rug plot
rug(x)


Not sure if this is helpful here, but thought I would post it for
review/critique. The 'max.pow' constant can be explicitly adjusted or
can be calculated automatically based upon input year ranges.

Best regards,

Marc Schwartz


On Wed, 2005-07-06 at 17:31 -0400, Gabor Grothendieck wrote:
> Not sure if I am missing something essential here but it would
> seem as simple as:
> 
> # data
> set.seed(1)
> x <- runif(500, 1500, 1990)  
> 
> # plot
> d <- density(x, from = 1500, to = 1990)
> plot(d$y ~ d$x, log = "x")
> rug(x)
> axis(1, seq(1500, 1990, 10), FALSE, tcl = -0.3)
> 
> 
> On 7/6/05, Michael Friendly <friendly at yorku.ca> wrote:
> > Thanks Duncan,
> > That is almost exactly what I want, except I want time to
> > go in the normal order, not backwards, so:
> > 
> > # plot on reverse log scale
> > years1500 <- runif(500, 1500, 1990)  # some fake data
> > x <- -log(2000-years1500)
> > from <- -log(2000-1990)
> > to <- -log(2000-1500)
> > plot(density(x, from=from, to=to), axes=F)
> > rug(x)
> > 
> > labels <- pretty(years1500)
> > labels <- labels[labels<2000]
> > axis(1, labels, at=-log(2000-labels))
> > 
> > minorticks <- pretty(years1500, n=20)
> > minorticks <- minorticks[minorticks<2000]
> > axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> > 
> > axis(2)
> > box()
> > 
> > -Michael
> > 
> > Duncan Murdoch wrote:
> > 
> > > On 7/6/2005 3:36 PM, Michael Friendly wrote:
> > >
> > >> I'd like to do some plots of historical event data on a reverse log
> > >> scale, started, say at the year 2000 and going
> > >> backwards in time, with tick marks spaced according to
> > >> log(2000-year).  For example, see:
> > >>
> > >> http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
> > >>
> > >> As an example, I'd like to create a density plot of such data with the
> > >> horizontal axis reverse-logged,
> > >> a transformation of this image:
> > >> http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
> > >>
> > >> Some initial code to do a standard density plot looks like this:
> > >>
> > >> mileyears <- read.csv("mileyears3.csv", skip=1,
> > >> col.names=c("key","year","where","add","junk"))
> > >> mileyears <- mileyears[,2:4]
> > >>
> > >> years <- mileyears$year
> > >> years1500 <- years[years>1500]
> > >> dens <- density(years1500, from=1500, to=1990)
> > >> plot(dens)
> > >> rug(years1500)
> > >>
> > >> I could calculate log(2000-year), but I'm not sure how to do the
> > >> plotting, do some minor tick marks
> > >> and label the major ones, say at 100 year intervals.
> > >
> > >
> > > I think you'll have to do everything explicitly.  That is, something
> > > like this:
> > >
> > > years1500 <- runif(500, 1500, 1990)  # some fake data
> > > x <- log(2000-years1500)
> > > from <- log(2000-1990)
> > > to <- log(2000-1500)
> > > plot(density(x, from=from, to=to), axes=F)
> > > rug(x)
> > >
> > > labels <- pretty(years1500)
> > > labels <- labels[labels<2000]
> > > axis(1, labels, at=log(2000-labels))
> > >
> > > minorticks <- pretty(years1500, n=20)
> > > minorticks <- minorticks[minorticks<2000]
> > > axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)
> > >
> > > axis(2)
> > > box()
> > >



From spencer.graves at pdf.com  Thu Jul  7 04:21:55 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 06 Jul 2005 19:21:55 -0700
Subject: [R] How to perform LSD or HSD tests with glm or lm
In-Reply-To: <d6d6fd440507061542222dc37f@mail.gmail.com>
References: <d6d6fd440507061542222dc37f@mail.gmail.com>
Message-ID: <42CC91C3.7080602@pdf.com>

	  TukeyHSD in package base works with aov objects.

	  simint in package multcomp has many more options.

	  RSiteSearch("Tukey HSD"), and RSiteSearch("LSD") also produced some 
interesting reading, including cautions by two of the leading 
contributors to R and to this list (e.g., 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/0470.html).

	  spencer graves

Ray Xiong wrote:
> Dear All,
> 
> I knew how to use glm or lm, but I do not know how to perform LSD or
> HSD on treatments with them. If someone can help me out, I greatly
> appreciate it.
> 
> Have a nice weekend.
> 
> Ray
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From daniel.ho at yale.edu  Thu Jul  7 04:31:12 2005
From: daniel.ho at yale.edu (Daniel E. Ho)
Date: Wed, 6 Jul 2005 22:31:12 -0400
Subject: [R] traceback and passing data frame
Message-ID: <010501c5829b$f11abbd0$6501a8c0@deh22>

Hello,

Is there a way to suppress traceback() from returning the full structure of
a dataframe that is passed internally in a function?

As a simple toy example, suppose we had two functions that pass a dataframe
internally:

data(Formaldehyde)
fn2 <- function(formula, data, ...) {
  res <- lm(formula, data, subset="some error", ...)
}
fn1 <- function(formula,data,...){
  tmp <- do.call("fn2",list(formula,data,...))
}
foo <- fn1(carb~optden,data=Formaldehyde)

fn2() clearly contains an error in subset.  This example is just to
illustrate the issue with traceback(), so please ignore the fact that this
is just an lm() call.  traceback() returns the following information:

6: stop("0 (non-NA) cases")
5: lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
4: lm(formula, data, subset = "some error", ...)
3: fn2(carb ~ optden, structure(list(carb = c(0.1, 0.3, 0.5, 0.6,
   0.7, 0.9), optden = c(0.086, 0.269, 0.446, 0.538, 0.626, 0.782
   )), .Names = c("carb", "optden"), row.names = c("1", "2", "3",
   "4", "5", "6"), class = "data.frame"))
2: do.call("fn2", list(formula, data, ...))
1: fn1(carb ~ optden, data = Formaldehyde)

Is there a way for line 3 of the traceback output NOT to include all data
and attributes?

In my case this would be helpful because if the data frame is large it can
run off the terminal window, so it would make diagnostics easier with larger
datasets.

Many thanks for your time,

Dan
--------------
System Info:
platform x86_64-redhat-linux-gnu
arch     x86_64
os       linux-gnu
system   x86_64, linux-gnu
status
major    2
minor    1.1
year     2005
month    06
day      20
language R



From kjetil at acelerate.com  Thu Jul  7 03:30:49 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 06 Jul 2005 21:30:49 -0400
Subject: [R] rlm/M/MM-estimator questions
In-Reply-To: <Pine.LNX.4.58.0507061928120.10149@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.58.0507061928120.10149@egon.stats.ucl.ac.uk>
Message-ID: <42CC85C9.5080407@acelerate.com>

Christian Hennig wrote:

>Hi list,
>
>1) How can the MM-estimator method="MM" in function rlm be tuned to 85%
>efficiency? It seems that there is a default tuning to 95%. I presume, but
>am not sure, that the MM-estimator uses phi=phi.bisquare as default and
>the tuning constant could be set by adding a parameter c=...
>Is this true? Which value to use for 85%?
>(In principle I should be able to figure that out theoretically, but it
>would be much easier if somebody already knew the constant or a
>straightforward way to compute it.)
>  
>
I have done this once, but cannot find the code or remember the 
constant. But given the constant, it is easy to do this
in R. rlm has an argument psi with default psi huber:

 > psi.huber
function (u, k = 1.345, deriv = 0)
{
    if (!deriv)
        return(pmin(1, k/abs(u)))
    abs(u) <= k
}
<environment: namespace:MASS>

Use this argument with   psi=function(u, k= your.value, deriv=0) 
psi.huber(u,k,deriv)

>2) The M-estimator with bisquare uses "rescaled MAD of the residuals" as
>scale estimator according to the rlm help page. Does this mean that a
>scale estimator is used which is computed from least squares residuals? Are
>M-estimator and residual scale estimator iterated until convergence of
>them both? (Does this converge?)
>

Not sure about this at the moment.

> Or what else? What does "rescaled" mean?
>  
>

"rescaled" means multiplied with the constant which makes it a 
consistent estimator
under the normal model, default in the R mad function

Kjetil

>Thank you,
>Christian
>
>
>*** NEW ADDRESS! ***
>Christian Hennig
>University College London, Department of Statistical Science
>Gower St., London WC1E 6BT, phone +44 207 679 1698
>chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From kjetil at acelerate.com  Thu Jul  7 04:43:46 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 06 Jul 2005 22:43:46 -0400
Subject: [R] Lack of independence in anova()
In-Reply-To: <42CC70FE.1010500@pdf.com>
References: <GHEKKACNLEADPKCNEEDFIEHBCEAA.pigood@verizon.net>	<Pine.A41.4.61b.0507060945070.109146@homer05.u.washington.edu>	<20050706220052.GA3138@stat.umu.se>
	<42CC70FE.1010500@pdf.com>
Message-ID: <42CC96E2.5010603@acelerate.com>

Spencer Graves wrote:

>Hi, G??ran:  I'll bite:
>
>	  (a) I'd like to see your counterexample.
>
>	  (b) I'd like to know what is wrong with my the following, apparently 
>defective, proof that they can't be independent:  First consider 
>indicator functions of independent events A, B, and C.
>
>	  P{(AC)&(BC)} = P{ABC} = PA*PB*PC.
>
>	  But P(AC)*P(BC) = PA*PB*(PC)^2.  Thus, AC and BC can be independent 
>only if PC = 0 or 1, i.e., the indicator of C is constant almost surely.
>
>	  Is there a flaw in this? 
>
As far as I can see, this is correct.

> If not, is there some reason this case 
>cannot be extended the product of arbitrary random variables X, Y, and 
>W=1/Z?
>  
>
Yes. Random variables are independent if all events which can be defined 
in terms of them are independent.
If Z is non-constant, it must be some event defined by Z with 
probability strictly beteween 0 and 1
and the above argument cannot be used.

Kjetil

>	  Thanks,
>	  spencer graves
>
>G??ran Brostr??m wrote:
>
>  
>
>>On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
>>(...)
>>
>>    
>>
>>>If X, Y, and Z are 
>>>independent and Z takes on more than one value then X/Z and Y/Z can't be 
>>>independent.
>>>      
>>>
>>Not really true. I  can produce a counterexample on request (admittedly
>>quite trivial though).
>>
>>G??ran Brostr??m
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From sean.oriordain at gmail.com  Thu Jul  7 06:36:28 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Thu, 7 Jul 2005 05:36:28 +0100
Subject: [R] Tempfile error
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409748B59@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409748B59@dc1ex2.air.org>
Message-ID: <8ed68eed050706213643bf572@mail.gmail.com>

Hi Harold,
I know nothing about Sweave (though it certainly sounds like a great
idea!).  Does Sweave hold these files open simultaneously - many
operating systems wouldn't be able to cope with 20,000 simultaneously
open files.

Could you be running out of disk space? or inodes - if you're on a
unix type system? can the directory hold that many files?  can the
disk hold that many files?  Many filesystems are formatted with an
assumption about the average file size.

Perhaps if you told us what type of operating system and type(s) of
filesystem(s)?

cheers!
Sean

On 7/6/05, Doran, Harold <HDoran at air.org> wrote:
> Dear List:
> 
> I am encountering an error that I can't resolve. I'm looping through
> rows of a dataframe to generate individual tex files using Sweave. At
> random points along the way, I encounter the following error
> 
>  Error in file() : cannot find unused tempfile name
> 
> At which point Sweave halts. There isn't a logical pattern that I can
> identify in terms of why the program stops at certain points. It just
> seems to be random as far as I can tell. I've searched the archives and
> of course Sweave FAQs but haven't found much that sheds light on what
> this error indicates and what I should do to resolve it.
> 
> There are approximately 20,000 rows, meaning that about 20,000 tex files
> are created. If I sample 5,000 or even 10,000 and run the program, I do
> not encounter an error. It only occurs when I run the program on the
> full dataframe and even then the error is not occuring at the same
> point. That is, the row at which the program halts varies each time.
> 
> Does anyone have any thoughts on this problem?
> 
> -Harold
> 
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mkeller at fam.tuwien.ac.at  Thu Jul  7 10:43:26 2005
From: mkeller at fam.tuwien.ac.at (Martin Keller-Ressel)
Date: Thu, 07 Jul 2005 08:43:26 -0000
Subject: [R] timezone problems
In-Reply-To: <p06210203bef19c99bfe2@[128.115.153.6]>
References: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>
	<p06210203bef19c99bfe2@[128.115.153.6]>
Message-ID: <op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>

Thank you Don for your hints. I have checked my environment vairable TZ  
again. But everything is set correctly. I think the problem is with  
Sys.timezone(). Maybe it is a conflict between how my system formats the  
time/date and what Sys.timezone() expects.
This is what I get on my system:

> Sys.getenv("TZ")
    TZ
"GMT"
> Sys.time()
[1] "2005-07-07 07:32:39 GMT"

## everything fine so far

> Sys.timezone()
[1] NA

## This is what Sys.timezone looks like:
> Sys.timezone
function ()
{
     z <- as.POSIXlt(Sys.time())
     attr(z, "tzone")[2 + z$isdst]
}
<environment: namespace:base>

> z <- as.POSIXlt(Sys.time())
> attributes(z)
$names
[1] "sec"   "min"   "hour"  "mday"  "mon"   "year"  "wday"  "yday"  "isdst"

$class
[1] "POSIXt"  "POSIXlt"

$tzone
[1] "GMT"

> attr(z,"tzone")
[1] "GMT"
> z$isdst
[1] 0
> attr(z,"tzone")[2]
[1] NA

I dont understand why Sys.timezone doesn't use attr(z,"tzone") but tries  
to read its (2+z$isdst)-th element.
Of course it would be easy to write a workaround, but I wonder why nobody  
else is having this problem.

best regards,

Martin Keller-Ressel



On Wed, 06 Jul 2005 14:45:25 -0000, Don MacQueen <macq at llnl.gov> wrote:

> How did you set the TZ system variable?
> If you did not use Sys.putenv(), try using it instead.
> Otherwise, I think you have to ask the package maintainer.
>
> You may be misleading yourself by using Sys.time() to test whether TZ is  
> set.
> What does Sys.getenv() tell you?
>
> I get a timezone code from Sys.time() even when TZ is not defined (see  
> example below).
> (but I do have a different OS)
>
>>  Sys.timezone()
> [1] ""
>>  Sys.time()
> [1] "2005-07-06 07:34:15 PDT"
>>  Sys.getenv('TZ')
> TZ
> ""
>>  Sys.putenv(TZ='US/Pacific')
>>  Sys.timezone()
> [1] "US/Pacific"
>>  Sys.getenv('TZ')
>            TZ
> "US/Pacific"
>>  Sys.time()
> [1] "2005-07-06 07:34:38 PDT"
>
>>  Sys.putenv(TZ='GMT')
>>  Sys.time()
> [1] "2005-07-06 14:35:45 GMT"
>
>>  version
>           _                       platform powerpc-apple-darwin7.9.0
> arch     powerpc                 os       darwin7.9.0              
> system   powerpc, darwin7.9.0    status                            
> major    2                       minor    1.1                      
> year     2005                    month    06                       
> day      20                      language R                        
> At 9:55 AM +0000 7/5/05, Martin Keller-Ressel wrote:
>> Hi,
>>
>> Im using R 2.1.1 and running Code that previously worked (on R 2.1.0 I  
>> believe) using the 'timeDate' function from the fCalendar package. The  
>> code now throws an error:
>>
>> Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")
>>
>> However I have read the documentation of the fCalendar package and I  
>> have set my system variable TZ to GMT.
>> I tracked the error down to the function Sys.timezone() which returns  
>> NA in spite of what Sys.time() returns.
>>
>>>  Sys.timezone()
>> [1] NA
>>
>>>  Sys.time()
>> [1] "2005-07-05 08:41:53 GMT"
>>
>> My version:
>>
>>>  version
>>           _
>> platform i386-pc-mingw32
>> arch     i386
>> os       mingw32
>> system   i386, mingw32
>> status
>> major    2
>> minor    1.1
>> year     2005
>> month    06
>> day      20
>> language R
>>
>> Any help is appreciated,
>>
>> Martin Keller-Ressel
>>
>>
>> ---
>> Martin Keller-Ressel
>> Research Unit of Financial and Actuarial Mathematics
>> TU Vienna
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>
>



-- 
Martin Keller-Ressel
Research Unit of Financial and Actuarial Mathematics
TU Vienna



From Matthias.Kohl at uni-bayreuth.de  Thu Jul  7 09:49:03 2005
From: Matthias.Kohl at uni-bayreuth.de (Matthias Kohl)
Date: Thu, 07 Jul 2005 09:49:03 +0200
Subject: [R] rlm/M/MM-estimator questions
In-Reply-To: <Pine.LNX.4.58.0507061928120.10149@egon.stats.ucl.ac.uk>
References: <Pine.LNX.4.58.0507061928120.10149@egon.stats.ucl.ac.uk>
Message-ID: <42CCDE6F.9040704@uni-bayreuth.de>

Christian Hennig wrote:

>Hi list,
>
>1) How can the MM-estimator method="MM" in function rlm be tuned to 85%
>efficiency? It seems that there is a default tuning to 95%. I presume, but
>am not sure, that the MM-estimator uses phi=phi.bisquare as default and
>the tuning constant could be set by adding a parameter c=...
>Is this true? Which value to use for 85%?
>(In principle I should be able to figure that out theoretically, but it
>would be much easier if somebody already knew the constant or a
>straightforward way to compute it.)
>  
>
Hi Christian,

I have not calculated the efficiency myself ...
But the thesis of  Matias Salibian-Barrera (SB 2000) might help you to 
find the answer
(cf. Chapter 4).
See: http://mathstat.math.carleton.ca:16080/~matias/thesis.pdf

As far as I understand the choice k0=1.548 is to obtain a breakdown 
point 0.5 whereas k0=1.988 leads to a breakdown point of 0.4 - at least 
in the location case; confer p. 60 of SB 2000.

In the article "Optimal robust $M$-estimates of location" by Fraiman, 
Yohai and Zamar (Ann. Stat. 29(1): 194 - 223) which is, of course, 
concerned with the location case, the authors recommend to use k0=1.988 
instead of k0=1.548 (cf. p. 206).

Hope that helps!
Matthias

>2) The M-estimator with bisquare uses "rescaled MAD of the residuals" as
>scale estimator according to the rlm help page. Does this mean that a
>scale estimator is used which is computed from least squares residuals? Are
>M-estimator and residual scale estimator iterated until convergence of
>them both? (Does this converge?) Or what else? What does "rescaled" mean?
>
>Thank you,
>Christian
>
>
>*** NEW ADDRESS! ***
>Christian Hennig
>University College London, Department of Statistical Science
>Gower St., London WC1E 6BT, phone +44 207 679 1698
>chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Allan at STATS.uct.ac.za  Thu Jul  7 09:59:35 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Thu, 07 Jul 2005 09:59:35 +0200
Subject: [R] r: LOOPING
Message-ID: <42CCE0E7.A9D9C17D@STATS.uct.ac.za>

hi all

i know that one should try and limit the amount of looping in R
programs. i have supplied some code below. i am interested in seeing how
the code cold be rewritten if we dont use the loops.


a brief overview of what is done in the code.
==============================================
==============================================
==============================================

1. the input file contains 120*500*61 cells. 120*500 rows and 61
columns.

2. we need to import the cells in 500 at a time and perform the same
operations on each sub group

3. the file contais numeric values. there are quite a lot of missing
values. this has been coded as NA in the text file (the file that is
imported)

4. for each variable we check for outliers. this is done by setting all
values that are greater than 3 standard deviations (sd) from the mean of
a variable to be equal to the 3 sd value.

5. the data set has one response variable , the first column, and 60
explanatory variables.

6. we regress each of the explanatory variables against the response and
record the slope of the explanatory variable. (i.e. simple linear
regression is performed)

7. nsize = 500 since we import 500 rows at a time

8. nruns = how many groups you want to run the analysis on

==============================================
==============================================
==============================================


TRY<-function(nsize=500,filename="C:/A.txt",nvar=61,nruns=1)
{

#the matrix with the payoff weights
fit.reg<-matrix(nrow=nruns,ncol=nvar-1)

for (ii in 1:nruns)
{
skip=1+(ii-1)*nsize

	#import the data in batches of "nsize*nvar"
	#save as a matrix and then delete "dscan" to save memory space

dscan<-scan(file=filename,sep="\t",skip=skip,nlines=nsize,fill=T,quiet=T)
	dm<-matrix(dscan,nrow=nsize,byrow=T)
	rm(dscan)

	#this calculates which of the columns have entries in the columns 
	#that are not NA
	#only perform regressions on those with more than 2 data points
	#obviously the number of points has to be much larger than 2
	#col.points = the number of points in the column that are not NA

	col.points<-apply(dm,2,function(x)
sum(match(x,rep(NA,nsize),nomatch=0)))
	col.points

	#adjust for outliers
	dm.new<-dm
	mean.dm.new<-apply(dm.new,2,function(x) mean(x,na.rm=T))
	sd.dm.new<-apply(dm.new,2,function(x) sd(x,na.rm=T))
	top.dm.new<-mean.dm.new+3*sd.dm.new
	bottom.dm.new<-mean.dm.new-3*sd.dm.new

	for (i in 1:nvar)
	{
		dm.new[,i][dm.new[,i]>top.dm.new[i]]<-top.dm.new[i]
		dm.new[,i][dm.new[,i]<bottom.dm.new[i]]<-bottom.dm.new[i]
	}

	#standardize the variables
	#we dont have to change the variable names here but i did!
	means.dm.new<-apply(dm.new,2,function(x) mean(x,na.rm=T))
	std.dm.new<-apply(dm.new,2,function(x) sd(x,na.rm=T))

	dm.new<-sweep(sweep(dm.new,2,means.dm.new,"-"),2,std.dm.new,"/")

	for (j in 2:nvar)
	{	
		'WE DO NOT PERFORM THE REGRESSION IF ALL VALUES IN THE COLUMN ARE "NA"
		if (col.points[j]!=nsize)
		{	
			#fit the regression equations
			fit.reg[ii,j-1]<-summary(lm(dm.new[,1]~dm.new[,j]))$coef[2,1]
		}
		else fit.reg[ii,j-1]<-"L"
	}
}

dm.names<-scan(file=filename,sep="\t",skip=0,nlines=1,fill=T,quiet=T,what="charachter")
dm.names<-matrix(dm.names,nrow=1,ncol=nvar,byrow=T)
colnames(fit.reg)<-dm.names[-1]

output<-c("$fit.reg")

list(fit.reg=fit.reg,output=output)

}

a=TRY(nsize=500,filename="C:/A.txt",nvar=61,nruns=1)


==============================================
==============================================
==============================================




thanking you in advance
/
allan

From m_osm at gmx.net  Thu Jul  7 10:06:58 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Thu, 7 Jul 2005 10:06:58 +0200 (MEST)
Subject: [R] =?iso-8859-1?q?R_under_suse_linux_9=2E3?=
Message-ID: <25009.1120723618@www8.gmx.net>

Hi, dear list members, 
 
I've been using R under windows XP and I am now changing 
my system to SUSE LINUX 9.3. 
 
I could figure out that there is no precompiled version 
of R for LINUX. To get me going, I would like your help 
regarding what I need to setup R under my SUSE LINUX. 
 
Which compiler do I need to be able to compile the source 
and I was wondering if I could get GNU compiler? 
 
I highly appreciate your help and for taking your time. 
 
 
Looking forward to hearing from you 
 
Regards  
 
Mahdi 

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net



From ligges at statistik.uni-dortmund.de  Thu Jul  7 10:51:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 10:51:45 +0200
Subject: [R] Tempfile error
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409748B59@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409748B59@dc1ex2.air.org>
Message-ID: <42CCED21.8000701@statistik.uni-dortmund.de>

Doran, Harold wrote:

> Dear List:
> 
> I am encountering an error that I can't resolve. I'm looping through
> rows of a dataframe to generate individual tex files using Sweave. At
> random points along the way, I encounter the following error
> 
>  Error in file() : cannot find unused tempfile name


Which version of R is this?
I think during one of the latest releases tempfile() name generation has 
been imporved, because R did not tried hard enough to find new (random) 
filenames for tempfiles in older releases of R.

Uwe Ligges


> At which point Sweave halts. There isn't a logical pattern that I can
> identify in terms of why the program stops at certain points. It just
> seems to be random as far as I can tell. I've searched the archives and
> of course Sweave FAQs but haven't found much that sheds light on what
> this error indicates and what I should do to resolve it.
> 
> There are approximately 20,000 rows, meaning that about 20,000 tex files
> are created. If I sample 5,000 or even 10,000 and run the program, I do
> not encounter an error. It only occurs when I run the program on the
> full dataframe and even then the error is not occuring at the same
> point. That is, the row at which the program halts varies each time. 
> 
> Does anyone have any thoughts on this problem?
> 
> -Harold
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From michael.watson at bbsrc.ac.uk  Thu Jul  7 10:58:04 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 7 Jul 2005 09:58:04 +0100
Subject: [R] Discriminant Function Analysis
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D8A2@iahce2knas1.iah.bbsrc.reserved>

Thanks for the answers Uwe!

So this is a common problem in biology - few number of cases and many,
many variables (genes, proteins, metabolites, etc etc)! 

Under these conditions, is discriminant function analysis not an ideal
method to use then?  Are there alternatives?

> 1) First problem, I got this error message:
> 
>>z <- lda(C0GRP_NA ~ ., dpi30)
> 
> Warning message:
> variables are collinear in: lda.default(x, grouping, ...) 
> 
> I guess this is not a good thing, however, I *did* get a result and it

> discriminated perfectly between my groups.  Can anyone explain what 
> this means?  Does it invalidate my results?

Well, 14 cases and 37 variables mean that not that many degrees of 
freedom are left.... ;-)
Of course, you get a perfect fit - with arbitrary data.



From ligges at statistik.uni-dortmund.de  Thu Jul  7 11:05:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 11:05:48 +0200
Subject: [R] Discriminant Function Analysis
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D8A2@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172D8A2@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <42CCF06C.1060408@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Thanks for the answers Uwe!
> 
> So this is a common problem in biology - few number of cases and many,
> many variables (genes, proteins, metabolites, etc etc)! 
> 
> Under these conditions, is discriminant function analysis not an ideal
> method to use then?  Are there alternatives?

No, obviously not "an ideal method", if used as is on the whole data.

Alternatives are certainly described in the literature - I am not 
specialised in this field (I mean, this gene stuff), hence do not want 
to specify misleading references here.

Uwe Ligges


> 
>>1) First problem, I got this error message:
>>
>>
>>>z <- lda(C0GRP_NA ~ ., dpi30)
>>
>>Warning message:
>>variables are collinear in: lda.default(x, grouping, ...) 
>>
>>I guess this is not a good thing, however, I *did* get a result and it
> 
> 
>>discriminated perfectly between my groups.  Can anyone explain what 
>>this means?  Does it invalidate my results?
> 
> 
> Well, 14 cases and 37 variables mean that not that many degrees of 
> freedom are left.... ;-)
> Of course, you get a perfect fit - with arbitrary data.



From ernesto at ipimar.pt  Thu Jul  7 11:26:22 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 07 Jul 2005 10:26:22 +0100
Subject: [R] R under suse linux 9.3
In-Reply-To: <25009.1120723618@www8.gmx.net>
References: <25009.1120723618@www8.gmx.net>
Message-ID: <42CCF53E.5050000@ipimar.pt>

Mahdi Osman wrote:
> Hi, dear list members, 
>  
> I've been using R under windows XP and I am now changing 
> my system to SUSE LINUX 9.3. 
>  
> I could figure out that there is no precompiled version 
> of R for LINUX. To get me going, I would like your help 
> regarding what I need to setup R under my SUSE LINUX. 
>  
> Which compiler do I need to be able to compile the source 
> and I was wondering if I could get GNU compiler? 
>  
> I highly appreciate your help and for taking your time. 
>  
>  
> Looking forward to hearing from you 
>  
> Regards  
>  
> Mahdi 
> 

Hi Mahdi,

Just uncompress the file

tar -xzf R-2.1.0.tar.gz

cd into the directory and do

./configure --prefix=/usr/local
make
make install

It might happen that some packages are not installed, the most common is 
not to have the devel version of "readline" and "f2c". Just start yast 
and install these packages.

Another issue is that you should run the previous commands with your 
user and the "make install" has root.

Regards

EJ



From plamy at daimi.au.dk  Thu Jul  7 11:37:15 2005
From: plamy at daimi.au.dk (Philippe Lamy)
Date: Thu, 07 Jul 2005 11:37:15 +0200
Subject: [R] How to get the minimum ?
Message-ID: <1120729035.42ccf7cb737f5@webmail.daimi.au.dk>

Hi,

I have a model with differents observations Xi.
Each observation belongs to a group, either A or B.
I would like to minimize a fonction like :

sum( Xi - Z)^2 + sum (Xi - aZ -b)^2
 A                B

The first sum contains all observations from group A and the second all
observations from group B.
I want to find the Z-value wich minimize this function. a and b are predefined
parameters.

Thanks for help.

Philippe



From p.dalgaard at biostat.ku.dk  Thu Jul  7 11:39:01 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jul 2005 11:39:01 +0200
Subject: [R] R under suse linux 9.3
In-Reply-To: <42CCF53E.5050000@ipimar.pt>
References: <25009.1120723618@www8.gmx.net> <42CCF53E.5050000@ipimar.pt>
Message-ID: <x2u0j7c5vu.fsf@turmalin.kubism.ku.dk>

Ernesto Jardim <ernesto at ipimar.pt> writes:

> Mahdi Osman wrote:
> > Hi, dear list members, 
> >  
> > I've been using R under windows XP and I am now changing 
> > my system to SUSE LINUX 9.3. 
> >  
> > I could figure out that there is no precompiled version 
> > of R for LINUX. To get me going, I would like your help 
> > regarding what I need to setup R under my SUSE LINUX. 
> >  
> > Which compiler do I need to be able to compile the source 
> > and I was wondering if I could get GNU compiler? 
> >  
> > I highly appreciate your help and for taking your time. 
> >  
> >  
> > Looking forward to hearing from you 
> >  
> > Regards  
> >  
> > Mahdi 
> > 
> 
> Hi Mahdi,
> 
> Just uncompress the file
> 
> tar -xzf R-2.1.0.tar.gz

Er, 2.1.1 has been out a couple of weeks...
 
> cd into the directory and do
> 
> ./configure --prefix=/usr/local
> make
> make install
> 
> It might happen that some packages are not installed, the most common is 
> not to have the devel version of "readline" and "f2c". Just start yast 
> and install these packages.

f2c ?? 

gcc-g77 more likely. Plus various other stuff relating to tcl/tk, etc.


> Another issue is that you should run the previous commands with your 
> user and the "make install" has root.

Yes, but what was ever wrong with 

http://cran.r-project.org/bin/linux/suse/9.3/RPMS/i586/R-base-2.1.1-1.i586.rpm

?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ernesto at ipimar.pt  Thu Jul  7 11:47:37 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 07 Jul 2005 10:47:37 +0100
Subject: [R] R under suse linux 9.3
In-Reply-To: <x2u0j7c5vu.fsf@turmalin.kubism.ku.dk>
References: <25009.1120723618@www8.gmx.net> <42CCF53E.5050000@ipimar.pt>
	<x2u0j7c5vu.fsf@turmalin.kubism.ku.dk>
Message-ID: <42CCFA39.6000000@ipimar.pt>

Peter Dalgaard wrote:
> Ernesto Jardim <ernesto at ipimar.pt> writes:
> 
> 
>>Mahdi Osman wrote:
>>
>>>Hi, dear list members, 
>>> 
>>>I've been using R under windows XP and I am now changing 
>>>my system to SUSE LINUX 9.3. 
>>> 
>>>I could figure out that there is no precompiled version 
>>>of R for LINUX. To get me going, I would like your help 
>>>regarding what I need to setup R under my SUSE LINUX. 
>>> 
>>>Which compiler do I need to be able to compile the source 
>>>and I was wondering if I could get GNU compiler? 
>>> 
>>>I highly appreciate your help and for taking your time. 
>>> 
>>> 
>>>Looking forward to hearing from you 
>>> 
>>>Regards  
>>> 
>>>Mahdi 
>>>
>>
>>Hi Mahdi,
>>
>>Just uncompress the file
>>
>>tar -xzf R-2.1.0.tar.gz
> 
> 
> Er, 2.1.1 has been out a couple of weeks...
>  
> 
>>cd into the directory and do
>>
>>./configure --prefix=/usr/local
>>make
>>make install
>>
>>It might happen that some packages are not installed, the most common is 
>>not to have the devel version of "readline" and "f2c". Just start yast 
>>and install these packages.
> 
> 
> f2c ?? 
> 
> gcc-g77 more likely. Plus various other stuff relating to tcl/tk, etc.
> 
> 
> 
>>Another issue is that you should run the previous commands with your 
>>user and the "make install" has root.
> 
> 
> Yes, but what was ever wrong with 
> 
> http://cran.r-project.org/bin/linux/suse/9.3/RPMS/i586/R-base-2.1.1-1.i586.rpm
> 
> ?
> 


Hi,

Probably nothing is wrong with the rpm, I just prefer to compile it 
myself ...

Regards

EJ



From ligges at statistik.uni-dortmund.de  Thu Jul  7 11:57:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 11:57:42 +0200
Subject: [R] timezone problems
In-Reply-To: <op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>
References: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>	<p06210203bef19c99bfe2@[128.115.153.6]>
	<op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <42CCFC96.7080304@statistik.uni-dortmund.de>

Martin Keller-Ressel wrote:

> Thank you Don for your hints. I have checked my environment vairable TZ  
> again. But everything is set correctly. I think the problem is with  
> Sys.timezone(). Maybe it is a conflict between how my system formats the  
> time/date and what Sys.timezone() expects.
> This is what I get on my system:
> 
> 
>>Sys.getenv("TZ")
> 
>     TZ
> "GMT"
> 
>>Sys.time()
> 
> [1] "2005-07-07 07:32:39 GMT"
> 
> ## everything fine so far
> 
> 
>>Sys.timezone()
> 
> [1] NA
> 
> ## This is what Sys.timezone looks like:
> 
>>Sys.timezone
> 
> function ()
> {
>      z <- as.POSIXlt(Sys.time())
>      attr(z, "tzone")[2 + z$isdst]
> }
> <environment: namespace:base>
> 
>>z <- as.POSIXlt(Sys.time())
>>attributes(z)
> 
> $names
> [1] "sec"   "min"   "hour"  "mday"  "mon"   "year"  "wday"  "yday"  "isdst"
> 
> $class
> [1] "POSIXt"  "POSIXlt"
> 
> $tzone
> [1] "GMT"
> 
> 
>>attr(z,"tzone")
> 
> [1] "GMT"
> 
>>z$isdst
> 
> [1] 0
> 
>>attr(z,"tzone")[2]
> 
> [1] NA
> 
> I dont understand why Sys.timezone doesn't use attr(z,"tzone") but tries  
> to read its (2+z$isdst)-th element.
> Of course it would be easy to write a workaround, but I wonder why nobody  
> else is having this problem.


I can confirm for R-2.1.1 under Windows NT 4.0 and it looks like a bug 
(somewhere down the way from as.POSIXlt). Don't have the time to look at 
it more closely, perhaps Brian knows it at once?
If this is not already in the bug repository (please check at first), 
can you submit a report, please? Thanks!

Uwe Ligges





> best regards,
> 
> Martin Keller-Ressel
> 
> 
> 
> On Wed, 06 Jul 2005 14:45:25 -0000, Don MacQueen <macq at llnl.gov> wrote:
> 
> 
>>How did you set the TZ system variable?
>>If you did not use Sys.putenv(), try using it instead.
>>Otherwise, I think you have to ask the package maintainer.
>>
>>You may be misleading yourself by using Sys.time() to test whether TZ is  
>>set.
>>What does Sys.getenv() tell you?
>>
>>I get a timezone code from Sys.time() even when TZ is not defined (see  
>>example below).
>>(but I do have a different OS)
>>
>>
>>> Sys.timezone()
>>
>>[1] ""
>>
>>> Sys.time()
>>
>>[1] "2005-07-06 07:34:15 PDT"
>>
>>> Sys.getenv('TZ')
>>
>>TZ
>>""
>>
>>> Sys.putenv(TZ='US/Pacific')
>>> Sys.timezone()
>>
>>[1] "US/Pacific"
>>
>>> Sys.getenv('TZ')
>>
>>           TZ
>>"US/Pacific"
>>
>>> Sys.time()
>>
>>[1] "2005-07-06 07:34:38 PDT"
>>
>>
>>> Sys.putenv(TZ='GMT')
>>> Sys.time()
>>
>>[1] "2005-07-06 14:35:45 GMT"
>>
>>
>>> version
>>
>>          _                       platform powerpc-apple-darwin7.9.0
>>arch     powerpc                 os       darwin7.9.0              
>>system   powerpc, darwin7.9.0    status                            
>>major    2                       minor    1.1                      
>>year     2005                    month    06                       
>>day      20                      language R                        
>>At 9:55 AM +0000 7/5/05, Martin Keller-Ressel wrote:
>>
>>>Hi,
>>>
>>>Im using R 2.1.1 and running Code that previously worked (on R 2.1.0 I  
>>>believe) using the 'timeDate' function from the fCalendar package. The  
>>>code now throws an error:
>>>
>>>Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")
>>>
>>>However I have read the documentation of the fCalendar package and I  
>>>have set my system variable TZ to GMT.
>>>I tracked the error down to the function Sys.timezone() which returns  
>>>NA in spite of what Sys.time() returns.
>>>
>>>
>>>> Sys.timezone()
>>>
>>>[1] NA
>>>
>>>
>>>> Sys.time()
>>>
>>>[1] "2005-07-05 08:41:53 GMT"
>>>
>>>My version:
>>>
>>>
>>>> version
>>>
>>>          _
>>>platform i386-pc-mingw32
>>>arch     i386
>>>os       mingw32
>>>system   i386, mingw32
>>>status
>>>major    2
>>>minor    1.1
>>>year     2005
>>>month    06
>>>day      20
>>>language R
>>>
>>>Any help is appreciated,
>>>
>>>Martin Keller-Ressel
>>>
>>>
>>>---
>>>Martin Keller-Ressel
>>>Research Unit of Financial and Actuarial Mathematics
>>>TU Vienna
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!  
>>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
>



From michael.watson at bbsrc.ac.uk  Thu Jul  7 12:07:06 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 7 Jul 2005 11:07:06 +0100
Subject: [R] R under suse linux 9.3
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D8AA@iahce2knas1.iah.bbsrc.reserved>

I've been successfully using R on SuSe linux for the last 2 years and I
use the rpm :-)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ernesto Jardim
Sent: 07 July 2005 10:48
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R under suse linux 9.3


Peter Dalgaard wrote:
> Ernesto Jardim <ernesto at ipimar.pt> writes:
> 
> 
>>Mahdi Osman wrote:
>>
>>>Hi, dear list members,
>>> 
>>>I've been using R under windows XP and I am now changing
>>>my system to SUSE LINUX 9.3. 
>>> 
>>>I could figure out that there is no precompiled version
>>>of R for LINUX. To get me going, I would like your help 
>>>regarding what I need to setup R under my SUSE LINUX. 
>>> 
>>>Which compiler do I need to be able to compile the source
>>>and I was wondering if I could get GNU compiler? 
>>> 
>>>I highly appreciate your help and for taking your time.
>>> 
>>> 
>>>Looking forward to hearing from you
>>> 
>>>Regards
>>> 
>>>Mahdi
>>>
>>
>>Hi Mahdi,
>>
>>Just uncompress the file
>>
>>tar -xzf R-2.1.0.tar.gz
> 
> 
> Er, 2.1.1 has been out a couple of weeks...
>  
> 
>>cd into the directory and do
>>
>>./configure --prefix=/usr/local
>>make
>>make install
>>
>>It might happen that some packages are not installed, the most common 
>>is
>>not to have the devel version of "readline" and "f2c". Just start yast

>>and install these packages.
> 
> 
> f2c ??
> 
> gcc-g77 more likely. Plus various other stuff relating to tcl/tk, etc.
> 
> 
> 
>>Another issue is that you should run the previous commands with your
>>user and the "make install" has root.
> 
> 
> Yes, but what was ever wrong with
> 
> http://cran.r-project.org/bin/linux/suse/9.3/RPMS/i586/R-base-2.1.1-1.
> i586.rpm
> 
> ?
> 


Hi,

Probably nothing is wrong with the rpm, I just prefer to compile it 
myself ...

Regards

EJ

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul  7 12:17:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 12:17:16 +0200
Subject: [R] r: LOOPING
In-Reply-To: <42CCE0E7.A9D9C17D@STATS.uct.ac.za>
References: <42CCE0E7.A9D9C17D@STATS.uct.ac.za>
Message-ID: <42CD012C.1090707@statistik.uni-dortmund.de>

Clark Allan wrote:

> hi all
> 
> i know that one should try and limit the amount of looping in R
> programs. i have supplied some code below. i am interested in seeing how
> the code cold be rewritten if we dont use the loops.


It is not always a good thing to remove loops (without having looked at 
each details of the code below).
One case seems to be described below, where you probably already get 
memory problems and hence cannot (at least not without memory penalty) 
"vectorize" any more.

"Compromise" is the keyword.

Best,
Uwe Ligges

> 
> a brief overview of what is done in the code.
> ==============================================
> ==============================================
> ==============================================
> 
> 1. the input file contains 120*500*61 cells. 120*500 rows and 61
> columns.
> 
> 2. we need to import the cells in 500 at a time and perform the same
> operations on each sub group
> 
> 3. the file contais numeric values. there are quite a lot of missing
> values. this has been coded as NA in the text file (the file that is
> imported)
> 
> 4. for each variable we check for outliers. this is done by setting all
> values that are greater than 3 standard deviations (sd) from the mean of
> a variable to be equal to the 3 sd value.
> 
> 5. the data set has one response variable , the first column, and 60
> explanatory variables.
> 
> 6. we regress each of the explanatory variables against the response and
> record the slope of the explanatory variable. (i.e. simple linear
> regression is performed)
> 
> 7. nsize = 500 since we import 500 rows at a time
> 
> 8. nruns = how many groups you want to run the analysis on
> 
> ==============================================
> ==============================================
> ==============================================
> 
> 
> TRY<-function(nsize=500,filename="C:/A.txt",nvar=61,nruns=1)
> {
> 
> #the matrix with the payoff weights
> fit.reg<-matrix(nrow=nruns,ncol=nvar-1)
> 
> for (ii in 1:nruns)
> {
> skip=1+(ii-1)*nsize
> 
> 	#import the data in batches of "nsize*nvar"
> 	#save as a matrix and then delete "dscan" to save memory space
> 
> dscan<-scan(file=filename,sep="\t",skip=skip,nlines=nsize,fill=T,quiet=T)
> 	dm<-matrix(dscan,nrow=nsize,byrow=T)
> 	rm(dscan)
> 
> 	#this calculates which of the columns have entries in the columns 
> 	#that are not NA
> 	#only perform regressions on those with more than 2 data points
> 	#obviously the number of points has to be much larger than 2
> 	#col.points = the number of points in the column that are not NA
> 
> 	col.points<-apply(dm,2,function(x)
> sum(match(x,rep(NA,nsize),nomatch=0)))
> 	col.points
> 
> 	#adjust for outliers
> 	dm.new<-dm
> 	mean.dm.new<-apply(dm.new,2,function(x) mean(x,na.rm=T))
> 	sd.dm.new<-apply(dm.new,2,function(x) sd(x,na.rm=T))
> 	top.dm.new<-mean.dm.new+3*sd.dm.new
> 	bottom.dm.new<-mean.dm.new-3*sd.dm.new
> 
> 	for (i in 1:nvar)
> 	{
> 		dm.new[,i][dm.new[,i]>top.dm.new[i]]<-top.dm.new[i]
> 		dm.new[,i][dm.new[,i]<bottom.dm.new[i]]<-bottom.dm.new[i]
> 	}
> 
> 	#standardize the variables
> 	#we dont have to change the variable names here but i did!
> 	means.dm.new<-apply(dm.new,2,function(x) mean(x,na.rm=T))
> 	std.dm.new<-apply(dm.new,2,function(x) sd(x,na.rm=T))
> 
> 	dm.new<-sweep(sweep(dm.new,2,means.dm.new,"-"),2,std.dm.new,"/")
> 
> 	for (j in 2:nvar)
> 	{	
> 		'WE DO NOT PERFORM THE REGRESSION IF ALL VALUES IN THE COLUMN ARE "NA"
> 		if (col.points[j]!=nsize)
> 		{	
> 			#fit the regression equations
> 			fit.reg[ii,j-1]<-summary(lm(dm.new[,1]~dm.new[,j]))$coef[2,1]
> 		}
> 		else fit.reg[ii,j-1]<-"L"
> 	}
> }
> 
> dm.names<-scan(file=filename,sep="\t",skip=0,nlines=1,fill=T,quiet=T,what="charachter")
> dm.names<-matrix(dm.names,nrow=1,ncol=nvar,byrow=T)
> colnames(fit.reg)<-dm.names[-1]
> 
> output<-c("$fit.reg")
> 
> list(fit.reg=fit.reg,output=output)
> 
> }
> 
> a=TRY(nsize=500,filename="C:/A.txt",nvar=61,nruns=1)
> 
> 
> ==============================================
> ==============================================
> ==============================================
> 
> 
> 
> 
> thanking you in advance
> /
> allan
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ivy_Li at smics.com  Thu Jul  7 12:28:48 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Thu, 7 Jul 2005 18:28:48 +0800
Subject: =?gb2312?B?tPC4tDogtPC4tDogW1JdIGZhaWwgaW4gYWRkaW5nIGxpYnJhcnkgaQ==?=
	=?gb2312?B?biBuZXcgdmVyc2lvbi4=?=
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF24@ex120.smic-sh.com>

Dear all,
	I have done every step as the previous mail.
1. unpack tools.zip into c:\cygwin
2. install Active perl in c:\Perl
3. install the mingw32 in c:\mingwin
4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"

	Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said. 
	 So in the Dos environment, at first, into the D:\>, I type the following code:
cd Program Files\R\rw2011\
bin\R CMD install /MyRpackages/example
	There are some error:
'make' is neither internal or external command, nor executable operation or batch file
*** installation of example failed ***
Removing 'D:/PROGRA~1/R/rw2011/library/example'

I think I have closed to success. heehee~~~~~
Thank you for your help.
I still need you and others help. Thank you very much!


----------
: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
: 2005630 19:16
: Ivy_Li
: r-help at stat.math.ethz.ch
: Re: : [R] fail in adding library in new version.


On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Dear Gabor,
>        Thank your for helping me so much!
>        I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> 1. unpack tools.zip into c:\cygwin
> 2. install Active perl in c:\Perl
> 3. install the mingw32 in c:\mingwin
> 4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)

If in the console you enter the command:

path

then it will display a semicolon separated list of folders.  You want the folder
that contains the tools to be at the beginning so that you eliminate
the possibility
of finding a different program of the same name first in a folder that comes
prior to the one where the tools are stored.

> 
> 5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> 
> 6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> cd \Program Files\R\rw2010
> But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"

I was assuming that MyRPackages and R are on the same disk.  If they are not
then you need to specify the disk too.  That is if MyRPackages is on C and R
is installed on D then install your package via:

d:
cd \Program Files\R\rw2010
bin\R CMD install c:/MyRPackages/example

Note that bin\R means to run R.exe in the bin subfolder of the current folder 
using command script install and the indicated source package.

> 
> I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?

If you are not sure where R is installed then enter the following at the Windows
console prompt to find out (this will work provided you let it install the key
into the registry when you installed R initially).  The reg command is a command
built into Windows (I used XP but I assume its the same on other versions)
that will query the Windows registry:

reg query hklm\software\r-core\r /v InstallPath

> 
> I still need your and others help. Thank you very much!
> 
> 
> 
> ----------
> : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> : 200566 10:21
> : Ivy_Li
> : r-help at stat.math.ethz.ch
> : Re: [R] fail in adding library in new version.
> 
> 
> On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > Hello everybody,
> >        Could I consult you a question?
> >        I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> 
> Getting the latest version of R is strongly recommended.  The suggestions
> below all assume the latest version and may or may not work if you do
> not upgrade.
> 
> > *       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> > *       Download the tools.zip
> > *       Unpack tools.zip into c:\cygwin
> > *       Install Active Perl in c:\Perl
> > *       Install the mingw32 port of gcc in c:\mingwin
> > *       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> 
> You may need to put these at the beginning of the path rather than the end.
> Also just as a check enter
>     path
> at the console to make sure that you have them.  You will likely
> have to start a new console session and possibly even reboot.
> 
> Also you need the Microsoft Help Compiler, hhc.  Suggest
> you reread the material on which tools you need.
> 
> > *       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> 
> In MyRPackages you would have a folder called example, in your case,
> that contains the package.  Within folder example, you would have the
> DESCRIPTION file, the R folder, etc.
> 
> > *       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> 
> You don't have to run R first.  You do need to make sure that R.exe can
> be found on your path or else use the absolute path name in referring to R.
> For example, if your path does not include R you could do something like this:
> 
> cd \Program Files\R\rw2010
> bin\R cmd install /MyRPackages/example

Sorry, there is an error in the above.  It should be:

bin\R CMD install c:/MyRPackages/example

or 

bin\Rcmd install c:/MyRPackages/example


> 
> Be sure to use forward slashes where shown above and backslashes
> where shown.
> 
> >        So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >
> 
> Try all these suggestions including upgrading R and if that does not work
> try posting screen dumps of the actual errors you are getting.
> 

Also try googling for

  making creating R packages

and you will find some privately written tutorials on all this.



From Ted.Harding at nessie.mcc.ac.uk  Thu Jul  7 12:18:09 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 07 Jul 2005 11:18:09 +0100 (BST)
Subject: [R] Lack of independence in anova()
In-Reply-To: <42CC8DAA.8070402@pdf.com>
Message-ID: <XFMail.050707111809.Ted.Harding@nessie.mcc.ac.uk>

My first reaction to Duncan's example was "Touch?? -- with apologies
to G??ran for suspecting on over-trivial example"! I had not thought
long enough about possible cases. Duncan is right; and maybe it is
the same example as G??ran was thinking of.

Regarding Spencer's argument below, in Duncan's statement he says
"Z is supported on +/- A" (i.e. Z = A or Z = -A),
so P(|Z| < 1) = 0 and so Spencer's 1-2z = 0 and z=1/2 (but Spencer
stipulates that Z is symmetric).

In general, suppose P(Z = A) = p > 0 and P(Z = -A) = q = 1-p.

Since X and Y are symmetric, X/A has the same distribution
as X/(-A) and similarly for Y; hence for any v and w,
P(X/Z <= v | X = z) is independent of z = +/- A, therefore
= P(X/Z <= v); and similarly for Y.

Also X/A, Y/A are independent, and so are X/(-A) and Y/(-A).

Hence P(X/Z <= v and Y/Z <= w)

    = p*P(X/Z <= v | Z = A)*P(Y/Z <= w | Z = A)

      + q*P(X/Z <= v | Z = -A)*P(Y/Z <= w | Z = -A)

    = (p + q)*P(X/Z <= v)*P(Y/Z <= w)

    = P(X/Z <= v)*P(Y/Z <= w)

so X/Z and Y/Z are independent.

However, interesting though it maybe, this is a side-issue
to the original question concerning independence of the F-ratios
in an ANOVA. Here, numerators and denominator are all positive,
so examples like the above are not relevant.

The original argument (that increasing Z diminishes both X/Z
and Y/Z simultaneously) applies; but it is also possible to
demonstrate analytically that P(X/Z <= v and Y/Z <= w) is
greater than P(X/Z <= v)*P(Y/Z <= w).

The original issue also was that, in R, there might be a bug
in anova(). However, one can, in R and independently of the
behaviour of anova(), demonstrate this positive correlation:

  C<-numeric(10000);
  for(i in (1:10000)){
    X<-rchisq(1000,5)/5
    Y<-rchisq(1000,5)/5
    Z<-rchisq(1000,20)/20
    C[i]<-cor(X/Z,Y/Z)
  }
 hist(C)

which shows that all 10000 correlations are positive.

Best wishes to all,
Ted.

On 07-Jul-05 Spencer Graves wrote:
> Hi, Duncan & G??ran:
> 
>         Consider the following:  X, Y, Z have symmetric distributions
with 
> the following restrictions:
> 
>         P(X=1)=P(X=-1)=x with P(|X|<1)=0 so P(|X|>1)=1-2x.
>         P(Y=1)=P(Y=-1)=y with P(|Y|<1)=0 so P(|Y|>1)=1-2y.
>         P(Z=1)=P(Z=-1)=z with P(|Z|>1)=0 so P(|Z|<1)=1-2z.
> 
>         Then
> 
>         P(X/Z=1)=2xz, P(Y/Z=1)=2yz, and
>         P{(X/Z=1)&(Y/Z)=1}=2xyz.
> 
>         Independence requires that this last probability is 4xyz^2. 
This is
> true only if z=0.5.  If z<0.5, then X/Z and Y/Z are clearly dependent.
>       
>         How's this?
>         spencer graves
> 
> Duncan Murdoch wrote:
> 
>> (Ted Harding) wrote:
>> 
>>>On 06-Jul-05 G??ran Brostr??m wrote:
>>>
>>>
>>>>On Wed, Jul 06, 2005 at 10:06:45AM -0700, Thomas Lumley wrote:
>>>>(...)
>>>>
>>>>
>>>>>If X, Y, and Z are independent and Z takes on more than one
>>>>>value then X/Z and Y/Z can't be independent.
>>>>
>>>>Not really true. I  can produce a counterexample on request
>>>>(admittedly quite trivial though).
>>>>
>>>>G??ran Brostr??m
>>>
>>>
>>>But true if both X  and Y have positive probability of being
>>>non-zero, n'est-pas?
>>>
>>>Tut, tut, G??ran!
>> 
>> 
>> If X and Y are independent with symmetric distributions about zero,
>> and 
>> Z is is supported on +/- A for some non-zero constant A, then X/Z and 
>> Y/Z are still independent.  There are probably other special cases
>> too.
>> 
>> Duncan Murdoch
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
> 
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Jul-05                                       Time: 11:18:04
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Thu Jul  7 13:04:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 13:04:27 +0200
Subject: =?GB2312?B?tPC4tDogtPC4tDogW1JdIGZhaWwgaW4gYWRkaW5nIGxpYnJhcg==?=
	=?GB2312?B?eSBpbiBuZXcgdmVyc2lvbi4=?=
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF24@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF24@ex120.smic-sh.com>
Message-ID: <42CD0C3B.1070407@statistik.uni-dortmund.de>

Ivy_Li wrote:

> Dear all,
> 	I have done every step as the previous mail.
> 1. unpack tools.zip into c:\cygwin
> 2. install Active perl in c:\Perl
> 3. install the mingw32 in c:\mingwin
> 4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
                    ^
such blanks are not allowed in the PATH variable



> 	Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said. 
> 	 So in the Dos environment, at first, into the D:\>, I type the following code:
> cd Program Files\R\rw2011\

MyRpackages does not need to be here.

> bin\R CMD install /MyRpackages/example

The first slash in "/MyRPackages" sugests that this is a top level
directory, which does not exist.
Even better, cd to MyRpackages, add R's bin dir to your path variable,
and simply say:

R CMD INSTALL example



> 	There are some error:
> 'make' is neither internal or external command, nor executable operation or batch file
> *** installation of example failed ***

Well, make.exe is not find in your path. Please check whether the file
exists and the path has been added.

Uwe Ligges


> Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> I think I have closed to success. heehee~~~~~
> Thank you for your help.
> I still need you and others help. Thank you very much!
> 
> 
> ----------
> : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> : 2005630 19:16
> : Ivy_Li
> : r-help at stat.math.ethz.ch
> : Re: : [R] fail in adding library in new version.
> 
> 
> On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> 
>>Dear Gabor,
>>       Thank your for helping me so much!
>>       I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
>>1. unpack tools.zip into c:\cygwin
>>2. install Active perl in c:\Perl
>>3. install the mingw32 in c:\mingwin
>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> 
> 
> If in the console you enter the command:
> 
> path
> 
> then it will display a semicolon separated list of folders.  You want the folder
> that contains the tools to be at the beginning so that you eliminate
> the possibility
> of finding a different program of the same name first in a folder that comes
> prior to the one where the tools are stored.
> 
> 
>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
>>
>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
>>cd \Program Files\R\rw2010
>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> 
> 
> I was assuming that MyRPackages and R are on the same disk.  If they are not
> then you need to specify the disk too.  That is if MyRPackages is on C and R
> is installed on D then install your package via:
> 
> d:
> cd \Program Files\R\rw2010
> bin\R CMD install c:/MyRPackages/example
> 
> Note that bin\R means to run R.exe in the bin subfolder of the current folder 
> using command script install and the indicated source package.
> 
> 
>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> 
> 
> If you are not sure where R is installed then enter the following at the Windows
> console prompt to find out (this will work provided you let it install the key
> into the registry when you installed R initially).  The reg command is a command
> built into Windows (I used XP but I assume its the same on other versions)
> that will query the Windows registry:
> 
> reg query hklm\software\r-core\r /v InstallPath
> 
> 
>>I still need your and others help. Thank you very much!
>>
>>
>>
>>----------
>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>: 200566 10:21
>>: Ivy_Li
>>: r-help at stat.math.ethz.ch
>>: Re: [R] fail in adding library in new version.
>>
>>
>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>
>>>Hello everybody,
>>>       Could I consult you a question?
>>>       I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
>>
>>Getting the latest version of R is strongly recommended.  The suggestions
>>below all assume the latest version and may or may not work if you do
>>not upgrade.
>>
>>
>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
>>>*       Download the tools.zip
>>>*       Unpack tools.zip into c:\cygwin
>>>*       Install Active Perl in c:\Perl
>>>*       Install the mingw32 port of gcc in c:\mingwin
>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
>>
>>You may need to put these at the beginning of the path rather than the end.
>>Also just as a check enter
>>    path
>>at the console to make sure that you have them.  You will likely
>>have to start a new console session and possibly even reboot.
>>
>>Also you need the Microsoft Help Compiler, hhc.  Suggest
>>you reread the material on which tools you need.
>>
>>
>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
>>
>>In MyRPackages you would have a folder called example, in your case,
>>that contains the package.  Within folder example, you would have the
>>DESCRIPTION file, the R folder, etc.
>>
>>
>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
>>
>>You don't have to run R first.  You do need to make sure that R.exe can
>>be found on your path or else use the absolute path name in referring to R.
>>For example, if your path does not include R you could do something like this:
>>
>>cd \Program Files\R\rw2010
>>bin\R cmd install /MyRPackages/example
> 
> 
> Sorry, there is an error in the above.  It should be:
> 
> bin\R CMD install c:/MyRPackages/example
> 
> or 
> 
> bin\Rcmd install c:/MyRPackages/example
> 
> 
> 
>>Be sure to use forward slashes where shown above and backslashes
>>where shown.
>>
>>
>>>       So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
>>>
>>
>>Try all these suggestions including upgrading R and if that does not work
>>try posting screen dumps of the actual errors you are getting.
>>
> 
> 
> Also try googling for
> 
>   making creating R packages
> 
> and you will find some privately written tutorials on all this.
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Matthias.Templ at statistik.gv.at  Thu Jul  7 13:14:37 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 7 Jul 2005 13:14:37 +0200
Subject: [R] Making Package, Chm error, Html Help Workshop
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAB3A@xchg1.statistik.local>

Hello,

When building my package (R CMD check) following error message occurs:
 ...
  varinf.plot  text     html     latex      example
  x            text     html     latex      example
make[2]: *** No rule to make target `disclosure.chm`. Stop.
cp: cannot stat 'D:/Programme/R/rw2010dev/disclosure/chm/disclosure.chm`: No such file or directory
make[1]: *** [chm-disclosure] Error 1
make: *** [pkg-disclosure] Error 2
*** Installation of disclosure failed ***
Removing `D:/Programme/R/rw2010dev/bin/disclosure.Rcheck/disclosure'
 ERROR
Installation failed.

It seems, that there is a problem with HTML Workshop. No chm??s were built.
When I uninstall the HTML Help Workshop and remove the entry in the path environmental variable and doing packaging after this, the same error occurs.
I??m sure, that I have written the right entry (the path of the HTML help Workschop, where the hhc.exe file is) in the path of the environmental variable as the instructions said.

(Windows XP, Intel, R 2.1.0, HTML Workshop 1.32)

Has anybody seen such a problem before?
Can anybody give me a hint, please?

Thank you very much,
Matthias



From rn001 at cebas.csic.es  Thu Jul  7 13:23:23 2005
From: rn001 at cebas.csic.es (javier garcia)
Date: Thu, 7 Jul 2005 13:23:23 +0200
Subject: [R] about image() function in R and colors
Message-ID: <200507071323.23842.rn001@cebas.csic.es>

Hi!

I've got a map in R imported from a GIS (GRASS) as a vector of factors.
So I've got 20 different levels in the map and I've created a vector of custom 
colors of exactly 20 colors in lenght.
I'm trying to use image()  (really plot.grassmeta() that call image()) to plot 
the map with those colors but it doesnt work and the colors are changed.

I would like that all points belonging to level1 are color 1 , and so on...

Please could you tell me if this procedure is not correct?


Best regards,

Javier  

-- 
A. Javier Garcia
Water and Soil conservation department
CEBAS-CSIC
Campus Universitario Espinardo
PO BOX 164
30100 Murcia (SPAIN)
Phone: +34 968 39 62 57
Fax: +34 968 39 62 13
email: rn001 at cebas.csic.es



From rich at mi.fu-berlin.de  Thu Jul  7 13:36:59 2005
From: rich at mi.fu-berlin.de (rich@mi.fu-berlin.de)
Date: Thu, 07 Jul 2005 13:36:59 +0200
Subject: [R] Problems with nlme: Quinidine example
Message-ID: <1120736219.42cd13dbd8fa2@webmail.mi.fu-berlin.de>

This concerns the "Clinical Study of Quinidine" example on page 380
of the book "Mixed-Effects Models in S and S-PLUS" by Pinheiro and Bates (2000).

I have tried to reproduce the example, but get an error:

> library(nlme)
> fm1Quin.nlme <- nlme(conc ~ quinModel(Subject, time, conc, dose, interval, lV,
lKa, lCl),
+                      data=Quinidine,
+                      fixed=lV + lKa + lCl ~ 1,
+                      random=pdDiag(lV + lCl ~ 1),
+                      groups= ~ Subject,
+                      start=list(fixed=c(5, -0.3, 2)),
+                      na.action=na.pass, naPattern= ~ !is.na(conc))
Error in solve.default(estimates[dimE[1] - (p:1), dimE[2] - (p:1), drop =
FALSE]) :
        system is computationally singular: reciprocal condition number =
6.61723e-17

Note:
 - I am running R version 2.1.0 on Linux.
 - The only difference between the code in the book and the code above is
   that I use na.pass instead of na.include for the na.action argument, but
   I don't think this is significant.

I would appreciate help from anybody who has been able to get this example to
work.



From joseclaudio.faria at terra.com.br  Thu Jul  7 13:57:01 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Thu, 07 Jul 2005 08:57:01 -0300
Subject: [R] Tables: Invitation to make a collective package
Message-ID: <42CD188D.6040003@terra.com.br>

Hi All,

I would like to make an invitation to make a collective package with all 
functions related to TABLES.

I know that there are many packages with these functions, the original idea is 
collect all this functions and to make a single package, because is arduous for 
the user know all this functions broadcast in many packages.

So, I think that the original packages can continue with its original functions, 
but, is very good to know that exist one package with many (I dream all) the 
functions related to tables.

I've been working with these functions (while I am learning R programming):

#######################
#   Tables - Package  #
#######################

#
# 1. Tables
#

#
# Common function
#
tb.make.table.I <- function(x,
                             start,
                             end,
                             h,
                             right)
{
   f    <- table(cut(x, br=seq(start, end, h), right=right)) # Absolut freq
   fr   <- f/length(x)                                       # Relative freq
   frP  <- 100*(f/length(x))                                 # Relative freq, %
   fac  <- cumsum(f)                                         # Cumulative freq
   facP <- 100*(cumsum(f/length(x)))                         # Cumulative freq, %
   fi   <- round(f, 2)
   fr   <- round(as.numeric(fr), 2)
   frP  <- round(as.numeric(frP), 2)
   fac  <- round(as.numeric(fac), 2)
   facP <- round(as.numeric(facP),2)
   res  <- data.frame(fi, fr, frP, fac, facP)                # Make final table
   names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
   return(res)
}

#
# Common function
#
tb.make.table.II <- function (x,
                               k,
                               breaks=c('Sturges', 'Scott', 'FD'),
                               right=FALSE)
{
   x <- na.omit(x)

   # User defines only x and/or 'breaks'
   # (x, {k,}[breaks, right])
   if (missing(k)) {
     brk   <- match.arg(breaks)
     switch(brk,
            Sturges = k <- nclass.Sturges(x),
            Scott   = k <- nclass.scott(x),
            FD      = k <- nclass.FD(x))
     tmp   <- range(x)
     start <- tmp[1] - abs(tmp[2])/100
     end   <- tmp[2] + abs(tmp[2])/100
     R     <- end-start
     h     <- R/k
   }

   # User defines 'x' and 'k'
   # (x, k,[breaks, right])
   else {
     tmp   <- range(x)
     start <- tmp[1] - abs(tmp[2])/100
     end   <- tmp[2] + abs(tmp[2])/100
     R     <- end-start
     h     <- R/abs(k)
   }
   tbl     <- tb.make.table.I(x, start, end, h, right)
   return(tbl)
}

#
# With Gabor Grotendieck suggestions (thanks Gabor, very much!)
#
tb.table <- function(x, ...) UseMethod("tb.table")

#
# Table form vectors
#
tb.table.default <- function(x,
                              k,
                              start,
                              end,
                              h, breaks=c('Sturges', 'Scott', 'FD'),
                              right=FALSE)
{
   # User defines nothing or not 'x' isn't numeric -> stop
   stopifnot(is.numeric(x))
   x <- na.omit(x)

   # User defines only 'x'
   # (x, {k, start, end, h}, [breaks, right])
   if (missing(k) && missing(start) && missing(end) && missing(h) ) {
     brk   <- match.arg(breaks)
     switch(brk,
            Sturges = k <- nclass.Sturges(x),
            Scott   = k <- nclass.scott(x),
            FD      = k <- nclass.FD(x))
     tmp   <- range(x)
     start <- tmp[1] - abs(tmp[2])/100
     end   <- tmp[2] + abs(tmp[2])/100
     R     <- end-start
     h     <- R/k
   }

   # User defines 'x' and 'k'
   # (x, k, {start, end, h}, [breaks, right])
   else if (missing(start) && missing(end) && missing(h)) {
     stopifnot(length(k) >= 1)
     tmp   <- range(x)
     start <- tmp[1] - abs(tmp[2])/100
     end   <- tmp[2] + abs(tmp[2])/100
     R     <- end-start
     h     <- R/abs(k)
   }

   # User defines 'x', 'start' and 'end'
   # (x, {k,} start, end, {h,} [breaks, right])
   else if (missing(k) && missing(h)) {
     stopifnot(length(start) >= 1, length(end) >=1)
     tmp <- range(x)
     R   <- end-start
     k   <- sqrt(abs(R))
     if (k < 5)  k <- 5 # min value of k
     h   <- R/k
   }

   # User defines 'x', 'start', 'end' and 'h'
   # (x, {k,} start, end, h, [breaks, right])
   else if (missing(k)) {
     stopifnot(length(start) >= 1, length(end) >= 1, length(h) >= 1)
   }

   else stop('Error, please, see the function sintax!')
   tbl <- tb.make.table.I(x, start, end, h, right)
   return(tbl)
}


#
# Table form data.frame
#
tb.table.data.frame <- function(df,
                                 k,
                                 by,
                                 breaks=c('Sturges', 'Scott', 'FD'),
                                 right=FALSE)
{
   stopifnot(is.data.frame(df))
   tmpList <- list()
   nameF   <- character()
   nameY   <- character()

   # User didn't defines a factor
   if (missing(by)) {
     logCol  <- sapply(df, is.numeric)
     for (i in 1:ncol(df)) {
       if (logCol[i]) {
         x       <- as.matrix(df[ ,i])
         tbl     <- tb.make.table.II(x, k, breaks, right)
         tmpList <- c(tmpList, list(tbl))
       }
     }
     valCol <- logCol[logCol]
     names(tmpList) <- names(valCol)
     return(tmpList)
   }

   # User defines one factor
   else {
     namesdf <- names(df)
     pos     <- which(namesdf == by)
     stopifnot(is.factor((df[[pos]])))
     numF    <- table(df[[pos]])
     for(i in 1:length(numF)) {
       tmpdf  <- subset(df, df[[pos]] == names(numF[i]))
       logCol <- sapply(tmpdf, is.numeric)
       for (j in 1:ncol(tmpdf)) {
         if (logCol[j]) {
           x            <- as.matrix(tmpdf[ ,j])
           tbl          <- tb.make.table.II(x, k, breaks, right)
           newFY        <- list(tbl)
           nameF        <- names(numF[i])
           nameY        <- names(logCol[j])
           nameFY       <- paste(nameF,'.', nameY, sep="")
           names(newFY) <- sub(' +$', '', nameFY)
           tmpList      <- c(tmpList, newFY)
         }
       }
     }
   }
   return(tmpList)
}

############################
#     Tables package       #
#         to try           #
############################

# 1.Tables
# 1.1. Tables from vectors

# Making a vector
set.seed(1)
x=rnorm(100, 5, 1)
#x=as.factor(rep(1:10, 10))  # to try

tbl <- tb.table(x)
print(tbl); cat('\n')

# Equal to above
tbl <- tb.table(x, breaks='Sturges')
print(tbl); cat('\n')

tbl <- tb.table(x, breaks='Scott')
print(tbl); cat('\n')

tbl <- tb.table(x, breaks='FD')
print(tbl); cat('\n')

tbl <- tb.table(x, breaks='F', right=T)
print(tbl); cat('\n')

tbl <- tb.table(x, k=4)
print(tbl); cat('\n')

tbl <- tb.table(x, k=20)
print(tbl); cat('\n')

# Partial
tbl <- tb.table(x, start=4, end=6)
print(tbl); cat('\n')

# Partial
tbl <- tb.table(x, start=4.5, end=5.5)
print(tbl); cat('\n')

# Nonsense
tbl <- tb.table(x, start=0, end=10, h=.5)
print(tbl); cat('\n')

# First and last class forced (fi=0)
tbl <- tb.table(x, start=1, end=9, h=1)
print(tbl); cat('\n')

tbl <- tb.table(x, start=1, end=10, h=2)
print(tbl); cat('\n')


# 1.2. Tables from data.frame

# 1.2.1. Making a data.frame
mdf=data.frame(X1=rep(LETTERS[1:4], 25),
                X2=as.factor(rep(1:10, 10)),
                Y1=c(NA, NA, rnorm(96, 10, 1), NA, NA),
                Y2=rnorm(100, 58, 4),
                Y3=c(NA, NA, rnorm(98, -20, 2)))

tbl <- tb.table(mdf)
print(tbl)

# Equal to above
tbl <- tb.table(mdf, breaks='Sturges')
print(tbl)

tbl <- tb.table(mdf, breaks='Scott')
print(tbl)

tbl <- tb.table(mdf, breaks='FD')
print(tbl)

tbl <- tb.table(mdf, k=4)
print(tbl)

tbl <- tb.table(mdf, k=10)
print(tbl)

levels(mdf$X1)
tbl=tb.table(mdf, k=5, by='X1')
length(tbl)
names(tbl)
print(tbl)

tbl=tb.table(mdf, breaks='FD', by='X1')
print(tbl)

# A 'big' result: X2 is a factor with 10 levels!
tbl=tb.table(mdf, breaks='FD', by='X2')
print(tbl)

# 1.2.2. Using 'iris'
tbl=tb.table(iris, k=5)
print(tbl)

levels(iris$Species)
tbl=tb.table(iris, k=5, by='Species')
length(tbl)
names(tbl)
print(tbl)

tbl=tb.table(iris, k=5, by='Species', right=T)
print(tbl)

tbl=tb.table(iris, breaks='FD', by='Species')
print(tbl)

library(MASS)
levels(Cars93$Origin)
tbl=tb.table(Cars93, k=5, by='Origin')
names(tbl)
print(tbl)

tbl=tb.table(Cars93, breaks='FD', by='Origin')
print(tbl)

I find that this package would be very useful and would like to hear the opinion 
of the interested parties in participating.

Best regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From jacques.veslot at cirad.fr  Thu Jul  7 13:55:25 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 07 Jul 2005 15:55:25 +0400
Subject: [R] About ade4 and overlaying points
Message-ID: <42CD182D.9040805@cirad.fr>

Dear R-users,

Is there an easy way to avoid points one upon another when ploting rows 
and columns of 'dudi' objects ? Maybe there is a function in ade4 or in 
an other package, or maybe someone has his or her own function to do 
this (for example to automatically modify a little the coordinates of 
these points to get a readable plot ?).

Thanks in advance.
Best regards,

Jacques VESLOT



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Jul  7 14:22:27 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 7 Jul 2005 08:22:27 -0400 
Subject: [R] about image() function in R and colors
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40C5@us-arlington-0668.mail.saic.com>

I do not know the solution to your problem, but I had the similar problems
with image() function and most of derived functions. It seems that 'image'
function was really not meant for displaying image data, instead it was
designed to display matrices in the image format. Matlab had the same
problem and ended up creating 2 functions: 'image' (similar to R's 'image')
and 'imshow' (designed for displaying image data). 

There are three major processing steps in the 'image' that are hard to
control or reverse:
1) scaling of the data intensities ( problem explained by Javier). Scaling
is not much of a problem if continuous palette of colors is used , or to
quote
"?image" when "'col' is a list of colors such as that generated by
'rainbow', 'heat.colors', 'topo.colors', 'terrain.colors' or similar
functions". However scaling causes problems in case of discontinuous
color-maps (palettes).
2) scaling of the data dimensions so it fits in default size window instead
of scaling of the window to fit the data. This results in image with
non-square pixels. There might be a way to force 'image' not to scale
dimensions, I just did not spend much time looking for it yet.
3) Flipping of the image. As "?image" shows one needs to "transpose and flip
matrix horizontally" or perform "image(t(volcano)[ncol(volcano):1,])" for
the image data to be visualized in proper orientation.

All those steps make sense in case of visualizing 2D data, but they are a
hindrance in case of visualizing images.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of javier garcia
Sent: Thursday, July 07, 2005 7:23 AM
To: R-Help
Cc: statsgrass at grass.itc.it
Subject: [R] about image() function in R and colors

Hi!

I've got a map in R imported from a GIS (GRASS) as a vector of factors.
So I've got 20 different levels in the map and I've created a vector of
custom colors of exactly 20 colors in lenght.
I'm trying to use image()  (really plot.grassmeta() that call image()) to
plot the map with those colors but it doesnt work and the colors are
changed.

I would like that all points belonging to level1 are color 1 , and so on...

Please could you tell me if this procedure is not correct?


Best regards,

Javier  

--
A. Javier Garcia
Water and Soil conservation department
CEBAS-CSIC
Campus Universitario Espinardo
PO BOX 164
30100 Murcia (SPAIN)
Phone: +34 968 39 62 57
Fax: +34 968 39 62 13
email: rn001 at cebas.csic.es

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pablo.granitto at iasma.it  Thu Jul  7 14:25:46 2005
From: pablo.granitto at iasma.it (pablo.granitto@iasma.it)
Date: Thu, 7 Jul 2005 14:25:46 +0200
Subject: [R] Kernlab: problem with small datasets
Message-ID: <OF7D5FB08F.A3F1D312-ONC1257037.00444726-C1257037.0044472A@iasma.it>


Hi,

I found a small problem with kernlab. The problem, I think, is that the
3-fold cross-validation performed to estimate probabilities is not
class-balanced, so the classifier could find empty classes.
The following example (maybe a little forced) show the error:

data(glass)
set.seed(1)
model<-ksvm(Type~.,data=glass[c(1:2,75:76,165:166,180:181,190:191),],type="C-svc",kernel="vanilla",prob.model=TRUE)

Setting default kernel parameters
Error in indexes[[j]] : subscript out of bounds
In addition: Warning message:
Variable(s) `' constant. Cannot scale data. in: .local(x, ...)

HTH,
 Pablo



From kurt.sys at telenet.be  Thu Jul  7 14:28:01 2005
From: kurt.sys at telenet.be (Kurt Sys)
Date: Thu, 07 Jul 2005 14:28:01 +0200
Subject: [R] tcltk package
Message-ID: <42CD1FD1.80401@telenet.be>

Hi all,

I have a package depending on the tcltk-package. However, I see that 
this package has been disappeared... Is there a reason why package 
'tcltk' is not available anymore? Or is it replaced by another one?

thx,
Kurt.



From ggrothendieck at gmail.com  Thu Jul  7 14:28:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Jul 2005 08:28:35 -0400
Subject: [R] Tables: Invitation to make a collective package
In-Reply-To: <42CD188D.6040003@terra.com.br>
References: <42CD188D.6040003@terra.com.br>
Message-ID: <971536df05070705287fc68da@mail.gmail.com>

If the functionality you are thinking of already exists across multiple
packages an alternative to creating a new package would be to create
a task view as in:
   http://cran.r-project.org/src/contrib/Views/
as explained in the ctv package and the article in R News 5/1.

On 7/7/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Hi All,
> 
> I would like to make an invitation to make a collective package with all
> functions related to TABLES.
> 
> I know that there are many packages with these functions, the original idea is
> collect all this functions and to make a single package, because is arduous for
> the user know all this functions broadcast in many packages.
> 
> So, I think that the original packages can continue with its original functions,
> but, is very good to know that exist one package with many (I dream all) the
> functions related to tables.
> 
> I've been working with these functions (while I am learning R programming):
> 
> #######################
> #   Tables - Package  #
> #######################
> 
> #
> # 1. Tables
> #
> 
> #
> # Common function
> #
> tb.make.table.I <- function(x,
>                             start,
>                             end,
>                             h,
>                             right)
> {
>   f    <- table(cut(x, br=seq(start, end, h), right=right)) # Absolut freq
>   fr   <- f/length(x)                                       # Relative freq
>   frP  <- 100*(f/length(x))                                 # Relative freq, %
>   fac  <- cumsum(f)                                         # Cumulative freq
>   facP <- 100*(cumsum(f/length(x)))                         # Cumulative freq, %
>   fi   <- round(f, 2)
>   fr   <- round(as.numeric(fr), 2)
>   frP  <- round(as.numeric(frP), 2)
>   fac  <- round(as.numeric(fac), 2)
>   facP <- round(as.numeric(facP),2)
>   res  <- data.frame(fi, fr, frP, fac, facP)                # Make final table
>   names(res) <- c('Class limits', 'fi', 'fr', 'fr(%)', 'fac', 'fac(%)')
>   return(res)
> }
> 
> #
> # Common function
> #
> tb.make.table.II <- function (x,
>                               k,
>                               breaks=c('Sturges', 'Scott', 'FD'),
>                               right=FALSE)
> {
>   x <- na.omit(x)
> 
>   # User defines only x and/or 'breaks'
>   # (x, {k,}[breaks, right])
>   if (missing(k)) {
>     brk   <- match.arg(breaks)
>     switch(brk,
>            Sturges = k <- nclass.Sturges(x),
>            Scott   = k <- nclass.scott(x),
>            FD      = k <- nclass.FD(x))
>     tmp   <- range(x)
>     start <- tmp[1] - abs(tmp[2])/100
>     end   <- tmp[2] + abs(tmp[2])/100
>     R     <- end-start
>     h     <- R/k
>   }
> 
>   # User defines 'x' and 'k'
>   # (x, k,[breaks, right])
>   else {
>     tmp   <- range(x)
>     start <- tmp[1] - abs(tmp[2])/100
>     end   <- tmp[2] + abs(tmp[2])/100
>     R     <- end-start
>     h     <- R/abs(k)
>   }
>   tbl     <- tb.make.table.I(x, start, end, h, right)
>   return(tbl)
> }
> 
> #
> # With Gabor Grotendieck suggestions (thanks Gabor, very much!)
> #
> tb.table <- function(x, ...) UseMethod("tb.table")
> 
> #
> # Table form vectors
> #
> tb.table.default <- function(x,
>                              k,
>                              start,
>                              end,
>                              h, breaks=c('Sturges', 'Scott', 'FD'),
>                              right=FALSE)
> {
>   # User defines nothing or not 'x' isn't numeric -> stop
>   stopifnot(is.numeric(x))
>   x <- na.omit(x)
> 
>   # User defines only 'x'
>   # (x, {k, start, end, h}, [breaks, right])
>   if (missing(k) && missing(start) && missing(end) && missing(h) ) {
>     brk   <- match.arg(breaks)
>     switch(brk,
>            Sturges = k <- nclass.Sturges(x),
>            Scott   = k <- nclass.scott(x),
>            FD      = k <- nclass.FD(x))
>     tmp   <- range(x)
>     start <- tmp[1] - abs(tmp[2])/100
>     end   <- tmp[2] + abs(tmp[2])/100
>     R     <- end-start
>     h     <- R/k
>   }
> 
>   # User defines 'x' and 'k'
>   # (x, k, {start, end, h}, [breaks, right])
>   else if (missing(start) && missing(end) && missing(h)) {
>     stopifnot(length(k) >= 1)
>     tmp   <- range(x)
>     start <- tmp[1] - abs(tmp[2])/100
>     end   <- tmp[2] + abs(tmp[2])/100
>     R     <- end-start
>     h     <- R/abs(k)
>   }
> 
>   # User defines 'x', 'start' and 'end'
>   # (x, {k,} start, end, {h,} [breaks, right])
>   else if (missing(k) && missing(h)) {
>     stopifnot(length(start) >= 1, length(end) >=1)
>     tmp <- range(x)
>     R   <- end-start
>     k   <- sqrt(abs(R))
>     if (k < 5)  k <- 5 # min value of k
>     h   <- R/k
>   }
> 
>   # User defines 'x', 'start', 'end' and 'h'
>   # (x, {k,} start, end, h, [breaks, right])
>   else if (missing(k)) {
>     stopifnot(length(start) >= 1, length(end) >= 1, length(h) >= 1)
>   }
> 
>   else stop('Error, please, see the function sintax!')
>   tbl <- tb.make.table.I(x, start, end, h, right)
>   return(tbl)
> }
> 
> 
> #
> # Table form data.frame
> #
> tb.table.data.frame <- function(df,
>                                 k,
>                                 by,
>                                 breaks=c('Sturges', 'Scott', 'FD'),
>                                 right=FALSE)
> {
>   stopifnot(is.data.frame(df))
>   tmpList <- list()
>   nameF   <- character()
>   nameY   <- character()
> 
>   # User didn't defines a factor
>   if (missing(by)) {
>     logCol  <- sapply(df, is.numeric)
>     for (i in 1:ncol(df)) {
>       if (logCol[i]) {
>         x       <- as.matrix(df[ ,i])
>         tbl     <- tb.make.table.II(x, k, breaks, right)
>         tmpList <- c(tmpList, list(tbl))
>       }
>     }
>     valCol <- logCol[logCol]
>     names(tmpList) <- names(valCol)
>     return(tmpList)
>   }
> 
>   # User defines one factor
>   else {
>     namesdf <- names(df)
>     pos     <- which(namesdf == by)
>     stopifnot(is.factor((df[[pos]])))
>     numF    <- table(df[[pos]])
>     for(i in 1:length(numF)) {
>       tmpdf  <- subset(df, df[[pos]] == names(numF[i]))
>       logCol <- sapply(tmpdf, is.numeric)
>       for (j in 1:ncol(tmpdf)) {
>         if (logCol[j]) {
>           x            <- as.matrix(tmpdf[ ,j])
>           tbl          <- tb.make.table.II(x, k, breaks, right)
>           newFY        <- list(tbl)
>           nameF        <- names(numF[i])
>           nameY        <- names(logCol[j])
>           nameFY       <- paste(nameF,'.', nameY, sep="")
>           names(newFY) <- sub(' +$', '', nameFY)
>           tmpList      <- c(tmpList, newFY)
>         }
>       }
>     }
>   }
>   return(tmpList)
> }
> 
> ############################
> #     Tables package       #
> #         to try           #
> ############################
> 
> # 1.Tables
> # 1.1. Tables from vectors
> 
> # Making a vector
> set.seed(1)
> x=rnorm(100, 5, 1)
> #x=as.factor(rep(1:10, 10))  # to try
> 
> tbl <- tb.table(x)
> print(tbl); cat('\n')
> 
> # Equal to above
> tbl <- tb.table(x, breaks='Sturges')
> print(tbl); cat('\n')
> 
> tbl <- tb.table(x, breaks='Scott')
> print(tbl); cat('\n')
> 
> tbl <- tb.table(x, breaks='FD')
> print(tbl); cat('\n')
> 
> tbl <- tb.table(x, breaks='F', right=T)
> print(tbl); cat('\n')
> 
> tbl <- tb.table(x, k=4)
> print(tbl); cat('\n')
> 
> tbl <- tb.table(x, k=20)
> print(tbl); cat('\n')
> 
> # Partial
> tbl <- tb.table(x, start=4, end=6)
> print(tbl); cat('\n')
> 
> # Partial
> tbl <- tb.table(x, start=4.5, end=5.5)
> print(tbl); cat('\n')
> 
> # Nonsense
> tbl <- tb.table(x, start=0, end=10, h=.5)
> print(tbl); cat('\n')
> 
> # First and last class forced (fi=0)
> tbl <- tb.table(x, start=1, end=9, h=1)
> print(tbl); cat('\n')
> 
> tbl <- tb.table(x, start=1, end=10, h=2)
> print(tbl); cat('\n')
> 
> 
> # 1.2. Tables from data.frame
> 
> # 1.2.1. Making a data.frame
> mdf=data.frame(X1=rep(LETTERS[1:4], 25),
>                X2=as.factor(rep(1:10, 10)),
>                Y1=c(NA, NA, rnorm(96, 10, 1), NA, NA),
>                Y2=rnorm(100, 58, 4),
>                Y3=c(NA, NA, rnorm(98, -20, 2)))
> 
> tbl <- tb.table(mdf)
> print(tbl)
> 
> # Equal to above
> tbl <- tb.table(mdf, breaks='Sturges')
> print(tbl)
> 
> tbl <- tb.table(mdf, breaks='Scott')
> print(tbl)
> 
> tbl <- tb.table(mdf, breaks='FD')
> print(tbl)
> 
> tbl <- tb.table(mdf, k=4)
> print(tbl)
> 
> tbl <- tb.table(mdf, k=10)
> print(tbl)
> 
> levels(mdf$X1)
> tbl=tb.table(mdf, k=5, by='X1')
> length(tbl)
> names(tbl)
> print(tbl)
> 
> tbl=tb.table(mdf, breaks='FD', by='X1')
> print(tbl)
> 
> # A 'big' result: X2 is a factor with 10 levels!
> tbl=tb.table(mdf, breaks='FD', by='X2')
> print(tbl)
> 
> # 1.2.2. Using 'iris'
> tbl=tb.table(iris, k=5)
> print(tbl)
> 
> levels(iris$Species)
> tbl=tb.table(iris, k=5, by='Species')
> length(tbl)
> names(tbl)
> print(tbl)
> 
> tbl=tb.table(iris, k=5, by='Species', right=T)
> print(tbl)
> 
> tbl=tb.table(iris, breaks='FD', by='Species')
> print(tbl)
> 
> library(MASS)
> levels(Cars93$Origin)
> tbl=tb.table(Cars93, k=5, by='Origin')
> names(tbl)
> print(tbl)
> 
> tbl=tb.table(Cars93, breaks='FD', by='Origin')
> print(tbl)
> 
> I find that this package would be very useful and would like to hear the opinion
> of the interested parties in participating.
> 
> Best regards,
> --
> Jose Claudio Faria
> Brasil/Bahia/UESC/DCET
> Estatistica Experimental/Prof. Adjunto
> mails:
>  joseclaudio.faria at terra.com.br
>  jc_faria at uesc.br
>  jc_faria at uol.com.br
> tel: 73-3634.2779
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Jul  7 14:32:57 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Jul 2005 08:32:57 -0400
Subject: [R] tcltk package
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA6E@usctmx1106.Merck.com>

It's included in the base R distribution, I believe.

Andy

> From: Kurt Sys
> 
> Hi all,
> 
> I have a package depending on the tcltk-package. However, I see that 
> this package has been disappeared... Is there a reason why package 
> 'tcltk' is not available anymore? Or is it replaced by another one?
> 
> thx,
> Kurt.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Thu Jul  7 14:40:29 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Jul 2005 08:40:29 -0400
Subject: =?GB2312?B?UmU6ILTwuLQ6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZA==?=
	=?GB2312?B?aW5nIGxpYnJhcnkgaW4gbmV3IHZlcnNpb24u?=
In-Reply-To: <42CD0C3B.1070407@statistik.uni-dortmund.de>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF24@ex120.smic-sh.com>
	<42CD0C3B.1070407@statistik.uni-dortmund.de>
Message-ID: <971536df05070705402579d8f5@mail.gmail.com>

On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Ivy_Li wrote:
> 
> > Dear all,
> >       I have done every step as the previous mail.
> > 1. unpack tools.zip into c:\cygwin
> > 2. install Active perl in c:\Perl
> > 3. install the mingw32 in c:\mingwin
> > 4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
>                    ^
> such blanks are not allowed in the PATH variable
> 
> 
> 
> >       Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> >        So in the Dos environment, at first, into the D:\>, I type the following code:
> > cd Program Files\R\rw2011\
> 
> MyRpackages does not need to be here.
> 
> > bin\R CMD install /MyRpackages/example
> 
> The first slash in "/MyRPackages" sugests that this is a top level
> directory, which does not exist.
> Even better, cd to MyRpackages, add R's bin dir to your path variable,
> and simply say:
> 
> R CMD INSTALL example

Another possibility is to put Rcmd.bat from the batch file collection

   http://cran.r-project.org/contrib/extra/batchfiles/

in your path.  It will use the registry to find R so you won't have
to modify your path (nor would you have to remodify it every time you 
install a new version of R which is what you would otherwise have to do):

cd \MyPackages
Rcmd install example

> 
> 
> 
> >       There are some error:
> > 'make' is neither internal or external command, nor executable operation or batch file
> > *** installation of example failed ***
> 
> Well, make.exe is not find in your path. Please check whether the file
> exists and the path has been added.
> 
> Uwe Ligges
> 
> 
> > Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >
> > I think I have closed to success. heehee~~~~~
> > Thank you for your help.
> > I still need you and others help. Thank you very much!
> >
> >
> > ----------
> > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > : 2005630 19:16
> > : Ivy_Li
> > : r-help at stat.math.ethz.ch
> > : Re: : [R] fail in adding library in new version.
> >
> >
> > On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >
> >>Dear Gabor,
> >>       Thank your for helping me so much!
> >>       I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> >>1. unpack tools.zip into c:\cygwin
> >>2. install Active perl in c:\Perl
> >>3. install the mingw32 in c:\mingwin
> >>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> >
> >
> > If in the console you enter the command:
> >
> > path
> >
> > then it will display a semicolon separated list of folders.  You want the folder
> > that contains the tools to be at the beginning so that you eliminate
> > the possibility
> > of finding a different program of the same name first in a folder that comes
> > prior to the one where the tools are stored.
> >
> >
> >>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> >>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> >>
> >>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> >>cd \Program Files\R\rw2010
> >>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> >
> >
> > I was assuming that MyRPackages and R are on the same disk.  If they are not
> > then you need to specify the disk too.  That is if MyRPackages is on C and R
> > is installed on D then install your package via:
> >
> > d:
> > cd \Program Files\R\rw2010
> > bin\R CMD install c:/MyRPackages/example
> >
> > Note that bin\R means to run R.exe in the bin subfolder of the current folder
> > using command script install and the indicated source package.
> >
> >
> >>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> >
> >
> > If you are not sure where R is installed then enter the following at the Windows
> > console prompt to find out (this will work provided you let it install the key
> > into the registry when you installed R initially).  The reg command is a command
> > built into Windows (I used XP but I assume its the same on other versions)
> > that will query the Windows registry:
> >
> > reg query hklm\software\r-core\r /v InstallPath
> >
> >
> >>I still need your and others help. Thank you very much!
> >>
> >>
> >>
> >>----------
> >>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>: 200566 10:21
> >>: Ivy_Li
> >>: r-help at stat.math.ethz.ch
> >>: Re: [R] fail in adding library in new version.
> >>
> >>
> >>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>
> >>>Hello everybody,
> >>>       Could I consult you a question?
> >>>       I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> >>
> >>Getting the latest version of R is strongly recommended.  The suggestions
> >>below all assume the latest version and may or may not work if you do
> >>not upgrade.
> >>
> >>
> >>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>>*       Download the tools.zip
> >>>*       Unpack tools.zip into c:\cygwin
> >>>*       Install Active Perl in c:\Perl
> >>>*       Install the mingw32 port of gcc in c:\mingwin
> >>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> >>
> >>You may need to put these at the beginning of the path rather than the end.
> >>Also just as a check enter
> >>    path
> >>at the console to make sure that you have them.  You will likely
> >>have to start a new console session and possibly even reboot.
> >>
> >>Also you need the Microsoft Help Compiler, hhc.  Suggest
> >>you reread the material on which tools you need.
> >>
> >>
> >>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> >>
> >>In MyRPackages you would have a folder called example, in your case,
> >>that contains the package.  Within folder example, you would have the
> >>DESCRIPTION file, the R folder, etc.
> >>
> >>
> >>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> >>
> >>You don't have to run R first.  You do need to make sure that R.exe can
> >>be found on your path or else use the absolute path name in referring to R.
> >>For example, if your path does not include R you could do something like this:
> >>
> >>cd \Program Files\R\rw2010
> >>bin\R cmd install /MyRPackages/example
> >
> >
> > Sorry, there is an error in the above.  It should be:
> >
> > bin\R CMD install c:/MyRPackages/example
> >
> > or
> >
> > bin\Rcmd install c:/MyRPackages/example
> >
> >
> >
> >>Be sure to use forward slashes where shown above and backslashes
> >>where shown.
> >>
> >>
> >>>       So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >>>
> >>
> >>Try all these suggestions including upgrading R and if that does not work
> >>try posting screen dumps of the actual errors you are getting.
> >>
> >
> >
> > Also try googling for
> >
> >   making creating R packages
> >
> > and you will find some privately written tutorials on all this.
> >
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From rpeng at jhsph.edu  Thu Jul  7 14:41:57 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 07 Jul 2005 08:41:57 -0400
Subject: [R] tcltk package
In-Reply-To: <42CD1FD1.80401@telenet.be>
References: <42CD1FD1.80401@telenet.be>
Message-ID: <42CD2315.8000309@jhsph.edu>

How do you know that it has disappeared?

-roger

Kurt Sys wrote:
> Hi all,
> 
> I have a package depending on the tcltk-package. However, I see that 
> this package has been disappeared... Is there a reason why package 
> 'tcltk' is not available anymore? Or is it replaced by another one?
> 
> thx,
> Kurt.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ligges at statistik.uni-dortmund.de  Thu Jul  7 14:49:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 14:49:07 +0200
Subject: =?GB2312?B?tPC4tDogtPC4tDogW1JdIGZhaWwgaW4gYWRkaW5nIGxpYnJhcg==?=
	=?GB2312?B?eSBpbiBuZXcgdmVyc2lvbi4=?=
In-Reply-To: <971536df05070705402579d8f5@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF24@ex120.smic-sh.com>	
	<42CD0C3B.1070407@statistik.uni-dortmund.de>
	<971536df05070705402579d8f5@mail.gmail.com>
Message-ID: <42CD24C3.7070400@statistik.uni-dortmund.de>

Gabor Grothendieck wrote:

> On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Ivy_Li wrote:
>>
>>
>>>Dear all,
>>>      I have done every step as the previous mail.
>>>1. unpack tools.zip into c:\cygwin
>>>2. install Active perl in c:\Perl
>>>3. install the mingw32 in c:\mingwin
>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
>>
>>                   ^
>>such blanks are not allowed in the PATH variable
>>
>>
>>
>>
>>>      Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
>>>       So in the Dos environment, at first, into the D:\>, I type the following code:
>>>cd Program Files\R\rw2011\
>>
>>MyRpackages does not need to be here.
>>
>>
>>>bin\R CMD install /MyRpackages/example
>>
>>The first slash in "/MyRPackages" sugests that this is a top level
>>directory, which does not exist.
>>Even better, cd to MyRpackages, add R's bin dir to your path variable,
>>and simply say:
>>
>>R CMD INSTALL example
> 
> 
> Another possibility is to put Rcmd.bat from the batch file collection
> 
>    http://cran.r-project.org/contrib/extra/batchfiles/
> 
> in your path.  It will use the registry to find R so you won't have
> to modify your path (nor would you have to remodify it every time you 
> install a new version of R which is what you would otherwise have to do):
> 
> cd \MyPackages
> Rcmd install example


Just for the records:

1. "cd \MyPackages" won't work, as I have already explained above.

2. I do *not* recommend this way, in particular I find it misleading to
provide these batch files on CRAN.

3. I have decided not to comment any more on this topic in future.

Uwe Ligges



> 
> 
>>
>>
>>>      There are some error:
>>>'make' is neither internal or external command, nor executable operation or batch file
>>>*** installation of example failed ***
>>
>>Well, make.exe is not find in your path. Please check whether the file
>>exists and the path has been added.
>>
>>Uwe Ligges
>>
>>
>>
>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
>>>
>>>I think I have closed to success. heehee~~~~~
>>>Thank you for your help.
>>>I still need you and others help. Thank you very much!
>>>
>>>
>>>----------
>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>>: 2005630 19:16
>>>: Ivy_Li
>>>: r-help at stat.math.ethz.ch
>>>: Re: : [R] fail in adding library in new version.
>>>
>>>
>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>>
>>>
>>>>Dear Gabor,
>>>>      Thank your for helping me so much!
>>>>      I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
>>>>1. unpack tools.zip into c:\cygwin
>>>>2. install Active perl in c:\Perl
>>>>3. install the mingw32 in c:\mingwin
>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
>>>
>>>
>>>If in the console you enter the command:
>>>
>>>path
>>>
>>>then it will display a semicolon separated list of folders.  You want the folder
>>>that contains the tools to be at the beginning so that you eliminate
>>>the possibility
>>>of finding a different program of the same name first in a folder that comes
>>>prior to the one where the tools are stored.
>>>
>>>
>>>
>>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
>>>>
>>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
>>>>cd \Program Files\R\rw2010
>>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
>>>
>>>
>>>I was assuming that MyRPackages and R are on the same disk.  If they are not
>>>then you need to specify the disk too.  That is if MyRPackages is on C and R
>>>is installed on D then install your package via:
>>>
>>>d:
>>>cd \Program Files\R\rw2010
>>>bin\R CMD install c:/MyRPackages/example
>>>
>>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
>>>using command script install and the indicated source package.
>>>
>>>
>>>
>>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
>>>
>>>
>>>If you are not sure where R is installed then enter the following at the Windows
>>>console prompt to find out (this will work provided you let it install the key
>>>into the registry when you installed R initially).  The reg command is a command
>>>built into Windows (I used XP but I assume its the same on other versions)
>>>that will query the Windows registry:
>>>
>>>reg query hklm\software\r-core\r /v InstallPath
>>>
>>>
>>>
>>>>I still need your and others help. Thank you very much!
>>>>
>>>>
>>>>
>>>>----------
>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>>>: 200566 10:21
>>>>: Ivy_Li
>>>>: r-help at stat.math.ethz.ch
>>>>: Re: [R] fail in adding library in new version.
>>>>
>>>>
>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>>>
>>>>
>>>>>Hello everybody,
>>>>>      Could I consult you a question?
>>>>>      I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
>>>>
>>>>Getting the latest version of R is strongly recommended.  The suggestions
>>>>below all assume the latest version and may or may not work if you do
>>>>not upgrade.
>>>>
>>>>
>>>>
>>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
>>>>>*       Download the tools.zip
>>>>>*       Unpack tools.zip into c:\cygwin
>>>>>*       Install Active Perl in c:\Perl
>>>>>*       Install the mingw32 port of gcc in c:\mingwin
>>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
>>>>
>>>>You may need to put these at the beginning of the path rather than the end.
>>>>Also just as a check enter
>>>>   path
>>>>at the console to make sure that you have them.  You will likely
>>>>have to start a new console session and possibly even reboot.
>>>>
>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
>>>>you reread the material on which tools you need.
>>>>
>>>>
>>>>
>>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
>>>>
>>>>In MyRPackages you would have a folder called example, in your case,
>>>>that contains the package.  Within folder example, you would have the
>>>>DESCRIPTION file, the R folder, etc.
>>>>
>>>>
>>>>
>>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
>>>>
>>>>You don't have to run R first.  You do need to make sure that R.exe can
>>>>be found on your path or else use the absolute path name in referring to R.
>>>>For example, if your path does not include R you could do something like this:
>>>>
>>>>cd \Program Files\R\rw2010
>>>>bin\R cmd install /MyRPackages/example
>>>
>>>
>>>Sorry, there is an error in the above.  It should be:
>>>
>>>bin\R CMD install c:/MyRPackages/example
>>>
>>>or
>>>
>>>bin\Rcmd install c:/MyRPackages/example
>>>
>>>
>>>
>>>
>>>>Be sure to use forward slashes where shown above and backslashes
>>>>where shown.
>>>>
>>>>
>>>>
>>>>>      So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
>>>>>
>>>>
>>>>Try all these suggestions including upgrading R and if that does not work
>>>>try posting screen dumps of the actual errors you are getting.
>>>>
>>>
>>>
>>>Also try googling for
>>>
>>>  making creating R packages
>>>
>>>and you will find some privately written tutorials on all this.
>>>
>>>
>>>
>>>------------------------------------------------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>



From ligges at statistik.uni-dortmund.de  Thu Jul  7 14:54:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 14:54:13 +0200
Subject: [R] Kernlab: problem with small datasets
In-Reply-To: <OF7D5FB08F.A3F1D312-ONC1257037.00444726-C1257037.0044472A@iasma.it>
References: <OF7D5FB08F.A3F1D312-ONC1257037.00444726-C1257037.0044472A@iasma.it>
Message-ID: <42CD25F5.9040307@statistik.uni-dortmund.de>

pablo.granitto at iasma.it wrote:

> Hi,
> 
> I found a small problem with kernlab. The problem, I think, is that the
> 3-fold cross-validation performed to estimate probabilities is not
> class-balanced, so the classifier could find empty classes.
> The following example (maybe a little forced) show the error:
> 
> data(glass)
> set.seed(1)
> model<-ksvm(Type~.,data=glass[c(1:2,75:76,165:166,180:181,190:191),],type="C-svc",kernel="vanilla",prob.model=TRUE)
> 
> Setting default kernel parameters
> Error in indexes[[j]] : subscript out of bounds
> In addition: Warning message:
> Variable(s) `' constant. Cannot scale data. in: .local(x, ...)


So time to report it to the package mainteiner (CCing), if you think it 
is an insufficiency or bug.

Uwe Ligges


> HTH,
>  Pablo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul  7 14:57:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 14:57:32 +0200
Subject: [R] Making Package, Chm error, Html Help Workshop
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAB3A@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD5027BAB3A@xchg1.statistik.local>
Message-ID: <42CD26BC.6070503@statistik.uni-dortmund.de>

TEMPL Matthias wrote:

> Hello,
> 
> When building my package (R CMD check) following error message occurs:
>  ...
>   varinf.plot  text     html     latex      example
>   x            text     html     latex      example
> make[2]: *** No rule to make target `disclosure.chm`. Stop.
> cp: cannot stat 'D:/Programme/R/rw2010dev/disclosure/chm/disclosure.chm`: No such file or directory
> make[1]: *** [chm-disclosure] Error 1
> make: *** [pkg-disclosure] Error 2
> *** Installation of disclosure failed ***
> Removing `D:/Programme/R/rw2010dev/bin/disclosure.Rcheck/disclosure'
>  ERROR
> Installation failed.
> 
> It seems, that there is a problem with HTML Workshop. No chm??s were built.
> When I uninstall the HTML Help Workshop and remove the entry in the path environmental variable and doing packaging after this, the same error occurs.
> I??m sure, that I have written the right entry (the path of the HTML help Workschop, where the hhc.exe file is) in the path of the environmental variable as the instructions said.
> 
> (Windows XP, Intel, R 2.1.0, HTML Workshop 1.32)
> 
> Has anybody seen such a problem before?
> Can anybody give me a hint, please?


hhc.exe is found, because it is not reported that hhc is not there.
I guess you have used an irregular name (from hhc's point of view) as 
the name or alias of a help topic, or you have an irregular name of an 
Rd file.
Have you tried with a recent and released version of R such as R-2.1.1? 
Your's is unreleased, obviously.
If it still won't work, please sent me the source package and I will 
take a look.

Uwe Ligges



> Thank you very much,
> Matthias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu Jul  7 14:56:50 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Jul 2005 08:56:50 -0400
Subject: =?GB2312?B?UmU6ILTwuLQ6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZA==?=
	=?GB2312?B?aW5nIGxpYnJhcnkgaW4gbmV3IHZlcnNpb24u?=
In-Reply-To: <42CD24C3.7070400@statistik.uni-dortmund.de>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF24@ex120.smic-sh.com>
	<42CD0C3B.1070407@statistik.uni-dortmund.de>
	<971536df05070705402579d8f5@mail.gmail.com>
	<42CD24C3.7070400@statistik.uni-dortmund.de>
Message-ID: <971536df050707055640c1f2d5@mail.gmail.com>

On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Gabor Grothendieck wrote:
> 
> > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >
> >>Ivy_Li wrote:
> >>
> >>
> >>>Dear all,
> >>>      I have done every step as the previous mail.
> >>>1. unpack tools.zip into c:\cygwin
> >>>2. install Active perl in c:\Perl
> >>>3. install the mingw32 in c:\mingwin
> >>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> >>
> >>                   ^
> >>such blanks are not allowed in the PATH variable
> >>
> >>
> >>
> >>
> >>>      Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> >>>       So in the Dos environment, at first, into the D:\>, I type the following code:
> >>>cd Program Files\R\rw2011\
> >>
> >>MyRpackages does not need to be here.
> >>
> >>
> >>>bin\R CMD install /MyRpackages/example
> >>
> >>The first slash in "/MyRPackages" sugests that this is a top level
> >>directory, which does not exist.
> >>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> >>and simply say:
> >>
> >>R CMD INSTALL example
> >
> >
> > Another possibility is to put Rcmd.bat from the batch file collection
> >
> >    http://cran.r-project.org/contrib/extra/batchfiles/
> >
> > in your path.  It will use the registry to find R so you won't have
> > to modify your path (nor would you have to remodify it every time you
> > install a new version of R which is what you would otherwise have to do):
> >
> > cd \MyPackages
> > Rcmd install example
> 
> 
> Just for the records:
> 
> 1. "cd \MyPackages" won't work, as I have already explained above.

If MyPackages is not a top level directory in the current drive
then it will not work. Otherwise it does work.

> 
> 2. I do *not* recommend this way, in particular I find it misleading to
> provide these batch files on CRAN.
> 

The alternative, at least as discussed in your post, is more work
since one will then have to change one's path every time one
reinstalls R.  This is just needless extra work and is error prone.  If you
forget to do it then you will be accessing the bin directory of the
wrong version of R.

> >>>      There are some error:
> >>>'make' is neither internal or external command, nor executable operation or batch file
> >>>*** installation of example failed ***
> >>
> >>Well, make.exe is not find in your path. Please check whether the file
> >>exists and the path has been added.
> >>
> >>Uwe Ligges
> >>
> >>
> >>
> >>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >>>
> >>>I think I have closed to success. heehee~~~~~
> >>>Thank you for your help.
> >>>I still need you and others help. Thank you very much!
> >>>
> >>>
> >>>----------
> >>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>: 2005630 19:16
> >>>: Ivy_Li
> >>>: r-help at stat.math.ethz.ch
> >>>: Re: : [R] fail in adding library in new version.
> >>>
> >>>
> >>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>
> >>>
> >>>>Dear Gabor,
> >>>>      Thank your for helping me so much!
> >>>>      I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> >>>>1. unpack tools.zip into c:\cygwin
> >>>>2. install Active perl in c:\Perl
> >>>>3. install the mingw32 in c:\mingwin
> >>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> >>>
> >>>
> >>>If in the console you enter the command:
> >>>
> >>>path
> >>>
> >>>then it will display a semicolon separated list of folders.  You want the folder
> >>>that contains the tools to be at the beginning so that you eliminate
> >>>the possibility
> >>>of finding a different program of the same name first in a folder that comes
> >>>prior to the one where the tools are stored.
> >>>
> >>>
> >>>
> >>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> >>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> >>>>
> >>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> >>>>cd \Program Files\R\rw2010
> >>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> >>>
> >>>
> >>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> >>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> >>>is installed on D then install your package via:
> >>>
> >>>d:
> >>>cd \Program Files\R\rw2010
> >>>bin\R CMD install c:/MyRPackages/example
> >>>
> >>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> >>>using command script install and the indicated source package.
> >>>
> >>>
> >>>
> >>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> >>>
> >>>
> >>>If you are not sure where R is installed then enter the following at the Windows
> >>>console prompt to find out (this will work provided you let it install the key
> >>>into the registry when you installed R initially).  The reg command is a command
> >>>built into Windows (I used XP but I assume its the same on other versions)
> >>>that will query the Windows registry:
> >>>
> >>>reg query hklm\software\r-core\r /v InstallPath
> >>>
> >>>
> >>>
> >>>>I still need your and others help. Thank you very much!
> >>>>
> >>>>
> >>>>
> >>>>----------
> >>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>>: 200566 10:21
> >>>>: Ivy_Li
> >>>>: r-help at stat.math.ethz.ch
> >>>>: Re: [R] fail in adding library in new version.
> >>>>
> >>>>
> >>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>>
> >>>>
> >>>>>Hello everybody,
> >>>>>      Could I consult you a question?
> >>>>>      I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> >>>>
> >>>>Getting the latest version of R is strongly recommended.  The suggestions
> >>>>below all assume the latest version and may or may not work if you do
> >>>>not upgrade.
> >>>>
> >>>>
> >>>>
> >>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>>>>*       Download the tools.zip
> >>>>>*       Unpack tools.zip into c:\cygwin
> >>>>>*       Install Active Perl in c:\Perl
> >>>>>*       Install the mingw32 port of gcc in c:\mingwin
> >>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> >>>>
> >>>>You may need to put these at the beginning of the path rather than the end.
> >>>>Also just as a check enter
> >>>>   path
> >>>>at the console to make sure that you have them.  You will likely
> >>>>have to start a new console session and possibly even reboot.
> >>>>
> >>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> >>>>you reread the material on which tools you need.
> >>>>
> >>>>
> >>>>
> >>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> >>>>
> >>>>In MyRPackages you would have a folder called example, in your case,
> >>>>that contains the package.  Within folder example, you would have the
> >>>>DESCRIPTION file, the R folder, etc.
> >>>>
> >>>>
> >>>>
> >>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> >>>>
> >>>>You don't have to run R first.  You do need to make sure that R.exe can
> >>>>be found on your path or else use the absolute path name in referring to R.
> >>>>For example, if your path does not include R you could do something like this:
> >>>>
> >>>>cd \Program Files\R\rw2010
> >>>>bin\R cmd install /MyRPackages/example
> >>>
> >>>
> >>>Sorry, there is an error in the above.  It should be:
> >>>
> >>>bin\R CMD install c:/MyRPackages/example
> >>>
> >>>or
> >>>
> >>>bin\Rcmd install c:/MyRPackages/example
> >>>
> >>>
> >>>
> >>>
> >>>>Be sure to use forward slashes where shown above and backslashes
> >>>>where shown.
> >>>>
> >>>>
> >>>>
> >>>>>      So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >>>>>
> >>>>
> >>>>Try all these suggestions including upgrading R and if that does not work
> >>>>try posting screen dumps of the actual errors you are getting.
> >>>>
> >>>
> >>>
> >>>Also try googling for
> >>>
> >>>  making creating R packages
> >>>
> >>>and you will find some privately written tutorials on all this.
> >>>
> >>>
> >>>
> >>>------------------------------------------------------------------------
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> 
>



From kurt.sys at telenet.be  Thu Jul  7 15:07:52 2005
From: kurt.sys at telenet.be (Kurt Sys)
Date: Thu, 07 Jul 2005 15:07:52 +0200
Subject: [R] tcltk package [SOLVED]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA6E@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA6E@usctmx1106.Merck.com>
Message-ID: <42CD2928.9080806@telenet.be>

I had R 2.0.1... It's not included in that distribution of R. It's ok in 
distribution 2.1.1.

thx (to all that've been replying that it's included in the base 
distribution),
Kurt.



Liaw, Andy wrote:

> It's included in the base R distribution, I believe.
> 
> Andy
> 
> 
>>From: Kurt Sys
>>
>>Hi all,
>>
>>I have a package depending on the tcltk-package. However, I see that 
>>this package has been disappeared... Is there a reason why package 
>>'tcltk' is not available anymore? Or is it replaced by another one?
>>
>>thx,
>>Kurt.



From danbebber at yahoo.co.uk  Thu Jul  7 15:10:29 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Thu, 7 Jul 2005 14:10:29 +0100 (BST)
Subject: [R] R crashes when spherical autocorrelation specified in nlme
Message-ID: <20050707131030.64630.qmail@web26310.mail.ukl.yahoo.com>

Dear list:
R crashes when I specify spatial autocorrelation in
nlme:

sp3 <- corSpher(c(30,0.75),~x+y|Site, nugget = T)
cs3 <- Initialize(sp3, data = sav)
sav.nlme1<-nlme(vc.asin ~ SSasymp(canopy, Asym, R0,
lrc),data = sav, fixed = Asym + R0 + lrc ~ 1, random =
R0 + lrc ~ 1|Site, start = list(fixed = sav.ini),
verbose = T, correlation = cs3)

There is a longish (~3 s) pause prior to the crash.
There is no crash if "correlation = cs3" is omitted.
The nugget effect is large (~0.75), so perhaps it is
not worth including, but I would still like to know if
the crash can be avoided.

My system is:
R 2.0.1
Windows XP SP2
Dual Intel Xeon 2.8GHz
2Gb RAM

Thanks,
Dan Bebber

Department of Plant Sciences
University of Oxford
UK



From junwen2u at gmail.com  Thu Jul  7 15:33:21 2005
From: junwen2u at gmail.com (John Wang)
Date: Thu, 7 Jul 2005 09:33:21 -0400
Subject: [R] Java and R help
Message-ID: <d2f1d531050707063362567f8e@mail.gmail.com>

>Im doing an aplication in Java and i have a program made in R what i
>want to launch with Java.
>
>I have the following instructions:
>
>Runtime r = Runtime.getRuntime();
>
>try
>{
>
>            System.out.println ("Llamada a R...");
>            p = r.exec(sRutaR);
>        }
>        catch (IOException e)
>        {
>            System.out.println ("Error lanzando R: " + e.getMessage());
>            e.printStackTrace();
>        }
>        catch (Exception ex)
>        {
>            System.out.println ("Error lanzando R!!!! " + ex.toString());
>            ex.printStackTrace();
>        }
>}
>
>and after that i wait for a file that R must to make called
>terminado.dat. But when i launch the process the file doesn't create
>until i destroy the process.
>
>can anyone explain what's happend with the process?
>
>Thx in advance and sorry for my poor english
>
The reason is that you did consume java output buffer, the process
hanged until you clear them. Try to use following StreamGobbler.java
to catch all the outputs:
//start of StreamGobbler.java
import java.util.*;
import java.io.*;

class StreamGobbler extends Thread
{
    InputStream is;
    String type;
    
    StreamGobbler(InputStream is, String type)
    {
        this.is = is;
        this.type = type;
    }
    
    public void run()
    {
        try
        {
            InputStreamReader isr = new InputStreamReader(is);
            BufferedReader br = new BufferedReader(isr);
            String line=null;
            while ( (line = br.readLine()) != null)
                System.out.println(type + ">" + line);    
            } catch (IOException ioe)
              {
                ioe.printStackTrace();  
              }
    }
}
//end of StreamGobbler 

then use this method to do system call:
    public boolean execWait(String comm){
        try{
            Process proc = Runtime.getRuntime().exec(comm);
            StreamGobbler errorGobbler = new
StreamGobbler(proc.getErrorStream(), "ERROR");
            
            // any output?
            StreamGobbler outputGobbler = new
StreamGobbler(proc.getInputStream(), "OUTPUT");
                
            // kick them off
            errorGobbler.start();
            outputGobbler.start();

            int returnVal = proc.waitFor();
            if (returnVal != 0) {
                return(false);
            }
        } catch (Exception e){
            e.printStackTrace();
            return false;
        }
        return true;
    }

Hope this helps,
Junwen



From luan_sheng at yahoo.com.cn  Thu Jul  7 15:46:29 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Thu, 7 Jul 2005 21:46:29 +0800
Subject: [R] What method I should to use for these data?
Message-ID: <200507071346.j67Dkp87014295@hypatia.math.ethz.ch>

Dear R user:

I am studying the allele data of two populations.
the following is the data:

	a1	a2	a3	a4	a5	a6	a7	a8	a9
a10	a11	a12	a13	a14	a15	a16	a17
pop1	0.0217 	0.0000 	0.0109 	0.0435 	0.0435 	0.0000 	0.0109 	0.0543
0.1739 	0.0761 	0.1413 	0.1522 	0.1087 	0.0870 	0.0435 	0.0217 	0.0109 
pop2	0.0213 	0.0213 	0.0000 	0.0000 	0.0000 	0.0426 	0.1702 	0.2128
0.1596 	0.1809 	0.0957 	0.0745 	0.0106 	0.0106 	0.0000 	0.0000 	0.0000 


a1,a2,a3 ...... a17 is the frequency of 17 alleles , the sum is 1. I want to
test  the significance of the distribution of 17 alleles between two
populations. How can I do? I want to use chisquare, is is right for these
data ?

can anyone  help me ? Thanks!!

luan
 Yellow Sea Fisheries Research Institute , Chinese Academy of Fishery
Sciences , Qingdao , 266071

__________________________________________________

G



From luan_sheng at yahoo.com.cn  Thu Jul  7 15:46:29 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Thu, 7 Jul 2005 21:46:29 +0800
Subject: [R] What method I should to use for these data?
Message-ID: <200507071346.j67DkpH2014294@hypatia.math.ethz.ch>

Dear R user:

I am studying the allele data of two populations.
the following is the data:

	a1	a2	a3	a4	a5	a6	a7	a8	a9
a10	a11	a12	a13	a14	a15	a16	a17
pop1	0.0217 	0.0000 	0.0109 	0.0435 	0.0435 	0.0000 	0.0109 	0.0543
0.1739 	0.0761 	0.1413 	0.1522 	0.1087 	0.0870 	0.0435 	0.0217 	0.0109 
pop2	0.0213 	0.0213 	0.0000 	0.0000 	0.0000 	0.0426 	0.1702 	0.2128
0.1596 	0.1809 	0.0957 	0.0745 	0.0106 	0.0106 	0.0000 	0.0000 	0.0000 


a1,a2,a3 ...... a17 is the frequency of 17 alleles , the sum is 1. I want to
test  the significance of the distribution of 17 alleles between two
populations. How can I do? I want to use chisquare, is is right for these
data ?

can anyone  help me ? Thanks!!

luan
 Yellow Sea Fisheries Research Institute , Chinese Academy of Fishery
Sciences , Qingdao , 266071

__________________________________________________

G



From ggrothendieck at gmail.com  Thu Jul  7 15:53:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Jul 2005 09:53:41 -0400
Subject: [R] plotting on a reverse log scale
In-Reply-To: <1120702309.3313.25.camel@localhost.localdomain>
References: <42CC32A3.9040605@yorku.ca> <42CC38E2.20605@stats.uwo.ca>
	<42CC42EC.4080101@yorku.ca>
	<971536df05070614311b163415@mail.gmail.com>
	<1120702309.3313.25.camel@localhost.localdomain>
Message-ID: <971536df0507070653155af0bb@mail.gmail.com>

There is an enhanced pretty function called nice in the Epi package
that also works with log data.  Not sure if this could lead to any 
simplifications in this problem?

On 7/6/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> I thought that I would take a stab at this. I should note however that
> my solution is heavily biased by the first log scale plot that Michael
> referenced below. To wit, my x axis has major ticks at powers of 10 and
> minor ticks at 1/10 of each major tick interval.
> 
> Thus, here is my approach:
> 
> # Set the max range to an integer power of 10 as may be required
> max.pow <- 5
> 
> # Set the data as others have done below
> set.seed(1)
> x <- runif(500, 1, 10^max.pow)
> d <- density(x, from = 1, to = 10^max.pow)
> 
> # Now do the density plot, leaving the x axis blank
> # Note that by doing a rev() on 'xlim' it reverses the min/max on the
> # x axis, so you don't have to worry about adjusting things.
> plot(d$y ~ d$x, xlim = rev(c(1, 10^max.pow)), log = "x",
>     xaxt = "n", type = "l",
>     xlab = "2000 - Year (log scale)")
> 
> # Set the axis major tick marks
> axis.at <- 10 ^ c(0:max.pow)
> 
> # Draw the major tick marks and label them using plotmath
> axis(1, at = axis.at, tcl = -1,
>     labels = parse(text = paste("10^", 0:max.pow, sep = "")))
> 
> # Now do the minor ticks, at 1/10 of each power of 10 interval
> axis(1, at = 1:10 * rep(axis.at[-1] / 10, each = 10),
>     tcl = -0.5, labels = FALSE)
> 
> # Do the rug plot
> rug(x)
> 
> 
> Not sure if this is helpful here, but thought I would post it for
> review/critique. The 'max.pow' constant can be explicitly adjusted or
> can be calculated automatically based upon input year ranges.
> 
> Best regards,
> 
> Marc Schwartz
> 
> 
> On Wed, 2005-07-06 at 17:31 -0400, Gabor Grothendieck wrote:
> > Not sure if I am missing something essential here but it would
> > seem as simple as:
> >
> > # data
> > set.seed(1)
> > x <- runif(500, 1500, 1990)
> >
> > # plot
> > d <- density(x, from = 1500, to = 1990)
> > plot(d$y ~ d$x, log = "x")
> > rug(x)
> > axis(1, seq(1500, 1990, 10), FALSE, tcl = -0.3)
> >
> >
> > On 7/6/05, Michael Friendly <friendly at yorku.ca> wrote:
> > > Thanks Duncan,
> > > That is almost exactly what I want, except I want time to
> > > go in the normal order, not backwards, so:
> > >
> > > # plot on reverse log scale
> > > years1500 <- runif(500, 1500, 1990)  # some fake data
> > > x <- -log(2000-years1500)
> > > from <- -log(2000-1990)
> > > to <- -log(2000-1500)
> > > plot(density(x, from=from, to=to), axes=F)
> > > rug(x)
> > >
> > > labels <- pretty(years1500)
> > > labels <- labels[labels<2000]
> > > axis(1, labels, at=-log(2000-labels))
> > >
> > > minorticks <- pretty(years1500, n=20)
> > > minorticks <- minorticks[minorticks<2000]
> > > axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> > >
> > > axis(2)
> > > box()
> > >
> > > -Michael
> > >
> > > Duncan Murdoch wrote:
> > >
> > > > On 7/6/2005 3:36 PM, Michael Friendly wrote:
> > > >
> > > >> I'd like to do some plots of historical event data on a reverse log
> > > >> scale, started, say at the year 2000 and going
> > > >> backwards in time, with tick marks spaced according to
> > > >> log(2000-year).  For example, see:
> > > >>
> > > >> http://euclid.psych.yorku.ca/SCS/Gallery/images/log-timeline.gif
> > > >>
> > > >> As an example, I'd like to create a density plot of such data with the
> > > >> horizontal axis reverse-logged,
> > > >> a transformation of this image:
> > > >> http://euclid.psych.yorku.ca/SCS/Gallery/milestone/Test/mileyears1.gif
> > > >>
> > > >> Some initial code to do a standard density plot looks like this:
> > > >>
> > > >> mileyears <- read.csv("mileyears3.csv", skip=1,
> > > >> col.names=c("key","year","where","add","junk"))
> > > >> mileyears <- mileyears[,2:4]
> > > >>
> > > >> years <- mileyears$year
> > > >> years1500 <- years[years>1500]
> > > >> dens <- density(years1500, from=1500, to=1990)
> > > >> plot(dens)
> > > >> rug(years1500)
> > > >>
> > > >> I could calculate log(2000-year), but I'm not sure how to do the
> > > >> plotting, do some minor tick marks
> > > >> and label the major ones, say at 100 year intervals.
> > > >
> > > >
> > > > I think you'll have to do everything explicitly.  That is, something
> > > > like this:
> > > >
> > > > years1500 <- runif(500, 1500, 1990)  # some fake data
> > > > x <- log(2000-years1500)
> > > > from <- log(2000-1990)
> > > > to <- log(2000-1500)
> > > > plot(density(x, from=from, to=to), axes=F)
> > > > rug(x)
> > > >
> > > > labels <- pretty(years1500)
> > > > labels <- labels[labels<2000]
> > > > axis(1, labels, at=log(2000-labels))
> > > >
> > > > minorticks <- pretty(years1500, n=20)
> > > > minorticks <- minorticks[minorticks<2000]
> > > > axis(1, labels=FALSE, at=log(2000-minorticks), tcl=-0.25)
> > > >
> > > > axis(2)
> > > > box()
> > > >
> 
>



From gb at stat.umu.se  Thu Jul  7 15:56:04 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 7 Jul 2005 15:56:04 +0200
Subject: [R] Lack of independence in anova()
In-Reply-To: <XFMail.050707111809.Ted.Harding@nessie.mcc.ac.uk>
References: <42CC8DAA.8070402@pdf.com>
	<XFMail.050707111809.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050707135604.GA16992@stat.umu.se>

On Thu, Jul 07, 2005 at 11:18:09AM +0100, Ted Harding wrote:
> My first reaction to Duncan's example was "Touch?? -- with apologies
> to G??ran for suspecting on over-trivial example"! 

No need to apologize; that was of course my first reaction to Thomas'
statement. 

> I had not thought
> long enough about possible cases. Duncan is right; and maybe it is
> the same example as G??ran was thinking of.

On second thought it was not difficult to find: (X, Y) bivariate standard
normal, P(Z = 1) = P(Z = -1) = 1/2.

[...]
 
> However, interesting though it maybe, this is a side-issue
> to the original question concerning independence of the F-ratios
> in an ANOVA. Here, numerators and denominator are all positive,
> so examples like the above are not relevant.
> 
> The original argument (that increasing Z diminishes both X/Z
> and Y/Z simultaneously) applies; but it is also possible to
> demonstrate analytically that P(X/Z <= v and Y/Z <= w) is
> greater than P(X/Z <= v)*P(Y/Z <= w).

Maybe it is simplest to calculate Cov(X/Z, Y/Z), which turns out to be
equal to E(X)E(Y)V(1/Z) (given total independence). So, a necessary
condition for independence is that at least one of these three terms is
zero. Which is impossible in the F-ratios case.

G??ran



From luan_sheng at yahoo.com.cn  Thu Jul  7 15:55:44 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Thu, 7 Jul 2005 21:55:44 +0800
Subject: [R] What method I should to use for these data?
Message-ID: <200507071356.j67DuXZs017473@hypatia.math.ethz.ch>

 

-----Original Message-----
From: luan_sheng [mailto:luan_sheng at yahoo.com.cn] 
Sent: Thursday, July 07, 2005 9:46 PM
To: (r-help at stat.math.ethz.ch)
Cc: (R-help at lists.R-project.org)
Subject: What method I should to use for these data?

Dear R user:

I am studying the allele data of two populations.
the following is the data:

	a1	a2	a3	a4	a5	a6	a7	a8	a9
a10	a11	a12	a13	a14	a15	a16	a17
pop1	0.0217 	0.0000 	0.0109 	0.0435 	0.0435 	0.0000 	0.0109 	0.0543
0.1739 	0.0761 	0.1413 	0.1522 	0.1087 	0.0870 	0.0435 	0.0217 	0.0109 
pop2	0.0213 	0.0213 	0.0000 	0.0000 	0.0000 	0.0426 	0.1702 	0.2128
0.1596 	0.1809 	0.0957 	0.0745 	0.0106 	0.0106 	0.0000 	0.0000 	0.0000 


a1,a2,a3 ...... a17 is the frequency of 17 alleles , the sum is 1. I want to
test  the significance of the distribution of 17 alleles between two
populations. How can I do? I want to use chisquare, is is right for these
data ?

can anyone  help me ? Thanks!!

luan
 Yellow Sea Fisheries Research Institute , Chinese Academy of Fishery
Sciences , Qingdao , 266071

__________________________________________________

G



From renaud.lancelot at cirad.fr  Thu Jul  7 16:27:47 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 07 Jul 2005 16:27:47 +0200
Subject: [R] About ade4 and overlaying points
In-Reply-To: <42CD182D.9040805@cirad.fr>
References: <42CD182D.9040805@cirad.fr>
Message-ID: <42CD3BE3.8050004@cirad.fr>

Jacques VESLOT a ??crit :
> Dear R-users,
> 
> Is there an easy way to avoid points one upon another when ploting rows 
> and columns of 'dudi' objects ? Maybe there is a function in ade4 or in 
> an other package, or maybe someone has his or her own function to do 
> this (for example to automatically modify a little the coordinates of 
> these points to get a readable plot ?).
> 
> Thanks in advance.
> Best regards,
> 
> Jacques VESLOT
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
Hello Jacques,

Have a look at ?jitter

Best,

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From navarre_sabine at yahoo.fr  Thu Jul  7 16:43:00 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Thu, 7 Jul 2005 16:43:00 +0200 (CEST)
Subject: [R] function par(mfrow....)
Message-ID: <20050707144300.64149.qmail@web26604.mail.ukl.yahoo.com>

Hi,
 
I have made  3 barplots differents in the some window plot with the function par(mfrow....),
but is it possible to give different dimension to this 3 parts.
for example, I want the first part smaller than the others.

I have attached my plot!

thanks
 
Sabine


		
---------------------------------


From HDoran at air.org  Thu Jul  7 16:46:06 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 7 Jul 2005 10:46:06 -0400
Subject: [R] Tempfile error
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409748C08@dc1ex2.air.org>

Dear Uwe and Sean:

Thank you for your reply. I think I did a poor job framing my problem.
Here are a few things I should have added. First, I previously stated
that the program would halt in random places. This is *not* true. It
always halts at the same row, which happens to be row 1038. There isn't
a logical reason for stopping in this row as far as I can tell, all data
needed to generate the file are present.

I am using R version 2.10 for Windows on an XP machine with 2 gb ram. My
tex distribution is MikTex (I'm not sure if that matters).

Traceback gives the following information

> traceback()
9: file()
8: driver$runcode(drobj, chunk, chunkopts)
7: Sweave("fam_template.Rnw", output = tmp2)
6: eval.with.vis(expr, envir, enclos)
5: eval.with.vis(ei, envir)
4: source("run_fam.r")
3: eval.with.vis(expr, envir, enclos)
2: eval.with.vis(ei, envir)
1: source(file.choose())

I did tempfile() to identify the location of the files and have cleaned
that out repeatedly. I've searched the archives and have seen some
discussion regarding and on.exit() command, but I'm not sure I see how
it would be utilized here. 

Here is the code for the looping file that subsets the dataframe. I
wonder if I need to add something here that would close a connection or
generate a random tempfile name to avoid this problem in the loop.

list <- unique(wide$stuid)
master = "master.tex"
for (i in list){
     tmp1 <- subset(wide, stuid==i)
     tmp2 <- paste(i, "tex", sep=".")
     Sweave("fam_template.Rnw", output=tmp2)
     file.append("fam_master.tex", tmp2)     
}

Thanks for your time on this,
Harold

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Thursday, July 07, 2005 4:52 AM
To: Doran, Harold
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Tempfile error

Doran, Harold wrote:

> Dear List:
> 
> I am encountering an error that I can't resolve. I'm looping through 
> rows of a dataframe to generate individual tex files using Sweave. At 
> random points along the way, I encounter the following error
> 
>  Error in file() : cannot find unused tempfile name


Which version of R is this?
I think during one of the latest releases tempfile() name generation has
been imporved, because R did not tried hard enough to find new (random)
filenames for tempfiles in older releases of R.

Uwe Ligges


> At which point Sweave halts. There isn't a logical pattern that I can 
> identify in terms of why the program stops at certain points. It just 
> seems to be random as far as I can tell. I've searched the archives 
> and of course Sweave FAQs but haven't found much that sheds light on 
> what this error indicates and what I should do to resolve it.
> 
> There are approximately 20,000 rows, meaning that about 20,000 tex 
> files are created. If I sample 5,000 or even 10,000 and run the 
> program, I do not encounter an error. It only occurs when I run the 
> program on the full dataframe and even then the error is not occuring 
> at the same point. That is, the row at which the program halts varies
each time.
> 
> Does anyone have any thoughts on this problem?
> 
> -Harold
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Thu Jul  7 16:55:28 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 07 Jul 2005 10:55:28 -0400
Subject: [R] function par(mfrow....)
In-Reply-To: <20050707144300.64149.qmail@web26604.mail.ukl.yahoo.com>
References: <20050707144300.64149.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42CD4260.3020202@optonline.net>

Navarre Sabine wrote:
> Hi,
>  
> I have made  3 barplots differents in the some window plot with the function par(mfrow....),
> but is it possible to give different dimension to this 3 parts.
> for example, I want the first part smaller than the others.
> ... 

Here is one approach using split.screen() :

split.screen(fig = rbind(c(0, 0.25, 0.25, 0.75),
                          c(0.25, 0.625, 0, 1), c(0.625, 1, 0, 1)))

screen(1)
barplot(runif(5), main="Smaller")

screen(2)
barplot(runif(8), main="Bigger")

screen(3)
barplot(runif(8), main="Bigger")

close.screen(all=T)

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From macq at llnl.gov  Thu Jul  7 16:58:46 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 7 Jul 2005 07:58:46 -0700
Subject: [R] timezone problems
In-Reply-To: <42CCFC96.7080304@statistik.uni-dortmund.de>
References: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>
	<p06210203bef19c99bfe2@[128.115.153.6]>
	<op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>
	<42CCFC96.7080304@statistik.uni-dortmund.de>
Message-ID: <p06210201bef2ef0111b1@[128.115.153.6]>

?POSIXt says:

      '"POSIXlt"' objects will often have an attribute '"tzone"', a
      character vector of length 3 giving the timezone name from the
      'TZ' environment variable and the names of the base timezone and
      the alternate (daylight-saving) timezone.  Sometimes this may just
      be of length one, giving the timezone name.

Therefore, any function that relies on either
    (1) the presence of a "tzone" attribute, or
    (2) when it is present, it necessarily has a length 3
is relying on something that is documented not to be reliable.

Sys.timezone(), as defined on Martin's system (see his email below) 
does both of these, so that would appear to be the problem.


It is not hard to verify that the tzone attribute is not always 
present, and when present, may be of length one or three.

>  Sys.putenv(TZ='US/Pacific')
>  z <- as.POSIXlt(Sys.time())
>  attributes(z)$tzone
[1] "US/Pacific" "PST"        "PDT"      

>  Sys.putenv(TZ='GMT')
>  z <- as.POSIXlt(Sys.time())
>  attributes(z)$tzone
[1] "GMT"

>  Sys.putenv(TZ='US/Eastern')
>  z <- as.POSIXlt(Sys.time())
>  attributes(z)$tzone
[1] "US/Eastern" "EST"        "EDT"      

>  z <- as.POSIXlt(c('2005-1-1','2005-6-1'))
>  names(attributes(z))
[1] "names" "class"


-Don

At 11:57 AM +0200 7/7/05, Uwe Ligges wrote:
>Martin Keller-Ressel wrote:
>
>>  Thank you Don for your hints. I have checked my environment vairable TZ 
>>  again. But everything is set correctly. I think the problem is with 
>>  Sys.timezone(). Maybe it is a conflict between how my system formats the 
>>  time/date and what Sys.timezone() expects.
>>  This is what I get on my system:
>>
>>
>>>Sys.getenv("TZ")
>>
>>      TZ
>>  "GMT"
>>
>>>Sys.time()
>>
>>  [1] "2005-07-07 07:32:39 GMT"
>>
>>  ## everything fine so far
>>
>>
>>>Sys.timezone()
>>
>>  [1] NA
>>
>>  ## This is what Sys.timezone looks like:
>>
>>>Sys.timezone
>>
>>  function ()
>>  {
>>       z <- as.POSIXlt(Sys.time())
>>       attr(z, "tzone")[2 + z$isdst]
>>  }
>>  <environment: namespace:base>
>>
>>>z <- as.POSIXlt(Sys.time())
>>>attributes(z)
>>
>>  $names
>>  [1] "sec"   "min"   "hour"  "mday"  "mon"   "year"  "wday"  "yday"  "isdst"
>>
>>  $class
>>  [1] "POSIXt"  "POSIXlt"
>>
>>  $tzone
>>  [1] "GMT"
>>
>>
>>>attr(z,"tzone")
>>
>>  [1] "GMT"
>>
>>>z$isdst
>>
>>  [1] 0
>>
>>>attr(z,"tzone")[2]
>>
>>  [1] NA
>>
>>  I dont understand why Sys.timezone doesn't use attr(z,"tzone") but tries 
>>  to read its (2+z$isdst)-th element.
>>  Of course it would be easy to write a workaround, but I wonder why nobody 
>>  else is having this problem.
>
>
>I can confirm for R-2.1.1 under Windows NT 4.0 and it looks like a bug
>(somewhere down the way from as.POSIXlt). Don't have the time to look at
>it more closely, perhaps Brian knows it at once?
>If this is not already in the bug repository (please check at first),
>can you submit a report, please? Thanks!
>
>Uwe Ligges
>
>
>
>
>
>>  best regards,
>>
>>  Martin Keller-Ressel
>>
>>
>>
>>  On Wed, 06 Jul 2005 14:45:25 -0000, Don MacQueen <macq at llnl.gov> wrote:
>>
>>
>>>How did you set the TZ system variable?
>>>If you did not use Sys.putenv(), try using it instead.
>>>Otherwise, I think you have to ask the package maintainer.
>>>
>>>You may be misleading yourself by using Sys.time() to test whether TZ is 
>>>set.
>>>What does Sys.getenv() tell you?
>>>
>>>I get a timezone code from Sys.time() even when TZ is not defined (see 
>>>example below).
>  >>(but I do have a different OS)
>  >>
>  >>
>  >>> Sys.timezone()
>  >>
>  >>[1] ""
>  >>
>  >>> Sys.time()
>  >>
>  >>[1] "2005-07-06 07:34:15 PDT"
>  >>
>  >>> Sys.getenv('TZ')
>  >>
>  >>TZ
>  >>""
>  >>
>  >>> Sys.putenv(TZ='US/Pacific')
>>>>  Sys.timezone()
>>>
>>>[1] "US/Pacific"
>>>
>>>>  Sys.getenv('TZ')
>>>
>>>            TZ
>>>"US/Pacific"
>>>
>>>>  Sys.time()
>>>
>>>[1] "2005-07-06 07:34:38 PDT"
>>>
>>>
>>>>  Sys.putenv(TZ='GMT')
>>>>  Sys.time()
>>>
>>>[1] "2005-07-06 14:35:45 GMT"
>>>
>>>
>>>>  version
>>>
>>>           _                       platform powerpc-apple-darwin7.9.0
>>>arch     powerpc                 os       darwin7.9.0             
>  >>system   powerpc, darwin7.9.0    status                           
>>>major    2                       minor    1.1                     
>>>year     2005                    month    06                      
>>>day      20                      language R                       
>>>At 9:55 AM +0000 7/5/05, Martin Keller-Ressel wrote:
>>>
>>>>Hi,
>>>>
>>>>Im using R 2.1.1 and running Code that previously worked (on R 2.1.0 I 
>>>>believe) using the 'timeDate' function from the fCalendar package. The 
>>>>code now throws an error:
>>>>
>>>>Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")
>>>>
>>>>However I have read the documentation of the fCalendar package and I 
>>>>have set my system variable TZ to GMT.
>>>>I tracked the error down to the function Sys.timezone() which returns 
>>>>NA in spite of what Sys.time() returns.
>>>>
>>>>
>>>>>  Sys.timezone()
>>>>
>>>>[1] NA
>>>>
>>>>
>>>>>  Sys.time()
>>>>
>>>>[1] "2005-07-05 08:41:53 GMT"
>>>>
>>>>My version:
>>>>
>>>>
>>>>>  version
>>>>
>>>>           _
>>>>platform i386-pc-mingw32
>>>>arch     i386
>>>>os       mingw32
>>>>system   i386, mingw32
>>>>status
>>>>major    2
>>>>minor    1.1
>>>>year     2005
>>>>month    06
>>>>day      20
>>>>language R
>>>>
>>>>Any help is appreciated,
>>>>
>>>>Martin Keller-Ressel
>>>>
>>>>
>>>>---
>>>>Martin Keller-Ressel
>>>>Research Unit of Financial and Actuarial Mathematics
>>>>TU Vienna
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ghislainv at gmail.com  Thu Jul  7 17:02:19 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Thu, 7 Jul 2005 17:02:19 +0200
Subject: [R] Fwd:  Plotting confidence intervals for lme
In-Reply-To: <ff51f02205070702382feef88b@mail.gmail.com>
References: <ff51f022050706070971afca67@mail.gmail.com>
	<42CC0A6D.2040604@pdf.com> <ff51f02205070702382feef88b@mail.gmail.com>
Message-ID: <ff51f02205070708022ffe19ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050707/3f82ef19/attachment.pl

From Matthias.Templ at statistik.gv.at  Thu Jul  7 17:02:33 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 7 Jul 2005 17:02:33 +0200
Subject: [R] function par(mfrow....)
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAB3D@xchg1.statistik.local>

Hi,

Is layout the function you will need?
?layout

E.g.:

l <- matrix(c(rep(1,4), rep(2,8),rep(3,8)), ncol=5)
layout(l)
layout.show(3)
plot(1,1)
plot(1:25,pch=1:25)
plot(2,5)

Best regards,
Matthias

> 
> Hi,
>  
> I have made  3 barplots differents in the some window plot 
> with the function par(mfrow....), but is it possible to give 
> different dimension to this 3 parts. for example, I want the 
> first part smaller than the others.
> 
> I have attached my plot!
> 
> thanks
>  
> Sabine
> 
> 
> 		
> ---------------------------------
> 
>



From buser at stat.math.ethz.ch  Thu Jul  7 17:04:15 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 7 Jul 2005 17:04:15 +0200
Subject: [R] How to get the minimum ?
In-Reply-To: <1120729035.42ccf7cb737f5@webmail.daimi.au.dk>
References: <1120729035.42ccf7cb737f5@webmail.daimi.au.dk>
Message-ID: <17101.17519.43685.763480@stat.math.ethz.ch>

Dear Philipe

You can use optimize (see ?optimize), e.g. :

funToMin <- function(x, data, a = 1, b = 1) {
  sum((data[data[,"group"]=="A","y"] - x)^2) +
    sum((data[data[,"group"]=="B","y"] - a*x - b)^2)
}

dat <- data.frame(y = rnorm(100), group = rep(c("A","B"), each = 50))
(m <- optimize(function(x) funToMin(x,dat), interval = c(-10,10)))

Please be careful. This function is only for demonstration
issue. It is bad programmed. It works if x is only 1 number,
but if you call the function, using a vector instead of a single
number (and I do not prevent this by checking it), you will get
warnings or errors. Therefore it will be better to use your own,
hopefully better programmed function in optimize.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Philippe Lamy writes:
 > Hi,
 > 
 > I have a model with differents observations Xi.
 > Each observation belongs to a group, either A or B.
 > I would like to minimize a fonction like :
 > 
 > sum( Xi - Z)^2 + sum (Xi - aZ -b)^2
 >  A                B
 > 
 > The first sum contains all observations from group A and the second all
 > observations from group B.
 > I want to find the Z-value wich minimize this function. a and b are predefined
 > parameters.
 > 
 > Thanks for help.
 > 
 > Philippe
 > 
 > ______________________________________________
 >



From mkeller at fam.tuwien.ac.at  Thu Jul  7 18:44:20 2005
From: mkeller at fam.tuwien.ac.at (Martin Keller-Ressel)
Date: Thu, 07 Jul 2005 16:44:20 -0000
Subject: [R] timezone problems
In-Reply-To: <p06210201bef2ef0111b1@[128.115.153.6]>
References: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>
	<p06210203bef19c99bfe2@[128.115.153.6]>
	<op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>
	<42CCFC96.7080304@statistik.uni-dortmund.de>
	<p06210201bef2ef0111b1@[128.115.153.6]>
Message-ID: <op.stjx36kn0efqu7@neyman.fam.tuwien.ac.at>

I cannot reproduce Don's results below on my system. However I do not know  
if timezone names can be expected to be compatible across systems/country  
settings/etc...

I get:
> Sys.putenv(TZ='US/Pacific')
>  z <- as.POSIXlt(Sys.time())
>  attributes(z)$tzone
[1] "\001S/Pacific" "US/"           "Pac"

which looks very ugly. I also tried

> Sys.putenv(TZ='MESZ')
>  z <- as.POSIXlt(Sys.time())
>  attributes(z)$tzone
[1] "\001ESZ" "MES"     "Z"

which is also probably not intended.
I filed a bug report a few hours ago (#7994) but only pointed out the  
problems regarding timezone 'GMT' discussed in the last mail, not the  
'new' problems associated with setting another timezone.

Martin Keller-Ressel


On Thu, 07 Jul 2005 14:58:46 -0000, Don MacQueen <macq at llnl.gov> wrote:

> ?POSIXt says:
>
>       '"POSIXlt"' objects will often have an attribute '"tzone"', a
>       character vector of length 3 giving the timezone name from the
>       'TZ' environment variable and the names of the base timezone and
>       the alternate (daylight-saving) timezone.  Sometimes this may just
>       be of length one, giving the timezone name.
>
> Therefore, any function that relies on either
>     (1) the presence of a "tzone" attribute, or
>     (2) when it is present, it necessarily has a length 3
> is relying on something that is documented not to be reliable.
>
> Sys.timezone(), as defined on Martin's system (see his email below) does  
> both of these, so that would appear to be the problem.
>
>
> It is not hard to verify that the tzone attribute is not always present,  
> and when present, may be of length one or three.
>
>>  Sys.putenv(TZ='US/Pacific')
>>  z <- as.POSIXlt(Sys.time())
>>  attributes(z)$tzone
> [1] "US/Pacific" "PST"        "PDT"
>>  Sys.putenv(TZ='GMT')
>>  z <- as.POSIXlt(Sys.time())
>>  attributes(z)$tzone
> [1] "GMT"
>
>>  Sys.putenv(TZ='US/Eastern')
>>  z <- as.POSIXlt(Sys.time())
>>  attributes(z)$tzone
> [1] "US/Eastern" "EST"        "EDT"
>>  z <- as.POSIXlt(c('2005-1-1','2005-6-1'))
>>  names(attributes(z))
> [1] "names" "class"
>
>
> -Don
>
> At 11:57 AM +0200 7/7/05, Uwe Ligges wrote:
>> Martin Keller-Ressel wrote:
>>
>>>  Thank you Don for your hints. I have checked my environment vairable  
>>> TZ  again. But everything is set correctly. I think the problem is  
>>> with  Sys.timezone(). Maybe it is a conflict between how my system  
>>> formats the  time/date and what Sys.timezone() expects.
>>>  This is what I get on my system:
>>>
>>>
>>>> Sys.getenv("TZ")
>>>
>>>      TZ
>>>  "GMT"
>>>
>>>> Sys.time()
>>>
>>>  [1] "2005-07-07 07:32:39 GMT"
>>>
>>>  ## everything fine so far
>>>
>>>
>>>> Sys.timezone()
>>>
>>>  [1] NA
>>>
>>>  ## This is what Sys.timezone looks like:
>>>
>>>> Sys.timezone
>>>
>>>  function ()
>>>  {
>>>       z <- as.POSIXlt(Sys.time())
>>>       attr(z, "tzone")[2 + z$isdst]
>>>  }
>>>  <environment: namespace:base>
>>>
>>>> z <- as.POSIXlt(Sys.time())
>>>> attributes(z)
>>>
>>>  $names
>>>  [1] "sec"   "min"   "hour"  "mday"  "mon"   "year"  "wday"  "yday"   
>>> "isdst"
>>>
>>>  $class
>>>  [1] "POSIXt"  "POSIXlt"
>>>
>>>  $tzone
>>>  [1] "GMT"
>>>
>>>
>>>> attr(z,"tzone")
>>>
>>>  [1] "GMT"
>>>
>>>> z$isdst
>>>
>>>  [1] 0
>>>
>>>> attr(z,"tzone")[2]
>>>
>>>  [1] NA
>>>
>>>  I dont understand why Sys.timezone doesn't use attr(z,"tzone") but  
>>> tries  to read its (2+z$isdst)-th element.
>>>  Of course it would be easy to write a workaround, but I wonder why  
>>> nobody  else is having this problem.
>>
>>
>> I can confirm for R-2.1.1 under Windows NT 4.0 and it looks like a bug
>> (somewhere down the way from as.POSIXlt). Don't have the time to look at
>> it more closely, perhaps Brian knows it at once?
>> If this is not already in the bug repository (please check at first),
>> can you submit a report, please? Thanks!
>>
>> Uwe Ligges
>>
>>
>>
>>
>>
>>>  best regards,
>>>
>>>  Martin Keller-Ressel
>>>
>>>
>>>
>>>  On Wed, 06 Jul 2005 14:45:25 -0000, Don MacQueen <macq at llnl.gov>  
>>> wrote:
>>>
>>>
>>>> How did you set the TZ system variable?
>>>> If you did not use Sys.putenv(), try using it instead.
>>>> Otherwise, I think you have to ask the package maintainer.
>>>>
>>>> You may be misleading yourself by using Sys.time() to test whether TZ  
>>>> is set.
>>>> What does Sys.getenv() tell you?
>>>>
>>>> I get a timezone code from Sys.time() even when TZ is not defined  
>>>> (see example below).
>>  >>(but I do have a different OS)
>>  >>
>>  >>
>>  >>> Sys.timezone()
>>  >>
>>  >>[1] ""
>>  >>
>>  >>> Sys.time()
>>  >>
>>  >>[1] "2005-07-06 07:34:15 PDT"
>>  >>
>>  >>> Sys.getenv('TZ')
>>  >>
>>  >>TZ
>>  >>""
>>  >>
>>  >>> Sys.putenv(TZ='US/Pacific')
>>>>>  Sys.timezone()
>>>>
>>>> [1] "US/Pacific"
>>>>
>>>>>  Sys.getenv('TZ')
>>>>
>>>>            TZ
>>>> "US/Pacific"
>>>>
>>>>>  Sys.time()
>>>>
>>>> [1] "2005-07-06 07:34:38 PDT"
>>>>
>>>>
>>>>>  Sys.putenv(TZ='GMT')
>>>>>  Sys.time()
>>>>
>>>> [1] "2005-07-06 14:35:45 GMT"
>>>>
>>>>
>>>>>  version
>>>>
>>>>           _                       platform powerpc-apple-darwin7.9.0
>>>> arch     powerpc                 os       darwin7.9.0
>>  >>system   powerpc, darwin7.9.0    status
>>>> major    2                       minor    1.1                      
>>>> year     2005                    month    06                       
>>>> day      20                      language R                       At  
>>>> 9:55 AM +0000 7/5/05, Martin Keller-Ressel wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> Im using R 2.1.1 and running Code that previously worked (on R 2.1.0  
>>>>> I believe) using the 'timeDate' function from the fCalendar package.  
>>>>> The code now throws an error:
>>>>>
>>>>> Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")
>>>>>
>>>>> However I have read the documentation of the fCalendar package and I  
>>>>> have set my system variable TZ to GMT.
>>>>> I tracked the error down to the function Sys.timezone() which  
>>>>> returns NA in spite of what Sys.time() returns.
>>>>>
>>>>>
>>>>>>  Sys.timezone()
>>>>>
>>>>> [1] NA
>>>>>
>>>>>
>>>>>>  Sys.time()
>>>>>
>>>>> [1] "2005-07-05 08:41:53 GMT"
>>>>>
>>>>> My version:
>>>>>
>>>>>
>>>>>>  version
>>>>>
>>>>>           _
>>>>> platform i386-pc-mingw32
>>>>> arch     i386
>>>>> os       mingw32
>>>>> system   i386, mingw32
>>>>> status
>>>>> major    2
>>>>> minor    1.1
>>>>> year     2005
>>>>> month    06
>>>>> day      20
>>>>> language R
>>>>>
>>>>> Any help is appreciated,
>>>>>
>>>>> Martin Keller-Ressel
>>>>>
>>>>>
>>>>> ---
>>>>> Martin Keller-Ressel
>>>>> Research Unit of Financial and Actuarial Mathematics
>>>>> TU Vienna
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!  
>>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>
>



-- 
Martin Keller-Ressel
Research Unit of Financial and Actuarial Mathematics
TU Vienna



From ohalawa at umich.edu  Thu Jul  7 17:47:24 2005
From: ohalawa at umich.edu (ohalawa@umich.edu)
Date: Thu, 07 Jul 2005 11:47:24 -0400
Subject: [R] Plotting Character Variable
Message-ID: <20050707114724.f7nf0hb6okk40wco@web.mail.umich.edu>

Any ideas about the following problem:

I have a matrix (A) that looks like this:

gene_names             values
hsa-mir-124              0.3
hsa-mir-234              0.1
hsa-mir-344              0.4
hsa-mir-333              0.7
.....                  .......

(This is a 2 by 22283 matrix: quite large)

I would like to plot the values, but output the gene_names as the plotting
symbol. I have tried regular x,y plots, but since the gene_names are quite
large and there are 22283 of them, it's impossible to fit them on the x-axis.

Basically, can I plot the above matrix

plot(gene_names, value) where the gene_names are used as the plotting symbol.

thank you,



From palvarez7777 at yahoo.es  Thu Jul  7 18:00:39 2005
From: palvarez7777 at yahoo.es (Alvarez Pedro)
Date: Thu, 7 Jul 2005 18:00:39 +0200 (CEST)
Subject: [R] plotcp
Message-ID: <20050707160039.76760.qmail@web25201.mail.ukl.yahoo.com>

win2000/R 2.0.1/rpart-version: 3.1-23

I use the plotcp-function of the rpart-package and it
works very well. however, plotcp plots the
cv-estimated relative errors only until the minimum of
the curve. How can I plot the values after the
minimum? Choosing the size-option for generating the
tree object does not help because in this case plotcp
plots only the resubstitution estimates.

Thanks.
Pedro


		
______________________________________________ 

Nuevos servicios, m??s seguridad



From HDoran at air.org  Thu Jul  7 18:10:00 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 7 Jul 2005 12:10:00 -0400
Subject: [R] Tempfile error
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409748C49@dc1ex2.air.org>

We just explored the issue a little further here and here is what we
have found. If we look in the temp directory we noticed that the last
temp file name is rf32767. This number coincides with 2^16 with a 16 bit
signed integer. Is there a way to modify the temp file settings to use
an unsigned or 32 bit integer for temp file names?  

We changed the location where temp files are stored. We also disabled
the realtime virus protection, but do not think this has anything to do
with the problem.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Thursday, July 07, 2005 10:46 AM
To: Uwe Ligges; Sean O'Riordain
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Tempfile error

Dear Uwe and Sean:

Thank you for your reply. I think I did a poor job framing my problem.
Here are a few things I should have added. First, I previously stated
that the program would halt in random places. This is *not* true. It
always halts at the same row, which happens to be row 1038. There isn't
a logical reason for stopping in this row as far as I can tell, all data
needed to generate the file are present.

I am using R version 2.10 for Windows on an XP machine with 2 gb ram. My
tex distribution is MikTex (I'm not sure if that matters).

Traceback gives the following information

> traceback()
9: file()
8: driver$runcode(drobj, chunk, chunkopts)
7: Sweave("fam_template.Rnw", output = tmp2)
6: eval.with.vis(expr, envir, enclos)
5: eval.with.vis(ei, envir)
4: source("run_fam.r")
3: eval.with.vis(expr, envir, enclos)
2: eval.with.vis(ei, envir)
1: source(file.choose())

I did tempfile() to identify the location of the files and have cleaned
that out repeatedly. I've searched the archives and have seen some
discussion regarding and on.exit() command, but I'm not sure I see how
it would be utilized here. 

Here is the code for the looping file that subsets the dataframe. I
wonder if I need to add something here that would close a connection or
generate a random tempfile name to avoid this problem in the loop.

list <- unique(wide$stuid)
master = "master.tex"
for (i in list){
     tmp1 <- subset(wide, stuid==i)
     tmp2 <- paste(i, "tex", sep=".")
     Sweave("fam_template.Rnw", output=tmp2)
     file.append("fam_master.tex", tmp2)     
}

Thanks for your time on this,
Harold

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: Thursday, July 07, 2005 4:52 AM
To: Doran, Harold
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Tempfile error

Doran, Harold wrote:

> Dear List:
> 
> I am encountering an error that I can't resolve. I'm looping through 
> rows of a dataframe to generate individual tex files using Sweave. At 
> random points along the way, I encounter the following error
> 
>  Error in file() : cannot find unused tempfile name


Which version of R is this?
I think during one of the latest releases tempfile() name generation has
been imporved, because R did not tried hard enough to find new (random)
filenames for tempfiles in older releases of R.

Uwe Ligges


> At which point Sweave halts. There isn't a logical pattern that I can 
> identify in terms of why the program stops at certain points. It just 
> seems to be random as far as I can tell. I've searched the archives 
> and of course Sweave FAQs but haven't found much that sheds light on 
> what this error indicates and what I should do to resolve it.
> 
> There are approximately 20,000 rows, meaning that about 20,000 tex 
> files are created. If I sample 5,000 or even 10,000 and run the 
> program, I do not encounter an error. It only occurs when I run the 
> program on the full dataframe and even then the error is not occuring 
> at the same point. That is, the row at which the program halts varies
each time.
> 
> Does anyone have any thoughts on this problem?
> 
> -Harold
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Thu Jul  7 18:11:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 7 Jul 2005 18:11:58 +0200
Subject: [R] tcltk package [SOLVED]
In-Reply-To: <42CD2928.9080806@telenet.be>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA6E@usctmx1106.Merck.com>
	<42CD2928.9080806@telenet.be>
Message-ID: <17101.21582.826251.182358@stat.math.ethz.ch>

>>>>> "Kurt" == Kurt Sys <kurt.sys at telenet.be>
>>>>>     on Thu, 07 Jul 2005 15:07:52 +0200 writes:

    Kurt> I had R 2.0.1... It's not included in that distribution of R. 

That's not correct.  The tcltk package has been part of R for a
very long time.

The question is where you got the version of 'R 2.0.1' from
that you mentioned.  It must have been built by someone who
didn't know how to make tcltk work ((and the "R Installation &
Administration" Manual tells you about this))

    Kurt> It's ok in  distribution 2.1.1.

i.e., in the one you got.
Note that it is really a matter of properly building R ... see above.

Martin

    Kurt> thx (to all that've been replying that it's included in the base 
    Kurt> distribution),
    Kurt> Kurt.



    Kurt> Liaw, Andy wrote:

    >> It's included in the base R distribution, I believe.
    >> 
    >> Andy
    >> 
    >> 
    >>> From: Kurt Sys
    >>> 
    >>> Hi all,
    >>> 
    >>> I have a package depending on the tcltk-package. However, I see that 
    >>> this package has been disappeared... Is there a reason why package 
    >>> 'tcltk' is not available anymore? Or is it replaced by another one?
    >>> 
    >>> thx,
    >>> Kurt.

    Kurt> ______________________________________________
    Kurt> R-help at stat.math.ethz.ch mailing list
    Kurt> https://stat.ethz.ch/mailman/listinfo/r-help
    Kurt> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


    Kurt> !DSPAM:42cd2d5784811931259031!



From mschwartz at mn.rr.com  Thu Jul  7 18:37:17 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 07 Jul 2005 11:37:17 -0500
Subject: [R] Plotting Character Variable
In-Reply-To: <20050707114724.f7nf0hb6okk40wco@web.mail.umich.edu>
References: <20050707114724.f7nf0hb6okk40wco@web.mail.umich.edu>
Message-ID: <1120754237.4195.23.camel@localhost.localdomain>

On Thu, 2005-07-07 at 11:47 -0400, ohalawa at umich.edu wrote:
> Any ideas about the following problem:
> 
> I have a matrix (A) that looks like this:
> 
> gene_names             values
> hsa-mir-124              0.3
> hsa-mir-234              0.1
> hsa-mir-344              0.4
> hsa-mir-333              0.7
> .....                  .......
> 
> (This is a 2 by 22283 matrix: quite large)

To split hairs, it would be a 22283 by 2 object in R's [row, column]
approach to indexing.

I am also presuming that "A" is a data frame, since you seem to have two
different data types above, with gene_names being a factor?

> I would like to plot the values, but output the gene_names as the plotting
> symbol. I have tried regular x,y plots, but since the gene_names are quite
> large and there are 22283 of them, it's impossible to fit them on the x-axis.
> 
> Basically, can I plot the above matrix
> 
> plot(gene_names, value) where the gene_names are used as the plotting symbol.
> 
> thank you,

Well...

I would defer to those with more experience in plotting genetic data,
but from a practical standpoint, it seems to me to be highly problematic
to plot >20,000 data points with labels and have them be human readable
without an STM....unless you have _very_ wide paper on a large format
plotter....  ;-)

That being said, one approach is to rotate the x axis labels vertically,
to make more room, while using points for the plotting symbols:

# Adjust bottom margin to make room for vertical labels
par(mar = c(7, 4, 4, 2))

plot(1:nrow(A), A$values, xaxt = "n", ann = FALSE, las = 2)

# use 'las = 3' to rotate the labels
axis(1, at = 1:nrow(A), 
     labels = as.character(A$gene_names), las = 3)


You might want to review some of the tools available at the Bioconductor
site to see if there are specialized plotting functions for this type of
data:

http://www.bioconductor.org/

HTH,

Marc Schwartz



From uofiowa at gmail.com  Thu Jul  7 18:49:58 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Thu, 7 Jul 2005 12:49:58 -0400
Subject: [R] Segmentation fault on exit
Message-ID: <3f87cc6d050707094966766240@mail.gmail.com>

> q()
Segmentation fault

I wrote a library and whenever this library is loaded R sigfaults on
exit. This is not hurting the running of my application since it
happens on exit but does anyone have an idea why might this happen?
The library is a collection of R scripts and has no C or FORTRAN code
under the src/ directory.



From kubovy at virginia.edu  Thu Jul  7 18:51:12 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 7 Jul 2005 12:51:12 -0400
Subject: [R] Failing to install Hmisc on Mac OS 10.3.9,
	R Version 2.1.0 Patched (2005-05-12)
Message-ID: <eb47e0905efbc0df224ad7fb14239199@virginia.edu>

Kindly cc me when replying to the list.

trying URL  
'http://www.biometrics.mtu.edu/CRAN/src/contrib/Hmisc_3.0-6.tar.gz'
Content type 'application/x-gzip' length 462535 bytes
opened URL
==================================================
downloaded 451Kb

* Installing *source* package 'Hmisc' ...
** libs
g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
gcc-3.3 -no-cpp-precomp  
-I/Library/Frameworks/R.framework/Resources/include   
-I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o  
Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o ranksort.o  
rcorr.o wclosest.o  -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2  
-lg2c -lSystem -lcc_dynamic -framework R
** R
** preparing package for lazy loading
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/ 
grDevices.so':
   dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/exec/R  
version mismatch for library: /usr/local/lib/libxml2.2.dylib  
(compatibility version of user: 9.0.0 greater than library's version:  
8.0.0)
Loading required package: grDevices
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library  
'/Library/Frameworks/R.framework/Resources/library/grDevices/libs/ 
grDevices.so':
   dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/exec/R  
version mismatch for library: /usr/local/lib/libxml2.2.dylib  
(compatibility version of user: 9.0.0 greater than library's version:  
8.0.0)
In addition: Warning message:
package grDevices in options("defaultPackages") was not found
** Removing  
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/ 
Hmisc'
** Restoring previous  
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/ 
Hmisc'
Error: package 'grDevices' could not be loaded
Execution halted
ERROR: lazy loading failed for package 'Hmisc'

When I try install.packages("grDevices")
Warning message:
no package 'grDevices' at the repositories in: download.packages(pkgs,  
destdir = tmpd, available = available,

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
Parcels:	Room 102		Gilmer Hall
		McCormick Road	Charlottesville, VA 22903
Office:	B011	+1-434-982-4729
Lab:		B019	+1-434-982-4751
Fax:		+1-434-982-4766
WWW:	http://www.people.virginia.edu/~mk9y/



From mi2kelgrum at yahoo.com  Thu Jul  7 19:20:26 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Thu, 7 Jul 2005 10:20:26 -0700 (PDT)
Subject: [R] xmat[1, 2:3] <- NULL
Message-ID: <20050707172026.65021.qmail@web60219.mail.yahoo.com>

I have a situation where I'm filling out a dataframe
from a database. Sometimes the database query doesn't
get anything, so I end up trying to place NULL in the
dataframe like below.

> temp <- NULL
> xmat <- as.data.frame(matrix(NA, 2, 3))
> xmat[1, 2:3] <- temp
Error in if (m < n * p && (n * p)%%m)
stop(gettextf("replacement has %d items, need %d",  : 
        missing value where TRUE/FALSE needed

I can't get the programme to accept that sometimes
what the query looks for just doesn't exist, and I
just want to move on to the next calculation leaving
the dataframe with a missing value in the given cell.
It's a real show stopper and I haven't found a way
round it.

Best wishes,
Mikkel

PS. I'm using dbGetQuery to query an SQLite database.



From mschwartz at mn.rr.com  Thu Jul  7 19:38:38 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 07 Jul 2005 12:38:38 -0500
Subject: [R] xmat[1, 2:3] <- NULL
In-Reply-To: <20050707172026.65021.qmail@web60219.mail.yahoo.com>
References: <20050707172026.65021.qmail@web60219.mail.yahoo.com>
Message-ID: <1120757918.4195.44.camel@localhost.localdomain>

On Thu, 2005-07-07 at 10:20 -0700, Mikkel Grum wrote:
> I have a situation where I'm filling out a dataframe
> from a database. Sometimes the database query doesn't
> get anything, so I end up trying to place NULL in the
> dataframe like below.
> 
> > temp <- NULL
> > xmat <- as.data.frame(matrix(NA, 2, 3))
> > xmat[1, 2:3] <- temp
> Error in if (m < n * p && (n * p)%%m)
> stop(gettextf("replacement has %d items, need %d",  : 
>         missing value where TRUE/FALSE needed
> 
> I can't get the programme to accept that sometimes
> what the query looks for just doesn't exist, and I
> just want to move on to the next calculation leaving
> the dataframe with a missing value in the given cell.
> It's a real show stopper and I haven't found a way
> round it.
> 
> Best wishes,
> Mikkel
> 
> PS. I'm using dbGetQuery to query an SQLite database.

NULL represents a zero length object in R.

Thus, trying to set only the first row in a data frame to NULL makes no
sense, since you cannot have a 0 length object that also has a single
row (as you seem to be trying to do above).

Since a data frame is a series of lists, you could do the following:

> temp <- NULL
> xmat <- as.data.frame(matrix(NA, 2, 3))

> xmat
  V1 V2 V3
1 NA NA NA
2 NA NA NA

> xmat[, 1] <- temp

> xmat
  V2 V3
1 NA NA
2 NA NA

which removes the first column in the data frame. This is the same as:

> xmat[, -1]
  V2 V3
1 NA NA
2 NA NA


You could also set the entire xmat to NULL as follows:

> xmat
  V1 V2 V3
1 NA NA NA
2 NA NA NA

> xmat <- NULL

> xmat
NULL


You can then test to see if 'xmat' is a NULL:

> is.null(xmat)
[1] TRUE

and base a boolean expression and resultant action on that result:

if (!is.null(xmat))
{
  do_calculations...
}

If your calculations are on a row by row basis, where NA's represent
missing data, you can also use one of several functions to eliminate
those rows. See ?na.action, ?na.omit and ?complete.cases for more
information and examples.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Thu Jul  7 19:42:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 19:42:45 +0200
Subject: [R] Segmentation fault on exit
In-Reply-To: <3f87cc6d050707094966766240@mail.gmail.com>
References: <3f87cc6d050707094966766240@mail.gmail.com>
Message-ID: <42CD6995.6080102@statistik.uni-dortmund.de>

Omar Lakkis wrote:

>>q()
> 
> Segmentation fault
> 
> I wrote a library and whenever this library is loaded R sigfaults on
> exit. This is not hurting the running of my application since it
> happens on exit but does anyone have an idea why might this happen?
> The library is a collection of R scripts and has no C or FORTRAN code
> under the src/ directory.
>

You are talking about a *package*, not a library.
We do not know, and cannot reproduce anything, obviously...
The posting guide asks you to specify reproducible examples.
Which version of R is this, and which OS?

Uwe Ligges




> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul  7 19:43:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 19:43:31 +0200
Subject: [R] xmat[1, 2:3] <- NULL
In-Reply-To: <20050707172026.65021.qmail@web60219.mail.yahoo.com>
References: <20050707172026.65021.qmail@web60219.mail.yahoo.com>
Message-ID: <42CD69C3.2090005@statistik.uni-dortmund.de>

Mikkel Grum wrote:

> I have a situation where I'm filling out a dataframe
> from a database. Sometimes the database query doesn't
> get anything, so I end up trying to place NULL in the
> dataframe like below.
> 
> 
>>temp <- NULL
>>xmat <- as.data.frame(matrix(NA, 2, 3))
>>xmat[1, 2:3] <- temp
> 
> Error in if (m < n * p && (n * p)%%m)
> stop(gettextf("replacement has %d items, need %d",  : 
>         missing value where TRUE/FALSE needed
> 
> I can't get the programme to accept that sometimes
> what the query looks for just doesn't exist, and I
> just want to move on to the next calculation leaving
> the dataframe with a missing value in the given cell.
> It's a real show stopper and I haven't found a way
> round it.


See ?try

Uwe Ligges


> Best wishes,
> Mikkel
> 
> PS. I'm using dbGetQuery to query an SQLite database.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pburns at pburns.seanet.com  Thu Jul  7 20:02:38 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 07 Jul 2005 19:02:38 +0100
Subject: [R] xmat[1, 2:3] <- NULL
In-Reply-To: <20050707172026.65021.qmail@web60219.mail.yahoo.com>
References: <20050707172026.65021.qmail@web60219.mail.yahoo.com>
Message-ID: <42CD6E3E.2090104@pburns.seanet.com>

Maybe I have it wrong, but I think you merely want:

temp <- NA

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Mikkel Grum wrote:

>I have a situation where I'm filling out a dataframe
>from a database. Sometimes the database query doesn't
>get anything, so I end up trying to place NULL in the
>dataframe like below.
>
>  
>
>>temp <- NULL
>>xmat <- as.data.frame(matrix(NA, 2, 3))
>>xmat[1, 2:3] <- temp
>>    
>>
>Error in if (m < n * p && (n * p)%%m)
>stop(gettextf("replacement has %d items, need %d",  : 
>        missing value where TRUE/FALSE needed
>
>I can't get the programme to accept that sometimes
>what the query looks for just doesn't exist, and I
>just want to move on to the next calculation leaving
>the dataframe with a missing value in the given cell.
>It's a real show stopper and I haven't found a way
>round it.
>
>Best wishes,
>Mikkel
>
>PS. I'm using dbGetQuery to query an SQLite database.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From friendly at yorku.ca  Thu Jul  7 20:49:40 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 07 Jul 2005 14:49:40 -0400
Subject: [R] plotting on a reverse log scale
Message-ID: <42CD7944.5010900@yorku.ca>

Thanks to all who replied, particularly Duncan Murdoch, whose solution I 
adopted.
It thought it might be of interest to some to see the results and 
compare these ways
of representing the distribution of historical events over time.
The events are the items I record on my site, Milestones in the History 
of Data
Visualization,
http://www.math.yorku.ca/SCS/Gallery/milestones

Here is the subset of events post 1500:

subset<-
c(1530, 1533, 1545, 1550, 1556, 1562, 1569, 1570, 1572, 1581,
1605, 1603, 1603, 1614, 1617, 1624, 1623, 1626, 1632, 1637, 1644,
1646, 1654, 1654, 1657, 1663, 1662, 1666, 1669, 1671, 1686, 1686,
1687, 1693, 1693, 1701, 1710, 1711, 1712, 1724, 1727, 1745, 1741,
1748, 1752, 1752, 1752, 1753, 1765, 1760, 1763, 1765, 1765, 1781,
1776, 1778, 1779, 1782, 1782, 1782, 1785, 1786, 1787, 1794, 1795,
1796, 1798, 1798, 1800, 1800, 1801, 1801, 1809, 1811, 1817, 1819,
1825, 1821, 1822, 1825, 1827, 1828, 1832, 1830, 1832, 1833, 1833,
1833, 1833, 1836, 1836, 1837, 1838, 1839, 1839, 1843, 1843, 1843,
1844, 1846, 1846, 1851, 1852, 1853, 1855, 1857, 1857, 1857, 1861,
1861, 1863, 1868, 1869, 1869, 1869, 1872, 1872, 1872, 1872, 1873,
1874, 1874, 1874, 1874, 1875, 1875, 1877, 1877, 1877, 1878, 1878,
1879, 1879, 1889, 1880, 1882, 1882, 1883, 1884, 1884, 1884, 1884,
1884, 1885, 1885, 1885, 1888, 1892, 1895, 1896, 1899, 1901, 1904,
1905, 1910, 1910, 1911, 1912, 1913, 1913, 1913, 1913, 1914, 1914,
1915, 1920, 1916, 1917, 1925, 1919, 1920, 1923, 1923, 1924, 1925,
1926, 1929, 1928, 1928, 1929, 1930, 1931, 1933, 1942, 1937, 1939,
1944, 1944, 1957, 1957, 1958, 1962, 1965, 1966, 1965, 1967, 1968,
1969, 1969, 1969, 1971, 1971, 1972, 1973, 1973, 1974, 1974, 1974,
1974, 1975, 1975, 1975, 1975, 1975, 1976, 1977, 1977, 1978, 1978,
1979, 1981, 1981, 1981, 1982, 1982, 1983, 1983, 1985, 1985, 1987,
1988, 1988, 1989, 1989, 1990, 1990, 1990, 1990, 1990, 1991, 1991,
1993, 1992, 1994, 1996, 1999)
 >

The standard density plot, labeled according to periods of time
shows quite interpretable trends,

#  standard plot
plot(density(subset, from=1500, to=1990, bw="sj"),
    main="Milestones: Time course of development",
    xlab="Year")
ref <-c(1600, 1700, 1800, 1850, 1900, 1950, 1975)
abline(v= ref, lty=3, col="blue")

labx<-c(1550, 1650, 1750, 1825, 1875, 1925, 1962, 1987)
laby<- 0.003 + 0.0003 * c(0, 1, 2, 3, 5, 3, 5, 2)
txt1 <-c("Early maps",
     "Measurement\n& theory",
     "New graphic\nforms",
     "Modern\nage",
     "Golden Age",
     "Modern dark\nages",
     "Re-birth",
     "Hi-D Vis")
text(labx, laby, labels=txt1, cex=1.25, xpd=TRUE)
rug(subset, quiet=TRUE)

The idea of a reverse log scale for representing events in time was 
suggested by
\whom{Heinz}{Von Foerster} in 1930, and this (below) produces the 
corresponding
plot;  you might imagine this as a view of the density of events standing at
the year 2000, and looking back at time through a log-scaled telescope.
I wanted to see what this looked like, but I'm not sure it is of 
particularly
greater use here, except to suggest alternative ways to represent historical
time.  Any comments?

#  reverse log plot
rlogyear <- -log(2000-subset)

#from <- -log(2000-1990)
#to <- -log(2000-start)
# need to swap, so from < to for density
to <- -log(2000-1990)
from <- -log(2000-start)


plot(density(rlogyear, from=from, to=to, bw="sj"), axes=F,
    main="Milestones: Time course of development",
    xlab="Year (reverse log scale)")
rug(rlogyear, quiet=TRUE)

labels <- pretty(subset)
labels <- c(labels[labels<2000], 1950, 1975, 1990)
axis(1, labels, at=-log(2000-labels))

minorticks <- pretty(subset, n=30)
minorticks <- minorticks[minorticks<2000]
axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
axis(2)

ref <-c(1600, 1700, 1800, 1850, 1900, 1950, 1975)
abline(v= -log(2000-ref), lty=3, col="blue")

labx<-c(1550, 1650, 1750, 1825, 1875, 1925, 1962, 1987)
laby<- 0.35 + 0.03 * c(-1, .5, 2.2, 4, 1.6, .3, -1, -2)
text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE)
box()

Final question:
  How can I reduce the interline space in multiline strings?
 From ?par, I tried lheight:
 > text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE, lheight=.8)
NULL
Warning message:
parameter "lheight" couldn't be set in high-level plot() function
 >

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From sundar.dorai-raj at pdf.com  Thu Jul  7 20:58:44 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 07 Jul 2005 13:58:44 -0500
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CD7944.5010900@yorku.ca>
References: <42CD7944.5010900@yorku.ca>
Message-ID: <42CD7B64.9080706@pdf.com>



Michael Friendly wrote:
> Thanks to all who replied, particularly Duncan Murdoch, whose solution I 
> adopted.
> It thought it might be of interest to some to see the results and 
> compare these ways
> of representing the distribution of historical events over time.
> The events are the items I record on my site, Milestones in the History 
> of Data
> Visualization,
> http://www.math.yorku.ca/SCS/Gallery/milestones
> 
> Here is the subset of events post 1500:
> 
> subset<-
> c(1530, 1533, 1545, 1550, 1556, 1562, 1569, 1570, 1572, 1581,
> 1605, 1603, 1603, 1614, 1617, 1624, 1623, 1626, 1632, 1637, 1644,
> 1646, 1654, 1654, 1657, 1663, 1662, 1666, 1669, 1671, 1686, 1686,
> 1687, 1693, 1693, 1701, 1710, 1711, 1712, 1724, 1727, 1745, 1741,
> 1748, 1752, 1752, 1752, 1753, 1765, 1760, 1763, 1765, 1765, 1781,
> 1776, 1778, 1779, 1782, 1782, 1782, 1785, 1786, 1787, 1794, 1795,
> 1796, 1798, 1798, 1800, 1800, 1801, 1801, 1809, 1811, 1817, 1819,
> 1825, 1821, 1822, 1825, 1827, 1828, 1832, 1830, 1832, 1833, 1833,
> 1833, 1833, 1836, 1836, 1837, 1838, 1839, 1839, 1843, 1843, 1843,
> 1844, 1846, 1846, 1851, 1852, 1853, 1855, 1857, 1857, 1857, 1861,
> 1861, 1863, 1868, 1869, 1869, 1869, 1872, 1872, 1872, 1872, 1873,
> 1874, 1874, 1874, 1874, 1875, 1875, 1877, 1877, 1877, 1878, 1878,
> 1879, 1879, 1889, 1880, 1882, 1882, 1883, 1884, 1884, 1884, 1884,
> 1884, 1885, 1885, 1885, 1888, 1892, 1895, 1896, 1899, 1901, 1904,
> 1905, 1910, 1910, 1911, 1912, 1913, 1913, 1913, 1913, 1914, 1914,
> 1915, 1920, 1916, 1917, 1925, 1919, 1920, 1923, 1923, 1924, 1925,
> 1926, 1929, 1928, 1928, 1929, 1930, 1931, 1933, 1942, 1937, 1939,
> 1944, 1944, 1957, 1957, 1958, 1962, 1965, 1966, 1965, 1967, 1968,
> 1969, 1969, 1969, 1971, 1971, 1972, 1973, 1973, 1974, 1974, 1974,
> 1974, 1975, 1975, 1975, 1975, 1975, 1976, 1977, 1977, 1978, 1978,
> 1979, 1981, 1981, 1981, 1982, 1982, 1983, 1983, 1985, 1985, 1987,
> 1988, 1988, 1989, 1989, 1990, 1990, 1990, 1990, 1990, 1991, 1991,
> 1993, 1992, 1994, 1996, 1999)
>  >
> 
> The standard density plot, labeled according to periods of time
> shows quite interpretable trends,
> 
> #  standard plot
> plot(density(subset, from=1500, to=1990, bw="sj"),
>     main="Milestones: Time course of development",
>     xlab="Year")
> ref <-c(1600, 1700, 1800, 1850, 1900, 1950, 1975)
> abline(v= ref, lty=3, col="blue")
> 
> labx<-c(1550, 1650, 1750, 1825, 1875, 1925, 1962, 1987)
> laby<- 0.003 + 0.0003 * c(0, 1, 2, 3, 5, 3, 5, 2)
> txt1 <-c("Early maps",
>      "Measurement\n& theory",
>      "New graphic\nforms",
>      "Modern\nage",
>      "Golden Age",
>      "Modern dark\nages",
>      "Re-birth",
>      "Hi-D Vis")
> text(labx, laby, labels=txt1, cex=1.25, xpd=TRUE)
> rug(subset, quiet=TRUE)
> 
> The idea of a reverse log scale for representing events in time was 
> suggested by
> \whom{Heinz}{Von Foerster} in 1930, and this (below) produces the 
> corresponding
> plot;  you might imagine this as a view of the density of events standing at
> the year 2000, and looking back at time through a log-scaled telescope.
> I wanted to see what this looked like, but I'm not sure it is of 
> particularly
> greater use here, except to suggest alternative ways to represent historical
> time.  Any comments?
> 
> #  reverse log plot
> rlogyear <- -log(2000-subset)
> 
> #from <- -log(2000-1990)
> #to <- -log(2000-start)
> # need to swap, so from < to for density
> to <- -log(2000-1990)
> from <- -log(2000-start)
> 
> 
> plot(density(rlogyear, from=from, to=to, bw="sj"), axes=F,
>     main="Milestones: Time course of development",
>     xlab="Year (reverse log scale)")
> rug(rlogyear, quiet=TRUE)
> 
> labels <- pretty(subset)
> labels <- c(labels[labels<2000], 1950, 1975, 1990)
> axis(1, labels, at=-log(2000-labels))
> 
> minorticks <- pretty(subset, n=30)
> minorticks <- minorticks[minorticks<2000]
> axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> axis(2)
> 
> ref <-c(1600, 1700, 1800, 1850, 1900, 1950, 1975)
> abline(v= -log(2000-ref), lty=3, col="blue")
> 
> labx<-c(1550, 1650, 1750, 1825, 1875, 1925, 1962, 1987)
> laby<- 0.35 + 0.03 * c(-1, .5, 2.2, 4, 1.6, .3, -1, -2)
> text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE)
> box()
> 
> Final question:
>   How can I reduce the interline space in multiline strings?
>  From ?par, I tried lheight:
>  > text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE, lheight=.8)
> NULL
> Warning message:
> parameter "lheight" couldn't be set in high-level plot() function
>  >
>



From sundar.dorai-raj at pdf.com  Thu Jul  7 21:00:23 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 07 Jul 2005 14:00:23 -0500
Subject: [R] plotting on a reverse log scale
In-Reply-To: <42CD7944.5010900@yorku.ca>
References: <42CD7944.5010900@yorku.ca>
Message-ID: <42CD7BC7.40204@pdf.com>



Michael Friendly wrote:
> Thanks to all who replied, particularly Duncan Murdoch, whose solution I 
> adopted.
> It thought it might be of interest to some to see the results and 
> compare these ways
> of representing the distribution of historical events over time.
> The events are the items I record on my site, Milestones in the History 
> of Data
> Visualization,
> http://www.math.yorku.ca/SCS/Gallery/milestones
> 
> Here is the subset of events post 1500:
> 
> subset<-
> c(1530, 1533, 1545, 1550, 1556, 1562, 1569, 1570, 1572, 1581,
> 1605, 1603, 1603, 1614, 1617, 1624, 1623, 1626, 1632, 1637, 1644,
> 1646, 1654, 1654, 1657, 1663, 1662, 1666, 1669, 1671, 1686, 1686,
> 1687, 1693, 1693, 1701, 1710, 1711, 1712, 1724, 1727, 1745, 1741,
> 1748, 1752, 1752, 1752, 1753, 1765, 1760, 1763, 1765, 1765, 1781,
> 1776, 1778, 1779, 1782, 1782, 1782, 1785, 1786, 1787, 1794, 1795,
> 1796, 1798, 1798, 1800, 1800, 1801, 1801, 1809, 1811, 1817, 1819,
> 1825, 1821, 1822, 1825, 1827, 1828, 1832, 1830, 1832, 1833, 1833,
> 1833, 1833, 1836, 1836, 1837, 1838, 1839, 1839, 1843, 1843, 1843,
> 1844, 1846, 1846, 1851, 1852, 1853, 1855, 1857, 1857, 1857, 1861,
> 1861, 1863, 1868, 1869, 1869, 1869, 1872, 1872, 1872, 1872, 1873,
> 1874, 1874, 1874, 1874, 1875, 1875, 1877, 1877, 1877, 1878, 1878,
> 1879, 1879, 1889, 1880, 1882, 1882, 1883, 1884, 1884, 1884, 1884,
> 1884, 1885, 1885, 1885, 1888, 1892, 1895, 1896, 1899, 1901, 1904,
> 1905, 1910, 1910, 1911, 1912, 1913, 1913, 1913, 1913, 1914, 1914,
> 1915, 1920, 1916, 1917, 1925, 1919, 1920, 1923, 1923, 1924, 1925,
> 1926, 1929, 1928, 1928, 1929, 1930, 1931, 1933, 1942, 1937, 1939,
> 1944, 1944, 1957, 1957, 1958, 1962, 1965, 1966, 1965, 1967, 1968,
> 1969, 1969, 1969, 1971, 1971, 1972, 1973, 1973, 1974, 1974, 1974,
> 1974, 1975, 1975, 1975, 1975, 1975, 1976, 1977, 1977, 1978, 1978,
> 1979, 1981, 1981, 1981, 1982, 1982, 1983, 1983, 1985, 1985, 1987,
> 1988, 1988, 1989, 1989, 1990, 1990, 1990, 1990, 1990, 1991, 1991,
> 1993, 1992, 1994, 1996, 1999)
>  >
> 
> The standard density plot, labeled according to periods of time
> shows quite interpretable trends,
> 
> #  standard plot
> plot(density(subset, from=1500, to=1990, bw="sj"),
>     main="Milestones: Time course of development",
>     xlab="Year")
> ref <-c(1600, 1700, 1800, 1850, 1900, 1950, 1975)
> abline(v= ref, lty=3, col="blue")
> 
> labx<-c(1550, 1650, 1750, 1825, 1875, 1925, 1962, 1987)
> laby<- 0.003 + 0.0003 * c(0, 1, 2, 3, 5, 3, 5, 2)
> txt1 <-c("Early maps",
>      "Measurement\n& theory",
>      "New graphic\nforms",
>      "Modern\nage",
>      "Golden Age",
>      "Modern dark\nages",
>      "Re-birth",
>      "Hi-D Vis")
> text(labx, laby, labels=txt1, cex=1.25, xpd=TRUE)
> rug(subset, quiet=TRUE)
> 
> The idea of a reverse log scale for representing events in time was 
> suggested by
> \whom{Heinz}{Von Foerster} in 1930, and this (below) produces the 
> corresponding
> plot;  you might imagine this as a view of the density of events standing at
> the year 2000, and looking back at time through a log-scaled telescope.
> I wanted to see what this looked like, but I'm not sure it is of 
> particularly
> greater use here, except to suggest alternative ways to represent historical
> time.  Any comments?
> 
> #  reverse log plot
> rlogyear <- -log(2000-subset)
> 
> #from <- -log(2000-1990)
> #to <- -log(2000-start)
> # need to swap, so from < to for density
> to <- -log(2000-1990)
> from <- -log(2000-start)
> 
> 
> plot(density(rlogyear, from=from, to=to, bw="sj"), axes=F,
>     main="Milestones: Time course of development",
>     xlab="Year (reverse log scale)")
> rug(rlogyear, quiet=TRUE)
> 
> labels <- pretty(subset)
> labels <- c(labels[labels<2000], 1950, 1975, 1990)
> axis(1, labels, at=-log(2000-labels))
> 
> minorticks <- pretty(subset, n=30)
> minorticks <- minorticks[minorticks<2000]
> axis(1, labels=FALSE, at=-log(2000-minorticks), tcl=-0.25)
> axis(2)
> 
> ref <-c(1600, 1700, 1800, 1850, 1900, 1950, 1975)
> abline(v= -log(2000-ref), lty=3, col="blue")
> 
> labx<-c(1550, 1650, 1750, 1825, 1875, 1925, 1962, 1987)
> laby<- 0.35 + 0.03 * c(-1, .5, 2.2, 4, 1.6, .3, -1, -2)
> text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE)
> box()
> 
> Final question:
>   How can I reduce the interline space in multiline strings?
>  From ?par, I tried lheight:
>  > text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE, lheight=.8)
> NULL
> Warning message:
> parameter "lheight" couldn't be set in high-level plot() function
>  >
> 

(sorry, it send on the wrong reply)

Try:

olpar <- par(lheight = 0.8, no.readonly = TRUE)
text(-log(2000-labx), laby, labels=txt1, cex=1.2, xpd=TRUE)
par(olpar)

HTH,

--sundar



From spencer.graves at pdf.com  Thu Jul  7 21:22:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Jul 2005 12:22:38 -0700
Subject: [R] What method I should to use for these data?
In-Reply-To: <200507071356.j67DuXZs017473@hypatia.math.ethz.ch>
References: <200507071356.j67DuXZs017473@hypatia.math.ethz.ch>
Message-ID: <42CD80FE.7060806@pdf.com>

	  I'm sorry, but I do not understand your question well enough to 
comment.  Are you familiar with "www.bioconductor.org"?  They have their 
own list serve and might be better equipped to help you.
Beyond this, I suggest you "read the posting guide, 
"http://www.R-project.org/posting-guide.html".  Many people have found 
answers in the course of following that procedure.  Moreover, following 
that procedure should increase the chances that you will receive a 
useful reply.

	  Good Luck!
	  spencer graves

luan_sheng wrote:

>  
> 
> -----Original Message-----
> From: luan_sheng [mailto:luan_sheng at yahoo.com.cn] 
> Sent: Thursday, July 07, 2005 9:46 PM
> To: (r-help at stat.math.ethz.ch)
> Cc: (R-help at lists.R-project.org)
> Subject: What method I should to use for these data?
> 
> Dear R user:
> 
> I am studying the allele data of two populations.
> the following is the data:
> 
> 	a1	a2	a3	a4	a5	a6	a7	a8	a9
> a10	a11	a12	a13	a14	a15	a16	a17
> pop1	0.0217 	0.0000 	0.0109 	0.0435 	0.0435 	0.0000 	0.0109 	0.0543
> 0.1739 	0.0761 	0.1413 	0.1522 	0.1087 	0.0870 	0.0435 	0.0217 	0.0109 
> pop2	0.0213 	0.0213 	0.0000 	0.0000 	0.0000 	0.0426 	0.1702 	0.2128
> 0.1596 	0.1809 	0.0957 	0.0745 	0.0106 	0.0106 	0.0000 	0.0000 	0.0000 
> 
> 
> a1,a2,a3 ...... a17 is the frequency of 17 alleles , the sum is 1. I want to
> test  the significance of the distribution of 17 alleles between two
> populations. How can I do? I want to use chisquare, is is right for these
> data ?
> 
> can anyone  help me ? Thanks!!
> 
> luan
>  Yellow Sea Fisheries Research Institute , Chinese Academy of Fishery
> Sciences , Qingdao , 266071
> 
> __________________________________________________
> 
> ????????????????G????????????????????????????????????????????????????????????????????????????
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From helprhelp at gmail.com  Thu Jul  7 21:38:14 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 7 Jul 2005 14:38:14 -0500
Subject: [R] randomForest
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>
Message-ID: <cdf8178305070712385c31140c@mail.gmail.com>

Hi there:
I have a question on random foresst:

recently i helped a friend with her random forest and i came with this problem:
her dataset has 6 classes and since the sample size is pretty small:
264 and the class distr is like this (Diag is the response variable)
sample.size <- lapply(1:6, function(i) sum(Diag==i))
> sample.size
[[1]]
[1] 36

[[2]]
[1] 12

[[3]]
[1] 120

[[4]]
[1] 36

[[5]]
[1] 30

[[6]]
[1] 30

I assigned this sample.size to sampsz for a stratiefied sampling
purpose and i got the following error:
Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument

if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is fine. Could you
tell me why?
btw, as to classification problem for this with uneven class number
situation, do u have some suggestions to improve its accuracy?  I
tried to use c() way to make the sampsz works but the result is
similar.

Thanks,

weiwei

On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> The limitation comes from the way categorical splits are represented in the
> code:  For a categorical variable with k categories, the split is
> represented by k binary digits: 0=right, 1=left.  So it takes k bits to
> store each split on k categories.  To save storage, this is `packed' into a
> 4-byte integer (32-bit), thus the limit of 32 categories.
> 
> The current Fortran code (version 5.x) by Breiman and Cutler gets around
> this limitation by storing the split in an integer array.  While this lifts
> the 32-category limit, it takes much more memory to store the splits.  I'm
> still trying to figure out a more memory efficient way of storing the splits
> without imposing the 32-category limit.  If anyone has suggestions, I'm all
> ears.
> 
> Best,
> Andy
> 
> > From: Arne.Muller at sanofi-aventis.com
> >
> > Hello,
> >
> > I'm using the random forest package. One of my factors in the
> > data set contains 41 levels (I can't code this as a numeric
> > value - in terms of linear models this would be a random
> > factor). The randomForest call comes back with an error
> > telling me that the limit is 32 categories.
> >
> > Is there any reason for this particular limit? Maybe it's
> > possible to recompile the module with a different cutoff?
> >
> >       thanks a  lot for your help,
> >       kind regards,
> >
> >
> >       Arne
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From maggie.zhu at pioneer.com  Thu Jul  7 21:40:37 2005
From: maggie.zhu at pioneer.com (Zhu, Maggie)
Date: Thu, 7 Jul 2005 14:40:37 -0500
Subject: [R] look for help on nlm in R
Message-ID: <19257B6BB7BAF143B2DA2964B72D5CA3018092B4@jhms08.phibred.com>

	Hi, 
	I had a hard time in learning nlm in R and appreciate any help. 
	I encounted the following error message from time to time when I tried different starting parameter values (three parameter values in this case) in nlm(f=SS.fun,p=c(0.1/40,0.1,2),hessian = FALSE,N.measure=object,h=20) 
	Error in f(x, ...) : only 0's may mix with negative subscripts 
	Basically I know the three parameter values should all be positive numbers. So, how to select appropriate starting parameter values to prevent this kind of error message? 
	Thanks much 
	Sincerely, 
	Maggie Zhu


This communication is for use by the intended recipient and ...{{dropped}}



From pjhernes at ucdavis.edu  Thu Jul  7 21:41:42 2005
From: pjhernes at ucdavis.edu (Peter J. Hernes)
Date: Thu, 07 Jul 2005 12:41:42 -0700
Subject: [R] Orthogonal Distance Regressions
Message-ID: <6.2.1.2.2.20050707115911.077347e0@bronze.ucdavis.edu>


Hi,

I work with environmental data and want to determine correlations between 
variables that either have no "dependent/independent" relationship or the 
relationship is unknown.  Therefore I prefer to use orthogonal distance 
regression (orthogonal linear regression, perpendicular sum of squares, 
etc.).  I am trying to get set up to do this in R, but the various 
terminologies are making it challenging for me to determine whether this 
capability exists in the base packages (it doesn't look like it to me) or 
which other package I should download to do this.  Searching the R website 
for any of the three terminologies above has not given me any obvious 
solutions.  I would appreciate any assistance from folks with experience 
doing this type of regression.  Thanks in advance!

Peter


Peter J. Hernes, Ph.D.
Land, Air and Water Resources - Hydrology
University of California
One Shields Avenue
Davis, CA 95616-8628
Tel: 530-752-7827
Fax: 530-752-5262
E-mail: pjhernes at ucdavis.edu
Faculty webpage: http://lawr.ucdavis.edu/faculty/hernes/



From murdoch at stats.uwo.ca  Thu Jul  7 21:44:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 07 Jul 2005 15:44:38 -0400
Subject: [R] randomForest
In-Reply-To: <cdf8178305070712385c31140c@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>
	<cdf8178305070712385c31140c@mail.gmail.com>
Message-ID: <42CD8626.2070109@stats.uwo.ca>

On 7/7/2005 3:38 PM, Weiwei Shi wrote:
> Hi there:
> I have a question on random foresst:
> 
> recently i helped a friend with her random forest and i came with this problem:
> her dataset has 6 classes and since the sample size is pretty small:
> 264 and the class distr is like this (Diag is the response variable)
> sample.size <- lapply(1:6, function(i) sum(Diag==i))
>> sample.size
> [[1]]
> [1] 36
> 
> [[2]]
> [1] 12
> 
> [[3]]
> [1] 120
> 
> [[4]]
> [1] 36
> 
> [[5]]
> [1] 30
> 
> [[6]]
> [1] 30
> 
> I assigned this sample.size to sampsz for a stratiefied sampling
> purpose and i got the following error:
> Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument
> 
> if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is fine. Could you
> tell me why?

The sum() function knows what to do on a vector, but not on a list.  You 
can turn your sample.size variable into a vector using

unlist(sample.size)

Duncan Murdoch

> btw, as to classification problem for this with uneven class number
> situation, do u have some suggestions to improve its accuracy?  I
> tried to use c() way to make the sampsz works but the result is
> similar.
> 
> Thanks,
> 
> weiwei
> 
> On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>> The limitation comes from the way categorical splits are represented in the
>> code:  For a categorical variable with k categories, the split is
>> represented by k binary digits: 0=right, 1=left.  So it takes k bits to
>> store each split on k categories.  To save storage, this is `packed' into a
>> 4-byte integer (32-bit), thus the limit of 32 categories.
>> 
>> The current Fortran code (version 5.x) by Breiman and Cutler gets around
>> this limitation by storing the split in an integer array.  While this lifts
>> the 32-category limit, it takes much more memory to store the splits.  I'm
>> still trying to figure out a more memory efficient way of storing the splits
>> without imposing the 32-category limit.  If anyone has suggestions, I'm all
>> ears.
>> 
>> Best,
>> Andy
>> 
>> > From: Arne.Muller at sanofi-aventis.com
>> >
>> > Hello,
>> >
>> > I'm using the random forest package. One of my factors in the
>> > data set contains 41 levels (I can't code this as a numeric
>> > value - in terms of linear models this would be a random
>> > factor). The randomForest call comes back with an error
>> > telling me that the limit is 32 categories.
>> >
>> > Is there any reason for this particular limit? Maybe it's
>> > possible to recompile the module with a different cutoff?
>> >
>> >       thanks a  lot for your help,
>> >       kind regards,
>> >
>> >
>> >       Arne
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html
>> >
>> >
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
> 
>



From helprhelp at gmail.com  Thu Jul  7 21:47:16 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 7 Jul 2005 14:47:16 -0500
Subject: [R] randomForest
In-Reply-To: <42CD8626.2070109@stats.uwo.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>
	<cdf8178305070712385c31140c@mail.gmail.com>
	<42CD8626.2070109@stats.uwo.ca>
Message-ID: <cdf81783050707124761b8cf73@mail.gmail.com>

it works.
thanks,

but: (just curious)
why i tried previously and i got

> is.vector(sample.size)
[1] TRUE

i also tried as.vector(sample.size) and assigned it to sampsz,it still
does not work.

On 7/7/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/7/2005 3:38 PM, Weiwei Shi wrote:
> > Hi there:
> > I have a question on random foresst:
> >
> > recently i helped a friend with her random forest and i came with this problem:
> > her dataset has 6 classes and since the sample size is pretty small:
> > 264 and the class distr is like this (Diag is the response variable)
> > sample.size <- lapply(1:6, function(i) sum(Diag==i))
> >> sample.size
> > [[1]]
> > [1] 36
> >
> > [[2]]
> > [1] 12
> >
> > [[3]]
> > [1] 120
> >
> > [[4]]
> > [1] 36
> >
> > [[5]]
> > [1] 30
> >
> > [[6]]
> > [1] 30
> >
> > I assigned this sample.size to sampsz for a stratiefied sampling
> > purpose and i got the following error:
> > Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument
> >
> > if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is fine. Could you
> > tell me why?
> 
> The sum() function knows what to do on a vector, but not on a list.  You
> can turn your sample.size variable into a vector using
> 
> unlist(sample.size)
> 
> Duncan Murdoch
> 
> > btw, as to classification problem for this with uneven class number
> > situation, do u have some suggestions to improve its accuracy?  I
> > tried to use c() way to make the sampsz works but the result is
> > similar.
> >
> > Thanks,
> >
> > weiwei
> >
> > On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> >> The limitation comes from the way categorical splits are represented in the
> >> code:  For a categorical variable with k categories, the split is
> >> represented by k binary digits: 0=right, 1=left.  So it takes k bits to
> >> store each split on k categories.  To save storage, this is `packed' into a
> >> 4-byte integer (32-bit), thus the limit of 32 categories.
> >>
> >> The current Fortran code (version 5.x) by Breiman and Cutler gets around
> >> this limitation by storing the split in an integer array.  While this lifts
> >> the 32-category limit, it takes much more memory to store the splits.  I'm
> >> still trying to figure out a more memory efficient way of storing the splits
> >> without imposing the 32-category limit.  If anyone has suggestions, I'm all
> >> ears.
> >>
> >> Best,
> >> Andy
> >>
> >> > From: Arne.Muller at sanofi-aventis.com
> >> >
> >> > Hello,
> >> >
> >> > I'm using the random forest package. One of my factors in the
> >> > data set contains 41 levels (I can't code this as a numeric
> >> > value - in terms of linear models this would be a random
> >> > factor). The randomForest call comes back with an error
> >> > telling me that the limit is 32 categories.
> >> >
> >> > Is there any reason for this particular limit? Maybe it's
> >> > possible to recompile the module with a different cutoff?
> >> >
> >> >       thanks a  lot for your help,
> >> >       kind regards,
> >> >
> >> >
> >> >       Arne
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide!
> >> > http://www.R-project.org/posting-guide.html
> >> >
> >> >
> >> >
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> >
> 
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From r4stat at gmail.com  Thu Jul  7 22:00:44 2005
From: r4stat at gmail.com (Steven T.)
Date: Thu, 7 Jul 2005 16:00:44 -0400
Subject: [R] manupulating a data frame column
Message-ID: <58ae3dc7050707130062bf03e4@mail.gmail.com>

Could someone tell me how to fix the following error? It looks like
that the reason is that df$x is of the class "factor". Thanks!

> x1<-LETTERS[1:8]; x2<-letters[1:8]; x1[2]<-NA; x1[4]<-NA;
> df<-data.frame(x1=x1, x2=x2)
> idx<-which(is.na(df$x1))
> df[idx,1]<-df[idx,2]
Warning message:
invalid factor level, NAs generated in: "[<-.factor"(`*tmp*`, iseq,
value = c(2, 4))
>



From andy_liaw at merck.com  Thu Jul  7 22:10:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Jul 2005 16:10:32 -0400
Subject: [R] randomForest
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7A@usctmx1106.Merck.com>

> From: Weiwei Shi
> 
> it works.
> thanks,
> 
> but: (just curious)
> why i tried previously and i got
> 
> > is.vector(sample.size)
> [1] TRUE

Because a list is also a vector:

> a <- c(list(1), list(2))
> a
[[1]]
[1] 1

[[2]]
[1] 2

> is.vector(a)
[1] TRUE
> is.numeric(a)
[1] FALSE

Actually, the way I initialize a list of known length is by something like:

myList <- vector(mode="list", length=veryLong)

Andy
 
 
> i also tried as.vector(sample.size) and assigned it to sampsz,it still
> does not work.
> 
> On 7/7/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > On 7/7/2005 3:38 PM, Weiwei Shi wrote:
> > > Hi there:
> > > I have a question on random foresst:
> > >
> > > recently i helped a friend with her random forest and i 
> came with this problem:
> > > her dataset has 6 classes and since the sample size is 
> pretty small:
> > > 264 and the class distr is like this (Diag is the 
> response variable)
> > > sample.size <- lapply(1:6, function(i) sum(Diag==i))
> > >> sample.size
> > > [[1]]
> > > [1] 36
> > >
> > > [[2]]
> > > [1] 12
> > >
> > > [[3]]
> > > [1] 120
> > >
> > > [[4]]
> > > [1] 36
> > >
> > > [[5]]
> > > [1] 30
> > >
> > > [[6]]
> > > [1] 30
> > >
> > > I assigned this sample.size to sampsz for a stratiefied sampling
> > > purpose and i got the following error:
> > > Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument
> > >
> > > if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is 
> fine. Could you
> > > tell me why?
> > 
> > The sum() function knows what to do on a vector, but not on 
> a list.  You
> > can turn your sample.size variable into a vector using
> > 
> > unlist(sample.size)
> > 
> > Duncan Murdoch
> > 
> > > btw, as to classification problem for this with uneven 
> class number
> > > situation, do u have some suggestions to improve its accuracy?  I
> > > tried to use c() way to make the sampsz works but the result is
> > > similar.
> > >
> > > Thanks,
> > >
> > > weiwei
> > >
> > > On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > >> The limitation comes from the way categorical splits are 
> represented in the
> > >> code:  For a categorical variable with k categories, the split is
> > >> represented by k binary digits: 0=right, 1=left.  So it 
> takes k bits to
> > >> store each split on k categories.  To save storage, this 
> is `packed' into a
> > >> 4-byte integer (32-bit), thus the limit of 32 categories.
> > >>
> > >> The current Fortran code (version 5.x) by Breiman and 
> Cutler gets around
> > >> this limitation by storing the split in an integer 
> array.  While this lifts
> > >> the 32-category limit, it takes much more memory to 
> store the splits.  I'm
> > >> still trying to figure out a more memory efficient way 
> of storing the splits
> > >> without imposing the 32-category limit.  If anyone has 
> suggestions, I'm all
> > >> ears.
> > >>
> > >> Best,
> > >> Andy
> > >>
> > >> > From: Arne.Muller at sanofi-aventis.com
> > >> >
> > >> > Hello,
> > >> >
> > >> > I'm using the random forest package. One of my factors in the
> > >> > data set contains 41 levels (I can't code this as a numeric
> > >> > value - in terms of linear models this would be a random
> > >> > factor). The randomForest call comes back with an error
> > >> > telling me that the limit is 32 categories.
> > >> >
> > >> > Is there any reason for this particular limit? Maybe it's
> > >> > possible to recompile the module with a different cutoff?
> > >> >
> > >> >       thanks a  lot for your help,
> > >> >       kind regards,
> > >> >
> > >> >
> > >> >       Arne
> > >> >
> > >> > ______________________________________________
> > >> > R-help at stat.math.ethz.ch mailing list
> > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > >> > PLEASE do read the posting guide!
> > >> > http://www.R-project.org/posting-guide.html
> > >> >
> > >> >
> > >> >
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > >>
> > >
> > >
> > 
> > 
> 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From murdoch at stats.uwo.ca  Thu Jul  7 22:13:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 07 Jul 2005 16:13:36 -0400
Subject: [R] randomForest
In-Reply-To: <cdf81783050707124761b8cf73@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>	
	<cdf8178305070712385c31140c@mail.gmail.com>	
	<42CD8626.2070109@stats.uwo.ca>
	<cdf81783050707124761b8cf73@mail.gmail.com>
Message-ID: <42CD8CF0.6010600@stats.uwo.ca>

On 7/7/2005 3:47 PM, Weiwei Shi wrote:
> it works.
> thanks,
> 
> but: (just curious)
> why i tried previously and i got
> 
>> is.vector(sample.size)
> [1] TRUE
> 
> i also tried as.vector(sample.size) and assigned it to sampsz,it still
> does not work.

Sorry, I used "vector" incorrectly.  Lists are vectors.  What sum needs 
is a numeric or complex vector, and lists are vectors of objects, not 
vectors of numbers.

You should use is.numeric(sample.size) to test whether you can sum 
sample.size.

Duncan Murdoch

> 
> On 7/7/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 7/7/2005 3:38 PM, Weiwei Shi wrote:
>> > Hi there:
>> > I have a question on random foresst:
>> >
>> > recently i helped a friend with her random forest and i came with this problem:
>> > her dataset has 6 classes and since the sample size is pretty small:
>> > 264 and the class distr is like this (Diag is the response variable)
>> > sample.size <- lapply(1:6, function(i) sum(Diag==i))
>> >> sample.size
>> > [[1]]
>> > [1] 36
>> >
>> > [[2]]
>> > [1] 12
>> >
>> > [[3]]
>> > [1] 120
>> >
>> > [[4]]
>> > [1] 36
>> >
>> > [[5]]
>> > [1] 30
>> >
>> > [[6]]
>> > [1] 30
>> >
>> > I assigned this sample.size to sampsz for a stratiefied sampling
>> > purpose and i got the following error:
>> > Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument
>> >
>> > if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is fine. Could you
>> > tell me why?
>> 
>> The sum() function knows what to do on a vector, but not on a list.  You
>> can turn your sample.size variable into a vector using
>> 
>> unlist(sample.size)
>> 
>> Duncan Murdoch
>> 
>> > btw, as to classification problem for this with uneven class number
>> > situation, do u have some suggestions to improve its accuracy?  I
>> > tried to use c() way to make the sampsz works but the result is
>> > similar.
>> >
>> > Thanks,
>> >
>> > weiwei
>> >
>> > On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
>> >> The limitation comes from the way categorical splits are represented in the
>> >> code:  For a categorical variable with k categories, the split is
>> >> represented by k binary digits: 0=right, 1=left.  So it takes k bits to
>> >> store each split on k categories.  To save storage, this is `packed' into a
>> >> 4-byte integer (32-bit), thus the limit of 32 categories.
>> >>
>> >> The current Fortran code (version 5.x) by Breiman and Cutler gets around
>> >> this limitation by storing the split in an integer array.  While this lifts
>> >> the 32-category limit, it takes much more memory to store the splits.  I'm
>> >> still trying to figure out a more memory efficient way of storing the splits
>> >> without imposing the 32-category limit.  If anyone has suggestions, I'm all
>> >> ears.
>> >>
>> >> Best,
>> >> Andy
>> >>
>> >> > From: Arne.Muller at sanofi-aventis.com
>> >> >
>> >> > Hello,
>> >> >
>> >> > I'm using the random forest package. One of my factors in the
>> >> > data set contains 41 levels (I can't code this as a numeric
>> >> > value - in terms of linear models this would be a random
>> >> > factor). The randomForest call comes back with an error
>> >> > telling me that the limit is 32 categories.
>> >> >
>> >> > Is there any reason for this particular limit? Maybe it's
>> >> > possible to recompile the module with a different cutoff?
>> >> >
>> >> >       thanks a  lot for your help,
>> >> >       kind regards,
>> >> >
>> >> >
>> >> >       Arne
>> >> >
>> >> > ______________________________________________
>> >> > R-help at stat.math.ethz.ch mailing list
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide!
>> >> > http://www.R-project.org/posting-guide.html
>> >> >
>> >> >
>> >> >
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> >>
>> >
>> >
>> 
>> 
> 
>



From helprhelp at gmail.com  Thu Jul  7 22:23:51 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 7 Jul 2005 15:23:51 -0500
Subject: [R] randomForest
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7A@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7A@usctmx1106.Merck.com>
Message-ID: <cdf81783050707132362e041f4@mail.gmail.com>

thanks. but can you suggest some ways for the classification problems
since for some specific class, there are too few observations.

the following is from adding sample.size :
> najie.rf.2 <- randomForest(Diag~., data=one.df[ind==1,4:ncol(one.df)], importance=T, sampsize=unlist(sample.size))
> najie.pred.2<- predict(najie.rf.2, one.df[ind==2,])

> table(observed=one.df[ind==2,"Diag"], predicted=najie.pred.2)
        predicted
observed  1  2  3  4  5  6
       1  6  0  1  0  0  1
       2  0  4  0  0  0  0
       3  1  0 37  0  0  0
       4  0  0  3  5  0  0
       5  1  0  3  0  8  0
       6  0  0  0  3  0  5

and class number returned from sample.size is like:
28, 8, 82, 28, 18, 22

Should I use gbm to try it since it might "focus" more on misplaced cases?

thanks,

weiwei


On 7/7/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > From: Weiwei Shi
> >
> > it works.
> > thanks,
> >
> > but: (just curious)
> > why i tried previously and i got
> >
> > > is.vector(sample.size)
> > [1] TRUE
> 
> Because a list is also a vector:
> 
> > a <- c(list(1), list(2))
> > a
> [[1]]
> [1] 1
> 
> [[2]]
> [1] 2
> 
> > is.vector(a)
> [1] TRUE
> > is.numeric(a)
> [1] FALSE
> 
> Actually, the way I initialize a list of known length is by something like:
> 
> myList <- vector(mode="list", length=veryLong)
> 
> Andy
> 
> 
> > i also tried as.vector(sample.size) and assigned it to sampsz,it still
> > does not work.
> >
> > On 7/7/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > > On 7/7/2005 3:38 PM, Weiwei Shi wrote:
> > > > Hi there:
> > > > I have a question on random foresst:
> > > >
> > > > recently i helped a friend with her random forest and i
> > came with this problem:
> > > > her dataset has 6 classes and since the sample size is
> > pretty small:
> > > > 264 and the class distr is like this (Diag is the
> > response variable)
> > > > sample.size <- lapply(1:6, function(i) sum(Diag==i))
> > > >> sample.size
> > > > [[1]]
> > > > [1] 36
> > > >
> > > > [[2]]
> > > > [1] 12
> > > >
> > > > [[3]]
> > > > [1] 120
> > > >
> > > > [[4]]
> > > > [1] 36
> > > >
> > > > [[5]]
> > > > [1] 30
> > > >
> > > > [[6]]
> > > > [1] 30
> > > >
> > > > I assigned this sample.size to sampsz for a stratiefied sampling
> > > > purpose and i got the following error:
> > > > Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument
> > > >
> > > > if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is
> > fine. Could you
> > > > tell me why?
> > >
> > > The sum() function knows what to do on a vector, but not on
> > a list.  You
> > > can turn your sample.size variable into a vector using
> > >
> > > unlist(sample.size)
> > >
> > > Duncan Murdoch
> > >
> > > > btw, as to classification problem for this with uneven
> > class number
> > > > situation, do u have some suggestions to improve its accuracy?  I
> > > > tried to use c() way to make the sampsz works but the result is
> > > > similar.
> > > >
> > > > Thanks,
> > > >
> > > > weiwei
> > > >
> > > > On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > >> The limitation comes from the way categorical splits are
> > represented in the
> > > >> code:  For a categorical variable with k categories, the split is
> > > >> represented by k binary digits: 0=right, 1=left.  So it
> > takes k bits to
> > > >> store each split on k categories.  To save storage, this
> > is `packed' into a
> > > >> 4-byte integer (32-bit), thus the limit of 32 categories.
> > > >>
> > > >> The current Fortran code (version 5.x) by Breiman and
> > Cutler gets around
> > > >> this limitation by storing the split in an integer
> > array.  While this lifts
> > > >> the 32-category limit, it takes much more memory to
> > store the splits.  I'm
> > > >> still trying to figure out a more memory efficient way
> > of storing the splits
> > > >> without imposing the 32-category limit.  If anyone has
> > suggestions, I'm all
> > > >> ears.
> > > >>
> > > >> Best,
> > > >> Andy
> > > >>
> > > >> > From: Arne.Muller at sanofi-aventis.com
> > > >> >
> > > >> > Hello,
> > > >> >
> > > >> > I'm using the random forest package. One of my factors in the
> > > >> > data set contains 41 levels (I can't code this as a numeric
> > > >> > value - in terms of linear models this would be a random
> > > >> > factor). The randomForest call comes back with an error
> > > >> > telling me that the limit is 32 categories.
> > > >> >
> > > >> > Is there any reason for this particular limit? Maybe it's
> > > >> > possible to recompile the module with a different cutoff?
> > > >> >
> > > >> >       thanks a  lot for your help,
> > > >> >       kind regards,
> > > >> >
> > > >> >
> > > >> >       Arne
> > > >> >
> > > >> > ______________________________________________
> > > >> > R-help at stat.math.ethz.ch mailing list
> > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> > PLEASE do read the posting guide!
> > > >> > http://www.R-project.org/posting-guide.html
> > > >> >
> > > >> >
> > > >> >
> > > >>
> > > >> ______________________________________________
> > > >> R-help at stat.math.ethz.ch mailing list
> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > > >>
> > > >
> > > >
> > >
> > >
> >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From khisht at yahoo.com  Thu Jul  7 11:46:47 2005
From: khisht at yahoo.com (mjs sad)
Date: Thu, 7 Jul 2005 02:46:47 -0700 (PDT)
Subject: [R] Question
Message-ID: <20050707094647.38973.qmail@web32212.mail.mud.yahoo.com>

Hi  

I am statistician and now I am starting to work with
R.
I have a question ,I want to see more than 1 figure in
working directory and I can't do this.

for example when I run plot(...) ,I see a plot ,if I
run another plot(...) the first plot change to second
plot and first plot disappear .

How can I see more than 1 figure in working directory?



From lauraholt_983 at hotmail.com  Thu Jul  7 19:59:34 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Thu, 07 Jul 2005 12:59:34 -0500
Subject: [R]  Main Title for multiple charts
Message-ID: <BAY105-F1D312DDC33E0826E77022D6D80@phx.gbl>

Hi R !

I have the following set up:

>par(mfrow=c(2,2))
>curve(dexp,from=0,to=5)
>hist(z1,main="Size 5")
>hist(z2,main="Size 15")
>hist(z3,main="Size 30")
>

I would like to put a title at the very top of the page that ties the theme 
of all the charts
together.  How would this be done, please?

Thanks in advance!

Laura Holt
mailto: lauraholt_983 at hotmail.com

_________________________________________________________________
Dont just search. Find. Check out the new MSN Search!



From sundar.dorai-raj at pdf.com  Thu Jul  7 22:40:05 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 07 Jul 2005 15:40:05 -0500
Subject: [R] Main Title for multiple charts
In-Reply-To: <BAY105-F1D312DDC33E0826E77022D6D80@phx.gbl>
References: <BAY105-F1D312DDC33E0826E77022D6D80@phx.gbl>
Message-ID: <42CD9325.3000200@pdf.com>



Laura Holt wrote:
> Hi R !
> 
> I have the following set up:
> 
>> par(mfrow=c(2,2))
>> curve(dexp,from=0,to=5)
>> hist(z1,main="Size 5")
>> hist(z2,main="Size 15")
>> hist(z3,main="Size 30")
>>
> 
> I would like to put a title at the very top of the page that ties the 
> theme of all the charts
> together.  How would this be done, please?
> 


How about this:

z1 <- rexp(100)
z2 <- rexp(100)
z3 <- rexp(100)
par(mfrow=c(2,2),oma = c(0, 0, 3, 0))
curve(dexp,from=0,to=5)
hist(z1,main="Size 5")
hist(z2,main="Size 15")
hist(z3,main="Size 30")
mtext("Densities", outer = TRUE, cex = 2)


--sundar



From mi2kelgrum at yahoo.com  Thu Jul  7 22:40:41 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Thu, 7 Jul 2005 13:40:41 -0700 (PDT)
Subject: [R] xmat[1, 2:3] <- NULL
In-Reply-To: <1120757918.4195.44.camel@localhost.localdomain>
Message-ID: <20050707204041.73954.qmail@web60223.mail.yahoo.com>

Thanks a lot!!  This was really a big help. The
following solves my problem:

xmat <- as.data.frame(matrix(NA, 2, 3))
temp <- dbGetQuery(...)
if (!is.null(temp)) {xmat[i, 2:3] <- temp}

I'm adding data to only some columns of a larger
matrix, on a row-by-row basis.

Best wishes,
Mikkel
--- "Marc Schwartz (via MN)" <mschwartz at mn.rr.com>
wrote:

> On Thu, 2005-07-07 at 10:20 -0700, Mikkel Grum
> wrote:
> > I have a situation where I'm filling out a
> dataframe
> > from a database. Sometimes the database query
> doesn't
> > get anything, so I end up trying to place NULL in
> the
> > dataframe like below.
> > 
> > > temp <- NULL
> > > xmat <- as.data.frame(matrix(NA, 2, 3))
> > > xmat[1, 2:3] <- temp
> > Error in if (m < n * p && (n * p)%%m)
> > stop(gettextf("replacement has %d items, need %d",
>  : 
> >         missing value where TRUE/FALSE needed
> > 
> > I can't get the programme to accept that sometimes
> > what the query looks for just doesn't exist, and I
> > just want to move on to the next calculation
> leaving
> > the dataframe with a missing value in the given
> cell.
> > It's a real show stopper and I haven't found a way
> > round it.
> > 
> > Best wishes,
> > Mikkel
> > 
> > PS. I'm using dbGetQuery to query an SQLite
> database.
> 
> NULL represents a zero length object in R.
> 
> Thus, trying to set only the first row in a data
> frame to NULL makes no
> sense, since you cannot have a 0 length object that
> also has a single
> row (as you seem to be trying to do above).
> 
> Since a data frame is a series of lists, you could
> do the following:
> 
> > temp <- NULL
> > xmat <- as.data.frame(matrix(NA, 2, 3))
> 
> > xmat
>   V1 V2 V3
> 1 NA NA NA
> 2 NA NA NA
> 
> > xmat[, 1] <- temp
> 
> > xmat
>   V2 V3
> 1 NA NA
> 2 NA NA
> 
> which removes the first column in the data frame.
> This is the same as:
> 
> > xmat[, -1]
>   V2 V3
> 1 NA NA
> 2 NA NA
> 
> 
> You could also set the entire xmat to NULL as
> follows:
> 
> > xmat
>   V1 V2 V3
> 1 NA NA NA
> 2 NA NA NA
> 
> > xmat <- NULL
> 
> > xmat
> NULL
> 
> 
> You can then test to see if 'xmat' is a NULL:
> 
> > is.null(xmat)
> [1] TRUE
> 
> and base a boolean expression and resultant action
> on that result:
> 
> if (!is.null(xmat))
> {
>   do_calculations...
> }
> 
> If your calculations are on a row by row basis,
> where NA's represent
> missing data, you can also use one of several
> functions to eliminate
> those rows. See ?na.action, ?na.omit and
> ?complete.cases for more
> information and examples.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
>



From gunter.berton at gene.com  Thu Jul  7 22:57:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 7 Jul 2005 13:57:01 -0700
Subject: [R] Orthogonal Distance Regressions
In-Reply-To: <6.2.1.2.2.20050707115911.077347e0@bronze.ucdavis.edu>
Message-ID: <200507072057.j67Kv1mj028448@hertz.gene.com>

I think you want principal components analysis. Google on this and ?prcomp
in R for more details.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter J. Hernes
> Sent: Thursday, July 07, 2005 12:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Orthogonal Distance Regressions
> 
> 
> Hi,
> 
> I work with environmental data and want to determine 
> correlations between 
> variables that either have no "dependent/independent" 
> relationship or the 
> relationship is unknown.  Therefore I prefer to use 
> orthogonal distance 
> regression (orthogonal linear regression, perpendicular sum 
> of squares, 
> etc.).  I am trying to get set up to do this in R, but the various 
> terminologies are making it challenging for me to determine 
> whether this 
> capability exists in the base packages (it doesn't look like 
> it to me) or 
> which other package I should download to do this.  Searching 
> the R website 
> for any of the three terminologies above has not given me any obvious 
> solutions.  I would appreciate any assistance from folks with 
> experience 
> doing this type of regression.  Thanks in advance!
> 
> Peter
> 
> 
> Peter J. Hernes, Ph.D.
> Land, Air and Water Resources - Hydrology
> University of California
> One Shields Avenue
> Davis, CA 95616-8628
> Tel: 530-752-7827
> Fax: 530-752-5262
> E-mail: pjhernes at ucdavis.edu
> Faculty webpage: http://lawr.ucdavis.edu/faculty/hernes/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Jul  7 22:56:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jul 2005 22:56:18 +0200
Subject: [R] Main Title for multiple charts
In-Reply-To: <42CD9325.3000200@pdf.com>
References: <BAY105-F1D312DDC33E0826E77022D6D80@phx.gbl>
	<42CD9325.3000200@pdf.com>
Message-ID: <x2fyuqtjwt.fsf@biostat.ku.dk>

Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:

> > I would like to put a title at the very top of the page that ties the 
> > theme of all the charts
> > together.  How would this be done, please?
> > 
> 
> 
> How about this:
> 
> z1 <- rexp(100)
> z2 <- rexp(100)
> z3 <- rexp(100)
> par(mfrow=c(2,2),oma = c(0, 0, 3, 0))
> curve(dexp,from=0,to=5)
> hist(z1,main="Size 5")
> hist(z2,main="Size 15")
> hist(z3,main="Size 30")
> mtext("Densities", outer = TRUE, cex = 2)

or even

title("Densities", outer = TRUE, cex.main=2)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Thu Jul  7 23:08:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 23:08:11 +0200
Subject: [R] Question
In-Reply-To: <20050707094647.38973.qmail@web32212.mail.mud.yahoo.com>
References: <20050707094647.38973.qmail@web32212.mail.mud.yahoo.com>
Message-ID: <42CD99BB.3020309@statistik.uni-dortmund.de>

mjs sad wrote:
> Hi  
> 
> I am statistician and now I am starting to work with
> R.
> I have a question ,I want to see more than 1 figure in
> working directory and I can't do this.
> 
> for example when I run plot(...) ,I see a plot ,if I
> run another plot(...) the first plot change to second
> plot and first plot disappear .
> 
> How can I see more than 1 figure in working directory?


YOur question is rather unclear and I don't see the relationship between 
multiple figures and the working directory, but I guess you are going to 
start more than one device at once, such as:

x11()
plot(1:10)
x11()
plot(1:55)

or maybe you are looking for argumet "mfrow" in ?par

Please read the posting guide.

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Jul  7 23:07:13 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Jul 2005 17:07:13 -0400
Subject: [R] randomForest
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7C@usctmx1106.Merck.com>

With small sample sizes the variability for estimate of test set error will
be large.  Instead of splitting the data once, you should consider
cross-validation or bootstrap for estimating performance.

AFAIK gbm as is won't handle more than two classes.  You will need to do
quite a bit of work to get it to do what MART does.

Andy

> From: Weiwei Shi 
> 
> thanks. but can you suggest some ways for the classification problems
> since for some specific class, there are too few observations.
> 
> the following is from adding sample.size :
> > najie.rf.2 <- randomForest(Diag~., 
> data=one.df[ind==1,4:ncol(one.df)], importance=T, 
> sampsize=unlist(sample.size))
> > najie.pred.2<- predict(najie.rf.2, one.df[ind==2,])
> 
> > table(observed=one.df[ind==2,"Diag"], predicted=najie.pred.2)
>         predicted
> observed  1  2  3  4  5  6
>        1  6  0  1  0  0  1
>        2  0  4  0  0  0  0
>        3  1  0 37  0  0  0
>        4  0  0  3  5  0  0
>        5  1  0  3  0  8  0
>        6  0  0  0  3  0  5
> 
> and class number returned from sample.size is like:
> 28, 8, 82, 28, 18, 22
> 
> Should I use gbm to try it since it might "focus" more on 
> misplaced cases?
> 
> thanks,
> 
> weiwei
> 
> 
> On 7/7/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > From: Weiwei Shi
> > >
> > > it works.
> > > thanks,
> > >
> > > but: (just curious)
> > > why i tried previously and i got
> > >
> > > > is.vector(sample.size)
> > > [1] TRUE
> > 
> > Because a list is also a vector:
> > 
> > > a <- c(list(1), list(2))
> > > a
> > [[1]]
> > [1] 1
> > 
> > [[2]]
> > [1] 2
> > 
> > > is.vector(a)
> > [1] TRUE
> > > is.numeric(a)
> > [1] FALSE
> > 
> > Actually, the way I initialize a list of known length is by 
> something like:
> > 
> > myList <- vector(mode="list", length=veryLong)
> > 
> > Andy
> > 
> > 
> > > i also tried as.vector(sample.size) and assigned it to 
> sampsz,it still
> > > does not work.
> > >
> > > On 7/7/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > > > On 7/7/2005 3:38 PM, Weiwei Shi wrote:
> > > > > Hi there:
> > > > > I have a question on random foresst:
> > > > >
> > > > > recently i helped a friend with her random forest and i
> > > came with this problem:
> > > > > her dataset has 6 classes and since the sample size is
> > > pretty small:
> > > > > 264 and the class distr is like this (Diag is the
> > > response variable)
> > > > > sample.size <- lapply(1:6, function(i) sum(Diag==i))
> > > > >> sample.size
> > > > > [[1]]
> > > > > [1] 36
> > > > >
> > > > > [[2]]
> > > > > [1] 12
> > > > >
> > > > > [[3]]
> > > > > [1] 120
> > > > >
> > > > > [[4]]
> > > > > [1] 36
> > > > >
> > > > > [[5]]
> > > > > [1] 30
> > > > >
> > > > > [[6]]
> > > > > [1] 30
> > > > >
> > > > > I assigned this sample.size to sampsz for a 
> stratiefied sampling
> > > > > purpose and i got the following error:
> > > > > Error in sum(..., na.rm = na.rm) : invalid 'mode' of argument
> > > > >
> > > > > if I use sampsz=c(36, 12, 120, 36, 30, 30), then it is
> > > fine. Could you
> > > > > tell me why?
> > > >
> > > > The sum() function knows what to do on a vector, but not on
> > > a list.  You
> > > > can turn your sample.size variable into a vector using
> > > >
> > > > unlist(sample.size)
> > > >
> > > > Duncan Murdoch
> > > >
> > > > > btw, as to classification problem for this with uneven
> > > class number
> > > > > situation, do u have some suggestions to improve its 
> accuracy?  I
> > > > > tried to use c() way to make the sampsz works but the 
> result is
> > > > > similar.
> > > > >
> > > > > Thanks,
> > > > >
> > > > > weiwei
> > > > >
> > > > > On 6/30/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > > >> The limitation comes from the way categorical splits are
> > > represented in the
> > > > >> code:  For a categorical variable with k categories, 
> the split is
> > > > >> represented by k binary digits: 0=right, 1=left.  So it
> > > takes k bits to
> > > > >> store each split on k categories.  To save storage, this
> > > is `packed' into a
> > > > >> 4-byte integer (32-bit), thus the limit of 32 categories.
> > > > >>
> > > > >> The current Fortran code (version 5.x) by Breiman and
> > > Cutler gets around
> > > > >> this limitation by storing the split in an integer
> > > array.  While this lifts
> > > > >> the 32-category limit, it takes much more memory to
> > > store the splits.  I'm
> > > > >> still trying to figure out a more memory efficient way
> > > of storing the splits
> > > > >> without imposing the 32-category limit.  If anyone has
> > > suggestions, I'm all
> > > > >> ears.
> > > > >>
> > > > >> Best,
> > > > >> Andy
> > > > >>
> > > > >> > From: Arne.Muller at sanofi-aventis.com
> > > > >> >
> > > > >> > Hello,
> > > > >> >
> > > > >> > I'm using the random forest package. One of my 
> factors in the
> > > > >> > data set contains 41 levels (I can't code this as a numeric
> > > > >> > value - in terms of linear models this would be a random
> > > > >> > factor). The randomForest call comes back with an error
> > > > >> > telling me that the limit is 32 categories.
> > > > >> >
> > > > >> > Is there any reason for this particular limit? Maybe it's
> > > > >> > possible to recompile the module with a different cutoff?
> > > > >> >
> > > > >> >       thanks a  lot for your help,
> > > > >> >       kind regards,
> > > > >> >
> > > > >> >
> > > > >> >       Arne
> > > > >> >
> > > > >> > ______________________________________________
> > > > >> > R-help at stat.math.ethz.ch mailing list
> > > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >> > PLEASE do read the posting guide!
> > > > >> > http://www.R-project.org/posting-guide.html
> > > > >> >
> > > > >> >
> > > > >> >
> > > > >>
> > > > >> ______________________________________________
> > > > >> R-help at stat.math.ethz.ch mailing list
> > > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >> PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > > >>
> > > > >
> > > > >
> > > >
> > > >
> > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> > >
> > 
> > 
> > 
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments, contains information of Merck & Co., Inc. (One 
> Merck Drive, Whitehouse Station, New Jersey, USA 08889), 
> and/or its affiliates (which may be known outside the United 
> States as Merck Frosst, Merck Sharp & Dohme or MSD and in 
> Japan, as Banyu) that may be confidential, proprietary 
> copyrighted and/or legally privileged. It is intended solely 
> for the use of the individual or entity named on this 
> message.  If you are not the intended recipient, and have 
> received this message in error, please notify us immediately 
> by reply e-mail and then delete it from your system.
> > 
> --------------------------------------------------------------
> ----------------
> > 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jul  7 23:23:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Jul 2005 23:23:00 +0200
Subject: [R] manupulating a data frame column
In-Reply-To: <58ae3dc7050707130062bf03e4@mail.gmail.com>
References: <58ae3dc7050707130062bf03e4@mail.gmail.com>
Message-ID: <42CD9D34.8010703@statistik.uni-dortmund.de>

Steven T. wrote:

> Could someone tell me how to fix the following error? It looks like
> that the reason is that df$x is of the class "factor". Thanks!

You are right.

Either don't make it a factor, if you don't want, or try something like 
the following in order to add the relevant levels to df$x1:


x1<-LETTERS[1:8]; x2<-letters[1:8]; x1[2]<-NA; x1[4]<-NA
df<-data.frame(x1=x1, x2=x2)

levels(df$x1) <- c(levels(df$x1), levels(df$x2))

idx<-which(is.na(df$x1))
df[idx,1]<-df[idx,2]


Uwe Ligges

> 
>>x1<-LETTERS[1:8]; x2<-letters[1:8]; x1[2]<-NA; x1[4]<-NA;
>>df<-data.frame(x1=x1, x2=x2)
>>idx<-which(is.na(df$x1))
>>df[idx,1]<-df[idx,2]
> 
> Warning message:
> invalid factor level, NAs generated in: "[<-.factor"(`*tmp*`, iseq,
> value = c(2, 4))
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Thu Jul  7 23:24:08 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 07 Jul 2005 17:24:08 -0400
Subject: [R] What method I should to use for these data?
In-Reply-To: <200507071356.j67DuXZs017473@hypatia.math.ethz.ch>
References: <200507071356.j67DuXZs017473@hypatia.math.ethz.ch>
Message-ID: <42CD9D78.7060704@acelerate.com>

luan_sheng wrote:

> 
>
>-----Original Message-----
>From: luan_sheng [mailto:luan_sheng at yahoo.com.cn] 
>Sent: Thursday, July 07, 2005 9:46 PM
>To: (r-help at stat.math.ethz.ch)
>Cc: (R-help at lists.R-project.org)
>Subject: What method I should to use for these data?
>
>Dear R user:
>
>I am studying the allele data of two populations.
>the following is the data:
>
>	a1	a2	a3	a4	a5	a6	a7	a8	a9
>a10	a11	a12	a13	a14	a15	a16	a17
>pop1	0.0217 	0.0000 	0.0109 	0.0435 	0.0435 	0.0000 	0.0109 	0.0543
>0.1739 	0.0761 	0.1413 	0.1522 	0.1087 	0.0870 	0.0435 	0.0217 	0.0109 
>pop2	0.0213 	0.0213 	0.0000 	0.0000 	0.0000 	0.0426 	0.1702 	0.2128
>0.1596 	0.1809 	0.0957 	0.0745 	0.0106 	0.0106 	0.0000 	0.0000 	0.0000 
>
>
>a1,a2,a3 ...... a17 is the frequency of 17 alleles , the sum is 1. I want to
>test  the significance of the distribution of 17 alleles between two
>populations. How can I do? I want to use chisquare, is is right for these
>data ?
>  
>
If you want to use chisquare, you need the counts and not only the 
proportions.
If that is right can be answered only if we know your hypothesis.

Kjetil

>can anyone  help me ? Thanks!!
>
>luan
> Yellow Sea Fisheries Research Institute , Chinese Academy of Fishery
>Sciences , Qingdao , 266071
>
>__________________________________________________
>
>????????????????G????????????????????????????????????????????????????????????????????????????
>
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From zhliur at yahoo.com  Thu Jul  7 23:27:41 2005
From: zhliur at yahoo.com (yyan liu)
Date: Thu, 7 Jul 2005 14:27:41 -0700 (PDT)
Subject: [R] spurious regression in R
Message-ID: <20050707212742.48159.qmail@web53103.mail.yahoo.com>

Hi:
 I am trying to do a spurious regression in R but I
can not find the function. Anybody used it before? The
problem I have is try to do a regression with several
time series. An alternative is to use the GLS function
to fit the linear regression with the correlation
structure AR(3) for the response (or residual). I hope
the residuals after the GLS regression will be
independent judged by Box-Ljung test. However, I dont
know how the residuals are defined in the GLS
function. Is it just y-yhat or y-yhat times the
(covariance matrix)^(-1/2). Because y-yhat still has
the AR(3) covariance structure and surely be rejected
by the Box-Ljung test. The latter will be independent
if the assumption of AR(3) correlation structure is
right. Any suggestion are highly appreciated. 
Thx!



From jjlusk at purdue.edu  Fri Jul  8 00:11:03 2005
From: jjlusk at purdue.edu (Lusk, Jeffrey J)
Date: Thu, 7 Jul 2005 17:11:03 -0500
Subject: [R] multivariate regression using R
Message-ID: <3FAB4F27134087479BB470D3EB7FD5D603ADF782@EXCH02.purdue.lcl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050707/100664d2/attachment.pl

From uofiowa at gmail.com  Fri Jul  8 00:25:00 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Thu, 7 Jul 2005 18:25:00 -0400
Subject: [R] q() ==> Segmentation fault
Message-ID: <3f87cc6d050707152555d6c3ea@mail.gmail.com>

I created the simple library, attached. When I terminate an R session
where the library has been loaded with q() a segmentation fault is
thrown. Is there any cleaning that I should be doing?

>From R session:
> q()
Segmentation fault

or from shell:
$ R CMD BATCH r.in
/usr/lib/R/bin/BATCH: line 55: 17359 Done                    ( echo
"invisible(options(echo = TRUE))"; cat ${in}; echo "proc.time()" )
     17360 Segmentation fault      | ${R_HOME}/bin/R ${opts} >${out} 2>&1

Attached, please, find the librray tmc.tmp. The library contains the
three files below.
I am using R 2.1.0 and RODBC 1.1-3 on debian. 

::::::::::::::
R/zzz.R
::::::::::::::
.First.lib <-
function (which.lib.loc, package, ...) {
        library(RODBC)

        connect()
}

.Last.lib <-
function (libpath, ...) {
}
::::::::::::::
R/db.R
::::::::::::::
connect <- function() {
        conn <<- odbcConnect('tmc', believeNRows = FALSE)
}

::::::::::::::
data/data.r
::::::::::::::
conn <- NULL

From p.dalgaard at biostat.ku.dk  Fri Jul  8 00:35:10 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jul 2005 00:35:10 +0200
Subject: [R] multivariate regression using R
In-Reply-To: <3FAB4F27134087479BB470D3EB7FD5D603ADF782@EXCH02.purdue.lcl>
References: <3FAB4F27134087479BB470D3EB7FD5D603ADF782@EXCH02.purdue.lcl>
Message-ID: <x2wto2nt29.fsf@turmalin.kubism.ku.dk>

"Lusk, Jeffrey J" <jjlusk at purdue.edu> writes:

> Does anyone know if there is a way to run multivariate linear regression
> in R?  I tried using the lm function (e.g., lm(dv1, dv2~iv1+iv2+iv3),
> but got error messages.  Is my syntax wrong, or do I need a particular
> package?

You need a matrix response: lm(cbind(dv1,dv2)~iv1+iv2+iv3) should do.
There is an anova() method for comparing the resulting "mlm" objects.

And yes, your syntax is wrong: you're calling lm with 2 arguments, dv1
and dv2~iv1+iv2+iv3.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Meredith.Briggs at team.telstra.com  Fri Jul  8 02:08:51 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 8 Jul 2005 10:08:51 +1000
Subject: [R] How do you sort a data frame on a selection of columns?
Message-ID: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050708/89e5d862/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Jul  8 02:21:15 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jul 2005 02:21:15 +0200
Subject: [R] How do you sort a data frame on a selection of columns?
In-Reply-To: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <x2slyqno5g.fsf@turmalin.kubism.ku.dk>

"Briggs, Meredith M" <Meredith.Briggs at team.telstra.com> writes:

> This is what to start with:
> 
> Data Frame      A          B	C	D
> 		c1	4	y	5
> 		c3	6	d	7
> 		c1	5	t	6
> 
> Now sort on A then C
> 
> This is what to end with:
> 
> Data Frame     A          B	C	D
> 		c1	5	t	6
> 		c1	4	y	5
> 		c3	6	d	7
> 
> I assume it is something like this:
> 
> attach(DF)

Attaching data frames before modifying them is not usually a good
idea. Especially if you forget to detach them again.

> sort(DF,partial=c(A,C))

o <- with(DF, order(A,C)) # or just order(DF$A, DF$C)
DF <- DF[o,]

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Fri Jul  8 02:22:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Jul 2005 20:22:58 -0400
Subject: [R] How do you sort a data frame on a selection of columns?
In-Reply-To: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <971536df050707172255ff05f7@mail.gmail.com>

On 7/7/05, Briggs, Meredith M <Meredith.Briggs at team.telstra.com> wrote:
> This is what to start with:
> 
> Data Frame      A          B    C       D
>                c1      4       y       5
>                c3      6       d       7
>                c1      5       t       6
> 
> Now sort on A then C
> 
> This is what to end with:
> 
> Data Frame     A          B     C       D
>                c1      5       t       6
>                c1      4       y       5
>                c3      6       d       7
> 

DF[order(DF$A, DF$C),]

There is also a function posted on r-help that can do this
easier.  Try

RSiteSearch("sort.data.frame")



From jwd at surewest.net  Fri Jul  8 02:18:01 2005
From: jwd at surewest.net (J Dougherty)
Date: Thu, 7 Jul 2005 17:18:01 -0700
Subject: [R] Problem compiling R 2.1.* on SUSE 9.2
Message-ID: <200507071718.01559.jwd@surewest.net>

I have been unable to compile either R 2.1.0 or 2.1.1 under SUSE 9.2.  The 
system simply hangs as far as I can tell.  All key board and mouse 
service dies.  I have had no problem compiling earlier versions of R through 
2.0.1, aside from remembering to include readline in the configuration.  
Configure runs without any warnings except that Info or html versions of the 
R Manuals.  The SUSE 9.2 install is generic with KDE 3.3 as the principal 
GUI.  Compiling also crashes under GNOME.  Because of the system hang, I 
can't provide any error codes.  Perusing /var/log/messages doesn't seem to 
yield any clues.  The system is a KDE AMD XP 2100, there is 1 GB of system 
ram, less than 10% of the harddisk space is in use, videocard is an nVidia 
GeForce4 Ti 4400.  The OS is SUSE 9.2 and it has current updates for 
security.

JWDougherty



From spencer.graves at pdf.com  Fri Jul  8 02:26:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Jul 2005 17:26:23 -0700
Subject: [R] How do you sort a data frame on a selection of columns?
In-Reply-To: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F905641AB79FE0@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <42CDC82F.4010203@pdf.com>

Does "order" do what you want?

spencer graves

Briggs, Meredith M wrote:

> This is what to start with:
> 
> Data Frame      A          B	C	D
> 		c1	4	y	5
> 		c3	6	d	7
> 		c1	5	t	6
> 
> Now sort on A then C
> 
> This is what to end with:
> 
> Data Frame     A          B	C	D
> 		c1	5	t	6
> 		c1	4	y	5
> 		c3	6	d	7
> 
> I assume it is something like this:
> 
> attach(DF)
> sort(DF,partial=c(A,C))
> 
> 
> 
> Thanks in advance.
> 
> Meredith
> 		
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Jul  8 04:16:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Jul 2005 19:16:35 -0700
Subject: [R] spurious regression in R
In-Reply-To: <20050707212742.48159.qmail@web53103.mail.yahoo.com>
References: <20050707212742.48159.qmail@web53103.mail.yahoo.com>
Message-ID: <42CDE203.8030502@pdf.com>

	  R has extensive time series capabilities within base R and especially 
add-on packages like the "dse" bundle and the several packages 
associated with "www.rmetrics.org".  I'm still a novice in this area. 
The source I've found most useful so far is the time series chapter in 
Venables and Ripley (2002) Modern Applied Statistics with S, 4th ed. 
(Springer).

	  Also, are you familiar with "vignette()"?  I found interesting the 
vignettes for "zoo", "dse1", and "dse2", accessed for example as follows:

	  z <- vignette("zoo")

	  z # This brings up a tutorial in Adobe Acrobat

	  edit(z)
# This extracts the R commands from the tutorial
# into a separate file that can be processed line by line,
# edited to test alternatives, etc.
# NOTE:  "edit(z)" did NOT work for me under XEmacs.
# If you use XEmacs, try "Stangle(z$file)";
# this should create a *.R file in "getwd()"

	  I suspect this will not answer your questions, but I hope it helps. 
Feel free to submit another question.  However, before you do, I suggest 
you read the posting guide! 
"http://www.R-project.org/posting-guide.html".  It might help you answer 
many of your questions yourself and increase the utility of answers you 
receive to other questions you post to this list.

	  spencer graves


yyan liu wrote:

> Hi:
>  I am trying to do a spurious regression in R but I
> can not find the function. Anybody used it before? The
> problem I have is try to do a regression with several
> time series. An alternative is to use the GLS function
> to fit the linear regression with the correlation
> structure AR(3) for the response (or residual). I hope
> the residuals after the GLS regression will be
> independent judged by Box-Ljung test. However, I dont
> know how the residuals are defined in the GLS
> function. Is it just y-yhat or y-yhat times the
> (covariance matrix)^(-1/2). Because y-yhat still has
> the AR(3) covariance structure and surely be rejected
> by the Box-Ljung test. The latter will be independent
> if the assumption of AR(3) correlation structure is
> right. Any suggestion are highly appreciated. 
> Thx!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From olaf.mersmann at gmail.com  Fri Jul  8 04:27:54 2005
From: olaf.mersmann at gmail.com (Olaf Mersmann)
Date: Fri, 8 Jul 2005 04:27:54 +0200
Subject: [R] pairs() uses col argument for axes coloring
Message-ID: <d386700a05070719276e6c3013@mail.gmail.com>

Hi list,

not sure if this is the wanted behavior, but running the following code:

> version
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.1
year     2005
month    06
day      20         
language R
> n <- 500
> d <- 4
> m <- matrix(runif(n*d, -1, 1), ncol=d)
> c <- hsv(apply(m, 1, function(x) {sum(x*x)/d}), 1, 1)
> pairs(m, col=c)

gives me the desired coloring of the points but also colors the axes.
Looking at the source for pairs() suggests, that this is the case
because col is part of the ... argument list which is passed on to
localAxis (and from there to axis). Wouldn't it be more approptiate to
use the same color box() uses to draw the border around each
scatterplot? If yes, should I open a bug for this or how would such a
feature request be handled?

-- Olaf Mersmann



From spencer.graves at pdf.com  Fri Jul  8 04:30:25 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Jul 2005 19:30:25 -0700
Subject: [R] Problems with nlme: Quinidine example
In-Reply-To: <1120736219.42cd13dbd8fa2@webmail.mi.fu-berlin.de>
References: <1120736219.42cd13dbd8fa2@webmail.mi.fu-berlin.de>
Message-ID: <42CDE541.7020507@pdf.com>

	  Since I've seen no reply to this so far, I will venture a few 
questions / suggestions.  I have not used nlme (nor nln for Maggie Zhu), 
so I can not comment on the specifics.  I have two general procedures 
for debugging when I get a cryptic error message.

	  First, in R, I can get the source just by entering the name of the 
function.  I copy the results into a script file and trace the code line 
by line until I identify what crashes the code.  Then I can work to 
identify what change I need to make, either to the code or prefereably 
to my argument(s), to make it work.

	  Requestion "nlme" produces the following:

 > nlme
function (model, data = sys.frame(sys.parent()), fixed, random = fixed,
     groups, start, correlation = NULL, weights = NULL, subset,
     method = c("ML", "REML"), na.action = na.fail, naPattern,
     control = list(), verbose = FALSE)
{
     UseMethod("nlme")
}

	  This is not particularly helpful by itself.  However, 
'methods("nlme")' returns the following:

[1] nlme.formula nlme.nlsList

	  I can list these two functions [or access them via getAnywhere if the 
name is followed by an asterisk (*)] and get more detail.

	  Second, with a complicated function call like the two nlme examples, 
I can try to delete or simplify arguments, e.g., delete terms from a 
formula, until I get something that either changes or eliminates the 
error message.

	  Finally, "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html".  It can help you formulate 
a question to increase the chances of a useful reply -- and you may even 
find the answer without waiting for someone on the list to reply.

	  spencer graves

rich at mi.fu-berlin.de wrote:

> This concerns the "Clinical Study of Quinidine" example on page 380
> of the book "Mixed-Effects Models in S and S-PLUS" by Pinheiro and Bates (2000).
> 
> I have tried to reproduce the example, but get an error:
> 
> 
>>library(nlme)
>>fm1Quin.nlme <- nlme(conc ~ quinModel(Subject, time, conc, dose, interval, lV,
> 
> lKa, lCl),
> +                      data=Quinidine,
> +                      fixed=lV + lKa + lCl ~ 1,
> +                      random=pdDiag(lV + lCl ~ 1),
> +                      groups= ~ Subject,
> +                      start=list(fixed=c(5, -0.3, 2)),
> +                      na.action=na.pass, naPattern= ~ !is.na(conc))
> Error in solve.default(estimates[dimE[1] - (p:1), dimE[2] - (p:1), drop =
> FALSE]) :
>         system is computationally singular: reciprocal condition number =
> 6.61723e-17
> 
> Note:
>  - I am running R version 2.1.0 on Linux.
>  - The only difference between the code in the book and the code above is
>    that I use na.pass instead of na.include for the na.action argument, but
>    I don't think this is significant.
> 
> I would appreciate help from anybody who has been able to get this example to
> work.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Jul  8 04:46:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Jul 2005 19:46:01 -0700
Subject: [R] Fwd:  Plotting confidence intervals for lme
In-Reply-To: <ff51f02205070708022ffe19ca@mail.gmail.com>
References: <ff51f022050706070971afca67@mail.gmail.com>	<42CC0A6D.2040604@pdf.com>
	<ff51f02205070702382feef88b@mail.gmail.com>
	<ff51f02205070708022ffe19ca@mail.gmail.com>
Message-ID: <42CDE8E9.6080509@pdf.com>

	  Have you considered the examples with "?TukeyHDS"?  This may not work 
with a standard "lme" object.  However, I suspect that I could probably 
get something useful from this for an "lme" by walking through the code 
line by line.

	  spencer graves

Ghislain Vieilledent wrote:

> ---------- Forwarded message ----------
> From: Ghislain Vieilledent <ghislainv at gmail.com>
> Date: 7 juil. 2005 11:38
> Subject: Re: [R] Plotting confidence intervals for lme
> To: Spencer Graves <spencer.graves at pdf.com>
> 
> That's what I was looking for. Thanks a lot!
> 
> That's true units are mixing and that values of coef depend on the contrasts 
> but I want to use this graph in order to compare levels' coefficient of each 
> of the factor taken independantly, not to compare factors' coefficients to 
> eachothers. Does it mean something?
> 
> Thanks again. 
> 
> Ghislain Vieilledent.
> 
> 
> 
> 2005/7/6, Spencer Graves <spencer.graves at pdf.com>:
> 
>>Consider the following extension of an example in "?lme":
>>
>>fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
>>int <- intervals(fm2)
>>class(int$fixed)
>>kf <- dim(int$fixed)[1] 
>>plot(int$fixed[,2], kf:1,
>>xlab="x", ylab="", xlim=range(int$fixed),
>>axes=FALSE)
>>axis(1)
>>axis(2, kf:1, dimnames(int$fixed)[[1]])
>>segments(int$fixed[,1], kf:1,
>>int$fixed[,3], kf:1) 
>>abline(v=0)
>>
>>However, we are mixing units, i.e., the units for the intercept are
>>"distance", while for "age" are "distance/time", and the interpretation
>>of the coefficient of a factor like Sex depends on contrasts used. 
>>Thus, I don't know how much sense it makes to prepare plots like this.
>>
>>For similar plots that make more sense, see Pinheiro and Bates (2000
>>Mixed-Effects Models in S and S-PLUS (Springer).
>>
>>spencer graves 
>>
>>Ghislain Vieilledent wrote:
>>
>>
>>>Hello and sorry to disturb.
>>>
>>>I'm trying to plot the confidence intervals for the fixed effects of a 
>>
>>lme.
>>
>>>I want to obtain graphically, if it is possible, a bar with Estimate, 
>>
>>upper 
>>
>>>and lower CI for each level of the factors.
>>>
>>>I know how to do for a lm model but for a lme one, I tried with
>>>plot(intervals(...)) and plot(ci(...)) from the gmodels package but it
>>>doesn't work well. 
>>>
>>>Thanks for you help and have a good day.
>>>
>>
>>--
>>Spencer Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com> <http://www.pdf.com>
>>Tel: 408-938-4420
>>Fax: 408-280-7915 
>>
> 
> 
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Ivy_Li at smics.com  Fri Jul  8 05:30:10 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Fri, 8 Jul 2005 11:30:10 +0800
Subject: [R] fail in adding library in new version.
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF27@ex120.smic-sh.com>

Dear all,
	I really appreciate your help. I think I have a little advancement. ^_^
	
	When I enter the Dos environment, at first, into the D:\>, I type the following code:
cd Program Files\R\rw2011\
bin\R CMD install /example

"example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.

Well, there are still aother error:

	---------- Making package example ------------
	  adding build stamp to DESCRIPTION
	error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
	Stop execute
	make[2]: *** [frontmatter] Error 1
	make[1]: *** [all] Error 2
	make: *** [pkg-example] Error 2
	*** Installation of example failed ***
	Removing 'D:/PROGRA~1/R/rw2011/library/example'

Please tell me which step is wrong?
Thanks a lot!

BG
Ivy_Li
 

----------
: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
: 200577 20:57
: Uwe Ligges
: Ivy_Li; r-help at stat.math.ethz.ch
: Re: : : [R] fail in adding library in new version.


On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Gabor Grothendieck wrote:
> 
> > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >
> >>Ivy_Li wrote:
> >>
> >>
> >>>Dear all,
> >>>      I have done every step as the previous mail.
> >>>1. unpack tools.zip into c:\cygwin
> >>>2. install Active perl in c:\Perl
> >>>3. install the mingw32 in c:\mingwin
> >>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> >>
> >>                   ^
> >>such blanks are not allowed in the PATH variable
> >>
> >>
> >>
> >>
> >>>      Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> >>>       So in the Dos environment, at first, into the D:\>, I type the following code:
> >>>cd Program Files\R\rw2011\
> >>
> >>MyRpackages does not need to be here.
> >>
> >>
> >>>bin\R CMD install /MyRpackages/example
> >>
> >>The first slash in "/MyRPackages" sugests that this is a top level
> >>directory, which does not exist.
> >>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> >>and simply say:
> >>
> >>R CMD INSTALL example
> >
> >
> > Another possibility is to put Rcmd.bat from the batch file collection
> >
> >    http://cran.r-project.org/contrib/extra/batchfiles/
> >
> > in your path.  It will use the registry to find R so you won't have
> > to modify your path (nor would you have to remodify it every time you
> > install a new version of R which is what you would otherwise have to do):
> >
> > cd \MyPackages
> > Rcmd install example
> 
> 
> Just for the records:
> 
> 1. "cd \MyPackages" won't work, as I have already explained above.

If MyPackages is not a top level directory in the current drive
then it will not work. Otherwise it does work.

> 
> 2. I do *not* recommend this way, in particular I find it misleading to
> provide these batch files on CRAN.
> 

The alternative, at least as discussed in your post, is more work
since one will then have to change one's path every time one
reinstalls R.  This is just needless extra work and is error prone.  If you
forget to do it then you will be accessing the bin directory of the
wrong version of R.

> >>>      There are some error:
> >>>'make' is neither internal or external command, nor executable operation or batch file
> >>>*** installation of example failed ***
> >>
> >>Well, make.exe is not find in your path. Please check whether the file
> >>exists and the path has been added.
> >>
> >>Uwe Ligges
> >>
> >>
> >>
> >>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >>>
> >>>I think I have closed to success. heehee~~~~~
> >>>Thank you for your help.
> >>>I still need you and others help. Thank you very much!
> >>>
> >>>
> >>>----------
> >>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>: 2005630 19:16
> >>>: Ivy_Li
> >>>: r-help at stat.math.ethz.ch
> >>>: Re: : [R] fail in adding library in new version.
> >>>
> >>>
> >>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>
> >>>
> >>>>Dear Gabor,
> >>>>      Thank your for helping me so much!
> >>>>      I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> >>>>1. unpack tools.zip into c:\cygwin
> >>>>2. install Active perl in c:\Perl
> >>>>3. install the mingw32 in c:\mingwin
> >>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> >>>
> >>>
> >>>If in the console you enter the command:
> >>>
> >>>path
> >>>
> >>>then it will display a semicolon separated list of folders.  You want the folder
> >>>that contains the tools to be at the beginning so that you eliminate
> >>>the possibility
> >>>of finding a different program of the same name first in a folder that comes
> >>>prior to the one where the tools are stored.
> >>>
> >>>
> >>>
> >>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> >>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> >>>>
> >>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> >>>>cd \Program Files\R\rw2010
> >>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> >>>
> >>>
> >>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> >>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> >>>is installed on D then install your package via:
> >>>
> >>>d:
> >>>cd \Program Files\R\rw2010
> >>>bin\R CMD install c:/MyRPackages/example
> >>>
> >>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> >>>using command script install and the indicated source package.
> >>>
> >>>
> >>>
> >>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> >>>
> >>>
> >>>If you are not sure where R is installed then enter the following at the Windows
> >>>console prompt to find out (this will work provided you let it install the key
> >>>into the registry when you installed R initially).  The reg command is a command
> >>>built into Windows (I used XP but I assume its the same on other versions)
> >>>that will query the Windows registry:
> >>>
> >>>reg query hklm\software\r-core\r /v InstallPath
> >>>
> >>>
> >>>
> >>>>I still need your and others help. Thank you very much!
> >>>>
> >>>>
> >>>>
> >>>>----------
> >>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>>: 200566 10:21
> >>>>: Ivy_Li
> >>>>: r-help at stat.math.ethz.ch
> >>>>: Re: [R] fail in adding library in new version.
> >>>>
> >>>>
> >>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>>
> >>>>
> >>>>>Hello everybody,
> >>>>>      Could I consult you a question?
> >>>>>      I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> >>>>
> >>>>Getting the latest version of R is strongly recommended.  The suggestions
> >>>>below all assume the latest version and may or may not work if you do
> >>>>not upgrade.
> >>>>
> >>>>
> >>>>
> >>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>>>>*       Download the tools.zip
> >>>>>*       Unpack tools.zip into c:\cygwin
> >>>>>*       Install Active Perl in c:\Perl
> >>>>>*       Install the mingw32 port of gcc in c:\mingwin
> >>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> >>>>
> >>>>You may need to put these at the beginning of the path rather than the end.
> >>>>Also just as a check enter
> >>>>   path
> >>>>at the console to make sure that you have them.  You will likely
> >>>>have to start a new console session and possibly even reboot.
> >>>>
> >>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> >>>>you reread the material on which tools you need.
> >>>>
> >>>>
> >>>>
> >>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> >>>>
> >>>>In MyRPackages you would have a folder called example, in your case,
> >>>>that contains the package.  Within folder example, you would have the
> >>>>DESCRIPTION file, the R folder, etc.
> >>>>
> >>>>
> >>>>
> >>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> >>>>
> >>>>You don't have to run R first.  You do need to make sure that R.exe can
> >>>>be found on your path or else use the absolute path name in referring to R.
> >>>>For example, if your path does not include R you could do something like this:
> >>>>
> >>>>cd \Program Files\R\rw2010
> >>>>bin\R cmd install /MyRPackages/example
> >>>
> >>>
> >>>Sorry, there is an error in the above.  It should be:
> >>>
> >>>bin\R CMD install c:/MyRPackages/example
> >>>
> >>>or
> >>>
> >>>bin\Rcmd install c:/MyRPackages/example
> >>>
> >>>
> >>>
> >>>
> >>>>Be sure to use forward slashes where shown above and backslashes
> >>>>where shown.
> >>>>
> >>>>
> >>>>
> >>>>>      So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >>>>>
> >>>>
> >>>>Try all these suggestions including upgrading R and if that does not work
> >>>>try posting screen dumps of the actual errors you are getting.
> >>>>
> >>>
> >>>
> >>>Also try googling for
> >>>
> >>>  making creating R packages
> >>>
> >>>and you will find some privately written tutorials on all this.
> >>>
> >>>
> >>>
> >>>------------------------------------------------------------------------
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> 
>



From ggrothendieck at gmail.com  Fri Jul  8 05:59:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Jul 2005 23:59:40 -0400
Subject: [R] fail in adding library in new version.
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF27@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF27@ex120.smic-sh.com>
Message-ID: <971536df05070720593dcfd5b8@mail.gmail.com>

You cannot use an empty DESCRIPTION file.
To get more info on the DESCRIPTION file see
1.1.1 of the Writing Extensions manual which you
can get to from the Help | Manuals menu entry in 
the Windows R GUI.  There is also an example
in that section.

Also \Program Files\R\rw2011\library contains one directory per
package and you can look at the DESCRIPTION files in each
for additional examples (although these are built files and you don't
need the Packaged and Built lines since those were added 
automatically). 

On 7/7/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Dear all,
>        I really appreciate your help. I think I have a little advancement. ^_^
> 
>        When I enter the Dos environment, at first, into the D:\>, I type the following code:
> cd Program Files\R\rw2011\
> bin\R CMD install /example
> 
> "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.
> 
> Well, there are still aother error:
> 
>        ---------- Making package example ------------
>          adding build stamp to DESCRIPTION
>        error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
>        Stop execute
>        make[2]: *** [frontmatter] Error 1
>        make[1]: *** [all] Error 2
>        make: *** [pkg-example] Error 2
>        *** Installation of example failed ***
>        Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> Please tell me which step is wrong?
> Thanks a lot!
> 
> BG
> Ivy_Li
> 
> 
> ----------
> : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> : 200577 20:57
> : Uwe Ligges
> : Ivy_Li; r-help at stat.math.ethz.ch
> : Re: : : [R] fail in adding library in new version.
> 
> 
> On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > Gabor Grothendieck wrote:
> >
> > > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > >
> > >>Ivy_Li wrote:
> > >>
> > >>
> > >>>Dear all,
> > >>>      I have done every step as the previous mail.
> > >>>1. unpack tools.zip into c:\cygwin
> > >>>2. install Active perl in c:\Perl
> > >>>3. install the mingw32 in c:\mingwin
> > >>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> > >>
> > >>                   ^
> > >>such blanks are not allowed in the PATH variable
> > >>
> > >>
> > >>
> > >>
> > >>>      Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> > >>>       So in the Dos environment, at first, into the D:\>, I type the following code:
> > >>>cd Program Files\R\rw2011\
> > >>
> > >>MyRpackages does not need to be here.
> > >>
> > >>
> > >>>bin\R CMD install /MyRpackages/example
> > >>
> > >>The first slash in "/MyRPackages" sugests that this is a top level
> > >>directory, which does not exist.
> > >>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> > >>and simply say:
> > >>
> > >>R CMD INSTALL example
> > >
> > >
> > > Another possibility is to put Rcmd.bat from the batch file collection
> > >
> > >    http://cran.r-project.org/contrib/extra/batchfiles/
> > >
> > > in your path.  It will use the registry to find R so you won't have
> > > to modify your path (nor would you have to remodify it every time you
> > > install a new version of R which is what you would otherwise have to do):
> > >
> > > cd \MyPackages
> > > Rcmd install example
> >
> >
> > Just for the records:
> >
> > 1. "cd \MyPackages" won't work, as I have already explained above.
> 
> If MyPackages is not a top level directory in the current drive
> then it will not work. Otherwise it does work.
> 
> >
> > 2. I do *not* recommend this way, in particular I find it misleading to
> > provide these batch files on CRAN.
> >
> 
> The alternative, at least as discussed in your post, is more work
> since one will then have to change one's path every time one
> reinstalls R.  This is just needless extra work and is error prone.  If you
> forget to do it then you will be accessing the bin directory of the
> wrong version of R.
> 
> > >>>      There are some error:
> > >>>'make' is neither internal or external command, nor executable operation or batch file
> > >>>*** installation of example failed ***
> > >>
> > >>Well, make.exe is not find in your path. Please check whether the file
> > >>exists and the path has been added.
> > >>
> > >>Uwe Ligges
> > >>
> > >>
> > >>
> > >>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> > >>>
> > >>>I think I have closed to success. heehee~~~~~
> > >>>Thank you for your help.
> > >>>I still need you and others help. Thank you very much!
> > >>>
> > >>>
> > >>>----------
> > >>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > >>>: 2005630 19:16
> > >>>: Ivy_Li
> > >>>: r-help at stat.math.ethz.ch
> > >>>: Re: : [R] fail in adding library in new version.
> > >>>
> > >>>
> > >>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > >>>
> > >>>
> > >>>>Dear Gabor,
> > >>>>      Thank your for helping me so much!
> > >>>>      I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> > >>>>1. unpack tools.zip into c:\cygwin
> > >>>>2. install Active perl in c:\Perl
> > >>>>3. install the mingw32 in c:\mingwin
> > >>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> > >>>
> > >>>
> > >>>If in the console you enter the command:
> > >>>
> > >>>path
> > >>>
> > >>>then it will display a semicolon separated list of folders.  You want the folder
> > >>>that contains the tools to be at the beginning so that you eliminate
> > >>>the possibility
> > >>>of finding a different program of the same name first in a folder that comes
> > >>>prior to the one where the tools are stored.
> > >>>
> > >>>
> > >>>
> > >>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> > >>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> > >>>>
> > >>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> > >>>>cd \Program Files\R\rw2010
> > >>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> > >>>
> > >>>
> > >>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> > >>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> > >>>is installed on D then install your package via:
> > >>>
> > >>>d:
> > >>>cd \Program Files\R\rw2010
> > >>>bin\R CMD install c:/MyRPackages/example
> > >>>
> > >>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> > >>>using command script install and the indicated source package.
> > >>>
> > >>>
> > >>>
> > >>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> > >>>
> > >>>
> > >>>If you are not sure where R is installed then enter the following at the Windows
> > >>>console prompt to find out (this will work provided you let it install the key
> > >>>into the registry when you installed R initially).  The reg command is a command
> > >>>built into Windows (I used XP but I assume its the same on other versions)
> > >>>that will query the Windows registry:
> > >>>
> > >>>reg query hklm\software\r-core\r /v InstallPath
> > >>>
> > >>>
> > >>>
> > >>>>I still need your and others help. Thank you very much!
> > >>>>
> > >>>>
> > >>>>
> > >>>>----------
> > >>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > >>>>: 200566 10:21
> > >>>>: Ivy_Li
> > >>>>: r-help at stat.math.ethz.ch
> > >>>>: Re: [R] fail in adding library in new version.
> > >>>>
> > >>>>
> > >>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > >>>>
> > >>>>
> > >>>>>Hello everybody,
> > >>>>>      Could I consult you a question?
> > >>>>>      I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> > >>>>
> > >>>>Getting the latest version of R is strongly recommended.  The suggestions
> > >>>>below all assume the latest version and may or may not work if you do
> > >>>>not upgrade.
> > >>>>
> > >>>>
> > >>>>
> > >>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> > >>>>>*       Download the tools.zip
> > >>>>>*       Unpack tools.zip into c:\cygwin
> > >>>>>*       Install Active Perl in c:\Perl
> > >>>>>*       Install the mingw32 port of gcc in c:\mingwin
> > >>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> > >>>>
> > >>>>You may need to put these at the beginning of the path rather than the end.
> > >>>>Also just as a check enter
> > >>>>   path
> > >>>>at the console to make sure that you have them.  You will likely
> > >>>>have to start a new console session and possibly even reboot.
> > >>>>
> > >>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> > >>>>you reread the material on which tools you need.
> > >>>>
> > >>>>
> > >>>>
> > >>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> > >>>>
> > >>>>In MyRPackages you would have a folder called example, in your case,
> > >>>>that contains the package.  Within folder example, you would have the
> > >>>>DESCRIPTION file, the R folder, etc.
> > >>>>
> > >>>>
> > >>>>
> > >>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> > >>>>
> > >>>>You don't have to run R first.  You do need to make sure that R.exe can
> > >>>>be found on your path or else use the absolute path name in referring to R.
> > >>>>For example, if your path does not include R you could do something like this:
> > >>>>
> > >>>>cd \Program Files\R\rw2010
> > >>>>bin\R cmd install /MyRPackages/example
> > >>>
> > >>>
> > >>>Sorry, there is an error in the above.  It should be:
> > >>>
> > >>>bin\R CMD install c:/MyRPackages/example
> > >>>
> > >>>or
> > >>>
> > >>>bin\Rcmd install c:/MyRPackages/example
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>>Be sure to use forward slashes where shown above and backslashes
> > >>>>where shown.
> > >>>>
> > >>>>
> > >>>>
> > >>>>>      So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> > >>>>>
> > >>>>
> > >>>>Try all these suggestions including upgrading R and if that does not work
> > >>>>try posting screen dumps of the actual errors you are getting.
> > >>>>
> > >>>
> > >>>
> > >>>Also try googling for
> > >>>
> > >>>  making creating R packages
> > >>>
> > >>>and you will find some privately written tutorials on all this.
> > >>>
> > >>>
> > >>>
> > >>>------------------------------------------------------------------------
> > >>>
> > >>>______________________________________________
> > >>>R-help at stat.math.ethz.ch mailing list
> > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >>
> > >>
> >
> >
>



From ggrothendieck at gmail.com  Fri Jul  8 06:48:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 00:48:10 -0400
Subject: [R] fail in adding library in new version.
In-Reply-To: <971536df05070720593dcfd5b8@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF27@ex120.smic-sh.com>
	<971536df05070720593dcfd5b8@mail.gmail.com>
Message-ID: <971536df05070721485dac60a7@mail.gmail.com>

There is also an even larger source of examples at:

http://cran.r-project.org/src/contrib/Descriptions/

although the built caveat mentioned below applies here as well.

On 7/7/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You cannot use an empty DESCRIPTION file.
> To get more info on the DESCRIPTION file see
> 1.1.1 of the Writing Extensions manual which you
> can get to from the Help | Manuals menu entry in
> the Windows R GUI.  There is also an example
> in that section.
> 
> Also \Program Files\R\rw2011\library contains one directory per
> package and you can look at the DESCRIPTION files in each
> for additional examples (although these are built files and you don't
> need the Packaged and Built lines since those were added
> automatically).
> 
> On 7/7/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > Dear all,
> >        I really appreciate your help. I think I have a little advancement. ^_^
> >
> >        When I enter the Dos environment, at first, into the D:\>, I type the following code:
> > cd Program Files\R\rw2011\
> > bin\R CMD install /example
> >
> > "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.
> >
> > Well, there are still aother error:
> >
> >        ---------- Making package example ------------
> >          adding build stamp to DESCRIPTION
> >        error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
> >        Stop execute
> >        make[2]: *** [frontmatter] Error 1
> >        make[1]: *** [all] Error 2
> >        make: *** [pkg-example] Error 2
> >        *** Installation of example failed ***
> >        Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >
> > Please tell me which step is wrong?
> > Thanks a lot!
> >
> > BG
> > Ivy_Li
> >
> >
> > ----------
> > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > : 200577 20:57
> > : Uwe Ligges
> > : Ivy_Li; r-help at stat.math.ethz.ch
> > : Re: : : [R] fail in adding library in new version.
> >
> >
> > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > > Gabor Grothendieck wrote:
> > >
> > > > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > > >
> > > >>Ivy_Li wrote:
> > > >>
> > > >>
> > > >>>Dear all,
> > > >>>      I have done every step as the previous mail.
> > > >>>1. unpack tools.zip into c:\cygwin
> > > >>>2. install Active perl in c:\Perl
> > > >>>3. install the mingw32 in c:\mingwin
> > > >>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> > > >>
> > > >>                   ^
> > > >>such blanks are not allowed in the PATH variable
> > > >>
> > > >>
> > > >>
> > > >>
> > > >>>      Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> > > >>>       So in the Dos environment, at first, into the D:\>, I type the following code:
> > > >>>cd Program Files\R\rw2011\
> > > >>
> > > >>MyRpackages does not need to be here.
> > > >>
> > > >>
> > > >>>bin\R CMD install /MyRpackages/example
> > > >>
> > > >>The first slash in "/MyRPackages" sugests that this is a top level
> > > >>directory, which does not exist.
> > > >>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> > > >>and simply say:
> > > >>
> > > >>R CMD INSTALL example
> > > >
> > > >
> > > > Another possibility is to put Rcmd.bat from the batch file collection
> > > >
> > > >    http://cran.r-project.org/contrib/extra/batchfiles/
> > > >
> > > > in your path.  It will use the registry to find R so you won't have
> > > > to modify your path (nor would you have to remodify it every time you
> > > > install a new version of R which is what you would otherwise have to do):
> > > >
> > > > cd \MyPackages
> > > > Rcmd install example
> > >
> > >
> > > Just for the records:
> > >
> > > 1. "cd \MyPackages" won't work, as I have already explained above.
> >
> > If MyPackages is not a top level directory in the current drive
> > then it will not work. Otherwise it does work.
> >
> > >
> > > 2. I do *not* recommend this way, in particular I find it misleading to
> > > provide these batch files on CRAN.
> > >
> >
> > The alternative, at least as discussed in your post, is more work
> > since one will then have to change one's path every time one
> > reinstalls R.  This is just needless extra work and is error prone.  If you
> > forget to do it then you will be accessing the bin directory of the
> > wrong version of R.
> >
> > > >>>      There are some error:
> > > >>>'make' is neither internal or external command, nor executable operation or batch file
> > > >>>*** installation of example failed ***
> > > >>
> > > >>Well, make.exe is not find in your path. Please check whether the file
> > > >>exists and the path has been added.
> > > >>
> > > >>Uwe Ligges
> > > >>
> > > >>
> > > >>
> > > >>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> > > >>>
> > > >>>I think I have closed to success. heehee~~~~~
> > > >>>Thank you for your help.
> > > >>>I still need you and others help. Thank you very much!
> > > >>>
> > > >>>
> > > >>>----------
> > > >>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > > >>>: 2005630 19:16
> > > >>>: Ivy_Li
> > > >>>: r-help at stat.math.ethz.ch
> > > >>>: Re: : [R] fail in adding library in new version.
> > > >>>
> > > >>>
> > > >>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > > >>>
> > > >>>
> > > >>>>Dear Gabor,
> > > >>>>      Thank your for helping me so much!
> > > >>>>      I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> > > >>>>1. unpack tools.zip into c:\cygwin
> > > >>>>2. install Active perl in c:\Perl
> > > >>>>3. install the mingw32 in c:\mingwin
> > > >>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> > > >>>
> > > >>>
> > > >>>If in the console you enter the command:
> > > >>>
> > > >>>path
> > > >>>
> > > >>>then it will display a semicolon separated list of folders.  You want the folder
> > > >>>that contains the tools to be at the beginning so that you eliminate
> > > >>>the possibility
> > > >>>of finding a different program of the same name first in a folder that comes
> > > >>>prior to the one where the tools are stored.
> > > >>>
> > > >>>
> > > >>>
> > > >>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> > > >>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> > > >>>>
> > > >>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> > > >>>>cd \Program Files\R\rw2010
> > > >>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> > > >>>
> > > >>>
> > > >>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> > > >>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> > > >>>is installed on D then install your package via:
> > > >>>
> > > >>>d:
> > > >>>cd \Program Files\R\rw2010
> > > >>>bin\R CMD install c:/MyRPackages/example
> > > >>>
> > > >>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> > > >>>using command script install and the indicated source package.
> > > >>>
> > > >>>
> > > >>>
> > > >>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> > > >>>
> > > >>>
> > > >>>If you are not sure where R is installed then enter the following at the Windows
> > > >>>console prompt to find out (this will work provided you let it install the key
> > > >>>into the registry when you installed R initially).  The reg command is a command
> > > >>>built into Windows (I used XP but I assume its the same on other versions)
> > > >>>that will query the Windows registry:
> > > >>>
> > > >>>reg query hklm\software\r-core\r /v InstallPath
> > > >>>
> > > >>>
> > > >>>
> > > >>>>I still need your and others help. Thank you very much!
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>----------
> > > >>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > > >>>>: 200566 10:21
> > > >>>>: Ivy_Li
> > > >>>>: r-help at stat.math.ethz.ch
> > > >>>>: Re: [R] fail in adding library in new version.
> > > >>>>
> > > >>>>
> > > >>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > > >>>>
> > > >>>>
> > > >>>>>Hello everybody,
> > > >>>>>      Could I consult you a question?
> > > >>>>>      I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> > > >>>>
> > > >>>>Getting the latest version of R is strongly recommended.  The suggestions
> > > >>>>below all assume the latest version and may or may not work if you do
> > > >>>>not upgrade.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> > > >>>>>*       Download the tools.zip
> > > >>>>>*       Unpack tools.zip into c:\cygwin
> > > >>>>>*       Install Active Perl in c:\Perl
> > > >>>>>*       Install the mingw32 port of gcc in c:\mingwin
> > > >>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> > > >>>>
> > > >>>>You may need to put these at the beginning of the path rather than the end.
> > > >>>>Also just as a check enter
> > > >>>>   path
> > > >>>>at the console to make sure that you have them.  You will likely
> > > >>>>have to start a new console session and possibly even reboot.
> > > >>>>
> > > >>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> > > >>>>you reread the material on which tools you need.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> > > >>>>
> > > >>>>In MyRPackages you would have a folder called example, in your case,
> > > >>>>that contains the package.  Within folder example, you would have the
> > > >>>>DESCRIPTION file, the R folder, etc.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> > > >>>>
> > > >>>>You don't have to run R first.  You do need to make sure that R.exe can
> > > >>>>be found on your path or else use the absolute path name in referring to R.
> > > >>>>For example, if your path does not include R you could do something like this:
> > > >>>>
> > > >>>>cd \Program Files\R\rw2010
> > > >>>>bin\R cmd install /MyRPackages/example
> > > >>>
> > > >>>
> > > >>>Sorry, there is an error in the above.  It should be:
> > > >>>
> > > >>>bin\R CMD install c:/MyRPackages/example
> > > >>>
> > > >>>or
> > > >>>
> > > >>>bin\Rcmd install c:/MyRPackages/example
> > > >>>
> > > >>>
> > > >>>
> > > >>>
> > > >>>>Be sure to use forward slashes where shown above and backslashes
> > > >>>>where shown.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>      So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> > > >>>>>
> > > >>>>
> > > >>>>Try all these suggestions including upgrading R and if that does not work
> > > >>>>try posting screen dumps of the actual errors you are getting.
> > > >>>>
> > > >>>
> > > >>>
> > > >>>Also try googling for
> > > >>>
> > > >>>  making creating R packages
> > > >>>
> > > >>>and you will find some privately written tutorials on all this.
> > > >>>
> > > >>>
> > > >>>
> > > >>>------------------------------------------------------------------------
> > > >>>
> > > >>>______________________________________________
> > > >>>R-help at stat.math.ethz.ch mailing list
> > > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > > >>
> > > >>
> > >
> > >
> >
>



From jacques.veslot at cirad.fr  Fri Jul  8 07:24:47 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 08 Jul 2005 09:24:47 +0400
Subject: [R] PCA and overlaying points
In-Reply-To: <42CD3BE3.8050004@cirad.fr>
References: <42CD182D.9040805@cirad.fr> <42CD3BE3.8050004@cirad.fr>
Message-ID: <42CE0E1F.9060001@cirad.fr>

Hi Renaud,

Thanks a lot for helping !

Actually, I did think about such a function (jitter), but it didn't 
really solve the problem, as it may change nothing for some overlaying 
points and even create new cases of overlaying points. Moreover, it 
modifies every coordinates, even if it is not necessary.

I hoped there might be an heavier function able to change coordinates in 
an let's say intelligent or surgical manner...


jacques



Renaud Lancelot a ??crit :
> Jacques VESLOT a ??crit :
> 
>> Dear R-users,
>>
>> Is there an easy way to avoid points one upon another when ploting 
>> rows and columns of 'dudi' objects ? Maybe there is a function in ade4 
>> or in an other package, or maybe someone has his or her own function 
>> to do this (for example to automatically modify a little the 
>> coordinates of these points to get a readable plot ?).
>>
>> Thanks in advance.
>> Best regards,
>>
>> Jacques VESLOT
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> Hello Jacques,
> 
> Have a look at ?jitter
> 
> Best,
> 
> Renaud
>



From Ivy_Li at smics.com  Fri Jul  8 08:08:07 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Fri, 8 Jul 2005 14:08:07 +0800
Subject: =?gb2312?B?tPC4tDogW1JdIGZhaWwgaW4gYWRkaW5nIGxpYnJhcnkgaW4gbmV3IA==?=
	=?gb2312?B?dmVyc2lvbi4=?=
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF29@ex120.smic-sh.com>

Dear All,
	I imitated the web age "Description" example to make my "description". I wrote:
Package: example
Version: 1.0
Date: 2005-07-08
Title: example library
Author: Ivy <Ivy_Li at smics.com>
Maintainer: Ivy <Ivy_Li at smics.com>
Description: simple function
License: GPL Version 2 or later.
Packaged: Fri July 8 13:00:00 2005; Ivy

And In Dos environment, I install "example" library.
There are some error: 

	---------- Making package example ------------
	  adding build stamp to DESCRIPTION
	  no R files in this package
	  no man files in this package
	  installing indices
	  installing help
	wc: D:/PROGRA~1/R/rw2011/library/example/R/example: No such file or directory adding MD5 sums

	* DONE (example)

Then the "DESCRIPTION", "MD5" and "Meta" folder are created in the path of "D:\Program Files\R\rw2011\library\example"
So please tell me which step was wrong?
Thank you very much!

----------
: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
: 200578 12:48
: Ivy_Li
: Uwe Ligges; r-help at stat.math.ethz.ch
: Re: [R] fail in adding library in new version.


There is also an even larger source of examples at:

http://cran.r-project.org/src/contrib/Descriptions/

although the built caveat mentioned below applies here as well.

On 7/7/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You cannot use an empty DESCRIPTION file.
> To get more info on the DESCRIPTION file see
> 1.1.1 of the Writing Extensions manual which you
> can get to from the Help | Manuals menu entry in
> the Windows R GUI.  There is also an example
> in that section.
> 
> Also \Program Files\R\rw2011\library contains one directory per
> package and you can look at the DESCRIPTION files in each
> for additional examples (although these are built files and you don't
> need the Packaged and Built lines since those were added
> automatically).
> 
> On 7/7/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > Dear all,
> >        I really appreciate your help. I think I have a little advancement. ^_^
> >
> >        When I enter the Dos environment, at first, into the D:\>, I type the following code:
> > cd Program Files\R\rw2011\
> > bin\R CMD install /example
> >
> > "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.
> >
> > Well, there are still aother error:
> >
> >        ---------- Making package example ------------
> >          adding build stamp to DESCRIPTION
> >        error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
> >        Stop execute
> >        make[2]: *** [frontmatter] Error 1
> >        make[1]: *** [all] Error 2
> >        make: *** [pkg-example] Error 2
> >        *** Installation of example failed ***
> >        Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >
> > Please tell me which step is wrong?
> > Thanks a lot!
> >
> > BG
> > Ivy_Li
> >
> >
> > ----------
> > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > : 200577 20:57
> > : Uwe Ligges
> > : Ivy_Li; r-help at stat.math.ethz.ch
> > : Re: : : [R] fail in adding library in new version.
> >
> >
> > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > > Gabor Grothendieck wrote:
> > >
> > > > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > > >
> > > >>Ivy_Li wrote:
> > > >>
> > > >>
> > > >>>Dear all,
> > > >>>      I have done every step as the previous mail.
> > > >>>1. unpack tools.zip into c:\cygwin
> > > >>>2. install Active perl in c:\Perl
> > > >>>3. install the mingw32 in c:\mingwin
> > > >>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> > > >>
> > > >>                   ^
> > > >>such blanks are not allowed in the PATH variable
> > > >>
> > > >>
> > > >>
> > > >>
> > > >>>      Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> > > >>>       So in the Dos environment, at first, into the D:\>, I type the following code:
> > > >>>cd Program Files\R\rw2011\
> > > >>
> > > >>MyRpackages does not need to be here.
> > > >>
> > > >>
> > > >>>bin\R CMD install /MyRpackages/example
> > > >>
> > > >>The first slash in "/MyRPackages" sugests that this is a top level
> > > >>directory, which does not exist.
> > > >>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> > > >>and simply say:
> > > >>
> > > >>R CMD INSTALL example
> > > >
> > > >
> > > > Another possibility is to put Rcmd.bat from the batch file collection
> > > >
> > > >    http://cran.r-project.org/contrib/extra/batchfiles/
> > > >
> > > > in your path.  It will use the registry to find R so you won't have
> > > > to modify your path (nor would you have to remodify it every time you
> > > > install a new version of R which is what you would otherwise have to do):
> > > >
> > > > cd \MyPackages
> > > > Rcmd install example
> > >
> > >
> > > Just for the records:
> > >
> > > 1. "cd \MyPackages" won't work, as I have already explained above.
> >
> > If MyPackages is not a top level directory in the current drive
> > then it will not work. Otherwise it does work.
> >
> > >
> > > 2. I do *not* recommend this way, in particular I find it misleading to
> > > provide these batch files on CRAN.
> > >
> >
> > The alternative, at least as discussed in your post, is more work
> > since one will then have to change one's path every time one
> > reinstalls R.  This is just needless extra work and is error prone.  If you
> > forget to do it then you will be accessing the bin directory of the
> > wrong version of R.
> >
> > > >>>      There are some error:
> > > >>>'make' is neither internal or external command, nor executable operation or batch file
> > > >>>*** installation of example failed ***
> > > >>
> > > >>Well, make.exe is not find in your path. Please check whether the file
> > > >>exists and the path has been added.
> > > >>
> > > >>Uwe Ligges
> > > >>
> > > >>
> > > >>
> > > >>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> > > >>>
> > > >>>I think I have closed to success. heehee~~~~~
> > > >>>Thank you for your help.
> > > >>>I still need you and others help. Thank you very much!
> > > >>>
> > > >>>
> > > >>>----------
> > > >>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > > >>>: 2005630 19:16
> > > >>>: Ivy_Li
> > > >>>: r-help at stat.math.ethz.ch
> > > >>>: Re: : [R] fail in adding library in new version.
> > > >>>
> > > >>>
> > > >>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > > >>>
> > > >>>
> > > >>>>Dear Gabor,
> > > >>>>      Thank your for helping me so much!
> > > >>>>      I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> > > >>>>1. unpack tools.zip into c:\cygwin
> > > >>>>2. install Active perl in c:\Perl
> > > >>>>3. install the mingw32 in c:\mingwin
> > > >>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> > > >>>
> > > >>>
> > > >>>If in the console you enter the command:
> > > >>>
> > > >>>path
> > > >>>
> > > >>>then it will display a semicolon separated list of folders.  You want the folder
> > > >>>that contains the tools to be at the beginning so that you eliminate
> > > >>>the possibility
> > > >>>of finding a different program of the same name first in a folder that comes
> > > >>>prior to the one where the tools are stored.
> > > >>>
> > > >>>
> > > >>>
> > > >>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> > > >>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> > > >>>>
> > > >>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> > > >>>>cd \Program Files\R\rw2010
> > > >>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> > > >>>
> > > >>>
> > > >>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> > > >>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> > > >>>is installed on D then install your package via:
> > > >>>
> > > >>>d:
> > > >>>cd \Program Files\R\rw2010
> > > >>>bin\R CMD install c:/MyRPackages/example
> > > >>>
> > > >>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> > > >>>using command script install and the indicated source package.
> > > >>>
> > > >>>
> > > >>>
> > > >>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> > > >>>
> > > >>>
> > > >>>If you are not sure where R is installed then enter the following at the Windows
> > > >>>console prompt to find out (this will work provided you let it install the key
> > > >>>into the registry when you installed R initially).  The reg command is a command
> > > >>>built into Windows (I used XP but I assume its the same on other versions)
> > > >>>that will query the Windows registry:
> > > >>>
> > > >>>reg query hklm\software\r-core\r /v InstallPath
> > > >>>
> > > >>>
> > > >>>
> > > >>>>I still need your and others help. Thank you very much!
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>----------
> > > >>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > > >>>>: 200566 10:21
> > > >>>>: Ivy_Li
> > > >>>>: r-help at stat.math.ethz.ch
> > > >>>>: Re: [R] fail in adding library in new version.
> > > >>>>
> > > >>>>
> > > >>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > > >>>>
> > > >>>>
> > > >>>>>Hello everybody,
> > > >>>>>      Could I consult you a question?
> > > >>>>>      I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> > > >>>>
> > > >>>>Getting the latest version of R is strongly recommended.  The suggestions
> > > >>>>below all assume the latest version and may or may not work if you do
> > > >>>>not upgrade.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> > > >>>>>*       Download the tools.zip
> > > >>>>>*       Unpack tools.zip into c:\cygwin
> > > >>>>>*       Install Active Perl in c:\Perl
> > > >>>>>*       Install the mingw32 port of gcc in c:\mingwin
> > > >>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> > > >>>>
> > > >>>>You may need to put these at the beginning of the path rather than the end.
> > > >>>>Also just as a check enter
> > > >>>>   path
> > > >>>>at the console to make sure that you have them.  You will likely
> > > >>>>have to start a new console session and possibly even reboot.
> > > >>>>
> > > >>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> > > >>>>you reread the material on which tools you need.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> > > >>>>
> > > >>>>In MyRPackages you would have a folder called example, in your case,
> > > >>>>that contains the package.  Within folder example, you would have the
> > > >>>>DESCRIPTION file, the R folder, etc.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> > > >>>>
> > > >>>>You don't have to run R first.  You do need to make sure that R.exe can
> > > >>>>be found on your path or else use the absolute path name in referring to R.
> > > >>>>For example, if your path does not include R you could do something like this:
> > > >>>>
> > > >>>>cd \Program Files\R\rw2010
> > > >>>>bin\R cmd install /MyRPackages/example
> > > >>>
> > > >>>
> > > >>>Sorry, there is an error in the above.  It should be:
> > > >>>
> > > >>>bin\R CMD install c:/MyRPackages/example
> > > >>>
> > > >>>or
> > > >>>
> > > >>>bin\Rcmd install c:/MyRPackages/example
> > > >>>
> > > >>>
> > > >>>
> > > >>>
> > > >>>>Be sure to use forward slashes where shown above and backslashes
> > > >>>>where shown.
> > > >>>>
> > > >>>>
> > > >>>>
> > > >>>>>      So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> > > >>>>>
> > > >>>>
> > > >>>>Try all these suggestions including upgrading R and if that does not work
> > > >>>>try posting screen dumps of the actual errors you are getting.
> > > >>>>
> > > >>>
> > > >>>
> > > >>>Also try googling for
> > > >>>
> > > >>>  making creating R packages
> > > >>>
> > > >>>and you will find some privately written tutorials on all this.
> > > >>>
> > > >>>
> > > >>>
> > > >>>------------------------------------------------------------------------
> > > >>>
> > > >>>______________________________________________
> > > >>>R-help at stat.math.ethz.ch mailing list
> > > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > > >>
> > > >>
> > >
> > >
> >
>



From jago at mclink.it  Fri Jul  8 08:15:35 2005
From: jago at mclink.it (stefano iacus)
Date: Fri, 8 Jul 2005 08:15:35 +0200
Subject: [R] [R-SIG-Mac] r version 2.1.0 and graphics in mac os 10.3.9
In-Reply-To: <423979b69fd7d67420a664c749025088@uga.edu>
References: <423979b69fd7d67420a664c749025088@uga.edu>
Message-ID: <5C07B2B9-FAFB-4A63-9402-FB23414E85A6@mclink.it>

Hi Luis,

please remove

/usr/local/lib/libxml*  files

and in particular /usr/local/lib/libxml2.2.dylib

from your system

sudo rm /usr/local/lib/libxml2.2.dylib


stefano

On 04/lug/05, at 20:42, Luis Borda de Agua wrote:

> I use mac os 10.3.9 and I've installed in my computer R 2.1.0 (I
> believe this is the latest R version).
>
> Although it works alright when I open R by clicking in the R icon, I
> cannot use the graphics facility when I open R from a X-terminal  
> window
> (or x-11 window).
>
> In fact, when I try to open R I get the message that I pasted below.
> Is this a R bug?
> Is R assuming that I should have the latest Mac OS Tiger installed?
> Or is it a problem related to my computer only?
>
> I did not have this problem when I used a previous version of R (I
> don't know which one was).
>
> Thank you in advance for your help.
>
> Luis BA.
>
> ___________________________________________
>
>
> % R
>
> R : Copyright 2005, The R Foundation for Statistical Computing
> Version 2.1.0 Patched (2005-05-12), ISBN 3-900051-07-0
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
>
> [Previously saved workspace restored]
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>          unable to load shared library
> '/Library/Frameworks/R.framework/Resources/library/grDevices/libs/
> grDevices.so':
>    dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/ 
> exec/R
> version mismatch for library: /usr/local/lib/libxml2.2.dylib
> (compatibility version of user: 9.0.0 greater than library's version:
> 8.0.0)
> Loading required package: grDevices
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>          unable to load shared library
> '/Library/Frameworks/R.framework/Resources/library/grDevices/libs/
> grDevices.so':
>    dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/ 
> exec/R
> version mismatch for library: /usr/local/lib/libxml2.2.dylib
> (compatibility version of user: 9.0.0 greater than library's version:
> 8.0.0)
> In addition: Warning message:
> package grDevices in options("defaultPackages") was not found
> Error: package 'grDevices' could not be loaded
>
>
> ________________________________________________________
> Luis Borda de Agua
> 2502 Department of Plant Biology
> University of Georgia
> Athens GA 30602
> USA
> Phone: (706) 583-0943
> Fax: (706) 542-1805
>
> _______________________________________________
> R-SIG-Mac mailing list
> R-SIG-Mac at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-mac
>
>



From sean.oriordain at gmail.com  Fri Jul  8 09:32:07 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Fri, 8 Jul 2005 08:32:07 +0100
Subject: [R] Problem compiling R 2.1.* on SUSE 9.2
In-Reply-To: <200507071718.01559.jwd@surewest.net>
References: <200507071718.01559.jwd@surewest.net>
Message-ID: <8ed68eed05070800324731cee@mail.gmail.com>

Hi!

what point exactly in the command sequence does it fail? during make?
or make install?

You might try saving the output and the error from the build into a
file? Something along the lines of
./configure 2>&1  >filename

or even
./configure 2>&1  >filename | tee configure_output.txt

etc.

cheers!
Sean


On 7/8/05, J Dougherty <jwd at surewest.net> wrote:
> I have been unable to compile either R 2.1.0 or 2.1.1 under SUSE 9.2.  The
> system simply hangs as far as I can tell.  All key board and mouse
> service dies.  I have had no problem compiling earlier versions of R through
> 2.0.1, aside from remembering to include readline in the configuration.
> Configure runs without any warnings except that Info or html versions of the
> R Manuals.  The SUSE 9.2 install is generic with KDE 3.3 as the principal
> GUI.  Compiling also crashes under GNOME.  Because of the system hang, I
> can't provide any error codes.  Perusing /var/log/messages doesn't seem to
> yield any clues.  The system is a KDE AMD XP 2100, there is 1 GB of system
> ram, less than 10% of the harddisk space is in use, videocard is an nVidia
> GeForce4 Ti 4400.  The OS is SUSE 9.2 and it has current updates for
> security.
> 
> JWDougherty
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vehbisinan at gmail.com  Fri Jul  8 09:30:49 2005
From: vehbisinan at gmail.com (Vehbi Sinan Tunalioglu)
Date: Fri, 08 Jul 2005 10:30:49 +0300
Subject: [R] r: LOOPING
In-Reply-To: <42CD012C.1090707@statistik.uni-dortmund.de>
References: <42CCE0E7.A9D9C17D@STATS.uct.ac.za>
	<42CD012C.1090707@statistik.uni-dortmund.de>
Message-ID: <42CE2BA9.9000102@gmail.com>

Uwe Ligges wrote:
> Clark Allan wrote:
>>
>>i know that one should try and limit the amount of looping in R
>>programs. i have supplied some code below. i am interested in seeing how
>>the code cold be rewritten if we dont use the loops.
> 
> It is not always a good thing to remove loops (without having looked at 
> each details of the code below).
> "Compromise" is the keyword.

If you have big routines for each iteration and especially you have back
references in the data structures you are manipulating, it becomes
really _hard to translate_ loop statements to filter-map-accumulator
routines provided by R (i.e. *apply functions). I really cannot find my
way in some situations. What I did so far is to write those routines in
C. Dirty hack :(

Maybe one should write a tutorial: "Howto avoid loops in R" by giving
possible scenarios.

--vst



From Ted.Harding at nessie.mcc.ac.uk  Fri Jul  8 10:30:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 08 Jul 2005 09:30:08 +0100 (BST)
Subject: [R] [OT] "Dispersion" in French
Message-ID: <XFMail.050708093008.Ted.Harding@nessie.mcc.ac.uk>

Greetings,

I'm posting this OT query here because of out very international
membership!

In the French sentence

  "Les taux de tirage sont calcul??s de mani??re ?? ce que la
   dispersion soit inf??rieure ?? 5 % dans chaque strate."

it would seem intended that the "dispersion" is to be calculated
in a specific way (unstated) -- otherwise, how to ensure that it
shall be less than 5%?

So my question is: What, in English, would one use for "dispersion"?

Standard deviation?

Just as in English, also in general French usage in statistics,
"dispersion" is a general term for which one can adopt various
"mesures de dispersion" such as "??cart standard", etc. So in
principle this sentence is as meaningful as the English "The sample
sizes are to be calculated so as to ensure that the dispersion
is less then 5% in each stratum." I would still want to know
which measure of dispersion is to be adopted!

Nevertheless, perhaps francophone statisticians will have a
"default" interpretation of "dispersion" in such a context.

With thanks for any help,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Jul-05                                       Time: 09:30:04
------------------------------ XFMail ------------------------------



From mi2kelgrum at yahoo.com  Fri Jul  8 10:38:33 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Fri, 8 Jul 2005 01:38:33 -0700 (PDT)
Subject: [R] xmat[1, 2:3] <- NULL
In-Reply-To: <42CD69C3.2090005@statistik.uni-dortmund.de>
Message-ID: <20050708083834.58963.qmail@web60221.mail.yahoo.com>

Thanks for this! This is even simpler than using
!is.null():

xmat <- as.data.frame(matrix(NA, 2, 3))
try(xmat[i, 2:3] <- dbGetQuery(...), silent = TRUE)

Best wishes,
Mikkel

--- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote:

> Mikkel Grum wrote:
> 
> > I have a situation where I'm filling out a
> dataframe
> > from a database. Sometimes the database query
> doesn't
> > get anything, so I end up trying to place NULL in
> the
> > dataframe like below.
> > 
> > 
> >>temp <- NULL
> >>xmat <- as.data.frame(matrix(NA, 2, 3))
> >>xmat[1, 2:3] <- temp
> > 
> > Error in if (m < n * p && (n * p)%%m)
> > stop(gettextf("replacement has %d items, need %d",
>  : 
> >         missing value where TRUE/FALSE needed
> > 
> > I can't get the programme to accept that sometimes
> > what the query looks for just doesn't exist, and I
> > just want to move on to the next calculation
> leaving
> > the dataframe with a missing value in the given
> cell.
> > It's a real show stopper and I haven't found a way
> > round it.
> 
> 
> See ?try
> 
> Uwe Ligges
> 
> 
> > Best wishes,
> > Mikkel
> > 
> > PS. I'm using dbGetQuery to query an SQLite
> database.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From tschoenhoff at gmail.com  Fri Jul  8 10:54:15 2005
From: tschoenhoff at gmail.com (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Fri, 8 Jul 2005 10:54:15 +0200
Subject: [R] Update on SuSE-9.2 fails
Message-ID: <200507081054.15490.tom_hoary@web.de>

Hi,

yesterday, I switched to newest version of R-2.1.1 by using YAst to 
install necessary SuSE-rpms from a separate local directory. So far , 
so good.

Afterwards I tried to find out if there are any updates for add-on 
packages, and of cause there were some of them requiring a 
refreshment.

Following this procedure: (as root) R> update.packages()

most packages were upgraded without complaint. Nevertheless, at least 
some of them failed to do so:

Matrix, debug, eha, fBasics, glmmML, mgcv, tseries,lme4

So, I've looked up mailinglists archive to find a quick fix suggested 
by B.D. Ripely, copying  R binary from /usr/bin/R to /usr/lib/R/bin 
(due to problems in hard-coded paths, IIRC).

Well, that's exactly what I did without any merit, so far!

So I started to re-install above mentioned packages, ending up where I 
was before by:

#R> install.packages("package", dependencies=TRUE)


---------------------------------error message-----------------------

1: installation of package 'Matrix' had non-zero exit status in: 
install.packages(update[, "Package"], instlib, contribute = 
contriburl,
--------------------------------------------------------------------------

Seems like I am stuck for the moment, any suggestions to head on?




My system:

SuSE-9.2
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.1
year     2005
month    06
day      20
language R



From phgrosjean at sciviews.org  Fri Jul  8 10:54:16 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 08 Jul 2005 10:54:16 +0200
Subject: [R] [OT] "Dispersion" in French
In-Reply-To: <XFMail.050708093008.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050708093008.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42CE3F38.5080006@sciviews.org>

Hello Ted,

I would interpret this sentence the same way as you do. for me, in 
French, "dispersion" is a general term and there is no clue of which 
dispersion measurement was used... perhaps elsewhere in your text?
Best,

Philippe

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

(Ted Harding) wrote:
> Greetings,
> 
> I'm posting this OT query here because of out very international
> membership!
> 
> In the French sentence
> 
>   "Les taux de tirage sont calcul??s de mani??re ?? ce que la
>    dispersion soit inf??rieure ?? 5 % dans chaque strate."
> 
> it would seem intended that the "dispersion" is to be calculated
> in a specific way (unstated) -- otherwise, how to ensure that it
> shall be less than 5%?
> 
> So my question is: What, in English, would one use for "dispersion"?
> 
> Standard deviation?
> 
> Just as in English, also in general French usage in statistics,
> "dispersion" is a general term for which one can adopt various
> "mesures de dispersion" such as "??cart standard", etc. So in
> principle this sentence is as meaningful as the English "The sample
> sizes are to be calculated so as to ensure that the dispersion
> is less then 5% in each stratum." I would still want to know
> which measure of dispersion is to be adopted!
> 
> Nevertheless, perhaps francophone statisticians will have a
> "default" interpretation of "dispersion" in such a context.
> 
> With thanks for any help,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 08-Jul-05                                       Time: 09:30:04
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From mi2kelgrum at yahoo.com  Fri Jul  8 11:01:50 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Fri, 8 Jul 2005 02:01:50 -0700 (PDT)
Subject: [R]  about image() function in R and colors
Message-ID: <20050708090150.32136.qmail@web60211.mail.yahoo.com>

Javier, on the image and colors, did you try using the
breaks option? You will need 21 breaks to display 20
colors (see ?image).

Janek, the option asp = 1 should generally solve your
scaling problem (point 2 below). I don't remember
where I got that from, but it works unless you use
functions like abline which will take up the whole
plot region. Then you can adjust the plot region using

pin = c(diff(range(a*x)), diff(range(a*y)))

see ?par

Best wishes,
Mikkel


R] about image() function in R and colors
Tuszynski, Jaroslaw W. JAROSLAW.W.TUSZYNSKI at
saic.com
Thu Jul 7 14:22:27 CEST 2005

    * Previous message: [R] Tables: Invitation to make
a collective package
    * Next message: [R] Kernlab: problem with small
datasets
    * Messages sorted by: [ date ] [ thread ] [
subject ] [ author ]

I do not know the solution to your problem, but I had
the similar problems
with image() function and most of derived functions.
It seems that 'image'
function was really not meant for displaying image
data, instead it was
designed to display matrices in the image format.
Matlab had the same
problem and ended up creating 2 functions: 'image'
(similar to R's 'image')
and 'imshow' (designed for displaying image data). 

There are three major processing steps in the 'image'
that are hard to
control or reverse:
1) scaling of the data intensities ( problem explained
by Javier). Scaling
is not much of a problem if continuous palette of
colors is used , or to
quote
"?image" when "'col' is a list of colors such as that
generated by
'rainbow', 'heat.colors', 'topo.colors',
'terrain.colors' or similar
functions". However scaling causes problems in case of
discontinuous
color-maps (palettes).
2) scaling of the data dimensions so it fits in
default size window instead
of scaling of the window to fit the data. This results
in image with
non-square pixels. There might be a way to force
'image' not to scale
dimensions, I just did not spend much time looking for
it yet.
3) Flipping of the image. As "?image" shows one needs
to "transpose and flip
matrix horizontally" or perform
"image(t(volcano)[ncol(volcano):1,])" for
the image data to be visualized in proper orientation.

All those steps make sense in case of visualizing 2D
data, but they are a
hindrance in case of visualizing images.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \

 Science Applications International Corporation 
<\__,|  
 (703) 676-4192                                   ">  
\
 Jaroslaw.W.Tuszynski at saic.com                    
`    \

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf
Of javier garcia
Sent: Thursday, July 07, 2005 7:23 AM
To: R-Help
Cc: statsgrass at grass.itc.it
Subject: [R] about image() function in R and colors

Hi!

I've got a map in R imported from a GIS (GRASS) as a
vector of factors.
So I've got 20 different levels in the map and I've
created a vector of
custom colors of exactly 20 colors in lenght.
I'm trying to use image()  (really plot.grassmeta()
that call image()) to
plot the map with those colors but it doesnt work and
the colors are
changed.

I would like that all points belonging to level1 are
color 1 , and so on...

Please could you tell me if this procedure is not
correct?


Best regards,

Javier  

--
A. Javier Garcia
Water and Soil conservation department
CEBAS-CSIC
Campus Universitario Espinardo
PO BOX 164
30100 Murcia (SPAIN)
Phone: +34 968 39 62 57
Fax: +34 968 39 62 13
email: rn001 at cebas.csic.es

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

    * Previous message: [R] Tables: Invitation to make
a collective package
    * Next message: [R] Kernlab: problem with small
datasets
    * Messages sorted by: [ date ] [ thread ] [
subject ] [ author ]




		
__________________________________ 

Stay connected, organized, and protected. Take the tour:



From dieter.menne at menne-biomed.de  Fri Jul  8 11:03:20 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 8 Jul 2005 09:03:20 +0000 (UTC)
Subject: [R] Problems with nlme: Quinidine example
References: <1120736219.42cd13dbd8fa2@webmail.mi.fu-berlin.de>
Message-ID: <loom.20050708T105550-909@post.gmane.org>

 <rich <at> mi.fu-berlin.de> writes:

> 
> This concerns the "Clinical Study of Quinidine" example on page 380
> of the book "Mixed-Effects Models in S and S-PLUS" by Pinheiro and Bates 
(2000).
> 
> I have tried to reproduce the example, but get an error:
...... 
>         system is computationally singular: reciprocal condition number =
> 6.61723e-17
> 

In R, the underlying non-linear solver is different from that used in S. In 
general, I found the S-version converged in more cases than the R-version, but 
the opposite may also be true.

Sometimes it helps to fiddle with the control parameters on nlme, for example I 
got around some ping-ponging by setting pnlsTol to a large value, but check the 
sanity of your results. Looks like this will not help in this case, as in 
library\nlme\scripts\ch08.R Douglas Bates has added the following:

## This fit just ping-pongs.
#fm1Quin.nlme <-
#  nlme(conc ~ quinModel(Subject, time, conc, dose, interval,
#                        lV, lKa, lCl),

The master has spoken, no chance. It's a good idea to check these files if an 
example from the book does not work.

Dieter Menne



From vito_ricci at yahoo.com  Fri Jul  8 11:13:43 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 8 Jul 2005 11:13:43 +0200 (CEST)
Subject: [R] Orthogonal regression
Message-ID: <20050708091343.1065.qmail@web41203.mail.yahoo.com>

Dear R-Users,

is there any statement to fit a orthogonal regression
in R environment?

Many thanks in advance.

Best regards,

Vito


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From lars.claussen at pik-potsdam.de  Fri Jul  8 11:32:36 2005
From: lars.claussen at pik-potsdam.de (Lars)
Date: Fri, 08 Jul 2005 11:32:36 +0200
Subject: [R] Making Package, Chm error, Html Help Workshop
Message-ID: <42CE4834.7050301@pik-potsdam.de>

Hi Matthias,

I had the same problem a couple of weeks ago. It wasn't easy to come 
around it but i made it.
Did you use the skeloton function in R?
If yes:
Did you fill out both the discripton-form and the help-pages?
Did you follow all the further steps recommended?
Did you point in the make-file direcetly to the place where you hhc.exe 
resides on your system?
What error messages gives you the error.log file in the rCheck-directory?
Maybe i can help you if you specify the whole procedure.


Greats, Lars



From murdoch at stats.uwo.ca  Fri Jul  8 13:34:19 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Jul 2005 07:34:19 -0400
Subject: [R] fail in adding library in new version.
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF27@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF27@ex120.smic-sh.com>
Message-ID: <42CE64BB.4090502@stats.uwo.ca>

Ivy_Li wrote:
> Dear all,
> 	I really appreciate your help. I think I have a little advancement. ^_^
> 	
> 	When I enter the Dos environment, at first, into the D:\>, I type the following code:
> cd Program Files\R\rw2011\
> bin\R CMD install /example
> 
> "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.

Read the R Extensions manual for a detailed description.  You can use 
the package.skeleton() function to create a template, but you need to 
edit it to make it acceptable.
> 
> Well, there are still aother error:
> 
> 	---------- Making package example ------------
> 	  adding build stamp to DESCRIPTION
> 	error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format

That's complaining about your bad DESCRIPTION file.

Duncan Murdoch
> 	Stop execute
> 	make[2]: *** [frontmatter] Error 1
> 	make[1]: *** [all] Error 2
> 	make: *** [pkg-example] Error 2
> 	*** Installation of example failed ***
> 	Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> Please tell me which step is wrong?
> Thanks a lot!
> 
> BG
> Ivy_Li
>  
> 
> ----------
> : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> : 200577 20:57
> : Uwe Ligges
> : Ivy_Li; r-help at stat.math.ethz.ch
> : Re: : : [R] fail in adding library in new version.
> 
> 
> On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Gabor Grothendieck wrote:
>>
>>
>>>On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>>
>>>
>>>>Ivy_Li wrote:
>>>>
>>>>
>>>>
>>>>>Dear all,
>>>>>     I have done every step as the previous mail.
>>>>>1. unpack tools.zip into c:\cygwin
>>>>>2. install Active perl in c:\Perl
>>>>>3. install the mingw32 in c:\mingwin
>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
>>>>
>>>>                  ^
>>>>such blanks are not allowed in the PATH variable
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>>     Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
>>>>>      So in the Dos environment, at first, into the D:\>, I type the following code:
>>>>>cd Program Files\R\rw2011\
>>>>
>>>>MyRpackages does not need to be here.
>>>>
>>>>
>>>>
>>>>>bin\R CMD install /MyRpackages/example
>>>>
>>>>The first slash in "/MyRPackages" sugests that this is a top level
>>>>directory, which does not exist.
>>>>Even better, cd to MyRpackages, add R's bin dir to your path variable,
>>>>and simply say:
>>>>
>>>>R CMD INSTALL example
>>>
>>>
>>>Another possibility is to put Rcmd.bat from the batch file collection
>>>
>>>   http://cran.r-project.org/contrib/extra/batchfiles/
>>>
>>>in your path.  It will use the registry to find R so you won't have
>>>to modify your path (nor would you have to remodify it every time you
>>>install a new version of R which is what you would otherwise have to do):
>>>
>>>cd \MyPackages
>>>Rcmd install example
>>
>>
>>Just for the records:
>>
>>1. "cd \MyPackages" won't work, as I have already explained above.
> 
> 
> If MyPackages is not a top level directory in the current drive
> then it will not work. Otherwise it does work.
> 
> 
>>2. I do *not* recommend this way, in particular I find it misleading to
>>provide these batch files on CRAN.
>>
> 
> 
> The alternative, at least as discussed in your post, is more work
> since one will then have to change one's path every time one
> reinstalls R.  This is just needless extra work and is error prone.  If you
> forget to do it then you will be accessing the bin directory of the
> wrong version of R.
> 
> 
>>>>>     There are some error:
>>>>>'make' is neither internal or external command, nor executable operation or batch file
>>>>>*** installation of example failed ***
>>>>
>>>>Well, make.exe is not find in your path. Please check whether the file
>>>>exists and the path has been added.
>>>>
>>>>Uwe Ligges
>>>>
>>>>
>>>>
>>>>
>>>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
>>>>>
>>>>>I think I have closed to success. heehee~~~~~
>>>>>Thank you for your help.
>>>>>I still need you and others help. Thank you very much!
>>>>>
>>>>>
>>>>>----------
>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>>>>: 2005630 19:16
>>>>>: Ivy_Li
>>>>>: r-help at stat.math.ethz.ch
>>>>>: Re: : [R] fail in adding library in new version.
>>>>>
>>>>>
>>>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>>>>
>>>>>
>>>>>
>>>>>>Dear Gabor,
>>>>>>     Thank your for helping me so much!
>>>>>>     I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
>>>>>>1. unpack tools.zip into c:\cygwin
>>>>>>2. install Active perl in c:\Perl
>>>>>>3. install the mingw32 in c:\mingwin
>>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
>>>>>
>>>>>
>>>>>If in the console you enter the command:
>>>>>
>>>>>path
>>>>>
>>>>>then it will display a semicolon separated list of folders.  You want the folder
>>>>>that contains the tools to be at the beginning so that you eliminate
>>>>>the possibility
>>>>>of finding a different program of the same name first in a folder that comes
>>>>>prior to the one where the tools are stored.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
>>>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
>>>>>>
>>>>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
>>>>>>cd \Program Files\R\rw2010
>>>>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
>>>>>
>>>>>
>>>>>I was assuming that MyRPackages and R are on the same disk.  If they are not
>>>>>then you need to specify the disk too.  That is if MyRPackages is on C and R
>>>>>is installed on D then install your package via:
>>>>>
>>>>>d:
>>>>>cd \Program Files\R\rw2010
>>>>>bin\R CMD install c:/MyRPackages/example
>>>>>
>>>>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
>>>>>using command script install and the indicated source package.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
>>>>>
>>>>>
>>>>>If you are not sure where R is installed then enter the following at the Windows
>>>>>console prompt to find out (this will work provided you let it install the key
>>>>>into the registry when you installed R initially).  The reg command is a command
>>>>>built into Windows (I used XP but I assume its the same on other versions)
>>>>>that will query the Windows registry:
>>>>>
>>>>>reg query hklm\software\r-core\r /v InstallPath
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>I still need your and others help. Thank you very much!
>>>>>>
>>>>>>
>>>>>>
>>>>>>----------
>>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>>>>>: 200566 10:21
>>>>>>: Ivy_Li
>>>>>>: r-help at stat.math.ethz.ch
>>>>>>: Re: [R] fail in adding library in new version.
>>>>>>
>>>>>>
>>>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>>>>>
>>>>>>
>>>>>>
>>>>>>>Hello everybody,
>>>>>>>     Could I consult you a question?
>>>>>>>     I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
>>>>>>
>>>>>>Getting the latest version of R is strongly recommended.  The suggestions
>>>>>>below all assume the latest version and may or may not work if you do
>>>>>>not upgrade.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
>>>>>>>*       Download the tools.zip
>>>>>>>*       Unpack tools.zip into c:\cygwin
>>>>>>>*       Install Active Perl in c:\Perl
>>>>>>>*       Install the mingw32 port of gcc in c:\mingwin
>>>>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
>>>>>>
>>>>>>You may need to put these at the beginning of the path rather than the end.
>>>>>>Also just as a check enter
>>>>>>  path
>>>>>>at the console to make sure that you have them.  You will likely
>>>>>>have to start a new console session and possibly even reboot.
>>>>>>
>>>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
>>>>>>you reread the material on which tools you need.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
>>>>>>
>>>>>>In MyRPackages you would have a folder called example, in your case,
>>>>>>that contains the package.  Within folder example, you would have the
>>>>>>DESCRIPTION file, the R folder, etc.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
>>>>>>
>>>>>>You don't have to run R first.  You do need to make sure that R.exe can
>>>>>>be found on your path or else use the absolute path name in referring to R.
>>>>>>For example, if your path does not include R you could do something like this:
>>>>>>
>>>>>>cd \Program Files\R\rw2010
>>>>>>bin\R cmd install /MyRPackages/example
>>>>>
>>>>>
>>>>>Sorry, there is an error in the above.  It should be:
>>>>>
>>>>>bin\R CMD install c:/MyRPackages/example
>>>>>
>>>>>or
>>>>>
>>>>>bin\Rcmd install c:/MyRPackages/example
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>Be sure to use forward slashes where shown above and backslashes
>>>>>>where shown.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>     So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
>>>>>>>
>>>>>>
>>>>>>Try all these suggestions including upgrading R and if that does not work
>>>>>>try posting screen dumps of the actual errors you are getting.
>>>>>>
>>>>>
>>>>>
>>>>>Also try googling for
>>>>>
>>>>> making creating R packages
>>>>>
>>>>>and you will find some privately written tutorials on all this.
>>>>>
>>>>>
>>>>>
>>>>>------------------------------------------------------------------------
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Fri Jul  8 13:44:26 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Jul 2005 07:44:26 -0400
Subject: [R] r: LOOPING
In-Reply-To: <42CE2BA9.9000102@gmail.com>
References: <42CCE0E7.A9D9C17D@STATS.uct.ac.za>	<42CD012C.1090707@statistik.uni-dortmund.de>
	<42CE2BA9.9000102@gmail.com>
Message-ID: <42CE671A.5080907@stats.uwo.ca>

Vehbi Sinan Tunalioglu wrote:
> Uwe Ligges wrote:
> 
>>Clark Allan wrote:
>>
>>>i know that one should try and limit the amount of looping in R
>>>programs. i have supplied some code below. i am interested in seeing how
>>>the code cold be rewritten if we dont use the loops.
>>
>>It is not always a good thing to remove loops (without having looked at 
>>each details of the code below).
>>"Compromise" is the keyword.
> 
> 
> If you have big routines for each iteration and especially you have back
> references in the data structures you are manipulating, it becomes
> really _hard to translate_ loop statements to filter-map-accumulator
> routines provided by R (i.e. *apply functions). I really cannot find my
> way in some situations. What I did so far is to write those routines in
> C. Dirty hack :(

It shouldn't be so hard to do this in R.  Functions have access to the 
variables that are visible in the environment where they were created. 
So something like

x <- rnorm(10)
for (i in 2:10) {
    print(x[i]-x[i-1])
}

can be translated pretty literally as

x <- rnorm(10)
sapply(2:10, function(i) print(x[i]-x[i-1]))

This is not true in S-PLUS, so if you want portability, you'll need to 
do it in a different way.

> Maybe one should write a tutorial: "Howto avoid loops in R" by giving
> possible scenarios.

I don't think it's necessary.  Loops really aren't so bad.

Duncan Murdoch



From HDoran at air.org  Fri Jul  8 13:49:08 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 8 Jul 2005 07:49:08 -0400
Subject: [R] Possible Solution to Tempfile error (for documentation)
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409748D4A@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050708/73118861/attachment.pl

From murdoch at stats.uwo.ca  Fri Jul  8 13:51:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Jul 2005 07:51:42 -0400
Subject: [R] pairs() uses col argument for axes coloring
In-Reply-To: <d386700a05070719276e6c3013@mail.gmail.com>
References: <d386700a05070719276e6c3013@mail.gmail.com>
Message-ID: <42CE68CE.9040305@stats.uwo.ca>

Olaf Mersmann wrote:
> Hi list,
> 
> not sure if this is the wanted behavior, but running the following code:

I'd say it's a bug.
> 
> 
>>version
> 
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20         
> language R
> 
>>n <- 500
>>d <- 4
>>m <- matrix(runif(n*d, -1, 1), ncol=d)
>>c <- hsv(apply(m, 1, function(x) {sum(x*x)/d}), 1, 1)
>>pairs(m, col=c)
> 
> 
> gives me the desired coloring of the points but also colors the axes.
> Looking at the source for pairs() suggests, that this is the case
> because col is part of the ... argument list which is passed on to
> localAxis (and from there to axis). Wouldn't it be more approptiate to
> use the same color box() uses to draw the border around each
> scatterplot? If yes, should I open a bug for this or how would such a
> feature request be handled?

The best way is to get the source to pairs() (from the SVN repository 
in https://svn.r-project.org/R/trunk/src/library/graphics/R/pairs.R not 
from looking at it in R), work out what changes are needed, and submit 
them as a patch.  You can post the patch in the R-bugs list, or just 
send it to me.   Thanks!

Duncan Murdoch



From dmbates at gmail.com  Fri Jul  8 14:01:07 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 8 Jul 2005 07:01:07 -0500
Subject: [R] Problems with nlme: Quinidine example
In-Reply-To: <loom.20050708T105550-909@post.gmane.org>
References: <1120736219.42cd13dbd8fa2@webmail.mi.fu-berlin.de>
	<loom.20050708T105550-909@post.gmane.org>
Message-ID: <40e66e0b05070805013b709e30@mail.gmail.com>

On 7/8/05, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
>  <rich <at> mi.fu-berlin.de> writes:
> 
> >
> > This concerns the "Clinical Study of Quinidine" example on page 380
> > of the book "Mixed-Effects Models in S and S-PLUS" by Pinheiro and Bates
> (2000).
> >
> > I have tried to reproduce the example, but get an error:
> ......
> >         system is computationally singular: reciprocal condition number =
> > 6.61723e-17
> >
> 
> In R, the underlying non-linear solver is different from that used in S. In
> general, I found the S-version converged in more cases than the R-version, but
> the opposite may also be true.
> 
> Sometimes it helps to fiddle with the control parameters on nlme, for example I
> got around some ping-ponging by setting pnlsTol to a large value, but check the
> sanity of your results. Looks like this will not help in this case, as in
> library\nlme\scripts\ch08.R Douglas Bates has added the following:
> 
> ## This fit just ping-pongs.
> #fm1Quin.nlme <-
> #  nlme(conc ~ quinModel(Subject, time, conc, dose, interval,
> #                        lV, lKa, lCl),
> 

Thanks for pointing that out Dieter.  I would have responded myself
except for more urgent matters intervening in the last few days.

The optimizer code that was (and, I imagine, still is) used in S-PLUS
will be available starting with R-2.2.0.  I have modified the lmer
function to use R's nlminb optimizer when available and will do the
same for the nlme package.  That should improve consistency of results
between the S-PLUS and the R versions of nlme.



From murdoch at stats.uwo.ca  Fri Jul  8 14:05:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Jul 2005 08:05:22 -0400
Subject: [R] Possible Solution to Tempfile error (for documentation)
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409748D4A@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409748D4A@dc1ex2.air.org>
Message-ID: <42CE6C02.9060502@stats.uwo.ca>

Doran, Harold wrote:
> Dear List:
> 
> I'm posting this to provide a possible solution and to document to what
> appears to be an R limitation. The solution is more of a cheap hack that
> works for now. To provide a little background, I am looping through a
> dataframe and creating Sweave documents using data from each row in the
> dataframe. It appears that this technique is not scalable to large
> dataframes without making some changes to the way tempfiles are handled.
> 
> In the background, R generates tempfiles in a directory using a
> sequential number. In my case, the numbers ranged from Rf1 to Rf32767.
> The next in line would of course be Rf32768. This number happens to
> coincide with 2^15-1, which my programmer colleagues tell me is the
> maximum positive value a software program with 16 bit processing can
> create. So, R recognizes that it cannot create a new tempfile and stops
> the loop.

In Windows (which I think you're using), R uses the C function rand() to 
generate a "random" filename.  This does have a maximum output of 32767. 
  It would be easy to change, but Windows file systems aren't 
particularly good at handling such large directories; maybe this limit 
is a sign that you need to change the algorithm?

> According to my collegues, programmers need to be aware of this
> particular number and often include some flexibility within the software
> to allow for the temp file settings to use an unsigned or 32 bit integer
> for temp file names. However, I do not believe R has this capacity. 
> 
> So, in the interim, I have created a call using shell() to go out the
> the OS and delete all temp files by brute force at certain intervals
> within the loop. Doing so after each iteration was prohibitively slow. I
> am still experimenting, but I have found that choosing some arbitrary
> number makes this solution feasible.
> 
> Within my for() loop, I have included the following conditional which
> goes out to DOS and deletes all temp files periodically. The portion of
> the shell command "/Q" is needed to make sure DOS does not prompt the
> user with the "Are you sure you want to delete the files". This bypasses
> that prompt and allows for the files to be deleted without confirmation.
> 
> 
> Here is the conditional within the for loop
> 
> if (i==700|i==1400|i==2100|i==2800|i==3500|i==4200){
> 	setwd(tempdir())
> 	shell(paste("del /Q *.*"))
> 	setwd("G:/path/where/I/want/Sweave/files")
> }

If this works, it indicates that the temp files are no longer needed by 
this point.  So the real question is, why are they still there? 
Shouldn't they have been deleted after they were used?

I wasn't following your previous posts, but did you create the tempfiles 
(in which case you should delete them once you were done with them), or 
did R?

Duncan Murdoch

> 
> I tested this solution and it has extended the life of my loop and
> appears to solve the problem. Hence, in the absence of making changes to
> the way tempfiles are handled inside the software, this has proven
> useful to me and maybe it will to others as well.
> 
> I'm not sure this is the best way to handle this and most welcome any
> better ideas, but it seems to provide a route that circumvents a
> problem.



From Matthias.Templ at statistik.gv.at  Fri Jul  8 15:08:24 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 8 Jul 2005 15:08:24 +0200
Subject: [R] Making Package, Chm error, Html Help Workshop [solved]
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAB40@xchg1.statistik.local>

Dear Uwe Ligges,

There was really a problem with an irregular name of a help topic (%cin% - the % was the problem) as you said. With the new R Version and with the corrected Rd file all works fine.

Thanks a lot for your help!
Matthias

> TEMPL Matthias wrote:
> 
> > Hello,
> > 
> > When building my package (R CMD check) following error
> message occurs:
> > ...
> >   varinf.plot  text     html     latex      example
> >   x            text     html     latex      example
> > make[2]: *** No rule to make target `disclosure.chm`. Stop.
> > cp: cannot stat
> > 'D:/Programme/R/rw2010dev/disclosure/chm/disclosure.chm`: 
> No such file or directory
> > make[1]: *** [chm-disclosure] Error 1
> > make: *** [pkg-disclosure] Error 2
> > *** Installation of disclosure failed ***
> > Removing `D:/Programme/R/rw2010dev/bin/disclosure.Rcheck/disclosure'
> >  ERROR
> > Installation failed.
> > 
> > It seems, that there is a problem with HTML Workshop. No chm??s were
> > built. When I uninstall the HTML Help Workshop and remove 
> the entry in
> > the path environmental variable and doing packaging after this, the
> > same error occurs. 
> I??m sure, that I have written the right
> entry (the 
> > path of the HTML help Workschop, where the hhc.exe file is) in the
> > path of the environmental variable as the instructions said.
> > 
> > (Windows XP, Intel, R 2.1.0, HTML Workshop 1.32)
> > 
> > Has anybody seen such a problem before?
> > Can anybody give me a hint, please?
> 
> 
> hhc.exe is found, because it is not reported that hhc is not
> there. I guess you have used an irregular name (from hhc's 
> point of view) as 
> the name or alias of a help topic, or you have an irregular 
> name of an 
> Rd file.
> Have you tried with a recent and released version of R such 
> as R-2.1.1? 
> Your's is unreleased, obviously.
> If it still won't work, please sent me the source package and I will 
> take a look.
> 
> Uwe Ligges
> 
> 
> 
> > Thank you very much,
> > Matthias
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
>



From renaud.lancelot at cirad.fr  Fri Jul  8 15:40:03 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Fri, 08 Jul 2005 15:40:03 +0200
Subject: [R] [OT] "Dispersion" in French
In-Reply-To: <XFMail.050708093008.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050708093008.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42CE8233.7070605@cirad.fr>

(Ted Harding) a ??crit :
> Greetings,
> 
> I'm posting this OT query here because of out very international
> membership!
> 
> In the French sentence
> 
>   "Les taux de tirage sont calcul??s de mani??re ?? ce que la
>    dispersion soit inf??rieure ?? 5 % dans chaque strate."
> 
> it would seem intended that the "dispersion" is to be calculated
> in a specific way (unstated) -- otherwise, how to ensure that it
> shall be less than 5%?
> 
> So my question is: What, in English, would one use for "dispersion"?

I would translate it into "dispersion" or "variability".

> Standard deviation?

I don't think so. This is much more precise than "dispersion".

> Just as in English, also in general French usage in statistics,
> "dispersion" is a general term for which one can adopt various
> "mesures de dispersion" such as "??cart standard", etc. So in
> principle this sentence is as meaningful as the English "The sample
> sizes are to be calculated so as to ensure that the dispersion
> is less then 5% in each stratum." I would still want to know
> which measure of dispersion is to be adopted!

So would I.

Best,

Renaud

> 
> Nevertheless, perhaps francophone statisticians will have a
> "default" interpretation of "dispersion" in such a context.
> 
> With thanks for any help,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 08-Jul-05                                       Time: 09:30:04
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From p.dalgaard at biostat.ku.dk  Fri Jul  8 15:58:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jul 2005 15:58:51 +0200
Subject: [R] [OT] "Dispersion" in French
In-Reply-To: <42CE8233.7070605@cirad.fr>
References: <XFMail.050708093008.Ted.Harding@nessie.mcc.ac.uk>
	<42CE8233.7070605@cirad.fr>
Message-ID: <x2u0j52yck.fsf@biostat.ku.dk>

Renaud Lancelot <renaud.lancelot at cirad.fr> writes:

> (Ted Harding) a ??crit :
> > Greetings,
> > 
> > I'm posting this OT query here because of out very international
> > membership!
> > 
> > In the French sentence
> > 
> >   "Les taux de tirage sont calcul??s de mani??re ?? ce que la
> >    dispersion soit inf??rieure ?? 5 % dans chaque strate."
> > 
....snip....

> > Just as in English, also in general French usage in statistics,
> > "dispersion" is a general term for which one can adopt various
> > "mesures de dispersion" such as "??cart standard", etc. So in
> > principle this sentence is as meaningful as the English "The sample
> > sizes are to be calculated so as to ensure that the dispersion
> > is less then 5% in each stratum." I would still want to know
> > which measure of dispersion is to be adopted!
> 
> So would I.

Unless the base measurements are percentages (which they might), my
guess would be that it is the coefficient of variation that is being
stated as below 5%.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From davidr at rhotrading.com  Fri Jul  8 16:16:30 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 8 Jul 2005 09:16:30 -0500
Subject: [R] Orthogonal regression
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FBAE5@rhosvr02.rhotrading.com>

This topic has come up a few times recently, so it must be 'in the wind'

these days. Depending on what approach you take and what area you are
coming 
from, it goes under the names Orthogonal [Distance] Regression, Total
Least 
Squares, Errors in Variables, Deming Regression. The middle two
encompass 
much more in statistics terms (and I don't know what Deming did for
sure.)

If you have just two variables and you assume they are observed without 
error, then the regression slope is just the ratio of the standard 
deviations with the sign of the correlation coefficient, and the line
passes 
through the means. There are assumptions about the errors that will
produce 
the same result as a 'best linear fit,' but these assumptions about the 
errors may not fit your situation. Do some reading in the TLS / EIV
arena to 
see if you really want bald ODR.

HTH,
David L. Reiner
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
> Sent: Friday, July 08, 2005 4:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Orthogonal regression
> 
> Dear R-Users,
> 
> is there any statement to fit a orthogonal regression
> in R environment?
> 
> Many thanks in advance.
> 
> Best regards,
> 
> Vito
> 
> 
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze
> the scientific learning process."
> George E. P. Box
> 
> "Statistical thinking will one day be as necessary for efficient
> citizenship as the ability to read and write"
> H. G. Wells
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese
> http://www.modugno.it/archivio/palesesanto_spirito/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From mi2kelgrum at yahoo.com  Fri Jul  8 16:16:57 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Fri, 8 Jul 2005 07:16:57 -0700 (PDT)
Subject: [R] removing factor level represented by less than x rows
Message-ID: <20050708141657.90864.qmail@web60220.mail.yahoo.com>

In a number of different situations I'm trying to
remove factor levels that are represented by less than
a certain number of rows, e.g. if I had the dataset aa
below and wanted to remove the species that are
represented in less than 2 rows:

data(iris)
aa <- iris[1:101,]

In this case, since I can see that the species
virginica only has one row, I can write:

table(aa$Species)
setosa versicolor  virginica 
        50         50          1 

aa[aa$Species != "virginica", ]

but:

aa[aa$Species == names(table(aa$Species)> 2),]

does not work.

This must be a fairly common task with a straight
forward solution that I can't see. Any ideas?

Best wishes,
Mikkel



From fezzi at stat.unibo.it  Fri Jul  8 16:24:08 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Fri, 8 Jul 2005 16:24:08 +0200 (CEST)
Subject: [R] Garch in a model with explanatory variables
Message-ID: <3565145.1120832648935.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear helpers,

does anyone know a function to fit a model with:

- y mean that is regressed on a set of explanatory variables

- y variace behaving as a garch or as a garch in mean


Thank you so much for your help,

Carlo



From uofiowa at gmail.com  Fri Jul  8 16:33:31 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 8 Jul 2005 10:33:31 -0400
Subject: [R] segfault on q() using RODBC ... was q() ==> Segmentation fault
In-Reply-To: <3f87cc6d050707152555d6c3ea@mail.gmail.com>
References: <3f87cc6d050707152555d6c3ea@mail.gmail.com>
Message-ID: <3f87cc6d050708073316a32592@mail.gmail.com>

---------- Forwarded message ----------
From: Omar Lakkis <uofiowa at gmail.com>
Date: Jul 7, 2005 6:25 PM
Subject: q() ==> Segmentation fault
To: r-help at stat.math.ethz.ch


I created the simple library, attached. When I terminate an R session
where the library has been loaded with q() a segmentation fault is
thrown. Is there any cleaning that I should be doing?

>From R session:
> q()
Segmentation fault

or from shell:
$ R CMD BATCH r.in
/usr/lib/R/bin/BATCH: line 55: 17359 Done                    ( echo
"invisible(options(echo = TRUE))"; cat ${in}; echo "proc.time()" )
     17360 Segmentation fault      | ${R_HOME}/bin/R ${opts} >${out} 2>&1

Attached, please, find the librray tmc.tmp. The library contains the
three files below.
I am using R 2.1.0 and RODBC 1.1-3 on debian.

::::::::::::::
R/zzz.R
::::::::::::::
.First.lib <-
function (which.lib.loc, package, ...) {
        library(RODBC)

        connect()
}

.Last.lib <-
function (libpath, ...) {
}
::::::::::::::
R/db.R
::::::::::::::
connect <- function() {
        conn <<- odbcConnect('tmc', believeNRows = FALSE)
}

::::::::::::::
data/data.r
::::::::::::::
conn <- NULL

From sundar.dorai-raj at pdf.com  Fri Jul  8 16:39:31 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 08 Jul 2005 09:39:31 -0500
Subject: [R] removing factor level represented by less than x rows
In-Reply-To: <20050708141657.90864.qmail@web60220.mail.yahoo.com>
References: <20050708141657.90864.qmail@web60220.mail.yahoo.com>
Message-ID: <42CE9023.5040908@pdf.com>



Mikkel Grum wrote:
> In a number of different situations I'm trying to
> remove factor levels that are represented by less than
> a certain number of rows, e.g. if I had the dataset aa
> below and wanted to remove the species that are
> represented in less than 2 rows:
> 
> data(iris)
> aa <- iris[1:101,]
> 
> In this case, since I can see that the species
> virginica only has one row, I can write:
> 
> table(aa$Species)
> setosa versicolor  virginica 
>         50         50          1 
> 
> aa[aa$Species != "virginica", ]
> 
> but:
> 
> aa[aa$Species == names(table(aa$Species)> 2),]
> 
> does not work.
> 

If you take a look at "table(aa$Species) > 2" you'll see your first 
mistake. Namely, the names are all still present. Your second mistake is 
to use "==" to match two names. "==" does not work like that. What you 
want is "%in%" instead.

I think you want the following:

keep <- levels(aa$Species)[table(aa$Species) > 2]
aa <- aa[aa$Species %in% keep, ]

However, the level for "virginica" is still present in the Species 
variable. If you would like to drop this completely, then try

aa$Species <- aa$Species[drop = TRUE]

HTH,

--sundar



From f.hahne at dkfz-heidelberg.de  Fri Jul  8 17:00:21 2005
From: f.hahne at dkfz-heidelberg.de (Florian Hahne)
Date: Fri, 08 Jul 2005 17:00:21 +0200
Subject: [R] exact conditional mantelhaen.test estimate is 0 ?!
Message-ID: <1120834821.5330.28.camel@perro.inet.dkfz-heidelberg.de>

Dear listers,

I am trying to compute the exact conditional test given strata margins
of a 2 by 2 by K array using the mantelhaen.test function to get a
common odds ratio estimate.
The estimate for the test on the following data is 0, which in my
opinion dosen't make any sense.

x <- array(c(53, 6098, 1006, 4521, 63, 8070, 1163, 6137), dim=c(2,2,2))
 x
, , 1

     [,1] [,2]
[1,]   53 1006
[2,] 6098 4521

, , 2

     [,1] [,2]
[1,]   63 1163
[2,] 8070 6137

mantelhaen.test(x, exact=TRUE)
      Exact conditional test of independence in 2 x 2 x k tables

data:  x
S = 116, p-value < 2.2e-16
alternative hypothesis: true common odds ratio is not equal to 1
95 percent confidence interval:
 0.0000000000 0.0001190186
sample estimates:
common odds ratio
                0

Could this be some kind of memory overflow issue since some of the
values are quite high? Or am I completely off here? 

I would appreciate any help on this,

Florian 

-- 
Florian Hahne <f.hahne at dkfz-heidelberg.de>



From f.harrell at vanderbilt.edu  Fri Jul  8 17:16:23 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 08 Jul 2005 10:16:23 -0500
Subject: [R] removing factor level represented by less than x rows
In-Reply-To: <20050708141657.90864.qmail@web60220.mail.yahoo.com>
References: <20050708141657.90864.qmail@web60220.mail.yahoo.com>
Message-ID: <42CE98C7.6070403@vanderbilt.edu>

Mikkel Grum wrote:
> In a number of different situations I'm trying to
> remove factor levels that are represented by less than
> a certain number of rows, e.g. if I had the dataset aa
> below and wanted to remove the species that are
> represented in less than 2 rows:
> 
> data(iris)
> aa <- iris[1:101,]
> 
> In this case, since I can see that the species
> virginica only has one row, I can write:
> 
> table(aa$Species)
> setosa versicolor  virginica 
>         50         50          1 
> 
> aa[aa$Species != "virginica", ]
> 
> but:
> 
> aa[aa$Species == names(table(aa$Species)> 2),]
> 
> does not work.
> 
> This must be a fairly common task with a straight
> forward solution that I can't see. Any ideas?
> 
> Best wishes,
> Mikkel

library(Hmisc)
?combine.levels

This doesn't remove levels but combines infrequent ones though.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From york at zipcon.net  Fri Jul  8 17:20:39 2005
From: york at zipcon.net (Anne York)
Date: Fri, 8 Jul 2005 08:20:39 -0700 (PDT)
Subject: [R] pairs() uses col argument for axes coloring
In-Reply-To: <d386700a05070719276e6c3013@mail.gmail.com>
References: <d386700a05070719276e6c3013@mail.gmail.com>
Message-ID: <Pine.LNX.4.62.0507080817310.13703@sasquatch>

A work-around for this problem was posted on April 7 by Bill 
Venables and Deepayan Sarkar using a panel function:

Anne
On Thu, 7 Apr 2005, Deepayan Sarkar wrote:

DS > On Thursday 07 April 2005 17:51, Anne York wrote:
DS > > The following command produces red axis line in a 
pairs
DS > > plot:
DS > >
DS > > pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
DS > > pch = "+", col = c("red", "green3",  "blue")[unclass(iris$Species)])
DS > >
DS > >
DS > > Trying to fool pairs in the following  way  produces 
the
DS > > same plot as above:
DS > >
DS > > pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",pch =
DS > > "+", col = c("black", "red", "green3", "blue")[ 1+
DS > > unclass(iris$Species)])
DS > >
DS > > One very kludgy work-around is to define a new level 1, say
DS > > "foo" in the first row of iris:
DS > >
DS > > iris2=iris
DS > > iris2$Species = as.character(iris2$Species)
DS > > iris2$Species[1]="foo"
DS > > iris2$Species = factor(iris2$Species)
DS > >
DS > > pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
DS > > species", pch = "+",
DS > > col = c( "black","red", "green3","blue")[ 
unclass(iris2$Species)])
DS > >
DS > > However, if any other row is redefined, the red-axis
DS > > persists. For example:
DS > >
DS > > iris2=iris
DS > > iris2$Species = as.character(iris2$Species)
DS > > iris2$Species[3]="foo"
DS > > iris2$Species = factor(iris2$Species)
DS > >
DS > >
DS > > pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
DS > > species", pch = "+",
DS > > col = c( "black","red", "green3","blue")[ nclass(iris2$Species)])
DS > >
DS > > I'd appreciate suggestions for a simpler work-around.
DS > 
DS > One possibility is something along the lines of
DS > 
DS > pairs(iris[1:4], 
DS >       panel = function(...)
DS >           points(..., 
DS >                  col = c("red", "green3", "blue")
DS >                          [unclass(iris$Species)]  ))
DS > 
DS > Deepayan
DS > 
DS >



On Fri, 8 Jul 2005, Olaf Mersmann wrote:

OM > Hi list,
OM > 
OM > not sure if this is the wanted behavior, but running the following code:
OM > 
OM > > version
OM > platform i386-pc-mingw32
OM > arch     i386
OM > os       mingw32
OM > system   i386, mingw32
OM > status
OM > major    2
OM > minor    1.1
OM > year     2005
OM > month    06
OM > day      20         
OM > language R
OM > > n <- 500
OM > > d <- 4
OM > > m <- matrix(runif(n*d, -1, 1), ncol=d)
OM > > c <- hsv(apply(m, 1, function(x) {sum(x*x)/d}), 1, 1)
OM > > pairs(m, col=c)
OM > 
OM > gives me the desired coloring of the points but also colors the axes.
OM > Looking at the source for pairs() suggests, that this is the case
OM > because col is part of the ... argument list which is passed on to
OM > localAxis (and from there to axis). Wouldn't it be more approptiate to
OM > use the same color box() uses to draw the border around each
OM > scatterplot? If yes, should I open a bug for this or how would such a
OM > feature request be handled?
OM > 
OM > -- Olaf Mersmann
OM > 
OM > ______________________________________________
OM > R-help at stat.math.ethz.ch mailing list
OM > https://stat.ethz.ch/mailman/listinfo/r-help
OM > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
OM > 
OM >



From sfalcon at fhcrc.org  Fri Jul  8 17:48:54 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 08 Jul 2005 08:48:54 -0700
Subject: [R] ANN: BioC2005 Conference, online registration now available
Message-ID: <m2pstt5me1.fsf@macaroni.local>

BioC2005: Where Biology and Software Connect
August 16 and 17 in Seattle, WA, USA

The online registration form is now available.  

http://www.bioconductor.org/meeting05/

(You will be redirected to our secure server:
https://cobra.fhcrc.org/BioC2005/)

About BioC2005:

This conference will highlight current developments within and beyond
Bioconductor and provide a forum in which to discuss the use and
design of software for analyzing data arising in biology with a focus
on Bioconductor and genomic data.

For more details, visit the website.


Best,

+ seth



From michael.hopkins at hopkins-research.com  Fri Jul  8 17:50:05 2005
From: michael.hopkins at hopkins-research.com (Michael Hopkins)
Date: Fri, 08 Jul 2005 16:50:05 +0100
Subject: [R] Plotting a simple subset
Message-ID: <BEF45F3D.3E398%michael.hopkins@hopkins-research.com>



Hi all

Just converting from Stata to R and struggling a little to come to terms
with the new philosophy/command line.

E.g. I want to plot x against y if x < 5

In Stata: graph x y, if( x < 5 )

How do I do this in R?  Have tried most of the obvious options without
success.

Can I have multiple subsets? I.e. In Stata: if( x < 5 && y > 3 )

TIA 

Michael


_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/

        _/    _/   _/_/_/             Hopkins Research Ltd
       _/    _/   _/    _/
      _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
     _/    _/   _/   _/
    _/    _/   _/     _/               'touch the future'
                   
_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/



From gunter.berton at gene.com  Fri Jul  8 18:04:24 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 8 Jul 2005 09:04:24 -0700
Subject: [R] Plotting a simple subset
In-Reply-To: <BEF45F3D.3E398%michael.hopkins@hopkins-research.com>
Message-ID: <200507081604.j68G4OsX013536@hertz.gene.com>

Please first read "An Introduction to R" (one of the pdf manuals that ships
with R) before posting these sorts of questions, as it is written
specifically to help you get started (I think fairly clearly).

Other (links to) learning resources may be found on the CRAN website. Please
take advantage of them. One that you may find particularly helpful is:

Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
Yes, the initial learning curve is steep. However, if you do any serious
data analysis and/or plotting, I promise that you will find it well worth
your while.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Hopkins
> Sent: Friday, July 08, 2005 8:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plotting a simple subset
> 
> 
> 
> Hi all
> 
> Just converting from Stata to R and struggling a little to 
> come to terms
> with the new philosophy/command line.
> 
> E.g. I want to plot x against y if x < 5
> 
> In Stata: graph x y, if( x < 5 )
> 
> How do I do this in R?  Have tried most of the obvious options without
> success.
> 
> Can I have multiple subsets? I.e. In Stata: if( x < 5 && y > 3 )
> 
> TIA 
> 
> Michael
> 
> 
> _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
> _/_/_/_/_/_/_/
> 
>         _/    _/   _/_/_/             Hopkins Research Ltd
>        _/    _/   _/    _/
>       _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
>      _/    _/   _/   _/
>     _/    _/   _/     _/               'touch the future'
>                    
> _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
> _/_/_/_/_/_/_/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Jul  8 18:05:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 08 Jul 2005 18:05:20 +0200
Subject: [R] Error message NA/NaN/Inf in foreign function call (arg 6)
 when using knn()
In-Reply-To: <26683572.1120663752028.JavaMail.tomcat@roadmaster>
References: <26683572.1120663752028.JavaMail.tomcat@roadmaster>
Message-ID: <42CEA440.50408@statistik.uni-dortmund.de>

Kerri-Ann Norton wrote:

> I am trying to use knn to do a nearest neighbor classification.  I tried using my dataset and got an error message so I used a simple example to try and understand what I was doing wrong and got the same message.  Here is what I typed into R:
>  try
>   [,1] [,2] [,3] [,4]
> r "A"  "A"  "T"  "G" 
> r "A"  "A"  "T"  "G" 
> f "A"  "A"  "c"  "G" 
> f "A"  "A"  "c"  "G" 
> f "A"  "A"  "c"  "G" 
> 
>>cl2 <-factor(c(rep("1",2), rep("2",3)))
>>cl2
> 
> [1] 1 1 2 2 2
> Levels: 1 2
> 
>>knn(try, try, cl2, k = 2)
> 
> Error in knn(try, try, cl2, k = 2) : NA/NaN/Inf in foreign function call (arg 6)
> In addition: Warning messages: 
> 1: NAs introduced by coercion 
> 2: NAs introduced by coercion 
> 
> I used try as test and train because I thought the error might be that the size of test and train data were different.  If someone could explain what the error means or how to fix it, I would greatly appreciate it.
> Kerri-Ann Norton
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


What is the euclidean distance between "c" and "T"?

I think this is a user error which the code does not intercept with a 
nice error message.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Fri Jul  8 18:22:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 08 Jul 2005 18:22:09 +0200
Subject: [R] how to do something like   symptoms==c('a', 'e', 'z')
In-Reply-To: <20050708181511.3a96f592@zygiella.local>
References: <20050708181511.3a96f592@zygiella.local>
Message-ID: <42CEA831.4040900@statistik.uni-dortmund.de>

RenE J.V. Bertin wrote:

> I find myself doing lots of tests like
> 
> 
>>subset( data, symptoms=='a' | symptoms=='e' | symptoms=='z' .... )
> 
> 
> with symptoms one of the factors contained in the data frame.
> 
> and I wonder if there is not an existing operator or function which implements this sort of repeated conditional in a more space-efficient fashion, something like
> 
> 
>>subset( data, symptoms==c('a','e','z') )


You are looking for %in%:

   subset(data, symptoms %in% c('a','e','z'))

Uwe Ligges


> 
> 
> (which is incorrect unless symptoms is, in this case, an integer multiple of 3 long).
> 
> Is there, or if not, is there a more efficient/elegant way than 
> 
> select.elements <- function(data, lst)
> {
> 	slctn <- data == lst[1]
> 	for( e in 2:length(lst) ){
> 		slctn <- slctn | (data==lst[e])
> 	}
> 	slctn
> }
> 
> 
> ##> select.elements( 1:10, c(1,5,10) )
>  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE
> 
> Thanks,
> RenE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Fri Jul  8 18:24:39 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 08 Jul 2005 12:24:39 -0400
Subject: [R] how to do something like   symptoms==c('a', 'e', 'z')
In-Reply-To: <20050708181511.3a96f592@zygiella.local>
References: <20050708181511.3a96f592@zygiella.local>
Message-ID: <42CEA8C7.7040504@optonline.net>

RenE J.V. Bertin wrote:
> I find myself doing lots of tests like
> 
> 
>>subset( data, symptoms=='a' | symptoms=='e' | symptoms=='z' .... )
> 
> 
> with symptoms one of the factors contained in the data frame.
> 
> and I wonder if there is not an existing operator or function which implements this sort of repeated conditional in a more space-efficient fashion, something like

   How about something like this:

mydata <- data.frame(Y = runif(100), X = rep(letters[1:5], 20))

subset(mydata, X %in% c("a", "c", "d"))

>>subset( data, symptoms==c('a','e','z') )

subset(data, symptoms %in% c("a", "e", "z"))

> 
> (which is incorrect unless symptoms is, in this case, an integer multiple of 3 long).
> 
> Is there, or if not, is there a more efficient/elegant way than 
> 
> select.elements <- function(data, lst)
> {
> 	slctn <- data == lst[1]
> 	for( e in 2:length(lst) ){
> 		slctn <- slctn | (data==lst[e])
> 	}
> 	slctn
> }
> 
> 
> ##> select.elements( 1:10, c(1,5,10) )
>  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE
> 
> Thanks,
> RenE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From s-plus at wiwi.uni-bielefeld.de  Fri Jul  8 18:24:41 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 08 Jul 2005 18:24:41 +0200
Subject: [R] Plotting a simple subset
References: <BEF45F3D.3E398%michael.hopkins@hopkins-research.com>
Message-ID: <42CEA8C9.3030505@wiwi.uni-bielefeld.de>

Michael Hopkins wrote:

>Hi all
>
>Just converting from Stata to R and struggling a little to come to terms
>with the new philosophy/command line.
>
>E.g. I want to plot x against y if x < 5
>
>In Stata: graph x y, if( x < 5 )
>
>How do I do this in R?  Have tried most of the obvious options without
>success.
>
>Can I have multiple subsets? I.e. In Stata: if( x < 5 && y > 3 )
>

Try:

x<-rnorm(20,0,5)
y<-2*x+rnorm(20)
# plot without condition
plot(x,y)
# some conditions on x and y
index<- x<5 & y>3
plot(x[index],y[index])

Peter Wolf

>
>TIA 
>
>Michael
>
>
>_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
>
>        _/    _/   _/_/_/             Hopkins Research Ltd
>       _/    _/   _/    _/
>      _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
>     _/    _/   _/   _/
>    _/    _/   _/     _/               'touch the future'
>                   
>_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at gmail.com  Fri Jul  8 18:24:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 12:24:26 -0400
Subject: [R] Plotting a simple subset
In-Reply-To: <200507081604.j68G4OsX013536@hertz.gene.com>
References: <BEF45F3D.3E398%michael.hopkins@hopkins-research.com>
	<200507081604.j68G4OsX013536@hertz.gene.com>
Message-ID: <971536df050708092410da0ccd@mail.gmail.com>

To be fair none of Introduction to R, ?plot nor the reference card
really cover this without substantial digging.  

# test data
x <- 1:10
y <- x*x

plot(x[x > 5], y[x > 5])

# or 

plot(y ~ x, subset = x > 5)

# We can have combine conditions like this: 

plot(y ~ x, subset = x > 5 & y < 50)

# also if your intention was really set the plot limits rather than
# condition on the data then you can use xlim= and ylim=, e.g.

plot(y ~ x, xlim = c(5, max(x)))


Read over all of these: ?plot, ?plot.formula, ?plot.default, ?"&", ?">",
?c, ?max noting that plot dispatches plot.formula if you specify a formula
in plot and that subset= is an argument to the latter.


On 7/8/05, Berton Gunter <gunter.berton at gene.com> wrote:
> Please first read "An Introduction to R" (one of the pdf manuals that ships
> with R) before posting these sorts of questions, as it is written
> specifically to help you get started (I think fairly clearly).
> 
> Other (links to) learning resources may be found on the CRAN website. Please
> take advantage of them. One that you may find particularly helpful is:
> 
> Newbies (and others!) may find useful the R Reference Card made available by
> Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
> the "Contributed" link on CRAN (where some other reference cards are also
> linked). It categorizes and organizes a bunch of R's basic, most used
> functions so that they can be easily found. For example, paste() is under
> the "Strings" heading and expand.grid() is under "Data Creation." For
> newbies struggling to find the right R function as well as veterans who
> can't quite remember the function name, it's very handy.
> 
> Yes, the initial learning curve is steep. However, if you do any serious
> data analysis and/or plotting, I promise that you will find it well worth
> your while.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
> 
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
> 
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Hopkins
> > Sent: Friday, July 08, 2005 8:50 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Plotting a simple subset
> >
> >
> >
> > Hi all
> >
> > Just converting from Stata to R and struggling a little to
> > come to terms
> > with the new philosophy/command line.
> >
> > E.g. I want to plot x against y if x < 5
> >
> > In Stata: graph x y, if( x < 5 )
> >
> > How do I do this in R?  Have tried most of the obvious options without
> > success.
> >
> > Can I have multiple subsets? I.e. In Stata: if( x < 5 && y > 3 )
> >
> > TIA
> >
> > Michael
> >
> >
> > _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
> > _/_/_/_/_/_/_/
> >
> >         _/    _/   _/_/_/             Hopkins Research Ltd
> >        _/    _/   _/    _/
> >       _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
> >      _/    _/   _/   _/
> >     _/    _/   _/     _/               'touch the future'
> >
> > _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
> > _/_/_/_/_/_/_/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From michael.hopkins at hopkins-research.com  Fri Jul  8 18:37:01 2005
From: michael.hopkins at hopkins-research.com (Michael Hopkins)
Date: Fri, 08 Jul 2005 17:37:01 +0100
Subject: [R] Plotting a simple subset
In-Reply-To: <42CEA8C9.3030505@wiwi.uni-bielefeld.de>
Message-ID: <BEF46A3D.3E3A8%michael.hopkins@hopkins-research.com>


Thanks to all of the respondents for helpful (except one) and super-fast
replies!

I think the two packages differ more than I thought in terms of philosophy,
but I have been seduced by the potential power and graphics facilities
available in R (not to mention cost and cross-platform availability) so I'll
persevere.

M

_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/

        _/    _/   _/_/_/             Hopkins Research Ltd
       _/    _/   _/    _/
      _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
     _/    _/   _/   _/
    _/    _/   _/     _/               'touch the future'
                   
_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/



From helprhelp at gmail.com  Fri Jul  8 19:05:42 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 8 Jul 2005 12:05:42 -0500
Subject: [R] comparing strength of association instead of strength of
	evidence?
In-Reply-To: <971536df05062416214e404045@mail.gmail.com>
References: <cdf817830506240759cd3ae6a@mail.gmail.com>
	<42BC5EA0.2030202@acelerate.com>
	<971536df05062416214e404045@mail.gmail.com>
Message-ID: <cdf817830507081005aadac2f@mail.gmail.com>

Dear all:
I still need some further help since I think the question itself might
be very interesting (i hope so:) :
the question is on chisq.test, my concern is which criteria should be
used here to evaluate the independence. The reason i use this old
subject of the email is, b/c I think the problem here is about how to
look at p.value, which evaluate the strength of evidence instead of
association. If i am wrong, please correct me.

the result looks like this:
   index   word.comb     id in.class0 in.class1      p.value odds.ratio
1      1  TOTAL|LAID 54|241         2         4 0.0004997501 0.00736433
2      2 THEFT|RECOV  52|53     40751       146 0.0004997501 4.17127643
3      3  BOLL|ACCID  10|21     36825      1202 0.0004997501 0.44178546
4      4  LAB|VANDAL   8|55     24192       429 0.0004997501 0.82876099
5      5 VANDAL|CAUS  55|59       801        64 0.0004997501 0.18405918
6      6    AI|TOTAL   9|54      1949        45 0.0034982509 0.63766766
7      7    AI|RECOV   9|53      2385        61 0.0004997501 0.57547012
8      8 THEFT|TOTAL  52|54     33651       110 0.0004997501 4.56174408

the target is to look for best subset of word.comb to differentiate
between class0 and class1. p.value is obtained via
p.chisq.sim[i] <- as.double(chisq.test(tab, sim=TRUE, B=myB)$p.value)
and B=20000 (I increased B and it won't help. the margin here is
class0=2162792
class1=31859
)

So, in conclusion, which one I should use first, odds.ratio or p.value
to find the best subset.

I read some on feature selection in text categorization (A comparative
study on feature selection in text categorization might be a good
ref.). Anyone here has other suggestions?

thanks,

weiwei


On 6/24/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/24/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> > Weiwei Shi wrote:
> >
> > >Hi,
> > >I asked this question before, which was hidden in a bunch of
> > >questions. I repharse it here and hope I can get some help this time:
> > >
> > >I have 2 contingency tables which have the same group variable Y. I
> > >want to compare the strength of association between X1/Y and X2/Y. I
> > >am not sure if comparing p-values IS the way  even though the
> > >probability of seeing such "weird" observation under H0 defines
> > >p-value and it might relate to the strength of association somehow.
> > >But I read the following statement from Alan Agresti's "An
> > >Introduction to Categorical Data Analysis" :
> > >"Chi-squared tests simply indicate the degree of EVIDENCE for an
> > >association....It is sensible to decompose chi-squared into
> > >components, study residuals, and estimate parameters such as odds
> > >ratios that describe the STRENGTH OF ASSOCIATION".
> > >
> > >
> > >
> > Here are some things you can do:
> >
> >  > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> >
> >  > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> >  > library(epitools) # on CRAN
> >  > ?odds.ratio
> > Help for 'odds.ratio' is shown in the browser
> >  > library(help=epitools) # on CRAN
> >  > tab1
> >      [,1]    [,2]
> > [1,] 11266 2151526
> > [2,]   125   31734
> >  > odds.ratio(11266, 125, 2151526, 31734)
> > Error in fisher.test(tab) : FEXACT error 40.
> > Out of workspace.                 # so this are evidently for tables
> > with smaller counts
> >  > library(vcd) # on CRAN
> >
> >  > ?oddsratio
> > Help for 'oddsratio' is shown in the browser
> >  > oddsratio( tab1)  # really is logodds ratio
> > [1] 0.2807548
> >  > plot(oddsratio( tab1) )
> >  > library(help=vcd) # on CRAN  Read this for many nice functions.
> >  > fourfoldplot(tab1)
> >  > mosaicplot(tab1)     # not really usefull for this table
> >
> > Also has a look at function Crosstable in package gmodels.
> >
> > To decompose the chisqure you can program yourselves:
> >
> > decomp.chi <- function(tab) {
> >        rows <-  rowSums(tab)
> >        cols <-   colSums(tab)
> >        N <-   sum(rows)
> >         E <- rows %o% cols / N
> >         contrib <- (tab-E)^2/E
> >         contrib }
> >
> >
> >  > decomp.chi(tab1)
> >          [,1]         [,2]
> > [1,] 0.1451026 0.0007570624
> > [2,] 9.8504915 0.0513942218
> >  >
> >
> > So you can easily see what cell contributes most to the overall chisquared.
> >
> > Kjetil
> >
> >
> >
> >
> >
> > >Can I do this "decomposition" in R for the following example including
> > >2 contingency tables?
> > >
> > >
> > >
> > >>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> > >>tab1
> > >>
> > >>
> > >      [,1]    [,2]
> > >[1,] 11266 2151526
> > >[2,]   125   31734
> > >
> > >
> > >
> > >>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> > >>tab2
> > >>
> > >>
> > >      [,1]    [,2]
> > >[1,] 43571 2119221
> > >[2,]    52   31807
> > >
> 
> 
> Here are a few more ways of doing this using chisq.test,
> glm and assocplot:
> 
> > ## chisq.test ###
> 
> > tab1.chisq <- chisq.test(tab1)
> 
> > # decomposition of chisq
> > resid(tab1.chisq)^2
>           [,1]         [,2]
> [1,] 0.1451026 0.0007570624
> [2,] 9.8504915 0.0513942218
> 
> > # same
> > with(tab1.chisq, (observed - expected)^2/expected)
>           [,1]         [,2]
> [1,] 0.1451026 0.0007570624
> [2,] 9.8504915 0.0513942218
> 
> 
> > # Pearson residuals
> > resid(tab1.chisq)
>            [,1]        [,2]
> [1,]  0.3809234 -0.02751477
> [2,] -3.1385493  0.22670294
> 
> > # same
> > with(tab1.chisq, (observed - expected)/sqrt(expected))
>            [,1]        [,2]
> [1,]  0.3809234 -0.02751477
> [2,] -3.1385493  0.22670294
> 
> 
> > ### glm ###
> > # Pearson residuals via glm
> 
> > tab1.df <- data.frame(count = c(tab1), A = gl(2,2), B = gl(2,1,4))
> > tab1.glm <- glm(count ~ ., tab1.df, family = poisson())
> > resid(tab1.glm, type = "pearson")
>           1           2           3           4
>  0.38092339 -3.13854927 -0.02751477  0.22670294
> > plot(tab1.glm)
> 
> > ### assocplot ###
> > # displaying Pearson residuals via an assocplot
> > assocplot(t(tab1))
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From gunter.berton at gene.com  Fri Jul  8 19:23:02 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 8 Jul 2005 10:23:02 -0700
Subject: [R] comparing strength of association instead of strength
	ofevidence?
In-Reply-To: <cdf817830507081005aadac2f@mail.gmail.com>
Message-ID: <200507081723.j68HN2Oc008224@ohm.gene.com>

But isn't this the old "chestnut" that any "effect" will be found
"significant" given enough data; and with too few data, not even a large one
can be distinguished from noise?

If so, it's a good question that has more to do with the philosophy of
science than statistics. Bayesians, of course, would disagree (but they
don't have p-values).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
> Sent: Friday, July 08, 2005 10:06 AM
> To: Gabor Grothendieck
> Cc: R-help at stat.math.ethz.ch; Kjetil Brinchmann Halvorsen
> Subject: Re: [R] comparing strength of association instead of 
> strength ofevidence?
> 
> Dear all:
> I still need some further help since I think the question itself might
> be very interesting (i hope so:) :
> the question is on chisq.test, my concern is which criteria should be
> used here to evaluate the independence. The reason i use this old
> subject of the email is, b/c I think the problem here is about how to
> look at p.value, which evaluate the strength of evidence instead of
> association. If i am wrong, please correct me.
> 
> the result looks like this:
>    index   word.comb     id in.class0 in.class1      p.value 
> odds.ratio
> 1      1  TOTAL|LAID 54|241         2         4 0.0004997501 
> 0.00736433
> 2      2 THEFT|RECOV  52|53     40751       146 0.0004997501 
> 4.17127643
> 3      3  BOLL|ACCID  10|21     36825      1202 0.0004997501 
> 0.44178546
> 4      4  LAB|VANDAL   8|55     24192       429 0.0004997501 
> 0.82876099
> 5      5 VANDAL|CAUS  55|59       801        64 0.0004997501 
> 0.18405918
> 6      6    AI|TOTAL   9|54      1949        45 0.0034982509 
> 0.63766766
> 7      7    AI|RECOV   9|53      2385        61 0.0004997501 
> 0.57547012
> 8      8 THEFT|TOTAL  52|54     33651       110 0.0004997501 
> 4.56174408
> 
> the target is to look for best subset of word.comb to differentiate
> between class0 and class1. p.value is obtained via
> p.chisq.sim[i] <- as.double(chisq.test(tab, sim=TRUE, B=myB)$p.value)
> and B=20000 (I increased B and it won't help. the margin here is
> class0=2162792
> class1=31859
> )
> 
> So, in conclusion, which one I should use first, odds.ratio or p.value
> to find the best subset.
> 
> I read some on feature selection in text categorization (A comparative
> study on feature selection in text categorization might be a good
> ref.). Anyone here has other suggestions?
> 
> thanks,
> 
> weiwei
> 
> 
> On 6/24/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 6/24/05, Kjetil Brinchmann Halvorsen 
> <kjetil at acelerate.com> wrote:
> > > Weiwei Shi wrote:
> > >
> > > >Hi,
> > > >I asked this question before, which was hidden in a bunch of
> > > >questions. I repharse it here and hope I can get some 
> help this time:
> > > >
> > > >I have 2 contingency tables which have the same group 
> variable Y. I
> > > >want to compare the strength of association between X1/Y 
> and X2/Y. I
> > > >am not sure if comparing p-values IS the way  even though the
> > > >probability of seeing such "weird" observation under H0 defines
> > > >p-value and it might relate to the strength of 
> association somehow.
> > > >But I read the following statement from Alan Agresti's "An
> > > >Introduction to Categorical Data Analysis" :
> > > >"Chi-squared tests simply indicate the degree of EVIDENCE for an
> > > >association....It is sensible to decompose chi-squared into
> > > >components, study residuals, and estimate parameters such as odds
> > > >ratios that describe the STRENGTH OF ASSOCIATION".
> > > >
> > > >
> > > >
> > > Here are some things you can do:
> > >
> > >  > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> > >
> > >  > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> > >  > library(epitools) # on CRAN
> > >  > ?odds.ratio
> > > Help for 'odds.ratio' is shown in the browser
> > >  > library(help=epitools) # on CRAN
> > >  > tab1
> > >      [,1]    [,2]
> > > [1,] 11266 2151526
> > > [2,]   125   31734
> > >  > odds.ratio(11266, 125, 2151526, 31734)
> > > Error in fisher.test(tab) : FEXACT error 40.
> > > Out of workspace.                 # so this are evidently 
> for tables
> > > with smaller counts
> > >  > library(vcd) # on CRAN
> > >
> > >  > ?oddsratio
> > > Help for 'oddsratio' is shown in the browser
> > >  > oddsratio( tab1)  # really is logodds ratio
> > > [1] 0.2807548
> > >  > plot(oddsratio( tab1) )
> > >  > library(help=vcd) # on CRAN  Read this for many nice functions.
> > >  > fourfoldplot(tab1)
> > >  > mosaicplot(tab1)     # not really usefull for this table
> > >
> > > Also has a look at function Crosstable in package gmodels.
> > >
> > > To decompose the chisqure you can program yourselves:
> > >
> > > decomp.chi <- function(tab) {
> > >        rows <-  rowSums(tab)
> > >        cols <-   colSums(tab)
> > >        N <-   sum(rows)
> > >         E <- rows %o% cols / N
> > >         contrib <- (tab-E)^2/E
> > >         contrib }
> > >
> > >
> > >  > decomp.chi(tab1)
> > >          [,1]         [,2]
> > > [1,] 0.1451026 0.0007570624
> > > [2,] 9.8504915 0.0513942218
> > >  >
> > >
> > > So you can easily see what cell contributes most to the 
> overall chisquared.
> > >
> > > Kjetil
> > >
> > >
> > >
> > >
> > >
> > > >Can I do this "decomposition" in R for the following 
> example including
> > > >2 contingency tables?
> > > >
> > > >
> > > >
> > > >>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> > > >>tab1
> > > >>
> > > >>
> > > >      [,1]    [,2]
> > > >[1,] 11266 2151526
> > > >[2,]   125   31734
> > > >
> > > >
> > > >
> > > >>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> > > >>tab2
> > > >>
> > > >>
> > > >      [,1]    [,2]
> > > >[1,] 43571 2119221
> > > >[2,]    52   31807
> > > >
> > 
> > 
> > Here are a few more ways of doing this using chisq.test,
> > glm and assocplot:
> > 
> > > ## chisq.test ###
> > 
> > > tab1.chisq <- chisq.test(tab1)
> > 
> > > # decomposition of chisq
> > > resid(tab1.chisq)^2
> >           [,1]         [,2]
> > [1,] 0.1451026 0.0007570624
> > [2,] 9.8504915 0.0513942218
> > 
> > > # same
> > > with(tab1.chisq, (observed - expected)^2/expected)
> >           [,1]         [,2]
> > [1,] 0.1451026 0.0007570624
> > [2,] 9.8504915 0.0513942218
> > 
> > 
> > > # Pearson residuals
> > > resid(tab1.chisq)
> >            [,1]        [,2]
> > [1,]  0.3809234 -0.02751477
> > [2,] -3.1385493  0.22670294
> > 
> > > # same
> > > with(tab1.chisq, (observed - expected)/sqrt(expected))
> >            [,1]        [,2]
> > [1,]  0.3809234 -0.02751477
> > [2,] -3.1385493  0.22670294
> > 
> > 
> > > ### glm ###
> > > # Pearson residuals via glm
> > 
> > > tab1.df <- data.frame(count = c(tab1), A = gl(2,2), B = gl(2,1,4))
> > > tab1.glm <- glm(count ~ ., tab1.df, family = poisson())
> > > resid(tab1.glm, type = "pearson")
> >           1           2           3           4
> >  0.38092339 -3.13854927 -0.02751477  0.22670294
> > > plot(tab1.glm)
> > 
> > > ### assocplot ###
> > > # displaying Pearson residuals via an assocplot
> > > assocplot(t(tab1))
> > 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mdowle at concordiafunds.com  Fri Jul  8 19:37:13 2005
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Fri, 8 Jul 2005 18:37:13 +0100 
Subject: [R] Problem with filter() ?
Message-ID: <78166BFC5165D811AA0400065BF0324BF07572@wisconsin.concordia>


Dear list,

This is ok :

> filter(1:5, 1, "recursive")
Time Series:
Start = 1 
End = 5 
Frequency = 1 
[1]  1  3  6 10 15

But this? :

> filter(c(rep(NA,5),1:5), 1, "recursive")
Time Series:
Start = 1 
End = 10 
Frequency = 1 
 [1] NA  0 NA  0 NA  0  2  5  9 14
> 

> version
         _                      
platform x86_64-redhat-linux-gnu
arch     x86_64                 
os       linux-gnu              
system   x86_64, linux-gnu      
status                          
major    2                      
minor    0.1                    
year     2004                   
month    11                     
day      15                     
language R                      
> 

Regards,
Matthew



From tlumley at u.washington.edu  Fri Jul  8 19:34:19 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 8 Jul 2005 10:34:19 -0700 (PDT)
Subject: [R] exact conditional mantelhaen.test estimate is 0 ?!
In-Reply-To: <1120834821.5330.28.camel@perro.inet.dkfz-heidelberg.de>
References: <1120834821.5330.28.camel@perro.inet.dkfz-heidelberg.de>
Message-ID: <Pine.A41.4.61b.0507081027320.269192@homer04.u.washington.edu>

On Fri, 8 Jul 2005, Florian Hahne wrote:

> Dear listers,
>
> I am trying to compute the exact conditional test given strata margins
> of a 2 by 2 by K array using the mantelhaen.test function to get a
> common odds ratio estimate.
> The estimate for the test on the following data is 0, which in my
> opinion dosen't make any sense.
>

Indeed it doesn't.  Some probabilities are underflowing to zero.  It is 
informative to look at the estimates from
    mantelhaen.test(round(x/s), exact=TRUE)
for various values of s.  The estimate is stable at about 0.04 for 
large s until s gets to just above 1.8, then quickly decreases to zero.

I think you need exact=FALSE for data sets this large.

 	-thomas



From rvaradha at jhsph.edu  Fri Jul  8 19:38:38 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 8 Jul 2005 13:38:38 -0400
Subject: [R] comparing strength of association instead of strength
	ofevidence?
In-Reply-To: <cdf817830507081005aadac2f@mail.gmail.com>
Message-ID: <OWA-1WqhUYLqVpUOF9v000060bc@owa-1.sph.ad.jhsph.edu>

Hi,

The following article by William DuMouchel, which takes an empirical Bayes
approach, might be helpful to you:

Bayesian Data Mining in Large Frequency Tables, with an Application to the
FDA Spontaneous Reporting System, The American Statistician, Vol. 53, No. 3
(Aug., 1999), pp. 177-190.

Best,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
> Sent: Friday, July 08, 2005 1:06 PM
> To: Gabor Grothendieck
> Cc: R-help at stat.math.ethz.ch; Kjetil Brinchmann Halvorsen
> Subject: Re: [R] comparing strength of association instead of strength
> ofevidence?
> 
> Dear all:
> I still need some further help since I think the question itself might
> be very interesting (i hope so:) :
> the question is on chisq.test, my concern is which criteria should be
> used here to evaluate the independence. The reason i use this old
> subject of the email is, b/c I think the problem here is about how to
> look at p.value, which evaluate the strength of evidence instead of
> association. If i am wrong, please correct me.
> 
> the result looks like this:
>    index   word.comb     id in.class0 in.class1      p.value odds.ratio
> 1      1  TOTAL|LAID 54|241         2         4 0.0004997501 0.00736433
> 2      2 THEFT|RECOV  52|53     40751       146 0.0004997501 4.17127643
> 3      3  BOLL|ACCID  10|21     36825      1202 0.0004997501 0.44178546
> 4      4  LAB|VANDAL   8|55     24192       429 0.0004997501 0.82876099
> 5      5 VANDAL|CAUS  55|59       801        64 0.0004997501 0.18405918
> 6      6    AI|TOTAL   9|54      1949        45 0.0034982509 0.63766766
> 7      7    AI|RECOV   9|53      2385        61 0.0004997501 0.57547012
> 8      8 THEFT|TOTAL  52|54     33651       110 0.0004997501 4.56174408
> 
> the target is to look for best subset of word.comb to differentiate
> between class0 and class1. p.value is obtained via
> p.chisq.sim[i] <- as.double(chisq.test(tab, sim=TRUE, B=myB)$p.value)
> and B=20000 (I increased B and it won't help. the margin here is
> class0=2162792
> class1=31859
> )
> 
> So, in conclusion, which one I should use first, odds.ratio or p.value
> to find the best subset.
> 
> I read some on feature selection in text categorization (A comparative
> study on feature selection in text categorization might be a good
> ref.). Anyone here has other suggestions?
> 
> thanks,
> 
> weiwei
> 
> 
> On 6/24/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 6/24/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> > > Weiwei Shi wrote:
> > >
> > > >Hi,
> > > >I asked this question before, which was hidden in a bunch of
> > > >questions. I repharse it here and hope I can get some help this time:
> > > >
> > > >I have 2 contingency tables which have the same group variable Y. I
> > > >want to compare the strength of association between X1/Y and X2/Y. I
> > > >am not sure if comparing p-values IS the way  even though the
> > > >probability of seeing such "weird" observation under H0 defines
> > > >p-value and it might relate to the strength of association somehow.
> > > >But I read the following statement from Alan Agresti's "An
> > > >Introduction to Categorical Data Analysis" :
> > > >"Chi-squared tests simply indicate the degree of EVIDENCE for an
> > > >association....It is sensible to decompose chi-squared into
> > > >components, study residuals, and estimate parameters such as odds
> > > >ratios that describe the STRENGTH OF ASSOCIATION".
> > > >
> > > >
> > > >
> > > Here are some things you can do:
> > >
> > >  > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> > >
> > >  > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> > >  > library(epitools) # on CRAN
> > >  > ?odds.ratio
> > > Help for 'odds.ratio' is shown in the browser
> > >  > library(help=epitools) # on CRAN
> > >  > tab1
> > >      [,1]    [,2]
> > > [1,] 11266 2151526
> > > [2,]   125   31734
> > >  > odds.ratio(11266, 125, 2151526, 31734)
> > > Error in fisher.test(tab) : FEXACT error 40.
> > > Out of workspace.                 # so this are evidently for tables
> > > with smaller counts
> > >  > library(vcd) # on CRAN
> > >
> > >  > ?oddsratio
> > > Help for 'oddsratio' is shown in the browser
> > >  > oddsratio( tab1)  # really is logodds ratio
> > > [1] 0.2807548
> > >  > plot(oddsratio( tab1) )
> > >  > library(help=vcd) # on CRAN  Read this for many nice functions.
> > >  > fourfoldplot(tab1)
> > >  > mosaicplot(tab1)     # not really usefull for this table
> > >
> > > Also has a look at function Crosstable in package gmodels.
> > >
> > > To decompose the chisqure you can program yourselves:
> > >
> > > decomp.chi <- function(tab) {
> > >        rows <-  rowSums(tab)
> > >        cols <-   colSums(tab)
> > >        N <-   sum(rows)
> > >         E <- rows %o% cols / N
> > >         contrib <- (tab-E)^2/E
> > >         contrib }
> > >
> > >
> > >  > decomp.chi(tab1)
> > >          [,1]         [,2]
> > > [1,] 0.1451026 0.0007570624
> > > [2,] 9.8504915 0.0513942218
> > >  >
> > >
> > > So you can easily see what cell contributes most to the overall
> chisquared.
> > >
> > > Kjetil
> > >
> > >
> > >
> > >
> > >
> > > >Can I do this "decomposition" in R for the following example
> including
> > > >2 contingency tables?
> > > >
> > > >
> > > >
> > > >>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> > > >>tab1
> > > >>
> > > >>
> > > >      [,1]    [,2]
> > > >[1,] 11266 2151526
> > > >[2,]   125   31734
> > > >
> > > >
> > > >
> > > >>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> > > >>tab2
> > > >>
> > > >>
> > > >      [,1]    [,2]
> > > >[1,] 43571 2119221
> > > >[2,]    52   31807
> > > >
> >
> >
> > Here are a few more ways of doing this using chisq.test,
> > glm and assocplot:
> >
> > > ## chisq.test ###
> >
> > > tab1.chisq <- chisq.test(tab1)
> >
> > > # decomposition of chisq
> > > resid(tab1.chisq)^2
> >           [,1]         [,2]
> > [1,] 0.1451026 0.0007570624
> > [2,] 9.8504915 0.0513942218
> >
> > > # same
> > > with(tab1.chisq, (observed - expected)^2/expected)
> >           [,1]         [,2]
> > [1,] 0.1451026 0.0007570624
> > [2,] 9.8504915 0.0513942218
> >
> >
> > > # Pearson residuals
> > > resid(tab1.chisq)
> >            [,1]        [,2]
> > [1,]  0.3809234 -0.02751477
> > [2,] -3.1385493  0.22670294
> >
> > > # same
> > > with(tab1.chisq, (observed - expected)/sqrt(expected))
> >            [,1]        [,2]
> > [1,]  0.3809234 -0.02751477
> > [2,] -3.1385493  0.22670294
> >
> >
> > > ### glm ###
> > > # Pearson residuals via glm
> >
> > > tab1.df <- data.frame(count = c(tab1), A = gl(2,2), B = gl(2,1,4))
> > > tab1.glm <- glm(count ~ ., tab1.df, family = poisson())
> > > resid(tab1.glm, type = "pearson")
> >           1           2           3           4
> >  0.38092339 -3.13854927 -0.02751477  0.22670294
> > > plot(tab1.glm)
> >
> > > ### assocplot ###
> > > # displaying Pearson residuals via an assocplot
> > > assocplot(t(tab1))
> >
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From corr at fas.harvard.edu  Fri Jul  8 19:52:00 2005
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Fri, 8 Jul 2005 13:52:00 -0400 (EDT)
Subject: [R] missing data imputation
Message-ID: <Pine.LNX.4.58.0507081340550.30329@ls03.fas.harvard.edu>


Dear R-help,

I am trying to impute missing data for the first time using R. The norm
package seems to work for me, but the missing values that it returns seem
odd at times -- for example it returns negative values for a variable that
should only be positive. Does this matter in data analysis, and/or is
there a way to limit the imputed values to be within the minimum and
maximum of the actual data? Below is the code I am using.

Thanks,

Anders Corr
Ph.D. Candidate


#DOWNLOAD DATA (61Kb)
download.file("http://www.people.fas.harvard.edu/~corr/tc.csv","C:/R")

#RUN NORM
tc <- read.csv("tc.csv", header = TRUE)
rngseed(1234567)   #set random number generator seed
s  <-  prelim.norm(tc)
thetahat <- em.norm(s)   #find the MLE for a starting value
theta <- da.norm(s,thetahat,steps=20,showits=TRUE,return.ymis=TRUE)  #take 20 steps
ximp <- imp.norm(s,thetahat,tc)  #impute missing data under the MLE



From joseclaudio.faria at terra.com.br  Fri Jul  8 19:57:15 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Fri, 08 Jul 2005 14:57:15 -0300
Subject: [R] Help with Mahalanobis
Message-ID: <42CEBE7B.9020309@terra.com.br>

Dear R list,

I'm trying to calculate Mahalanobis distances for 'Species' of 'iris' data
as obtained below:

Squared Distance to Species From Species:

               Setosa Versicolor Virginica
Setosa 	           0   89.86419 179.38471
Versicolor  89.86419          0  17.20107
Virginica  179.38471   17.20107         0

These distances were obtained with proc 'CANDISC' of SAS, please,
see Output 21.1.2: Iris Data: Squared Mahalanobis Distances from
http://www.id.unizh.ch/software/unix/statmath/sas/sasdoc/stat/chap21/sect19.htm

 From these distances my intention is to make a cluster analysis as below, using
the package 'mclust':

In prior mail, my basic question was: how to obtain this matrix with R
from 'iris' data?

Well, I think that the basic soluction to calculate this distances is:

#
# --- Begin R script 1 ---
#
x   = as.matrix(iris[,1:4])
tra = iris[,5]

man = manova(x ~ tra)

# Mahalanobis
E    = summary(man)$SS[2] #Matrix E
S    = as.matrix(E$Residuals)/man$df.residual
InvS = solve(S)
ms = matrix(unlist(by(x, tra, mean)), byrow=T, ncol=ncol(x))
colnames(ms) = names(iris[1:4])
rownames(ms) = c('Set', 'Ver', 'Vir')
D2.12 = (ms[1,] - ms[2,])%*%InvS%*%(ms[1,] - ms[2,])
print(D2.12)
D2.13 = (ms[1,] - ms[3,])%*%InvS%*%(ms[1,] - ms[3,])
print(D2.13)
D2.23 = (ms[2,] - ms[3,])%*%InvS%*%(ms[2,] - ms[3,])
print(D2.23)
#
# --- End R script 1 ---
#

Well, I would like to generalize a soluction to obtain
the matrices like 'Mah' (below) or a complete matrix like in the
Output 21.1.2. Somebody could help me?

#
# --- Begin R script 2 ---
#

Mah = c(        0,
          89.86419,        0,
         179.38471, 17.20107, 0)

n = 3
D = matrix(0, n, n)

nam = c('Set', 'Ver', 'Vir')
rownames(D) = nam
colnames(D) = nam

k = 0
for (i in 1:n) {
    for (j in 1:i) {
       k      = k+1
       D[i,j] = Mah[k]
       D[j,i] = Mah[k]
    }
}

D=sqrt(D) #D2 -> D

library(mclust)
dendroS = hclust(as.dist(D), method='single')
dendroC = hclust(as.dist(D), method='complete')

win.graph(w = 3.5, h = 6)
split.screen(c(2, 1))
screen(1)
plot(dendroS, main='Single', sub='', xlab='', ylab='', col='blue')

screen(2)
plot(dendroC, main='Complete', sub='', xlab='', col='red')
#
# --- End R script 2 ---
#

I always need of this type of analysis and I'm not founding how to make it in
the CRAN documentation (Archives, packages: mclust, cluster, fpc and mva).

Regards,
--
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br



From alexbri at netcabo.pt  Fri Jul  8 20:07:38 2005
From: alexbri at netcabo.pt (alexbri)
Date: Fri, 8 Jul 2005 19:07:38 +0100
Subject: [R] explained deviance in multinom
Message-ID: <EA91707AE6F4C84495513EFF5117E897062217AF@VS2.hdi.tvcabo>

Hi:

 

I'm working with multinomial models with library nnet, and I'm trying to get the explained deviance (pseudo R^2) of my models.

I am assuming that:

pseudo R^2= 1 - dev(model) / dev (null)

where dev(model) is the deviance for the fitted model and dev(null) is the deviance for the null model (with the intercept only).

 

library(nnet)

full.model<- multinom(cbind(factor1, factor2 ,., factor5) ~ x1 + x2 + x3, weights=total, data=mydata)

null.model<- multinom(cbind(factor1, factor2 ,., factor5) ~ +1, weights=total, data=mydata)

 

Then I calculated 

pseudoR^2 = 1 - full.model$deviance / null.model$deviance

 

I'm obtaining very low values for pseudoR^2 (there is not much difference between the deviances of the two models). full.model fits (graphically) very well to the data , so I think that the problem is in the null.model (maybe it is not well defined) or with the calculus of pseudoR^2.

 

Can someone please give me some suggestions about this?

Thanks in advance

alex



From chrish at stats.ucl.ac.uk  Fri Jul  8 20:14:03 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Fri, 8 Jul 2005 19:14:03 +0100 (BST)
Subject: [R] Help with Mahalanobis
In-Reply-To: <42CEBE7B.9020309@terra.com.br>
References: <42CEBE7B.9020309@terra.com.br>
Message-ID: <Pine.LNX.4.58.0507081907170.18958@egon.stats.ucl.ac.uk>

Dear Jose,

normal mixture clustering (mclust) operates on points times variables data
and not on a distance matrix. Therefore
it doesn't make sense to compute Mahalanobis distances before using
mclust.
Furthermore, cluster analysis based on distance matrices (hclust or pam,
say) operates on a point by point distance matrix (be it Mahalanobis or
something else). You show a group by group matrix below, for which I don't
see any purpose in cluster analysis.
Have you looked at function mahalanobis?

Christian


On Fri, 8 Jul 2005, Jose Claudio Faria wrote:

> Dear R list,
>
> I'm trying to calculate Mahalanobis distances for 'Species' of 'iris' data
> as obtained below:
>
> Squared Distance to Species From Species:
>
>                Setosa Versicolor Virginica
> Setosa 	           0   89.86419 179.38471
> Versicolor  89.86419          0  17.20107
> Virginica  179.38471   17.20107         0
>
> These distances were obtained with proc 'CANDISC' of SAS, please,
> see Output 21.1.2: Iris Data: Squared Mahalanobis Distances from
> http://www.id.unizh.ch/software/unix/statmath/sas/sasdoc/stat/chap21/sect19.htm
>
>  From these distances my intention is to make a cluster analysis as below, using
> the package 'mclust':
>
> In prior mail, my basic question was: how to obtain this matrix with R
> from 'iris' data?
>
> Well, I think that the basic soluction to calculate this distances is:
>
> #
> # --- Begin R script 1 ---
> #
> x   = as.matrix(iris[,1:4])
> tra = iris[,5]
>
> man = manova(x ~ tra)
>
> # Mahalanobis
> E    = summary(man)$SS[2] #Matrix E
> S    = as.matrix(E$Residuals)/man$df.residual
> InvS = solve(S)
> ms = matrix(unlist(by(x, tra, mean)), byrow=T, ncol=ncol(x))
> colnames(ms) = names(iris[1:4])
> rownames(ms) = c('Set', 'Ver', 'Vir')
> D2.12 = (ms[1,] - ms[2,])%*%InvS%*%(ms[1,] - ms[2,])
> print(D2.12)
> D2.13 = (ms[1,] - ms[3,])%*%InvS%*%(ms[1,] - ms[3,])
> print(D2.13)
> D2.23 = (ms[2,] - ms[3,])%*%InvS%*%(ms[2,] - ms[3,])
> print(D2.23)
> #
> # --- End R script 1 ---
> #
>
> Well, I would like to generalize a soluction to obtain
> the matrices like 'Mah' (below) or a complete matrix like in the
> Output 21.1.2. Somebody could help me?
>
> #
> # --- Begin R script 2 ---
> #
>
> Mah = c(        0,
>           89.86419,        0,
>          179.38471, 17.20107, 0)
>
> n = 3
> D = matrix(0, n, n)
>
> nam = c('Set', 'Ver', 'Vir')
> rownames(D) = nam
> colnames(D) = nam
>
> k = 0
> for (i in 1:n) {
>     for (j in 1:i) {
>        k      = k+1
>        D[i,j] = Mah[k]
>        D[j,i] = Mah[k]
>     }
> }
>
> D=sqrt(D) #D2 -> D
>
> library(mclust)
> dendroS = hclust(as.dist(D), method='single')
> dendroC = hclust(as.dist(D), method='complete')
>
> win.graph(w = 3.5, h = 6)
> split.screen(c(2, 1))
> screen(1)
> plot(dendroS, main='Single', sub='', xlab='', ylab='', col='blue')
>
> screen(2)
> plot(dendroC, main='Complete', sub='', xlab='', col='red')
> #
> # --- End R script 2 ---
> #
>
> I always need of this type of analysis and I'm not founding how to make it in
> the CRAN documentation (Archives, packages: mclust, cluster, fpc and mva).
>
> Regards,
> --
> Jose Claudio Faria
> Brasil/Bahia/UESC/DCET
> Estatistica Experimental/Prof. Adjunto
> mails:
>   joseclaudio.faria at terra.com.br
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** NEW ADDRESS! ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From Katharina.Steinmann at stud.unibas.ch  Fri Jul  8 20:51:12 2005
From: Katharina.Steinmann at stud.unibas.ch (K. Steinmann)
Date: Fri,  8 Jul 2005 20:51:12 +0200
Subject: [R] extract prop. of. var in pca
Message-ID: <1120848672.42cecb2040543@webmail.unibas.ch>

Dear R-helpers,

Using the package Lattice, I performed a PCA.

For example
pca.summary <- summary(pc.cr <- princomp(USArrests, cor = TRUE))

The Output of "pca.summary" looks as follows:

Importance of components:
                          Comp.1    Comp.2    Comp.3     Comp.4
Standard deviation     1.5748783 0.9948694 0.5971291 0.41644938
Proportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752
Cumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000


How can I extract the proportion of variance?

Since names(pca.summary) or str(pca.summary) do not contain the proportion of
variance,
it seems I can not use something similar like pca.summary[[2]]$Comp.1[3].
I can't see how the values are stored.


Thanks in advance.

K. St.



From ccleland at optonline.net  Fri Jul  8 21:03:29 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 08 Jul 2005 15:03:29 -0400
Subject: [R] extract prop. of. var in pca
In-Reply-To: <1120848672.42cecb2040543@webmail.unibas.ch>
References: <1120848672.42cecb2040543@webmail.unibas.ch>
Message-ID: <42CECE01.3030800@optonline.net>

K. Steinmann wrote:
> Dear R-helpers,
> 
> Using the package Lattice, I performed a PCA.
> 
> For example
> pca.summary <- summary(pc.cr <- princomp(USArrests, cor = TRUE))
> 
> The Output of "pca.summary" looks as follows:
> 
> Importance of components:
>                           Comp.1    Comp.2    Comp.3     Comp.4
> Standard deviation     1.5748783 0.9948694 0.5971291 0.41644938
> Proportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752
> Cumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000
> 
> 
> How can I extract the proportion of variance?

   Instead of trying to get it from the summary, how about using this:

eigen(cor(USArrests))$values / ncol(USArrests)

and/or

cumsum(eigen(cor(USArrests))$values / ncol(USArrests))

   But note in the details section of ?princomp the following:

The calculation is done using 'eigen' on the correlation or
covariance matrix, as determined by 'cor'.  This is done for
compatibility with the S-PLUS result.  A preferred method of
calculation is to use 'svd' on 'x', as is done in 'prcomp'.

   Thus you could also do:

svd(cor(USArrests))$d / ncol(USArrests)

> Since names(pca.summary) or str(pca.summary) do not contain the proportion of
> variance,
> it seems I can not use something similar like pca.summary[[2]]$Comp.1[3].
> I can't see how the values are stored.
> 
> 
> Thanks in advance.
> 
> K. St.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From bret at tamu.edu  Fri Jul  8 21:22:14 2005
From: bret at tamu.edu (Bret Collier)
Date: Fri, 08 Jul 2005 14:22:14 -0500
Subject: [R] Overlying a Normal Dist in a Barplot
Message-ID: <s2ce8c34.039@wfscgate.tamu.edu>

R-Users,
Hopefully someone can shed some light on these questions as I had
little luck searching the archives (although I probably missed something
in my search due to the search phrase).  I estimated multinomial
probabilities for some count data (number successful offspring) ranging
from 0 to 8 (9 possible response categories).  I constructed a barplot
(using barplot2) and I want to "overlay" a normal distribution on the
figure (using rnorm (1000, mean, sd)).  My intent is to show that using
a mean(and associated sd) estimated from discrete count data may not be
a valid representation of the distribution of successful offspring.  

Obviously the x and y axes (as structured in barplot2) will not be
equivalent for these 2 sets of information and this shows up in my
example below. 

1)  Is it possible to somehow reconcile the underlying x-axis to the
same scale as would be needed to overly the normal distribution (e.g.
where 2.5 would fall on the normal density, I could relate it to 2.5 on
the barplot)?  Then, using axis (side=4) I assume I could insert a
y-axis for the normal distribution.

2)  Is lines(density(x)) the appropriate way to insert a normal
distribution into this type of figure?  Should I use 'curve'?

If someone could point me in the right direction, I would appreciate
it.

TIA, Bret

Example:

testdata 
0    0.196454948
1    0.063515510
2    0.149187592
3    0.237813885
4    0.282127031
5    0.066469719
6    0.001477105
7    0.001477105
8    0.001477105


x<-rnorm(1000, 2.84, 1.57)
barplot2(testdata, xlab="Fledgling Number", 
             ylab="Probability", ylim=c(0, 1), col="black", 
             border="black", axis.lty=1)
lines(density(x))


--Version--
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From Achim.Zeileis at wu-wien.ac.at  Fri Jul  8 21:29:25 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 8 Jul 2005 21:29:25 +0200 (CEST)
Subject: [R] extract prop. of. var in pca
In-Reply-To: <1120848672.42cecb2040543@webmail.unibas.ch>
References: <1120848672.42cecb2040543@webmail.unibas.ch>
Message-ID: <Pine.LNX.4.58.0507082126090.29368@thorin.ci.tuwien.ac.at>

On Fri, 8 Jul 2005, K. Steinmann wrote:

> Dear R-helpers,
>
> Using the package Lattice, I performed a PCA.

In my R installation, the function princomp() is contained in the stats
package, not lattice (sic!).

> For example
> pca.summary <- summary(pc.cr <- princomp(USArrests, cor = TRUE))
>
> The Output of "pca.summary" looks as follows:
>
> Importance of components:
>                           Comp.1    Comp.2    Comp.3     Comp.4
> Standard deviation     1.5748783 0.9948694 0.5971291 0.41644938
> Proportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752
> Cumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000
>
> How can I extract the proportion of variance?

The standard deviations are in
  pc.cr$sdev
(see also ?princomp), thus the variances are the squared values and the
proportions can be computed as
  pc.cr$sdev^2/sum(pc.cr$sdev^2)
and by taking the cumsum() the cumulative proportion can be computed. This
is exactly what the the print method for "summary.princomp" objects does
(see stats:::print.summary.princomp).

Best,
Z

> Since names(pca.summary) or str(pca.summary) do not contain the proportion of
> variance,
> it seems I can not use something similar like pca.summary[[2]]$Comp.1[3].
> I can't see how the values are stored.
>
>
> Thanks in advance.
>
> K. St.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeff.horner at vanderbilt.edu  Fri Jul  8 21:45:11 2005
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Fri, 08 Jul 2005 14:45:11 -0500
Subject: [R] Multiple assignments in one statement
Message-ID: <42CED7C7.8080208@vanderbilt.edu>

Is this possible?

For instance, I have a function that returns a vector length 3. In one 
statement I'd like to assign each element of the vector to different 
variables. Syntactically, I hoped this would work:

c(x,y,z) <- myfun();

Thanks,
-- 
Jeffrey Horner       Computer Systems Analyst         School of Medicine
615-322-8606         Department of Biostatistics   Vanderbilt University



From reid_huntsinger at merck.com  Fri Jul  8 22:04:16 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 8 Jul 2005 16:04:16 -0400
Subject: [R] Multiple assignments in one statement
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A94F8@uswpmx00.merck.com>

Yes and no. Your function doesn't work because c() is a function and R is
call-by-value, so c(x,y,z) is just a value, like 7. 

You could use lists like this:

> l <- list(x=NULL,y=NULL,z=NULL)
> l[c("x","y","z")] <- 1:3
> l
$x
[1] 1

$y
[1] 2

$z
[1] 3

That, together with attaching lists (help(attach)), might give you the
effect you want.

Reid Huntsinger



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Horner
Sent: Friday, July 08, 2005 3:45 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Multiple assignments in one statement


Is this possible?

For instance, I have a function that returns a vector length 3. In one 
statement I'd like to assign each element of the vector to different 
variables. Syntactically, I hoped this would work:

c(x,y,z) <- myfun();

Thanks,
-- 
Jeffrey Horner       Computer Systems Analyst         School of Medicine
615-322-8606         Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Fri Jul  8 22:08:49 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 8 Jul 2005 15:08:49 -0500
Subject: [R] "more" and "tab" functionalities in R under linux
Message-ID: <cdf8178305070813085c1ba6aa@mail.gmail.com>

Hi,
forgive me if it is due to my "laziness" :)
I am wondering if there are functionalities in R, which can do like
"more" and "tab" in linux:
more(one.data.frame) so I can browse through it. Sometimes I can use
one.data.frame[1:100,], but still not as good as "more" in linux.

tab:
can I use tab to auto complete an defined object name in R so I don't
have to type the full name? I knew ESS can do it but it is a little
bit funny when I use ESS and it can delete something that you cannot
delete from R, like pressing del key all the time and you see what i
mean.

thanks,

weiwei
-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From jerk_alert at hotmail.com  Fri Jul  8 22:22:46 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Fri, 08 Jul 2005 20:22:46 +0000
Subject: [R] Finding indices of NA values in a data frame
Message-ID: <BAY101-F31A935E7C8D6329A54DA90E8DB0@phx.gbl>

Hi all,

I've got a data frame with NA values scattered throughout. I would like to 
find the rows which contain an NA. Trying to find the indices of the NAs 
with grep() doesn't work (apparently you can't convert NA to "NA" character 
value, even with as.character()) so I was hoping someone could tell me how 
to find rows which contain NA.

Thanks in advance,
-Ken



From sundar.dorai-raj at pdf.com  Fri Jul  8 22:30:57 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 08 Jul 2005 15:30:57 -0500
Subject: [R] Finding indices of NA values in a data frame
In-Reply-To: <BAY101-F31A935E7C8D6329A54DA90E8DB0@phx.gbl>
References: <BAY101-F31A935E7C8D6329A54DA90E8DB0@phx.gbl>
Message-ID: <42CEE281.9080808@pdf.com>



Ken Termiso wrote:
> Hi all,
> 
> I've got a data frame with NA values scattered throughout. I would like to 
> find the rows which contain an NA. Trying to find the indices of the NAs 
> with grep() doesn't work (apparently you can't convert NA to "NA" character 
> value, even with as.character()) so I was hoping someone could tell me how 
> to find rows which contain NA.
> 
> Thanks in advance,
> -Ken
> 

?is.na or ?complete.cases

--sundar



From fgao at vt.edu  Fri Jul  8 22:36:54 2005
From: fgao at vt.edu (Feng Gao)
Date: Fri, 8 Jul 2005 16:36:54 -0400
Subject: [R] Finite Mixture Models with logistic regression
Message-ID: <43599CE4@zathras>

Do we have any R package that can do analysis on finite mixture model with 
logistic regression? Thanks

Faith

Feng Gao
    Dept. of Statistics
    Virginia Tech.
    Email: fgao at vt.edu



From zhliur at yahoo.com  Fri Jul  8 23:22:14 2005
From: zhliur at yahoo.com (yyan liu)
Date: Fri, 8 Jul 2005 14:22:14 -0700 (PDT)
Subject: [R] time series regression
Message-ID: <20050708212214.96960.qmail@web53102.mail.yahoo.com>

Hi:
  I have two time series y(t) and x(t). I want to
regress Y on X. Because Y is a time series and may
have autocorrelation such as AR(p), so it is not
efficient to use OLS directly. The model I am trying
to fit is like
Y(t)=beta0+beta1*X(t)+rho*Y(t-1)+e(t)

e(t) is iid normal random error. Anybody know whether
there is a function in R can fit such models? The
function can also let me specify how many beta's and
rho's I can have in the model. 

Thx a lot!

liu



From f.harrell at vanderbilt.edu  Sat Jul  9 00:59:56 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 08 Jul 2005 17:59:56 -0500
Subject: [R] missing data imputation
In-Reply-To: <Pine.LNX.4.58.0507081340550.30329@ls03.fas.harvard.edu>
References: <Pine.LNX.4.58.0507081340550.30329@ls03.fas.harvard.edu>
Message-ID: <42CF056C.4010006@vanderbilt.edu>

Anders Schwartz Corr wrote:
> Dear R-help,
> 
> I am trying to impute missing data for the first time using R. The norm
> package seems to work for me, but the missing values that it returns seem
> odd at times -- for example it returns negative values for a variable that
> should only be positive. Does this matter in data analysis, and/or is
> there a way to limit the imputed values to be within the minimum and
> maximum of the actual data? Below is the code I am using.
> 
> Thanks,
> 
> Anders Corr
> Ph.D. Candidate

Yes that matters.  That's one reason I wrote the aregImpute function in 
the Hmisc package.  By default it uses predictive mean matching so it 
can't produce illegal values.

Frank

> 
> 
> #DOWNLOAD DATA (61Kb)
> download.file("http://www.people.fas.harvard.edu/~corr/tc.csv","C:/R")
> 
> #RUN NORM
> tc <- read.csv("tc.csv", header = TRUE)
> rngseed(1234567)   #set random number generator seed
> s  <-  prelim.norm(tc)
> thetahat <- em.norm(s)   #find the MLE for a starting value
> theta <- da.norm(s,thetahat,steps=20,showits=TRUE,return.ymis=TRUE)  #take 20 steps
> ximp <- imp.norm(s,thetahat,tc)  #impute missing data under the MLE
>
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From brianscholl1973 at yahoo.com  Sat Jul  9 01:06:56 2005
From: brianscholl1973 at yahoo.com (Brian Scholl)
Date: Fri, 8 Jul 2005 16:06:56 -0700 (PDT)
Subject: [R] help with ARIMA and predict
Message-ID: <20050708230656.7897.qmail@web31903.mail.mud.yahoo.com>

I'm trying to do the following out of sample
regression with autoregressive terms and additional x
variables:

y(t+1)=const+B(L)*y(t)+C(1)*x_1(t)...+C(K)*x_K(t)

where:
B(L) = lag polynom. for AR terms
C(1..K) = are the coeffs. on K exogenous variables
that have only 1 lag

Question 1: 
-----------

Suppose I use arima to fit the model: 

df.y<-arima(yvec,order=c(L,0,0),xreg=xmat[,(1:K)],n.cond=maximum.lag)


Now suppose I want to do a 1-period ahead prediction
based on the results of this regression, using
predict: 

predict(df.y,newxreg=newx,n.ahead=1)

I'm expecting newx to be 1X3.  After all, I just want
to predict 1 value of y, so in my mind I should just
need 1 time period's observation of x (i.e. #
rows=n.ahead). I'm sort of expecting predict to grab
the last two values of yvec to use as y(t),y(t-1) in
prediction.  If I make such a pass, I get: 

Error in predict.Arima(df.y, newxreg = newx) : 
'xreg' and 'newxreg' have different numbers of columns

If I try passing 2+ rows of x, predict accepts the
call and I get: 

Time Series:
Start = 41 
End = 42 
Frequency = 1 
[1] -0.03165 -0.03165 (for simplicity I passed two
identical rows of x)  

$se
Time Series:
Start = 41 
End = 41 
Frequency = 1 
[1] 0.02707

So I'm puzzled as to what I'm doing wrong.  When I
have n.ahead rows in newxreg, I get an error, but by
passing a second row in it is accepted. But what am I
predicting in the latter case? Is R requiring another
row so that it can form a prediction of y(t) to use in
forecasting y(t+1) (this is not what I want to do), or
have I simply goofed in some other way?  

Is there a better way to do this? I've also attempted
something similar using lm, but I'm unclear how to
interpret the "predicted" time series it returns.  
The obvious alternative is to construct the forecast
using df.y$coef and a relevant data vector.  

Q2: 
---

Suppose I want to select the autoregressive order
using AIC.  If I have understood, in the excellent
MASS text comments (p415) that comparisons are only
valid if n.cond is the same for each model.  Yet, when
I set n.cond=maximum.lag (say =5), I get df.y$n.cond
=0.  So I'm unclear if the AICs are comparable for
different models (i.e. different L's and different
K's).



From ggrothendieck at gmail.com  Sat Jul  9 01:47:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 19:47:53 -0400
Subject: [R] time series regression
In-Reply-To: <20050708212214.96960.qmail@web53102.mail.yahoo.com>
References: <20050708212214.96960.qmail@web53102.mail.yahoo.com>
Message-ID: <971536df05070816472720ecc9@mail.gmail.com>

On 7/8/05, yyan liu <zhliur at yahoo.com> wrote:
> Hi:
>  I have two time series y(t) and x(t). I want to
> regress Y on X. Because Y is a time series and may
> have autocorrelation such as AR(p), so it is not
> efficient to use OLS directly. The model I am trying
> to fit is like
> Y(t)=beta0+beta1*X(t)+rho*Y(t-1)+e(t)
> 
> e(t) is iid normal random error. Anybody know whether
> there is a function in R can fit such models? The
> function can also let me specify how many beta's and
> rho's I can have in the model.
> 

Make sure you have the latest version of packages dyn and zoo
from CRAN and try this:

library(dyn) # this also pulls in zoo
set.seed(1)
x <- zoo(rnorm(50))
y <- 1 + x + rnorm(49)

# regress y on x
y.lm.1 <- dyn$lm(y ~ x)
y.lm.1

# regress y on x and lag of y.  We use update to just add last term to model.
y.lm.2 <- update(y.lm.1, . ~ . + lag(y, -1))
y.lm.2

anova(y.lm.1, y.lm.2)  # difference not significant.  Use y.lm.1.

# regress y on x and its lag and on lags 1 and 2 of y
# this shows that in lag(x,k) that k may be a vector
y.lm.3 <- dyn$lm(y ~ x + lag(x,-1) + lag(y, -seq(2)))
y.lm.3

For more info try:

package?dyn
vignette("zoo")



From ljin at lbl.gov  Sat Jul  9 01:55:13 2005
From: ljin at lbl.gov (Ling Jin)
Date: Fri, 08 Jul 2005 16:55:13 -0700
Subject: [R] merge
Message-ID: <42CF1261.7030906@lbl.gov>

Hi all,

I have two data frames to merge by a column containing the site names 
(as characters). However, somehow, one of the site names of one data 
frame have fixed length, say 8, so the names sometimes have spaces at 
the end. For example, the site name is "ST", but in one data frame, it 
is "ST        ". Therefore, the merge function won't recognize that "ST" 
and "ST       " are the same, so won't merge accordingly.

Is there a easy way to deal with it? Or I should do something during 
data import? (BTW, I imported the data using read.csv)


Thanks!

Ling



From ggrothendieck at gmail.com  Sat Jul  9 01:55:34 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 19:55:34 -0400
Subject: [R] "more" and "tab" functionalities in R under linux
In-Reply-To: <cdf8178305070813085c1ba6aa@mail.gmail.com>
References: <cdf8178305070813085c1ba6aa@mail.gmail.com>
Message-ID: <971536df05070816554fe94f7c@mail.gmail.com>

Here are some possibilities:
- head(iris) will show the first few rows of the data frame
- edit(iris) will put up a spreadsheet with the data frame in it that
you can scroll
- In JGR (a GUI front end for R) you can use the object browser (ctrl-B)
- If the object is a file rather than a data frame use file.show

On 7/8/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Hi,
> forgive me if it is due to my "laziness" :)
> I am wondering if there are functionalities in R, which can do like
> "more" and "tab" in linux:
> more(one.data.frame) so I can browse through it. Sometimes I can use
> one.data.frame[1:100,], but still not as good as "more" in linux.
> 
> tab:
> can I use tab to auto complete an defined object name in R so I don't
> have to type the full name? I knew ESS can do it but it is a little
> bit funny when I use ESS and it can delete something that you cannot
> delete from R, like pressing del key all the time and you see what i
> mean.
> 
> thanks,
> 
> weiwei
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sat Jul  9 02:00:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 20:00:00 -0400
Subject: [R] Multiple assignments in one statement
In-Reply-To: <42CED7C7.8080208@vanderbilt.edu>
References: <42CED7C7.8080208@vanderbilt.edu>
Message-ID: <971536df05070817001196d6e5@mail.gmail.com>

Check out:
http://tolstoy.newcastle.edu.au/R/help/04/06/1430.html
http://tolstoy.newcastle.edu.au/R/help/04/06/1406.html

On 7/8/05, Jeffrey Horner <jeff.horner at vanderbilt.edu> wrote:
> Is this possible?
> 
> For instance, I have a function that returns a vector length 3. In one
> statement I'd like to assign each element of the vector to different
> variables. Syntactically, I hoped this would work:
> 
> c(x,y,z) <- myfun();
> 
> Thanks,
> --
> Jeffrey Horner       Computer Systems Analyst         School of Medicine
> 615-322-8606         Department of Biostatistics   Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sat Jul  9 02:10:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 20:10:33 -0400
Subject: [R] merge
In-Reply-To: <42CF1261.7030906@lbl.gov>
References: <42CF1261.7030906@lbl.gov>
Message-ID: <971536df05070817101a2d06c9@mail.gmail.com>

trim in package gdata will trim spaces off the beginning and end.


On 7/8/05, Ling Jin <ljin at lbl.gov> wrote:
> Hi all,
> 
> I have two data frames to merge by a column containing the site names
> (as characters). However, somehow, one of the site names of one data
> frame have fixed length, say 8, so the names sometimes have spaces at
> the end. For example, the site name is "ST", but in one data frame, it
> is "ST        ". Therefore, the merge function won't recognize that "ST"
> and "ST       " are the same, so won't merge accordingly.
> 
> Is there a easy way to deal with it? Or I should do something during
> data import? (BTW, I imported the data using read.csv)
> 
> 
> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Sat Jul  9 02:47:16 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 08 Jul 2005 19:47:16 -0500
Subject: [R] merge
In-Reply-To: <971536df05070817101a2d06c9@mail.gmail.com>
References: <42CF1261.7030906@lbl.gov>
	<971536df05070817101a2d06c9@mail.gmail.com>
Message-ID: <1120870036.3924.13.camel@localhost.localdomain>

One other option during the import is to set 'strip.white = TRUE' in
read.csv(). See ?read.csv for more information. Bear in mind that this
will strip both leading and trailing white space in all columns, which
may have unintended consequences.

Yet another post-import option, would be to use sub() on specific
columns:

> df <- data.frame(A = c("ST  ", "ST", "ST   ", "ST ", "ST      "), 
                   B = letters[1:5])
> df
         A B
1     ST   a
2       ST b
3    ST    c
4      ST  d
5 ST       e

> df$A <- sub('[[:space:]]+$', '', as.character(df$A))
> df
   A B
1 ST a
2 ST b
3 ST c
4 ST d
5 ST e

See ?sub for more information. Be cautious in this case, as you will
need to coerce any factors to character vectors as I have done above,
and then possibly re-coerce to a factor as you may require.

HTH,

Marc Schwartz


On Fri, 2005-07-08 at 20:10 -0400, Gabor Grothendieck wrote:
> trim in package gdata will trim spaces off the beginning and end.
> 
> 
> On 7/8/05, Ling Jin <ljin at lbl.gov> wrote:
> > Hi all,
> > 
> > I have two data frames to merge by a column containing the site names
> > (as characters). However, somehow, one of the site names of one data
> > frame have fixed length, say 8, so the names sometimes have spaces at
> > the end. For example, the site name is "ST", but in one data frame, it
> > is "ST        ". Therefore, the merge function won't recognize that "ST"
> > and "ST       " are the same, so won't merge accordingly.
> > 
> > Is there a easy way to deal with it? Or I should do something during
> > data import? (BTW, I imported the data using read.csv)
> > 
> > 
> > Thanks!
> > 
> > Ling
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Jul  9 03:39:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 8 Jul 2005 21:39:31 -0400
Subject: [R] It was a sad day for the Statistics profession
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7F@usctmx1106.Merck.com>

For those of you who have not heard:  Prof. Breiman passed away on July 5th.
http://www.berkeley.edu/news/media/releases/2005/07/07_breiman.shtml

Andy



From ggrothendieck at gmail.com  Sat Jul  9 03:58:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Jul 2005 21:58:02 -0400
Subject: [R] Overlying a Normal Dist in a Barplot
In-Reply-To: <s2ce8c34.039@wfscgate.tamu.edu>
References: <s2ce8c34.039@wfscgate.tamu.edu>
Message-ID: <971536df0507081858a7fb904@mail.gmail.com>

On 7/8/05, Bret Collier <bret at tamu.edu> wrote:
> R-Users,
> Hopefully someone can shed some light on these questions as I had
> little luck searching the archives (although I probably missed something
> in my search due to the search phrase).  I estimated multinomial
> probabilities for some count data (number successful offspring) ranging
> from 0 to 8 (9 possible response categories).  I constructed a barplot
> (using barplot2) and I want to "overlay" a normal distribution on the
> figure (using rnorm (1000, mean, sd)).  My intent is to show that using
> a mean(and associated sd) estimated from discrete count data may not be
> a valid representation of the distribution of successful offspring.
> 
> Obviously the x and y axes (as structured in barplot2) will not be
> equivalent for these 2 sets of information and this shows up in my
> example below.
> 
> 1)  Is it possible to somehow reconcile the underlying x-axis to the
> same scale as would be needed to overly the normal distribution (e.g.
> where 2.5 would fall on the normal density, I could relate it to 2.5 on
> the barplot)?  Then, using axis (side=4) I assume I could insert a
> y-axis for the normal distribution.
> 
> 2)  Is lines(density(x)) the appropriate way to insert a normal
> distribution into this type of figure?  Should I use 'curve'?
> 
> If someone could point me in the right direction, I would appreciate
> it.
> 
> TIA, Bret
> 
> Example:
> 
> testdata
> 0    0.196454948
> 1    0.063515510
> 2    0.149187592
> 3    0.237813885
> 4    0.282127031
> 5    0.066469719
> 6    0.001477105
> 7    0.001477105
> 8    0.001477105
> 
> 
> x<-rnorm(1000, 2.84, 1.57)
> barplot2(testdata, xlab="Fledgling Number",
>             ylab="Probability", ylim=c(0, 1), col="black",
>             border="black", axis.lty=1)
> lines(density(x))
> 

Maybe something like this using rect and curve:

# data from your post
testdata <- c(0.196454948, 0.06351551, 0.149187592, 0.237813885, 
  0.282127031, 0.066469719, 0.001477105, 0.001477105, 0.001477105)
x <- 0:9

# setup plot ranges noting max of normal density is at mean
xrange <- range(x) + c(-0.5,+0.5)
yrange <- range(c(testdata, dnorm(2.84, 2.84, 1.57), 0))
plot(xrange, yrange, type = "n", xlab = "X", ylab = "Probability", xaxt = "n")
axis(1, x)

# draw bars using rect and density using curve
rect(x - 0.5, 0, x + 0.5, testdata, col = "lightgrey")
curve(dnorm(x, 2.84, 1.57), min(xrange), max(xrange), add = TRUE)



From MSchwartz at mn.rr.com  Sat Jul  9 04:38:23 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 08 Jul 2005 21:38:23 -0500
Subject: [R] Overlying a Normal Dist in a Barplot
In-Reply-To: <971536df0507081858a7fb904@mail.gmail.com>
References: <s2ce8c34.039@wfscgate.tamu.edu>
	<971536df0507081858a7fb904@mail.gmail.com>
Message-ID: <1120876703.3924.30.camel@localhost.localdomain>

On Fri, 2005-07-08 at 21:58 -0400, Gabor Grothendieck wrote:
> On 7/8/05, Bret Collier <bret at tamu.edu> wrote:
> > R-Users,
> > Hopefully someone can shed some light on these questions as I had
> > little luck searching the archives (although I probably missed something
> > in my search due to the search phrase).  I estimated multinomial
> > probabilities for some count data (number successful offspring) ranging
> > from 0 to 8 (9 possible response categories).  I constructed a barplot
> > (using barplot2) and I want to "overlay" a normal distribution on the
> > figure (using rnorm (1000, mean, sd)).  My intent is to show that using
> > a mean(and associated sd) estimated from discrete count data may not be
> > a valid representation of the distribution of successful offspring.
> > 
> > Obviously the x and y axes (as structured in barplot2) will not be
> > equivalent for these 2 sets of information and this shows up in my
> > example below.
> > 
> > 1)  Is it possible to somehow reconcile the underlying x-axis to the
> > same scale as would be needed to overly the normal distribution (e.g.
> > where 2.5 would fall on the normal density, I could relate it to 2.5 on
> > the barplot)?  Then, using axis (side=4) I assume I could insert a
> > y-axis for the normal distribution.
> > 
> > 2)  Is lines(density(x)) the appropriate way to insert a normal
> > distribution into this type of figure?  Should I use 'curve'?
> > 
> > If someone could point me in the right direction, I would appreciate
> > it.
> > 
> > TIA, Bret
> > 
> > Example:
> > 
> > testdata
> > 0    0.196454948
> > 1    0.063515510
> > 2    0.149187592
> > 3    0.237813885
> > 4    0.282127031
> > 5    0.066469719
> > 6    0.001477105
> > 7    0.001477105
> > 8    0.001477105
> > 
> > 
> > x<-rnorm(1000, 2.84, 1.57)
> > barplot2(testdata, xlab="Fledgling Number",
> >             ylab="Probability", ylim=c(0, 1), col="black",
> >             border="black", axis.lty=1)
> > lines(density(x))
> > 
> 
> Maybe something like this using rect and curve:
> 
> # data from your post
> testdata <- c(0.196454948, 0.06351551, 0.149187592, 0.237813885, 
>   0.282127031, 0.066469719, 0.001477105, 0.001477105, 0.001477105)
> x <- 0:9
> 
> # setup plot ranges noting max of normal density is at mean
> xrange <- range(x) + c(-0.5,+0.5)
> yrange <- range(c(testdata, dnorm(2.84, 2.84, 1.57), 0))
> plot(xrange, yrange, type = "n", xlab = "X", ylab = "Probability", xaxt = "n")
> axis(1, x)
> 
> # draw bars using rect and density using curve
> rect(x - 0.5, 0, x + 0.5, testdata, col = "lightgrey")
> curve(dnorm(x, 2.84, 1.57), min(xrange), max(xrange), add = TRUE)

Nice solution Gabor.

I had to think about this one for a bit, and think I may have it using
barplot[2]():

> testdata
  V1          V2
1  0 0.196454948
2  1 0.063515510
3  2 0.149187592
4  3 0.237813885
5  4 0.282127031
6  5 0.066469719
7  6 0.001477105
8  7 0.001477105
9  8 0.001477105


# Change 'space = 0' so that bars are adjacent to each other
# This also puts each bar center at seq(0.5, 8.5, 1)
mp <- barplot2(testdata[, 2], xlab="Fledgling Number", 
               ylab="Probability", ylim=c(0, .3),  
               axis.lty = 1, names.arg = testdata[, 1],
               space = 0)

> mp
      [,1]
 [1,]  0.5
 [2,]  1.5
 [3,]  2.5
 [4,]  3.5
 [5,]  4.5
 [6,]  5.5
 [7,]  6.5
 [8,]  7.5
 [9,]  8.5
 
x <- testdata[, 1]

# Now do the curve and shift the mean by +0.5
# To coincide with the bar centers
curve(dnorm(x, 2.84 + 0.5, 1.57), add = TRUE)


I think that does it.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Sat Jul  9 06:51:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Jul 2005 00:51:08 -0400
Subject: [R] Overlying a Normal Dist in a Barplot
In-Reply-To: <971536df0507081858a7fb904@mail.gmail.com>
References: <s2ce8c34.039@wfscgate.tamu.edu>
	<971536df0507081858a7fb904@mail.gmail.com>
Message-ID: <971536df05070821514b6f21b1@mail.gmail.com>

Offline, Marc pointed out to me that boxplot has an at= argument.
This suggests that we could substitute a boxplot command for the
rect command since a boxplot of c(0,a) looks like a bar from 0 to a if 
we use medlty=0 (which omits the median line) and boxwex=1
(which eliminates the space between the boxes).  This does have the 
advantage that one does not have to compute the corners of the rectangles
which my prior solution had to do.

(I also simplified the yrange calculation based on the fact that the height
of the density curve is less than the maximum testdata point so we can just 
take the range of the that.  Also I corrected x which should be 0:8 rather than 
what I wrote in the previous post.)

# data
testdata <- c(0.196454948, 0.063515510, 0.149187592, 0.237813885, 0.282127031, 
0.066469719, 0.001477105, 0.001477105, 0.001477105)
x <- 0:8

# setup plot ranges and axes
xrange <- range(x) + c(-0.5, +0.5)
yrange <- c(0, max(testdata)) 
plot(xrange, yrange, type = "n", xlab = "X", ylab = "Probability", xaxt = "n")

# draw bars using boxplot and density using curve
boxplot(as.data.frame(rbind(0,testdata)), at = x, names = x,
	boxwex = 1, medlty = 0, add = TRUE, col = "lightgrey")
curve(dnorm(x, 2.84, 1.57), min(xrange), max(xrange), add = TRUE)


On 7/8/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/8/05, Bret Collier <bret at tamu.edu> wrote:
> > R-Users,
> > Hopefully someone can shed some light on these questions as I had
> > little luck searching the archives (although I probably missed something
> > in my search due to the search phrase).  I estimated multinomial
> > probabilities for some count data (number successful offspring) ranging
> > from 0 to 8 (9 possible response categories).  I constructed a barplot
> > (using barplot2) and I want to "overlay" a normal distribution on the
> > figure (using rnorm (1000, mean, sd)).  My intent is to show that using
> > a mean(and associated sd) estimated from discrete count data may not be
> > a valid representation of the distribution of successful offspring.
> >
> > Obviously the x and y axes (as structured in barplot2) will not be
> > equivalent for these 2 sets of information and this shows up in my
> > example below.
> >
> > 1)  Is it possible to somehow reconcile the underlying x-axis to the
> > same scale as would be needed to overly the normal distribution (e.g.
> > where 2.5 would fall on the normal density, I could relate it to 2.5 on
> > the barplot)?  Then, using axis (side=4) I assume I could insert a
> > y-axis for the normal distribution.
> >
> > 2)  Is lines(density(x)) the appropriate way to insert a normal
> > distribution into this type of figure?  Should I use 'curve'?
> >
> > If someone could point me in the right direction, I would appreciate
> > it.
> >
> > TIA, Bret
> >
> > Example:
> >
> > testdata
> > 0    0.196454948
> > 1    0.063515510
> > 2    0.149187592
> > 3    0.237813885
> > 4    0.282127031
> > 5    0.066469719
> > 6    0.001477105
> > 7    0.001477105
> > 8    0.001477105
> >
> >
> > x<-rnorm(1000, 2.84, 1.57)
> > barplot2(testdata, xlab="Fledgling Number",
> >             ylab="Probability", ylim=c(0, 1), col="black",
> >             border="black", axis.lty=1)
> > lines(density(x))
> >
> 
> Maybe something like this using rect and curve:
> 
> # data from your post
> testdata <- c(0.196454948, 0.06351551, 0.149187592, 0.237813885,
>  0.282127031, 0.066469719, 0.001477105, 0.001477105, 0.001477105)
> x <- 0:9
> 
> # setup plot ranges noting max of normal density is at mean
> xrange <- range(x) + c(-0.5,+0.5)
> yrange <- range(c(testdata, dnorm(2.84, 2.84, 1.57), 0))
> plot(xrange, yrange, type = "n", xlab = "X", ylab = "Probability", xaxt = "n")
> axis(1, x)
> 
> # draw bars using rect and density using curve
> rect(x - 0.5, 0, x + 0.5, testdata, col = "lightgrey")
> curve(dnorm(x, 2.84, 1.57), min(xrange), max(xrange), add = TRUE)
>



From spencer.graves at pdf.com  Sat Jul  9 07:03:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 08 Jul 2005 22:03:09 -0700
Subject: [R] help with ARIMA and predict
In-Reply-To: <20050708230656.7897.qmail@web31903.mail.mud.yahoo.com>
References: <20050708230656.7897.qmail@web31903.mail.mud.yahoo.com>
Message-ID: <42CF5A8D.7040902@pdf.com>

	  What class is "newx" when you get the error message?  Is it a vector 
or a 1xK array?  If the former, force it to be an array.  (Hint: 
"array(1:4, dim=c(2,2))[1,]" is a vector of length 2, while "array(1:4, 
dim=c(2,2))[1,,drop=FALSE]" is a 1 x 2 matrix..(

	  If this does not solve your problem, then I suggest you make up a few 
very simple examples where you know the the answers under the most 
plausible thoughts about what it's doing, and try those.  I've also 
learned a lot by copying the arima function into a script file and 
walking through it line by line.

	  If none of this work for you, I suggest you read the posting guide! 
"http://www.R-project.org/posting-guide.html".  People often answer 
their own questions from following suggestions in in the posting guide. 
  If that fails, they seem more likely to get useful answers from the 
questions they post after following this guide.

	  spencer graves

Brian Scholl wrote:

> I'm trying to do the following out of sample
> regression with autoregressive terms and additional x
> variables:
> 
> y(t+1)=const+B(L)*y(t)+C(1)*x_1(t)...+C(K)*x_K(t)
> 
> where:
> B(L) = lag polynom. for AR terms
> C(1..K) = are the coeffs. on K exogenous variables
> that have only 1 lag
> 
> Question 1: 
> -----------
> 
> Suppose I use arima to fit the model: 
> 
> df.y<-arima(yvec,order=c(L,0,0),xreg=xmat[,(1:K)],n.cond=maximum.lag)
> 
> 
> Now suppose I want to do a 1-period ahead prediction
> based on the results of this regression, using
> predict: 
> 
> predict(df.y,newxreg=newx,n.ahead=1)
> 
> I'm expecting newx to be 1X3.  After all, I just want
> to predict 1 value of y, so in my mind I should just
> need 1 time period's observation of x (i.e. #
> rows=n.ahead). I'm sort of expecting predict to grab
> the last two values of yvec to use as y(t),y(t-1) in
> prediction.  If I make such a pass, I get: 
> 
> Error in predict.Arima(df.y, newxreg = newx) : 
> 'xreg' and 'newxreg' have different numbers of columns
> 
> If I try passing 2+ rows of x, predict accepts the
> call and I get: 
> 
> Time Series:
> Start = 41 
> End = 42 
> Frequency = 1 
> [1] -0.03165 -0.03165 (for simplicity I passed two
> identical rows of x)  
> 
> $se
> Time Series:
> Start = 41 
> End = 41 
> Frequency = 1 
> [1] 0.02707
> 
> So I'm puzzled as to what I'm doing wrong.  When I
> have n.ahead rows in newxreg, I get an error, but by
> passing a second row in it is accepted. But what am I
> predicting in the latter case? Is R requiring another
> row so that it can form a prediction of y(t) to use in
> forecasting y(t+1) (this is not what I want to do), or
> have I simply goofed in some other way?  
> 
> Is there a better way to do this? I've also attempted
> something similar using lm, but I'm unclear how to
> interpret the "predicted" time series it returns.  
> The obvious alternative is to construct the forecast
> using df.y$coef and a relevant data vector.  
> 
> Q2: 
> ---
> 
> Suppose I want to select the autoregressive order
> using AIC.  If I have understood, in the excellent
> MASS text comments (p415) that comparisons are only
> valid if n.cond is the same for each model.  Yet, when
> I set n.cond=maximum.lag (say =5), I get df.y$n.cond
> =0.  So I'm unclear if the AICs are comparable for
> different models (i.e. different L's and different
> K's).
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From f.hahne at dkfz-heidelberg.de  Sat Jul  9 12:11:51 2005
From: f.hahne at dkfz-heidelberg.de (f.hahne@dkfz-heidelberg.de)
Date: Sat, 09 Jul 2005 12:11:51 +0200 (CEST)
Subject: [R] exact conditional mantelhaen.test estimate is 0 ?!
In-Reply-To: <Pine.A41.4.61b.0507081027320.269192@homer04.u.washington.edu>
References: <1120834821.5330.28.camel@perro.inet.dkfz-heidelberg.de>
	<Pine.A41.4.61b.0507081027320.269192@homer04.u.washington.edu>
Message-ID: <1120903911.42cfa2e7ca4a3@webmail.inet.dkfz-heidelberg.de>

Thanks for the quick answer, that's what I was thinking too. Just wasn't sure
wether this was a bug or a matter of computations becoming singular.
Florian

> On Fri, 8 Jul 2005, Florian Hahne wrote:
> 
> > Dear listers,
> >
> > I am trying to compute the exact conditional test given strata margins
> > of a 2 by 2 by K array using the mantelhaen.test function to get a
> > common odds ratio estimate.
> > The estimate for the test on the following data is 0, which in my
> > opinion dosen't make any sense.
> >
> 
> Indeed it doesn't.  Some probabilities are underflowing to zero.  It is 
> informative to look at the estimates from
>     mantelhaen.test(round(x/s), exact=TRUE)
> for various values of s.  The estimate is stable at about 0.04 for 
> large s until s gets to just above 1.8, then quickly decreases to zero.
> 
> I think you need exact=FALSE for data sets this large.
> 
>  	-thomas
>



From NPB at danskeslagterier.dk  Sat Jul  9 12:31:42 2005
From: NPB at danskeslagterier.dk (Niels Peter Baadsgaard)
Date: Sat, 09 Jul 2005 12:31:42 +0200
Subject: [R] Svar: R-help Digest, Vol 29, Issue 9
Message-ID: <s2cfc3c3.058@danskeslagterier.dk>

Jeg er p?? ferie indtil 1/8.
Med venlig hilsen
Niels Peter



From antoniou at central.ntua.gr  Sat Jul  9 12:53:23 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Sat, 9 Jul 2005 13:53:23 +0300
Subject: [R] getting a variable from an object named using paste
Message-ID: <18143782-046E-4F0A-8F3C-ED3F5E75C643@central.ntua.gr>

Hello,

The subject could be articulated better ;(, but I am stuck...

In any case, my problem is the following: I am trying to use knn, and  
it requires a classification. So I am using cmeans for this. What I  
want to do is add the classification as a new column in my data.frame  
(to be used for knn). Now, the trick is that I would like to name the  
output of the cmeans classification based on the number of clusters  
(e.g. cmeans30 for 30 clusters). To do this, I do the following:


[The file for this example can be downloaded from:

http://mit.edu/costas/www/station-1-120103.txt   ]


library(e1071)
lala <- read.csv("station-1-120103.txt",header=F)
ll <- cbind(lala$V25,lala$V22)
num.of.clust=50
assign(paste("cmeans",num.of.clust,sep=""),cmeans(ll,num.of.clust))

and sure enough if I type cmeans50 I get the output of this cmeans run.

This also works, naturally:

llc<-cbind(ll,cmeans50$cluster)

[and I now get three columns, where the third column is the cluster ID.]

However, when I do what I really want (i.e. not call cmeans50  
explicitly, but through the name that I constructed programmatically):

llc <- cbind(ll,(paste("cmeans",num.of.clust,sep=""))$cluster)

I only get two columns (the last term has no effect).

Any ideas as to what I am doing wrong?

Thanks a lot!

Costas


--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From costas at mit.edu  Sat Jul  9 13:01:55 2005
From: costas at mit.edu (Constantinos Antoniou)
Date: Sat, 9 Jul 2005 14:01:55 +0300
Subject: [R] getting a variable from an object named using paste
Message-ID: <0CEAB142-5F59-4D25-8BBF-B3EBCC017ABF@mit.edu>

Hello,

The subject could be articulated better ;(, but I am stuck...

In any case, my problem is the following: I am trying to use knn, and  
it requires a classification. So I am using cmeans for this. What I  
want to do is add the classification as a new column in my data.frame  
(to be used for knn). Now, the trick is that I would like to name the  
output of the cmeans classification based on the number of clusters  
(e.g. cmeans30 for 30 clusters). To do this, I do the following:


[The file for this example can be downloaded from:

http://mit.edu/costas/www/station-1-120103.txt   ]


library(e1071)
lala <- read.csv("station-1-120103.txt",header=F)
ll <- cbind(lala$V25,lala$V22)
num.of.clust=50
assign(paste("cmeans",num.of.clust,sep=""),cmeans(ll,num.of.clust))

and sure enough if I type cmeans50 I get the output of this cmeans run.

This also works, naturally:

llc<-cbind(ll,cmeans50$cluster)

[and I now get three columns, where the third column is the cluster ID.]

However, when I do what I really want (i.e. not call cmeans50  
explicitly, but through the name that I constructed programmatically):

llc <- cbind(ll,(paste("cmeans",num.of.clust,sep=""))$cluster)

I only get two columns (the last term has no effect).

Any ideas as to what I am doing wrong?

Thanks a lot!

Costas



-- 
Constantinos Antoniou, Ph.D.
Massachusetts Institute of Technology
Intelligent Transportation Systems Program
77 Massachusetts Ave., Rm. 1-249, Cambridge, MA 02139



From joseclaudio.faria at terra.com.br  Sat Jul  9 12:46:39 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sat, 09 Jul 2005 07:46:39 -0300
Subject: [R] Help with Mahalanobis
In-Reply-To: <Pine.LNX.4.58.0507081907170.18958@egon.stats.ucl.ac.uk>
References: <42CEBE7B.9020309@terra.com.br>
	<Pine.LNX.4.58.0507081907170.18958@egon.stats.ucl.ac.uk>
Message-ID: <42CFAB0F.8040609@terra.com.br>

Christian Hennig wrote:

> Dear Jose,
> 
> normal mixture clustering (mclust) operates on points times variables data
> and not on a distance matrix. Therefore
> it doesn't make sense to compute Mahalanobis distances before using
> mclust.
> Furthermore, cluster analysis based on distance matrices (hclust or pam,
> say) operates on a point by point distance matrix (be it Mahalanobis or
> something else). You show a group by group matrix below, for which I don't
> see any purpose in cluster analysis.
> Have you looked at function mahalanobis?
> 
> Christian

Dear Christian,

First of all, thanks for the reply!

So, multivariate analysis is not my field of domain, I'm studying this because
it is necessary in my works.

I'm using 'iris' only as an example of my real problem, because I normally work
with many response variables (5 or more), with replicates (10 or more) of many
groups (20 or more). In these cases, I think, the final dendogram using 'mclust'
package is not very good/clear.

I learned, in these cases, that the generalized distance of Mahalanobis,
obtained as in the prior example (see script), is one of the best choice to
study the similarity between the groups. Do you agree?

If yes, I need to cluster the objects from this matrix of distances between the
groups. My option by 'mclust' package was because I'm studying also it, no more,
and I think that, for the purpose, it works nice.

Could you help me about another (and simple) choice of analyze?

JCFaria

> On Fri, 8 Jul 2005, Jose Claudio Faria wrote:
> 
> 
>>Dear R list,
>>
>>I'm trying to calculate Mahalanobis distances for 'Species' of 'iris' data
>>as obtained below:
>>
>>Squared Distance to Species From Species:
>>
>>               Setosa Versicolor Virginica
>>Setosa 	           0   89.86419 179.38471
>>Versicolor  89.86419          0  17.20107
>>Virginica  179.38471   17.20107         0
>>
>>These distances were obtained with proc 'CANDISC' of SAS, please,
>>see Output 21.1.2: Iris Data: Squared Mahalanobis Distances from
>>http://www.id.unizh.ch/software/unix/statmath/sas/sasdoc/stat/chap21/sect19.htm
>>
>> From these distances my intention is to make a cluster analysis as below, using
>>the package 'mclust':
>>
>>In prior mail, my basic question was: how to obtain this matrix with R
>>from 'iris' data?
>>
>>Well, I think that the basic soluction to calculate this distances is:
>>
>>#
>># --- Begin R script 1 ---
>>#
>>x   = as.matrix(iris[,1:4])
>>tra = iris[,5]
>>
>>man = manova(x ~ tra)
>>
>># Mahalanobis
>>E    = summary(man)$SS[2] #Matrix E
>>S    = as.matrix(E$Residuals)/man$df.residual
>>InvS = solve(S)
>>ms = matrix(unlist(by(x, tra, mean)), byrow=T, ncol=ncol(x))
>>colnames(ms) = names(iris[1:4])
>>rownames(ms) = c('Set', 'Ver', 'Vir')
>>D2.12 = (ms[1,] - ms[2,])%*%InvS%*%(ms[1,] - ms[2,])
>>print(D2.12)
>>D2.13 = (ms[1,] - ms[3,])%*%InvS%*%(ms[1,] - ms[3,])
>>print(D2.13)
>>D2.23 = (ms[2,] - ms[3,])%*%InvS%*%(ms[2,] - ms[3,])
>>print(D2.23)
>>#
>># --- End R script 1 ---
>>#
>>
>>Well, I would like to generalize a soluction to obtain
>>the matrices like 'Mah' (below) or a complete matrix like in the
>>Output 21.1.2. Somebody could help me?
>>
>>#
>># --- Begin R script 2 ---
>>#
>>
>>Mah = c(        0,
>>          89.86419,        0,
>>         179.38471, 17.20107, 0)
>>
>>n = 3
>>D = matrix(0, n, n)
>>
>>nam = c('Set', 'Ver', 'Vir')
>>rownames(D) = nam
>>colnames(D) = nam
>>
>>k = 0
>>for (i in 1:n) {
>>    for (j in 1:i) {
>>       k      = k+1
>>       D[i,j] = Mah[k]
>>       D[j,i] = Mah[k]
>>    }
>>}
>>
>>D=sqrt(D) #D2 -> D
>>
>>library(mclust)
>>dendroS = hclust(as.dist(D), method='single')
>>dendroC = hclust(as.dist(D), method='complete')
>>
>>win.graph(w = 3.5, h = 6)
>>split.screen(c(2, 1))
>>screen(1)
>>plot(dendroS, main='Single', sub='', xlab='', ylab='', col='blue')
>>
>>screen(2)
>>plot(dendroC, main='Complete', sub='', xlab='', col='red')
>>#
>># --- End R script 2 ---
>>#
>>
>>I always need of this type of analysis and I'm not founding how to make it in
>>the CRAN documentation (Archives, packages: mclust, cluster, fpc and mva).
>>
>>Regards,
>>--
>>Jose Claudio Faria
>>Brasil/Bahia/UESC/DCET
>>Estatistica Experimental/Prof. Adjunto
>>mails:
>>  joseclaudio.faria at terra.com.br
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> *** NEW ADDRESS! ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 
> Esta mensagem foi verificada pelo E-mail Protegido Terra.
> Scan engine: McAfee VirusScan / Atualizado em 08/07/2005 / Vers??o: 4.4.00 - Dat 4531
> Proteja o seu e-mail Terra: http://mail.terra.com.br/
> 
> 


-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ligges at statistik.uni-dortmund.de  Sat Jul  9 14:30:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 09 Jul 2005 14:30:45 +0200
Subject: [R] getting a variable from an object named using paste
In-Reply-To: <0CEAB142-5F59-4D25-8BBF-B3EBCC017ABF@mit.edu>
References: <0CEAB142-5F59-4D25-8BBF-B3EBCC017ABF@mit.edu>
Message-ID: <42CFC375.1040309@statistik.uni-dortmund.de>

Constantinos Antoniou wrote:
> Hello,
> 
> The subject could be articulated better ;(, but I am stuck...
> 
> In any case, my problem is the following: I am trying to use knn, and  
> it requires a classification. So I am using cmeans for this. What I  
> want to do is add the classification as a new column in my data.frame  
> (to be used for knn). Now, the trick is that I would like to name the  
> output of the cmeans classification based on the number of clusters  
> (e.g. cmeans30 for 30 clusters). To do this, I do the following:
> 
> 
> [The file for this example can be downloaded from:
> 
> http://mit.edu/costas/www/station-1-120103.txt   ]
> 
> 
> library(e1071)
> lala <- read.csv("station-1-120103.txt",header=F)
> ll <- cbind(lala$V25,lala$V22)
> num.of.clust=50
> assign(paste("cmeans",num.of.clust,sep=""),cmeans(ll,num.of.clust))
> 
> and sure enough if I type cmeans50 I get the output of this cmeans run.
> 
> This also works, naturally:
> 
> llc<-cbind(ll,cmeans50$cluster)
> 
> [and I now get three columns, where the third column is the cluster ID.]
> 
> However, when I do what I really want (i.e. not call cmeans50  
> explicitly, but through the name that I constructed programmatically):
> 
> llc <- cbind(ll,(paste("cmeans",num.of.clust,sep=""))$cluster)

This is a FAQ: use get()

Uwe Ligges


> I only get two columns (the last term has no effect).
> 
> Any ideas as to what I am doing wrong?
> 
> Thanks a lot!
> 
> Costas
> 
> 
>



From david.whiting at ncl.ac.uk  Sat Jul  9 15:06:09 2005
From: david.whiting at ncl.ac.uk (David Whiting)
Date: Sat, 09 Jul 2005 14:06:09 +0100
Subject: [R] "more" and "tab" functionalities in R under linux
In-Reply-To: <971536df05070816554fe94f7c@mail.gmail.com>
References: <cdf8178305070813085c1ba6aa@mail.gmail.com>
	<971536df05070816554fe94f7c@mail.gmail.com>
Message-ID: <42CFCBC1.1040901@ncl.ac.uk>

Here's a very simple function that more literally emulates 'more'. It
would probably become irritating on dataframes with many columns or
rows. (Note that this is quick code thrown together and could probably
be simplified and/or improved.)

more <- function(x, num.rows=20)
{
  ## Purpose:display a dataframe in a 'more'-like manner
  ## -------------------------------------------------------------------
  ## Arguments:
  ## x: a data frame
  ## num.rows: number of rows to show at a time.
  ## -------------------------------------------------------------------
  show.more <- TRUE
  start.row <- 1
  while (show.more) {
    print(x[start.row:(start.row + num.rows), ])
    pc.shown <- round(((start.row + num.rows) / nrow(x)) * 100, 1)
    ans <- readline(paste("--More--(", pc.shown, "%)\n", sep=""))
    if (ans == "q") {
      show.more <- FALSE
    } else {
      start.row <- start.row + num.rows + 1
    }
  }
}

David

Gabor Grothendieck wrote:
> Here are some possibilities:
> - head(iris) will show the first few rows of the data frame
> - edit(iris) will put up a spreadsheet with the data frame in it that
> you can scroll
> - In JGR (a GUI front end for R) you can use the object browser (ctrl-B)
> - If the object is a file rather than a data frame use file.show
> 
> On 7/8/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> 
>>Hi,
>>forgive me if it is due to my "laziness" :)
>>I am wondering if there are functionalities in R, which can do like
>>"more" and "tab" in linux:
>>more(one.data.frame) so I can browse through it. Sometimes I can use
>>one.data.frame[1:100,], but still not as good as "more" in linux.
>>
>>tab:
>>can I use tab to auto complete an defined object name in R so I don't
>>have to type the full name? I knew ESS can do it but it is a little
>>bit funny when I use ESS and it can delete something that you cannot
>>delete from R, like pressing del key all the time and you see what i
>>mean.
>>
>>thanks,
>>
>>weiwei
>>--
>>Weiwei Shi, Ph.D
>>
>>"Did you always know?"
>>"No, I did not. But I believed..."
>>---Matrix III
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From uofiowa at gmail.com  Fri Jul  8 18:10:00 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 8 Jul 2005 12:10:00 -0400
Subject: [R] R.oo static field
Message-ID: <3f87cc6d050708091078b95235@mail.gmail.com>

How can I define a static member of a class? not a static method,
rather a static field that would be accessed by all instances of the
class.



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul  9 16:03:35 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 09 Jul 2005 15:03:35 +0100 (BST)
Subject: [R] missing data imputation
In-Reply-To: <Pine.LNX.4.58.0507081340550.30329@ls03.fas.harvard.edu>
Message-ID: <XFMail.050709150335.Ted.Harding@nessie.mcc.ac.uk>

On 08-Jul-05 Anders Schwartz Corr wrote:
> 
> Dear R-help,
> 
> I am trying to impute missing data for the first time using R.
> The norm package seems to work for me, but the missing values
> that it returns seem odd at times -- for example it returns
> negative values for a variable that should only be positive.
> Does this matter in data analysis, and/or is there a way to
> limit the imputed values to be within the minimum and
> maximum of the actual data? Below is the code I am using.

If you have a variable that should only be positive, then strictly
speaking you should not treat it as normally distributed, since
a normal distribution -- however large the mean, however small
the variance -- theoretically has positive probability of giving
negative values. So what you have observed in your data is within
the job-description of the normal distribution.

In practice, whether this matters in data analysis depends on
the range of values in a typical dataset, on the mean and SD
of a typical fitted normal distribution, on the probability
that such a distribution will give a negative value, and on
the sample size. (Evem if P(<0) is only 10^(-4), if you are
dealing with sample sizes of 10^6 you are very likely to get
some negative values).

Whether it matters in practice also depends, of course, on
whether it matters in practice. What, in the real world, will
break if there's a negative value or two in there?

In many cases people simply treat negative estimates of variables
which are intrinsically non-negative very crudely: if it comes
out negative, replaceit with zero. This too is often a quick
fix where the fact that it is a lie simply has no practical
importance. But, of course, it may matter! That depends ...
(see above).

It is also the case that imputed values generated by a procedure
such as NORM have greater dispersion than the variable itself.
This is a consequence of the way such imputation works, since
each imputation is drawn from a *random* instance of a normal
distribution, the mean and the variance of this distribution
being sampled from the Bayesian posterior distribution of these
parameters given the complete data and the covariates of the
incomplete data. So it is more likely that an imputed value will
be negative than that an observed value will be negative.

It is also worth looking at the shape of the histogram of such
a variable. In many applications (though not all), this may
exhibit positive skewness which would suggest that a log-normal
distribution would be a better fit in any case. In that case,
use the logarithm of the data, which will have (to within the
adequacy of fit) have a normal distribution. Run your imputations,
and then take the exponential of the results thereby transforming
back to the scale of the original variable. This result is necessarily
positive, so "anomalous" negative values simply cannot occur.

Also, remember that a variable to which you may have very reasonably
attributed a normal distribution (because of good fit to the data)
may be intrinsically positive solely for *semantic* reasons. E.g.
it may be a measured length. God made all lengths positive, and you
and we know this. But R, and NORM, and rnorom(), and all their
friends, do not know this. Of semantics they know nothing. And the
Daemon of Randomness will see a normal distribution, and mischievously
spit negative values at you, simply because they are there ...

However, this is just general advice, though it may give you
something to think about.

Meanwhile, I will try to have a look at the dataset whose URL
you give, and see if I have any more specific comments.

I've also noted Frank Harrel's comment about aregImpute, and
will bear it in mind. Note, however, that this does not do
multiple imputation on the same lines as NORM (or the other
Shafer-derived MI packages). See ?aregImpute section "Details".
And, specifically, from the "Description":

  "The 'transcan' function creates flexible additive imputation
   models but provides only an approximation to true multiple
   imputation as the imputation models are fixed before all
   multiple imputations are drawn. This ignores variability
   caused by having to fit the imputation models. 'aregImpute'
   takes all aspects of uncertainty in the imputations into
   account by using the bootstrap to approximate the process
   of drawing predicted values from a full Bayesian predictive
   distribution."

so that the Rubin/Shafer method described above (see paragraph
about dispersion of imputed values) is not fully implemented.
  
Best wishes,
Ted.



From costas at mit.edu  Sat Jul  9 16:44:36 2005
From: costas at mit.edu (Constantinos Antoniou)
Date: Sat, 9 Jul 2005 17:44:36 +0300
Subject: [R] getting a variable from an object named using paste
In-Reply-To: <42CFC375.1040309@statistik.uni-dortmund.de>
References: <0CEAB142-5F59-4D25-8BBF-B3EBCC017ABF@mit.edu>
	<42CFC375.1040309@statistik.uni-dortmund.de>
Message-ID: <6E7B3BD4-CC03-4C72-A2CC-B966FCADD7C1@mit.edu>

Indeed. Thanks and sorry,

Costas

PS. also, sorry about sending the original post twice... I did not  
want to pollute the list any more with an apology then...


On 09  2005, at 3:30 , Uwe Ligges wrote:

> Constantinos Antoniou wrote:
>
>> Hello,
>> The subject could be articulated better ;(, but I am stuck...
>> In any case, my problem is the following: I am trying to use knn,  
>> and  it requires a classification. So I am using cmeans for this.  
>> What I  want to do is add the classification as a new column in my  
>> data.frame  (to be used for knn). Now, the trick is that I would  
>> like to name the  output of the cmeans classification based on the  
>> number of clusters  (e.g. cmeans30 for 30 clusters). To do this, I  
>> do the following:
>> [The file for this example can be downloaded from:
>> http://mit.edu/costas/www/station-1-120103.txt   ]
>> library(e1071)
>> lala <- read.csv("station-1-120103.txt",header=F)
>> ll <- cbind(lala$V25,lala$V22)
>> num.of.clust=50
>> assign(paste("cmeans",num.of.clust,sep=""),cmeans(ll,num.of.clust))
>> and sure enough if I type cmeans50 I get the output of this cmeans  
>> run.
>> This also works, naturally:
>> llc<-cbind(ll,cmeans50$cluster)
>> [and I now get three columns, where the third column is the  
>> cluster ID.]
>> However, when I do what I really want (i.e. not call cmeans50   
>> explicitly, but through the name that I constructed  
>> programmatically):
>> llc <- cbind(ll,(paste("cmeans",num.of.clust,sep=""))$cluster)
>>
>
> This is a FAQ: use get()
>
> Uwe Ligges
>
>
>
>> I only get two columns (the last term has no effect).
>> Any ideas as to what I am doing wrong?
>> Thanks a lot!
>> Costas
>>
>
>
>



-- 
Constantinos Antoniou, Ph.D.
Massachusetts Institute of Technology
Intelligent Transportation Systems Program
77 Massachusetts Ave., Rm. 1-249, Cambridge, MA 02139



From liuwensui at gmail.com  Sat Jul  9 17:12:52 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 9 Jul 2005 11:12:52 -0400
Subject: [R] It was a sad day for the Statistics profession
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7F@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA7F@usctmx1106.Merck.com>
Message-ID: <1115a2b0050709081257522970@mail.gmail.com>

I started learning data mining by reading his book 'classification and
regression tree'. His  paper and website about random forest are the
best resource for my learning in statistics.

It is truly a big loss in statistics. I feel terribly bad. 

On 7/8/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> For those of you who have not heard:  Prof. Breiman passed away on July 5th.
> http://www.berkeley.edu/news/media/releases/2005/07/07_breiman.shtml
> 
> Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From spencer.graves at pdf.com  Sat Jul  9 17:34:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Jul 2005 08:34:52 -0700
Subject: [R] Finite Mixture Models with logistic regression
In-Reply-To: <43599CE4@zathras>
References: <43599CE4@zathras>
Message-ID: <42CFEE9C.20207@pdf.com>

	  As I've seen no replies to this so far, I will make my feeble 
attempt.  "RSiteSearch('mixture regression')" just produced 107 hits, 
the fourth of which was for function moc in package moc.  The 
description looked promising, but I have not used it.

	  Also, "RSiteSearch('logistic regression mixture')" produced 20 hits 
just now, two of which were for functions in J. K. Lindsey's gnlm 
package.  At least one of these is supposed to fit a mixture of single 
nuggetts at 0 and 1 with a logistic regression model.  Lindsey's 
packages are not available for all platforms and so are not available on 
CRAN.  To get them, you need to go to his personal web site.  When I 
Googled just now for "J. K. Lindsey", his web site was the first hit.

	  Even if this is not adequate for your needs, you can get ideas from 
reading the code.  If you try something and can't get it to work, please 
submit another question.  To increase the chances of a useful reply, 
PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".
	
	  spencer graves

Feng Gao wrote:

> Do we have any R package that can do analysis on finite mixture model with 
> logistic regression? Thanks
> 
> Faith
> 
> Feng Gao
>     Dept. of Statistics
>     Virginia Tech.
>     Email: fgao at vt.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sat Jul  9 17:47:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Jul 2005 11:47:19 -0400
Subject: [R] Finite Mixture Models with logistic regression
In-Reply-To: <42CFEE9C.20207@pdf.com>
References: <43599CE4@zathras> <42CFEE9C.20207@pdf.com>
Message-ID: <971536df05070908477c797917@mail.gmail.com>

On 7/9/05, Spencer Graves <spencer.graves at pdf.com> wrote:

> Lindsey's
> packages are not available for all platforms and so are not available on
> CRAN.  

Is it true that all CRAN packages in the main repository are available on 
all platforms?

(There is at least an other-software area:
   http://cran.r-project.org/other-software.html
where things that do not fit into the main repository can go.)



From maechler at stat.math.ethz.ch  Sat Jul  9 17:51:38 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 9 Jul 2005 17:51:38 +0200
Subject: [R] look for help on nlm in R
In-Reply-To: <19257B6BB7BAF143B2DA2964B72D5CA3018092B4@jhms08.phibred.com>
References: <19257B6BB7BAF143B2DA2964B72D5CA3018092B4@jhms08.phibred.com>
Message-ID: <17103.62090.504482.60075@stat.math.ethz.ch>

>>>>> "MaggieZ" == Zhu, Maggie <maggie.zhu at pioneer.com>
>>>>>     on Thu, 7 Jul 2005 14:40:37 -0500 writes:

    MaggieZ> I had a hard time in learning nlm in R and appreciate any help. 

    MaggieZ> I encounted the following error message from time
    MaggieZ> to time when I tried different starting parameter
    MaggieZ> values (three parameter values in this case) in

     >>  nlm(f=SS.fun,p=c(0.1/40,0.1,2),hessian = FALSE,N.measure=object,h=20) 

     Error in f(x, ...) : only 0's may mix with negative subscripts 

    MaggieZ> Basically I know the three parameter values should all be positive numbers. So, how to select appropriate starting parameter values to prevent this kind of error message? 

Maggie, it's a bug in the 'SS.fun' function that you are asking
nlm() to minimize.  So it would be best to show us your
SS.fun().

BTW:  "subscripts" are the `` i ''  in  x[i]
      in its the subscripts that become wrong in your SS.fun function.

Regards,
Martin Maechler, ETH Zurich



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul  9 17:49:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 09 Jul 2005 16:49:08 +0100 (BST)
Subject: [R] missing data imputation
In-Reply-To: <XFMail.050709150335.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050709164908.Ted.Harding@nessie.mcc.ac.uk>


On 09-Jul-05 Ted Harding wrote:
> On 08-Jul-05 Anders Schwartz Corr wrote:
>> [...]
> ]...]
> Meanwhile, I will try to have a look at the dataset whose URL
> you give, and see if I have any more specific comments.

Now that I look at the histograms of your 21 variables, I would
not think of treating most of them as anything like normally
distributed (for a few, a normal distribution might roughly
reflect the underlying distribution, though it would only fit
where it touches).

Nor is it obvious what kind of distribution to think of trying
for many of them. Perhaps you have ideas, from your knowledge
of the field the data were drawn from, of what kind of model
to use. But not many types of explicit model are implemented
MI software anywhere, let alone in R.

These considerations rule out trying NORM or anything similar,
since such approaches depend strongly on a reasonably good model
for the distribution of the data.

In any case, it looks as though some of them are categorical,
with 2 or 3 levels, and NORM is rarely good for such variables.
You should in any case consider the 'mix' package when some
variables are discrete and some are continuous (and can be
assumed to be, or transformed to be) normally distributed.
But. for the reasons above, I wouldn't go in that direction
anyway.

> I've also noted Frank Harrel's comment about aregImpute, and
> will bear it in mind.
> [...]

The sort of approach implied by the above comments suggests
an approach which is much less dependent on model assumptions.

The most model-free approach is in the family of "hot deck"
approaches where the imputed values of a variable are randomly
sampled from the observed values of this variable, attempting
to match the observed covariates of the group sampled from with
the observed covariates of the value to be imputed.

I've not used aregImpute, but from reading ?aregImpute it does
seem that there is an underlying "hot deck" mechanism, so it
may suit your purpose well. However, from the "Description"
and "Details" of aregImpute, it seems that there is also an
element of quasi-modelling involved as well, albeit on a basically
non-parametric basis.

The person to comment on this would be Frank Harrell himself!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Jul-05                                       Time: 16:49:04
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Sat Jul  9 17:57:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Jul 2005 08:57:41 -0700
Subject: [R] Finite Mixture Models with logistic regression
In-Reply-To: <971536df05070908477c797917@mail.gmail.com>
References: <43599CE4@zathras> <42CFEE9C.20207@pdf.com>
	<971536df05070908477c797917@mail.gmail.com>
Message-ID: <42CFF3F5.1000907@pdf.com>

Hi, Gabor:

	  You are correct, Gabor.  I also found Lindsey's home page via 
www.r-project.org -> CRAN -> (select a local mirror) -> Software:  Other 
-> "R-related projects" -> "Jim Lindsey's R page".  What I meant to say 
but wasn't clear was that the standard install.packages('gnlm') did not 
work.

	  Thanks for the clarification.
	  spencer graves

Gabor Grothendieck wrote:

> On 7/9/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
> 
>>Lindsey's
>>packages are not available for all platforms and so are not available on
>>CRAN.  
> 
> 
> Is it true that all CRAN packages in the main repository are available on 
> all platforms?
> 
> (There is at least an other-software area:
>    http://cran.r-project.org/other-software.html
> where things that do not fit into the main repository can go.)

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ssquid at gmail.com  Sat Jul  9 18:04:39 2005
From: ssquid at gmail.com (Y Y)
Date: Sat, 9 Jul 2005 11:04:39 -0500
Subject: [R] package loading smooth.lf (LOCFIT),
	couldn't find function "smooth.lf"
In-Reply-To: <200504141208.22869.maarranz@tol-project.org>
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A8@exs1.backup>
	<200504141208.22869.maarranz@tol-project.org>
Message-ID: <7148b3d005070909046ce3d3f9@mail.gmail.com>

After loading locfit, I am unable to access functions within locfit.

following
http://www.herine.net/locfit/start.html

> library("locfit")
> x <- 10*runif(100)
> y <- 5*sin(x)+rnorm(100)
> fit <- smooth.lf(x,y)
Error: couldn't find function "smooth.lf"


> fit <- locfit(y~lp(x))
Error in eval(expr, envir, enclos) : couldn't find function "lp"

library() or package manager GUI  tells me locfit is loaded

Any ideas on how to fix this ?

SS, running on R 2.1, MacOS 10.3.9



From p.dalgaard at biostat.ku.dk  Sat Jul  9 19:11:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Jul 2005 19:11:43 +0200
Subject: [R] Finite Mixture Models with logistic regression
In-Reply-To: <971536df05070908477c797917@mail.gmail.com>
References: <43599CE4@zathras> <42CFEE9C.20207@pdf.com>
	<971536df05070908477c797917@mail.gmail.com>
Message-ID: <x2hdf3hpkg.fsf@turmalin.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> On 7/9/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
> > Lindsey's
> > packages are not available for all platforms and so are not available on
> > CRAN.  
> 
> Is it true that all CRAN packages in the main repository are available on 
> all platforms?

No, but I don't think that's why Jim doesn't want to put his packages
on CRAN. Rather, he doesn't believe in documentation standards,
version control, automated cross-platform checking and such (as
opposed to retaining full control and being able to change his code at
will).

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ravidmurthy at yahoo.com  Sat Jul  9 22:15:38 2005
From: ravidmurthy at yahoo.com (Ravi Murthy)
Date: Sat, 9 Jul 2005 13:15:38 -0700 (PDT)
Subject: [R] Quantile normalization and NA
Message-ID: <20050709201538.97346.qmail@web30712.mail.mud.yahoo.com>

Hi,
I am new to R,
I am doing quantile normalization with a dat matix of
384X124 and I find that while computing the quantile
normailzation it introduces 'NA' into some of the
cells, can someone help me to overcome this problem ?


This is the command that goes like upto g62 for 124
colomns

>g1 <- normalize.quantiles(exprs(MSExpr[,1:2]))

For a small set of data there is no problem, but for a
large set of data, it introduces "NA" in the place
where it is suppose to geneerate data .

Ravi
ravidmurthy at yahoo.com  

Raviabi



From spencer.graves at pdf.com  Sat Jul  9 22:38:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Jul 2005 13:38:08 -0700
Subject: [R] R.oo static field
In-Reply-To: <3f87cc6d050708091078b95235@mail.gmail.com>
References: <3f87cc6d050708091078b95235@mail.gmail.com>
Message-ID: <42D035B0.5030306@pdf.com>

	  Since I've seen no replies to your question, I will offer brief 
comments that will certainly not meet your needs but might get you 
started.  I don't know what you mean by a "static member of a class".  I 
just got 3 hits from RSiteSearch("static member of a class"), but I 
don't know if any one of them would help you.

	  You might get more replies if you prepare another post giving a very 
short toy example in the form of a very few R commands to help explain 
what you want to do and the deficiencies in what you tried, using the 
posting guide "http://www.R-project.org/posting-guide.html" to help you 
prepare your question.  The archives contain much information on 
classes.  In particular, you might get something from Thomas Lumley's 
"Programmers Nitch" in the June 2004 R News.  I also recommend chapters 
4 and 5 in Venables and Ripley (2000) S Programming (Springer);  the 
"polynomial" class discussed in sec. 4.3 is implemented in R package 
"polynom".  The vision for what classes have become in recent versions 
of S-Plus and R is outlined in Chambers (1998) Programming with Data 
(Springer).  However, the actual implementation in S-Plus and R were 
sufficiently different from that book, at least when I tried to read it 
a few years ago, that I found it somewhat frustrating.

	  spencer graves

Omar Lakkis wrote:

> How can I define a static member of a class? not a static method,
> rather a static field that would be accessed by all instances of the
> class.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sat Jul  9 22:57:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Jul 2005 13:57:35 -0700
Subject: [R] explained deviance in multinom
In-Reply-To: <EA91707AE6F4C84495513EFF5117E897062217AF@VS2.hdi.tvcabo>
References: <EA91707AE6F4C84495513EFF5117E897062217AF@VS2.hdi.tvcabo>
Message-ID: <42D03A3F.7010802@pdf.com>

	  RSiteSearch("R-squared for glm") produced 17 hits, the second of 
which cited three articles that might interest you 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48688.html).  Other 
comments suggest that the percent of deviance explained may not be 
terribly useful (e.g., 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48689.html).

	  If this is not adequte, PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html" and submit another 
question.  If you follow the posting guide, I believe it will likely 
shorten the time it takes you to solve whatever problem brought you to R.

	  spencer graves

alexbri wrote:

> Hi:
> 
>  
> 
> I'm working with multinomial models with library nnet, and I'm trying to get the explained deviance (pseudo R^2) of my models.
> 
> I am assuming that:
> 
> pseudo R^2= 1 - dev(model) / dev (null)
> 
> where dev(model) is the deviance for the fitted model and dev(null) is the deviance for the null model (with the intercept only).
> 
>  
> 
> library(nnet)
> 
> full.model<- multinom(cbind(factor1, factor2 ,., factor5) ~ x1 + x2 + x3, weights=total, data=mydata)
> 
> null.model<- multinom(cbind(factor1, factor2 ,., factor5) ~ +1, weights=total, data=mydata)
> 
>  
> 
> Then I calculated 
> 
> pseudoR^2 = 1 - full.model$deviance / null.model$deviance
> 
>  
> 
> I'm obtaining very low values for pseudoR^2 (there is not much difference between the deviances of the two models). full.model fits (graphically) very well to the data , so I think that the problem is in the null.model (maybe it is not well defined) or with the calculus of pseudoR^2.
> 
>  
> 
> Can someone please give me some suggestions about this?
> 
> Thanks in advance
> 
> alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sat Jul  9 23:24:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Jul 2005 14:24:43 -0700
Subject: [R] Garch in a model with explanatory variables
In-Reply-To: <3565145.1120832648935.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
References: <3565145.1120832648935.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
Message-ID: <42D0409B.3080506@pdf.com>

	  RSiteSearch("GARCH with explanatory variables") produced one 
irrelevant hit, but RSiteSearch("GARCH with explanatory variables") 
produced 15 hits.  Tobias Muhlhofer suggested one could "write out the 
loglikelihood (and possibly its gradient) which you could then maximise 
with optim() and friends. Adrian's GARCH(1,1) is hard-coded with an 
analytic gradient -- the convenience of having this powerful routine 
pre-made and comes at the price of its lack of flexibility.  Diethelm's 
fSeries from Rmetrics can estimate Garch models by calling Ox. That
may work for you too." 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/52132.html).  Even if 
none of these solved your problem, you could copy the code of something 
that seemed close to what you wanted, walk through it line (or fill it 
with print statements) until you understood it well enough to modify it 
to do what you want.

	  Phineas Campbell replied to an apparently similar question, "This 
might be one of those situations in which you should say what what
you are trying to do rather than how you are trying to do it." 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/52144.html).

	  I doubt if this will answer your question, but it may provide some 
useful ideas.  Feel free to submit another question, but I suggest you 
read the posting guide! http://www.R-project.org/posting-guide.html;  I 
think that guide will likely increase your chances of getting a useful 
reply.

	  spencer graves	

Carlo Fezzi wrote:

> Dear helpers,
> 
> does anyone know a function to fit a model with:
> 
> - y mean that is regressed on a set of explanatory variables
> 
> - y variace behaving as a garch or as a garch in mean
> 
> 
> Thank you so much for your help,
> 
> Carlo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From corr at fas.harvard.edu  Sun Jul 10 00:33:10 2005
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Sat, 9 Jul 2005 18:33:10 -0400 (EDT)
Subject: [R] aregImpute: beginner's question
Message-ID: <Pine.LNX.4.58.0507091819390.28124@ls03.fas.harvard.edu>



Hello R-help,

Thanks for everyone's very helpful suggestions so far. I am now trying to
use aregImpute for my missing data imputation. Here are the code and error
messages. Any suggestions would be very much appreciated.

Sincerely,

Anders Corr


########################################

#Question for R-Help on aregImpute

########################################


#DOWNLOAD DATA (61Kb)
download.file("http://www.people.fas.harvard.edu/~corr/tc.csv","C:/R")
tc <- read.csv("tc.csv", header = TRUE)
d <- as.data.frame(tc)
n <- naclus(d)
plot(n); naplot(n)  # Show patterns of NAs

#RUN aregImpute
set.seed(5)
f  <- aregImpute(~y +
podb2+propdemocracy+avetrade1984dollars+concentration+cycle+polarity+propmid+terrgainer+
demgainer+ fedgainer+ popdengainer+ urbpopgainer+ tradeopgainer+
gdppcgainer+ terrloser+ demloser+ fedloser+ popdenloser+ urbpoploser+
tradeoploser+ gdppcloser, n.impute=100, defaultLinear=TRUE, data=d)
par(mfrow=c(2,3))
plot(f, diagnostics=TRUE, maxn=2)
fmi <- fit.mult.impute(y ~
podb2+propdemocracy+avetrade1984dollars+concentration+cycle+polarity+propmid+terrgainer+
demgainer+ fedgainer+ popdengainer+ urbpopgainer+ tradeopgainer+
gdppcgainer+ terrloser+ demloser+ fedloser+ popdenloser+ urbpoploser+
tradeoploser+ gdppcloser, lm, f,
                       data=d)






> ########################################
>
> #Question for R-Help on aregImpute
>
> ########################################
>
>
> #DOWNLOAD DATA (61Kb)
> download.file("http://www.people.fas.harvard.edu/~corr/tc.csv","C:/R")
trying URL `http://www.people.fas.harvard.edu/~corr/tc.csv'
Content type `text/plain' length 62770 bytes
opened URL
downloaded 61Kb

> tc <- read.csv("tc.csv", header = TRUE)
> d <- as.data.frame(tc)
> n <- naclus(d)
> plot(n); naplot(n)  # Show patterns of NAs
>
> #RUN aregImpute
> set.seed(5)
> f  <- aregImpute(~y +
podb2+propdemocracy+avetrade1984dollars+concentration+cycle+polarity+propmid+terrgainer+
demgainer+ fedgainer+ popdengainer+ urbpopgainer+ tradeopgainer+
gdppcgainer+ terrloser+ demloser+ fedloser+ popdenloser+ urbpoploser+
tradeoploser+ gdppcloser, n.impute=100, defaultLinear=TRUE, data=d)
Iteration:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
25 26 27 28 29 30 31 32 33 34
Error in lm.fit.qr.bare(f$tx, f$ty) :
NA/NaN/Inf in foreign function call (arg 1)
> par(mfrow=c(2,3))
> plot(f, diagnostics=TRUE, maxn=2)
2222222222222222222222> fmi <- fit.mult.impute(y ~
podb2+propdemocracy+avetrade1984dollars+concentration+cycle+polarity+propmid+terrgainer+
demgainer+ fedgainer+ popdengainer+ urbpopgainer+ tradeopgainer+
gdppcgainer+ terrloser+ demloser+ fedloser+ popdenloser+ urbpoploser+
tradeoploser+ gdppcloser, lm, f,
+                        data=d)
Error in impute.transcan(xtrans, imputation = i, data = data, list.out =
TRUE,  :
        inconsistant naming of observations led to differing length
vectors
>



From joseclaudio.faria at terra.com.br  Sun Jul 10 02:42:15 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sat, 09 Jul 2005 21:42:15 -0300
Subject: [R] Help to make a specific matrix
Message-ID: <42D06EE7.9000407@terra.com.br>

Dear R users,

The solution is probably simple but I need someone to point me to it.
How can I to generate a matrix from a numeric sequence of 1:10 like 'A' or 'B' 
below:

A)
|--------------------|
|      1  2  3  4  5 |
|--------------------|
| 1 |  0             |
| 2 |  1  0          |
| 3 |  2  5  0       |
| 4 |  3  6  8  0    |
| 5 |  4  7  9 10  0 |
|--------------------|

B)
|--------------------|
|      1  2  3  4  5 |
|--------------------|
| 1 |  0  1  2  3  4 |
| 2 |  1  0  5  6  7 |
| 3 |  2  5  0  8  9 |
| 4 |  3  6  8  0 10 |
| 5 |  4  7  9 10  0 |
|--------------------|

This question is related with the possible combinations of five objects two the 
two, i.e, C(5,2).

Any help would be greatly appreciated.

Regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ggrothendieck at gmail.com  Sun Jul 10 03:21:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 9 Jul 2005 21:21:42 -0400
Subject: [R] Help to make a specific matrix
In-Reply-To: <42D06EE7.9000407@terra.com.br>
References: <42D06EE7.9000407@terra.com.br>
Message-ID: <971536df05070918214db16adb@mail.gmail.com>

On 7/9/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Dear R users,
> 
> The solution is probably simple but I need someone to point me to it.
> How can I to generate a matrix from a numeric sequence of 1:10 like 'A' or 'B'
> below:
> 
> A)
> |--------------------|
> |      1  2  3  4  5 |
> |--------------------|
> | 1 |  0             |
> | 2 |  1  0          |
> | 3 |  2  5  0       |
> | 4 |  3  6  8  0    |
> | 5 |  4  7  9 10  0 |
> |--------------------|
> 
> B)
> |--------------------|
> |      1  2  3  4  5 |
> |--------------------|
> | 1 |  0  1  2  3  4 |
> | 2 |  1  0  5  6  7 |
> | 3 |  2  5  0  8  9 |
> | 4 |  3  6  8  0 10 |
> | 5 |  4  7  9 10  0 |
> |--------------------|
> 

Try this and see ?lower.tri

A <- matrix(0,5,5)
A[lower.tri(A)] <- 1:10
B <- A + t(A)



From joseclaudio.faria at terra.com.br  Sun Jul 10 03:30:35 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sat, 09 Jul 2005 22:30:35 -0300
Subject: [R] Help to make a specific matrix
In-Reply-To: <971536df05070918214db16adb@mail.gmail.com>
References: <42D06EE7.9000407@terra.com.br>
	<971536df05070918214db16adb@mail.gmail.com>
Message-ID: <42D07A3B.7030107@terra.com.br>

Gabor Grothendieck wrote:
> On 7/9/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> 
>>Dear R users,
>>
>>The solution is probably simple but I need someone to point me to it.
>>How can I to generate a matrix from a numeric sequence of 1:10 like 'A' or 'B'
>>below:
>>
>>A)
>>|--------------------|
>>|      1  2  3  4  5 |
>>|--------------------|
>>| 1 |  0             |
>>| 2 |  1  0          |
>>| 3 |  2  5  0       |
>>| 4 |  3  6  8  0    |
>>| 5 |  4  7  9 10  0 |
>>|--------------------|
>>
>>B)
>>|--------------------|
>>|      1  2  3  4  5 |
>>|--------------------|
>>| 1 |  0  1  2  3  4 |
>>| 2 |  1  0  5  6  7 |
>>| 3 |  2  5  0  8  9 |
>>| 4 |  3  6  8  0 10 |
>>| 5 |  4  7  9 10  0 |
>>|--------------------|
>>
> 
> 
> Try this and see ?lower.tri
> 
> A <- matrix(0,5,5)
> A[lower.tri(A)] <- 1:10
> B <- A + t(A)

Dear Gabor,

Thank you, very much: this soluction is nice!

Best,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ligges at statistik.uni-dortmund.de  Sun Jul 10 12:19:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 10 Jul 2005 12:19:00 +0200
Subject: [R] package loading smooth.lf (LOCFIT),
 couldn't find function "smooth.lf"
In-Reply-To: <7148b3d005070909046ce3d3f9@mail.gmail.com>
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A8@exs1.backup>	<200504141208.22869.maarranz@tol-project.org>
	<7148b3d005070909046ce3d3f9@mail.gmail.com>
Message-ID: <42D0F614.2060709@statistik.uni-dortmund.de>

Y Y wrote:

> After loading locfit, I am unable to access functions within locfit.
> 
> following
> http://www.herine.net/locfit/start.html
> 
> 
>>library("locfit")
>>x <- 10*runif(100)
>>y <- 5*sin(x)+rnorm(100)
>>fit <- smooth.lf(x,y)
> 
> Error: couldn't find function "smooth.lf"

So it is time to ask the maintainers of the package and the cited URL 
(CCing both) to update at least one of them (package or 
http://www.herine.net/locfit/start.html)

While the web page states the last update is "(December 16, 2004 
version)", the version on CRAN is locfit_1.1-9.tar.gz dated 14-Sep-2004.

Uwe Ligges



> 
> 
>>fit <- locfit(y~lp(x))
> 
> Error in eval(expr, envir, enclos) : couldn't find function "lp"
> 
> library() or package manager GUI  tells me locfit is loaded
> 
> Any ideas on how to fix this ?
> 
> SS, running on R 2.1, MacOS 10.3.9
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From t.muhlhofer at lse.ac.uk  Sun Jul 10 12:22:58 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Sun, 10 Jul 2005 11:22:58 +0100
Subject: [R] Garch in a model with explanatory variables
In-Reply-To: <mailman.9.1120989601.32475.r-help@stat.math.ethz.ch>
References: <mailman.9.1120989601.32475.r-help@stat.math.ethz.ch>
Message-ID: <42D0F702.3000207@lse.ac.uk>

The Ox interface in fSeries is quite an easy way to accomplish this, 
although it produces some garbage, both in your current environment 
within R, as well as in the directory in which you are running R. You 
have to be careful also if you're on a Linux or other UNIX system, as 
the function has Windows pathnames hard coded into it, which you need to 
alter to the ones where your Ox resides.

Tobias



From ligges at statistik.uni-dortmund.de  Sun Jul 10 12:34:41 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 10 Jul 2005 12:34:41 +0200
Subject: [R] Quantile normalization and NA
In-Reply-To: <20050709201538.97346.qmail@web30712.mail.mud.yahoo.com>
References: <20050709201538.97346.qmail@web30712.mail.mud.yahoo.com>
Message-ID: <42D0F9C1.4030602@statistik.uni-dortmund.de>

Ravi Murthy wrote:

> Hi,
> I am new to R,
> I am doing quantile normalization with a dat matix of
> 384X124 and I find that while computing the quantile
> normailzation it introduces 'NA' into some of the
> cells, can someone help me to overcome this problem ?
> 
> 
> This is the command that goes like upto g62 for 124
> colomns
> 
> 
>>g1 <- normalize.quantiles(exprs(MSExpr[,1:2]))

Do you mean the function normalize.quantiles() from package "affy" 
(please always tell the package, if the function is not in base R)?
It's more appropriate to ask on the Bioconductor mailing list if 
Bioconductor packages are the subject of interest.

And you might want to give a simple, reproducible, but 
non-bandwith-wasting example (perhaps by uploadiung data to some web 
site) in order to make the Bioconductor folks able help you.

Uwe Ligges

> 
> For a small set of data there is no problem, but for a
> large set of data, it introduces "NA" in the place
> where it is suppose to geneerate data .
> 
> Ravi
> ravidmurthy at yahoo.com  
> 
> Raviabi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From torenizh at post.tau.ac.il  Sun Jul 10 15:24:32 2005
From: torenizh at post.tau.ac.il (Yzhar Toren)
Date: Sun, 10 Jul 2005 15:24:32 +0200
Subject: [R] Using a string as a filter
Message-ID: <42D12190.4020405@post.tau.ac.il>

Hi ,

I want to be able to filter out results using a string. I'm running an 
automated script that reads a list of filters I get from an external 
source and applys them to my data frame consecutively.

For example I want to get : data[protocol==1], data[protocol==2] ...

If I define
filter1 <- "protocol==1" (as a string)
filter2 <- "protocol==2"
...
How can I use these variables to choose subsets ?

I managed to find a very awkward method by using a function that calls a 
formula (and using as.formula() for the string I want to get), but I 
would love to find a more efficient way

Thank you !
Yzhar Toren, Tel-Aviv university.



From andy_liaw at merck.com  Sun Jul 10 14:26:41 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 10 Jul 2005 08:26:41 -0400
Subject: [R] package loading smooth.lf (LOCFIT),
 couldn't find functio n "smooth.lf"
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA81@usctmx1106.Merck.com>

The version of locfit on the web site mentioned apparently has been revised
by Prof. Loader, and is newer than the CRAN version that I have been
maintaining.  If Prof. Loader is OK with it, I will take a look and see if I
can get the new version into CRAN-conforming form and upload to CRAN.

Meanwhile, make sure you're using the package from Prof. Loader's web page,
instead of the one on CRAN, if you want the new features.

Andy

> From: Uwe Ligges 
> 
> Y Y wrote:
> 
> > After loading locfit, I am unable to access functions within locfit.
> > 
> > following
> > http://www.herine.net/locfit/start.html
> > 
> > 
> >>library("locfit")
> >>x <- 10*runif(100)
> >>y <- 5*sin(x)+rnorm(100)
> >>fit <- smooth.lf(x,y)
> > 
> > Error: couldn't find function "smooth.lf"
> 
> So it is time to ask the maintainers of the package and the cited URL 
> (CCing both) to update at least one of them (package or 
> http://www.herine.net/locfit/start.html)
> 
> While the web page states the last update is "(December 16, 2004 
> version)", the version on CRAN is locfit_1.1-9.tar.gz dated 
> 14-Sep-2004.
> 
> Uwe Ligges
> 
> 
> 
> > 
> > 
> >>fit <- locfit(y~lp(x))
> > 
> > Error in eval(expr, envir, enclos) : couldn't find function "lp"
> > 
> > library() or package manager GUI  tells me locfit is loaded
> > 
> > Any ideas on how to fix this ?
> > 
> > SS, running on R 2.1, MacOS 10.3.9
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ramasamy at cancer.org.uk  Sun Jul 10 14:31:49 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 10 Jul 2005 13:31:49 +0100
Subject: [R] Quantile normalization and NA
In-Reply-To: <42D0F9C1.4030602@statistik.uni-dortmund.de>
References: <20050709201538.97346.qmail@web30712.mail.mud.yahoo.com>
	<42D0F9C1.4030602@statistik.uni-dortmund.de>
Message-ID: <1120998709.6100.7.camel@dhcp-63.ccc.ox.ac.uk>

I agree with Uwe on both points.

Have you checked if your inputs, i.e. exprs(MSExpr[ ,1:2]), contain
missing values to begin with ?

Out of curiosity are you trying to apply this method to two colour
arrays ? You might want to enquire the BioConductor mailing list about
the merits of doing this over standard techniques (i.e. LOESS
normalisation) as I do not think this is widely done.

Regards, Adai



On Sun, 2005-07-10 at 12:34 +0200, Uwe Ligges wrote:
> Ravi Murthy wrote:
> 
> > Hi,
> > I am new to R,
> > I am doing quantile normalization with a dat matix of
> > 384X124 and I find that while computing the quantile
> > normailzation it introduces 'NA' into some of the
> > cells, can someone help me to overcome this problem ?
> > 
> > 
> > This is the command that goes like upto g62 for 124
> > colomns
> > 
> > 
> >>g1 <- normalize.quantiles(exprs(MSExpr[,1:2]))
> 
> Do you mean the function normalize.quantiles() from package "affy" 
> (please always tell the package, if the function is not in base R)?
> It's more appropriate to ask on the Bioconductor mailing list if 
> Bioconductor packages are the subject of interest.
> 
> And you might want to give a simple, reproducible, but 
> non-bandwith-wasting example (perhaps by uploadiung data to some web 
> site) in order to make the Bioconductor folks able help you.
> 
> Uwe Ligges
> 
> > 
> > For a small set of data there is no problem, but for a
> > large set of data, it introduces "NA" in the place
> > where it is suppose to geneerate data .
> > 
> > Ravi
> > ravidmurthy at yahoo.com  
> > 
> > Raviabi
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Sun Jul 10 14:36:37 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 10 Jul 2005 13:36:37 +0100
Subject: [R] Help to make a specific matrix
In-Reply-To: <42D06EE7.9000407@terra.com.br>
References: <42D06EE7.9000407@terra.com.br>
Message-ID: <1120998997.6100.12.camel@dhcp-63.ccc.ox.ac.uk>

In addition to Gabor's solution, you might be interested in the
combinations function from the gtools package.

 library(gtools)
 combinations(5,2)
      [,1] [,2]
 [1,]    1    2
 [2,]    1    3
 [3,]    1    4
 [4,]    1    5
 [5,]    2    3
 [6,]    2    4
 [7,]    2    5
 [8,]    3    4
 [9,]    3    5
[10,]    4    5



On Sat, 2005-07-09 at 21:42 -0300, Jose Claudio Faria wrote:
> Dear R users,
> 
> The solution is probably simple but I need someone to point me to it.
> How can I to generate a matrix from a numeric sequence of 1:10 like 'A' or 'B' 
> below:
> 
> A)
> |--------------------|
> |      1  2  3  4  5 |
> |--------------------|
> | 1 |  0             |
> | 2 |  1  0          |
> | 3 |  2  5  0       |
> | 4 |  3  6  8  0    |
> | 5 |  4  7  9 10  0 |
> |--------------------|
> 
> B)
> |--------------------|
> |      1  2  3  4  5 |
> |--------------------|
> | 1 |  0  1  2  3  4 |
> | 2 |  1  0  5  6  7 |
> | 3 |  2  5  0  8  9 |
> | 4 |  3  6  8  0 10 |
> | 5 |  4  7  9 10  0 |
> |--------------------|
> 
> This question is related with the possible combinations of five objects two the 
> two, i.e, C(5,2).
> 
> Any help would be greatly appreciated.
> 
> Regards,



From ggrothendieck at gmail.com  Sun Jul 10 14:49:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 10 Jul 2005 08:49:42 -0400
Subject: [R] Using a string as a filter
In-Reply-To: <42D12190.4020405@post.tau.ac.il>
References: <42D12190.4020405@post.tau.ac.il>
Message-ID: <971536df05071005495dc57196@mail.gmail.com>

On 7/10/05, Yzhar Toren <torenizh at post.tau.ac.il> wrote:
> Hi ,
> 
> I want to be able to filter out results using a string. I'm running an
> automated script that reads a list of filters I get from an external
> source and applys them to my data frame consecutively.
> 
> For example I want to get : data[protocol==1], data[protocol==2] ...
> 
> If I define
> filter1 <- "protocol==1" (as a string)
> filter2 <- "protocol==2"
> ...

protocol <- 1:5
data <- 11:15
filter <- "protocol==1"
data[eval(parse(text=filter))] # 11



From kjetil at acelerate.com  Sun Jul 10 14:51:54 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 10 Jul 2005 08:51:54 -0400
Subject: [R] comparing strength of association instead of strength of
 evidence?
In-Reply-To: <cdf817830507081005aadac2f@mail.gmail.com>
References: <cdf817830506240759cd3ae6a@mail.gmail.com>	<42BC5EA0.2030202@acelerate.com>	<971536df05062416214e404045@mail.gmail.com>
	<cdf817830507081005aadac2f@mail.gmail.com>
Message-ID: <42D119EA.2060306@acelerate.com>

Weiwei Shi wrote:

>Dear all:
>I still need some further help since I think the question itself might
>be very interesting (i hope so:) :
>the question is on chisq.test, my concern is which criteria should be
>used here to evaluate the independence. The reason i use this old
>subject of the email is, b/c I think the problem here is about how to
>look at p.value, which evaluate the strength of evidence instead of
>association. If i am wrong, please correct me.
>
>the result looks like this:
>   index   word.comb     id in.class0 in.class1      p.value odds.ratio
>1      1  TOTAL|LAID 54|241         2         4 0.0004997501 0.00736433
>2      2 THEFT|RECOV  52|53     40751       146 0.0004997501 4.17127643
>3      3  BOLL|ACCID  10|21     36825      1202 0.0004997501 0.44178546
>4      4  LAB|VANDAL   8|55     24192       429 0.0004997501 0.82876099
>5      5 VANDAL|CAUS  55|59       801        64 0.0004997501 0.18405918
>6      6    AI|TOTAL   9|54      1949        45 0.0034982509 0.63766766
>7      7    AI|RECOV   9|53      2385        61 0.0004997501 0.57547012
>8      8 THEFT|TOTAL  52|54     33651       110 0.0004997501 4.56174408
>
>the target is to look for best subset of word.comb to differentiate
>between class0 and class1. p.value is obtained via
>p.chisq.sim[i] <- as.double(chisq.test(tab, sim=TRUE, B=myB)$p.value)
>and B=20000 (I increased B and it won't help. the margin here is
>class0=2162792
>class1=31859
>)
>
>So, in conclusion, which one I should use first, odds.ratio or p.value
>to find the best subset.
>
>  
>
If your goal is to discriminate between two different classes, why not 
calculate directly
a measure of discriminative ability, such as probability of correct 
classification?

Kjetil

>I read some on feature selection in text categorization (A comparative
>study on feature selection in text categorization might be a good
>ref.). Anyone here has other suggestions?
>
>thanks,
>
>weiwei
>
>
>On 6/24/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>  
>
>>On 6/24/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
>>    
>>
>>>Weiwei Shi wrote:
>>>
>>>      
>>>
>>>>Hi,
>>>>I asked this question before, which was hidden in a bunch of
>>>>questions. I repharse it here and hope I can get some help this time:
>>>>
>>>>I have 2 contingency tables which have the same group variable Y. I
>>>>want to compare the strength of association between X1/Y and X2/Y. I
>>>>am not sure if comparing p-values IS the way  even though the
>>>>probability of seeing such "weird" observation under H0 defines
>>>>p-value and it might relate to the strength of association somehow.
>>>>But I read the following statement from Alan Agresti's "An
>>>>Introduction to Categorical Data Analysis" :
>>>>"Chi-squared tests simply indicate the degree of EVIDENCE for an
>>>>association....It is sensible to decompose chi-squared into
>>>>components, study residuals, and estimate parameters such as odds
>>>>ratios that describe the STRENGTH OF ASSOCIATION".
>>>>
>>>>
>>>>
>>>>        
>>>>
>>>Here are some things you can do:
>>>
>>> > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
>>>
>>> > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
>>> > library(epitools) # on CRAN
>>> > ?odds.ratio
>>>Help for 'odds.ratio' is shown in the browser
>>> > library(help=epitools) # on CRAN
>>> > tab1
>>>     [,1]    [,2]
>>>[1,] 11266 2151526
>>>[2,]   125   31734
>>> > odds.ratio(11266, 125, 2151526, 31734)
>>>Error in fisher.test(tab) : FEXACT error 40.
>>>Out of workspace.                 # so this are evidently for tables
>>>with smaller counts
>>> > library(vcd) # on CRAN
>>>
>>> > ?oddsratio
>>>Help for 'oddsratio' is shown in the browser
>>> > oddsratio( tab1)  # really is logodds ratio
>>>[1] 0.2807548
>>> > plot(oddsratio( tab1) )
>>> > library(help=vcd) # on CRAN  Read this for many nice functions.
>>> > fourfoldplot(tab1)
>>> > mosaicplot(tab1)     # not really usefull for this table
>>>
>>>Also has a look at function Crosstable in package gmodels.
>>>
>>>To decompose the chisqure you can program yourselves:
>>>
>>>decomp.chi <- function(tab) {
>>>       rows <-  rowSums(tab)
>>>       cols <-   colSums(tab)
>>>       N <-   sum(rows)
>>>        E <- rows %o% cols / N
>>>        contrib <- (tab-E)^2/E
>>>        contrib }
>>>
>>>
>>> > decomp.chi(tab1)
>>>         [,1]         [,2]
>>>[1,] 0.1451026 0.0007570624
>>>[2,] 9.8504915 0.0513942218
>>> >
>>>
>>>So you can easily see what cell contributes most to the overall chisquared.
>>>
>>>Kjetil
>>>
>>>
>>>
>>>
>>>
>>>      
>>>
>>>>Can I do this "decomposition" in R for the following example including
>>>>2 contingency tables?
>>>>
>>>>
>>>>
>>>>        
>>>>
>>>>>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
>>>>>tab1
>>>>>
>>>>>
>>>>>          
>>>>>
>>>>     [,1]    [,2]
>>>>[1,] 11266 2151526
>>>>[2,]   125   31734
>>>>
>>>>
>>>>
>>>>        
>>>>
>>>>>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
>>>>>tab2
>>>>>
>>>>>
>>>>>          
>>>>>
>>>>     [,1]    [,2]
>>>>[1,] 43571 2119221
>>>>[2,]    52   31807
>>>>
>>>>        
>>>>
>>Here are a few more ways of doing this using chisq.test,
>>glm and assocplot:
>>
>>    
>>
>>>## chisq.test ###
>>>      
>>>
>>>tab1.chisq <- chisq.test(tab1)
>>>      
>>>
>>># decomposition of chisq
>>>resid(tab1.chisq)^2
>>>      
>>>
>>          [,1]         [,2]
>>[1,] 0.1451026 0.0007570624
>>[2,] 9.8504915 0.0513942218
>>
>>    
>>
>>># same
>>>with(tab1.chisq, (observed - expected)^2/expected)
>>>      
>>>
>>          [,1]         [,2]
>>[1,] 0.1451026 0.0007570624
>>[2,] 9.8504915 0.0513942218
>>
>>
>>    
>>
>>># Pearson residuals
>>>resid(tab1.chisq)
>>>      
>>>
>>           [,1]        [,2]
>>[1,]  0.3809234 -0.02751477
>>[2,] -3.1385493  0.22670294
>>
>>    
>>
>>># same
>>>with(tab1.chisq, (observed - expected)/sqrt(expected))
>>>      
>>>
>>           [,1]        [,2]
>>[1,]  0.3809234 -0.02751477
>>[2,] -3.1385493  0.22670294
>>
>>
>>    
>>
>>>### glm ###
>>># Pearson residuals via glm
>>>      
>>>
>>>tab1.df <- data.frame(count = c(tab1), A = gl(2,2), B = gl(2,1,4))
>>>tab1.glm <- glm(count ~ ., tab1.df, family = poisson())
>>>resid(tab1.glm, type = "pearson")
>>>      
>>>
>>          1           2           3           4
>> 0.38092339 -3.13854927 -0.02751477  0.22670294
>>    
>>
>>>plot(tab1.glm)
>>>      
>>>
>>>### assocplot ###
>>># displaying Pearson residuals via an assocplot
>>>assocplot(t(tab1))
>>>      
>>>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From f.harrell at vanderbilt.edu  Sun Jul 10 15:13:36 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 10 Jul 2005 09:13:36 -0400
Subject: [R] aregImpute: beginner's question
In-Reply-To: <Pine.LNX.4.58.0507091819390.28124@ls03.fas.harvard.edu>
References: <Pine.LNX.4.58.0507091819390.28124@ls03.fas.harvard.edu>
Message-ID: <42D11F00.2090205@vanderbilt.edu>

Anders Schwartz Corr wrote:

. . .
> 
> podb2+propdemocracy+avetrade1984dollars+concentration+cycle+polarity+propmid+terrgainer+
> demgainer+ fedgainer+ popdengainer+ urbpopgainer+ tradeopgainer+
> gdppcgainer+ terrloser+ demloser+ fedloser+ popdenloser+ urbpoploser+
> tradeoploser+ gdppcloser, n.impute=100, defaultLinear=TRUE, data=d)
> Iteration:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
> 25 26 27 28 29 30 31 32 33 34
> Error in lm.fit.qr.bare(f$tx, f$ty) :
> NA/NaN/Inf in foreign function call (arg 1)

This is probably a singularity.  Remove one variable at a time from the 
formula and re-run aregImpute.  That may help you find a culprit.  Also, 
you may not need 100 imputations.

Frank

> 
>>par(mfrow=c(2,3))
>>plot(f, diagnostics=TRUE, maxn=2)
> 
> 2222222222222222222222> fmi <- fit.mult.impute(y ~
> podb2+propdemocracy+avetrade1984dollars+concentration+cycle+polarity+propmid+terrgainer+
> demgainer+ fedgainer+ popdengainer+ urbpopgainer+ tradeopgainer+
> gdppcgainer+ terrloser+ demloser+ fedloser+ popdenloser+ urbpoploser+
> tradeoploser+ gdppcloser, lm, f,
> +                        data=d)
> Error in impute.transcan(xtrans, imputation = i, data = data, list.out =
> TRUE,  :
>         inconsistant naming of observations led to differing length
> vectors
> 

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From f.harrell at vanderbilt.edu  Sun Jul 10 15:10:40 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 10 Jul 2005 09:10:40 -0400
Subject: [R] missing data imputation
In-Reply-To: <XFMail.050709150335.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050709150335.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42D11E50.1060209@vanderbilt.edu>

(Ted Harding) wrote:
[...]

> 
> In many cases people simply treat negative estimates of variables
> which are intrinsically non-negative very crudely: if it comes
> out negative, replaceit with zero. This too is often a quick
> fix where the fact that it is a lie simply has no practical
> importance. But, of course, it may matter! That depends ...
> (see above).

That will result in a strange distribution of imputed values.

. . .

> I've also noted Frank Harrel's comment about aregImpute, and
> will bear it in mind. Note, however, that this does not do
> multiple imputation on the same lines as NORM (or the other
> Shafer-derived MI packages). See ?aregImpute section "Details".
> And, specifically, from the "Description":

It is different, but aregImpute approximates the full Bayesian 
procedure.  MICE is another approach to approximating it, and aregImpute 
seems to agree well with MICE when you force linearity in aregImpute 
(because like NORM, MICE cannot handle nonlinearity).

Frank

> 
>   "The 'transcan' function creates flexible additive imputation
>    models but provides only an approximation to true multiple
>    imputation as the imputation models are fixed before all
>    multiple imputations are drawn. This ignores variability
>    caused by having to fit the imputation models. 'aregImpute'
>    takes all aspects of uncertainty in the imputations into
>    account by using the bootstrap to approximate the process
>    of drawing predicted values from a full Bayesian predictive
>    distribution."
> 
> so that the Rubin/Shafer method described above (see paragraph
> about dispersion of imputed values) is not fully implemented.
>   
> Best wishes,
> Ted.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From helprhelp at gmail.com  Sun Jul 10 17:19:27 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sun, 10 Jul 2005 10:19:27 -0500
Subject: [R] comparing strength of association instead of strength of
	evidence?
In-Reply-To: <42D119EA.2060306@acelerate.com>
References: <cdf817830506240759cd3ae6a@mail.gmail.com>
	<42BC5EA0.2030202@acelerate.com>
	<971536df05062416214e404045@mail.gmail.com>
	<cdf817830507081005aadac2f@mail.gmail.com>
	<42D119EA.2060306@acelerate.com>
Message-ID: <cdf8178305071008194aa4880c@mail.gmail.com>

For this step, my purpose is feature construction and feature
subsetting. In this project, I am using contrast association rule
mining to build word-combinations; in the previous example, 2-itemset
was created from CAR and tested for their "dependency" on class for
feature subsetting/selection. Other methods like mutual information
might be used too.  Any methods that won't replicate "mechanisms" in
my following step (see below) can be tried here.  Could you detail
what you meant by "a discriminative measure"?

The whole datasets also contain many non-words variables. To combine
both data mining and text categorization is the focus and interests of
this project. Decision tree, Bayesian network or SVM/LSI might be
candidates.

Thanks for further suggestion,

Weiwei



On 7/10/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> Weiwei Shi wrote:
> 
> >Dear all:
> >I still need some further help since I think the question itself might
> >be very interesting (i hope so:) :
> >the question is on chisq.test, my concern is which criteria should be
> >used here to evaluate the independence. The reason i use this old
> >subject of the email is, b/c I think the problem here is about how to
> >look at p.value, which evaluate the strength of evidence instead of
> >association. If i am wrong, please correct me.
> >
> >the result looks like this:
> >   index   word.comb     id in.class0 in.class1      p.value odds.ratio
> >1      1  TOTAL|LAID 54|241         2         4 0.0004997501 0.00736433
> >2      2 THEFT|RECOV  52|53     40751       146 0.0004997501 4.17127643
> >3      3  BOLL|ACCID  10|21     36825      1202 0.0004997501 0.44178546
> >4      4  LAB|VANDAL   8|55     24192       429 0.0004997501 0.82876099
> >5      5 VANDAL|CAUS  55|59       801        64 0.0004997501 0.18405918
> >6      6    AI|TOTAL   9|54      1949        45 0.0034982509 0.63766766
> >7      7    AI|RECOV   9|53      2385        61 0.0004997501 0.57547012
> >8      8 THEFT|TOTAL  52|54     33651       110 0.0004997501 4.56174408
> >
> >the target is to look for best subset of word.comb to differentiate
> >between class0 and class1. p.value is obtained via
> >p.chisq.sim[i] <- as.double(chisq.test(tab, sim=TRUE, B=myB)$p.value)
> >and B=20000 (I increased B and it won't help. the margin here is
> >class0=2162792
> >class1=31859
> >)
> >
> >So, in conclusion, which one I should use first, odds.ratio or p.value
> >to find the best subset.
> >
> >
> >
> If your goal is to discriminate between two different classes, why not
> calculate directly
> a measure of discriminative ability, such as probability of correct
> classification?
> 
> Kjetil
> 
> >I read some on feature selection in text categorization (A comparative
> >study on feature selection in text categorization might be a good
> >ref.). Anyone here has other suggestions?
> >
> >thanks,
> >
> >weiwei
> >
> >
> >On 6/24/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >
> >>On 6/24/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> >>
> >>
> >>>Weiwei Shi wrote:
> >>>
> >>>
> >>>
> >>>>Hi,
> >>>>I asked this question before, which was hidden in a bunch of
> >>>>questions. I repharse it here and hope I can get some help this time:
> >>>>
> >>>>I have 2 contingency tables which have the same group variable Y. I
> >>>>want to compare the strength of association between X1/Y and X2/Y. I
> >>>>am not sure if comparing p-values IS the way  even though the
> >>>>probability of seeing such "weird" observation under H0 defines
> >>>>p-value and it might relate to the strength of association somehow.
> >>>>But I read the following statement from Alan Agresti's "An
> >>>>Introduction to Categorical Data Analysis" :
> >>>>"Chi-squared tests simply indicate the degree of EVIDENCE for an
> >>>>association....It is sensible to decompose chi-squared into
> >>>>components, study residuals, and estimate parameters such as odds
> >>>>ratios that describe the STRENGTH OF ASSOCIATION".
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>Here are some things you can do:
> >>>
> >>> > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> >>>
> >>> > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> >>> > library(epitools) # on CRAN
> >>> > ?odds.ratio
> >>>Help for 'odds.ratio' is shown in the browser
> >>> > library(help=epitools) # on CRAN
> >>> > tab1
> >>>     [,1]    [,2]
> >>>[1,] 11266 2151526
> >>>[2,]   125   31734
> >>> > odds.ratio(11266, 125, 2151526, 31734)
> >>>Error in fisher.test(tab) : FEXACT error 40.
> >>>Out of workspace.                 # so this are evidently for tables
> >>>with smaller counts
> >>> > library(vcd) # on CRAN
> >>>
> >>> > ?oddsratio
> >>>Help for 'oddsratio' is shown in the browser
> >>> > oddsratio( tab1)  # really is logodds ratio
> >>>[1] 0.2807548
> >>> > plot(oddsratio( tab1) )
> >>> > library(help=vcd) # on CRAN  Read this for many nice functions.
> >>> > fourfoldplot(tab1)
> >>> > mosaicplot(tab1)     # not really usefull for this table
> >>>
> >>>Also has a look at function Crosstable in package gmodels.
> >>>
> >>>To decompose the chisqure you can program yourselves:
> >>>
> >>>decomp.chi <- function(tab) {
> >>>       rows <-  rowSums(tab)
> >>>       cols <-   colSums(tab)
> >>>       N <-   sum(rows)
> >>>        E <- rows %o% cols / N
> >>>        contrib <- (tab-E)^2/E
> >>>        contrib }
> >>>
> >>>
> >>> > decomp.chi(tab1)
> >>>         [,1]         [,2]
> >>>[1,] 0.1451026 0.0007570624
> >>>[2,] 9.8504915 0.0513942218
> >>> >
> >>>
> >>>So you can easily see what cell contributes most to the overall chisquared.
> >>>
> >>>Kjetil
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>>Can I do this "decomposition" in R for the following example including
> >>>>2 contingency tables?
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> >>>>>tab1
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>     [,1]    [,2]
> >>>>[1,] 11266 2151526
> >>>>[2,]   125   31734
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> >>>>>tab2
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>     [,1]    [,2]
> >>>>[1,] 43571 2119221
> >>>>[2,]    52   31807
> >>>>
> >>>>
> >>>>
> >>Here are a few more ways of doing this using chisq.test,
> >>glm and assocplot:
> >>
> >>
> >>
> >>>## chisq.test ###
> >>>
> >>>
> >>>tab1.chisq <- chisq.test(tab1)
> >>>
> >>>
> >>># decomposition of chisq
> >>>resid(tab1.chisq)^2
> >>>
> >>>
> >>          [,1]         [,2]
> >>[1,] 0.1451026 0.0007570624
> >>[2,] 9.8504915 0.0513942218
> >>
> >>
> >>
> >>># same
> >>>with(tab1.chisq, (observed - expected)^2/expected)
> >>>
> >>>
> >>          [,1]         [,2]
> >>[1,] 0.1451026 0.0007570624
> >>[2,] 9.8504915 0.0513942218
> >>
> >>
> >>
> >>
> >>># Pearson residuals
> >>>resid(tab1.chisq)
> >>>
> >>>
> >>           [,1]        [,2]
> >>[1,]  0.3809234 -0.02751477
> >>[2,] -3.1385493  0.22670294
> >>
> >>
> >>
> >>># same
> >>>with(tab1.chisq, (observed - expected)/sqrt(expected))
> >>>
> >>>
> >>           [,1]        [,2]
> >>[1,]  0.3809234 -0.02751477
> >>[2,] -3.1385493  0.22670294
> >>
> >>
> >>
> >>
> >>>### glm ###
> >>># Pearson residuals via glm
> >>>
> >>>
> >>>tab1.df <- data.frame(count = c(tab1), A = gl(2,2), B = gl(2,1,4))
> >>>tab1.glm <- glm(count ~ ., tab1.df, family = poisson())
> >>>resid(tab1.glm, type = "pearson")
> >>>
> >>>
> >>          1           2           3           4
> >> 0.38092339 -3.13854927 -0.02751477  0.22670294
> >>
> >>
> >>>plot(tab1.glm)
> >>>
> >>>
> >>>### assocplot ###
> >>># displaying Pearson residuals via an assocplot
> >>>assocplot(t(tab1))
> >>>
> >>>
> >
> >
> >
> >
> 
> 
> --
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass construction.
>                --  Mahdi Elmandjra
> 
> 
> 
> 
> --
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> Version: 7.0.323 / Virus Database: 267.8.11/44 - Release Date: 08/07/2005
> 
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From joseclaudio.faria at terra.com.br  Sun Jul 10 18:24:07 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sun, 10 Jul 2005 13:24:07 -0300
Subject: [R] Help with Mahalanobis
Message-ID: <42D14BA7.6000108@terra.com.br>

Well, as I did not get a satisfactory reply to the original question I tried to 
make a basic function that, I find, solve the question.

I think it is not the better function, but it is working.

So, perhaps it can be useful to other people.

#
# Calculate the matrix of Mahalanobis Distances between groups
# from data.frames
#
# by: Jos?? Cl??udio Faria
# date: 10/7/05 13:23:48
#

D2Mah = function(y, x) {

   stopifnot(is.data.frame(y), !missing(x))
   stopifnot(dim(y)[1] != dim(x)[1])
   y    = as.matrix(y)
   x    = as.factor(x)
   man  = manova(y ~ x)
   E    = summary(man)$SS[2] #Matrix E
   S    = as.matrix(E$Residuals)/man$df.residual
   InvS = solve(S)
   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))

   colnames(mds) = names(y)
   Objects       = levels(x)
   rownames(mds) = Objects

   library(gtools)
   nObjects = nrow(mds)
   comb     = combinations(nObjects, 2)

   tmpD2 = numeric()
   for (i in 1:dim(comb)[1]){
     a = comb[i,1]
     b = comb[i,2]
     tmpD2[i] = (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,])
   }

   # Thanks Gabor for the below
   tmpMah = matrix(0, nObjects, nObjects, dimnames=list(Objects, Objects))
   tmpMah[lower.tri(tmpMah)] = tmpD2
   D2 = tmpMah + t(tmpMah)
   return(D2)
}

#
# To try
#
D2M = D2Mah(iris[,1:4], iris[,5])
print(D2M)

Thanks all for the complementary aid (specially to Gabor).

Regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ggrothendieck at gmail.com  Sun Jul 10 19:11:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 10 Jul 2005 13:11:15 -0400
Subject: [R] Help with Mahalanobis
In-Reply-To: <42D14BA7.6000108@terra.com.br>
References: <42D14BA7.6000108@terra.com.br>
Message-ID: <971536df05071010111cf3140e@mail.gmail.com>

I think you could simplify this by replacing everything after the
nObjects = nrow(mds) line with just these two statements.

  f <- function(a,b) mapply(function(a,b)
    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)

  D2 <- outer(seq(nObjects), seq(nObjects), f)

This also eliminates dependence on gtools and the complexity
of dealing with triangular matrices.

Regards.

Here it is in full:

D2Mah2 = function(y, x) {

  stopifnot(is.data.frame(y), !missing(x))
  stopifnot(dim(y)[1] != dim(x)[1])
  y    = as.matrix(y)
  x    = as.factor(x)
  man  = manova(y ~ x)
  E    = summary(man)$SS[2] #Matrix E
  S    = as.matrix(E$Residuals)/man$df.residual
  InvS = solve(S)
  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))

  library(gtools)
  nObjects = nrow(mds)

  ### changed part is next two statements
  f <- function(a,b) mapply(function(a,b)
    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)

  D2 <- outer(seq(nObjects), seq(nObjects), f)
}

#
# test
#
D2M2 = D2Mah2(iris[,1:4], iris[,5])
print(D2M2)




On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Well, as I did not get a satisfactory reply to the original question I tried to
> make a basic function that, I find, solve the question.
> 
> I think it is not the better function, but it is working.
> 
> So, perhaps it can be useful to other people.
> 
> #
> # Calculate the matrix of Mahalanobis Distances between groups
> # from data.frames
> #
> # by: Jos?? Cl??udio Faria
> # date: 10/7/05 13:23:48
> #
> 
> D2Mah = function(y, x) {
> 
>   stopifnot(is.data.frame(y), !missing(x))
>   stopifnot(dim(y)[1] != dim(x)[1])
>   y    = as.matrix(y)
>   x    = as.factor(x)
>   man  = manova(y ~ x)
>   E    = summary(man)$SS[2] #Matrix E
>   S    = as.matrix(E$Residuals)/man$df.residual
>   InvS = solve(S)
>   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> 
>   colnames(mds) = names(y)
>   Objects       = levels(x)
>   rownames(mds) = Objects
> 
>   library(gtools)
>   nObjects = nrow(mds)
>   comb     = combinations(nObjects, 2)
> 
>   tmpD2 = numeric()
>   for (i in 1:dim(comb)[1]){
>     a = comb[i,1]
>     b = comb[i,2]
>     tmpD2[i] = (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,])
>   }
> 
>   # Thanks Gabor for the below
>   tmpMah = matrix(0, nObjects, nObjects, dimnames=list(Objects, Objects))
>   tmpMah[lower.tri(tmpMah)] = tmpD2
>   D2 = tmpMah + t(tmpMah)
>   return(D2)
> }
> 
> #
> # To try
> #
> D2M = D2Mah(iris[,1:4], iris[,5])
> print(D2M)
> 
> Thanks all for the complementary aid (specially to Gabor).
> 
> Regards,
> --
> Jose Claudio Faria
> Brasil/Bahia/UESC/DCET
> Estatistica Experimental/Prof. Adjunto
> mails:
>  joseclaudio.faria at terra.com.br
>  jc_faria at uesc.br
>  jc_faria at uol.com.br
> tel: 73-3634.2779
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dusa.adrian at gmail.com  Sun Jul 10 20:13:37 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 10 Jul 2005 18:13:37 +0000 (UTC)
Subject: [R] not supressing leading zeros when reading a table?
Message-ID: <loom.20050710T200503-585@post.gmane.org>

Dear R list,

I have a dataset with a column which should be read as character, like this:

  name surname answer
1 xx   yyy     "00100"
2 rrr  hhh     "01"

When reading this dataset with read.table, I get
1 xx   yyy     100
2 rrr  hhh     1

The string column consists in answers to multiple choice questions, not all 
having the same number of answers. I could format the answers using formatC but 
there are over a hundred different questions in there.

I tried with quote="\"'" without any luck. Googling after this take me nowhere 
either. It should be simple but I seem to miss it...
Can anybody point me to the right direction?

TIA,
Adrian



From MSchwartz at mn.rr.com  Sun Jul 10 22:18:59 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 10 Jul 2005 15:18:59 -0500
Subject: [R] not supressing leading zeros when reading a table?
In-Reply-To: <loom.20050710T200503-585@post.gmane.org>
References: <loom.20050710T200503-585@post.gmane.org>
Message-ID: <1121026739.3924.72.camel@localhost.localdomain>

On Sun, 2005-07-10 at 18:13 +0000, Adrian Dusa wrote:
> Dear R list,
> 
> I have a dataset with a column which should be read as character, like this:
> 
>   name surname answer
> 1 xx   yyy     "00100"
> 2 rrr  hhh     "01"
> 
> When reading this dataset with read.table, I get
> 1 xx   yyy     100
> 2 rrr  hhh     1
> 
> The string column consists in answers to multiple choice questions, not all 
> having the same number of answers. I could format the answers using formatC but 
> there are over a hundred different questions in there.
> 
> I tried with quote="\"'" without any luck. Googling after this take me nowhere 
> either. It should be simple but I seem to miss it...
> Can anybody point me to the right direction?
> 
> TIA,
> Adrian

With your example data saved in a file called "test.txt":

> df <- read.table("test.txt", header = TRUE, colClasses = "character")

> df
  name surname answer
1   xx     yyy  00100
2  rrr     hhh     01

> str(df)
`data.frame':	2 obs. of  3 variables:
 $ name   : chr  "xx" "rrr"
 $ surname: chr  "yyy" "hhh"
 $ answer : chr  "00100" "01"


See the colClasses argument in ?read.table.

HTH,

Marc Schwartz



From murdoch at stats.uwo.ca  Sun Jul 10 22:20:23 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 10 Jul 2005 16:20:23 -0400
Subject: [R] not supressing leading zeros when reading a table?
In-Reply-To: <loom.20050710T200503-585@post.gmane.org>
References: <loom.20050710T200503-585@post.gmane.org>
Message-ID: <42D18307.6010804@stats.uwo.ca>

Adrian Dusa wrote:
> Dear R list,
> 
> I have a dataset with a column which should be read as character, like this:
> 
>   name surname answer
> 1 xx   yyy     "00100"
> 2 rrr  hhh     "01"
> 
> When reading this dataset with read.table, I get
> 1 xx   yyy     100
> 2 rrr  hhh     1
> 
> The string column consists in answers to multiple choice questions, not all 
> having the same number of answers. I could format the answers using formatC but 
> there are over a hundred different questions in there.
> 
> I tried with quote="\"'" without any luck. Googling after this take me nowhere 
> either. It should be simple but I seem to miss it...
> Can anybody point me to the right direction?

By default, read.table guesses about the column type. Yours looks 
numeric, even though it is not.

Use the colClasses argument of read.table to specify the column type. 
If you only have the 3 columns above, colClasses="character" should work.

Duncan Murdoch



From guerinche at gmail.com  Sun Jul 10 22:26:48 2005
From: guerinche at gmail.com (alejandro munoz)
Date: Sun, 10 Jul 2005 15:26:48 -0500
Subject: [R] not supressing leading zeros when reading a table?
In-Reply-To: <loom.20050710T200503-585@post.gmane.org>
References: <loom.20050710T200503-585@post.gmane.org>
Message-ID: <98c62e1105071013264b03c6d0@mail.gmail.com>

Adrian,

To prevent coercion to numeric, try:

mydata <- read.table("myfile", colClasses="character")

HTH.

alejandro
On 7/10/05, Adrian Dusa <dusa.adrian at gmail.com> wrote:
> Dear R list,
> 
> I have a dataset with a column which should be read as character, like this:
> 
>   name surname answer
> 1 xx   yyy     "00100"
> 2 rrr  hhh     "01"
> 
> When reading this dataset with read.table, I get
> 1 xx   yyy     100
> 2 rrr  hhh     1
> 
> The string column consists in answers to multiple choice questions, not all
> having the same number of answers. I could format the answers using formatC but
> there are over a hundred different questions in there.
> 
> I tried with quote="\"'" without any luck. Googling after this take me nowhere
> either. It should be simple but I seem to miss it...
> Can anybody point me to the right direction?
> 
> TIA,
> Adrian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dusa.adrian at gmail.com  Sun Jul 10 22:47:01 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 10 Jul 2005 23:47:01 +0300
Subject: [R] not supressing leading zeros when reading a table?
In-Reply-To: <98c62e1105071013264b03c6d0@mail.gmail.com>
References: <loom.20050710T200503-585@post.gmane.org>
	<98c62e1105071013264b03c6d0@mail.gmail.com>
Message-ID: <be5487e705071013473d43ff62@mail.gmail.com>

On 7/10/05, alejandro munoz <guerinche at gmail.com> wrote:
> Adrian,
> 
> To prevent coercion to numeric, try:
> 
> mydata <- read.table("myfile", colClasses="character")
> 
> HTH.
> 
> alejandro
> On 7/10/05, Adrian Dusa <dusa.adrian at gmail.com> wrote:
> > Dear R list,
> [...snip...]

Thank you all, I got it. 
This is my favourite super fast ever helpful help list (gosh, I didn't
even expect an answer Sundays at 10 pm! ).
Best,
Adrian



From holtlaura at gmail.com  Mon Jul 11 00:29:35 2005
From: holtlaura at gmail.com (Laura Holt)
Date: Sun, 10 Jul 2005 17:29:35 -0500
Subject: [R]  O/T -2 Log Lambda and Chi Square
Message-ID: <16b13251050710152936a1bf77@mail.gmail.com>

Hi R People:

Sorry about the off topic question.  Does anyone know the reference
for "-2 Log Lambda  is approx dist. Chi square", please?

It may be Bartlett, but I'm not sure....

thanks in advance!

Sincerely,
Laura Holt
mailto: holtlaura at gmail.com



From ggrothendieck at gmail.com  Mon Jul 11 01:08:50 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 10 Jul 2005 19:08:50 -0400
Subject: [R] Help with Mahalanobis
In-Reply-To: <971536df05071010111cf3140e@mail.gmail.com>
References: <42D14BA7.6000108@terra.com.br>
	<971536df05071010111cf3140e@mail.gmail.com>
Message-ID: <971536df05071016087e003c1e@mail.gmail.com>

And here is one more simplification using the buildin mahalanobis
function:

D2Mah3 = function(y, x) {

 stopifnot(is.data.frame(y), !missing(x))
 stopifnot(dim(y)[1] != dim(x)[1])
 y    = as.matrix(y)
 x    = as.factor(x)
 man  = manova(y ~ x)
 E    = summary(man)$SS[2] #Matrix E
 S    = as.matrix(E$Residuals)/man$df.residual
 InvS = solve(S)
 mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))

 nObjects = nrow(mds)

 ### changed part is next two statements
 f <- function(a,b) mapply(function(a,b)
   mahalanobis(mds[a,],mds[b,],InvS,TRUE), a, b)

 D2 <- outer(seq(nObjects), seq(nObjects), f)
}

#
# test
#
D2M3 = D2Mah3(iris[,1:4], iris[,5])


On 7/10/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I think you could simplify this by replacing everything after the
> nObjects = nrow(mds) line with just these two statements.
> 
>  f <- function(a,b) mapply(function(a,b)
>    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
> 
>  D2 <- outer(seq(nObjects), seq(nObjects), f)
> 
> This also eliminates dependence on gtools and the complexity
> of dealing with triangular matrices.
> 
> Regards.
> 
> Here it is in full:
> 
> D2Mah2 = function(y, x) {
> 
>  stopifnot(is.data.frame(y), !missing(x))
>  stopifnot(dim(y)[1] != dim(x)[1])
>  y    = as.matrix(y)
>  x    = as.factor(x)
>  man  = manova(y ~ x)
>  E    = summary(man)$SS[2] #Matrix E
>  S    = as.matrix(E$Residuals)/man$df.residual
>  InvS = solve(S)
>  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> 
>  library(gtools)
>  nObjects = nrow(mds)
> 
>  ### changed part is next two statements
>  f <- function(a,b) mapply(function(a,b)
>    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
> 
>  D2 <- outer(seq(nObjects), seq(nObjects), f)
> }
> 
> #
> # test
> #
> D2M2 = D2Mah2(iris[,1:4], iris[,5])
> print(D2M2)
> 
> 
> 
> 
> On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> > Well, as I did not get a satisfactory reply to the original question I tried to
> > make a basic function that, I find, solve the question.
> >
> > I think it is not the better function, but it is working.
> >
> > So, perhaps it can be useful to other people.
> >
> > #
> > # Calculate the matrix of Mahalanobis Distances between groups
> > # from data.frames
> > #
> > # by: Jos?? Cl??udio Faria
> > # date: 10/7/05 13:23:48
> > #
> >
> > D2Mah = function(y, x) {
> >
> >   stopifnot(is.data.frame(y), !missing(x))
> >   stopifnot(dim(y)[1] != dim(x)[1])
> >   y    = as.matrix(y)
> >   x    = as.factor(x)
> >   man  = manova(y ~ x)
> >   E    = summary(man)$SS[2] #Matrix E
> >   S    = as.matrix(E$Residuals)/man$df.residual
> >   InvS = solve(S)
> >   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> >
> >   colnames(mds) = names(y)
> >   Objects       = levels(x)
> >   rownames(mds) = Objects
> >
> >   library(gtools)
> >   nObjects = nrow(mds)
> >   comb     = combinations(nObjects, 2)
> >
> >   tmpD2 = numeric()
> >   for (i in 1:dim(comb)[1]){
> >     a = comb[i,1]
> >     b = comb[i,2]
> >     tmpD2[i] = (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,])
> >   }
> >
> >   # Thanks Gabor for the below
> >   tmpMah = matrix(0, nObjects, nObjects, dimnames=list(Objects, Objects))
> >   tmpMah[lower.tri(tmpMah)] = tmpD2
> >   D2 = tmpMah + t(tmpMah)
> >   return(D2)
> > }
> >
> > #
> > # To try
> > #
> > D2M = D2Mah(iris[,1:4], iris[,5])
> > print(D2M)
> >
> > Thanks all for the complementary aid (specially to Gabor).
> >
> > Regards,
> > --
> > Jose Claudio Faria
> > Brasil/Bahia/UESC/DCET
> > Estatistica Experimental/Prof. Adjunto
> > mails:
> >  joseclaudio.faria at terra.com.br
> >  jc_faria at uesc.br
> >  jc_faria at uol.com.br
> > tel: 73-3634.2779
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From larryxie at gmail.com  Mon Jul 11 01:10:04 2005
From: larryxie at gmail.com (Larry Xie)
Date: Sun, 10 Jul 2005 18:10:04 -0500
Subject: [R] Boxplot in R
Message-ID: <2320026105071016105b72dec4@mail.gmail.com>

I am trying to draw a plot like Matlab does: 

The upper extreme whisker represents 95% of the data;
The upper hinge represents 75% of the data;
The median represents 50% of the data;
The lower hinge represents 25% of the data;
The lower extreme whisker represents 5% of the data.

It looks like:

  ---         95%
   |
   |
-------       75%
|     |
|-----|       50%
|     |
|     |
-------       25%
   |
  ---         5%

Anyone can give me some hints as to how to draw a boxplot like that?
What function does it? I tried boxplot() but couldn't figure it out.
If it's boxplot(), what arguments should I pass to the function? Thank
you for your help. I'd appreciate it.

Larry



From joseclaudio.faria at terra.com.br  Mon Jul 11 01:13:05 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sun, 10 Jul 2005 20:13:05 -0300
Subject: [R] Help with Mahalanobis
In-Reply-To: <971536df05071010111cf3140e@mail.gmail.com>
References: <42D14BA7.6000108@terra.com.br>
	<971536df05071010111cf3140e@mail.gmail.com>
Message-ID: <42D1AB81.8040607@terra.com.br>

Indeed, it is very nice Gabor (as always)!

So, a doubt: how to preserve the 'rowname' and 'colname' of D2, like in the 
first function? I think it is useful to posterior analyzes (as cluster, for 
example).

Regards,

# A small correction (reference to gtools was eliminated)
D2Mah2 = function(y, x) {
   stopifnot(is.data.frame(y), !missing(x))
   stopifnot(dim(y)[1] != dim(x)[1])
   y    = as.matrix(y)
   x    = as.factor(x)
   man  = manova(y ~ x)
   E    = summary(man)$SS[2] #Matrix E
   S    = as.matrix(E$Residuals)/man$df.residual
   InvS = solve(S)
   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
   nObjects = nrow(mds)
   f = function(a,b) mapply(function(a,b)
     (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a, b)
   D2 = outer(seq(nObjects), seq(nObjects), f)
}

#
# test
#
D2M2 = D2Mah2(iris[,1:4], iris[,5])
print(D2M2)

Gabor Grothendieck wrote:
> I think you could simplify this by replacing everything after the
> nObjects = nrow(mds) line with just these two statements.
> 
>   f <- function(a,b) mapply(function(a,b)
>     (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
> 
>   D2 <- outer(seq(nObjects), seq(nObjects), f)
> 
> This also eliminates dependence on gtools and the complexity
> of dealing with triangular matrices.
> 
> Regards.
> 
> Here it is in full:
> 
> D2Mah2 = function(y, x) {
> 
>   stopifnot(is.data.frame(y), !missing(x))
>   stopifnot(dim(y)[1] != dim(x)[1])
>   y    = as.matrix(y)
>   x    = as.factor(x)
>   man  = manova(y ~ x)
>   E    = summary(man)$SS[2] #Matrix E
>   S    = as.matrix(E$Residuals)/man$df.residual
>   InvS = solve(S)
>   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> 
>   library(gtools)
>   nObjects = nrow(mds)
> 
>   ### changed part is next two statements
>   f <- function(a,b) mapply(function(a,b)
>     (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
> 
>   D2 <- outer(seq(nObjects), seq(nObjects), f)
> }
> 
> #
> # test
> #
> D2M2 = D2Mah2(iris[,1:4], iris[,5])
> print(D2M2)
> 
> 
> 
> 
> On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> 
>>Well, as I did not get a satisfactory reply to the original question I tried to
>>make a basic function that, I find, solve the question.
>>
>>I think it is not the better function, but it is working.
>>
>>So, perhaps it can be useful to other people.
>>
>>#
>># Calculate the matrix of Mahalanobis Distances between groups
>># from data.frames
>>#
>># by: Jos?? Cl??udio Faria
>># date: 10/7/05 13:23:48
>>#
>>
>>D2Mah = function(y, x) {
>>
>>  stopifnot(is.data.frame(y), !missing(x))
>>  stopifnot(dim(y)[1] != dim(x)[1])
>>  y    = as.matrix(y)
>>  x    = as.factor(x)
>>  man  = manova(y ~ x)
>>  E    = summary(man)$SS[2] #Matrix E
>>  S    = as.matrix(E$Residuals)/man$df.residual
>>  InvS = solve(S)
>>  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
>>
>>  colnames(mds) = names(y)
>>  Objects       = levels(x)
>>  rownames(mds) = Objects
>>
>>  library(gtools)
>>  nObjects = nrow(mds)
>>  comb     = combinations(nObjects, 2)
>>
>>  tmpD2 = numeric()
>>  for (i in 1:dim(comb)[1]){
>>    a = comb[i,1]
>>    b = comb[i,2]
>>    tmpD2[i] = (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,])
>>  }
>>
>>  # Thanks Gabor for the below
>>  tmpMah = matrix(0, nObjects, nObjects, dimnames=list(Objects, Objects))
>>  tmpMah[lower.tri(tmpMah)] = tmpD2
>>  D2 = tmpMah + t(tmpMah)
>>  return(D2)
>>}
>>
>>#
>># To try
>>#
>>D2M = D2Mah(iris[,1:4], iris[,5])
>>print(D2M)
>>
>>Thanks all for the complementary aid (specially to Gabor).
>>
>>Regards,
>>--
>>Jose Claudio Faria
>>Brasil/Bahia/UESC/DCET
>>Estatistica Experimental/Prof. Adjunto
>>mails:
>> joseclaudio.faria at terra.com.br
>> jc_faria at uesc.br
>> jc_faria at uol.com.br
>>tel: 73-3634.2779
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> Esta mensagem foi verificada pelo E-mail Protegido Terra.
> Scan engine: McAfee VirusScan / Atualizado em 08/07/2005 / Vers??o: 4.4.00 - Dat 4531
> Proteja o seu e-mail Terra: http://mail.terra.com.br/
> 
> 
> 


-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From ggrothendieck at gmail.com  Mon Jul 11 01:26:34 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 10 Jul 2005 19:26:34 -0400
Subject: [R] Help with Mahalanobis
In-Reply-To: <42D1AB81.8040607@terra.com.br>
References: <42D14BA7.6000108@terra.com.br>
	<971536df05071010111cf3140e@mail.gmail.com>
	<42D1AB81.8040607@terra.com.br>
Message-ID: <971536df05071016267beee144@mail.gmail.com>

This one adds the labels:


D2Mah4 = function(y, x) {

 stopifnot(is.data.frame(y), !missing(x))
 stopifnot(dim(y)[1] != dim(x)[1])
 y    = as.matrix(y)
 x    = as.factor(x)
 man  = manova(y ~ x)
 E    = summary(man)$SS[2] #Matrix E
 S    = as.matrix(E$Residuals)/man$df.residual
 InvS = solve(S)
 mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))

 f <- function(a,b) mapply(function(a,b)
   mahalanobis(mds[a,],mds[b,],InvS,TRUE), a, b)
 seq. <- seq(length = nrow(mds))
 names(seq.) <- levels(x)
 D2 <- outer(seq., seq., f)
}

#
# test
#
D2M4 = D2Mah4(iris[,1:4], iris[,5])
print(D2M4)



On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> Indeed, it is very nice Gabor (as always)!
> 
> So, a doubt: how to preserve the 'rowname' and 'colname' of D2, like in the
> first function? I think it is useful to posterior analyzes (as cluster, for
> example).
> 
> Regards,
> 
> # A small correction (reference to gtools was eliminated)
> D2Mah2 = function(y, x) {
>   stopifnot(is.data.frame(y), !missing(x))
>   stopifnot(dim(y)[1] != dim(x)[1])
>   y    = as.matrix(y)
>   x    = as.factor(x)
>   man  = manova(y ~ x)
>   E    = summary(man)$SS[2] #Matrix E
>   S    = as.matrix(E$Residuals)/man$df.residual
>   InvS = solve(S)
>   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
>   nObjects = nrow(mds)
>   f = function(a,b) mapply(function(a,b)
>     (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a, b)
>   D2 = outer(seq(nObjects), seq(nObjects), f)
> }
> 
> #
> # test
> #
> D2M2 = D2Mah2(iris[,1:4], iris[,5])
> print(D2M2)
> 
> Gabor Grothendieck wrote:
> > I think you could simplify this by replacing everything after the
> > nObjects = nrow(mds) line with just these two statements.
> >
> >   f <- function(a,b) mapply(function(a,b)
> >     (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
> >
> >   D2 <- outer(seq(nObjects), seq(nObjects), f)
> >
> > This also eliminates dependence on gtools and the complexity
> > of dealing with triangular matrices.
> >
> > Regards.
> >
> > Here it is in full:
> >
> > D2Mah2 = function(y, x) {
> >
> >   stopifnot(is.data.frame(y), !missing(x))
> >   stopifnot(dim(y)[1] != dim(x)[1])
> >   y    = as.matrix(y)
> >   x    = as.factor(x)
> >   man  = manova(y ~ x)
> >   E    = summary(man)$SS[2] #Matrix E
> >   S    = as.matrix(E$Residuals)/man$df.residual
> >   InvS = solve(S)
> >   mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> >
> >   library(gtools)
> >   nObjects = nrow(mds)
> >
> >   ### changed part is next two statements
> >   f <- function(a,b) mapply(function(a,b)
> >     (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
> >
> >   D2 <- outer(seq(nObjects), seq(nObjects), f)
> > }
> >
> > #
> > # test
> > #
> > D2M2 = D2Mah2(iris[,1:4], iris[,5])
> > print(D2M2)
> >
> >
> >
> >
> > On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> >
> >>Well, as I did not get a satisfactory reply to the original question I tried to
> >>make a basic function that, I find, solve the question.
> >>
> >>I think it is not the better function, but it is working.
> >>
> >>So, perhaps it can be useful to other people.
> >>
> >>#
> >># Calculate the matrix of Mahalanobis Distances between groups
> >># from data.frames
> >>#
> >># by: Jos?? Cl??udio Faria
> >># date: 10/7/05 13:23:48
> >>#
> >>
> >>D2Mah = function(y, x) {
> >>
> >>  stopifnot(is.data.frame(y), !missing(x))
> >>  stopifnot(dim(y)[1] != dim(x)[1])
> >>  y    = as.matrix(y)
> >>  x    = as.factor(x)
> >>  man  = manova(y ~ x)
> >>  E    = summary(man)$SS[2] #Matrix E
> >>  S    = as.matrix(E$Residuals)/man$df.residual
> >>  InvS = solve(S)
> >>  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> >>
> >>  colnames(mds) = names(y)
> >>  Objects       = levels(x)
> >>  rownames(mds) = Objects
> >>
> >>  library(gtools)
> >>  nObjects = nrow(mds)
> >>  comb     = combinations(nObjects, 2)
> >>
> >>  tmpD2 = numeric()
> >>  for (i in 1:dim(comb)[1]){
> >>    a = comb[i,1]
> >>    b = comb[i,2]
> >>    tmpD2[i] = (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,])
> >>  }
> >>
> >>  # Thanks Gabor for the below
> >>  tmpMah = matrix(0, nObjects, nObjects, dimnames=list(Objects, Objects))
> >>  tmpMah[lower.tri(tmpMah)] = tmpD2
> >>  D2 = tmpMah + t(tmpMah)
> >>  return(D2)
> >>}
> >>
> >>#
> >># To try
> >>#
> >>D2M = D2Mah(iris[,1:4], iris[,5])
> >>print(D2M)
> >>
> >>Thanks all for the complementary aid (specially to Gabor).
> >>
> >>Regards,
> >>--
> >>Jose Claudio Faria
> >>Brasil/Bahia/UESC/DCET
> >>Estatistica Experimental/Prof. Adjunto
> >>mails:
> >> joseclaudio.faria at terra.com.br
> >> jc_faria at uesc.br
> >> jc_faria at uol.com.br
> >>tel: 73-3634.2779
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> >
> > Esta mensagem foi verificada pelo E-mail Protegido Terra.
> > Scan engine: McAfee VirusScan / Atualizado em 08/07/2005 / Vers??o: 4.4.00 - Dat 4531
> > Proteja o seu e-mail Terra: http://mail.terra.com.br/
> >
> >
> >
> 
> 
> --
> Jose Claudio Faria
> Brasil/Bahia/UESC/DCET
> Estatistica Experimental/Prof. Adjunto
> mails:
>  joseclaudio.faria at terra.com.br
>  jc_faria at uesc.br
>  jc_faria at uol.com.br
> tel: 73-3634.2779
>



From joseclaudio.faria at terra.com.br  Mon Jul 11 01:33:45 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sun, 10 Jul 2005 20:33:45 -0300
Subject: [R] Help with Mahalanobis
In-Reply-To: <971536df05071016267beee144@mail.gmail.com>
References: <42D14BA7.6000108@terra.com.br>	
	<971536df05071010111cf3140e@mail.gmail.com>	
	<42D1AB81.8040607@terra.com.br>
	<971536df05071016267beee144@mail.gmail.com>
Message-ID: <42D1B059.4070306@terra.com.br>

I think we now have a very good R function here.
Thanks for all Gabor!

JCFaria

Gabor Grothendieck wrote:
> This one adds the labels:
> 
> 
> D2Mah4 = function(y, x) {
> 
>  stopifnot(is.data.frame(y), !missing(x))
>  stopifnot(dim(y)[1] != dim(x)[1])
>  y    = as.matrix(y)
>  x    = as.factor(x)
>  man  = manova(y ~ x)
>  E    = summary(man)$SS[2] #Matrix E
>  S    = as.matrix(E$Residuals)/man$df.residual
>  InvS = solve(S)
>  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
> 
>  f <- function(a,b) mapply(function(a,b)
>    mahalanobis(mds[a,],mds[b,],InvS,TRUE), a, b)
>  seq. <- seq(length = nrow(mds))
>  names(seq.) <- levels(x)
>  D2 <- outer(seq., seq., f)
> }
> 
> #
> # test
> #
> D2M4 = D2Mah4(iris[,1:4], iris[,5])
> print(D2M4)
> 
> 
> 
> On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
> 
>>Indeed, it is very nice Gabor (as always)!
>>
>>So, a doubt: how to preserve the 'rowname' and 'colname' of D2, like in the
>>first function? I think it is useful to posterior analyzes (as cluster, for
>>example).
>>
>>Regards,
>>
>># A small correction (reference to gtools was eliminated)
>>D2Mah2 = function(y, x) {
>>  stopifnot(is.data.frame(y), !missing(x))
>>  stopifnot(dim(y)[1] != dim(x)[1])
>>  y    = as.matrix(y)
>>  x    = as.factor(x)
>>  man  = manova(y ~ x)
>>  E    = summary(man)$SS[2] #Matrix E
>>  S    = as.matrix(E$Residuals)/man$df.residual
>>  InvS = solve(S)
>>  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
>>  nObjects = nrow(mds)
>>  f = function(a,b) mapply(function(a,b)
>>    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a, b)
>>  D2 = outer(seq(nObjects), seq(nObjects), f)
>>}
>>
>>#
>># test
>>#
>>D2M2 = D2Mah2(iris[,1:4], iris[,5])
>>print(D2M2)
>>
>>Gabor Grothendieck wrote:
>>
>>>I think you could simplify this by replacing everything after the
>>>nObjects = nrow(mds) line with just these two statements.
>>>
>>>  f <- function(a,b) mapply(function(a,b)
>>>    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
>>>
>>>  D2 <- outer(seq(nObjects), seq(nObjects), f)
>>>
>>>This also eliminates dependence on gtools and the complexity
>>>of dealing with triangular matrices.
>>>
>>>Regards.
>>>
>>>Here it is in full:
>>>
>>>D2Mah2 = function(y, x) {
>>>
>>>  stopifnot(is.data.frame(y), !missing(x))
>>>  stopifnot(dim(y)[1] != dim(x)[1])
>>>  y    = as.matrix(y)
>>>  x    = as.factor(x)
>>>  man  = manova(y ~ x)
>>>  E    = summary(man)$SS[2] #Matrix E
>>>  S    = as.matrix(E$Residuals)/man$df.residual
>>>  InvS = solve(S)
>>>  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
>>>
>>>  library(gtools)
>>>  nObjects = nrow(mds)
>>>
>>>  ### changed part is next two statements
>>>  f <- function(a,b) mapply(function(a,b)
>>>    (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,]), a,b)
>>>
>>>  D2 <- outer(seq(nObjects), seq(nObjects), f)
>>>}
>>>
>>>#
>>># test
>>>#
>>>D2M2 = D2Mah2(iris[,1:4], iris[,5])
>>>print(D2M2)
>>>
>>>
>>>
>>>
>>>On 7/10/05, Jose Claudio Faria <joseclaudio.faria at terra.com.br> wrote:
>>>
>>>
>>>>Well, as I did not get a satisfactory reply to the original question I tried to
>>>>make a basic function that, I find, solve the question.
>>>>
>>>>I think it is not the better function, but it is working.
>>>>
>>>>So, perhaps it can be useful to other people.
>>>>
>>>>#
>>>># Calculate the matrix of Mahalanobis Distances between groups
>>>># from data.frames
>>>>#
>>>># by: Jos?? Cl??udio Faria
>>>># date: 10/7/05 13:23:48
>>>>#
>>>>
>>>>D2Mah = function(y, x) {
>>>>
>>>> stopifnot(is.data.frame(y), !missing(x))
>>>> stopifnot(dim(y)[1] != dim(x)[1])
>>>> y    = as.matrix(y)
>>>> x    = as.factor(x)
>>>> man  = manova(y ~ x)
>>>> E    = summary(man)$SS[2] #Matrix E
>>>> S    = as.matrix(E$Residuals)/man$df.residual
>>>> InvS = solve(S)
>>>> mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))
>>>>
>>>> colnames(mds) = names(y)
>>>> Objects       = levels(x)
>>>> rownames(mds) = Objects
>>>>
>>>> library(gtools)
>>>> nObjects = nrow(mds)
>>>> comb     = combinations(nObjects, 2)
>>>>
>>>> tmpD2 = numeric()
>>>> for (i in 1:dim(comb)[1]){
>>>>   a = comb[i,1]
>>>>   b = comb[i,2]
>>>>   tmpD2[i] = (mds[a,] - mds[b,])%*%InvS%*%(mds[a,] - mds[b,])
>>>> }
>>>>
>>>> # Thanks Gabor for the below
>>>> tmpMah = matrix(0, nObjects, nObjects, dimnames=list(Objects, Objects))
>>>> tmpMah[lower.tri(tmpMah)] = tmpD2
>>>> D2 = tmpMah + t(tmpMah)
>>>> return(D2)
>>>>}
>>>>
>>>>#
>>>># To try
>>>>#
>>>>D2M = D2Mah(iris[,1:4], iris[,5])
>>>>print(D2M)
>>>>
>>>>Thanks all for the complementary aid (specially to Gabor).
>>>>
>>>>Regards,
>>>>--
>>>>Jose Claudio Faria
>>>>Brasil/Bahia/UESC/DCET
>>>>Estatistica Experimental/Prof. Adjunto
>>>>mails:
>>>>joseclaudio.faria at terra.com.br
>>>>jc_faria at uesc.br
>>>>jc_faria at uol.com.br
>>>>tel: 73-3634.2779
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>Esta mensagem foi verificada pelo E-mail Protegido Terra.
>>>Scan engine: McAfee VirusScan / Atualizado em 08/07/2005 / Vers??o: 4.4.00 - Dat 4531
>>>Proteja o seu e-mail Terra: http://mail.terra.com.br/
>>>
>>>
>>>
>>
>>
>>--
>>Jose Claudio Faria
>>Brasil/Bahia/UESC/DCET
>>Estatistica Experimental/Prof. Adjunto
>>mails:
>> joseclaudio.faria at terra.com.br
>> jc_faria at uesc.br
>> jc_faria at uol.com.br
>>tel: 73-3634.2779



From spencer.graves at pdf.com  Mon Jul 11 01:55:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 10 Jul 2005 16:55:09 -0700
Subject: [R] O/T -2 Log Lambda and Chi Square
In-Reply-To: <16b13251050710152936a1bf77@mail.gmail.com>
References: <16b13251050710152936a1bf77@mail.gmail.com>
Message-ID: <42D1B55D.8000108@pdf.com>

	  There is a huge and growing literature on this, including 
Crainiceanu, Ruppert and Vogelsang (2003) "some properties of likelihood 
ratio tests in linear mixed models" 
(http://www.orie.cornell.edu/~davidr/papers/zeroprob_rev01.pdf).  The 
nlme package includes a function "simulate.lme" to evalute the adequacy 
of alternative distributions for 2*log(likelihood ratio) for the results 
of lme.

	  Much of the careful work on this rests on asymptotic normality of the 
maximum likelihood estimates, and this is the same for 2*log(likelihood 
ratio) as the standard quadratic form in the MLEs.  However, the latter 
is affected by parameter effects, whereas the likelihood ratio statistic 
is only impacted by the intrinsic curvature of the manifold upon which 
the log(likelihood) vector is projected to obtain the MLEs.  For 
nonlinear regression, Bates and Watts (1988) Nonlinear Regression and 
Its Applications (Wiley) computed measures of intrinsic and parameter 
effects curvature for a number of published nonlinear regression 
examples.  In nearly all their examples, the intrinsic curvature was in 
negligible, especially when compared to the parameter effects.

	  If this does not answer your question (or lead you to an answer), 
please try a more specific question.

	  spencer graves	

Laura Holt wrote:

> Hi R People:
> 
> Sorry about the off topic question.  Does anyone know the reference
> for "-2 Log Lambda  is approx dist. Chi square", please?
> 
> It may be Bartlett, but I'm not sure....
> 
> thanks in advance!
> 
> Sincerely,
> Laura Holt
> mailto: holtlaura at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From joseclaudio.faria at terra.com.br  Mon Jul 11 02:47:44 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Sun, 10 Jul 2005 21:47:44 -0300
Subject: [R] Help: Mahalanobis distances between 'Species' from iris
In-Reply-To: <4702645135092E4497088F71D9C8F51A128BC0@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128BC0@afhex01.dpi.wa.gov.au>
Message-ID: <42D1C1B0.8020408@terra.com.br>

Dear Mulholland,

I'm sorry, I was not clearly and objective sufficiently.
Below you can see what I'm was trying to do:

# D2Mah by JCFaria and Gabor Grothiendieck: 10/7/05 20:46:41
D2Mah = function(y, x) {

  stopifnot(is.data.frame(y), !missing(x))
  stopifnot(dim(y)[1] != dim(x)[1])
  y    = as.matrix(y)
  x    = as.factor(x)
  man  = manova(y ~ x)
  E    = summary(man)$SS[2] #Matrix E
  S    = as.matrix(E$Residuals)/man$df.residual
  InvS = solve(S)
  mds  = matrix(unlist(by(y, x, mean)), byrow=T, ncol=ncol(y))

  f = function(a,b) mapply(function(a,b)
    mahalanobis(mds[a,], mds[b,], InvS, TRUE), a, b)
  seq. = seq(length = nrow(mds))
  names(seq.) = levels(x)
  D2 = outer(seq., seq., f)
}

#
# test
#
D2M = D2Mah(iris[,1:4], iris[,5])
print(D2M)
dend = hclust(as.dist(sqrt(D2M)), method='complete')
plot(dend)

So, Thanks for the reply.
Best,

JCFaria

Mulholland, Tom wrote:
> Why don't you use mahalanobis in the stats package. The function "Returns the Mahalanobis distance of all rows in 'x' and the vector mu='center' with respect to Sigma='cov'. This is (for vector 'x') defined as
> 
>                  D^2 = (x - mu)' Sigma^{-1} (x - mu)
> 
> I don't have any idea what this does but it appears to be talking about the same topic. If it is not suitable package fpc has related functions and adehabitat has a function for "Habitat Suitability Mapping with Mahalanobis Distances"
> 
> Tom
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Jose 
>>Claudio Faria
>>Sent: Thursday, 7 July 2005 5:29 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Help: Mahalanobis distances between 'Species' from iris
>>
>>
>>Dear R list,
>>
>>I'm trying to calculate Mahalanobis distances for 'Species' 
>>of 'iris' data
>>as obtained below:
>>
>>Squared Distance to Species From Species:
>>
>>               Setosa Versicolor Virginica
>>Setosa 	           0   89.86419 179.38471
>>Versicolor  89.86419          0  17.20107
>>Virginica  179.38471   17.20107         0
>>
>>This distances above were obtained with proc 'CANDISC' of SAS, please,
>>see Output 21.1.2: Iris Data: Squared Mahalanobis Distances from
>>http://www.id.unizh.ch/software/unix/statmath/sas/sasdoc/stat/
>>chap21/sect19.htm
>>
>> From this distance my intention is to make a cluster 
>>analysis as below, using
>>the package 'mclust':
>>
>>#
>># --- Begin R script ---
>>#
>># For units compatibility of 'iris' from R dataset and 'iris' 
>>data used in
>># the SAS example:
>>Measures = iris[,1:4]*10
>>Species  = iris[,5]
>>irisSAS  = data.frame(Measures, Species)
>>
>>n   = 3
>>Mah = c(        0,
>>          89.86419,        0,
>>         179.38471, 17.20107, 0)
>>
>># My Question is: how to obtain 'Mah' with R from 'irisSAS' data?
>>
>>D = matrix(0, n, n)
>>
>>nam = c('Set', 'Ver', 'Vir')
>>rownames(D) = nam
>>colnames(D) = nam
>>
>>k = 0
>>for (i in 1:n) {
>>    for (j in 1:i) {
>>       k      = k+1
>>       D[i,j] = Mah[k]
>>       D[j,i] = Mah[k]
>>    }
>>}
>>
>>D=sqrt(D) #D2 -> D
>>
>>library(mclust)
>>dendroS = hclust(as.dist(D), method='single')
>>dendroC = hclust(as.dist(D), method='complete')
>>
>>win.graph(w = 3.5, h = 6)
>>split.screen(c(2, 1))
>>screen(1)
>>plot(dendroS, main='Single', sub='', xlab='', ylab='', col='blue')
>>
>>screen(2)
>>plot(dendroC, main='Complete', sub='', xlab='', col='red')
>>#
>># --- End R script ---
>>#
>>
>>I always need of this type of analysis and I'm not founding 
>>how to make it in 
>>the CRAN documentation (Archives, packages: mclust, cluster, 
>>fpc and mva).
>>
>>Regards,
>>-- 
>>Jose Claudio Faria
>>Brasil/Bahia/UESC/DCET
>>Estatistica Experimental/Prof. Adjunto
>>mails:
>>  joseclaudio.faria at terra.com.br
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>



From ramasamy at cancer.org.uk  Mon Jul 11 03:54:28 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 02:54:28 +0100
Subject: [R] Boxplot in R
In-Reply-To: <2320026105071016105b72dec4@mail.gmail.com>
References: <2320026105071016105b72dec4@mail.gmail.com>
Message-ID: <1121046868.24367.23.camel@dhcp-63.ccc.ox.ac.uk>

1) The boxplot in R does the 25%, 50% and 75% mark as you want
2) Check out the range argument in boxplot. I think you can redefine
your 5% and 95% quantile in terms of IQR for symmetric distribution and
hence use this feature.

However I find it easier to calculate these numbers manually and feed it
into bxp() function. Here is one such function (with lots of room for
improvement).


matlab.boxplot <- function(m){
  m  <- as.matrix(m)
  bp <- boxplot(data.frame(m), plot=FALSE)
  
  bp$stats <- apply( m, 2, function(x) 
                   quantile(x, c(0.05,0.25, 0.5, 0.75, 0.95), na.rm=T) )
  
  tmp <- apply( m, 2, function(x){
    under <- x[ which( x < quantile(x, 0.05, na.rm=T) ) ]
    over  <- x[ which( x > quantile(x, 0.95, na.rm=T) ) ]
    return( c(under, over) )
  })   # always a matrix in this case
  bp$out   <- c(tmp)
  
  bp$group <- rep(1:ncol(tmp), each=nrow(tmp))
  
  bxp(bp)
}


Some usage examples : 

 matlab.boxplot( rnorm(50) )    # a vector

 my.mat <- matrix( rnorm(300), nc=3 )
 matlab.boxplot( my.mat )       # a matrix


Regards, Adai




On Sun, 2005-07-10 at 18:10 -0500, Larry Xie wrote:
> I am trying to draw a plot like Matlab does: 
> 
> The upper extreme whisker represents 95% of the data;
> The upper hinge represents 75% of the data;
> The median represents 50% of the data;
> The lower hinge represents 25% of the data;
> The lower extreme whisker represents 5% of the data.
> 
> It looks like:
> 
>   ---         95%
>    |
>    |
> -------       75%
> |     |
> |-----|       50%
> |     |
> |     |
> -------       25%
>    |
>   ---         5%
> 
> Anyone can give me some hints as to how to draw a boxplot like that?
> What function does it? I tried boxplot() but couldn't figure it out.
> If it's boxplot(), what arguments should I pass to the function? Thank
> you for your help. I'd appreciate it.
> 
> Larry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Mon Jul 11 04:04:44 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 03:04:44 +0100
Subject: [R] Boxplot in R
In-Reply-To: <2320026105071016105b72dec4@mail.gmail.com>
References: <2320026105071016105b72dec4@mail.gmail.com>
Message-ID: <1121047485.24367.29.camel@dhcp-63.ccc.ox.ac.uk>

Just an addendum on the philosophical aspect of doing this.

By selecting the 5% and 95% quantiles, you are always going to get 10%
of the data as "extreme" and these points may not necessarily outliers.
So when you are comparing information from multiple columns (i.e.
boxplots), it is harder to say which column contains more extreme value
compared to others etc.

Regards, Adai



On Sun, 2005-07-10 at 18:10 -0500, Larry Xie wrote:
> I am trying to draw a plot like Matlab does: 
> 
> The upper extreme whisker represents 95% of the data;
> The upper hinge represents 75% of the data;
> The median represents 50% of the data;
> The lower hinge represents 25% of the data;
> The lower extreme whisker represents 5% of the data.
> 
> It looks like:
> 
>   ---         95%
>    |
>    |
> -------       75%
> |     |
> |-----|       50%
> |     |
> |     |
> -------       25%
>    |
>   ---         5%
> 
> Anyone can give me some hints as to how to draw a boxplot like that?
> What function does it? I tried boxplot() but couldn't figure it out.
> If it's boxplot(), what arguments should I pass to the function? Thank
> you for your help. I'd appreciate it.
> 
> Larry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Mon Jul 11 08:48:36 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 11 Jul 2005 08:48:36 +0200
Subject: [R] randomForest
In-Reply-To: <42CD8626.2070109@stats.uwo.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>
	<cdf8178305070712385c31140c@mail.gmail.com>
	<42CD8626.2070109@stats.uwo.ca>
Message-ID: <17106.5700.477778.953801@stat.math.ethz.ch>

>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Thu, 07 Jul 2005 15:44:38 -0400 writes:

    Duncan> On 7/7/2005 3:38 PM, Weiwei Shi wrote:
    >> Hi there:
    >> I have a question on random foresst:
    >> 
    >> recently i helped a friend with her random forest and i came with this problem:
    >> her dataset has 6 classes and since the sample size is pretty small:
    >> 264 and the class distr is like this (Diag is the response variable)

    >> sample.size <- lapply(1:6, function(i) sum(Diag==i))
    >>> sample.size
    >> [[1]]
    >> [1] 36

    ....

and later you get problems because you didn't know that a *list*
such as 'sample.size' should be made into a so called
*atomic vector* {and there's a function  is.atomic(.) ! to test for it}
and Duncan and others told you about unlist().

Now there are two things I'd want to add:

1) If you had used 

      s.size <- table(Diag)

   you had used a faster and simpler expression with the same result.
   Though in general (when there can be zero counts), to give the
   same result, you'd need

      s.size <- table(factor(Diag, levels = 1:6))

   Still a bit preferable to the lapply(.) IMO


2)  You should get into the habit of using
     sapply(.)   rather than  lapply(.).

    sapply() originally was exactly devised for the above task:
    and stands for ``[s]implified lapply''.

    It always returns an ``unlisted'' result when appropriate.

Regards,
Martin Maechler, ETH Zurich



From navarre_sabine at yahoo.fr  Mon Jul 11 10:56:31 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Mon, 11 Jul 2005 10:56:31 +0200 (CEST)
Subject: [R] choice of graph
Message-ID: <20050711085631.37269.qmail@web26601.mail.ukl.yahoo.com>

Hi,
 
It's about 2 weeks that I think about a graph to translate my datas. But I don't have an really idea.
I 'm going to expose you my problem:

I have a questionnaire with 15 questions and you have more possibilties to answer to these.
For example:
     The trainer is competent:   Yes   No
     I have learn a lot at the training:   Bad     Quit bad       Medium     Good      Quit good
.... etc

I would like to represent all my question on the same plot and differentiate the different type of answer possible.

On my first impression, I had doing a classification of the different type of answer and doing a barplot, but my responsable don't want a classification but want to see all the question with their own type of answer.

I have attached my first idea!

Can you please help me?


Thanks a lot
 
Sabine


		
---------------------------------


From Scott.Williams at petermac.org  Mon Jul 11 11:03:29 2005
From: Scott.Williams at petermac.org (Williams Scott)
Date: Mon, 11 Jul 2005 19:03:29 +1000
Subject: [R] validation, calibration and Design
Message-ID: <E4D2DC69ACDD79429E91AB7F2F1C48F2010454B3@email01.petermac.org.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050711/1c5d1d84/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Mon Jul 11 11:07:25 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 11 Jul 2005 11:07:25 +0200 (CEST)
Subject: [R] time series regression
In-Reply-To: <20050708212214.96960.qmail@web53102.mail.yahoo.com>
References: <20050708212214.96960.qmail@web53102.mail.yahoo.com>
Message-ID: <Pine.LNX.4.58.0507111103580.2227@thorin.ci.tuwien.ac.at>

On Fri, 8 Jul 2005, yyan liu wrote:

> Hi:
>   I have two time series y(t) and x(t). I want to
> regress Y on X. Because Y is a time series and may
> have autocorrelation such as AR(p), so it is not
> efficient to use OLS directly. The model I am trying
> to fit is like
> Y(t)=beta0+beta1*X(t)+rho*Y(t-1)+e(t)
>
> e(t) is iid normal random error. Anybody know whether
> there is a function in R can fit such models? The
> function can also let me specify how many beta's and
> rho's I can have in the model.

If you want to estimate the model by ML, you can use arima() and specify
further regressors via the `xreg' argument.
Estimation by OLS can be done via lm(), but that typically requires
setting up the lags yourself. More convenient interfaces are provided in
the `dyn' package by Gabor Grothendieck and my `dynlm' package.
Z

> Thx a lot!
>
> liu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Mon Jul 11 11:13:28 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 11 Jul 2005 11:13:28 +0200
Subject: [R] choice of graph
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAB48@xchg1.statistik.local>

> Hi,
>  
> It's about 2 weeks that I think about a graph to translate my 
> datas. But I don't have an really idea. I 'm going to expose 
> you my problem:
> 
> I have a questionnaire with 15 questions and you have more 
> possibilties to answer to these. For example:
>      The trainer is competent:   Yes   No
>      I have learn a lot at the training:   Bad     Quit bad   
>     Medium     Good      Quit good
> .... etc
> 
> I would like to represent all my question on the same plot 
> and differentiate the different type of answer possible.
> 
> On my first impression, I had doing a classification of the 
> different type of answer and doing a barplot, 

Did you try a mosaicplot too?
?mosaicplot

Best,
Matthias

> but my 
> responsable don't want a classification but want to see all 
> the question with their own type of answer.
> 
> I have attached my first idea!
> 
> Can you please help me?
> 
> 
> Thanks a lot
>  
> Sabine
> 
> 
> 		
> ---------------------------------
> 
>



From robin.smit at tno.nl  Mon Jul 11 11:30:30 2005
From: robin.smit at tno.nl (Smit, R. (Robin))
Date: Mon, 11 Jul 2005 11:30:30 +0200
Subject: [R] (no subject)
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F9ED@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050711/94776efd/attachment.pl

From Heinz.schild at caselab.info  Mon Jul 11 11:42:20 2005
From: Heinz.schild at caselab.info (Heinz Schild)
Date: Mon, 11 Jul 2005 11:42:20 +0200
Subject: [R] Problems with R on OS X
Message-ID: <F14115FF-D023-4C0F-99B5-935FAF7BF57E@caselab.info>

I used R on OS X 10.3x quite some time with no serious problems.  
Sometimes R stopped when I tried to execute a bigger program. After  
updating to OS X to version 10.4 R worked but I still had the problem  
with bigger programs. Therefore I re-installed R on top of the  
existing R version. The installation finished properly but suddenly R  
did not work. Then I reinstalled OS X 10.4 because I thought some  
left over registry information from R may caused the problem. R still  
crashed. I hoped the problem would be overcome with the new R version  
1.12. Unfortunately this is not the case. The error report (see  
appendix) says that Thread 0 crashed.
What can I do?
Heinz Schild

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Errror Report.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050711/cde75492/ErrrorReport.txt
-------------- next part --------------


From constant.depiereux at aqte.be  Mon Jul 11 11:54:08 2005
From: constant.depiereux at aqte.be (Constant =?iso-8859-1?q?Depi=E8reux?=)
Date: Mon, 11 Jul 2005 11:54:08 +0200
Subject: [R] R on kubuntu
Message-ID: <200507111154.08476.constant.depiereux@aqte.be>

Hello all,

I am planning to redeploy my workstation under KUBUNTU.

Does any body has any r experience installing/using r on this platform?

Best regards.


-- 
Constant Depi??reux
Managing Director
Applied QUality Technologies Europe sprl
Rue des D??port??s 123, B-4800 Verviers
(Tel) +32 87 292175 - (Fax) +32 87 292171 - (Mobile) +32 475 555 818
(Web) http://www.aqte.be - (Courriel) constant.depiereux at aqte.be
(Skype) cdepiereux



From p.dalgaard at biostat.ku.dk  Mon Jul 11 12:03:15 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jul 2005 12:03:15 +0200
Subject: [R] R on kubuntu
In-Reply-To: <200507111154.08476.constant.depiereux@aqte.be>
References: <200507111154.08476.constant.depiereux@aqte.be>
Message-ID: <x2y88d7j8c.fsf@turmalin.kubism.ku.dk>

Constant Depi??reux <constant.depiereux at aqte.be> writes:

> Hello all,
> 
> I am planning to redeploy my workstation under KUBUNTU.
> 
> Does any body has any r experience installing/using r on this platform?

I don't think it's any different than any other Debian-derived
platform. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ernesto at ipimar.pt  Mon Jul 11 12:09:48 2005
From: ernesto at ipimar.pt (ernesto)
Date: Mon, 11 Jul 2005 11:09:48 +0100
Subject: [R] R on kubuntu
In-Reply-To: <200507111154.08476.constant.depiereux@aqte.be>
References: <200507111154.08476.constant.depiereux@aqte.be>
Message-ID: <42D2456C.4060007@ipimar.pt>

Constant Depi??reux wrote:

>Hello all,
>
>I am planning to redeploy my workstation under KUBUNTU.
>
>Does any body has any r experience installing/using r on this platform?
>
>Best regards.
>
>
>  
>

Hi,

I've just moved from SuSE 9.1 to Ubuntu 5.04. The instalation is clean
and easy, although you have to waste some time selecting packages.

After I got all the packages installed I just compiled R like usual and
it is working very well.

Regards

EJ

ps: I have to tell you that Ubuntu was a very good surprise for me.



From danbebber at yahoo.co.uk  Mon Jul 11 12:10:57 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Mon, 11 Jul 2005 11:10:57 +0100 (BST)
Subject: [R] plot(cox.zph()): customize xlab & ylab
Message-ID: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>

Hello,

plot(cox.zph(my.ph),var=1,xlab="Year")

gives the error:
Error in plot.default(range(xx), yr, type = "n", xlab
= "Time", ylab = ylab[i],  : formal argument "xlab"
matched by multiple actual arguments

How can I customize the xlab and ylab for plots of
cox.zph?

Thanks,
Dan Bebber

Department of Plant Sciences
University of Oxford
UK


		
___________________________________________________________ 
How much free photo storage do you get? Store your holiday



From sekemp at glam.ac.uk  Mon Jul 11 12:17:57 2005
From: sekemp at glam.ac.uk (Kemp S E (Comp))
Date: Mon, 11 Jul 2005 11:17:57 +0100
Subject: [R] estVARXar parameter significance
Message-ID: <0BA7EE4D4646E0409D458D347C508B783C635F@MAILSERV1.uni.glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050711/12a84330/attachment.pl

From ramasamy at cancer.org.uk  Mon Jul 11 12:19:16 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 11:19:16 +0100
Subject: [R] calculating dispersion formula using deviance ( was Re:
	(no	subject) )
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F9ED@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F9ED@MS-DT01VS01.tsn.tno.nl>
Message-ID: <1121077156.5944.8.camel@ipc143004.lif.icnet.uk>

Please try to use a meaningful subject line. See below for comments.



On Mon, 2005-07-11 at 11:30 +0200, Smit, R. (Robin) wrote:
> Hello,
>  
> The estimate of glm dispersion can be based on the deviance or on the
> Pearson statistic.
> I have compared output from R glm() to another statastical package and
> it appears that R uses the Pearson statistic.

A quick search would also highlight the following thread
http://www.r-project.org/nocvs/mail/r-help/2002/6938.html

> I was wondering if it is possible to make use R the deviance instead by
> modifying the glm(...) function?

I don't know what the formula for using the deviance is but _IF_ it is
the square root of ratio of null deviance by its degrees of freedom,
then sqrt( fit$deviance / fit$df.null ) should be useful.


> Thanks for your attention.
>  
> Kind regards,
> Robin Smit
> 
>  
> 
> This e-mail and its contents are subject to the DISCLAIMER at http://www.tno.nl/disclaimer/email.html
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Mon Jul 11 13:06:43 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 12:06:43 +0100
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>
References: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>
Message-ID: <1121080003.5944.12.camel@ipc143004.lif.icnet.uk>

I am not sure if there is an easy way around this. An ugly hack is to
make a copy the function "survival:::plot.cox.zph" and make your
modified function. But there are others in the list who might know
neater solutions.

Regards, Adai


On Mon, 2005-07-11 at 11:10 +0100, Dan Bebber wrote:
> Hello,
> 
> plot(cox.zph(my.ph),var=1,xlab="Year")
> 
> gives the error:
> Error in plot.default(range(xx), yr, type = "n", xlab
> = "Time", ylab = ylab[i],  : formal argument "xlab"
> matched by multiple actual arguments
> 
> How can I customize the xlab and ylab for plots of
> cox.zph?
> 
> Thanks,
> Dan Bebber
> 
> Department of Plant Sciences
> University of Oxford
> UK
> 
> 
> 		
> ___________________________________________________________ 
> How much free photo storage do you get? Store your holiday
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Tristan.Lefebure at univ-lyon1.fr  Mon Jul 11 13:20:10 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Mon, 11 Jul 2005 13:20:10 +0200
Subject: [R] R on kubuntu
In-Reply-To: <200507111154.08476.constant.depiereux@aqte.be>
References: <200507111154.08476.constant.depiereux@aqte.be>
Message-ID: <200507111320.10793.lefebure@univ-lyon1.fr>

No problem at all.
If you allow "universe" packages, many binary R packages are available (from 
the GNU/Linux Debian sid). 


On Monday 11 July 2005 11:54, Constant Depi??reux wrote:
> Hello all,
>
> I am planning to redeploy my workstation under KUBUNTU.
>
> Does any body has any r experience installing/using r on this platform?
>
> Best regards.

-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'??cologie des hydrosyst??mes fluviaux (UMR 5023)
Universit?? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From r.hankin at noc.soton.ac.uk  Mon Jul 11 13:25:49 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 11 Jul 2005 12:25:49 +0100
Subject: [R] Sweave and complex numbers
Message-ID: <8C9C8E75-26DB-47AB-A40B-51BAECC7F55C@soc.soton.ac.uk>

Hi

When using Sweave, most of my functions get called with complex  
arguments.

They get typeset in with additions that I don't want; "1+1i" appears  
as "1 + (0 + 1i)"
and I would rather have plain old "1+1i".

Example follows:




\documentclass[a4paper]{article}

\title{A Test File}
\author{Robin Hankin}

\usepackage{a4wide}

\begin{document}

\maketitle

A simple example:
<<print=TRUE>>=
f <- function(x){
x+c(1,1+1i)}
f(6+7i)
@


Question: why does \verb=c(1,1+1i)= get printed as
\verb=c(1,1 + (0+1i))= ?

And how can I stop it?

\end{document}




can anyone help me here?







--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From iwhite at staffmail.ed.ac.uk  Mon Jul 11 13:37:32 2005
From: iwhite at staffmail.ed.ac.uk (I M S White)
Date: Mon, 11 Jul 2005 12:37:32 +0100 (BST)
Subject: [R]  polr (MASS) link functions
Message-ID: <Pine.GSO.4.58.0507111235550.25972@holyrood.ed.ac.uk>

When we analyse ordered categorical data, with categories I<II<III,
should we expect an equivalent analysis if the ordering is changed to
I>II>III ?  Not with a cloglog link (see example below). However, I
suspect the cloglog analysis on the reordered categories is equivalent
to a loglog analysis on the original ordering.  Perhaps for consistency
polr should include a loglog link? Or warn that a different answer is
available, possibly more appropriate?

Cheese tasting example from McCullagh and Nelder:

> library(MASS)
> frqs <- c(0,0,1,7,8,8,19,8,1,
+ 6,9,12,11,7,6,1,0,0,
+ 1,1,6,8,23,7,5,1,0,
+ 0,0,0,1,3,7,14,16,11)
> Y1 <- ordered(rep(1:9,4))
> Y2 <-  ordered(rep(9:1,4))
> trt <- factor(rep(c("A","B","C","D"),each=9))
> fit1 <- polr(Y1 ~ trt, method="clog",weight=frqs)
> fit2 <- polr(Y2 ~ trt, method="clog",weight=frqs)
> print(fit1)
Call:
polr(formula = Y1 ~ trt, weights = frqs, method = "clog")

Coefficients:
     trtB      trtC      trtD
-1.809487 -0.879997  0.987389

Intercepts:
        1|2         2|3         3|4         4|5         5|6         6|7
-2.50668784 -2.04055560 -1.42592187 -0.80166219 -0.01722569  0.55135471
        7|8         8|9
 1.55688848  2.84302173

Residual Deviance: 725.2341
AIC: 747.2341

> print(fit2)
Call:
polr(formula = Y2 ~ trt, weights = frqs, method = "clog")

Coefficients:
      trtB       trtC       trtD
 1.9513455  1.1033902 -0.9588354

Intercepts:
       1|2        2|3        3|4        4|5        5|6        6|7        7|8
-1.3832854 -0.5216350  0.4225419  0.9994969  1.8317538  2.5777980  3.4532818
       8|9
 4.4026602

Residual Deviance: 718.0052
AIC: 740.0052


======================================
I.White
University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Tel: 0131 650 5490  Fax: 0131 650 6564
E-mail: iwhite at staffmail.ed.ac.uk



From murdoch at stats.uwo.ca  Mon Jul 11 13:50:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Jul 2005 07:50:11 -0400
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <1121080003.5944.12.camel@ipc143004.lif.icnet.uk>
References: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>
	<1121080003.5944.12.camel@ipc143004.lif.icnet.uk>
Message-ID: <42D25CF3.2090707@stats.uwo.ca>

Adaikalavan Ramasamy wrote:
> I am not sure if there is an easy way around this. An ugly hack is to
> make a copy the function "survival:::plot.cox.zph" and make your
> modified function. But there are others in the list who might know
> neater solutions.

This hack is uglier (and might not work properly on some devices, but 
you could try):

plot(cox.zph(my.ph),var=1,col.lab="white")
title(xlab="Year", ylab="Beta for blah blah blah")

Duncan Murdoch
> 
> Regards, Adai
> 
> 
> On Mon, 2005-07-11 at 11:10 +0100, Dan Bebber wrote:
> 
>>Hello,
>>
>>plot(cox.zph(my.ph),var=1,xlab="Year")
>>
>>gives the error:
>>Error in plot.default(range(xx), yr, type = "n", xlab
>>= "Time", ylab = ylab[i],  : formal argument "xlab"
>>matched by multiple actual arguments
>>
>>How can I customize the xlab and ylab for plots of
>>cox.zph?
>>
>>Thanks,
>>Dan Bebber
>>
>>Department of Plant Sciences
>>University of Oxford
>>UK
>>
>>
>>		
>>___________________________________________________________ 
>>How much free photo storage do you get? Store your holiday
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Mon Jul 11 14:02:27 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Jul 2005 08:02:27 -0400
Subject: [R] Sweave and complex numbers
In-Reply-To: <8C9C8E75-26DB-47AB-A40B-51BAECC7F55C@soc.soton.ac.uk>
References: <8C9C8E75-26DB-47AB-A40B-51BAECC7F55C@soc.soton.ac.uk>
Message-ID: <42D25FD3.3040603@stats.uwo.ca>

Robin Hankin wrote:
> Hi
> 
> When using Sweave, most of my functions get called with complex  
> arguments.
> 
> They get typeset in with additions that I don't want; "1+1i" appears  
> as "1 + (0 + 1i)"
> and I would rather have plain old "1+1i".
> 
> Example follows:
> 
> 
> 
> 
> \documentclass[a4paper]{article}
> 
> \title{A Test File}
> \author{Robin Hankin}
> 
> \usepackage{a4wide}
> 
> \begin{document}
> 
> \maketitle
> 
> A simple example:
> <<print=TRUE>>=
> f <- function(x){
> x+c(1,1+1i)}
> f(6+7i)
> @
> 
> 
> Question: why does \verb=c(1,1+1i)= get printed as
> \verb=c(1,1 + (0+1i))= ?
> 
> And how can I stop it?
> 
> \end{document}
> 
> 
> 
> 
> can anyone help me here?

The R parser only understands pure imaginary constants as complex 
numbers.  It parses 1+1i as the sum of the real constant 1 and the 
complex constant 0+1i.

This isn't easy to work around.  Sweave could special case these, or the 
parser or deparser could, but it looks messy to me.  I'd guess the best 
solution would be if it happened in the parser (i.e. a real constant 
plus an imaginary constant was folded into a complex constant), but 
right now our parser doesn't do any sorts of optimizations like this. 
It's not easy to add the first one, not least because the parser doesn't 
know for sure that 1+1i really is a complex number:  you might have 
redefined the meaning of "+".

Maybe someone else has an answer to your 2nd question.

Duncan Murdoch



From ajayshah at mayin.org  Mon Jul 11 14:21:13 2005
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 11 Jul 2005 17:51:13 +0530 (IST)
Subject: [R] Misbehaviour of DSE
Message-ID: <20050711122113.EAF9415D3C5@lubyanka.local>


Folks,

I am finding problems with using "dse":

> library(dse1)
Loading required package: tframe
Error: c("package '%s' required by '%s' could not be found", "setRNG", "dse1")
> library(dse2)
Loading required package: setRNG
Error: package 'setRNG' could not be loaded
In addition: Warning message:
there is no package called 'setRNG' in: library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc) 

This is on R 2.1 on an Apple ibook (OS X) "panther".

Thanks,

	-ans.



From maechler at stat.math.ethz.ch  Mon Jul 11 14:36:35 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 11 Jul 2005 14:36:35 +0200
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <1121047485.24367.29.camel@dhcp-63.ccc.ox.ac.uk>
References: <2320026105071016105b72dec4@mail.gmail.com>
	<1121047485.24367.29.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <17106.26579.427794.767713@stat.math.ethz.ch>

>>>>> "AdaiR" == Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>>>>>     on Mon, 11 Jul 2005 03:04:44 +0100 writes:

    AdaiR> Just an addendum on the philosophical aspect of doing
    AdaiR> this.  By selecting the 5% and 95% quantiles, you are
    AdaiR> always going to get 10% of the data as "extreme" and
    AdaiR> these points may not necessarily outliers.  So when
    AdaiR> you are comparing information from multiple columns
    AdaiR> (i.e.  boxplots), it is harder to say which column
    AdaiR> contains more extreme value compared to others etc.

Yes, indeed!

People {and software implementations} have several times provided
differing definitions of how the boxplot whiskers should be defined.

I strongly believe that this is very often a very bad idea!!

A boxplot should be a universal mean communication and so one
should be *VERY* reluctant redefining the outliers.

I just find that Matlab (in their statistics toolbox)
does *NOT* use such a silly 5% / 95% definition of the whiskers,
at least not according to their documentation.
That's very good (and I wonder where you, Larry, got the idea of
the 5 / 95 %).
Using such a fixed percentage is really a very inferior idea to
John Tukey's definition {the one in use in all implementations
of S (including R) probably for close to 20 years now}.

I see one flaw in Tukey's definition {which is shared of course
by any silly "percentage" based ``outlier'' definition}:

   The non-dependency on the sample size.

If you have a 1000 (or even many more) points,
you'll get more and more `outliers' even for perfectly normal data.

But then, I assume John Tukey would have told us to do more
sophisticated things {maybe things like the "violin plots"} than
boxplot  if you have really very many data points, you may want
to see more features -- or he would have agreed to use 
   boxplot(*,  range = monotone_slowly_growing(n) )
for largish sample sizes n.

Martin Maechler, ETH Zurich




    AdaiR> Regards, Adai

    AdaiR> On Sun, 2005-07-10 at 18:10 -0500, Larry Xie wrote:
    >> I am trying to draw a plot like Matlab does: 
    >> 
    >> The upper extreme whisker represents 95% of the data;
    >> The upper hinge represents 75% of the data;
    >> The median represents 50% of the data;
    >> The lower hinge represents 25% of the data;
    >> The lower extreme whisker represents 5% of the data.
    >> 
    >> It looks like:
    >> 
    >> ---         95%
    >> |
    >> |
    >> -------       75%
    >> |     |
    >> |-----|       50%
    >> |     |
    >> |     |
    >> -------       25%
    >> |
    >> ---         5%
    >> 
    >> Anyone can give me some hints as to how to draw a boxplot like that?
    >> What function does it? I tried boxplot() but couldn't figure it out.
    >> If it's boxplot(), what arguments should I pass to the function? Thank
    >> you for your help. I'd appreciate it.



From stefan.albrecht at allianz.com  Mon Jul 11 14:56:46 2005
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Mon, 11 Jul 2005 14:56:46 +0200
Subject: [R] class<- vs. as()
Message-ID: <OFB1408645.997A7404-ONC125703B.0046463C-C125703B.00471EB0@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050711/68b16f11/attachment.pl

From f.harrell at vanderbilt.edu  Mon Jul 11 15:02:40 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 11 Jul 2005 09:02:40 -0400
Subject: [R] validation, calibration and Design
In-Reply-To: <E4D2DC69ACDD79429E91AB7F2F1C48F2010454B3@email01.petermac.org.au>
References: <E4D2DC69ACDD79429E91AB7F2F1C48F2010454B3@email01.petermac.org.au>
Message-ID: <42D26DF0.2020903@vanderbilt.edu>

Williams Scott wrote:
>  
> 
> Hi R experts,
> 
>  
> 
> I am trying to do a prognostic model validation study, using cancer
> survival data. There are 2 data sets - 1500 cases used to develop a
> nomogram, and another of 800 cases used as an independent validation
> cohort.  I have validated the nomogram in the original data (easy with
> the Design tools), and then want to show that it also has good results
> with the independent data using 60 month survival. I would also like to
> show that the nomogram is significantly different to an existing model
> based on 60 month survival data generated by it (eg by McNemar's test).

Scott,

A nomogram is a graphical device, not a model to validate.  It merely 
represents a model.

If the 800 subjects came from the same hospitals in roughly the same 
time era, you are doing an internal validation and this is an 
exceedingly inefficient way to do it.  Not only is this wasting 800 
subjects from developing the model, but the validation sample is not 
large enough to yield reliable accuracy estimates.  And I don't see how 
McNemar's test applies as nothing is binary about this problem.

If you have two models that have identical degrees of overfitting (e.g. 
were based on the same number of CANDIDATE degrees of freedom) you can 
use the rcorrp.cens function to test for differences in discrimination 
or paired predictions.

If you really have an external sample (say 800 subjects from another 
country) you can use the groupkm function in Design to get a 
Kaplan-Meier-based calibration curve.  Otherwise I would recombine the 
data, develop the model on all subjects you can get, and use the 
bootstrap to validate it.

Frank

> 
> Hence, somewhat shortened:           
> 
>  
> 
> #using R 2.01 on Windows
> 
> library(Hmisc)
> 
> library(Design)
> 
>  
> 
> data1 #dataframe with predictor variables A and B, cens and time 
> 
>       columns (months)
> 
> ddist1 <- datadist(data1) 
> 
> options(datadist='ddist1') 
> 
>  
> 
> s1 <- Surv(data1$time, data1$cens)
> 
>  
> 
> cph.nomo <- cph(s1 ~ A+B, surv=T, x=T, y=T, time.inc=60)
> 
>  
> 
> survcph <- Survival(cph.nomo, x=T, y=T, time.inc=60, surv=T)
> 
> surv5 <- function(lp) survcph(60, lp)
> 
> nomogram(cph.nomo, lp=T, conf.int=F, fun=list(surv5, surv7), 
> 
> funlabel=c("5 yr DFS"))
> 
>  
> 
> # now have a useful nomogram model, with good discrimination and
> 
> #calibration when checked with validate and calibrate (not shown)
> 
> #....move on to validation cohort of n=800
> 
>  
> 
> Data2 #Validation data with same predictor variables A, B, cens, time
> 
> # do I need to put data2 into datadist??
> 
>  
> 
> s2 <- Surv(data2$time, data2$cens)
> 
>  
> 
> #able to derive 60 month estimates of survival using
> 
> data2.est5 <- survest(cph.nomo, expand.grid(A=data2$A, B=data2$B), 
> 
> times=c(60), conf.int=0)
> 
>  
> 
> rcorr.cens(data2.est5$surv, s2) # tests discrimination of the model 
> 
> #against the validation data observed censored data
> 
>  
> 
> # I cant find a way to use calibrate in this setting though??
> 
> # Also, if I have the 5 year estimates for 2 different models, I can 
> 
> #     use rcorr.cens to show discrimination, but which values are 
> 
> #     suitable for a test of difference (eg with McNemars)?
> 
> # I have tried predict / newdata function a number of ways but it 
> 
> #     typically returns an error relating to unequal vector lengths
> 
>  
> 
>  
> 
> What I cant work out is where to go now to derive a calibration curve of
> the predicted 5 year result (val.data5) and the observed  (s2). Or can I
> do it another way? For example, could I merge the 2 data frames and use
> lines1:1500 to build the model and the last 800 lines to validate?
> 
>  
> 
> Obviously I am a novice, and sure to be missing something simple. I have
> spent countless hours pouring over Prof Harrell's text (which is great
> but doesn't have a specific example of this) and Design Help plus the R
> news archive with no success, so any help is very much appreciated. 
> 
>  
> 
> Scott Williams MD
> 
> Peter MacCallum Cancer Centre
> 
> Melbourne Australia
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From lauraholt_983 at hotmail.com  Sun Jul 10 23:36:32 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Sun, 10 Jul 2005 16:36:32 -0500
Subject: [R]  Off topic -2 Ln Lambda and Chi square
Message-ID: <BAY105-F36AD4E66B47D00F0DFD5A0D6DD0@phx.gbl>

Dear R :

Sorry for the off topic question, but does anyone know the reference for
the -2 Ln Lambda following a Chi Square distribution, please?

Possibly one of Bartlett's?

Thanks in advance!

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From p.dalgaard at biostat.ku.dk  Mon Jul 11 15:04:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jul 2005 15:04:52 +0200
Subject: [R] Sweave and complex numbers
In-Reply-To: <42D25FD3.3040603@stats.uwo.ca>
References: <8C9C8E75-26DB-47AB-A40B-51BAECC7F55C@soc.soton.ac.uk>
	<42D25FD3.3040603@stats.uwo.ca>
Message-ID: <x2u0j17atn.fsf@turmalin.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:


> > Question: why does \verb=c(1,1+1i)= get printed as
> > \verb=c(1,1 + (0+1i))= ?
> 
> The R parser only understands pure imaginary constants as complex 
> numbers.  It parses 1+1i as the sum of the real constant 1 and the 
> complex constant 0+1i.
> 
> This isn't easy to work around.  Sweave could special case these, or the 
> parser or deparser could, but it looks messy to me.  I'd guess the best 
> solution would be if it happened in the parser (i.e. a real constant 
> plus an imaginary constant was folded into a complex constant), but 
> right now our parser doesn't do any sorts of optimizations like this. 
> It's not easy to add the first one, not least because the parser doesn't 
> know for sure that 1+1i really is a complex number:  you might have 
> redefined the meaning of "+".

Hmmmm... I'd say that the user would deserve what she gets in that
case. I suspect that the real issue is that R's tokenizer is
hand-written and not smart enough to recognize complex constants.
Figuring out whether there is a good reason for that or whether we
might actually use an automatic tokenizer like flex is somewhere on
the far end of my want-to-do list...

One thing that could be done quite immediately is to let the deparser
forget about a zero real part and drop the parentheses in that case.
After all "1i" is perfectly legal R, and

> identical(1i,0+1i)
[1] TRUE

(Those parse/deparse asymmetries are maddening. *Next* time we
redesign R we should get this straightened out....)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ramasamy at cancer.org.uk  Mon Jul 11 15:10:28 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 14:10:28 +0100
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <42D25CF3.2090707@stats.uwo.ca>
References: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>
	<1121080003.5944.12.camel@ipc143004.lif.icnet.uk>
	<42D25CF3.2090707@stats.uwo.ca>
Message-ID: <1121087428.5944.27.camel@ipc143004.lif.icnet.uk>

Duncan, your solution could be simplified using ann=FALSE in the plot

 fit <- coxph( Surv(futime, fustat) ~ age + rx, ovarian)
 plot( cox.zph(fit), ann=F )
 title( xlab="My own label", ylab="A new label", main="A clever title")

Now, why did I not think of this before ?

Regards, Adai



On Mon, 2005-07-11 at 07:50 -0400, Duncan Murdoch wrote:
> Adaikalavan Ramasamy wrote:
> > I am not sure if there is an easy way around this. An ugly hack is to
> > make a copy the function "survival:::plot.cox.zph" and make your
> > modified function. But there are others in the list who might know
> > neater solutions.
> 
> This hack is uglier (and might not work properly on some devices, but 
> you could try):
> 
> plot(cox.zph(my.ph),var=1,col.lab="white")
> title(xlab="Year", ylab="Beta for blah blah blah")
> 
> Duncan Murdoch
> > 
> > Regards, Adai
> > 
> > 
> > On Mon, 2005-07-11 at 11:10 +0100, Dan Bebber wrote:
> > 
> >>Hello,
> >>
> >>plot(cox.zph(my.ph),var=1,xlab="Year")
> >>
> >>gives the error:
> >>Error in plot.default(range(xx), yr, type = "n", xlab
> >>= "Time", ylab = ylab[i],  : formal argument "xlab"
> >>matched by multiple actual arguments
> >>
> >>How can I customize the xlab and ylab for plots of
> >>cox.zph?
> >>
> >>Thanks,
> >>Dan Bebber
> >>
> >>Department of Plant Sciences
> >>University of Oxford
> >>UK
> >>
> >>
> >>		
> >>___________________________________________________________ 
> >>How much free photo storage do you get? Store your holiday
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From murdoch at stats.uwo.ca  Mon Jul 11 15:20:31 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Jul 2005 09:20:31 -0400
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <1121087428.5944.27.camel@ipc143004.lif.icnet.uk>
References: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>	
	<1121080003.5944.12.camel@ipc143004.lif.icnet.uk>	
	<42D25CF3.2090707@stats.uwo.ca>
	<1121087428.5944.27.camel@ipc143004.lif.icnet.uk>
Message-ID: <42D2721F.50800@stats.uwo.ca>

On 7/11/2005 9:10 AM, Adaikalavan Ramasamy wrote:
> Duncan, your solution could be simplified using ann=FALSE in the plot
> 
>  fit <- coxph( Surv(futime, fustat) ~ age + rx, ovarian)
>  plot( cox.zph(fit), ann=F )
>  title( xlab="My own label", ylab="A new label", main="A clever title")
> 
> Now, why did I not think of this before ?

That's nearly perfect.  I think the only problem is that plot.cox.zph 
can do multiple frames, and this will only allow annotation on the last 
one.  But if you put it in a loop and do them one at a time, this should 
be fine.

Duncan Murdoch

> 
> Regards, Adai
> 
> 
> 
> On Mon, 2005-07-11 at 07:50 -0400, Duncan Murdoch wrote:
>> Adaikalavan Ramasamy wrote:
>> > I am not sure if there is an easy way around this. An ugly hack is to
>> > make a copy the function "survival:::plot.cox.zph" and make your
>> > modified function. But there are others in the list who might know
>> > neater solutions.
>> 
>> This hack is uglier (and might not work properly on some devices, but 
>> you could try):
>> 
>> plot(cox.zph(my.ph),var=1,col.lab="white")
>> title(xlab="Year", ylab="Beta for blah blah blah")
>> 
>> Duncan Murdoch
>> > 
>> > Regards, Adai
>> > 
>> > 
>> > On Mon, 2005-07-11 at 11:10 +0100, Dan Bebber wrote:
>> > 
>> >>Hello,
>> >>
>> >>plot(cox.zph(my.ph),var=1,xlab="Year")
>> >>
>> >>gives the error:
>> >>Error in plot.default(range(xx), yr, type = "n", xlab
>> >>= "Time", ylab = ylab[i],  : formal argument "xlab"
>> >>matched by multiple actual arguments
>> >>
>> >>How can I customize the xlab and ylab for plots of
>> >>cox.zph?
>> >>
>> >>Thanks,
>> >>Dan Bebber
>> >>
>> >>Department of Plant Sciences
>> >>University of Oxford
>> >>UK
>> >>
>> >>
>> >>		
>> >>___________________________________________________________ 
>> >>How much free photo storage do you get? Store your holiday
>> >>
>> >>______________________________________________
>> >>R-help at stat.math.ethz.ch mailing list
>> >>https://stat.ethz.ch/mailman/listinfo/r-help
>> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> >>
>> > 
>> > 
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>>



From Mark.Edmondson-jones at nottingham.ac.uk  Mon Jul 11 15:57:27 2005
From: Mark.Edmondson-jones at nottingham.ac.uk (Mark Edmondson-Jones)
Date: Mon, 11 Jul 2005 14:57:27 +0100
Subject: [R] misc3d package
Message-ID: <s2d288ed.025@ccw0m1.nottingham.ac.uk>

Hi,

I am trying to install the misc3d package on a Windows (XP) installation of R 2.0.1 using install.packages("misc3d") but with no success.  I have used this approach with other packages OK, but for misc3d I get the following output...

trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 27996 bytes
opened URL
downloaded 27Kb

Warning message: 
No package "misc3d" on CRAN. in: download.packages(pkgs, destdir = tmpd, available = available,  

Any help would be much appreciated.

Regards,
Mark


This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From costas at mit.edu  Mon Jul 11 16:06:41 2005
From: costas at mit.edu (Constantinos Antoniou)
Date: Mon, 11 Jul 2005 17:06:41 +0300
Subject: [R] Problems with corARMA
In-Reply-To: <42A92876.6050508@cyllene.uwa.edu.au>
References: <42A92876.6050508@cyllene.uwa.edu.au>
Message-ID: <5BC9FD70-CAAA-46B7-999E-2477FE389F84@mit.edu>

Dear All,

I just came across the same error message (running a gnls{nlme}  
model). [R-2.1.1 on Mac OS 10.4]

For a reproducible example, please download the file:

http://mit.edu/costas/www/GR.txt

and run the following:


GR <- read.table("GR.txt",header=T)
attach(GR)
myyear <- year-1969

library(nlme)
gnls4a <- gnls(fatalities/vehicles1000~a0*(vehicles1000/ 
population1000)^a1,start=c 
(a0=1,a1=1),data=GR,na.action=na.omit,correlation=corARMA(form=~myyear))


Thanks a lot,

Costas



On 10  2005, at 8:43 , Pamela McCaskie wrote:

> Dear all
> I am tryiing to fit the following lme with an ARMA correlation  
> structure:
>
> test <- lme(fixed=fev1f~year, random=~1|id2, data=pheno2,
> correlation=corARMA(value=0.2, form=~year|id2), na.action=na.omit)
>
> But I get the following error message:
>
> Error in getGroupsFormula.default(correlation, asList = TRUE) :
>         "Form" argument must be a formula
>
> I have used this same form argument with differerent correlation
> structures and it has worked fine. Does anyone know why it won't
> recognise ~year | id2 (or even ~ 1 | id2) as a formula?
>
> Any help would be great
> Pam
>
> -- 
> Pamela A McCaskie
> BSc(Mathematical Sciences)(Hons)
>
> Western Australian Institute for Medical Research
> University of Western Australia
> SCGH Campus
> Ground Floor, B Block
> QE-II Medical Centre
> Hospital Avenue, Nedlands
> Western Australia  6009
> AUSTRALIA
> Email:        pmccask at cyllene.uwa.edu.au
> Phone:        +61-8-9346 1612
> Mob:          0417 926 607
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>
>



-- 
Constantinos Antoniou, Ph.D.
Massachusetts Institute of Technology
Intelligent Transportation Systems Program
77 Massachusetts Ave., Rm. 1-249, Cambridge, MA 02139



From Achim.Zeileis at wu-wien.ac.at  Mon Jul 11 16:07:46 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 11 Jul 2005 16:07:46 +0200 (CEST)
Subject: [R] misc3d package
In-Reply-To: <s2d288ed.025@ccw0m1.nottingham.ac.uk>
References: <s2d288ed.025@ccw0m1.nottingham.ac.uk>
Message-ID: <Pine.LNX.4.58.0507111606320.2750@thorin.ci.tuwien.ac.at>

On Mon, 11 Jul 2005, Mark Edmondson-Jones wrote:

> Hi,
>
> I am trying to install the misc3d package on a Windows (XP) installation
> of R 2.0.1 using install.packages("misc3d") but with no success.  I have
> used this approach with other packages OK, but for misc3d I get the
> following output...

Try using a current version of R for which there is a pre-compiled
windows binary available or install the source package.
Best,
Z

> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 27996 bytes
> opened URL
> downloaded 27Kb
>
> Warning message:
> No package "misc3d" on CRAN. in: download.packages(pkgs, destdir = tmpd, available = available,
>
> Any help would be much appreciated.
>
> Regards,
> Mark
>
>
> This message has been checked for viruses but the contents of an attachment
> may still contain software viruses, which could damage your computer system:
> you are advised to perform your own checks. Email communications with the
> University of Nottingham may be monitored as permitted by UK legislation.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Mon Jul 11 16:14:15 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Jul 2005 10:14:15 -0400
Subject: [R] misc3d package
In-Reply-To: <s2d288ed.025@ccw0m1.nottingham.ac.uk>
References: <s2d288ed.025@ccw0m1.nottingham.ac.uk>
Message-ID: <42D27EB7.7050309@stats.uwo.ca>

On 7/11/2005 9:57 AM, Mark Edmondson-Jones wrote:
> Hi,
> 
> I am trying to install the misc3d package on a Windows (XP) installation of R 2.0.1 using install.packages("misc3d") but with no success.  I have used this approach with other packages OK, but for misc3d I get the following output...
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 27996 bytes
> opened URL
> downloaded 27Kb
> 
> Warning message: 
> No package "misc3d" on CRAN. in: download.packages(pkgs, destdir = tmpd, available = available,  
> 
> Any help would be much appreciated.

The message should be read at face value.  There is no version of misc3d 
for R 2.0.1.  I think misc3d was released in June, well after 2.1.0 came 
out, and we don't do binary builds for old releases.

If you want to use that package, you'll need to upgrade (or try to 
compile it yourself).

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Mon Jul 11 16:18:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 11 Jul 2005 16:18:18 +0200
Subject: [R] misc3d package
In-Reply-To: <s2d288ed.025@ccw0m1.nottingham.ac.uk>
References: <s2d288ed.025@ccw0m1.nottingham.ac.uk>
Message-ID: <42D27FAA.2000601@statistik.uni-dortmund.de>

misc3d is a very new package.
CRAN's windows binary repository for R-2.0.x is no longer updated, hence 
does not contain the package. The corresponding ReadMe tells us: "Last 
update: 19.04.2005."

Either upgrade to R-2.1.1 and try again, or compile the package from 
sources yourself.

Uwe Ligges



Mark Edmondson-Jones wrote:

> Hi,
> 
> I am trying to install the misc3d package on a Windows (XP) installation of R 2.0.1 using install.packages("misc3d") but with no success.  I have used this approach with other packages OK, but for misc3d I get the following output...
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 27996 bytes
> opened URL
> downloaded 27Kb
> 
> Warning message: 
> No package "misc3d" on CRAN. in: download.packages(pkgs, destdir = tmpd, available = available,  
>
> Any help would be much appreciated.
> 
> Regards,
> Mark
> 
> 
> This message has been checked for viruses but the contents of an attachment
> may still contain software viruses, which could damage your computer system:
> you are advised to perform your own checks. Email communications with the
> University of Nottingham may be monitored as permitted by UK legislation.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Mon Jul 11 16:21:56 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 11 Jul 2005 07:21:56 -0700 (PDT)
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <1121080003.5944.12.camel@ipc143004.lif.icnet.uk>
References: <20050711101058.86136.qmail@web26302.mail.ukl.yahoo.com>
	<1121080003.5944.12.camel@ipc143004.lif.icnet.uk>
Message-ID: <Pine.A41.4.61b.0507110721040.179706@homer03.u.washington.edu>

On Mon, 11 Jul 2005, Adaikalavan Ramasamy wrote:

> I am not sure if there is an easy way around this. An ugly hack is to
> make a copy the function "survival:::plot.cox.zph" and make your
> modified function. But there are others in the list who might know
> neater solutions.

If you then send a patch to the package maintainer it stops being an ugly 
hack and turns into an example of collaborative open-source development :)

 	-thomas



From spencer.graves at pdf.com  Mon Jul 11 16:30:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 11 Jul 2005 07:30:52 -0700
Subject: [R] class<- vs. as()
In-Reply-To: <OFB1408645.997A7404-ONC125703B.0046463C-C125703B.00471EB0@inside.allianz.de>
References: <OFB1408645.997A7404-ONC125703B.0046463C-C125703B.00471EB0@inside.allianz.de>
Message-ID: <42D2829C.5060606@pdf.com>

	  What do you want?  Consider the following:

 > v <- matrix(1:9, 3)
 > class(v[1,1])
[1] "integer"
 > class(as.vector(v))
[1] "integer"
 > v2 <- v
 > dim(v2) <- NULL
 > class(v2)
[1] "integer"

	  spencer graves

stefan.albrecht at allianz.com wrote:

> 
> 
> 
> Dear all,
> 
> I would appreciate a lot, if someone could explain to me in a simple
> way, why the assignment class<- is not always working and one has to
> take as() like in the example below.
> 
> 
>>(v <- matrix(1:9, 3))
> 
>      [,1] [,2] [,3]
> [1,]    1    4    7
> [2,]    2    5    8
> [3,]    3    6    9
> 
>>class(v)
> 
> [1] "matrix"
> 
> 
>>class(v) <- "integer"
>>class(v)
> 
> [1] "matrix"
> 
>>v
> 
>      [,1] [,2] [,3]
> [1,]    1    4    7
> [2,]    2    5    8
> [3,]    3    6    9
> 
> 
>>(vi <- as(v, "integer"))
> 
> [1] 1 2 3 4 5 6 7 8 9
> 
>>class(vi)
> 
> [1] "integer"
> 
> 
> With many thanks,
> 
> Stefan
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From helprhelp at gmail.com  Mon Jul 11 16:41:41 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 11 Jul 2005 09:41:41 -0500
Subject: [R] randomForest
In-Reply-To: <17106.5700.477778.953801@stat.math.ethz.ch>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>
	<cdf8178305070712385c31140c@mail.gmail.com>
	<42CD8626.2070109@stats.uwo.ca>
	<17106.5700.477778.953801@stat.math.ethz.ch>
Message-ID: <cdf8178305071107414437b66b@mail.gmail.com>

Thanks.
Many people pointed that out. (It was due to that I only knew lappy by
that time :).


On 7/11/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
> >>>>>     on Thu, 07 Jul 2005 15:44:38 -0400 writes:
> 
>     Duncan> On 7/7/2005 3:38 PM, Weiwei Shi wrote:
>     >> Hi there:
>     >> I have a question on random foresst:
>     >>
>     >> recently i helped a friend with her random forest and i came with this problem:
>     >> her dataset has 6 classes and since the sample size is pretty small:
>     >> 264 and the class distr is like this (Diag is the response variable)
> 
>     >> sample.size <- lapply(1:6, function(i) sum(Diag==i))
>     >>> sample.size
>     >> [[1]]
>     >> [1] 36
> 
>     ....
> 
> and later you get problems because you didn't know that a *list*
> such as 'sample.size' should be made into a so called
> *atomic vector* {and there's a function  is.atomic(.) ! to test for it}
> and Duncan and others told you about unlist().
> 
> Now there are two things I'd want to add:
> 
> 1) If you had used
> 
>       s.size <- table(Diag)
> 
>    you had used a faster and simpler expression with the same result.
>    Though in general (when there can be zero counts), to give the
>    same result, you'd need
> 
>       s.size <- table(factor(Diag, levels = 1:6))
> 
>    Still a bit preferable to the lapply(.) IMO
> 
> 
> 2)  You should get into the habit of using
>      sapply(.)   rather than  lapply(.).
> 
>     sapply() originally was exactly devised for the above task:
>     and stands for ``[s]implified lapply''.
> 
>     It always returns an ``unlisted'' result when appropriate.
> 
> Regards,
> Martin Maechler, ETH Zurich
> 
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From h.andersson at nioo.knaw.nl  Mon Jul 11 16:44:11 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 11 Jul 2005 16:44:11 +0200
Subject: [R] R on kubuntu
In-Reply-To: <200507111320.10793.lefebure@univ-lyon1.fr>
References: <200507111154.08476.constant.depiereux@aqte.be>
	<200507111320.10793.lefebure@univ-lyon1.fr>
Message-ID: <dau0lm$das$1@sea.gmane.org>

However according to:

http://packages.ubuntu.com/hoary/math/r-base

you will have to live with R version 2.0.1 until the next version of 
Ubuntu is released. (If I understood the Ubuntu policy...)

Or is there some other repository providing more updated binaries for 
Ubuntu 5.04?

Cheers, Henrik Andersson

Lefebure Tristan wrote:
> No problem at all.
> If you allow "universe" packages, many binary R packages are available (from 
> the GNU/Linux Debian sid). 
> 
> 
> On Monday 11 July 2005 11:54, Constant Depi??reux wrote:
> 
>>Hello all,
>>
>>I am planning to redeploy my workstation under KUBUNTU.
>>
>>Does any body has any r experience installing/using r on this platform?
>>
>>Best regards.
> 
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From danbebber at yahoo.co.uk  Mon Jul 11 17:04:09 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Mon, 11 Jul 2005 16:04:09 +0100 (BST)
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <Pine.A41.4.61b.0507110721040.179706@homer03.u.washington.edu>
Message-ID: <20050711150409.33531.qmail@web26310.mail.ukl.yahoo.com>

Dear all,

I've modified the plot.cox.zph function to allow
customized xlab and ylab (see below). Someone might
like to confirm that it works.

Thanks for all the assistance.

Dan
___________________________________

plot.cox.zph <- function (x, resid = TRUE, se = TRUE,
df = 4, nsmo = 40, var, 
    xlab="Time",ylab = paste("Beta(t) for",
dimnames(yy)[[2]]),...) 
{
    xx <- x$x
    yy <- x$y
    d <- nrow(yy)
    df <- max(df)
    nvar <- ncol(yy)
    pred.x <- seq(from = min(xx), to = max(xx), length
= nsmo)
    temp <- c(pred.x, xx)
    lmat <- ns(temp, df = df, intercept = TRUE)
    pmat <- lmat[1:nsmo, ]
    xmat <- lmat[-(1:nsmo), ]
    qmat <- qr(xmat)
    if (se) {
        bk <- backsolve(qmat$qr[1:df, 1:df], diag(df))
        xtx <- bk %*% t(bk)
        seval <- d * ((pmat %*% xtx) * pmat) %*%
rep(1, df)
    }
    if (missing(var)) 
        var <- 1:nvar
    else {
        if (is.character(var)) 
            var <- match(var, dimnames(yy)[[2]])
        if (any(is.na(var)) || max(var) > nvar ||
min(var) < 
            1) 
            stop("Invalid variable requested")
    }
    if (x$transform == "log") {
        xx <- exp(xx)
        pred.x <- exp(pred.x)
    }
    else if (x$transform != "identity") {
        xtime <- as.numeric(dimnames(yy)[[1]])
        apr1 <- approx(xx, xtime, seq(min(xx),
max(xx), length = 17)[2 * 
            (1:8)])
        temp <- signif(apr1$y, 2)
        apr2 <- approx(xtime, xx, temp)
        xaxisval <- apr2$y
        xaxislab <- rep("", 8)
        for (i in 1:8) xaxislab[i] <- format(temp[i])
    }
    for (i in var) {
        y <- yy[, i]
        yhat <- pmat %*% qr.coef(qmat, y)
        if (resid) 
            yr <- range(yhat, y)
        else yr <- range(yhat)
        if (se) {
            temp <- 2 * sqrt(x$var[i, i] * seval)
            yup <- yhat + temp
            ylow <- yhat - temp
            yr <- range(yr, yup, ylow)
        }
        if (x$transform == "identity") 
            plot(range(xx), yr, type = "n", xlab =
xlab, ylab = ylab[i], 
                ...)
        else if (x$transform == "log") 
            plot(range(xx), yr, type = "n", xlab =
xlab, ylab = ylab[i], 
                log = "x", ...)
        else {
            plot(range(xx), yr, type = "n", xlab =
xlab, ylab = ylab[i], 
                axes = FALSE, ...)
            axis(1, xaxisval, xaxislab)
            axis(2)
            box()
        }
        if (resid) 
            points(xx, y)
        lines(pred.x, yhat)
        if (se) {
            lines(pred.x, yup, lty = 2)
            lines(pred.x, ylow, lty = 2)
        }
    }
}


--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Mon, 11 Jul 2005, Adaikalavan Ramasamy wrote:
> 
> > I am not sure if there is an easy way around this.
> An ugly hack is to
> > make a copy the function "survival:::plot.cox.zph"
> and make your
> > modified function. But there are others in the
> list who might know
> > neater solutions.
> 
> If you then send a patch to the package maintainer
> it stops being an ugly 
> hack and turns into an example of collaborative
> open-source development :)
> 
>  	-thomas
> 
>



From goedman at mac.com  Mon Jul 11 17:08:44 2005
From: goedman at mac.com (Rob J Goedman)
Date: Mon, 11 Jul 2005 08:08:44 -0700
Subject: [R] Problems with R on OS X
In-Reply-To: <F14115FF-D023-4C0F-99B5-935FAF7BF57E@caselab.info>
References: <F14115FF-D023-4C0F-99B5-935FAF7BF57E@caselab.info>
Message-ID: <E8580C3C-4394-4250-8704-18E4F6A542DE@mac.com>

Hi Heinz,

Can you send me your version of: '~/Library/Preferences/org.R- 
project.R.plist'?

Most Mac OS questions are posted/answered on R-SIG-Mac.

Thanks,
Rob


On Jul 11, 2005, at 2:42 AM, Heinz Schild wrote:

> I used R on OS X 10.3x quite some time with no serious problems.  
> Sometimes R stopped when I tried to execute a bigger program. After  
> updating to OS X to version 10.4 R worked but I still had the  
> problem with bigger programs. Therefore I re-installed R on top of  
> the existing R version. The installation finished properly but  
> suddenly R did not work. Then I reinstalled OS X 10.4 because I  
> thought some left over registry information from R may caused the  
> problem. R still crashed. I hoped the problem would be overcome  
> with the new R version 1.12. Unfortunately this is not the case.  
> The error report (see appendix) says that Thread 0 crashed.
> What can I do?
> Heinz Schild
>
>
> <Errror Report.txt>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From andy_liaw at merck.com  Mon Jul 11 17:18:55 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Jul 2005 11:18:55 -0400
Subject: [R] Off topic -2 Ln Lambda and Chi square
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA83@usctmx1106.Merck.com>

If you meant the lambda as the likelihood ratio test statistic, the
asymptotic chi-squared distribution comes from the asymptotic normality of
the MLEs.  The proof is in a paper by Abraham Wald in 1943.  See Stuart &
Ord (Kendall's Advanced Statistics) for discussion (e.g., vol. 2, 5th
edition).

Andy

> From: Laura Holt
> 
> Dear R :
> 
> Sorry for the off topic question, but does anyone know the 
> reference for
> the -2 Ln Lambda following a Chi Square distribution, please?
> 
> Possibly one of Bartlett's?
> 
> Thanks in advance!
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dmbates at gmail.com  Mon Jul 11 17:20:58 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 11 Jul 2005 10:20:58 -0500
Subject: [R] R on kubuntu
In-Reply-To: <dau0lm$das$1@sea.gmane.org>
References: <200507111154.08476.constant.depiereux@aqte.be>
	<200507111320.10793.lefebure@univ-lyon1.fr>
	<dau0lm$das$1@sea.gmane.org>
Message-ID: <40e66e0b0507110820463be91@mail.gmail.com>

On 7/11/05, Henrik Andersson <h.andersson at nioo.knaw.nl> wrote:
> However according to:
> 
> http://packages.ubuntu.com/hoary/math/r-base
> 
> you will have to live with R version 2.0.1 until the next version of
> Ubuntu is released. (If I understood the Ubuntu policy...)
> 
> Or is there some other repository providing more updated binaries for
> Ubuntu 5.04?

It's possible to install the latest packages from the Debian
repository but awkward.  You would need to add a debian/testing or
debian/unstable repository to /etc/apt/sources.list and configure apt
to use the Ubuntu archives unless you explicitly require the testing
or unstable version of a package.  I think that trying to install such
a version will require you to update your entire compiler tool chain,
which may be more effort than you want to make.

I just recently installed Kubuntu on a machine and will, if I manage
to get the hardware working, do another today.  If that works I try
instaling  r-base-dev/unstabl and report back to the r-sig-debian
mailing list.

(Note that there is a special mailing list for Debian-related
questions and I am moving this thread to that list.)
> Cheers, Henrik Andersson
> 
> Lefebure Tristan wrote:
> > No problem at all.
> > If you allow "universe" packages, many binary R packages are available (from
> > the GNU/Linux Debian sid).
> >
> >
> > On Monday 11 July 2005 11:54, Constant Depi??reux wrote:
> >
> >>Hello all,
> >>
> >>I am planning to redeploy my workstation under KUBUNTU.
> >>
> >>Does any body has any r experience installing/using r on this platform?
> >>
> >>Best regards.
> >
> >
> 
> 
> --
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Tristan.Lefebure at univ-lyon1.fr  Mon Jul 11 17:21:59 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Mon, 11 Jul 2005 17:21:59 +0200
Subject: [R] R on kubuntu
In-Reply-To: <dau0lm$das$1@sea.gmane.org>
References: <200507111154.08476.constant.depiereux@aqte.be>
	<200507111320.10793.lefebure@univ-lyon1.fr>
	<dau0lm$das$1@sea.gmane.org>
Message-ID: <200507111721.59879.lefebure@univ-lyon1.fr>

On Monday 11 July 2005 16:44, Henrik Andersson wrote:
> Or is there some other repository providing more updated binaries for
> Ubuntu 5.04?

I don't think, but I'm fine with R 2.0.1 ...
The next ubuntu release, Ubuntu 5.10 (The Breezy Badger), is for October 2005 
and will include R 2.1.1 (http://packages.ubuntu.com/breezy/math/r-base).


-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'??cologie des hydrosyst??mes fluviaux (UMR 5023)
Universit?? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From azzalini at stat.unipd.it  Mon Jul 11 17:43:41 2005
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 11 Jul 2005 17:43:41 +0200
Subject: [R] Off topic -2 Ln Lambda and Chi square
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA83@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA83@usctmx1106.Merck.com>
Message-ID: <20050711174341.120343f2.azzalini@stat.unipd.it>

On Mon, 11 Jul 2005 11:18:55 -0400, Liaw, Andy wrote:

LA> If you meant the lambda as the likelihood ratio test statistic, the
LA> asymptotic chi-squared distribution comes from the asymptotic
LA> normality of the MLEs.  The proof is in a paper by Abraham Wald in
LA> 1943.  See Stuart & Ord (Kendall's Advanced Statistics) for
LA> discussion (e.g., vol. 2, 5th edition).
LA> 
LA> Andy
LA> 
LA> > From: Laura Holt
LA> > 
LA> > Dear R :
LA> > 
LA> > Sorry for the off topic question, but does anyone know the 
LA> > reference for
LA> > the -2 Ln Lambda following a Chi Square distribution, please?
LA> > 

see

S. S Wilks ( 1938)
The large-sample distribution of the likelihood ratio for testing composite hypotheses
- Ann. Math. Stat, 9, 60-62

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From stdmr03 at moravian.edu  Mon Jul 11 18:02:04 2005
From: stdmr03 at moravian.edu (Rundle, Daniel)
Date: Mon, 11 Jul 2005 12:02:04 -0400
Subject: [R] Weighted nls
Message-ID: <CBA302933458F04BBEA8681D583E54F569039A@mail2.moravian.edu>

Dear R Community,
 
I am attempting to perform a weighted non-linear least squares fit.  It has already been noted that the weights option is not yet implemented for the nls function, but no one seems to offer any suggestions for getting around this problem.  I am still curious if a) anyone has code they have written which includes a weight options for nls, or b) if there is another model which would allow least squares fitting to a non-linear function.
 
Thank you for reading this,
 
Dan



From jerk_alert at hotmail.com  Mon Jul 11 18:13:58 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Mon, 11 Jul 2005 16:13:58 +0000
Subject: [R] Isolating string containing only file name from complete path
Message-ID: <BAY101-F32467AF1B9E65E16417411E8DC0@phx.gbl>

Hi all,

What I'd like to do is to is to be able to extract a string corresponding to 
only the file name from a string containing the complete path, i.e. from the 
following path string:

"/Users/ken/Desktop/test/runs/file1"

I would like to end up with:

"file1"

This would be most ideally done in a platform-independent way.

Thanks in advance,
-Ken



From ramasamy at cancer.org.uk  Mon Jul 11 18:25:40 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 17:25:40 +0100
Subject: [R] plot(cox.zph()): customize xlab & ylab
In-Reply-To: <20050711150409.33531.qmail@web26310.mail.ukl.yahoo.com>
References: <20050711150409.33531.qmail@web26310.mail.ukl.yahoo.com>
Message-ID: <1121099140.5944.95.camel@ipc143004.lif.icnet.uk>

Dan, I think this works fine now. 

I think the 'ylab' argument in other plot function take only a single
value whereas in the case of plot.cox.zph, it needs a vector. It is
especially confusing when this function does plots everything on a
single page by default and all you see is the last plot.

Thus, can I suggest that you check that the length of the ylab is the
same as the number of terms in the coxph call. To do this you could put
something along the following anywhere after the line "yy <- x$y"

n <- length(coefficients(fit))
if( length(ylab) != n){
  warning( paste("'ylab' is of length", length(ylab),
                 "and was expecting a length of", n, "elements.") )
}

Regards, Adai



On Mon, 2005-07-11 at 16:04 +0100, Dan Bebber wrote:
> Dear all,
> 
> I've modified the plot.cox.zph function to allow
> customized xlab and ylab (see below). Someone might
> like to confirm that it works.
> 
> Thanks for all the assistance.
> 
> Dan
> ___________________________________
> 
> plot.cox.zph <- function (x, resid = TRUE, se = TRUE,
> df = 4, nsmo = 40, var, 
>     xlab="Time",ylab = paste("Beta(t) for",
> dimnames(yy)[[2]]),...) 
> {
>     xx <- x$x
>     yy <- x$y
>     d <- nrow(yy)
>     df <- max(df)
>     nvar <- ncol(yy)
>     pred.x <- seq(from = min(xx), to = max(xx), length
> = nsmo)
>     temp <- c(pred.x, xx)
>     lmat <- ns(temp, df = df, intercept = TRUE)
>     pmat <- lmat[1:nsmo, ]
>     xmat <- lmat[-(1:nsmo), ]
>     qmat <- qr(xmat)
>     if (se) {
>         bk <- backsolve(qmat$qr[1:df, 1:df], diag(df))
>         xtx <- bk %*% t(bk)
>         seval <- d * ((pmat %*% xtx) * pmat) %*%
> rep(1, df)
>     }
>     if (missing(var)) 
>         var <- 1:nvar
>     else {
>         if (is.character(var)) 
>             var <- match(var, dimnames(yy)[[2]])
>         if (any(is.na(var)) || max(var) > nvar ||
> min(var) < 
>             1) 
>             stop("Invalid variable requested")
>     }
>     if (x$transform == "log") {
>         xx <- exp(xx)
>         pred.x <- exp(pred.x)
>     }
>     else if (x$transform != "identity") {
>         xtime <- as.numeric(dimnames(yy)[[1]])
>         apr1 <- approx(xx, xtime, seq(min(xx),
> max(xx), length = 17)[2 * 
>             (1:8)])
>         temp <- signif(apr1$y, 2)
>         apr2 <- approx(xtime, xx, temp)
>         xaxisval <- apr2$y
>         xaxislab <- rep("", 8)
>         for (i in 1:8) xaxislab[i] <- format(temp[i])
>     }
>     for (i in var) {
>         y <- yy[, i]
>         yhat <- pmat %*% qr.coef(qmat, y)
>         if (resid) 
>             yr <- range(yhat, y)
>         else yr <- range(yhat)
>         if (se) {
>             temp <- 2 * sqrt(x$var[i, i] * seval)
>             yup <- yhat + temp
>             ylow <- yhat - temp
>             yr <- range(yr, yup, ylow)
>         }
>         if (x$transform == "identity") 
>             plot(range(xx), yr, type = "n", xlab =
> xlab, ylab = ylab[i], 
>                 ...)
>         else if (x$transform == "log") 
>             plot(range(xx), yr, type = "n", xlab =
> xlab, ylab = ylab[i], 
>                 log = "x", ...)
>         else {
>             plot(range(xx), yr, type = "n", xlab =
> xlab, ylab = ylab[i], 
>                 axes = FALSE, ...)
>             axis(1, xaxisval, xaxislab)
>             axis(2)
>             box()
>         }
>         if (resid) 
>             points(xx, y)
>         lines(pred.x, yhat)
>         if (se) {
>             lines(pred.x, yup, lty = 2)
>             lines(pred.x, ylow, lty = 2)
>         }
>     }
> }
> 
> 
> --- Thomas Lumley <tlumley at u.washington.edu> wrote:
> 
> > On Mon, 11 Jul 2005, Adaikalavan Ramasamy wrote:
> > 
> > > I am not sure if there is an easy way around this.
> > An ugly hack is to
> > > make a copy the function "survival:::plot.cox.zph"
> > and make your
> > > modified function. But there are others in the
> > list who might know
> > > neater solutions.
> > 
> > If you then send a patch to the package maintainer
> > it stops being an ugly 
> > hack and turns into an example of collaborative
> > open-source development :)
> > 
> >  	-thomas
> > 
> > 
> 
> 
> 
> 	
> 	
> 		
> ___________________________________________________________ 
> Yahoo! Messenger - NEW crystal clear PC to PC calling worldwide with voicemail http://uk.messenger.yahoo.com
>



From spluque at gmail.com  Mon Jul 11 18:24:00 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Mon, 11 Jul 2005 11:24:00 -0500
Subject: [R] Isolating string containing only file name from complete
	path
References: <BAY101-F32467AF1B9E65E16417411E8DC0@phx.gbl>
Message-ID: <87br59nwf3.fsf@gmail.com>

"Ken Termiso" <jerk_alert at hotmail.com> wrote:
> Hi all,
>
> What I'd like to do is to is to be able to extract a string
> corresponding to only the file name from a string containing the
> complete path, i.e. from the following path string:
>
> "/Users/ken/Desktop/test/runs/file1"
>
> I would like to end up with:
>
> "file1"


?basename

-- 
Sebastian P. Luque



From ramasamy at cancer.org.uk  Mon Jul 11 18:29:18 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 17:29:18 +0100
Subject: [R] Isolating string containing only file name from
	complete	path
In-Reply-To: <BAY101-F32467AF1B9E65E16417411E8DC0@phx.gbl>
References: <BAY101-F32467AF1B9E65E16417411E8DC0@phx.gbl>
Message-ID: <1121099358.5944.98.camel@ipc143004.lif.icnet.uk>

x <- "/Users/ken/Desktop/test/runs/file1.txt"
basename(x)
[1] "file1.txt"



On Mon, 2005-07-11 at 16:13 +0000, Ken Termiso wrote:
> Hi all,
> 
> What I'd like to do is to is to be able to extract a string corresponding to 
> only the file name from a string containing the complete path, i.e. from the 
> following path string:
> 
> "/Users/ken/Desktop/test/runs/file1"
> 
> I would like to end up with:
> 
> "file1"
> 
> This would be most ideally done in a platform-independent way.
> 
> Thanks in advance,
> -Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From smcnary at charm.net  Mon Jul 11 18:43:37 2005
From: smcnary at charm.net (Scot W McNary)
Date: Mon, 11 Jul 2005 12:43:37 -0400 (EDT)
Subject: [R] small first graph of par(3,2), other 5 are correct
Message-ID: <20050711121319.Y47208@fellspt.charm.net>


Hi,

I'm trying to produce 6 graphs on a single page using code I've borrowed 
from an example by Paul Murrell:

(http://www.stat.auckland.ac.nz/~paul/RGraphics/custombase-xmastree.R).

It involves placing 6 horizontal barplots on one page and adding common 
labels.

The problem is the first graph in my figure (the one in the (1,1) 
position) is smaller than the other 5.  A toy example is included below.

When I compare the par() options set after producing each graph, all of 
the parameters that change look like what I would expect:

> names(after.g1[after.g1%in%after.g2=="FALSE"])
[1] "fig" "mai" "mar" "mfg" "plt"

> what.chgd<-names(after.g1[after.g1%in%after.g2=="FALSE"])
>
> after.g1[what.chgd]
$fig
[1] 0.0000000 0.5000000 0.6666667 1.0000000

$mai
[1] 0.0309375 0.0618750 0.3093750 0.3093750

$mar
[1] 0.5 1.0 5.0 5.0

$mfg
[1] 1 1 3 2

$plt
[1] 0.02357143 0.88214285 0.01437097 0.85629032

>
> after.g2[what.chgd]
$fig
[1] 0.5000000 1.0000000 0.6666667 1.0000000

$mai
[1] 0.0309375 0.3093750 0.3093750 0.0618750

$mar
[1] 0.5 5.0 5.0 1.0

$mfg
[1] 1 2 3 2

$plt
[1] 0.11785715 0.97642857 0.01437097 0.85629032



One other aspect of this is that the problem does not occur if after I 
create all 6 graphs once, I rerun the code again, but omit only the 
"par(mfrow=c(3,2))" statement.  If I don't reset par(), then the first 
graph is then the identical size of the other five.

Up to now I've gotten around this by simply running the code twice, 
omitting the "par(mfrow=c(3,2))" the second time and printing the result. 
Now, however, I'm sending the graphs to a pdf file and must set the 
"par(mfrow=c(3,2))" after opening the pdf device, so the problem shows up 
with each time.

Any help on fixing this problem so that all 6 graphs appear the same size 
on the figure would be most welcome.

Thanks,

Scot


####


Here is the toy example that shows the layout and plots in my figure:

par(mfrow=c(3,2))

groups<-LETTERS[1:5]

### for graph 1 ###

# data
leftci <- c(1:5)
rightci <- c(2:6)

# left column graph so:
par(mar=c(0.5, 1, 5, 5))
# right column graph so:
#par(mar=c(0.5, 5, 5, 1))

plot.new()
title(main = "Graph 1")
plot.window(xlim=c(0, 8), ylim=c(-1.5, 5.5))

ticks <- seq(0, 8, 1)
y <- 1:5  # how many spaces on y axis: 1 for each group
h <- 0.2  # height?  a function of y?

segments(0, y, 8, y, lty="dotted")  # dotted segments on which bar lies 
(like a grid)
rect(leftci, y-h, rightci, y+h, col="dark grey")
#mtext(groups, at=y, adj=1, side=2, las=2, cex=.75)
par(cex.axis=1.0, mex=0.5)
axis(1, at=ticks, labels=abs(ticks), pos=0, )

box("inner", col="grey")

after.g1<-show(par())

### for graph 2 ###

# data:
leftci <- c(1:5)
rightci <- c(2:6)

# left column graph so:
#par(mar=c(0.5, 1, 5, 5))
# right column graph so:
par(mar=c(0.5, 5, 5, 1))

plot.new()
title(main = "Graph 2")

plot.window(xlim=c(0, 8), ylim=c(-1.5, 5.5))

ticks <- seq(0, 8, 1)
y <- 1:5  # how many spaces on y axis: 1 for each group
h <- 0.2  # height?  a function of y?

segments(0, y, 8, y, lty="dotted")  # dotted segments on which bar lies 
(like a grid)
rect(leftci, y-h, rightci, y+h, col="dark grey")
mtext(groups, at=y, adj=.5, side=2, las=2, cex=.75, line = 5) # this line 
only for right column graphs
par(cex.axis=1.0, mex=0.5)
axis(1, at=ticks, labels=abs(ticks), pos=0, )

box("inner", col="grey")

# end graph 2

after.g2<-show(par())

### graph 3###

# data
leftci <- c(1:5)
rightci <- c(2:6)

# left column graph so:
par(mar=c(0.5, 1, 5, 5))
# right column graph so:
#par(mar=c(0.5, 5, 5, 1))

plot.new()
title(main = "Graph 3")
plot.window(xlim=c(0, 8), ylim=c(-1.5, 5.5))

ticks <- seq(0, 8, 1)
y <- 1:5  # how many spaces on y axis: 1 for each group
h <- 0.2  # height?  a function of y?

segments(0, y, 8, y, lty="dotted")  # dotted segments on which bar lies 
(like a grid)
rect(leftci, y-h, rightci, y+h, col="dark grey")
#mtext(groups, at=y, adj=1, side=2, las=2, cex=.75)
par(cex.axis=1.0, mex=0.5)
axis(1, at=ticks, labels=abs(ticks), pos=0, )

box("inner", col="grey")

after.g3<-show(par())

### for graph 4 ###

# data:
leftci <- c(1:5)
rightci <- c(2:6)

# left column graph so:
#par(mar=c(0.5, 1, 5, 5))
# right column graph so:
par(mar=c(0.5, 5, 5, 1))

plot.new()
title(main = "Graph 4")

plot.window(xlim=c(0, 8), ylim=c(-1.5, 5.5))

ticks <- seq(0, 8, 1)
y <- 1:5  # how many spaces on y axis: 1 for each group
h <- 0.2  # height?  a function of y?

segments(0, y, 8, y, lty="dotted")  # dotted segments on which bar lies 
(like a grid)
rect(leftci, y-h, rightci, y+h, col="dark grey")
mtext(groups, at=y, adj=.5, side=2, las=2, cex=.75, line = 5) # this line 
only for right column graphs
par(cex.axis=1.0, mex=0.5)
axis(1, at=ticks, labels=abs(ticks), pos=0, )

box("inner", col="grey")

after.g4<-show(par())

# end graph 4

### graph 5###

# data
leftci <- c(1:5)
rightci <- c(2:6)

# left column graph so:
par(mar=c(0.5, 1, 5, 5))
# right column graph so:
#par(mar=c(0.5, 5, 5, 1))

plot.new()
title(main = "Graph 5")
plot.window(xlim=c(0, 8), ylim=c(-1.5, 5.5))

ticks <- seq(0, 8, 1)
y <- 1:5  # how many spaces on y axis: 1 for each group
h <- 0.2  # height?  a function of y?

segments(0, y, 8, y, lty="dotted")  # dotted segments on which bar lies 
(like a grid)
rect(leftci, y-h, rightci, y+h, col="dark grey")
#mtext(groups, at=y, adj=1, side=2, las=2, cex=.75)
par(cex.axis=1.0, mex=0.5)
axis(1, at=ticks, labels=abs(ticks), pos=0, )

box("inner", col="grey")

after.g5<-show(par())

### for graph 6 ###

# data:
leftci <- c(1:5)
rightci <- c(2:6)

# left column graph so:
#par(mar=c(0.5, 1, 5, 5))
# right column graph so:
par(mar=c(0.5, 5, 5, 1))

plot.new()
title(main = "Graph 6")

plot.window(xlim=c(0, 8), ylim=c(-1.5, 5.5))

ticks <- seq(0, 8, 1)
y <- 1:5  # how many spaces on y axis: 1 for each group
h <- 0.2  # height?  a function of y?

segments(0, y, 8, y, lty="dotted")  # dotted segments on which bar lies 
(like a grid)
rect(leftci, y-h, rightci, y+h, col="dark grey")
mtext(groups, at=y, adj=.5, side=2, las=2, cex=.75, line = 5) # this line 
only for right column graphs
par(cex.axis=1.0, mex=0.5)
axis(1, at=ticks, labels=abs(ticks), pos=0, )

box("inner", col="grey")

after.g6<-show(par())

# end graph 6




> version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.1
year     2005
month    06
day      20
language R




--
   Scot W. McNary  email:smcnary at charm.net



From S.O.Nyangoma at amc.uva.nl  Mon Jul 11 18:47:04 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Mon, 11 Jul 2005 18:47:04 +0200
Subject: [R] exact values for p-values
Message-ID: <17247091723f9a.1723f9a1724709@amc.uva.nl>

Hi there,
If I do an lm, I get p-vlues as

p-value: < 2.2e-16

Suppose am interested in exact value such as 

p-value = 1.6e-16 (note = and not <)

How do I go about it?

stephen



From oliver at bic.mni.mcgill.ca  Mon Jul 11 18:51:16 2005
From: oliver at bic.mni.mcgill.ca (Oliver Lyttelton)
Date: Mon, 11 Jul 2005 12:51:16 -0400
Subject: [R] Projection Pursuit
Message-ID: <HJEOLCCIEHBKJJLPOEBFAEBBCCAA.oliver@bic.mni.mcgill.ca>



Hello,

Just a quick question about ppr in library modreg.

I have looked at Ripley and Venables 2002 and it says that projection
pursuit works "by projecting X in M carefully chosen directions"

I want to know how it choses the directions? I presume it moves around the
high-dimensional space of unit vectors finding ones that separate the
response variables, but how.

I looked at the code, but can't get deeper in than the Fortran call to
Smart.

I tried to look up Friedman 87, but can't find an electronic reference.

Can anyone help, or point me in the direction of an online reference?

Thanks in advance,

Oliver Lyttelton



From S.O.Nyangoma at amc.uva.nl  Mon Jul 11 18:52:01 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Mon, 11 Jul 2005 18:52:01 +0200
Subject: [R] exact values for p-values - more information.
Message-ID: <16461b3164d4a4.164d4a416461b3@amc.uva.nl>

 Hi there,
 If I do an lm, I get p-vlues as
 
 p-value: < 2.2e-16
 
This is obtained from F =39540 with df1 = 1, df2 = 7025.

 Suppose am interested in exact value such as 
 
 p-value = 1.6e-16 (note = and not <)
 
 How do I go about it?
 
 stephen



From ernesto at ipimar.pt  Mon Jul 11 19:14:19 2005
From: ernesto at ipimar.pt (ernesto)
Date: Mon, 11 Jul 2005 18:14:19 +0100
Subject: [R] R on kubuntu
In-Reply-To: <dau0lm$das$1@sea.gmane.org>
References: <200507111154.08476.constant.depiereux@aqte.be>	<200507111320.10793.lefebure@univ-lyon1.fr>
	<dau0lm$das$1@sea.gmane.org>
Message-ID: <42D2A8EB.7010507@ipimar.pt>

You can compile the newest version...

EJ


Henrik Andersson wrote:

>However according to:
>
>http://packages.ubuntu.com/hoary/math/r-base
>
>you will have to live with R version 2.0.1 until the next version of 
>Ubuntu is released. (If I understood the Ubuntu policy...)
>
>Or is there some other repository providing more updated binaries for 
>Ubuntu 5.04?
>
>Cheers, Henrik Andersson
>
>Lefebure Tristan wrote:
>  
>
>>No problem at all.
>>If you allow "universe" packages, many binary R packages are available (from 
>>the GNU/Linux Debian sid). 
>>
>>
>>On Monday 11 July 2005 11:54, Constant Depi??reux wrote:
>>
>>    
>>
>>>Hello all,
>>>
>>>I am planning to redeploy my workstation under KUBUNTU.
>>>
>>>Does any body has any r experience installing/using r on this platform?
>>>
>>>Best regards.
>>>      
>>>
>>    
>>
>
>
>  
>



From Achim.Zeileis at wu-wien.ac.at  Mon Jul 11 19:22:44 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 11 Jul 2005 19:22:44 +0200 (CEST)
Subject: [R] exact values for p-values - more information.
In-Reply-To: <16461b3164d4a4.164d4a416461b3@amc.uva.nl>
References: <16461b3164d4a4.164d4a416461b3@amc.uva.nl>
Message-ID: <Pine.LNX.4.58.0507111909490.3075@thorin.ci.tuwien.ac.at>

On Mon, 11 Jul 2005, S.O. Nyangoma wrote:

>  Hi there,
>  If I do an lm, I get p-vlues as
>
>  p-value: < 2.2e-16
>
> This is obtained from F =39540 with df1 = 1, df2 = 7025.
>
>  Suppose am interested in exact value such as
>
>  p-value = 1.6e-16 (note = and not <)
>
>  How do I go about it?

You can always extract the `exact' p-value from the "summary.lm" object or
you can compute it by hand via
  pf(39540, df1 = 1, df2 = 7025, lower.tail = FALSE)
For all practical purposes, the above means that the p-value is 0.
I guess you are on a 32-bit machine, then it also means that the p-value
is smaller than the Machine epsilon
  .Machine$double.eps

So if you want to report the p-value somewhere, I think R's output should
be more than precise enough. If you want to compute some other values that
depend on such a p-value, then it is probably wiser to compute on a log
scale, i.e. instead
  pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE)
use
  pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE, log.p = TRUE)

However, don't expect to be able to evaluate it at such extreme values
such as 39540.
Z



From ripley at stats.ox.ac.uk  Mon Jul 11 19:28:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Jul 2005 18:28:10 +0100 (BST)
Subject: [R] demo(scoping)
In-Reply-To: <20050704020812.5162f251@localhost.localdomain>
References: <20050704020812.5162f251@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0507111822450.5172@gannet.stats>

Why have you sent a message about this, which no indication except the 
non-English word `entercount'?

Note that the message came from within try(), so it was intentional.
If you look at the source it says

 	try(ross$withdraw(500)) # no way..

It is helpful to learn how the code being demonstrated (here try) works.
If you don't understand it. please explain your difficulty in as much 
detail as you can.


On Mon, 4 Jul 2005, ronggui wrote:

> entercount an error with demo(scoping).
>
>> demo(scoping)
>
>
>        demo(scoping)
>        ---- ~~~~~~~
> ___snip_____
>
>
>> ross$balance()
> Your balance is 120
>
>
>> try(ross$withdraw(500))
> Error in ross$withdraw(500) : You don't have that much money!
>
>
>> version
>         _
> platform i486-pc-linux-gnu
> arch     i486
> os       linux-gnu
> system   i486, linux-gnu
> status   beta
> major    2
> minor    1.1
> year     2005
> month    06
> day      13
> language R
>
>
> -- 
> Department of Sociology
> Fudan University,Shanghai
> Blog:http://sociology.yculblog.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 11 19:37:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Jul 2005 18:37:45 +0100 (BST)
Subject: [R] timezone problems
In-Reply-To: <op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>
References: <op.stfpuic90efqu7@neyman.fam.tuwien.ac.at>
	<p06210203bef19c99bfe2@[128.115.153.6]>
	<op.stjbuoxq0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <Pine.LNX.4.61.0507111833470.5172@gannet.stats>

For the record, this was covered by an answer to your bug report.

The problem is your OS which mishandles a timezone of `GMT', so 
Sys.getenv("TZ") as `GMT' is not actually setting your OS to GMT.
Hence NA is the correct answer.

I know no way to set Windows to GMT as distinct from the timezone of 
London (with summer time).

On Thu, 7 Jul 2005, Martin Keller-Ressel wrote:

> Thank you Don for your hints. I have checked my environment vairable TZ
> again. But everything is set correctly. I think the problem is with
> Sys.timezone(). Maybe it is a conflict between how my system formats the
> time/date and what Sys.timezone() expects.
> This is what I get on my system:
>
>> Sys.getenv("TZ")
>    TZ
> "GMT"
>> Sys.time()
> [1] "2005-07-07 07:32:39 GMT"
>
> ## everything fine so far
>
>> Sys.timezone()
> [1] NA
>
> ## This is what Sys.timezone looks like:
>> Sys.timezone
> function ()
> {
>     z <- as.POSIXlt(Sys.time())
>     attr(z, "tzone")[2 + z$isdst]
> }
> <environment: namespace:base>
>
>> z <- as.POSIXlt(Sys.time())
>> attributes(z)
> $names
> [1] "sec"   "min"   "hour"  "mday"  "mon"   "year"  "wday"  "yday"  "isdst"
>
> $class
> [1] "POSIXt"  "POSIXlt"
>
> $tzone
> [1] "GMT"
>
>> attr(z,"tzone")
> [1] "GMT"
>> z$isdst
> [1] 0
>> attr(z,"tzone")[2]
> [1] NA
>
> I dont understand why Sys.timezone doesn't use attr(z,"tzone") but tries
> to read its (2+z$isdst)-th element.
> Of course it would be easy to write a workaround, but I wonder why nobody
> else is having this problem.
>
> best regards,
>
> Martin Keller-Ressel
>
>
>
> On Wed, 06 Jul 2005 14:45:25 -0000, Don MacQueen <macq at llnl.gov> wrote:
>
>> How did you set the TZ system variable?
>> If you did not use Sys.putenv(), try using it instead.
>> Otherwise, I think you have to ask the package maintainer.
>>
>> You may be misleading yourself by using Sys.time() to test whether TZ is
>> set.
>> What does Sys.getenv() tell you?
>>
>> I get a timezone code from Sys.time() even when TZ is not defined (see
>> example below).
>> (but I do have a different OS)
>>
>>>  Sys.timezone()
>> [1] ""
>>>  Sys.time()
>> [1] "2005-07-06 07:34:15 PDT"
>>>  Sys.getenv('TZ')
>> TZ
>> ""
>>>  Sys.putenv(TZ='US/Pacific')
>>>  Sys.timezone()
>> [1] "US/Pacific"
>>>  Sys.getenv('TZ')
>>            TZ
>> "US/Pacific"
>>>  Sys.time()
>> [1] "2005-07-06 07:34:38 PDT"
>>
>>>  Sys.putenv(TZ='GMT')
>>>  Sys.time()
>> [1] "2005-07-06 14:35:45 GMT"
>>
>>>  version
>>           _                       platform powerpc-apple-darwin7.9.0
>> arch     powerpc                 os       darwin7.9.0
>> system   powerpc, darwin7.9.0    status
>> major    2                       minor    1.1
>> year     2005                    month    06
>> day      20                      language R
>> At 9:55 AM +0000 7/5/05, Martin Keller-Ressel wrote:
>>> Hi,
>>>
>>> Im using R 2.1.1 and running Code that previously worked (on R 2.1.0 I
>>> believe) using the 'timeDate' function from the fCalendar package. The
>>> code now throws an error:
>>>
>>> Error in if (Sys.timezone() != "GMT") warning("Set timezone to GMT!")
>>>
>>> However I have read the documentation of the fCalendar package and I
>>> have set my system variable TZ to GMT.
>>> I tracked the error down to the function Sys.timezone() which returns
>>> NA in spite of what Sys.time() returns.
>>>
>>>>  Sys.timezone()
>>> [1] NA
>>>
>>>>  Sys.time()
>>> [1] "2005-07-05 08:41:53 GMT"
>>>
>>> My version:
>>>
>>>>  version
>>>           _
>>> platform i386-pc-mingw32
>>> arch     i386
>>> os       mingw32
>>> system   i386, mingw32
>>> status
>>> major    2
>>> minor    1.1
>>> year     2005
>>> month    06
>>> day      20
>>> language R
>>>
>>> Any help is appreciated,
>>>
>>> Martin Keller-Ressel
>>>
>>>
>>> ---
>>> Martin Keller-Ressel
>>> Research Unit of Financial and Actuarial Mathematics
>>> TU Vienna
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>
>>
>
>
>
> -- 
> Martin Keller-Ressel
> Research Unit of Financial and Actuarial Mathematics
> TU Vienna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Mon Jul 11 19:38:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Jul 2005 13:38:46 -0400
Subject: [R] Weighted nls
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA84@usctmx1106.Merck.com>

Please do your homework, as the posting guide asks you to.  

1.  From the Example section of ?nls:

## weighted nonlinear regression
Treated <- Puromycin[Puromycin$state == "treated", ]
weighted.MM <- function(resp, conc, Vm, K)
{
    ## Purpose: exactly as white book p.451 -- RHS for nls()
    ##  Weighted version of Michaelis-Menten model
    ## ------------------------------------------------------------
    ## Arguments: 'y', 'x' and the two parameters (see book)
    ## ------------------------------------------------------------
    ## Author: Martin Maechler, Date: 23 Mar 2001, 18:48

    pred <- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}

Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
              start = list(Vm = 200, K = 0.1),
              trace = TRUE)

2.  This has been covered several times on this list.  Search the archive.

Andy

> From: Rundle, Daniel
> 
> Dear R Community,
>  
> I am attempting to perform a weighted non-linear least 
> squares fit.  It has already been noted that the weights 
> option is not yet implemented for the nls function, but no 
> one seems to offer any suggestions for getting around this 
> problem.  I am still curious if a) anyone has code they have 
> written which includes a weight options for nls, or b) if 
> there is another model which would allow least squares 
> fitting to a non-linear function.
>  
> Thank you for reading this,
>  
> Dan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From spencer.graves at pdf.com  Mon Jul 11 19:39:37 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 11 Jul 2005 10:39:37 -0700
Subject: [R] exact values for p-values - more information.
In-Reply-To: <Pine.LNX.4.58.0507111909490.3075@thorin.ci.tuwien.ac.at>
References: <16461b3164d4a4.164d4a416461b3@amc.uva.nl>
	<Pine.LNX.4.58.0507111909490.3075@thorin.ci.tuwien.ac.at>
Message-ID: <42D2AED9.5030602@pdf.com>

	  I just checked:

 > pf(39540, 1, 7025, lower.tail=FALSE, log.p=TRUE)
[1] -Inf

	  This is not correct.  With 7025 denominator degrees of freedom, we 
might use the chi-square approximation to the F distribution:
	
 > pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)
[1] -19775.52

	  In sum, my best approximation to  pf(39540, 1, 7025, 
lower.tail=FALSE, log.p=TRUE), given only a minute to work on this, is 
exp(pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)) = exp(-19775.52).

	  I'm confident that many violations of assumptions would likely be 
more important than the differences between "p-value: < 2.2e-16" and 
these other two answers.  However, I have also used numbers like 
exp(-19775.52) to guestimate relative degrees of plausibility for 
different alternatives.  That doesn't mean they are right, only the best 
I can get with the available resources.

	  spencer graves

Achim Zeileis wrote:

> On Mon, 11 Jul 2005, S.O. Nyangoma wrote:
> 
> 
>> Hi there,
>> If I do an lm, I get p-vlues as
>>
>> p-value: < 2.2e-16
>>
>>This is obtained from F =39540 with df1 = 1, df2 = 7025.
>>
>> Suppose am interested in exact value such as
>>
>> p-value = 1.6e-16 (note = and not <)
>>
>> How do I go about it?
> 
> 
> You can always extract the `exact' p-value from the "summary.lm" object or
> you can compute it by hand via
>   pf(39540, df1 = 1, df2 = 7025, lower.tail = FALSE)
> For all practical purposes, the above means that the p-value is 0.
> I guess you are on a 32-bit machine, then it also means that the p-value
> is smaller than the Machine epsilon
>   .Machine$double.eps
> 
> So if you want to report the p-value somewhere, I think R's output should
> be more than precise enough. If you want to compute some other values that
> depend on such a p-value, then it is probably wiser to compute on a log
> scale, i.e. instead
>   pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE)
> use
>   pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE, log.p = TRUE)
> 
> However, don't expect to be able to evaluate it at such extreme values
> such as 39540.
> Z
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From andy_liaw at merck.com  Mon Jul 11 19:49:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Jul 2005 13:49:33 -0400
Subject: [R] Projection Pursuit
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA85@usctmx1106.Merck.com>

Google can be helpful.  The 10th hit I got from "projection pursuit
regression" is http://www.scs.gmu.edu/~jgentle/csi991/03f/ppreg1024.rtf,
which gives some rough outline of the algorithm.  If you want more detail,
Ripley (1996) PRNN would suffice, I believe.

Andy

> From: Oliver Lyttelton
> 
> Hello,
> 
> Just a quick question about ppr in library modreg.
> 
> I have looked at Ripley and Venables 2002 and it says that projection
> pursuit works "by projecting X in M carefully chosen directions"
> 
> I want to know how it choses the directions? I presume it 
> moves around the
> high-dimensional space of unit vectors finding ones that separate the
> response variables, but how.
> 
> I looked at the code, but can't get deeper in than the Fortran call to
> Smart.
> 
> I tried to look up Friedman 87, but can't find an electronic 
> reference.
> 
> Can anyone help, or point me in the direction of an online reference?
> 
> Thanks in advance,
> 
> Oliver Lyttelton
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Mon Jul 11 19:52:14 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jul 2005 19:52:14 +0200
Subject: [R] Weighted nls
In-Reply-To: <CBA302933458F04BBEA8681D583E54F569039A@mail2.moravian.edu>
References: <CBA302933458F04BBEA8681D583E54F569039A@mail2.moravian.edu>
Message-ID: <x2y88dtelt.fsf@turmalin.kubism.ku.dk>

"Rundle, Daniel" <stdmr03 at moravian.edu> writes:

> Dear R Community,
>  
> I am attempting to perform a weighted non-linear least squares fit.
> It has already been noted that the weights option is not yet
> implemented for the nls function, but no one seems to offer any
> suggestions for getting around this problem. I am still curious if
> a) anyone has code they have written which includes a weight options
> for nls, or b) if there is another model which would allow least
> squares fitting to a non-linear function.
>  

gnls in the nlme package. Or follow the example on help(nls) immediately
after the      

## weighted nonlinear regression

(!)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From David.Brahm at geodecapital.com  Mon Jul 11 19:55:43 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Mon, 11 Jul 2005 13:55:43 -0400
Subject: [R] small first graph of par(3,2), other 5 are correct
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BC03@MSGBOSCLF2WIN.DMN1.FMR.COM>

Scot,

Here is your toy example in more condensed form:

x11()
par(mar=c(0.5, 1, 5, 5))
par(mfrow=c(3,2))
plot(1:10)
par("mex")                   # mex=1.0 here
par(cex.axis=1.0, mex=0.5)   # Now you change it
for (i in 1:5) plot(1:10)

When you build your first plot (effectively at the plot.new() command),
your "mex" parameter is 1, and that leaves lots of space in the margins.
Then you set mex=0.5, which holds for all subsequent plots, and they get
less margin space.

-- David Brahm (brahm at alum.mit.edu)



From ramasamy at cancer.org.uk  Mon Jul 11 19:57:09 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 18:57:09 +0100
Subject: [R] exact values for p-values - more information.
In-Reply-To: <16461b3164d4a4.164d4a416461b3@amc.uva.nl>
References: <16461b3164d4a4.164d4a416461b3@amc.uva.nl>
Message-ID: <1121104630.5944.106.camel@ipc143004.lif.icnet.uk>

Compare the following

   t.test( 1:100, 101:200 )$p.value 
   t.test( 1:100, 101:200 )

In the latter, the print method truncates to 2.2e-16. 
You can go as far as (depending on your machine)

   .Machine$double.xmin
   [1] 2.225074e-308

before it becomes indistinguishable from zero.

But there are good reasons to truncate it at 2.2e-16 such as the
difficulty in trying to accurately estimate the extreme tail
probabilities.

Regards, Adai



On Mon, 2005-07-11 at 18:52 +0200, S.O. Nyangoma wrote:
>  Hi there,
>  If I do an lm, I get p-vlues as
>  
>  p-value: < 2.2e-16
>  
> This is obtained from F =39540 with df1 = 1, df2 = 7025.
> 
>  Suppose am interested in exact value such as 
>  
>  p-value = 1.6e-16 (note = and not <)
>  
>  How do I go about it?
>  
>  stephen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jenny at stat.ubc.ca  Mon Jul 11 20:48:32 2005
From: jenny at stat.ubc.ca (Jenny Bryan)
Date: Mon, 11 Jul 2005 11:48:32 -0700
Subject: [R] indexing into and modifying dendrograms
Message-ID: <5cecd1f1c042c733ca9c9bcec42f670a@stat.ubc.ca>

I would like to be able to exert certain types of control over the
plotting of dendrograms (representing hierarchical clusterings) that I
think is best achieved by modifying the dendrogram object
prior to plotting.  I am using the "dendrogram" class and associated
methods.

Define the cluster number of each cluster formed as the corresponding
row of the merge object.  So, if you are clustering m objects, the
cluster numbers range from 1 to m-1, with cluster m-1 containing all m
objects, by definition.  I basically want a way to index into an
object of class dendrogram using the above definition of cluster
number and/or to act on a dendrogram, where I specify the target node
using cluster number.

The first application would be to 'flip' the two elements in target
node of the dendrogram (made clear in the small example below). (The
setting is genomics and I have applications where I want to man-handle
my dendrograms to make certain features of the clustering more obvious
to the naked eye.)  I could imagine other, related actions that would
be useful in decorating dendrograms.

I think I need a function that takes a dendrogram and cluster
number(s) as input and returns the relevant part(s) of the dendrogram
object -- but in a form that makes it easy to then, say, set certain
attributes (perhaps recursively) for the target nodes (and perhaps
those contained in it).  I'm including a small example below that
hopefully illustrates this (it looks long, but it's mostly comments!).

Any help would be appreciated.

Jenny Bryan

## get ready for step-by-step figures
par(mfrow = c(2,2))

## get 5 objects, with 2-dimensional features
pts <- rbind(c(2,1.6),
              c(1.8,2.4),
              c(2.1, 2.7),
              c(5,2.6),
              c(4.7,3.1))
plot(pts, xlim = c(0,6), ylim = c(0,4),type = "n",
      xlab = "Feature 1", ylab = "Feature 2")
points(pts,pch = as.character(1:5))

## build a hierarhical tree, store as a dendrogram
aggTree <- hclust(dist(pts), method = "single")
(dend1 <- JB.as.dendrogram.hclust(aggTree))
## NOTE: only thing I added to official version of
## as.dendrogram.hclust:
## each node has an attribute cNum, which gives
## the merge step at which it was formed,
## i.e. gives the row of the merge object which
## describes the formation of that node
## one new line near end of nMerge loop:
## ***************
## *** 51,56 ****
## --- 51,60 ----
##   				     attr(z[[x[2]]], "midpoint"))/2
##   	}
##  	 attr(zk, "height") <- oHgt[k]
## +
## +         ## JB added July 6 2005
## +         attr(zk, "cNum") <- k
## +
##   	z[[k <- as.character(k)]] <- zk
##       }
##       z <- z[[k]]
attributes(dend1)
attributes(dend1[[1]])
## here's a table relating dend1 and the cNum attribute
## dend1               cNum
## -------------------------
## dend1                4
## dend1[[1]]           2
## dend1[[2]]           3
## dend1[[2]][[1]]  <not set>
## dend1[[2]][[1]]      1

## use cNum attribute in "edgetext"
## following example in dendrogram documentation
## would really rather associate with the node than the edge
## but current plotting function has no notion of nodetext
addE <- function(n) {
   if(!is.leaf(n)) {
     attr(n, "edgePar") <- list(p.col="plum")
     attr(n, "edgetext") <- attr(n,"cNum")
   }
   n
}
dend2 <- dendrapply(dend1, addE)
## overlays the cNum ("cluster number") attribute on dendrogram
plot(dend2, main = "dend2")
## why does no plum polygon appear around the '4' for the root
## edge?

## swap order of clusters 2 and 3,
## i.e. 'flip' cluster 4
dend3 <- dend2
dend3[[1]] <- dend2[[2]]
dend3[[2]] <- dend2[[1]]
plot(dend3, main = "dend3")
## wish I could achieve with 'dend3 <- flip(dend2, cNum = 4)

## swap order of cluster 1 and object 1,
## i.e. 'flip' cluster 3
dend4 <- dend2
dend4[[2]][[1]] <- dend2[[2]][[2]]
dend4[[2]][[2]] <- dend2[[2]][[1]]
plot(dend4, main = "dend4")
## wish I could achieve with 'dend4 <- flip(dend2, cNum = 3)

## finally, it's clear that the midpoint attribute would also
## need to be modified by 'flip'



From sundar.dorai-raj at pdf.com  Mon Jul 11 21:21:07 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 11 Jul 2005 14:21:07 -0500
Subject: [R] building packages on Windows
Message-ID: <42D2C6A3.7010409@pdf.com>

Hi, all,

I just recently upgraded my computer though I'm using the same OS (XP).
But now I'm having difficulty building packages and I cannot seem to
solve the problem. I'm using R-2.1.1pat on Windows XP.

Here is what I tried:

D:\Users\sundard\slib\sundar\R>R CMD CHECK sundar
* checking for working latex ... OK
* using log directory 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck'
* using R version 2.1.1, 2005-06-21
* checking for file 'sundar/DESCRIPTION' ... OK
* this is package 'sundar' version '1.1'
* checking if this is a source package ... OK

installing R.css in D:/Users/sundard/slib/sundar/R/sundar.Rcheck


---------- Making package sundar ------------
   adding build stamp to DESCRIPTION
   installing NAMESPACE file and metadata
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file
'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar/NAMESPACE'

Execution halted
make[2]: *** [nmspace] Error 1
make[1]: *** [all] Error 2
make: *** [pkg-sundar] Error 2
*** Installation of sundar failed ***

Removing 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar'
  ERROR
Installation failed.

I've also tried to remove the NAMESPACE which then passes `check' but
fails on `build' with the following:

* checking for file 'sundar/DESCRIPTION' ... OK
* preparing 'sundar':
* checking DESCRIPTION meta-information ... OK
* removing junk files
Error: cannot open file 'sundar/DESCRIPTION' for reading

If this is relevant, here is my PATH:
C:\WINDOWS\system32;
C:\WINDOWS;
.;
D:\R\rw2011pat\bin;
D:\R\tools\bin;
D:\Perl\bin;
D:\Tcl\bin;
D:\TeXLive\bin\win32;
D:\Mingw\bin;
C:\Program Files\Insightful\splus62\cmd;
C:\Program Files\HTML Help Workshop

Here, the D:\R\tools\bin directory contains the utilities for building R
from source, downloaded today. Am I missing something obvious?

Thanks,

--sundar



From murdoch at stats.uwo.ca  Mon Jul 11 21:38:19 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Jul 2005 15:38:19 -0400
Subject: [R] building packages on Windows
In-Reply-To: <42D2C6A3.7010409@pdf.com>
References: <42D2C6A3.7010409@pdf.com>
Message-ID: <42D2CAAB.7070403@stats.uwo.ca>

On 7/11/2005 3:21 PM, Sundar Dorai-Raj wrote:
> Hi, all,
> 
> I just recently upgraded my computer though I'm using the same OS (XP).
> But now I'm having difficulty building packages and I cannot seem to
> solve the problem. I'm using R-2.1.1pat on Windows XP.
> 
> Here is what I tried:
> 
> D:\Users\sundard\slib\sundar\R>R CMD CHECK sundar
> * checking for working latex ... OK
> * using log directory 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck'
> * using R version 2.1.1, 2005-06-21
> * checking for file 'sundar/DESCRIPTION' ... OK
> * this is package 'sundar' version '1.1'
> * checking if this is a source package ... OK
> 
> installing R.css in D:/Users/sundard/slib/sundar/R/sundar.Rcheck
> 
> 
> ---------- Making package sundar ------------
>    adding build stamp to DESCRIPTION
>    installing NAMESPACE file and metadata
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file
> 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar/NAMESPACE'
> 
> Execution halted
> make[2]: *** [nmspace] Error 1
> make[1]: *** [all] Error 2
> make: *** [pkg-sundar] Error 2
> *** Installation of sundar failed ***
> 
> Removing 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar'
>   ERROR
> Installation failed.
> 
> I've also tried to remove the NAMESPACE which then passes `check' but
> fails on `build' with the following:
> 
> * checking for file 'sundar/DESCRIPTION' ... OK
> * preparing 'sundar':
> * checking DESCRIPTION meta-information ... OK
> * removing junk files
> Error: cannot open file 'sundar/DESCRIPTION' for reading
> 
> If this is relevant, here is my PATH:
> C:\WINDOWS\system32;
> C:\WINDOWS;
> .;
> D:\R\rw2011pat\bin;
> D:\R\tools\bin;
> D:\Perl\bin;
> D:\Tcl\bin;
> D:\TeXLive\bin\win32;
> D:\Mingw\bin;
> C:\Program Files\Insightful\splus62\cmd;
> C:\Program Files\HTML Help Workshop
> 
> Here, the D:\R\tools\bin directory contains the utilities for building R
> from source, downloaded today. Am I missing something obvious?

Yes, you need to put the Windows directories later in your path.  There 
are some programs in the toolset which have like-named commands in 
Windows; you need to use ours, not theirs.

There are other differences between your path and the one recommended in 
the Admin and Installation manual; they may also be causing problems.

Duncan Murdoch



From sundar.dorai-raj at pdf.com  Mon Jul 11 22:06:26 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 11 Jul 2005 15:06:26 -0500
Subject: [R] building packages on Windows
In-Reply-To: <42D2CAAB.7070403@stats.uwo.ca>
References: <42D2C6A3.7010409@pdf.com> <42D2CAAB.7070403@stats.uwo.ca>
Message-ID: <42D2D142.4010901@pdf.com>



Duncan Murdoch wrote:
> On 7/11/2005 3:21 PM, Sundar Dorai-Raj wrote:
> 
>> Hi, all,
>>
>> I just recently upgraded my computer though I'm using the same OS (XP).
>> But now I'm having difficulty building packages and I cannot seem to
>> solve the problem. I'm using R-2.1.1pat on Windows XP.
>>
>> Here is what I tried:
>>
>> D:\Users\sundard\slib\sundar\R>R CMD CHECK sundar
>> * checking for working latex ... OK
>> * using log directory 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck'
>> * using R version 2.1.1, 2005-06-21
>> * checking for file 'sundar/DESCRIPTION' ... OK
>> * this is package 'sundar' version '1.1'
>> * checking if this is a source package ... OK
>>
>> installing R.css in D:/Users/sundard/slib/sundar/R/sundar.Rcheck
>>
>>
>> ---------- Making package sundar ------------
>>    adding build stamp to DESCRIPTION
>>    installing NAMESPACE file and metadata
>> Error in file(file, "r") : unable to open connection
>> In addition: Warning message:
>> cannot open file
>> 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar/NAMESPACE'
>>
>> Execution halted
>> make[2]: *** [nmspace] Error 1
>> make[1]: *** [all] Error 2
>> make: *** [pkg-sundar] Error 2
>> *** Installation of sundar failed ***
>>
>> Removing 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar'
>>   ERROR
>> Installation failed.
>>
>> I've also tried to remove the NAMESPACE which then passes `check' but
>> fails on `build' with the following:
>>
>> * checking for file 'sundar/DESCRIPTION' ... OK
>> * preparing 'sundar':
>> * checking DESCRIPTION meta-information ... OK
>> * removing junk files
>> Error: cannot open file 'sundar/DESCRIPTION' for reading
>>
>> If this is relevant, here is my PATH:
>> C:\WINDOWS\system32;
>> C:\WINDOWS;
>> .;
>> D:\R\rw2011pat\bin;
>> D:\R\tools\bin;
>> D:\Perl\bin;
>> D:\Tcl\bin;
>> D:\TeXLive\bin\win32;
>> D:\Mingw\bin;
>> C:\Program Files\Insightful\splus62\cmd;
>> C:\Program Files\HTML Help Workshop
>>
>> Here, the D:\R\tools\bin directory contains the utilities for building R
>> from source, downloaded today. Am I missing something obvious?
> 
> 
> Yes, you need to put the Windows directories later in your path.  There 
> are some programs in the toolset which have like-named commands in 
> Windows; you need to use ours, not theirs.
> 
> There are other differences between your path and the one recommended in 
> the Admin and Installation manual; they may also be causing problems.
> 
> Duncan Murdoch


Hi, Duncan,

Thanks for the reply. I moved things around so that my PATH now looks like:

.;
D:\R\tools\bin;
D:\Perl\bin;
D:\Mingw\bin;
D:\TeXLive\bin\win32;
C:\Program Files\HTML Help Workshop;
D:\R\rw2011pat\bin;
D:\Tcl\bin;
C:\Program Files\Insightful\splus62\cmd;
C:\WINDOWS;
C:\WINDOWS\system32

However, the problem was actually to do with permissions. Apparently 
when I when I do the following:

chmod 666 DESCRIPTION NAMESPACE

The permissions must have been changed when upgrading my computer since 
I didn't have any problem on my previous machine. However, now another 
problem has arisen:

D:\Users\sundard\slib\sundar\R>R CMD install sundar


---------- Making package sundar ------------
   adding build stamp to DESCRIPTION
   installing NAMESPACE file and metadata

<output snipped>

Compiling d:\Users\sundard\slib\sundar\R\sundar\chm\sundar.chm

HHC5003: Error: Compilation failed while compiling logo.jpg.
HHC5003: Error: Compilation failed while compiling Rchm.css.


The following files were not compiled:
logo.jpg
Rchm.css
   adding MD5 sums

* DONE (sundar)

This still installs fine except the chmhelp file is not created as it 
should be. I've deleted the chm subdirectory and tried to reinstall and 
I get the same error. This seems not to be an R problem, but possibly 
some configuration I'm not aware of, but I'd be grateful for any insight.

Thanks,

--sundar



From ggrothendieck at gmail.com  Mon Jul 11 22:06:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Jul 2005 16:06:54 -0400
Subject: [R] building packages on Windows
In-Reply-To: <42D2CAAB.7070403@stats.uwo.ca>
References: <42D2C6A3.7010409@pdf.com> <42D2CAAB.7070403@stats.uwo.ca>
Message-ID: <971536df05071113064930b464@mail.gmail.com>

On 7/11/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/11/2005 3:21 PM, Sundar Dorai-Raj wrote:
> > Hi, all,
> >
> > I just recently upgraded my computer though I'm using the same OS (XP).
> > But now I'm having difficulty building packages and I cannot seem to
> > solve the problem. I'm using R-2.1.1pat on Windows XP.
> >
> > Here is what I tried:
> >
> > D:\Users\sundard\slib\sundar\R>R CMD CHECK sundar
> > * checking for working latex ... OK
> > * using log directory 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck'
> > * using R version 2.1.1, 2005-06-21
> > * checking for file 'sundar/DESCRIPTION' ... OK
> > * this is package 'sundar' version '1.1'
> > * checking if this is a source package ... OK
> >
> > installing R.css in D:/Users/sundard/slib/sundar/R/sundar.Rcheck
> >
> >
> > ---------- Making package sundar ------------
> >    adding build stamp to DESCRIPTION
> >    installing NAMESPACE file and metadata
> > Error in file(file, "r") : unable to open connection
> > In addition: Warning message:
> > cannot open file
> > 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar/NAMESPACE'
> >
> > Execution halted
> > make[2]: *** [nmspace] Error 1
> > make[1]: *** [all] Error 2
> > make: *** [pkg-sundar] Error 2
> > *** Installation of sundar failed ***
> >
> > Removing 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar'
> >   ERROR
> > Installation failed.
> >
> > I've also tried to remove the NAMESPACE which then passes `check' but
> > fails on `build' with the following:
> >
> > * checking for file 'sundar/DESCRIPTION' ... OK
> > * preparing 'sundar':
> > * checking DESCRIPTION meta-information ... OK
> > * removing junk files
> > Error: cannot open file 'sundar/DESCRIPTION' for reading
> >
> > If this is relevant, here is my PATH:
> > C:\WINDOWS\system32;
> > C:\WINDOWS;
> > .;
> > D:\R\rw2011pat\bin;
> > D:\R\tools\bin;
> > D:\Perl\bin;
> > D:\Tcl\bin;
> > D:\TeXLive\bin\win32;
> > D:\Mingw\bin;
> > C:\Program Files\Insightful\splus62\cmd;
> > C:\Program Files\HTML Help Workshop
> >
> > Here, the D:\R\tools\bin directory contains the utilities for building R
> > from source, downloaded today. Am I missing something obvious?
> 
> Yes, you need to put the Windows directories later in your path.  There
> are some programs in the toolset which have like-named commands in
> Windows; you need to use ours, not theirs.
> 
> There are other differences between your path and the one recommended in
> the Admin and Installation manual; they may also be causing problems.

In 
    http://cran.r-project.org/contrib/extra/batchfiles/ 

there is a batch file called Rfind.bat that searches your system and lists 
the paths of the various tools used by R.  It does not actually change 
any environment variables nor does it change any other aspect of your 
system -- its display only so its generally safe to run.

This might help you in configuration.



From ramasamy at cancer.org.uk  Mon Jul 11 22:23:44 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Jul 2005 21:23:44 +0100
Subject: [R] indexing into and modifying dendrograms
In-Reply-To: <5cecd1f1c042c733ca9c9bcec42f670a@stat.ubc.ca>
References: <5cecd1f1c042c733ca9c9bcec42f670a@stat.ubc.ca>
Message-ID: <1121113424.5944.135.camel@ipc143004.lif.icnet.uk>

Probably not answering your questions here, but have you considered the
functions prune.tree and snip.tree from package tree or prune.rpart and
snip.rpart from the package rpart ?



On Mon, 2005-07-11 at 11:48 -0700, Jenny Bryan wrote:
> I would like to be able to exert certain types of control over the
> plotting of dendrograms (representing hierarchical clusterings) that I
> think is best achieved by modifying the dendrogram object
> prior to plotting.  I am using the "dendrogram" class and associated
> methods.
> 
> Define the cluster number of each cluster formed as the corresponding
> row of the merge object.  So, if you are clustering m objects, the
> cluster numbers range from 1 to m-1, with cluster m-1 containing all m
> objects, by definition.  I basically want a way to index into an
> object of class dendrogram using the above definition of cluster
> number and/or to act on a dendrogram, where I specify the target node
> using cluster number.
> 
> The first application would be to 'flip' the two elements in target
> node of the dendrogram (made clear in the small example below). (The
> setting is genomics and I have applications where I want to man-handle
> my dendrograms to make certain features of the clustering more obvious
> to the naked eye.)  I could imagine other, related actions that would
> be useful in decorating dendrograms.
> 
> I think I need a function that takes a dendrogram and cluster
> number(s) as input and returns the relevant part(s) of the dendrogram
> object -- but in a form that makes it easy to then, say, set certain
> attributes (perhaps recursively) for the target nodes (and perhaps
> those contained in it).  I'm including a small example below that
> hopefully illustrates this (it looks long, but it's mostly comments!).
> 
> Any help would be appreciated.
> 
> Jenny Bryan
> 
> ## get ready for step-by-step figures
> par(mfrow = c(2,2))
> 
> ## get 5 objects, with 2-dimensional features
> pts <- rbind(c(2,1.6),
>               c(1.8,2.4),
>               c(2.1, 2.7),
>               c(5,2.6),
>               c(4.7,3.1))
> plot(pts, xlim = c(0,6), ylim = c(0,4),type = "n",
>       xlab = "Feature 1", ylab = "Feature 2")
> points(pts,pch = as.character(1:5))
> 
> ## build a hierarhical tree, store as a dendrogram
> aggTree <- hclust(dist(pts), method = "single")
> (dend1 <- JB.as.dendrogram.hclust(aggTree))
> ## NOTE: only thing I added to official version of
> ## as.dendrogram.hclust:
> ## each node has an attribute cNum, which gives
> ## the merge step at which it was formed,
> ## i.e. gives the row of the merge object which
> ## describes the formation of that node
> ## one new line near end of nMerge loop:
> ## ***************
> ## *** 51,56 ****
> ## --- 51,60 ----
> ##   				     attr(z[[x[2]]], "midpoint"))/2
> ##   	}
> ##  	 attr(zk, "height") <- oHgt[k]
> ## +
> ## +         ## JB added July 6 2005
> ## +         attr(zk, "cNum") <- k
> ## +
> ##   	z[[k <- as.character(k)]] <- zk
> ##       }
> ##       z <- z[[k]]
> attributes(dend1)
> attributes(dend1[[1]])
> ## here's a table relating dend1 and the cNum attribute
> ## dend1               cNum
> ## -------------------------
> ## dend1                4
> ## dend1[[1]]           2
> ## dend1[[2]]           3
> ## dend1[[2]][[1]]  <not set>
> ## dend1[[2]][[1]]      1
> 
> ## use cNum attribute in "edgetext"
> ## following example in dendrogram documentation
> ## would really rather associate with the node than the edge
> ## but current plotting function has no notion of nodetext
> addE <- function(n) {
>    if(!is.leaf(n)) {
>      attr(n, "edgePar") <- list(p.col="plum")
>      attr(n, "edgetext") <- attr(n,"cNum")
>    }
>    n
> }
> dend2 <- dendrapply(dend1, addE)
> ## overlays the cNum ("cluster number") attribute on dendrogram
> plot(dend2, main = "dend2")
> ## why does no plum polygon appear around the '4' for the root
> ## edge?
> 
> ## swap order of clusters 2 and 3,
> ## i.e. 'flip' cluster 4
> dend3 <- dend2
> dend3[[1]] <- dend2[[2]]
> dend3[[2]] <- dend2[[1]]
> plot(dend3, main = "dend3")
> ## wish I could achieve with 'dend3 <- flip(dend2, cNum = 4)
> 
> ## swap order of cluster 1 and object 1,
> ## i.e. 'flip' cluster 3
> dend4 <- dend2
> dend4[[2]][[1]] <- dend2[[2]][[2]]
> dend4[[2]][[2]] <- dend2[[2]][[1]]
> plot(dend4, main = "dend4")
> ## wish I could achieve with 'dend4 <- flip(dend2, cNum = 3)
> 
> ## finally, it's clear that the midpoint attribute would also
> ## need to be modified by 'flip'
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From S.O.Nyangoma at amc.uva.nl  Mon Jul 11 22:37:06 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Mon, 11 Jul 2005 22:37:06 +0200
Subject: [R] exact values for p-values - more information.
Message-ID: <e89014e86fa5.e86fa5e89014@amc.uva.nl>

Hi there,
Actually my aim was to compare anumber of extreme values (e.g. 39540) 
with df1=1, df2=7025 via p-values.

Spencer mentions that 

"However, I have also used numbers like 
exp(-19775.52) to guestimate relative degrees of plausibility for 
different alternatives."

Can someone point to me an article using this method?

Regards. Stephen.

 

----- Original Message -----
From: Spencer Graves <spencer.graves at pdf.com>
Date: Monday, July 11, 2005 7:39 pm
Subject: Re: [R] exact values for p-values - more information.

>          I just checked:
> 
> > pf(39540, 1, 7025, lower.tail=FALSE, log.p=TRUE)
> [1] -Inf
> 
>          This is not correct.  With 7025 denominator degrees of 
> freedom, we 
> might use the chi-square approximation to the F distribution:
> 	
> > pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)
> [1] -19775.52
> 
>          In sum, my best approximation to  pf(39540, 1, 7025, 
> lower.tail=FALSE, log.p=TRUE), given only a minute to work on 
> this, is 
> exp(pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)) = exp(-19775.52).
> 
>          I'm confident that many violations of assumptions would 
> likely be 
> more important than the differences between "p-value: < 2.2e-16" 
> and 
 That doesn't mean they are right, only 
> the best 
> I can get with the available resources.
> 
>          spencer graves
> 
> Achim Zeileis wrote:
> 
> > On Mon, 11 Jul 2005, S.O. Nyangoma wrote:
> > 
> > 
> >> Hi there,
> >> If I do an lm, I get p-vlues as
> >>
> >> p-value: < 2.2e-16
> >>
> >>This is obtained from F =39540 with df1 = 1, df2 = 7025.
> >>
> >> Suppose am interested in exact value such as
> >>
> >> p-value = 1.6e-16 (note = and not <)
> >>
> >> How do I go about it?
> > 
> > 
> > You can always extract the `exact' p-value from the "summary.lm" 
> object or
> > you can compute it by hand via
> >   pf(39540, df1 = 1, df2 = 7025, lower.tail = FALSE)
> > For all practical purposes, the above means that the p-value is 0.
> > I guess you are on a 32-bit machine, then it also means that the 
> p-value
> > is smaller than the Machine epsilon
> >   .Machine$double.eps
> > 
> > So if you want to report the p-value somewhere, I think R's 
> output should
> > be more than precise enough. If you want to compute some other 
> values that
> > depend on such a p-value, then it is probably wiser to compute 
> on a log
> > scale, i.e. instead
> >   pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE)
> > use
> >   pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE, log.p = TRUE)
> > 
> > However, don't expect to be able to evaluate it at such extreme 
> values> such as 39540.
> > Z
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-
> project.org/posting-guide.html
> 
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ripley at stats.ox.ac.uk  Mon Jul 11 23:41:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Jul 2005 22:41:03 +0100 (BST)
Subject: [R] Dependence of bundle dse on setRNG (was Misbehaviour of DSE)
In-Reply-To: <20050711122113.EAF9415D3C5@lubyanka.local>
References: <20050711122113.EAF9415D3C5@lubyanka.local>
Message-ID: <Pine.LNX.4.61.0507112214360.8255@gannet.stats>

As the posting guide says, there is no R 2.1.  The first message suggests 
this is R 2.1.0, and the posting guide does ask you to use the latest 
version (and to quote versions accurately).

The dse bundle depends on package setRNG, which you have not installed,
so you need to do that.  Look at e.g.

> library(help=dse2)

which the posting guide actually asks you to do and include in your 
posting.  (And the message you got is pretty clearcut.)

(Suggestion to Paul Gilbert: it would work better to have the setRNG 
dependence in the top-level DESCRIPTION and not in the dependence for each 
package.  That way install.packages() can work out it is required from the 
information on CRAN, which is only at bundle level.)

On Mon, 11 Jul 2005, Ajay Shah wrote:

> I am finding problems with using "dse":
>
>> library(dse1)
> Loading required package: tframe
> Error: c("package '%s' required by '%s' could not be found", "setRNG", "dse1")
>> library(dse2)
> Loading required package: setRNG
> Error: package 'setRNG' could not be loaded
> In addition: Warning message:
> there is no package called 'setRNG' in: library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc)
>
> This is on R 2.1 on an Apple ibook (OS X) "panther".

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 11 23:51:36 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 11 Jul 2005 22:51:36 +0100 (BST)
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <17106.26579.427794.767713@stat.math.ethz.ch>
Message-ID: <XFMail.050711225136.Ted.Harding@nessie.mcc.ac.uk>

On 11-Jul-05 Martin Maechler wrote:
>>>>>> "AdaiR" == Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>>>>>>     on Mon, 11 Jul 2005 03:04:44 +0100 writes:
> 
>     AdaiR> Just an addendum on the philosophical aspect of doing
>     AdaiR> this.  By selecting the 5% and 95% quantiles, you are
>     AdaiR> always going to get 10% of the data as "extreme" and
>     AdaiR> these points may not necessarily outliers.  So when
>     AdaiR> you are comparing information from multiple columns
>     AdaiR> (i.e.  boxplots), it is harder to say which column
>     AdaiR> contains more extreme value compared to others etc.
> 
> Yes, indeed!
> 
> People {and software implementations} have several times provided
> differing definitions of how the boxplot whiskers should be defined.
> 
> I strongly believe that this is very often a very bad idea!!
> 
> A boxplot should be a universal mean communication and so one
> should be *VERY* reluctant redefining the outliers.
> 
> I just find that Matlab (in their statistics toolbox)
> does *NOT* use such a silly 5% / 95% definition of the whiskers,
> at least not according to their documentation.
> That's very good (and I wonder where you, Larry, got the idea of
> the 5 / 95 %).
> Using such a fixed percentage is really a very inferior idea to
> John Tukey's definition {the one in use in all implementations
> of S (including R) probably for close to 20 years now}.
> 
> I see one flaw in Tukey's definition {which is shared of course
> by any silly "percentage" based ``outlier'' definition}:
> 
>    The non-dependency on the sample size.
> 
> If you have a 1000 (or even many more) points,
> you'll get more and more `outliers' even for perfectly normal data.
> 
> But then, I assume John Tukey would have told us to do more
> sophisticated things {maybe things like the "violin plots"} than
> boxplot  if you have really very many data points, you may want
> to see more features -- or he would have agreed to use 
>    boxplot(*,  range = monotone_slowly_growing(n) )
> for largish sample sizes n.
> 
> Martin Maechler, ETH Zurich

I happily agree with Martin's essay on Boxplot philiosophy.

It would cerainly confuse boxplot watchers if the interpretation
of what they saw had to vary from case to case. The fact that
careful (and necessarily detailed) explanations of what was
different this time would be necessary in the text would not
help much, and would defeat the primary objective of the boxplot
which is to present a summary of features of the data in a form
which can be grasped visually very quickly indeed.

I'm sure many of us have at times felt some frustration at the
rigidly precise numerical interpretations which Tukey imposed
on the elements of his many EDA techniques; but this did ensure
that the viewer really knew, at a glance, what he was looking at.

EDA brilliantly combined several aspects of "looking at data":
selection of features of the data; highly efficient encoding of
these, and of their inter-relationships, into a medium directly
adapted to visual perception; robustness (so that the perceptions
were not unstable with respect to wondering just what the underlying
distribution might be); accessibility (in the sense of being truly
understood) to non-theoreticians; and capacity to be implemented on
primitive information technology.

Indeed, one might say that the "core team" of EDA consists of the
techniques for which you need only pencil and paper.

Nevertheless, Tukey was no rigid dogmatist. His objective was
always to give a good representation of the data, and he would
happily shift his ground, or adapt a technique (albeit probably
giving it a different name), or devise a new one, if that would
be useful for the case in hand.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 11-Jul-05                                       Time: 22:19:47
------------------------------ XFMail ------------------------------



From hans.gardfjell at emg.umu.se  Mon Jul 11 23:57:18 2005
From: hans.gardfjell at emg.umu.se (Hans Gardfjell)
Date: Mon, 11 Jul 2005 23:57:18 +0200
Subject: [R] R on kubuntu
Message-ID: <42D2EB3E.70601@emg.umu.se>

Dear R users,

It's possible to install newer versions of R from the Ubuntu depositories,
but the new version of R can only be found in 'breezy', the 'unstable'
version of Ubuntu. You can however create a mixed system with most of your packages
from the old 'hoary' and only some (like R) from the unstable 'breezy' by using the
pinning mechanism in apt-get. Check out the instructions on pinning in the
Ubuntu wiki: https://wiki.ubuntu.com/PinningHowto?highlight=%28apt-get+-t%29

In short it works like this:

1. Put in links to other depositories in /etc/apt/sources.list
2. Set the pinning priorities in /etc/apt/preferences
3. Install newer R-base with "apt-get -t breezy install r-base"

Cheers,

Hans Gardfjell
Dept. of Ecology and Environmental Science
Ume?? University, Sweden


 >Hello all,
 >
 >I am planning to redeploy my workstation under KUBUNTU.
 >
 >Does any body has any r experience installing/using r on this platform?
 >
 >Best regards.
 >
 >
 >--
 >Constant Depi??reux
 >Managing Director
 >Applied QUality Technologies Europe sprl
 >Rue des D??port??s 123, B-4800 Verviers
 >(Tel) +32 87 292175 - (Fax) +32 87 292171 - (Mobile) +32 475 555 818
 >(Web) http://www.aqte.be - (Courriel) constant.depiereux at aqte.be 
<https://stat.ethz.ch/mailman/listinfo/r-help>
 >(Skype) cdepiereux



From gunter.berton at gene.com  Tue Jul 12 00:18:23 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 11 Jul 2005 15:18:23 -0700
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <XFMail.050711225136.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200507112218.j6BMINfL009032@meitner.gene.com>

FWIW:

I have been an enthusiastic user of boxplots for decades. Of course, the
issue of how to handle the whiskers ("outliers"] is a valid one, and indeed
sample size related. Dogma is always dangerous. I got to know John Tukey
somewhat (I used to chauffer him to and from meetings with a group of Merck
statisticians), and I,too,think he would have been the first to agree that
some flexibility here is wise. 

HOWEVER, the chief advantage of boxplots is their simplicity at displaying
simultaneously and easily **several** important aspects of the data, of
which outliers are probably the most problematic (as they often result in
severe distortion of the plots without careful scaling). Even with dozens of
boxplots, center, scale, and skewness are easy to discern and compare. I
think this would NOT be true of "violin" plots and other more complex
versions -- simplicity can be a virtue.

Finally, a tidbit for boxplot afficianados: how does one detect bimodality
from a boxplot?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
> Sent: Monday, July 11, 2005 2:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Boxplot philosophy {was "Boxplot in R"}
> 
> On 11-Jul-05 Martin Maechler wrote:
> >>>>>> "AdaiR" == Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> >>>>>>     on Mon, 11 Jul 2005 03:04:44 +0100 writes:
> > 
> >     AdaiR> Just an addendum on the philosophical aspect of doing
> >     AdaiR> this.  By selecting the 5% and 95% quantiles, you are
> >     AdaiR> always going to get 10% of the data as "extreme" and
> >     AdaiR> these points may not necessarily outliers.  So when
> >     AdaiR> you are comparing information from multiple columns
> >     AdaiR> (i.e.  boxplots), it is harder to say which column
> >     AdaiR> contains more extreme value compared to others etc.
> > 
> > Yes, indeed!
> > 
> > People {and software implementations} have several times provided
> > differing definitions of how the boxplot whiskers should be defined.
> > 
> > I strongly believe that this is very often a very bad idea!!
> > 
> > A boxplot should be a universal mean communication and so one
> > should be *VERY* reluctant redefining the outliers.
> > 
> > I just find that Matlab (in their statistics toolbox)
> > does *NOT* use such a silly 5% / 95% definition of the whiskers,
> > at least not according to their documentation.
> > That's very good (and I wonder where you, Larry, got the idea of
> > the 5 / 95 %).
> > Using such a fixed percentage is really a very inferior idea to
> > John Tukey's definition {the one in use in all implementations
> > of S (including R) probably for close to 20 years now}.
> > 
> > I see one flaw in Tukey's definition {which is shared of course
> > by any silly "percentage" based ``outlier'' definition}:
> > 
> >    The non-dependency on the sample size.
> > 
> > If you have a 1000 (or even many more) points,
> > you'll get more and more `outliers' even for perfectly normal data.
> > 
> > But then, I assume John Tukey would have told us to do more
> > sophisticated things {maybe things like the "violin plots"} than
> > boxplot  if you have really very many data points, you may want
> > to see more features -- or he would have agreed to use 
> >    boxplot(*,  range = monotone_slowly_growing(n) )
> > for largish sample sizes n.
> > 
> > Martin Maechler, ETH Zurich
> 
> I happily agree with Martin's essay on Boxplot philiosophy.
> 
> It would cerainly confuse boxplot watchers if the interpretation
> of what they saw had to vary from case to case. The fact that
> careful (and necessarily detailed) explanations of what was
> different this time would be necessary in the text would not
> help much, and would defeat the primary objective of the boxplot
> which is to present a summary of features of the data in a form
> which can be grasped visually very quickly indeed.
> 
> I'm sure many of us have at times felt some frustration at the
> rigidly precise numerical interpretations which Tukey imposed
> on the elements of his many EDA techniques; but this did ensure
> that the viewer really knew, at a glance, what he was looking at.
> 
> EDA brilliantly combined several aspects of "looking at data":
> selection of features of the data; highly efficient encoding of
> these, and of their inter-relationships, into a medium directly
> adapted to visual perception; robustness (so that the perceptions
> were not unstable with respect to wondering just what the underlying
> distribution might be); accessibility (in the sense of being truly
> understood) to non-theoreticians; and capacity to be implemented on
> primitive information technology.
> 
> Indeed, one might say that the "core team" of EDA consists of the
> techniques for which you need only pencil and paper.
> 
> Nevertheless, Tukey was no rigid dogmatist. His objective was
> always to give a good representation of the data, and he would
> happily shift his ground, or adapt a technique (albeit probably
> giving it a different name), or devise a new one, if that would
> be useful for the case in hand.
> 
> Best wishes to all,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 11-Jul-05                                       Time: 22:19:47
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ForresterG at landcareresearch.co.nz  Tue Jul 12 00:40:44 2005
From: ForresterG at landcareresearch.co.nz (Guy Forrester)
Date: Tue, 12 Jul 2005 10:40:44 +1200
Subject: [R] CIs in predict?
Message-ID: <s2d39e35.090@smtp.landcareresearch.co.nz>

Dear All,


I am trying to put some Confidence intervals on some regressions from a linear model with no luck.  I can extract the fitted values using 'predict', but am having difficulty in getting at the confidence intervals, or the standard errors.

Any suggestions would be welcome

Cheers

Guy

Using Version 2.1.0  (2005-04-18) on a PC




vol.mod3 <- lm(log.volume~log.area*lake,data=vol)
summary(vol.mod3)

plot(c(1.3,2.5),c(-0.7,0.45),type="n",xlab="Log area",ylab="Log volume")

areapred.a <- seq(min(vol$log.area[vol$lake=="a"]), max(vol$log.area[vol$lake=="a"]), length=100)
areapred.b <- seq(min(vol$log.area[vol$lake=="b"]), max(vol$log.area[vol$lake=="b"]), length=100)


preda <- predict(vol.mod3, data.frame(log.area=areapred.a,interval="confidence" ,lake=rep("a",100)))

#This gives the fitted values as predicted, but no CIs
> preda
           1            2            3            4            5            6            7            8            9 
-0.562577529 -0.553263576 -0.543949624 -0.534635671 -0.525321718 -0.516007765 -0.506693813 -0.497379860 -0.488065907 
          10           11           12           13           14           15           16           17           18 
-0.478751955 -0.469438002 -0.460124049 -0.450810097 -0.441496144 -0.432182191 -0.422868239 -0.413554286 -0.404240333 
          19           20           21           22           23           24           25           26           27 
-0.394926380 -0.385612428 -0.376298475 ETC ETC

#As does this, but with no SEs
> preda <- predict(vol.mod3, data.frame(log.area=areapred.a,se.fit=T ,lake=rep("a",100)))
> preda
           1            2            3            4            5            6            7            8            9           10 
-0.562577529 -0.553263576 -0.543949624 -0.534635671 -0.525321718 -0.516007765 -0.506693813 -0.497379860 -0.488065907 -0.478751955 
          11           12           13           14           15           16           17           18           19           20 
-0.469438002 -0.460124049 -0.450810097 ETC ETC




--------------------------------------------------------
Guy J Forrester
Biometrician
Manaaki Whenua - Landcare Research
PO Box 69, Lincoln, New Zealand.
Tel. +64 3 325 6701 x3738
Fax +64 3 325 2418
E-mail ForresterG at LandcareResearch.co.nz 
www.LandcareResearch.co.nz 


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
WARNING: This email and any attachments may be confidential ...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Jul 12 01:34:44 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jul 2005 01:34:44 +0200
Subject: [R] CIs in predict?
In-Reply-To: <s2d39e35.090@smtp.landcareresearch.co.nz>
References: <s2d39e35.090@smtp.landcareresearch.co.nz>
Message-ID: <x2br58c3xn.fsf@turmalin.kubism.ku.dk>

"Guy Forrester" <ForresterG at landcareresearch.co.nz> writes:

> Dear All,
> 
> 
> I am trying to put some Confidence intervals on some regressions from a linear model with no luck.  I can extract the fitted values using 'predict', but am having difficulty in getting at the confidence intervals, or the standard errors.
> 
> Any suggestions would be welcome

help(predict.lm) should get you there soon enough.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From blomsp at ozemail.com.au  Tue Jul 12 01:47:21 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Tue, 12 Jul 2005 09:47:21 +1000
Subject: [R] CIs in predict?
In-Reply-To: <s2d39e35.090@smtp.landcareresearch.co.nz>
References: <s2d39e35.090@smtp.landcareresearch.co.nz>
Message-ID: <6.2.1.2.0.20050712093902.01be7c08@mail.ozemail.com.au>


At 08:40 AM 12/07/2005, Guy Forrester wrote:
>Dear All,
>
>
>I am trying to put some Confidence intervals on some regressions from a 
>linear model with no luck.  I can extract the fitted values using 
>'predict', but am having difficulty in getting at the confidence 
>intervals, or the standard errors.
>
>Any suggestions would be welcome
>
>Cheers
>
>Guy
>
>Using Version 2.1.0  (2005-04-18) on a PC
>
>
>
>
>vol.mod3 <- lm(log.volume~log.area*lake,data=vol)
>summary(vol.mod3)
>
>plot(c(1.3,2.5),c(-0.7,0.45),type="n",xlab="Log area",ylab="Log volume")
>
>areapred.a <- seq(min(vol$log.area[vol$lake=="a"]), 
>max(vol$log.area[vol$lake=="a"]), length=100)
>areapred.b <- seq(min(vol$log.area[vol$lake=="b"]), 
>max(vol$log.area[vol$lake=="b"]), length=100)
>
>
>preda <- predict(vol.mod3, 
>data.frame(log.area=areapred.a,interval="confidence" ,lake=rep("a",100)))

You have interval="confidence" inside your call to data.frame, not inside 
your call to predict. Hence you are creating a data frame with a variable 
called interval, with one level called confidence, and predict does not see 
interval="confidence" at all! See ?predict.lm.

HTH,

Simon.


>#This gives the fitted values as predicted, but no CIs
> > preda
>            1            2            3            4            5 
>    6            7            8            9
>-0.562577529 -0.553263576 -0.543949624 -0.534635671 -0.525321718 
>-0.516007765 -0.506693813 -0.497379860 -0.488065907
>           10           11           12           13           14 
>   15           16           17           18
>-0.478751955 -0.469438002 -0.460124049 -0.450810097 -0.441496144 
>-0.432182191 -0.422868239 -0.413554286 -0.404240333
>           19           20           21           22           23 
>   24           25           26           27
>-0.394926380 -0.385612428 -0.376298475 ETC ETC
>
>#As does this, but with no SEs
> > preda <- predict(vol.mod3, data.frame(log.area=areapred.a,se.fit=T 
> ,lake=rep("a",100)))
> > preda
>            1            2            3            4            5 
>    6            7            8            9           10
>-0.562577529 -0.553263576 -0.543949624 -0.534635671 -0.525321718 
>-0.516007765 -0.506693813 -0.497379860 -0.488065907 -0.478751955
>           11           12           13           14           15 
>   16           17           18           19           20
>-0.469438002 -0.460124049 -0.450810097 ETC ETC
>
>
>
>
>--------------------------------------------------------
>Guy J Forrester
>Biometrician
>Manaaki Whenua - Landcare Research
>PO Box 69, Lincoln, New Zealand.
>Tel. +64 3 325 6701 x3738
>Fax +64 3 325 2418
>E-mail ForresterG at LandcareResearch.co.nz
>www.LandcareResearch.co.nz
>
>
>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>WARNING: This email and any attachments may be confidential ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From w.northcott at unsw.edu.au  Tue Jul 12 01:48:16 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Tue, 12 Jul 2005 09:48:16 +1000
Subject: [R] Problems with R on OS X
In-Reply-To: <mailman.7.1121076001.14303.r-help@stat.math.ethz.ch>
References: <mailman.7.1121076001.14303.r-help@stat.math.ethz.ch>
Message-ID: <19AE1084-FE53-4DBC-AC9C-AD7B30C60F19@unsw.edu.au>

On 11/07/2005, at 8:00 PM, Heinz Schild wrote:
> I used R on OS X 10.3x quite some time with no serious problems.
> Sometimes R stopped when I tried to execute a bigger program. After
> updating to OS X to version 10.4 R worked but I still had the problem
> with bigger programs. Therefore I re-installed R on top of the
> existing R version. The installation finished properly but suddenly R
> did not work. Then I reinstalled OS X 10.4 because I thought some
> left over registry information from R may caused the problem. R still
> crashed. I hoped the problem would be overcome with the new R version
> 1.12. Unfortunately this is not the case. The error report (see
> appendix) says that Thread 0 crashed.
> What can I do?
> Heinz Schild

The best place for MacOS issues is the r-sig-mac list to which I have  
copied this mesage.

MacOS X does not have a registry!  Configuration files for MacOS  
applications live in ~/Library/Preferences.  The file for the R GUI  
app is called org.R-project.R.plist.  The rest of R on a Mac is just  
like any other UNIX platform with settings and history in ~/.Rdata  
and ~/.Rhistory.  (Files starting with a period '.' are invisible in  
the MacOS desktop.)  If you want to trash the configuration just  
delete these three files.

The version 1.12 you quote is only for the GUI, which is just a front  
end for the R framework.  The current version of the framework binary  
is 2.1.1, which you will see on the 'R Console' window when you start  
the application.  If you try the 1.12 GUI with a version of the  
framework other than 2.1.1 it is very likely to crash.  If you  
install the complete package from CRAN it will install both the GUI  
application and the framework.

BIll Northcott



From villegas.ro at gmail.com  Tue Jul 12 02:01:44 2005
From: villegas.ro at gmail.com (R V)
Date: Tue, 12 Jul 2005 02:01:44 +0200
Subject: [R] nlme plot
Message-ID: <29cf683505071117014b4c2e11@mail.gmail.com>

Hello,

I am running this script from Pinheiro & Bates book in R Version 2.1.1 (WinXP).
But, I can't plot Figure 2.3.
What's wrong?


TIA.
Rod.

---------------------------------------------------------
>library(nlme)
> names( Orthodont )
[1] "distance" "age"      "Subject"  "Sex"     
> levels( Orthodont$Sex )
[1] "Male"   "Female"
> OrthoFem <- Orthodont[ Orthodont$Sex == "Female", ]
> 
> fm1OrthF <- lme( distance ~ age, data = OrthoFem, random = ~ 1 | Subject )
> fm2OrthF <- update( fm1OrthF, random = ~ age | Subject )
> orthLRTsim <- simulate.lme( fm1OrthF, fm2OrthF, nsim = 1000 )
> plot( orthLRTsim, df = c(1, 2) )    # produces Figure 2.3
Error in if ((dfType <- as.double(names(x)[1])) == 1) { : 
	argument is of length zero
Execution halted



From villegas.ro at gmail.com  Tue Jul 12 02:19:30 2005
From: villegas.ro at gmail.com (R V)
Date: Tue, 12 Jul 2005 02:19:30 +0200
Subject: [R]  simulate.lme plot
Message-ID: <29cf6835050711171923c1d1df@mail.gmail.com>

Hello,
I am running this script from Pinheiro & Bates book in R (Version 2.1.1,WinXP).
But, I can't plot Figure 2.3.
What's wrong?
Thanks, Rod.

---------------------------------------------------------
>library(nlme)
> names( Orthodont )
[1] "distance" "age"      "Subject"  "Sex"
> levels( Orthodont$Sex )
[1] "Male"   "Female"
> OrthoFem <- Orthodont[ Orthodont$Sex == "Female", ]
>
> fm1OrthF <- lme( distance ~ age, data = OrthoFem, random = ~ 1 | Subject )
> fm2OrthF <- update( fm1OrthF, random = ~ age | Subject )
> orthLRTsim <- simulate.lme( fm1OrthF, fm2OrthF, nsim = 1000 )
> plot( orthLRTsim, df = c(1, 2) )    # produces Figure 2.3
Error in if ((dfType <- as.double(names(x)[1])) == 1) { :
       argument is of length zero
Execution halted



From dimitrijoe at yahoo.com.br  Tue Jul 12 03:01:28 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Mon, 11 Jul 2005 22:01:28 -0300
Subject: [R] transition matrix and discretized data
Message-ID: <000d01c5867d$3d300b80$1600a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050711/ce8fd159/attachment.pl

From dmbates at gmail.com  Tue Jul 12 04:05:04 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 11 Jul 2005 21:05:04 -0500
Subject: [R] nlme plot
In-Reply-To: <29cf683505071117014b4c2e11@mail.gmail.com>
References: <29cf683505071117014b4c2e11@mail.gmail.com>
Message-ID: <40e66e0b050711190545645517@mail.gmail.com>

On 7/11/05, R V <villegas.ro at gmail.com> wrote:
> Hello,
> 
> I am running this script from Pinheiro & Bates book in R Version 2.1.1 (WinXP).
> But, I can't plot Figure 2.3.
> What's wrong?

There was a change in the way that R handles assignments of names of
components and that affected the construction of this plot. I've
rewritten the plot construction code (the logic in the current version
was bizarre) and will upload a new version of nlme after I test this
out.

By the way, there is a simpler way of reproducing the examples in our book.  Use

source(system.file("scripts/ch02.R", package = "nlme"))

but don't try that with the current version of the nlme package. 
There is another "infelicity" (to use Bill Venables' term) that I will
correct.


> 
> 
> TIA.
> Rod.
> 
> ---------------------------------------------------------
> >library(nlme)
> > names( Orthodont )
> [1] "distance" "age"      "Subject"  "Sex"
> > levels( Orthodont$Sex )
> [1] "Male"   "Female"
> > OrthoFem <- Orthodont[ Orthodont$Sex == "Female", ]
> >
> > fm1OrthF <- lme( distance ~ age, data = OrthoFem, random = ~ 1 | Subject )
> > fm2OrthF <- update( fm1OrthF, random = ~ age | Subject )
> > orthLRTsim <- simulate.lme( fm1OrthF, fm2OrthF, nsim = 1000 )
> > plot( orthLRTsim, df = c(1, 2) )    # produces Figure 2.3
> Error in if ((dfType <- as.double(names(x)[1])) == 1) { :
>         argument is of length zero
> Execution halted
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Tue Jul 12 04:07:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 11 Jul 2005 19:07:35 -0700
Subject: [R] exact values for p-values - more information.
In-Reply-To: <e89014e86fa5.e86fa5e89014@amc.uva.nl>
References: <e89014e86fa5.e86fa5e89014@amc.uva.nl>
Message-ID: <42D325E7.8080403@pdf.com>

	  I don't have a reference, but you could look under data mining and p 
values / Bonferroni.

	  spencer graves

S.O. Nyangoma wrote:

> Hi there,
> Actually my aim was to compare anumber of extreme values (e.g. 39540) 
> with df1=1, df2=7025 via p-values.
> 
> Spencer mentions that 
> 
> "However, I have also used numbers like 
> exp(-19775.52) to guestimate relative degrees of plausibility for 
> different alternatives."
> 
> Can someone point to me an article using this method?
> 
> Regards. Stephen.
> 
>  
> 
> ----- Original Message -----
> From: Spencer Graves <spencer.graves at pdf.com>
> Date: Monday, July 11, 2005 7:39 pm
> Subject: Re: [R] exact values for p-values - more information.
> 
> 
>>         I just checked:
>>
>>
>>>pf(39540, 1, 7025, lower.tail=FALSE, log.p=TRUE)
>>
>>[1] -Inf
>>
>>         This is not correct.  With 7025 denominator degrees of 
>>freedom, we 
>>might use the chi-square approximation to the F distribution:
>>	
>>
>>>pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)
>>
>>[1] -19775.52
>>
>>         In sum, my best approximation to  pf(39540, 1, 7025, 
>>lower.tail=FALSE, log.p=TRUE), given only a minute to work on 
>>this, is 
>>exp(pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)) = exp(-19775.52).
>>
>>         I'm confident that many violations of assumptions would 
>>likely be 
>>more important than the differences between "p-value: < 2.2e-16" 
>>and 
> 
>  That doesn't mean they are right, only 
> 
>>the best 
>>I can get with the available resources.
>>
>>         spencer graves
>>
>>Achim Zeileis wrote:
>>
>>
>>>On Mon, 11 Jul 2005, S.O. Nyangoma wrote:
>>>
>>>
>>>
>>>>Hi there,
>>>>If I do an lm, I get p-vlues as
>>>>
>>>>p-value: < 2.2e-16
>>>>
>>>>This is obtained from F =39540 with df1 = 1, df2 = 7025.
>>>>
>>>>Suppose am interested in exact value such as
>>>>
>>>>p-value = 1.6e-16 (note = and not <)
>>>>
>>>>How do I go about it?
>>>
>>>
>>>You can always extract the `exact' p-value from the "summary.lm" 
>>
>>object or
>>
>>>you can compute it by hand via
>>>  pf(39540, df1 = 1, df2 = 7025, lower.tail = FALSE)
>>>For all practical purposes, the above means that the p-value is 0.
>>>I guess you are on a 32-bit machine, then it also means that the 
>>
>>p-value
>>
>>>is smaller than the Machine epsilon
>>>  .Machine$double.eps
>>>
>>>So if you want to report the p-value somewhere, I think R's 
>>
>>output should
>>
>>>be more than precise enough. If you want to compute some other 
>>
>>values that
>>
>>>depend on such a p-value, then it is probably wiser to compute 
>>
>>on a log
>>
>>>scale, i.e. instead
>>>  pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE)
>>>use
>>>  pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE, log.p = TRUE)
>>>
>>>However, don't expect to be able to evaluate it at such extreme 
>>
>>values> such as 39540.
>>
>>>Z
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-
>>
>>project.org/posting-guide.html
>>
>>-- 
>>Spencer Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com>
>>Tel:  408-938-4420
>>Fax: 408-280-7915
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-
>>guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Tue Jul 12 04:08:39 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 11 Jul 2005 19:08:39 -0700
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <200507112218.j6BMINfL009032@meitner.gene.com>
References: <200507112218.j6BMINfL009032@meitner.gene.com>
Message-ID: <42D32627.4070407@pdf.com>

	  I'll bite:  How does one detect bimodalidty from a boxplot?

	  spencer graves

Berton Gunter wrote:

> FWIW:
> 
> I have been an enthusiastic user of boxplots for decades. Of course, the
> issue of how to handle the whiskers ("outliers"] is a valid one, and indeed
> sample size related. Dogma is always dangerous. I got to know John Tukey
> somewhat (I used to chauffer him to and from meetings with a group of Merck
> statisticians), and I,too,think he would have been the first to agree that
> some flexibility here is wise. 
> 
> HOWEVER, the chief advantage of boxplots is their simplicity at displaying
> simultaneously and easily **several** important aspects of the data, of
> which outliers are probably the most problematic (as they often result in
> severe distortion of the plots without careful scaling). Even with dozens of
> boxplots, center, scale, and skewness are easy to discern and compare. I
> think this would NOT be true of "violin" plots and other more complex
> versions -- simplicity can be a virtue.
> 
> Finally, a tidbit for boxplot afficianados: how does one detect bimodality
> from a boxplot?
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
>>Sent: Monday, July 11, 2005 2:52 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Boxplot philosophy {was "Boxplot in R"}
>>
>>On 11-Jul-05 Martin Maechler wrote:
>>
>>>>>>>>"AdaiR" == Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>>>>>>>>    on Mon, 11 Jul 2005 03:04:44 +0100 writes:
>>>
>>>    AdaiR> Just an addendum on the philosophical aspect of doing
>>>    AdaiR> this.  By selecting the 5% and 95% quantiles, you are
>>>    AdaiR> always going to get 10% of the data as "extreme" and
>>>    AdaiR> these points may not necessarily outliers.  So when
>>>    AdaiR> you are comparing information from multiple columns
>>>    AdaiR> (i.e.  boxplots), it is harder to say which column
>>>    AdaiR> contains more extreme value compared to others etc.
>>>
>>>Yes, indeed!
>>>
>>>People {and software implementations} have several times provided
>>>differing definitions of how the boxplot whiskers should be defined.
>>>
>>>I strongly believe that this is very often a very bad idea!!
>>>
>>>A boxplot should be a universal mean communication and so one
>>>should be *VERY* reluctant redefining the outliers.
>>>
>>>I just find that Matlab (in their statistics toolbox)
>>>does *NOT* use such a silly 5% / 95% definition of the whiskers,
>>>at least not according to their documentation.
>>>That's very good (and I wonder where you, Larry, got the idea of
>>>the 5 / 95 %).
>>>Using such a fixed percentage is really a very inferior idea to
>>>John Tukey's definition {the one in use in all implementations
>>>of S (including R) probably for close to 20 years now}.
>>>
>>>I see one flaw in Tukey's definition {which is shared of course
>>>by any silly "percentage" based ``outlier'' definition}:
>>>
>>>   The non-dependency on the sample size.
>>>
>>>If you have a 1000 (or even many more) points,
>>>you'll get more and more `outliers' even for perfectly normal data.
>>>
>>>But then, I assume John Tukey would have told us to do more
>>>sophisticated things {maybe things like the "violin plots"} than
>>>boxplot  if you have really very many data points, you may want
>>>to see more features -- or he would have agreed to use 
>>>   boxplot(*,  range = monotone_slowly_growing(n) )
>>>for largish sample sizes n.
>>>
>>>Martin Maechler, ETH Zurich
>>
>>I happily agree with Martin's essay on Boxplot philiosophy.
>>
>>It would cerainly confuse boxplot watchers if the interpretation
>>of what they saw had to vary from case to case. The fact that
>>careful (and necessarily detailed) explanations of what was
>>different this time would be necessary in the text would not
>>help much, and would defeat the primary objective of the boxplot
>>which is to present a summary of features of the data in a form
>>which can be grasped visually very quickly indeed.
>>
>>I'm sure many of us have at times felt some frustration at the
>>rigidly precise numerical interpretations which Tukey imposed
>>on the elements of his many EDA techniques; but this did ensure
>>that the viewer really knew, at a glance, what he was looking at.
>>
>>EDA brilliantly combined several aspects of "looking at data":
>>selection of features of the data; highly efficient encoding of
>>these, and of their inter-relationships, into a medium directly
>>adapted to visual perception; robustness (so that the perceptions
>>were not unstable with respect to wondering just what the underlying
>>distribution might be); accessibility (in the sense of being truly
>>understood) to non-theoreticians; and capacity to be implemented on
>>primitive information technology.
>>
>>Indeed, one might say that the "core team" of EDA consists of the
>>techniques for which you need only pencil and paper.
>>
>>Nevertheless, Tukey was no rigid dogmatist. His objective was
>>always to give a good representation of the data, and he would
>>happily shift his ground, or adapt a technique (albeit probably
>>giving it a different name), or devise a new one, if that would
>>be useful for the case in hand.
>>
>>Best wishes to all,
>>Ted.
>>
>>
>>--------------------------------------------------------------------
>>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>>Fax-to-email: +44 (0)870 094 0861
>>Date: 11-Jul-05                                       Time: 22:19:47
>>------------------------------ XFMail ------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jyzz88 at gmail.com  Tue Jul 12 04:48:19 2005
From: jyzz88 at gmail.com (Luke)
Date: Mon, 11 Jul 2005 22:48:19 -0400
Subject: [R] R CMD INSTALL use differenct c++ compiler
Message-ID: <27583b40050711194835257bfd@mail.gmail.com>

Dear R Users,

When I installed e1071 use R CMD INSTALL, I got

configure: WARNING: g++ 2.96 cannot reliably be used with this package.
configure: error: Please use a different C++ compiler.

But how to let R CMD INSTALL use a different C++ compiler? and which
C++ compiler is good?

-Luke



From MessageRelease at massey.ac.nz  Tue Jul 12 06:34:42 2005
From: MessageRelease at massey.ac.nz (MessageRelease@massey.ac.nz)
Date: Tue, 12 Jul 2005 16:34:42 +1200
Subject: [R] ---- Message Quarantined! ----
Message-ID: <D42d348620001@TUR-MM3.massey.ac.nz>

MailMarshal has quarantined the following message you sent:

   Message: B42d3485f0001.000000000001.0002.mml
   From:    r-help at lists.r-project.org
   To:      m.kirchberg at massey.ac.nz
   Subject: Good day


With the proliferation of mass mailer type viruses, Massey University has 
had to implement a policy that quarantines emails with attachments that could 
be potential viruses, e.g. VBS, SCR and EXE.

However, this message is eligible for Self-Service Message Release.  If this 
attachment is genuine and you are certain that it does not contain any viruses, 
please simply reply to this message without editing it.

The quarantined email will be automatically released to m.kirchberg at massey.ac.nz 
upon receipt of your reply, else it will be deleted after 14 days.

Please note that your reply will be processed by the automated release system. 
If you have any queries or comments, please send them to Help.Desk at massey.ac.nz.

========================================================================
Message release code: MMRelCode
ZYoJxwg/WQoP+jKD7y9MEPkkJa7kC4lbnw7+GjUyOmMbCbdfbt
MkS6FoVQwZpNHhLosQRcgvZoUocQ+3XTWkjwMxfFQw0Ea0zz/z
VtE7ZIuQXVj8ZKBR0Q4YvUEN2Qa/6Q5POCRmm/qTc6d+Wg==
MMEndRelCode
========================================================================

From medp9193 at nus.edu.sg  Tue Jul 12 06:51:00 2005
From: medp9193 at nus.edu.sg (Tan Hui Hui Jenny)
Date: Tue, 12 Jul 2005 12:51:00 +0800
Subject: [R] How to obtain Frequency Tables for Histogram outputs and
	Frequency Polygons
Message-ID: <16CDECA355E2FD48A3C5A51E31A78E4D05E96B@MBOX23.stu.nus.edu.sg>

Couple of questions:
 
1. How can I obtain the frequency tables for a histogram chart?
2. Is there a short cut to obtain the frequency polygons directly without having to generate the frequency table and doing a line plot?
 
Thanks ina dvance for any reply.
 
j.
 
Example data:
0.3,0.229,0.218,0.211,0.207,0.21,0.212,0.232,0.312,0.289,0.486,0.475,0.274,0.478,0.284,0.477,0.281,0.29,0.29,0.32,0.28,0.404,0.489,0.406,0.267,0.456,0.443,0.457,0.411,0.444,0.401,0.468,0.309,0.425,0.312,0.3,0.464,0.457,0.442,0.469,0.423,0.403,0.432,0.379,0.361,0.37,0.47,0.374,0.387,0.381,0.311,0.257,0.381,0.162,0.229,0.221,0.25,0.164,0.368,0.367,0.332,0.165,0.152,0.478,0.489,0.482,0.167,0.164,0.164,0.15,0.149,0.368,0.366,0.361,0.359,0.359,0.364,0.364



From ggrothendieck at gmail.com  Tue Jul 12 07:12:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 01:12:11 -0400
Subject: [R] How to obtain Frequency Tables for Histogram outputs and
	Frequency Polygons
In-Reply-To: <16CDECA355E2FD48A3C5A51E31A78E4D05E96B@MBOX23.stu.nus.edu.sg>
References: <16CDECA355E2FD48A3C5A51E31A78E4D05E96B@MBOX23.stu.nus.edu.sg>
Message-ID: <971536df05071122126e8693d0@mail.gmail.com>

On 7/12/05, Tan Hui Hui Jenny <medp9193 at nus.edu.sg> wrote:
> Couple of questions:
> 
> 1. How can I obtain the frequency tables for a histogram chart?
> 2. Is there a short cut to obtain the frequency polygons directly without having to generate the frequency table and doing a line plot?

res <- hist(x)  # res contains various data about histogram

# create red polygons using rect
plot(range(res$breaks), range(res$counts), type="n")
rect(res$breaks[-length(res$breaks)],0,res$breaks[-1],res$counts, col="red")



From ligges at statistik.uni-dortmund.de  Tue Jul 12 08:25:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2005 08:25:45 +0200
Subject: [R] R CMD INSTALL use differenct c++ compiler
In-Reply-To: <27583b40050711194835257bfd@mail.gmail.com>
References: <27583b40050711194835257bfd@mail.gmail.com>
Message-ID: <42D36269.6080105@statistik.uni-dortmund.de>

Luke wrote:
> Dear R Users,
> 
> When I installed e1071 use R CMD INSTALL, I got
> 
> configure: WARNING: g++ 2.96 cannot reliably be used with this package.
> configure: error: Please use a different C++ compiler.
> 
> But how to let R CMD INSTALL use a different C++ compiler? and which
> C++ compiler is good?
> 

That one from GCC is perfect, but in a much more recent version ...

Uwe Ligges



> -Luke
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From admin at biostatistic.de  Tue Jul 12 08:31:53 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Tue, 12 Jul 2005 08:31:53 +0200
Subject: [R] High resolution plots
Message-ID: <42D363D9.2080703@biostatistic.de>

Is there any possibility to get high resolution plots in a windows xp 
system?
I tried it with the device function png(filename = 
"c:/r/highresplot%d.png",pointsize=12, res=900)
but when I try to set: width = 480, height = 480 or   pointsize = 12, 
the text is not scaled in the same way as the plots.

with regards
Knut Krueger
http://www.biostatistic.de



From ligges at statistik.uni-dortmund.de  Tue Jul 12 09:14:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2005 09:14:34 +0200
Subject: [R] High resolution plots
In-Reply-To: <42D363D9.2080703@biostatistic.de>
References: <42D363D9.2080703@biostatistic.de>
Message-ID: <42D36DDA.2030207@statistik.uni-dortmund.de>

Knut Krueger wrote:

> Is there any possibility to get high resolution plots in a windows xp 
> system?
> I tried it with the device function png(filename = 
> "c:/r/highresplot%d.png",pointsize=12, res=900)
> but when I try to set: width = 480, height = 480 or   pointsize = 12, 
> the text is not scaled in the same way as the plots.

If you really want high quality, why do you choose a bitmaped device 
rather than non-bitmapped devices such as ps, pdf or wmf?

Uwe Ligges

> with regards
> Knut Krueger
> http://www.biostatistic.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jul 12 09:21:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2005 09:21:24 +0200
Subject: [R] building packages on Windows
In-Reply-To: <42D2C6A3.7010409@pdf.com>
References: <42D2C6A3.7010409@pdf.com>
Message-ID: <42D36F74.50802@statistik.uni-dortmund.de>

Duncan,

I just see that the tar.exe in the tools.zip is still broken (causing 
wrong permissions in the Windows file system). This might be the fact 
why so many people are asking questins related to R's and its packages' 
make/build/install processes.

Duncan, can you please update the tools.zip with Brian's patched version 
of tar.exe (dated 2003-07-21 rather than 2002-05-26).

Thank you!

Best,
Uwe



Sundar Dorai-Raj wrote:

> Hi, all,
> 
> I just recently upgraded my computer though I'm using the same OS (XP).
> But now I'm having difficulty building packages and I cannot seem to
> solve the problem. I'm using R-2.1.1pat on Windows XP.
> 
> Here is what I tried:
> 
> D:\Users\sundard\slib\sundar\R>R CMD CHECK sundar
> * checking for working latex ... OK
> * using log directory 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck'
> * using R version 2.1.1, 2005-06-21
> * checking for file 'sundar/DESCRIPTION' ... OK
> * this is package 'sundar' version '1.1'
> * checking if this is a source package ... OK
> 
> installing R.css in D:/Users/sundard/slib/sundar/R/sundar.Rcheck
> 
> 
> ---------- Making package sundar ------------
>    adding build stamp to DESCRIPTION
>    installing NAMESPACE file and metadata
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file
> 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar/NAMESPACE'
> 
> Execution halted
> make[2]: *** [nmspace] Error 1
> make[1]: *** [all] Error 2
> make: *** [pkg-sundar] Error 2
> *** Installation of sundar failed ***
> 
> Removing 'D:/Users/sundard/slib/sundar/R/sundar.Rcheck/sundar'
>   ERROR
> Installation failed.
> 
> I've also tried to remove the NAMESPACE which then passes `check' but
> fails on `build' with the following:
> 
> * checking for file 'sundar/DESCRIPTION' ... OK
> * preparing 'sundar':
> * checking DESCRIPTION meta-information ... OK
> * removing junk files
> Error: cannot open file 'sundar/DESCRIPTION' for reading
> 
> If this is relevant, here is my PATH:
> C:\WINDOWS\system32;
> C:\WINDOWS;
> .;
> D:\R\rw2011pat\bin;
> D:\R\tools\bin;
> D:\Perl\bin;
> D:\Tcl\bin;
> D:\TeXLive\bin\win32;
> D:\Mingw\bin;
> C:\Program Files\Insightful\splus62\cmd;
> C:\Program Files\HTML Help Workshop
> 
> Here, the D:\R\tools\bin directory contains the utilities for building R
> from source, downloaded today. Am I missing something obvious?
> 
> Thanks,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jul 12 09:45:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 08:45:18 +0100 (BST)
Subject: [R] R CMD INSTALL use differenct c++ compiler
In-Reply-To: <27583b40050711194835257bfd@mail.gmail.com>
References: <27583b40050711194835257bfd@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0507120833390.19878@gannet.stats>

On Mon, 11 Jul 2005, Luke wrote:

> Dear R Users,
>
> When I installed e1071 use R CMD INSTALL, I got
>
> configure: WARNING: g++ 2.96 cannot reliably be used with this package.
> configure: error: Please use a different C++ compiler.
>
> But how to let R CMD INSTALL use a different C++ compiler? and which
> C++ compiler is good?

The issue here is that `g++ 2.96' was never a released compiler.  See

 	http://gcc.gnu.org/gcc-2.96.html

That did not stop some Linux distros distributing a compiler claiming to 
have that version number, but with serious bugs.  This was all a very long 
time ago (2000!), and gcc 3.4.4 is good.  (There are some issues with 
gcc 4.0.0, some of which are resolved in 4.0.1 released last week.)

It would make sense to install an updated compiler suite and rebuild 
R using it: since this is likely to a Linux OS (you did not say) it would 
be prudent to update the whole OS since many many bugs and security holes 
are known in Linuxen of that vintage.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul 12 09:52:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 08:52:28 +0100 (BST)
Subject: [R] High resolution plots
In-Reply-To: <42D363D9.2080703@biostatistic.de>
References: <42D363D9.2080703@biostatistic.de>
Message-ID: <Pine.LNX.4.61.0507120848310.19878@gannet.stats>

Please read carefully what `resolution' means for a png() device (and a 
PNG file).  It is a hint to the viewer in the file header (often ignored), 
as described in the R help file.

On Tue, 12 Jul 2005, Knut Krueger wrote:

> Is there any possibility to get high resolution plots in a windows xp
> system?
> I tried it with the device function png(filename =
> "c:/r/highresplot%d.png",pointsize=12, res=900)
> but when I try to set: width = 480, height = 480 or   pointsize = 12,
> the text is not scaled in the same way as the plots.

We have no idea what that means.  Those arguments are the defaults, and 
so make no difference to the actual plot.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Eduardo.Garcia at uv.es  Tue Jul 12 10:15:48 2005
From: Eduardo.Garcia at uv.es (Eduardo.Garcia@uv.es)
Date: Tue, 12 Jul 2005 10:15:48 +0200 (CEST)
Subject: [R] testing for significance in random-effect factors using lmer
Message-ID: <2794545991emoigar@uv.es>

Hi, I would like to know whether it is possible to obtain a value of 
significance for random effects when aplying the lme or related 
functions. The default output in R is just a variance and standard 
deviation measurement.

I feel it would be possible to obtain the significance of these random 
effects by comparing models with and without these effects. However, 
I'm not used to perform this in R and I would thank any easy guide or 
example.

Thanks.
--
********************************
Eduardo Mois??s Garc??a Roger

Institut Cavanilles de Biodiversitat i Biologia
Evolutiva - ICBIBE.
Tel. +34963543664
Fax  +34963543670



From Achim.Zeileis at wu-wien.ac.at  Tue Jul 12 10:19:04 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 12 Jul 2005 10:19:04 +0200 (CEST)
Subject: [R] exact values for p-values - more information.
In-Reply-To: <e89014e86fa5.e86fa5e89014@amc.uva.nl>
References: <e89014e86fa5.e86fa5e89014@amc.uva.nl>
Message-ID: <Pine.LNX.4.58.0507121018060.4690@thorin.ci.tuwien.ac.at>

On Mon, 11 Jul 2005, S.O. Nyangoma wrote:

> Hi there,
> Actually my aim was to compare anumber of extreme values (e.g. 39540)
> with df1=1, df2=7025 via p-values.

If they have the same degrees of freedom, use the test statistic and not
the p value for comparing them.
Z

> Spencer mentions that
>
> "However, I have also used numbers like
> exp(-19775.52) to guestimate relative degrees of plausibility for
> different alternatives."
>
> Can someone point to me an article using this method?
>
> Regards. Stephen.
>
>
>
> ----- Original Message -----
> From: Spencer Graves <spencer.graves at pdf.com>
> Date: Monday, July 11, 2005 7:39 pm
> Subject: Re: [R] exact values for p-values - more information.
>
> >          I just checked:
> >
> > > pf(39540, 1, 7025, lower.tail=FALSE, log.p=TRUE)
> > [1] -Inf
> >
> >          This is not correct.  With 7025 denominator degrees of
> > freedom, we
> > might use the chi-square approximation to the F distribution:
> >
> > > pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)
> > [1] -19775.52
> >
> >          In sum, my best approximation to  pf(39540, 1, 7025,
> > lower.tail=FALSE, log.p=TRUE), given only a minute to work on
> > this, is
> > exp(pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)) = exp(-19775.52).
> >
> >          I'm confident that many violations of assumptions would
> > likely be
> > more important than the differences between "p-value: < 2.2e-16"
> > and
>  That doesn't mean they are right, only
> > the best
> > I can get with the available resources.
> >
> >          spencer graves
> >
> > Achim Zeileis wrote:
> >
> > > On Mon, 11 Jul 2005, S.O. Nyangoma wrote:
> > >
> > >
> > >> Hi there,
> > >> If I do an lm, I get p-vlues as
> > >>
> > >> p-value: < 2.2e-16
> > >>
> > >>This is obtained from F =39540 with df1 = 1, df2 = 7025.
> > >>
> > >> Suppose am interested in exact value such as
> > >>
> > >> p-value = 1.6e-16 (note = and not <)
> > >>
> > >> How do I go about it?
> > >
> > >
> > > You can always extract the `exact' p-value from the "summary.lm"
> > object or
> > > you can compute it by hand via
> > >   pf(39540, df1 = 1, df2 = 7025, lower.tail = FALSE)
> > > For all practical purposes, the above means that the p-value is 0.
> > > I guess you are on a 32-bit machine, then it also means that the
> > p-value
> > > is smaller than the Machine epsilon
> > >   .Machine$double.eps
> > >
> > > So if you want to report the p-value somewhere, I think R's
> > output should
> > > be more than precise enough. If you want to compute some other
> > values that
> > > depend on such a p-value, then it is probably wiser to compute
> > on a log
> > > scale, i.e. instead
> > >   pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE)
> > > use
> > >   pf(70, df1 = 1, df2 = 7025, lower.tail = FALSE, log.p = TRUE)
> > >
> > > However, don't expect to be able to evaluate it at such extreme
> > values> such as 39540.
> > > Z
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-
> > project.org/posting-guide.html
> >
> > --
> > Spencer Graves, PhD
> > Senior Development Engineer
> > PDF Solutions, Inc.
> > 333 West San Carlos Street Suite 700
> > San Jose, CA 95110, USA
> >
> > spencer.graves at pdf.com
> > www.pdf.com <http://www.pdf.com>
> > Tel:  408-938-4420
> > Fax: 408-280-7915
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Tue Jul 12 10:28:04 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 12 Jul 2005 10:28:04 +0200
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <42D32627.4070407@pdf.com>
References: <200507112218.j6BMINfL009032@meitner.gene.com>
	<42D32627.4070407@pdf.com>
Message-ID: <17107.32532.683205.544710@stat.math.ethz.ch>

>>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Mon, 11 Jul 2005 19:08:39 -0700 writes:

    Spencer> I'll bite:  How does one detect bimodalidty from a boxplot?

One does not,
and that (whole area) was the main reason I mentioned  "violin
plots" which do show features such as bimodality:

There's a simple violin plot in the (recommended and hence
always available) package "lattice"

 library(lattice)
 example(panel.vioplot)

but for the sake of bimodality {and the version of violin plot
that was in the 1998 Am.Stat. paper}

 install.packages("vioplot") # if needed
 example(vioplot)

Both of these examples use the basic boxplot but *add* to it, so
you can, e.g., clearly discern bimodality.

Martin Maechler



From S.O.Nyangoma at amc.uva.nl  Tue Jul 12 10:38:43 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Tue, 12 Jul 2005 10:38:43 +0200
Subject: [R] exact values for p-values - more information.
Message-ID: <166d58c166fe94.166fe94166d58c@amc.uva.nl>

> If they have the same degrees of freedom, use the test statistic 
> and not
> the p value for comparing them.
> Z

I appretiate your input to this discussion. Do you know of a reference 
to your statement above? 

I had actually used the test-statistic which in my case is r-squared 
to compare them. This is in my view was adequate but I also think it 
is more convincing to say something about the p-values (difficulties 
in computing them, and hence the rationale of solely using the test-
stat).

regards. Stephen.



From shv736 at yahoo.com  Tue Jul 12 10:39:34 2005
From: shv736 at yahoo.com (Vassily Shvets)
Date: Tue, 12 Jul 2005 01:39:34 -0700 (PDT)
Subject: [R] empirical A matrix for dynamic system and markov analyses
Message-ID: <20050712083934.72602.qmail@web52308.mail.yahoo.com>

I'm working with a dynamic system that I've started to
analyse using msm(I've emailed to chris, orignator of
the program, separately, but maybe he's on holiday).
I'd like to know if anyone has considered the relation
between the markov analysis done with msm on the one
hand, and dynamic system analysis done with MATLAB?
That is, I'm working with a set of empirical data that
are more or less comparable to the kind of 'hospital'
analyses done in msm manual(my data has 4 states, and
'patients' move from one state to another over a time
period, n is varying from 120 to much larger).

Using the msm program gives me a nice Q and P matrices
for the system. However, I'm interested in making a
non homogenous system out of it(introducing
covariates, but covariates which also vary with time,
so that the system is approximately dy = Ax + Bz ).

One reason I'd like to go  further than msm is taking
me right now is that the 'fit' statistic that msm
provides is I think, sort of narrowly focused on the
markov property. I guess I'm trying to ask if anyone
has analyzed such empirical data using the state space
system in MATLAB? The question also seems to involve
a/the ML estimation done by msm versus b/ the least
squares estimation of parameter that evidently is
predominant in the 'estimation toolbox' that runs with
MATLAB.
thanks,
s



From A.Robinson at ms.unimelb.edu.au  Tue Jul 12 10:48:04 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 12 Jul 2005 18:48:04 +1000
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <17107.32532.683205.544710@stat.math.ethz.ch>
References: <200507112218.j6BMINfL009032@meitner.gene.com>
	<42D32627.4070407@pdf.com>
	<17107.32532.683205.544710@stat.math.ethz.ch>
Message-ID: <20050712084804.GA610@ms.unimelb.edu.au>

Rob Hyndman has written on a version of the boxplot that can show
bimodality.  See, particularly, Figure 2 in:

Hyndman, R.J. (1996) "Computing and graphing highest density regions",
Amer. Statist., 50, 120-126. 

Andrew

On Tue, Jul 12, 2005 at 10:28:04AM +0200, Martin Maechler wrote:
> >>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
> >>>>>     on Mon, 11 Jul 2005 19:08:39 -0700 writes:
> 
>     Spencer> I'll bite:  How does one detect bimodalidty from a boxplot?
> 
> One does not,
> and that (whole area) was the main reason I mentioned  "violin
> plots" which do show features such as bimodality:
> 
> There's a simple violin plot in the (recommended and hence
> always available) package "lattice"
> 
>  library(lattice)
>  example(panel.vioplot)
> 
> but for the sake of bimodality {and the version of violin plot
> that was in the 1998 Am.Stat. paper}
> 
>  install.packages("vioplot") # if needed
>  example(vioplot)
> 
> Both of these examples use the basic boxplot but *add* to it, so
> you can, e.g., clearly discern bimodality.
> 
> Martin Maechler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at unimelb.edu.au       Website: http://www.ms.unimelb.edu.au



From Achim.Zeileis at wu-wien.ac.at  Tue Jul 12 10:51:45 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 12 Jul 2005 10:51:45 +0200 (CEST)
Subject: [R] exact values for p-values - more information.
In-Reply-To: <166d58c166fe94.166fe94166d58c@amc.uva.nl>
References: <166d58c166fe94.166fe94166d58c@amc.uva.nl>
Message-ID: <Pine.LNX.4.58.0507121043220.4727@thorin.ci.tuwien.ac.at>

On Tue, 12 Jul 2005, S.O. Nyangoma wrote:

> > If they have the same degrees of freedom, use the test statistic
> > and not
> > the p value for comparing them.
> > Z
>
> I appretiate your input to this discussion. Do you know of a reference
> to your statement above?

?? Any basic statistics book? Distribution functions tend to be
monotonous.

> I had actually used the test-statistic which in my case is r-squared
> to compare them. This is in my view was adequate but I also think it
> is more convincing to say something about the p-values

Not really `more' convincing, it's all pretty equivalent when the number
of estimated parameters is the same. You can also compare the fitted
models via their associated residual sum of squares which I would find
most intuitive because that is the objective function you are trying to
minimize via OLS.
Z

> (difficulties
> in computing them, and hence the rationale of solely using the test-
> stat).
>
> regards. Stephen.
>
>
>



From S.O.Nyangoma at amc.uva.nl  Tue Jul 12 11:06:39 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Tue, 12 Jul 2005 11:06:39 +0200
Subject: [R] what is the .Machine$double.xmin for a 64 bit machine?
Message-ID: <ed8f97ed823e.ed823eed8f97@amc.uva.nl>


I use a 32 bit machine. For those using 64 bit machines, 

what is the .Machine$double.xmin for for machines?

regards. Stephen.




----- Original Message -----
From: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
Date: Tuesday, July 12, 2005 10:51 am
Subject: Re: [R] exact values for p-values - more information.

> On Tue, 12 Jul 2005, S.O. Nyangoma wrote:
> 
> > > If they have the same degrees of freedom, use the test statistic
> > > and not
> > > the p value for comparing them.
> > > Z
> >
> > I appretiate your input to this discussion. Do you know of a 
> reference> to your statement above?
> 
> ?? Any basic statistics book? Distribution functions tend to be
> monotonous.
> 
> > I had actually used the test-statistic which in my case is r-
squared
> > to compare them. This is in my view was adequate but I also 
> think it
> > is more convincing to say something about the p-values
> 
> Not really `more' convincing, it's all pretty equivalent when the 
> numberof estimated parameters is the same. You can also compare 
> the fitted
> models via their associated residual sum of squares which I would 
find
> most intuitive because that is the objective function you are 
> trying to
> minimize via OLS.
> Z
> 
> > (difficulties
> > in computing them, and hence the rationale of solely using the 
> test-
> > stat).
> >
> > regards. Stephen.
> >
> >
> >
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 12 10:43:46 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 12 Jul 2005 09:43:46 +0100 (BST)
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <42D32627.4070407@pdf.com>
Message-ID: <XFMail.050712094346.Ted.Harding@nessie.mcc.ac.uk>

On 12-Jul-05 Spencer Graves wrote:
>         I'll bite:  How does one detect bimodalidty from a boxplot?
> 
>         spencer graves
> 
> Berton Gunter wrote:
> [...]
>> Finally, a tidbit for boxplot afficianados: how does one detect
>> bimodality from a boxplot?
>> 
>> -- Bert Gunter

Not definitively detectable of course, but certainly suspectable.

Hint: What is the relationship between the distances from the "median"
line to the extremes of the "box", and from the extremes of the "box"
to the ends of the "whiskers", in the case of a uniform distribution?



If that doesn't trigger it, then:

Hint:

  unimod<-rnorm(1000,0,sqrt(26))
  nomod<-sqrt(12*26)*runif(1000)
  boxplot(data.frame(cbind(unimod,nomod,bimod)))

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jul-05                                       Time: 09:42:48
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 12 11:27:17 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 12 Jul 2005 10:27:17 +0100 (BST)
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <XFMail.050712094346.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050712102441.Ted.Harding@nessie.mcc.ac.uk>

On 12-Jul-05 Ted Harding wrote:
> On 12-Jul-05 Spencer Graves wrote:
>>         I'll bite:  How does one detect bimodalidty from a boxplot?
>> 
>>         spencer graves
>> 
>> Berton Gunter wrote:
>> [...]
>>> Finally, a tidbit for boxplot afficianados: how does one detect
>>> bimodality from a boxplot?
>>> 
>>> -- Bert Gunter
> 
> Not definitively detectable of course, but certainly suspectable.
> 
> Hint: What is the relationship between the distances from the "median"
> line to the extremes of the "box", and from the extremes of the "box"
> to the ends of the "whiskers", in the case of a uniform distribution?
> 
> 
> 
> If that doesn't trigger it, then:
> 
> Hint:
> 
>   unimod<-rnorm(1000,0,sqrt(26))
>   nomod<-sqrt(12*26)*runif(1000)
>   boxplot(data.frame(cbind(unimod,nomod,bimod)))

OOPS!! (cut-&-pasted the wrong lines):

  unimod<-rnorm(1000,0,sqrt(26))
  nomod<-sqrt(12*26)*(runif(1000)-0.5)
  bimod<-c(rnorm(500,-5,1),rnorm(500,5,1))
  boxplot(data.frame(cbind(unimod,nomod,bimod)))

> Best wishes,
> Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jul-05                                       Time: 10:24:34
------------------------------ XFMail ------------------------------



From lzhtom at hotmail.com  Tue Jul 12 11:53:14 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Tue, 12 Jul 2005 09:53:14 +0000
Subject: [R] how to generate argument from a vector automatically
Message-ID: <BAY12-F107DD7DEFD45EE716786D5C7DF0@phx.gbl>

hi netters

i have a vector NAMES containing a series of variable names: 
NAMES=c(x,r,z,m,st,qr,.....nn).
i wanna fit a regression tree by using the code:  
           my.tree<-tree(y~x+r+z+m+....nn,my.dataframe)

but i don't want to type out "x+r+z+m+....+nn" one by one, as there are so 
many variables. besides, sometimes i wanna put the code in a function. so i 
need to have the argument "x+r+z+m+....+nn" generated from NAMES 
automatically.

i've tried the code: paste(X,collpase="+") but it didn't work.

could anybody give me a hint?



From HDoran at air.org  Tue Jul 12 11:53:31 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 12 Jul 2005 05:53:31 -0400
Subject: [R] testing for significance in random-effect factors using lmer
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41BCF@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/984ac6e2/attachment.pl

From usenet at s-boehringer.de  Tue Jul 12 11:56:48 2005
From: usenet at s-boehringer.de (usenet@s-boehringer.de)
Date: Tue, 12 Jul 2005 11:56:48 +0200 (CEST)
Subject: [R] bug in chdir option of source
Message-ID: <41044.132.252.149.100.1121162208.squirrel@webmail.loomes.de>

I'm on R 2.1.0.

In the "source" function there is a bug preventing the proper use of the
chdir option (which simply doesn't work).

The problem is that in the function the following line occurs:
 file <- file(file, "r", encoding = encoding)

This overwrites the variable "file" and later causes the check
    if (chdir && is.character(file) && (path <- dirname(file)) !=
        ".")
to fail.

I have some generic files which are not yet a package that I regularly
want to source into many other files. How do people handle situations like
this right now?

Thanks, Stefan



From renaud.lancelot at cirad.fr  Tue Jul 12 11:57:04 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Tue, 12 Jul 2005 11:57:04 +0200
Subject: [R] Boxplot philosophy {was "Boxplot in R"}
In-Reply-To: <17107.32532.683205.544710@stat.math.ethz.ch>
References: <200507112218.j6BMINfL009032@meitner.gene.com>	<42D32627.4070407@pdf.com>
	<17107.32532.683205.544710@stat.math.ethz.ch>
Message-ID: <42D393F0.2040705@cirad.fr>

Martin Maechler a ??crit :
>>>>>>"Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>>    on Mon, 11 Jul 2005 19:08:39 -0700 writes:
> 
> 
>     Spencer> I'll bite:  How does one detect bimodalidty from a boxplot?
> 
> One does not,
> and that (whole area) was the main reason I mentioned  "violin
> plots" which do show features such as bimodality:
> 
> There's a simple violin plot in the (recommended and hence
> always available) package "lattice"
> 
>  library(lattice)
>  example(panel.vioplot)

Or rather

example(panel.violin)

Best,

Renaud


> 
> but for the sake of bimodality {and the version of violin plot
> that was in the 1998 Am.Stat. paper}
> 
>  install.packages("vioplot") # if needed
>  example(vioplot)
> 
> Both of these examples use the basic boxplot but *add* to it, so
> you can, e.g., clearly discern bimodality.
> 
> Martin Maechler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From ajayshah at mayin.org  Tue Jul 12 12:07:07 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 12 Jul 2005 15:37:07 +0530
Subject: [R] Puzzled at ifelse()
Message-ID: <20050712100707.GA3175@lubyanka.local>

I have a situation where this is fine:

  > if (length(x)>15) {
      clever <- rr.ATM(x, maxtrim=7)
    } else {
      clever <- rr.ATM(x)
    }
  > clever
  $ATM
  [1] 1848.929

  $sigma
  [1] 1.613415

  $trim
  [1] 0

  $lo
  [1] 1845.714

  $hi
  [1] 1852.143

But this variant, using ifelse(), breaks:

  > clever <- ifelse(length(x)>15, rr.ATM(x, maxtrim=7), rr.ATM(x))
  > clever
  [[1]]
  [1] 1848.929

What am I doing wrong?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ligges at statistik.uni-dortmund.de  Tue Jul 12 12:10:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2005 12:10:17 +0200
Subject: [R] bug in chdir option of source
In-Reply-To: <41044.132.252.149.100.1121162208.squirrel@webmail.loomes.de>
References: <41044.132.252.149.100.1121162208.squirrel@webmail.loomes.de>
Message-ID: <42D39709.6010106@statistik.uni-dortmund.de>

usenet at s-boehringer.de wrote:

> I'm on R 2.1.0.
> 
> In the "source" function there is a bug preventing the proper use of the
> chdir option (which simply doesn't work).
> 
> The problem is that in the function the following line occurs:
>  file <- file(file, "r", encoding = encoding)
> 
> This overwrites the variable "file" and later causes the check
>     if (chdir && is.character(file) && (path <- dirname(file)) !=
>         ".")
> to fail.
> 
> I have some generic files which are not yet a package that I regularly
> want to source into many other files. How do people handle situations like
> this right now?

They use R-2.1.1 where the bug has been fixed.

Uwe Ligges



> Thanks, Stefan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From R.A.Sanderson at newcastle.ac.uk  Tue Jul 12 13:10:43 2005
From: R.A.Sanderson at newcastle.ac.uk (Roy Sanderson)
Date: Tue, 12 Jul 2005 11:10:43 +0000
Subject: [R] Design: predict.lrm does not recognise lrm.fit object
Message-ID: <3.0.3.32.20050712111043.0085c2c0@popin.ncl.ac.uk>

Hello

I'm using logistic regression from the Design library (lrm), then fastbw to
undertake a backward selection and create a reduced model, before trying to
make predictions against an independent set of data using predict.lrm with
the reduced model.  I wouldn't normally use this method, but I'm
contrasting the results with an AIC/MMI approach.  The script contains:

# Determine full logistic regression
lrm_logist = lrm(PresAbs ~ Size + X2ndpc + soil + AAR + tjan.jun,
data=training)
# Backward selection of variables in model
lrm_stp = fastbw(lrm_logist, rule="p", sls=0.05)
# Fit reduced model
lrm_reduced = lrm.fit(training[,lrm_stp$parms.kept[-1]], training$PresAbs)
# Predict using parameters from reduced model against a new dataset
predict(lrm_reduced, testing, type="fitted.ind")

It is the last command that fails, reporting the error:
Error in getOldDesign(fit): fit was not created with a Design library
fitting function.

On further investigation, the class of the object from lrm.fit is only
"lrm", whereas predict.lrm seems to be expecting objects with the class
"lrm", "Design", "glm", judging from the examples on the predict.lrm help
page.  Many of the attributes differ also.  Does anyone know a simple
work-around for this problem?

Many thanks
Roy



From r.shengzhe at gmail.com  Tue Jul 12 12:28:09 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 12 Jul 2005 12:28:09 +0200
Subject: [R] help: how to use tkevent.generate(...)
Message-ID: <ea57975b050712032818076dc8@mail.gmail.com>

Hello,

I use package "tcltk" to do some GUI programming, and want to find a
function which can do the operation "click a button", just like using
a mouse to click. If tkevent.generate can do that? I tried it as
below, but failed. Please give me a hint!

tt <- tktoplevel()
tkwm.title(tt,"Simple Dialog")

onOK <- function(){print("OK")}
onCancel <- function(){print("Cancel")}
OK.but <- tkbutton(tt, text="  OK  ", command=onOK)
Cancel.but <- tkbutton(tt, text="Cancel",command=onCancel)
tkgrid(OK.but,Cancel.but)

tkevent.generate(tt, onOK)


Thank you,
Shengzhe



From murdoch at stats.uwo.ca  Tue Jul 12 12:32:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 12 Jul 2005 06:32:50 -0400
Subject: [R] Puzzled at ifelse()
In-Reply-To: <20050712100707.GA3175@lubyanka.local>
References: <20050712100707.GA3175@lubyanka.local>
Message-ID: <42D39C52.9050509@stats.uwo.ca>

Ajay Narottam Shah wrote:
> I have a situation where this is fine:
> 
>   > if (length(x)>15) {
>       clever <- rr.ATM(x, maxtrim=7)
>     } else {
>       clever <- rr.ATM(x)
>     }
>   > clever
>   $ATM
>   [1] 1848.929
> 
>   $sigma
>   [1] 1.613415
> 
>   $trim
>   [1] 0
> 
>   $lo
>   [1] 1845.714
> 
>   $hi
>   [1] 1852.143
> 
> But this variant, using ifelse(), breaks:
> 
>   > clever <- ifelse(length(x)>15, rr.ATM(x, maxtrim=7), rr.ATM(x))
>   > clever
>   [[1]]
>   [1] 1848.929
> 
> What am I doing wrong?

if (test) expr1 else expr2

evaluates only one of expr1 or expr2 according to test, and returns that 
result.

ifelse(test, expr1, expr2)

executes all three of test, expr1, and expr2, and puts together a vector 
response where for each element of test that is true (non-zero), the 
corresponding element of one of the expr's is selected.

I'd guess your x is length 1, so your test is length 1, and only the 
first element of the result of rr.ATM is returned (which is not very 
useful).

To get what you want, you should write

clever <- if(length(x) > 15) rr.ATM(x, maxtrim=7) else rr.ATM(x);

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Tue Jul 12 12:38:44 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jul 2005 12:38:44 +0200
Subject: [R] what is the .Machine$double.xmin for a 64 bit machine?
In-Reply-To: <ed8f97ed823e.ed823eed8f97@amc.uva.nl>
References: <ed8f97ed823e.ed823eed8f97@amc.uva.nl>
Message-ID: <x2br58cnrf.fsf@turmalin.kubism.ku.dk>

"S.O. Nyangoma" <S.O.Nyangoma at amc.uva.nl> writes:

> I use a 32 bit machine. For those using 64 bit machines, 
> 
> what is the .Machine$double.xmin for for machines?
> 
> regards. Stephen.
> 

Depends on the FPU which is basically independent of the address mode:

PIII, Fedora Core 4:

>  .Machine$double.xmin
[1] 2.225074e-308

Opteron, Fedora Core 4:

> .Machine$double.xmin
[1] 2.225074e-308


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From admin at biostatistic.de  Tue Jul 12 12:43:13 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Tue, 12 Jul 2005 12:43:13 +0200
Subject: [R] High resolution plots
In-Reply-To: <Pine.GSO.4.31.0507121004110.28959-100000@markov.stats>
References: <Pine.GSO.4.31.0507121004110.28959-100000@markov.stats>
Message-ID: <42D39EC1.3060609@biostatistic.de>

Sorry i sent the answer not to the mailing list - here it is

Brian D Ripley schrieb:

>On Tue, 12 Jul 2005, Knut Krueger wrote:
>
>  
>
>>Prof Brian Ripley schrieb:
>>
>>    
>>
>>>Please read carefully what `resolution' means for a png() device (and a
>>>PNG file).  It is a hint to the viewer in the file header (often ignored),
>>>as described in the R help file.
>>>
>>>      
>>>
>  
>
>>>On Tue, 12 Jul 2005, Knut Krueger wrote:
>>>
>>>
>>>
>>>      
>>>
>>>>Is there any possibility to get high resolution plots in a windows xp
>>>>system?
>>>>I tried it with the device function png(filename =
>>>>"c:/r/highresplot%d.png",pointsize=12, res=900)
>>>>but when I try to set: width = 480, height = 480 or   pointsize = 12,
>>>>the text is not scaled in the same way as the plots.
>>>>        
>>>>
Ok first:
I do not want anybody to get angry and I am the first time in this group.
I will make any mistakes like sending you your own help file and I
apologize in advance for those mistakes.

>Not if you just include those arguments or not.  Yes if you include
>res=900 or not, but that is not what you said.
>  
>
I tried to include res=600 and res=300 there was no difference in the
file, both size was 4kb  480*480 Pixel and 150 dpi
png(filename = "c:/r/highresplot%d.png",width = 480, height =
480,pointsize=12, res=300)
png(filename = "c:/r/highresplot%d.png",width = 480, height =
480,pointsize=12, res=600)


second attempt was to increase width = 480, height = 480 to width = 960,
height = 960

png(filename = "c:/r/highresplot%d.png",width = 960, height =
960,pointsize=12, res=600)
png(filename = "c:/r/highresplot%d.png",width = 480, height =
480,pointsize=12, res=300)


both is the file size 8kb either with res=300 or res=600
double file size with double size of the device that's what i expected.

The letters are nearly unreadable because they are very small - ok
png(filename = "c:/r/highresplot%d.png",width = 480, height =
480,pointsize=24, res=300)
Letters are in the same size as
png(filename = "c:/r/highresplot%d.png",width = 480, height =
480,pointsize=12, res=300)
also that's what I expected, but the quality of the characters is as bad
as when I would extract the plot with any graphic application
and the small lines from boxplots are sometimes visible sometimes not.


>Why aer you sending me a help file I wrote, one which you seem to be
>unable to understand?  I am not the one failing to notice what it says.
>


But maybe you are right that I am not understanding the help file, but
I hope that clarifies my problems and my mistakes

with regards
Knut Krueger
http://www.biostatistic.de



From ligges at statistik.uni-dortmund.de  Tue Jul 12 12:49:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2005 12:49:33 +0200
Subject: [R] how to generate argument from a vector automatically
In-Reply-To: <BAY12-F107DD7DEFD45EE716786D5C7DF0@phx.gbl>
References: <BAY12-F107DD7DEFD45EE716786D5C7DF0@phx.gbl>
Message-ID: <42D3A03D.90604@statistik.uni-dortmund.de>

zhihua li wrote:

> hi netters
> 
> i have a vector NAMES containing a series of variable names: 
> NAMES=c(x,r,z,m,st,qr,.....nn).
> i wanna fit a regression tree by using the code:            
> my.tree<-tree(y~x+r+z+m+....nn,my.dataframe)
> 
> but i don't want to type out "x+r+z+m+....+nn" one by one, as there are 
> so many variables. besides, sometimes i wanna put the code in a 
> function. so i need to have the argument "x+r+z+m+....+nn" generated 
> from NAMES automatically.
> 
> i've tried the code: paste(X,collpase="+") but it didn't work.

You have to construct it as.formula() from these characters,
or even better use short term formulas such as "y~.", if all variables
should be used anyway.

Uwe Ligges

> could anybody give me a hint?
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From karen at biology.biol.wits.ac.za  Tue Jul 12 12:50:33 2005
From: karen at biology.biol.wits.ac.za (Karen Kotschy)
Date: Tue, 12 Jul 2005 12:50:33 +0200 (SAST)
Subject: [R] adding a factor column based on levels of another factor
Message-ID: <Pine.LNX.4.58.0507121157030.4900@euclea.sevenc.private>

Hi R users

Does anyone out there have a better/quicker way of adding a factor column
to a data frame based on levels of another factor?

I have a (large) data frame consisting of records for individual plants,
each represented by a unique ID number. The species of each plant is
indicated in the column "species", which is a factor column with many
different levels (species). There are multiple records for each species,
and there is no pattern to the order in which the species names appear in
the data frame.

e.g.
   uniqueID species   elev   ht   diam
1  1        sp2       3.5    1.3  55
2  2        sp2       4.2    0.5  15
3  3        sp3       3.2    1.0  13
4  4        sp65      2.2    2.0  14
5  5        sp43      5.4    5.7  20
6  6        sp2       2.5    4.1  32
7  7        sp12      1.1    0.9  5
8  8        sp3       3.4    3.6  2

I would like to add a factor column to this data frame, indicating to
which group each individual belongs. All individuals of the same species
will belong to the same group.

Is there a quick way of saying "for all instances of species1, give the
value 5, for all instances of species2, give the value 4, etc" (where 5
and 4 are levels of a factor)?

The only way I can think of doing it is to split the data frame by
species, then add a column to each subset showing the group, then
re-join all the subsets. This seems clumsy and prone to errors. Anyone
know a better way?

I've looked at expand.grid and gl but they don't seem to do what I want.

Thanks!

Karen Kotschy
Centre for Water in the Environment
University of the Witwatersrand
Johannesburg
South Africa



From ramasamy at cancer.org.uk  Tue Jul 12 12:52:37 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 12 Jul 2005 11:52:37 +0100
Subject: [R] how to generate argument from a vector automatically
In-Reply-To: <BAY12-F107DD7DEFD45EE716786D5C7DF0@phx.gbl>
References: <BAY12-F107DD7DEFD45EE716786D5C7DF0@phx.gbl>
Message-ID: <1121165557.5946.20.camel@ipc143004.lif.icnet.uk>

You can do it by subsetting the dataframe 

  df <- data.frame( y=rbinom(100, 1, prob=0.5), 
                    x1=rnorm(100), x2=rnorm(100), x3=rnorm(100),
                    z1=rnorm(100), z2=rnorm(100) )
  names <- c("x1", "x2", "x3")
  tree( y ~ . , data = df[ , c("y", names) ] )

This solution is useless when you want to consider interactions between
some terms etc. Another more flexible solution is from last example from
help(formula) :

   xnam <- paste("x", 1:3, sep="")
   call <- as.formula(paste("y ~ ", paste(xnam, collapse= "+")))
   tree( call, df )

Regards, Adai



On Tue, 2005-07-12 at 09:53 +0000, zhihua li wrote:
> hi netters
> 
> i have a vector NAMES containing a series of variable names: 
> NAMES=c(x,r,z,m,st,qr,.....nn).
> i wanna fit a regression tree by using the code:  
>            my.tree<-tree(y~x+r+z+m+....nn,my.dataframe)
> 
> but i don't want to type out "x+r+z+m+....+nn" one by one, as there are so 
> many variables. besides, sometimes i wanna put the code in a function. so i 
> need to have the argument "x+r+z+m+....+nn" generated from NAMES 
> automatically.
> 
> i've tried the code: paste(X,collpase="+") but it didn't work.
> 
> could anybody give me a hint?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jul 12 13:02:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 12:02:19 +0100 (BST)
Subject: [R] bug in chdir option of source
In-Reply-To: <41044.132.252.149.100.1121162208.squirrel@webmail.loomes.de>
References: <41044.132.252.149.100.1121162208.squirrel@webmail.loomes.de>
Message-ID: <Pine.LNX.4.61.0507121200530.23383@gannet.stats>

Please read the posting guide (as we do ask) and use the current version 
as it asks to see if the `bug' has already been fixed.

This was fixed a while ago, definitely in 2.1.1.

On Tue, 12 Jul 2005 usenet at s-boehringer.de wrote:

> I'm on R 2.1.0.
>
> In the "source" function there is a bug preventing the proper use of the
> chdir option (which simply doesn't work).
>
> The problem is that in the function the following line occurs:
> file <- file(file, "r", encoding = encoding)
>
> This overwrites the variable "file" and later causes the check
>    if (chdir && is.character(file) && (path <- dirname(file)) !=
>        ".")
> to fail.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul 12 13:07:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 12:07:53 +0100 (BST)
Subject: [R] what is the .Machine$double.xmin for a 64 bit machine?
In-Reply-To: <ed8f97ed823e.ed823eed8f97@amc.uva.nl>
References: <ed8f97ed823e.ed823eed8f97@amc.uva.nl>
Message-ID: <Pine.LNX.4.61.0507121203260.23383@gannet.stats>

It's a floating-point quantity: `64 bit' refers to the pointer size 
(only).

Almost all current R platforms use IEC60559 arithmetic for real numbers 
and 32-bit integers for integers, so differ only in the way the compiler 
orders operations and stores to memory (thereby losing precision on some 
CPUs).

On Tue, 12 Jul 2005, S.O. Nyangoma wrote:

>
> I use a 32 bit machine. For those using 64 bit machines,
>
> what is the .Machine$double.xmin for for machines?
>
> regards. Stephen.
>
>
>
>
> ----- Original Message -----
> From: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
> Date: Tuesday, July 12, 2005 10:51 am
> Subject: Re: [R] exact values for p-values - more information.
>
>> On Tue, 12 Jul 2005, S.O. Nyangoma wrote:
>>
>>>> If they have the same degrees of freedom, use the test statistic
>>>> and not
>>>> the p value for comparing them.
>>>> Z
>>>
>>> I appretiate your input to this discussion. Do you know of a
>> reference> to your statement above?
>>
>> ?? Any basic statistics book? Distribution functions tend to be
>> monotonous.
>>
>>> I had actually used the test-statistic which in my case is r-
> squared
>>> to compare them. This is in my view was adequate but I also
>> think it
>>> is more convincing to say something about the p-values
>>
>> Not really `more' convincing, it's all pretty equivalent when the
>> numberof estimated parameters is the same. You can also compare
>> the fitted
>> models via their associated residual sum of squares which I would
> find
>> most intuitive because that is the objective function you are
>> trying to
>> minimize via OLS.
>> Z
>>
>>> (difficulties
>>> in computing them, and hence the rationale of solely using the
>> test-
>>> stat).
>>>
>>> regards. Stephen.
>>>
>>>
>>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christian.ritter at shell.com  Tue Jul 12 13:08:31 2005
From: christian.ritter at shell.com (Ritter, Christian C GSMCIL-GSTMS/2)
Date: Tue, 12 Jul 2005 13:08:31 +0200
Subject: [R] unexpected behavior in bwplot
Message-ID: <156CDC8CCFD1894295D2907F16337A48B2AF19@bru-s-006.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/c56e85b6/attachment.pl

From r.hankin at noc.soton.ac.uk  Tue Jul 12 13:11:47 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 12 Jul 2005 12:11:47 +0100
Subject: [R] elegant matrix creation
Message-ID: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>

Hi

I want to write a little function that takes a vector of arbitrary
length "n" and returns a matrix of size n+1 by n+1.

I can't easily describe it, but the following function that works for
n=3 should convey what I'm trying to do:


f <- function(x){
   matrix(c(
    1           ,   0      ,   0 , 0,
x[1]          ,   1      ,   0 , 0,
x[1]*x[2]     , x[2]     ,   1 , 0,
x[1]*x[2]*x[3], x[2]*x[3], x[3], 1
),
4,4,         byrow=T)
}

f(c(10,7,2))
      [,1] [,2] [,3] [,4]
[1,]    1    0    0    0
[2,]   10    1    0    0
[3,]   70    7    1    0
[4,]  140   14    2    1
 >


As one goes down column "i", the entries get multiplied by successive
elements of x,  starting with x[i], after the first "1"

As one goes along a row, one takes a product of the tail end of x,
until the zeroes kick in.


Am I missing some clever solution?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From hb at maths.lth.se  Tue Jul 12 13:36:08 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 12 Jul 2005 13:36:08 +0200
Subject: [R] R.oo static field
In-Reply-To: <3f87cc6d050708091078b95235@mail.gmail.com>
References: <3f87cc6d050708091078b95235@mail.gmail.com>
Message-ID: <42D3AB28.1050304@maths.lth.se>

Omar Lakkis wrote:
> How can I define a static member of a class? not a static method,
> rather a static field that would be accessed by all instances of the
> class.

To define a static field in an Object class (the R.oo package), I 
recommend you to use a private field <.field> and then create a virtual 
field <field> by defining methods get<Field>() and set<Field(). These 
methods should get and set the field <.field> of the _static_ instance 
of the class. Below is an example, which also modifies the static field 
already in the constructor something which otherwise may be tricky. I 
will add this example to the help pages of R.oo in the next release.

Hope it helps

Henrik Bengtsson (author of R.oo)

######################################################################
# Example illustrating how to "emulate" static fields using virtual
# fields, i.e. get- and set-methods.  Here we use a private static
# field '.count' of the static class instance 'MyClass', i.e.
# MyClass$.count.  Then we define a virtual field 'count' via method
# getCount() to access this static field.  This will make all queries
# for 'count' of any object to use the static field instead.  In the
# same way is assignment controlled via the setCount() method.  A
# side effect of this way of coding is that all MyClass instances will
# also have the private field '.count' (set to zero except for the
# static field that is).
######################################################################
library(R.oo)

setConstructorS3("MyClass", function(...) {
   # Create an instance (the static class instance included)
   this <- extend(Object(), "MyClass",
     .count = 0
   )

   # In order for a static field to be updated in the
   # constructor it has to be done after extend().
   this$count <- this$count + 1;

   # Return the object
   this;
})


setMethodS3("as.character", "MyClass", function(this, ...) {
   paste(class(this)[1], ": Number of instances: ", this$count, sep="");
})


# Get virtual field 'count', e.g. obj$count.
setMethodS3("getCount", "MyClass", function(this, ...) {
   MyClass$.count;
})


# Set virtual field 'count', e.g. obj$count <- value.
setMethodS3("setCount", "MyClass", function(this, value, ...) {
   MyClass$.count <- value;
})


# Create four instances of class 'MyClass'
obj <- lapply(1:4, MyClass)
print(obj)
print(MyClass$count)
print(obj[[1]]$count)
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: MyClass.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/ecfaa653/MyClass.pl

From p.dalgaard at biostat.ku.dk  Tue Jul 12 13:41:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jul 2005 13:41:02 +0200
Subject: [R] help: how to use tkevent.generate(...)
In-Reply-To: <ea57975b050712032818076dc8@mail.gmail.com>
References: <ea57975b050712032818076dc8@mail.gmail.com>
Message-ID: <x27jfwckvl.fsf@turmalin.kubism.ku.dk>

wu sz <r.shengzhe at gmail.com> writes:

> Hello,
> 
> I use package "tcltk" to do some GUI programming, and want to find a
> function which can do the operation "click a button", just like using
> a mouse to click. If tkevent.generate can do that? I tried it as
> below, but failed. Please give me a hint!
> 
> tt <- tktoplevel()
> tkwm.title(tt,"Simple Dialog")
> 
> onOK <- function(){print("OK")}
> onCancel <- function(){print("Cancel")}
> OK.but <- tkbutton(tt, text="  OK  ", command=onOK)
> Cancel.but <- tkbutton(tt, text="Cancel",command=onCancel)
> tkgrid(OK.but,Cancel.but)
> 
> tkevent.generate(tt, onOK)

That would never work: At the very least you need to send *events* and
to the right widget, not the toplevel. If you just wanted to run onOK,
there's an easier way: onOK(), or generically:

  tcl("eval", tkcget(OK.but, command=NULL))

to run the command associated with OK.but (whether it is an R callback
or not).

The right way to trigger a button press event turns out to be a bit
tricky, but Googling about (for the tcl/tk counterpart "event
generate") found the solution: You need not one, but three events:
Entering the button, pressing the mouse button, releasing the mouse
button, i.e.:

tkevent.generate(OK.but,"<Enter>")
tkevent.generate(OK.but,"<1>")
tkevent.generate(OK.but,"<ButtonRelease-1>")


for (e in c("<Enter>","<1>","<ButtonRelease-1>"))
   tkevent.generate(OK.but,e)






-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From usenet at s-boehringer.de  Tue Jul 12 13:49:38 2005
From: usenet at s-boehringer.de (usenet@s-boehringer.de)
Date: Tue, 12 Jul 2005 13:49:38 +0200 (CEST)
Subject: [R] bug in chdir option of source
Message-ID: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>

I apologize for not having investigated enough.

However I want to bring up the point that upgrading can be a very
tedious thing. I administer a cluster for which upgrading brings it to a
halt for several hours stopping all calculations going on. At the moment
I have no way to go from 2.1.0 to 2.1.1.

I just want to add this point to the release policy of R which
historically has very quick release cycles which on the one hand is a
very positive thing but on the other hand can be problematic with
respect to some aspects.

Thanks again, Stefan


On Tue, 2005-07-12 at 13:02, Prof Brian Ripley wrote:
> Please read the posting guide (as we do ask) and use the current version
> as it asks to see if the `bug' has already been fixed.
>
> This was fixed a while ago, definitely in 2.1.1.
>
> On Tue, 12 Jul 2005 usenet at s-boehringer.de wrote:
>
> > I'm on R 2.1.0.
> >
> > In the "source" function there is a bug preventing the proper use of the
> > chdir option (which simply doesn't work).
> >
> > The problem is that in the function the following line occurs:
> > file <- file(file, "r", encoding = encoding)
> >
> > This overwrites the variable "file" and later causes the check
> >    if (chdir && is.character(file) && (path <- dirname(file)) !=
> >        ".")
> > to fail.



From ahenningsen at email.uni-kiel.de  Tue Jul 12 13:53:06 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 12 Jul 2005 13:53:06 +0200
Subject: [R] elegant matrix creation
In-Reply-To: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
References: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
Message-ID: <200507121353.06578.ahenningsen@email.uni-kiel.de>

Maybe not elegant, but I guess this function does what you want:

f <- function( x ) {
   n <- length( x ) + 1
   m <- matrix( 0, nrow = n, ncol = n )
   diag(m) <- 1
   for( j in 1:( n-1 ) ) {
      for( i in ( j+1 ):n ) {
         m[ i, j ] <- prod( x[ j:(i-1) ] )
      }
   }
   return( m )
}

Best wishes,
Arne

On Tuesday 12 July 2005 13:11, Robin Hankin wrote:
> Hi
>
> I want to write a little function that takes a vector of arbitrary
> length "n" and returns a matrix of size n+1 by n+1.
>
> I can't easily describe it, but the following function that works for
> n=3 should convey what I'm trying to do:
>
>
> f <- function(x){
>    matrix(c(
>     1           ,   0      ,   0 , 0,
> x[1]          ,   1      ,   0 , 0,
> x[1]*x[2]     , x[2]     ,   1 , 0,
> x[1]*x[2]*x[3], x[2]*x[3], x[3], 1
> ),
> 4,4,         byrow=T)
> }
>
> f(c(10,7,2))
>       [,1] [,2] [,3] [,4]
> [1,]    1    0    0    0
> [2,]   10    1    0    0
> [3,]   70    7    1    0
> [4,]  140   14    2    1
>
>
>
> As one goes down column "i", the entries get multiplied by successive
> elements of x,  starting with x[i], after the first "1"
>
> As one goes along a row, one takes a product of the tail end of x,
> until the zeroes kick in.
>
>
> Am I missing some clever solution?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From h.andersson at nioo.knaw.nl  Tue Jul 12 13:53:15 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Tue, 12 Jul 2005 13:53:15 +0200
Subject: [R] adding a factor column based on levels of another factor
In-Reply-To: <Pine.LNX.4.58.0507121157030.4900@euclea.sevenc.private>
References: <Pine.LNX.4.58.0507121157030.4900@euclea.sevenc.private>
Message-ID: <db0b1d$1ik$1@sea.gmane.org>

First create a dataframe with the translation you want, i.e.

one column with the species and another with the number you want in the end.

Then merge these two dataframes using 'merge' and voila..

I would start with looking at ?merge

Cheers, Henrik Andersson

Karen Kotschy wrote:
> Hi R users
> 
> Does anyone out there have a better/quicker way of adding a factor column
> to a data frame based on levels of another factor?
> 
> I have a (large) data frame consisting of records for individual plants,
> each represented by a unique ID number. The species of each plant is
> indicated in the column "species", which is a factor column with many
> different levels (species). There are multiple records for each species,
> and there is no pattern to the order in which the species names appear in
> the data frame.
> 
> e.g.
>    uniqueID species   elev   ht   diam
> 1  1        sp2       3.5    1.3  55
> 2  2        sp2       4.2    0.5  15
> 3  3        sp3       3.2    1.0  13
> 4  4        sp65      2.2    2.0  14
> 5  5        sp43      5.4    5.7  20
> 6  6        sp2       2.5    4.1  32
> 7  7        sp12      1.1    0.9  5
> 8  8        sp3       3.4    3.6  2
> 
> I would like to add a factor column to this data frame, indicating to
> which group each individual belongs. All individuals of the same species
> will belong to the same group.
> 
> Is there a quick way of saying "for all instances of species1, give the
> value 5, for all instances of species2, give the value 4, etc" (where 5
> and 4 are levels of a factor)?
> 
> The only way I can think of doing it is to split the data frame by
> species, then add a column to each subset showing the group, then
> re-join all the subsets. This seems clumsy and prone to errors. Anyone
> know a better way?
> 
> I've looked at expand.grid and gl but they don't seem to do what I want.
> 
> Thanks!
> 
> Karen Kotschy
> Centre for Water in the Environment
> University of the Witwatersrand
> Johannesburg
> South Africa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From ripley at stats.ox.ac.uk  Tue Jul 12 13:56:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 12:56:37 +0100 (BST)
Subject: [R] bug in chdir option of source
In-Reply-To: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>
References: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>
Message-ID: <Pine.LNX.4.61.0507121250500.1285@gannet.stats>

On Tue, 12 Jul 2005 usenet at s-boehringer.de wrote:

> I apologize for not having investigated enough.
>
> However I want to bring up the point that upgrading can be a very
> tedious thing. I administer a cluster for which upgrading brings it to a
> halt for several hours stopping all calculations going on. At the moment
> I have no way to go from 2.1.0 to 2.1.1.
>
> I just want to add this point to the release policy of R which
> historically has very quick release cycles which on the one hand is a
> very positive thing but on the other hand can be problematic with
> respect to some aspects.

I really don't understand.  Just install the current R in a different 
place and try it.  If it works better, use it for newly-started jobs.
We too run a large cluster (and a few hundred loosely connected machines) 
and are able to upgrade pretty transparently even when long jobs are 
running.

For those who are reluctant to upgrade, I would suggest never installing a 
.0 release (of any software, not just R).

>
> Thanks again, Stefan
>
>
> On Tue, 2005-07-12 at 13:02, Prof Brian Ripley wrote:
>> Please read the posting guide (as we do ask) and use the current version
>> as it asks to see if the `bug' has already been fixed.
>>
>> This was fixed a while ago, definitely in 2.1.1.
>>
>> On Tue, 12 Jul 2005 usenet at s-boehringer.de wrote:
>>
>>> I'm on R 2.1.0.
>>>
>>> In the "source" function there is a bug preventing the proper use of the
>>> chdir option (which simply doesn't work).
>>>
>>> The problem is that in the function the following line occurs:
>>> file <- file(file, "r", encoding = encoding)
>>>
>>> This overwrites the variable "file" and later causes the check
>>>    if (chdir && is.character(file) && (path <- dirname(file)) !=
>>>        ".")
>>> to fail.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul 12 13:58:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 12:58:40 +0100 (BST)
Subject: [R] Puzzled at ifelse()
In-Reply-To: <20050712100707.GA3175@lubyanka.local>
References: <20050712100707.GA3175@lubyanka.local>
Message-ID: <Pine.LNX.4.61.0507121247550.1285@gannet.stats>

This is working exactly as documented.  Nothing `breaks'!

What does the help page say?

      'ifelse' returns a value with the same shape as 'test' which is
      filled with elements selected from either 'yes' or 'no' depending
      on whether the element of 'test' is 'TRUE' or 'FALSE'.

'test' is of length one and true, so you got the first element of 'yes', 
which is a list as the first (and only) element of the answer.

On Tue, 12 Jul 2005, Ajay Narottam Shah wrote:

> I have a situation where this is fine:
>
>  > if (length(x)>15) {
>      clever <- rr.ATM(x, maxtrim=7)
>    } else {
>      clever <- rr.ATM(x)
>    }
>  > clever
>  $ATM
>  [1] 1848.929
>
>  $sigma
>  [1] 1.613415
>
>  $trim
>  [1] 0
>
>  $lo
>  [1] 1845.714
>
>  $hi
>  [1] 1852.143
>
> But this variant, using ifelse(), breaks:
>
>  > clever <- ifelse(length(x)>15, rr.ATM(x, maxtrim=7), rr.ATM(x))
>  > clever
>  [[1]]
>  [1] 1848.929
>
> What am I doing wrong?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Tue Jul 12 14:09:51 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 12 Jul 2005 08:09:51 -0400
Subject: [R] bug in chdir option of source
In-Reply-To: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>
References: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>
Message-ID: <42D3B30F.3050203@stats.uwo.ca>

usenet at s-boehringer.de wrote:
> I apologize for not having investigated enough.
> 
> However I want to bring up the point that upgrading can be a very
> tedious thing. I administer a cluster for which upgrading brings it to a
> halt for several hours stopping all calculations going on. At the moment
> I have no way to go from 2.1.0 to 2.1.1.

One simple solution is to test on a different system (e.g. a standalone 
one).  If the bug is present in 2.1.0 there but absent on 2.1.1, it's 
probably fixed on your cluster, too.

Another option which may or may not work is just to read the NEWS file 
for the newer releases.  It's not always obvious that a bug fix there 
fixes your own problem, but sometimes it is.
> 
> I just want to add this point to the release policy of R which
> historically has very quick release cycles which on the one hand is a
> very positive thing but on the other hand can be problematic with
> respect to some aspects.

It's hard to decide which versions to install.  On the one hand, if 
installs are difficult, you want to do it infrequently -- so maybe skip 
all the x.y.0 releases.  On the other hand, if you have unusual 
hardware, you are likely to see bugs that others don't see -- so 
installing the betas and the .0 releases will help to get your bugs fixed.

This is why it's useful to have two systems, one for "production", one 
for testing.  Then you can follow both policies.

Duncan Murdoch
> 
> Thanks again, Stefan
> 
> 
> On Tue, 2005-07-12 at 13:02, Prof Brian Ripley wrote:
> 
>>Please read the posting guide (as we do ask) and use the current version
>>as it asks to see if the `bug' has already been fixed.
>>
>>This was fixed a while ago, definitely in 2.1.1.
>>
>>On Tue, 12 Jul 2005 usenet at s-boehringer.de wrote:
>>
>>
>>>I'm on R 2.1.0.
>>>
>>>In the "source" function there is a bug preventing the proper use of the
>>>chdir option (which simply doesn't work).
>>>
>>>The problem is that in the function the following line occurs:
>>>file <- file(file, "r", encoding = encoding)
>>>
>>>This overwrites the variable "file" and later causes the check
>>>   if (chdir && is.character(file) && (path <- dirname(file)) !=
>>>       ".")
>>>to fail.
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Jul 12 14:10:50 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jul 2005 14:10:50 +0200
Subject: [R] bug in chdir option of source
In-Reply-To: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>
References: <48550.132.252.149.100.1121168978.squirrel@webmail.loomes.de>
Message-ID: <x23bqkcjhx.fsf@turmalin.kubism.ku.dk>

usenet at s-boehringer.de writes:

> I apologize for not having investigated enough.
> 
> However I want to bring up the point that upgrading can be a very
> tedious thing. I administer a cluster for which upgrading brings it to a
> halt for several hours stopping all calculations going on. At the moment
> I have no way to go from 2.1.0 to 2.1.1.

Hmm, I'll believe you, but the blame for that can be shifted in
various directions. 

> I just want to add this point to the release policy of R which
> historically has very quick release cycles which on the one hand is a
> very positive thing but on the other hand can be problematic with
> respect to some aspects.

Actually, we have quite slow release cycles compared to other
OpenSource projects. It's basically twice a year plus minor patches.
According to freshmeat.net, a project's "vitality" essentially drops
to zero a month after the latest release.

Now if only people would actually try the betas rather than reporting
errors after the release, we'd have a much better chance of avoiding
problems like yours...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From f.harrell at vanderbilt.edu  Tue Jul 12 14:34:35 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 12 Jul 2005 08:34:35 -0400
Subject: [R] Design: predict.lrm does not recognise lrm.fit object
In-Reply-To: <3.0.3.32.20050712111043.0085c2c0@popin.ncl.ac.uk>
References: <3.0.3.32.20050712111043.0085c2c0@popin.ncl.ac.uk>
Message-ID: <42D3B8DB.4030101@vanderbilt.edu>

Roy Sanderson wrote:
> Hello
> 
> I'm using logistic regression from the Design library (lrm), then fastbw to
> undertake a backward selection and create a reduced model, before trying to
> make predictions against an independent set of data using predict.lrm with
> the reduced model.  I wouldn't normally use this method, but I'm
> contrasting the results with an AIC/MMI approach.  The script contains:
> 
> # Determine full logistic regression
> lrm_logist = lrm(PresAbs ~ Size + X2ndpc + soil + AAR + tjan.jun,
> data=training)
> # Backward selection of variables in model
> lrm_stp = fastbw(lrm_logist, rule="p", sls=0.05)
> # Fit reduced model
> lrm_reduced = lrm.fit(training[,lrm_stp$parms.kept[-1]], training$PresAbs)
> # Predict using parameters from reduced model against a new dataset
> predict(lrm_reduced, testing, type="fitted.ind")

lrm.fit is used internally by lrm and related functions.  You'll need to 
use lrm.  Take lrm_stp$names.kept and create a formula - here is a first 
stab:

form <- as.formula(paste('PresAbs ~', 
paste(lrm_stp$names.kept,collapse='+')))
lrm_reduced <- lrm(form, ...)

This only works because you are forcing everything to be linear (highly 
  unlikely) and are not allowing any interactions.

Remember that is you use resampling to validate the model you need to 
start with the full list of candidate variables for each resample, 
having validate or calibrate functions repeat fastbw internally.

Frank

> 
> It is the last command that fails, reporting the error:
> Error in getOldDesign(fit): fit was not created with a Design library
> fitting function.
> 
> On further investigation, the class of the object from lrm.fit is only
> "lrm", whereas predict.lrm seems to be expecting objects with the class
> "lrm", "Design", "glm", judging from the examples on the predict.lrm help
> page.  Many of the attributes differ also.  Does anyone know a simple
> work-around for this problem?
> 
> Many thanks
> Roy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ligges at statistik.uni-dortmund.de  Tue Jul 12 14:35:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2005 14:35:42 +0200
Subject: [R] elegant matrix creation
In-Reply-To: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
References: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
Message-ID: <42D3B91E.8060907@statistik.uni-dortmund.de>

Robin Hankin wrote:


What about

foo <- function(a){
     n <- length(a)
     X <- diag(n+1)
     X[lower.tri(X)] <- unlist(lapply(seq(n),
         function(x) cumprod(c(1, a)[-seq(x)])))
     X
}

foo(c(10,7,2))


Uwe Ligges



> Hi
> 
> I want to write a little function that takes a vector of arbitrary
> length "n" and returns a matrix of size n+1 by n+1.
> 
> I can't easily describe it, but the following function that works for
> n=3 should convey what I'm trying to do:
> 
> 
> f <- function(x){
>    matrix(c(
>     1           ,   0      ,   0 , 0,
> x[1]          ,   1      ,   0 , 0,
> x[1]*x[2]     , x[2]     ,   1 , 0,
> x[1]*x[2]*x[3], x[2]*x[3], x[3], 1
> ),
> 4,4,         byrow=T)
> }
> 
> f(c(10,7,2))
>       [,1] [,2] [,3] [,4]
> [1,]    1    0    0    0
> [2,]   10    1    0    0
> [3,]   70    7    1    0
> [4,]  140   14    2    1
>  >
> 
> 
> As one goes down column "i", the entries get multiplied by successive
> elements of x,  starting with x[i], after the first "1"
> 
> As one goes along a row, one takes a product of the tail end of x,
> until the zeroes kick in.
> 
> 
> Am I missing some clever solution?
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Jul 12 14:35:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 08:35:42 -0400
Subject: [R] elegant matrix creation
In-Reply-To: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
References: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
Message-ID: <971536df050712053573e60ec9@mail.gmail.com>

On 7/12/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
> 
> I want to write a little function that takes a vector of arbitrary
> length "n" and returns a matrix of size n+1 by n+1.
> 
> I can't easily describe it, but the following function that works for
> n=3 should convey what I'm trying to do:
> 
> 
> f <- function(x){
>   matrix(c(
>    1           ,   0      ,   0 , 0,
> x[1]          ,   1      ,   0 , 0,
> x[1]*x[2]     , x[2]     ,   1 , 0,
> x[1]*x[2]*x[3], x[2]*x[3], x[3], 1
> ),
> 4,4,         byrow=T)
> }
> 
> f(c(10,7,2))
>      [,1] [,2] [,3] [,4]
> [1,]    1    0    0    0
> [2,]   10    1    0    0
> [3,]   70    7    1    0
> [4,]  140   14    2    1
>  >
> 
> 
> As one goes down column "i", the entries get multiplied by successive
> elements of x,  starting with x[i], after the first "1"
> 
> As one goes along a row, one takes a product of the tail end of x,
> until the zeroes kick in.

I have not checked this generally but at least for
the 4x4 case its inverse is 0 except for 1s on the 
diagonal and -x on the subdiagonal.  We can use
diff on a diagonal matrix to give a matrix with
a diagonal and superdiagonal and then massage that
into the required form, invert and round --
leave off the rounding if the components of x
are not known to be integer.

round(solve(diag(4) - t(diff(diag(5))[,1:4])+diag(4) * c(0,x)))



From allan_sta_staff_sci_main_uct at mail.uct.ac.za  Tue Jul 12 15:11:32 2005
From: allan_sta_staff_sci_main_uct at mail.uct.ac.za (allan_sta_staff_sci_main_uct@mail.uct.ac.za)
Date: Tue, 12 Jul 2005 15:11:32 +0200
Subject: [R] R: to the power
Message-ID: <1121173892.42d3c1848e058@webmail.uct.ac.za>

hi all

why does R do this:

(-8)^(1/3)=NaN

the answer should be : -2

a silly question but i kept on getting errors in some of my code due to this
problem.

i solve the problem as follows:

say we want : (-a)^(1/3)

then : sign(a)*(a^(1/3)) works

but there has to be a simpler way of soing such a simple mathematical operation.

thanking you
/
allan



From ripley at stats.ox.ac.uk  Tue Jul 12 15:11:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Jul 2005 14:11:56 +0100 (BST)
Subject: [R] : Re:  Dispersion in glm (was (no subject))
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F9ED@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F9ED@MS-DT01VS01.tsn.tno.nl>
Message-ID: <Pine.LNX.4.61.0507121409340.7064@gannet.stats>

Actually, glm() does not estimate the dispersion at all, so you will need
to be more specific.

For example, summary.glm() and predict.glm() use the Pearson statistic if 
dispersion=NULL (the default) for most families.  You can supply any other 
value you choose, and the MASS package makes use of this for ML estimation 
of the dispersion parameter (related to the shape) of the gamma family.

There are rather good reasons (serious bias) not to use the deviance
estimate in the binomial and Poisson families (see the example plots in
MASS4), and good reasons not to use either in the gamma family.  As the
Pearson and deviance estimates agree for the gaussian, that does leave
begging the question of why you want to do this.  Further, McCullagh &
Nelder have general arguments why the Pearson estimate might always be
preferred to the deviance one.  So that `another statastical package' 
appears to need justification for its choice.


On Mon, 11 Jul 2005, Smit, R. (Robin) wrote:

> The estimate of glm dispersion can be based on the deviance or on the
> Pearson statistic.
> I have compared output from R glm() to another statastical package and
> it appears that R uses the Pearson statistic.
> I was wondering if it is possible to make use R the deviance instead by
> modifying the glm(...) function?
> Thanks for your attention.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thewavyx at gmail.com  Tue Jul 12 15:16:21 2005
From: thewavyx at gmail.com (Eric Rodriguez)
Date: Tue, 12 Jul 2005 15:16:21 +0200
Subject: [R] Sample Datasets for binary classifiers
Message-ID: <47779aab050712061677abd6fc@mail.gmail.com>

Hi,

I hope I'm not totally Off Topic, but I'm actually working on binary
classifier with probabilistic output [0 -> 1]. I tested my methods
with some sample datasets from UCI Database but I'm still in need of
some samples. Especially, I'm looking for binary datasets with a
probabilistic information of belonging to one of the two classes.
for example, not only values X_1..X_n  associated with a class Y = (C1
or C2) but rather the probability of belonging to class C2  (or
equivalently C1).
Maybe I could generate this probabilistic Y with some bayesian network
but I was wondering if there are other datasets or ideas that could
help me.

Thanks and don't hesitate to ask extra information if my request is
not understandable.

Eric



From sean.oriordain at gmail.com  Tue Jul 12 15:18:57 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Tue, 12 Jul 2005 13:18:57 +0000
Subject: [R] R: to the power
In-Reply-To: <1121173892.42d3c1848e058@webmail.uct.ac.za>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
Message-ID: <8ed68eed0507120618568af340@mail.gmail.com>

> (-8+0i)^(1/3)
[1] 1+1.732051i

ie complex...

On 12/07/05, allan_sta_staff_sci_main_uct at mail.uct.ac.za
<allan_sta_staff_sci_main_uct at mail.uct.ac.za> wrote:
> hi all
> 
> why does R do this:
> 
> (-8)^(1/3)=NaN
> 
> the answer should be : -2
> 
> a silly question but i kept on getting errors in some of my code due to this
> problem.
> 
> i solve the problem as follows:
> 
> say we want : (-a)^(1/3)
> 
> then : sign(a)*(a^(1/3)) works
> 
> but there has to be a simpler way of soing such a simple mathematical operation.
> 
> thanking you
> /
> allan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From r.hankin at noc.soton.ac.uk  Tue Jul 12 15:29:48 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 12 Jul 2005 14:29:48 +0100
Subject: [R] R: to the power
In-Reply-To: <1121173892.42d3c1848e058@webmail.uct.ac.za>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
Message-ID: <1C67C1C0-84A1-4C54-9029-A1258167D128@soc.soton.ac.uk>

Hi

I find that one often needs to keep reals real and complexes complex.

Try this:

"cuberooti" <-
   function (x)
{
   if (is.complex(x)) {
     return(sqrt(x + (0+0i)))
   }
   sign(x)*  abs(x)^(1/3)
}


best wishes

[see that (0+0i) sitting there!]

Robin



On 12 Jul 2005, at 14:11, allan_sta_staff_sci_main_uct at mail.uct.ac.za  
wrote:

> hi all
>
> why does R do this:
>
> (-8)^(1/3)=NaN
>
> the answer should be : -2
>
> a silly question but i kept on getting errors in some of my code  
> due to this
> problem.
>
> i solve the problem as follows:
>
> say we want : (-a)^(1/3)
>
> then : sign(a)*(a^(1/3)) works
>
> but there has to be a simpler way of soing such a simple  
> mathematical operation.
>
> thanking you
> /
> allan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From r.hankin at noc.soton.ac.uk  Tue Jul 12 15:39:42 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 12 Jul 2005 14:39:42 +0100
Subject: [R] elegant matrix creation
In-Reply-To: <971536df050712053573e60ec9@mail.gmail.com>
References: <CCB5B6A1-DC6B-4C3D-A782-C0F407B0FF48@soc.soton.ac.uk>
	<971536df050712053573e60ec9@mail.gmail.com>
Message-ID: <0396AEFA-6748-4D3D-B781-288673D416B6@soc.soton.ac.uk>

Gabor


I cannot begin to tell you how much value you have added to my research
with your observation.

A real "eureka" moment for me.

[oh, and it  answered my question as well]


kia ora


Robin


On 12 Jul 2005, at 13:35, Gabor Grothendieck wrote:

> On 7/12/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
>
>> Hi
>>
>> I want to write a little function that takes a vector of arbitrary
>> length "n" and returns a matrix of size n+1 by n+1.
>>
>> I can't easily describe it, but the following function that works for
>> n=3 should convey what I'm trying to do:
>>
>>
>> f <- function(x){
>>   matrix(c(
>>    1           ,   0      ,   0 , 0,
>> x[1]          ,   1      ,   0 , 0,
>> x[1]*x[2]     , x[2]     ,   1 , 0,
>> x[1]*x[2]*x[3], x[2]*x[3], x[3], 1
>> ),
>> 4,4,         byrow=T)
>> }
>>
>> f(c(10,7,2))
>>      [,1] [,2] [,3] [,4]
>> [1,]    1    0    0    0
>> [2,]   10    1    0    0
>> [3,]   70    7    1    0
>> [4,]  140   14    2    1
>>
>>>
>>>
>>
>>
>> As one goes down column "i", the entries get multiplied by successive
>> elements of x,  starting with x[i], after the first "1"
>>
>> As one goes along a row, one takes a product of the tail end of x,
>> until the zeroes kick in.
>>
>
> I have not checked this generally but at least for
> the 4x4 case its inverse is 0 except for 1s on the
> diagonal and -x on the subdiagonal.  We can use
> diff on a diagonal matrix to give a matrix with
> a diagonal and superdiagonal and then massage that
> into the required form, invert and round --
> leave off the rounding if the components of x
> are not known to be integer.
>
> round(solve(diag(4) - t(diff(diag(5))[,1:4])+diag(4) * c(0,x)))
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From murdoch at stats.uwo.ca  Tue Jul 12 15:51:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 12 Jul 2005 09:51:36 -0400
Subject: [R] R: to the power
In-Reply-To: <1C67C1C0-84A1-4C54-9029-A1258167D128@soc.soton.ac.uk>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
	<1C67C1C0-84A1-4C54-9029-A1258167D128@soc.soton.ac.uk>
Message-ID: <42D3CAE8.7080209@stats.uwo.ca>

On 7/12/2005 9:29 AM, Robin Hankin wrote:
> Hi
> 
> I find that one often needs to keep reals real and complexes complex.
> 
> Try this:
> 
> "cuberooti" <-
>    function (x)
> {
>    if (is.complex(x)) {
>      return(sqrt(x + (0+0i)))
>    }
>    sign(x)*  abs(x)^(1/3)
> }
> 
> 
> best wishes
> 
> [see that (0+0i) sitting there!]

I don't understand this.

1.  I don't think you meant to use sqrt() there, did you??

2.  What effect does the 0+0i have?  x has already been determined to be 
complex.

Duncan Murdoch



From allan_sta_staff_sci_main_uct at mail.uct.ac.za  Tue Jul 12 15:53:56 2005
From: allan_sta_staff_sci_main_uct at mail.uct.ac.za (allan_sta_staff_sci_main_uct@mail.uct.ac.za)
Date: Tue, 12 Jul 2005 15:53:56 +0200
Subject: [R] R: to the power
In-Reply-To: <42D3CAE8.7080209@stats.uwo.ca>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
	<1C67C1C0-84A1-4C54-9029-A1258167D128@soc.soton.ac.uk>
	<42D3CAE8.7080209@stats.uwo.ca>
Message-ID: <1121176436.42d3cb747c8cf@webmail.uct.ac.za>

hi all

i simply wanted to work with real numbers and thought that (-8)^(1/3) should
work.

sorry for not making the question clearer.

/
allan

Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:

> On 7/12/2005 9:29 AM, Robin Hankin wrote:
> > Hi
> >
> > I find that one often needs to keep reals real and complexes complex.
> >
> > Try this:
> >
> > "cuberooti" <-
> >    function (x)
> > {
> >    if (is.complex(x)) {
> >      return(sqrt(x + (0+0i)))
> >    }
> >    sign(x)*  abs(x)^(1/3)
> > }
> >
> >
> > best wishes
> >
> > [see that (0+0i) sitting there!]
>
> I don't understand this.
>
> 1.  I don't think you meant to use sqrt() there, did you??
>
> 2.  What effect does the 0+0i have?  x has already been determined to be
> complex.
>
> Duncan Murdoch
>



From r.hankin at noc.soton.ac.uk  Tue Jul 12 16:01:00 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 12 Jul 2005 15:01:00 +0100
Subject: [R] R: to the power
In-Reply-To: <42D3CAE8.7080209@stats.uwo.ca>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
	<1C67C1C0-84A1-4C54-9029-A1258167D128@soc.soton.ac.uk>
	<42D3CAE8.7080209@stats.uwo.ca>
Message-ID: <BDF2FDAD-31DA-449D-AAAA-988957C7AD77@soc.soton.ac.uk>


On 12 Jul 2005, at 14:51, Duncan Murdoch wrote:

> On 7/12/2005 9:29 AM, Robin Hankin wrote:
>

[bogus function snipped]

> I don't understand this.
>
> 1.  I don't think you meant to use sqrt() there, did you??
>
> 2.  What effect does the 0+0i have?  x has already been determined  
> to be
> complex.
>
> Duncan Murdoch
>

Er, guilty on all counts.  I plead the hot weather.

I intended to modify and post the following function
for cube roots.


"sqrti" <-
   function (x)
{
   if (is.complex(x)) {
     return(sqrt(x))
   }
   if (any(x < 0)) {
     return(sqrt(x + 0i))
   }
   return(sqrt(x))
}

[which deals with square roots in a consistent way, handling positive  
reals,
and negative reals, and complexes, in a nice way]

sorry about that




rksh


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From murdoch at stats.uwo.ca  Tue Jul 12 16:07:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 12 Jul 2005 10:07:22 -0400
Subject: [R] R: to the power
In-Reply-To: <1121176436.42d3cb747c8cf@webmail.uct.ac.za>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
	<1C67C1C0-84A1-4C54-9029-A1258167D128@soc.soton.ac.uk>
	<42D3CAE8.7080209@stats.uwo.ca>
	<1121176436.42d3cb747c8cf@webmail.uct.ac.za>
Message-ID: <42D3CE9A.7080309@stats.uwo.ca>

On 7/12/2005 9:53 AM, allan_sta_staff_sci_main_uct at mail.uct.ac.za wrote:
> hi all
> 
> i simply wanted to work with real numbers and thought that (-8)^(1/3) should
> work.

It might work in an ideal world, but not in the R floating point world. 
  There's no way to express (1/3) exactly.   Since (-8)^(1/3 + epsilon) 
(restricted to the reals) is not defined for epsilon near zero but not 
exactly zero, it's really hopeless to expect R to give you what you wanted.

*You* know that you're taking an odd root of a negative number, but R 
doesn't.  You need to use your knowledge to rewrite that mathematical 
expression as the mathematically equivalent -(8^(1/3)) and then things 
will be fine.

Duncan Murdoch



> 
> sorry for not making the question clearer.
> 
> /
> allan
> 
> Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:
> 
>> On 7/12/2005 9:29 AM, Robin Hankin wrote:
>> > Hi
>> >
>> > I find that one often needs to keep reals real and complexes complex.
>> >
>> > Try this:
>> >
>> > "cuberooti" <-
>> >    function (x)
>> > {
>> >    if (is.complex(x)) {
>> >      return(sqrt(x + (0+0i)))
>> >    }
>> >    sign(x)*  abs(x)^(1/3)
>> > }
>> >
>> >
>> > best wishes
>> >
>> > [see that (0+0i) sitting there!]
>>
>> I don't understand this.
>>
>> 1.  I don't think you meant to use sqrt() there, did you??
>>
>> 2.  What effect does the 0+0i have?  x has already been determined to be
>> complex.
>>
>> Duncan Murdoch
>>
> 
> 
> 
> 
> ----------------------------------------------------------------
> This message was sent using IMP, the Internet Messaging Program.



From davidr at rhotrading.com  Tue Jul 12 16:16:55 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Tue, 12 Jul 2005 09:16:55 -0500
Subject: [R] R: to the power
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FBB50@rhosvr02.rhotrading.com>

In general, x^y is evaluated as exp(y*log(x)). In your case, x is
negative, so log(x) is NaN. Note also that 1/3 is not represented
exactly in your computer anyway, so you would not get an exact cube root
this way; e.g.:

R> format((1234567891112^3)^(1/3),digits=16)
[1] "1234567891112.001"

(Probably a bad example, but you get the idea.)

In general, sign(x)*(abs(x)^(1/3)) is the way to go for cube roots.

David L. Reiner
 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of
> allan_sta_staff_sci_main_uct at mail.uct.ac.za
> Sent: Tuesday, July 12, 2005 8:54 AM
> To: Duncan Murdoch
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R: to the power
> 
> hi all
> 
> i simply wanted to work with real numbers and thought that (-8)^(1/3)
> should
> work.
> 
> sorry for not making the question clearer.
> 
> /
> allan
> 
> Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:
> 
> > On 7/12/2005 9:29 AM, Robin Hankin wrote:
> > > Hi
> > >
> > > I find that one often needs to keep reals real and complexes
complex.
> > >
> > > Try this:
> > >
> > > "cuberooti" <-
> > >    function (x)
> > > {
> > >    if (is.complex(x)) {
> > >      return(sqrt(x + (0+0i)))
> > >    }
> > >    sign(x)*  abs(x)^(1/3)
> > > }
> > >
> > >
> > > best wishes
> > >
> > > [see that (0+0i) sitting there!]
> >
> > I don't understand this.
> >
> > 1.  I don't think you meant to use sqrt() there, did you??
> >
> > 2.  What effect does the 0+0i have?  x has already been determined
to be
> > complex.
> >
> > Duncan Murdoch
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ecoinformatics at gmail.com  Tue Jul 12 16:22:38 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Tue, 12 Jul 2005 16:22:38 +0200
Subject: [R] question for IF ELSE usage
Message-ID: <15f8e67d050712072240fa32e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/d67c3455/attachment.pl

From dimitrijoe at yahoo.com.br  Tue Jul 12 16:24:00 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Tue, 12 Jul 2005 11:24:00 -0300
Subject: [R] vectorizaton
Message-ID: <003201c586ed$5adde5b0$1600a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/f83b063c/attachment.pl

From andy_liaw at merck.com  Tue Jul 12 16:38:02 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 12 Jul 2005 10:38:02 -0400
Subject: [R] vectorizaton
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA8A@usctmx1106.Merck.com>

Try:

mymat <- rowMeans(P, dims=2)

Andy

> From: Dimitri Joe
> 
> Hi,
> 
> I got 1000 NxN matrices grouped in one array. I want one 
> matrix in which p_ij is the average of all the 1000 matrices 
> in the array. Here's what I'm trying to do:
> 
> # P is the NxNx1000 array
> 
> for(i in 1:N)
> for(j in 1:N)
> for(k in 1: 1000)
> mymat[ i, j ] <- mean( P [i , j , k ]  )
> 
> Otherwise, I could have a NxNx1000 vector, and get the N^2 
> means of the 1+ (N^2)*(0: 999) elements. I don't know which 
> is more efficient, but I wouldn't know how to carry this last 
> solution out anyway. Any ideas on how to do this efficiently?
> 
> Thanks,
> 
> Dimitri
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dave at kanecap.com  Tue Jul 12 16:43:33 2005
From: dave at kanecap.com (David Kane)
Date: Tue, 12 Jul 2005 10:43:33 -0400
Subject: [R] Determining response variable in a formula
Message-ID: <17107.55061.616922.514532@gargle.gargle.HOWL>

I have a formula from which I want to deduce the name of the response
variable. One way of doing so is as follows:

> my.form <- as.formula("y ~ x + z")
> all.vars(my.form)[1]
[1] "y"
> 

Is there a better way and/or preferrred method of determining "y" from
my.form than this one? In messing around with terms, I came up with:

> all.vars(terms(my.form))[attr(terms(my.form), "response")]
[1] "y"
> 

But that seems too ugly to be best.

Thanks,

Dave Kane

> R.version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R                
>



From conrad.halling at bifx.org  Tue Jul 12 16:45:20 2005
From: conrad.halling at bifx.org (Conrad Halling)
Date: Tue, 12 Jul 2005 10:45:20 -0400
Subject: [R] Off topic -2 Ln Lambda and Chi square
In-Reply-To: <BAY105-F36AD4E66B47D00F0DFD5A0D6DD0@phx.gbl>
References: <BAY105-F36AD4E66B47D00F0DFD5A0D6DD0@phx.gbl>
Message-ID: <42D3D780.8060102@bifx.org>

This is a theorem for maximum likelihood tests. See:

Theorem 12.2 (presented without proof), page 391,  in "John E. Freund's 
Mathematical Statistics with Applications", Seventh Edition, by Irwin 
Miller and Marylees Miller. Upper Saddle River, N.J.: Pearson Prentice 
Hall, 2004.

    Theorem 12.2: For large n, the distribution of -2 ln Lambda 
approaches, under very general conditions, the chi-square distribution 
with 1 degree of freedom.

Theorem 6.3.1 (given with a proof), p. 335, in "Introduction to 
Mathematical Statistics", Sixth Edition, by RV Hogg, JW McKean, and AT 
Craig. 2005. Upper Saddle River, New Jersey: Pearson Prentice Hall.

Proofs are also given in:

Testing Statistical Hypotheses, Second Edition, by E. L. Lehmann. New 
York: John Wiley and Sons, Inc., 1986

and

Mathematical Statistics, by S. S. Wilkes. New York: John Wiley and Sons, 
Inc., 1962.

-- Conrad

Laura Holt wrote:

>Dear R :
>
>Sorry for the off topic question, but does anyone know the reference for
>the -2 Ln Lambda following a Chi Square distribution, please?
>
>Possibly one of Bartlett's?
>
>Thanks in advance!
>
>Sincerely,
>Laura Holt
>mailto: lauraholt_983 at hotmail.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>

-- 
Conrad Halling
conrad.halling at bifx.org



From mschwartz at mn.rr.com  Tue Jul 12 16:46:43 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 12 Jul 2005 09:46:43 -0500
Subject: [R] question for IF ELSE usage
In-Reply-To: <15f8e67d050712072240fa32e2@mail.gmail.com>
References: <15f8e67d050712072240fa32e2@mail.gmail.com>
Message-ID: <1121179603.4193.13.camel@localhost.localdomain>

On Tue, 2005-07-12 at 16:22 +0200, ecoinfo wrote:
> Hi R users,
>  Maybe the question is too simple.
>  In a IF ... ELSE ... statement "if(cond) cons.expr else alt.expr", IF and 
> ELSE should be at the same line? 
> For example,
>  if (x1==12)
> {
> y1 <- 5 
> }else
> {
> y1 <- 3
> }
>  is right, while
>  if (x1==12)
> {
> y1 <- 5 
> }
> else # Error: syntax error
> {
> y1 <- 3
> }
>  is wrong? 
>  Thanks

Note the following from the Details section of ?"if"

"Note that it is a common mistake to forget to put braces ({ .. })
around your statements, e.g., after if(..) or for(....). In particular,
you should not have a newline between } and else to avoid a syntax error
in entering a if ... else construct at the keyboard or via source. For
that reason, one (somewhat extreme) attitude of defensive programming is
to always use braces, e.g., for if clauses."


One other approach is the following:

if (x1 == 12) 
{
  y1 <- 5 
} else {
  y1 <- 3
}

Note the presence of both braces on the 'else' line.

HTH,

Marc Schwartz



From koen.hufkens at telenet.be  Tue Jul 12 16:51:18 2005
From: koen.hufkens at telenet.be (koen.hufkens@pandora.be)
Date: Tue, 12 Jul 2005 14:51:18 +0000
Subject: [R] Complex plotting in R
Message-ID: <W7985627828283501121179878@kostunerix.telenet-ops.be>

Hi list,

I'm looking for a function or a combination of functions to do panel plotting of mixed graph types with the same x axis.

I would like to construct a panel with 3 stacked windows with on top a histogram, below that 2 cdf plots. They all have the same x axis value but different y axis values. Is it possible to construct something like that?

I've looked into the lattice package but it doesn't seem to solve my problem because I clearly want 1 x axis for all the graphs so not just a panel with 3 full graphs.

Something like this:

http://www.ats.ucla.edu/stat/sas/faq/InfluencePlots6.gif

(copyright to whom copyright is due...)

but with the top and bottom graphs merged.

Best regards,
Koen Hufkens



From ecoinformatics at gmail.com  Tue Jul 12 16:52:42 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Tue, 12 Jul 2005 16:52:42 +0200
Subject: [R] question for IF ELSE usage
In-Reply-To: <1121179603.4193.13.camel@localhost.localdomain>
References: <15f8e67d050712072240fa32e2@mail.gmail.com>
	<1121179603.4193.13.camel@localhost.localdomain>
Message-ID: <15f8e67d05071207525cc60b28@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/7b25a84e/attachment.pl

From sundar.dorai-raj at pdf.com  Tue Jul 12 16:58:35 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 12 Jul 2005 09:58:35 -0500
Subject: [R] Determining response variable in a formula
In-Reply-To: <17107.55061.616922.514532@gargle.gargle.HOWL>
References: <17107.55061.616922.514532@gargle.gargle.HOWL>
Message-ID: <42D3DA9B.3010505@pdf.com>



David Kane wrote:
> I have a formula from which I want to deduce the name of the response
> variable. One way of doing so is as follows:
> 
> 
>>my.form <- as.formula("y ~ x + z")
>>all.vars(my.form)[1]
> 
> [1] "y"
> 
> 
> Is there a better way and/or preferrred method of determining "y" from
> my.form than this one? In messing around with terms, I came up with:
> 
> 
>>all.vars(terms(my.form))[attr(terms(my.form), "response")]
> 
> [1] "y"
> 
> 
> But that seems too ugly to be best.
> 
> Thanks,
> 
> Dave Kane
> 
> 
>>R.version
> 
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    2                
> minor    1.0              
> year     2005             
> month    04               
> day      18               
> language R                
> 
> 

David,

Using all.vars as you have might be dangerous if you ever encounter a 
one sided formula. For example,

all.vars(y ~ x)[1] # "y"
all.vars(~x)[1] # "x"

You might want to look at nlme::getResponseFormula or I wrote a function 
a while back:

parse.formula <- function(formula) {
   vars <- terms(as.formula(formula))
   y <- if(attr(vars, "response"))
     nlme::getResponseFormula(formula)
   x <- nlme::getCovariateFormula(formula)
   z <- nlme::getGroupsFormula(formula)
   list(response = all.vars(y),
        covariates = all.vars(x),
        groups = all.vars(z))
}

parse.formula(y ~ x)$response # "y"
parse.formula( ~ x)$response # character(0)


HTH,

--sundar



From olivier.celhay at gmail.com  Tue Jul 12 17:03:20 2005
From: olivier.celhay at gmail.com (Olivier Celhay)
Date: Tue, 12 Jul 2005 17:03:20 +0200
Subject: [R] SOS Boosting
Message-ID: <9b6ea3690507120803198cb68f@mail.gmail.com>

Hi,

I am trying to implement the Adaboost.M1. algorithm as described in
"The Elements of Statistical Learning" p.301
I don't use Dtettling 's library "boost" because :
  - I don't understande the difference beetween Logitboost and L2boost
  - I 'd like to use larger trees than stumps.

By using option weights set to (1/n, 1/n, ..., 1/n) in rpart or tree
function, the tree obtained is trivial (just root, no split) whereas
without weight or for each weight >1,trees are just fine.

So here is my question : how are weights taken into account in optimal
tree's discovery ?
Did someone implement boosting algorithm ?

Regards,

Olivier Celhay   -  Student  -  Paris, France



From 0034058 at fudan.edu.cn  Tue Jul 12 17:00:33 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 12 Jul 2005 23:00:33 +0800
Subject: [R] Determining response variable in a formula
Message-ID: <0IJI00NKDRX81Z@mail.fudan.edu.cn>

is it what you want?
>dat<-data.frame(x=rnorm(10),y=rnorm(10),z=rnorm(10))
> my.form <- as.formula(y ~ x + z)
> my.form
y ~ x + z
> m<-model.frame(my.form,data=dat)
> model.extract(m,"response")
         1          2          3          4          5          6          7 
-0.3434826  1.0145622 -0.4749584  0.4018080 -0.3039126 -0.8180650  0.5455521 
         8          9         10 
 1.1460328  0.8038568  1.1092655 

======= 2005-07-12 22:43:33 =======

>I have a formula from which I want to deduce the name of the response
>variable. One way of doing so is as follows:
>
>> my.form <- as.formula("y ~ x + z")
>> all.vars(my.form)[1]
>[1] "y"
>> 
>
>Is there a better way and/or preferrred method of determining "y" from
>my.form than this one? In messing around with terms, I came up with:
>
>> all.vars(terms(my.form))[attr(terms(my.form), "response")]
>[1] "y"
>> 
>
>But that seems too ugly to be best.
>
>Thanks,
>
>Dave Kane
>
>> R.version
>         _                
>platform i686-pc-linux-gnu
>arch     i686             
>os       linux-gnu        
>system   i686, linux-gnu  
>status                    
>major    2                
>minor    1.0              
>year     2005             
>month    04               
>day      18               
>language R                
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-07-12

------
Deparment of Sociology
Fudan University

Blog:http://sociology.yculblog.com



From carsten.steinhoff at stud.uni-goettingen.de  Tue Jul 12 17:49:34 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Tue, 12 Jul 2005 17:49:34 +0200
Subject: [R] three par. fitting with fitdistr
Message-ID: <E1DsN0s-0000DT-Ny@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/c9871961/attachment.pl

From buser at stat.math.ethz.ch  Tue Jul 12 17:52:44 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 12 Jul 2005 17:52:44 +0200
Subject: [R] adding a factor column based on levels of another factor
In-Reply-To: <Pine.LNX.4.58.0507121157030.4900@euclea.sevenc.private>
References: <Pine.LNX.4.58.0507121157030.4900@euclea.sevenc.private>
Message-ID: <17107.59212.49475.313409@stat.math.ethz.ch>

Hi Karen 

I am not sure if I understand correctly your question. If no,
please ignore this answer.
Do you want a new factor "group" which contains the same
information like "species", just with other names, e.g
1,2,... or "A","B",... ?

If yes you can do it like this

## Your data.frame (without ht & diam)
dat <- data.frame(uniqueID = factor(1:8),
                  species = c("sp2", "sp2", "sp3", "sp65", "sp43", "sp2",
                    "sp12", "sp3"),
                  elev = c(3.5, 4.2, 3.2, 2.2, 5.4, 2.5, 1.1, 3.4))
str(dat)
## new factor group (copy of species)
dat[,"group"] <- dat[,"species"]
## rename the levels into "1", "2", ... or whatever you want:
levels(dat[,"group"]) <- list("3" = "sp12", "4" = "sp2", "2" = "sp3",
                              "5" = "sp43", "1" = "sp65")
## control
dat[,"species"]
dat[,"group"]

Please be careful. This only changes the labels into 1,2,...
If you use for example 

as.numeric(dat[,"group"])

you will get the values that are behind the original
alphabetical ordering, meaning "sp12" is 1, "sp2" is 2, etc.
You can change this, too if necessary, using as.character() and
as.numeric() as well.

I hope this is helpful fro your problem.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Karen Kotschy writes:
 > Hi R users
 > 
 > Does anyone out there have a better/quicker way of adding a factor column
 > to a data frame based on levels of another factor?
 > 
 > I have a (large) data frame consisting of records for individual plants,
 > each represented by a unique ID number. The species of each plant is
 > indicated in the column "species", which is a factor column with many
 > different levels (species). There are multiple records for each species,
 > and there is no pattern to the order in which the species names appear in
 > the data frame.
 > 
 > e.g.
 >    uniqueID species   elev   ht   diam
 > 1  1        sp2       3.5    1.3  55
 > 2  2        sp2       4.2    0.5  15
 > 3  3        sp3       3.2    1.0  13
 > 4  4        sp65      2.2    2.0  14
 > 5  5        sp43      5.4    5.7  20
 > 6  6        sp2       2.5    4.1  32
 > 7  7        sp12      1.1    0.9  5
 > 8  8        sp3       3.4    3.6  2
 > 
 > I would like to add a factor column to this data frame, indicating to
 > which group each individual belongs. All individuals of the same species
 > will belong to the same group.
 > 
 > Is there a quick way of saying "for all instances of species1, give the
 > value 5, for all instances of species2, give the value 4, etc" (where 5
 > and 4 are levels of a factor)?
 > 
 > The only way I can think of doing it is to split the data frame by
 > species, then add a column to each subset showing the group, then
 > re-join all the subsets. This seems clumsy and prone to errors. Anyone
 > know a better way?
 > 
 > I've looked at expand.grid and gl but they don't seem to do what I want.
 > 
 > Thanks!
 > 
 > Karen Kotschy
 > Centre for Water in the Environment
 > University of the Witwatersrand
 > Johannesburg
 > South Africa
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Max.Kuhn at pfizer.com  Tue Jul 12 18:03:31 2005
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 12 Jul 2005 12:03:31 -0400
Subject: [R] SOS Boosting
Message-ID: <71257D09F114DA4A8E134DEAC70F25D30288549E@groamrexm03.amer.pfizer.com>

>Hi,
>
>I am trying to implement the Adaboost.M1. algorithm as described in
>"The Elements of Statistical Learning" p.301
>I don't use Dtettling 's library "boost" because :
>  - I don't understande the difference beetween Logitboost and L2boost
>  - I 'd like to use larger trees than stumps.
>

It also doesn't have a predict function, which is why I don't use it much.

>By using option weights set to (1/n, 1/n, ..., 1/n) in rpart or tree
>function, the tree obtained is trivial (just root, no split) whereas
>without weight or for each weight >1,trees are just fine.
>
>So here is my question : how are weights taken into account in optimal
>tree's discovery ?
>Did someone implement boosting algorithm ?

Check out the gbm package. It is fairly close to MART in the reference 
you mentioned. To get to adaboost with stumps, you should look at the 
arguments 

  distribution = "adaboost"
  interaction.depth =  1

To get more information see ?gbm (if you have it installed) or the 
file gbm.pdf in the doc directory of the library.

Max

>
>Regards,
>
>Olivier Celhay   -  Student  -  Paris, France


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From maechler at stat.math.ethz.ch  Tue Jul 12 18:13:33 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 12 Jul 2005 18:13:33 +0200
Subject: [R] three par. fitting with fitdistr
In-Reply-To: <E1DsN0s-0000DT-Ny@s2.stud.uni-goettingen.de>
References: <E1DsN0s-0000DT-Ny@s2.stud.uni-goettingen.de>
Message-ID: <17107.60461.558439.457253@stat.math.ethz.ch>

>>>>> "Carsten" == Carsten Steinhoff <carsten.steinhoff at stud.uni-goettingen.de>
>>>>>     on Tue, 12 Jul 2005 17:49:34 +0200 writes:

    Carsten> Hello,

    Carsten> I want to fit a tree parameter distribution to
    Carsten> given data. I tried it with sample data using the
    Carsten> "fitdistr" function.
 
    Carsten> Here my workflow that didn't had any result:
 
    Carsten> I started with the generalized gamma distr, which is:
    Carsten> r*dgamma(x^r,shape,rate)
 
    Carsten> The R-function is: 
 
    Carsten> ggamma = function (x,r,shape,rate) r*dgamma(x^r,shape,rate=rate)
 
    Carsten> For the first step I assumed r = 1 and I generated
    Carsten> random numbers with the "standard" Gamma distr.
 
    Carsten> rn=rgamma(1000,10,5)
 
    Carsten> In the last step I want to reconstruct the parameters from the dataset:
 
    Carsten> library(MASS)
    Carsten> fitdistr(rn, ggamma, list(r=1,shape=10,rate=5))
 
 
    Carsten> But there is an error: Error in fitdistr(rn, ggamma, list(r = 1, shape = 10,
    Carsten> rate = 5)) :  optimization failed
    Carsten> although I should have a nearly model-made dataset.
 
    Carsten> Where is the problem and how could it be solved?

ggamma() is not a density.
You need to provide the correct multiplication factor (a
function of 'r' at least) in the definition of ggamma() such
that it *does* integrate to 1.

Martin

 
    Carsten> Carsten

    Carsten> [[alternative HTML version deleted]]
	     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    please read the posting guide and learn how to not produce
    the above !



From Luc.Vereecken at chem.kuleuven.ac.be  Tue Jul 12 18:28:26 2005
From: Luc.Vereecken at chem.kuleuven.ac.be (Luc Vereecken)
Date: Tue, 12 Jul 2005 18:28:26 +0200
Subject: [R] Installing RSPerl and Statistics::R
Message-ID: <6.2.3.4.0.20050712173953.01e00a50@arrhenius.chem.kuleuven.ac.be>

Hi list,

For my research, I started using R as a statistics engine, driven by 
perl scripts that manage my data and computational jobs. I recently 
upgraded my cluster to Suse 9.2, and obviously wanted to upgrade R to 
it's latest version, 2.1.1. Installation of R-2.1.1 went flawlessly 
thanks to the rpm's created by Detlef Steuer (see cran, thanks Detlef).

To interface perl and R, two packages are available: RSPerl and 
Statistics::R. Neither of the two installed out of the box, so I 
describe the problems here, as well as their solutions, for use by others.


My scripts are based on a perl package Statistics::R, available from 
the perl CPAN in version 0.02. Installation of such a perl package is 
normally performed by doing:
         perl -MCPAN -e 'install Statistics::R'
but the installation failed due to errors in the tests requiring me 
to ctrl-C the test program. I was able to force an install of this 
package by entering the CPAN shell:
         perl -MCPAN -e shell
and executing a forced install:
         force install Statistics::R
killing the test program halfway the install. Some investigation 
learned that Statistics::R as it is available at this time can not 
possible run as R is invoked as
         R --slave --vanilla --gui=none
which used to work in older version of R, but the current versions no 
longer support --gui=none causing an error "ERROR: unknown GUI none". 
Strangely, invoking "R BATCH" sets the gui to none according to the 
manual, so either gui=none still exists internally but is simply no 
longer supported by the R script, or the R BATCH manual is not 
correctly updated.
Statistics::R can be installed and made to work by using the forced 
install described above, and by editing the startup perl routines 
removing the gui=none option from the command line. This command line 
can be found in 
/usr/lib/perl5/site_perl/5.8.5/Statistics/R/Bridge/Linux.pm for my 
installation, it should be similar for other perl versions and 
library-locations.


RSPerl 0.7, available from www.omagahat.org/RSPerl/ , gives on it's 
main page the command
         R INSTALL -c -l <wherever> RSPerl_0.7-0.tar.gz
or
         R INSTALL -c -l <wherever> --configure-args='--with-in-perl' 
RSPerl_0.7-0.tar.gz
as installation command. In the "calling R from perl" PDF document, 
the command given is:
         R CMD INSTALL -c -configure-args='-with-in-perl' RSPerl
None of these commands work due to typo's.
A working command line is:
         R CMD INSTALL -c -l <installdir> 
--configure-args='--with-in-perl' RSPerl_0.7-0.tar.gz

Hope this is of use to anyone,

Luc Vereecken



From ggrothendieck at gmail.com  Tue Jul 12 18:58:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 12:58:08 -0400
Subject: [R] Installing RSPerl and Statistics::R
In-Reply-To: <6.2.3.4.0.20050712173953.01e00a50@arrhenius.chem.kuleuven.ac.be>
References: <6.2.3.4.0.20050712173953.01e00a50@arrhenius.chem.kuleuven.ac.be>
Message-ID: <971536df05071209581a332acd@mail.gmail.com>

On 7/12/05, Luc Vereecken <Luc.Vereecken at chem.kuleuven.ac.be> wrote:
> Hi list,
> 
> For my research, I started using R as a statistics engine, driven by
> perl scripts that manage my data and computational jobs. I recently
> upgraded my cluster to Suse 9.2, and obviously wanted to upgrade R to
> it's latest version, 2.1.1. Installation of R-2.1.1 went flawlessly
> thanks to the rpm's created by Detlef Steuer (see cran, thanks Detlef).
> 
> To interface perl and R, two packages are available: RSPerl and
> Statistics::R. Neither of the two installed out of the box, so I
> describe the problems here, as well as their solutions, for use by others.
> 
> 
> My scripts are based on a perl package Statistics::R, available from
> the perl CPAN in version 0.02. Installation of such a perl package is
> normally performed by doing:
>         perl -MCPAN -e 'install Statistics::R'
> but the installation failed due to errors in the tests requiring me
> to ctrl-C the test program. I was able to force an install of this
> package by entering the CPAN shell:
>         perl -MCPAN -e shell
> and executing a forced install:
>         force install Statistics::R
> killing the test program halfway the install. Some investigation
> learned that Statistics::R as it is available at this time can not
> possible run as R is invoked as
>         R --slave --vanilla --gui=none
> which used to work in older version of R, but the current versions no
> longer support --gui=none causing an error "ERROR: unknown GUI none".
> Strangely, invoking "R BATCH" sets the gui to none according to the
> manual, so either gui=none still exists internally but is simply no
> longer supported by the R script, or the R BATCH manual is not
> correctly updated.
> Statistics::R can be installed and made to work by using the forced
> install described above, and by editing the startup perl routines
> removing the gui=none option from the command line. This command line
> can be found in
> /usr/lib/perl5/site_perl/5.8.5/Statistics/R/Bridge/Linux.pm for my
> installation, it should be similar for other perl versions and
> library-locations.
> 
> 
> RSPerl 0.7, available from www.omagahat.org/RSPerl/ , gives on it's
> main page the command
>         R INSTALL -c -l <wherever> RSPerl_0.7-0.tar.gz
> or
>         R INSTALL -c -l <wherever> --configure-args='--with-in-perl'
> RSPerl_0.7-0.tar.gz
> as installation command. In the "calling R from perl" PDF document,
> the command given is:
>         R CMD INSTALL -c -configure-args='-with-in-perl' RSPerl
> None of these commands work due to typo's.
> A working command line is:
>         R CMD INSTALL -c -l <installdir>
> --configure-args='--with-in-perl' RSPerl_0.7-0.tar.gz
> 
> Hope this is of use to anyone,
> 
> Luc Vereecken

I don't know the answer to your question and do all my analyses on
Windows rather than UNIX but I used to use perl for data processing
and another software for statistical analysis and what I really liked
about moving to R was that it is sufficiently powerful that I could eliminate
all my data processing perl code and replace it with much easier to 
maintain R code so now everything is in one environment.  You might 
think about whether that is feasible for you too.



From sue at xlsolutions-corp.com  Tue Jul 12 19:02:17 2005
From: sue at xlsolutions-corp.com (sue@xlsolutions-corp.com)
Date: Tue, 12 Jul 2005 10:02:17 -0700
Subject: [R] Course***R/Splus Advanced Programming *** Atlanta
Message-ID: <20050712170217.2800.qmail@webmail13.mesa1.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our "Advanced R/Splus programming" course taught by R Development
Core Team Guru!

www.xlsolutions-corp.com/Radv.htm

*********Atlanta------------  July 28th-29th, 2005

Ask for group discount and reserve your seat Now  (payment due after
the class)

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578


Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount. Over half of the seats in this
class
are currently reserved.  Register now to secure your seat in this
course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From rogeriorosas at gmail.com  Tue Jul 12 19:37:14 2005
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Tue, 12 Jul 2005 14:37:14 -0300
Subject: [R] simple question: reading lower triangular matrix
Message-ID: <42D3FFCA.1020200@gmail.com>

Dear list,

I will like to learn how to read a lower triangular matrix in R. The
input file *.txt have the following format:

     A   B   C   D   E
A   0   
B   1    0
C   2    5    0   
D   3    6    8    0  
E   4    7    9   10    0


How this can be done?

Thanks in advance for your help

Rog??rio



From mark.salsburg at gmail.com  Tue Jul 12 19:45:43 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 12 Jul 2005 13:45:43 -0400
Subject: [R] interactive plot showing label
Message-ID: <dd48e20f05071210455ac28c1d@mail.gmail.com>

I have the following data:

gene_name                 microarray expression
A                                       2323
B                                       1983
....                                        .....


I have about 10,000 observations.

I would like to know if there is a way to create an interactive plot..

Mainly one that shows all the points, but when a point is clicked on
it shows what gene_name it is

If not, does anyone recommend another way to output a plot (using
maybe bioconducter bc this is microarray expression) that is
informative about the gene_names.



From Seeliger.Curt at epamail.epa.gov  Tue Jul 12 19:51:03 2005
From: Seeliger.Curt at epamail.epa.gov (Seeliger.Curt@epamail.epa.gov)
Date: Tue, 12 Jul 2005 10:51:03 -0700
Subject: [R] Calculation of group summaries
Message-ID: <OF71BFF9FD.504E2567-ON8825703C.00572781-8825703C.00620EE8@epamail.epa.gov>

I know R has a steep learning curve, but from where I stand the slope
looks like a sheer cliff.  I'm pawing through the available docs and
have come across examples which come close to what I want but are
proving difficult for me to modify for my use.

Calculating simple group means is fairly straight forward:
  data(PlantGrowth)
  attach(PlantGrowth)
  stack(mean(unstack(PlantGrowth)))

I'd like to do something slightly more complex, using a data frame and
groups identified by unique combinations of three id variables.  There
may be thousands of such combinations in the data.  This is easy in SQL:

  select year,
         site_id,
         visit_no,
         mean(undercut) AS meanUndercut,
         count(undercut) AS nUndercut,
         std(undercut) AS stdUndercut
  from channelMorphology
  group by year, site_id, visit_no
      ;

Reading a CSV written by SAS and selecting only records expected to have
values is also straight forward in R, but getting those summary values
for each site visit is currently beyond me:

  sub<-read.csv('c:/data/channelMorphology.csv'
               ,header=TRUE
               ,na.strings='.'
               ,sep=','
               ,strip.white=TRUE
               )

  undercut<-subset(sub,
                  ,TRANSDIR %in% c('LF','RT')

,select=c('YEAR','SITE_ID','VISIT_NO','TRANSECT','TRANSDIR'
                           ,'UNDERCUT'
                           )
                  ,drop=TRUE
                  )


Thanks all for your help.
cur
--
Curt Seeliger, Data Ranger
CSC, EPA/WED contractor
541/754-4638
seeliger.curt at epa.gov



From btyner at gmail.com  Tue Jul 12 19:54:15 2005
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 12 Jul 2005 12:54:15 -0500
Subject: [R] getting panel.loess to use updated version of loess.smooth
Message-ID: <42D403C7.1060200@stat.purdue.edu>

I'm updating the loess routines to allow for, among other things, 
arbitrary local polynomial degree and number of predictors. For now, 
I've given the updated package its own namespace. The trouble is, 
panel.loess still calls the original code in package:stats instead of 
the new loess package, regardless of whether package:loess or 
package:lattice comes first in the search list. If I export panel.loess 
from the new package, then it can't see grid.lines anymore.

I've tried using fixInNamespace to change the loess.smooth in 
package:stats to point to the updated simpleLoess, but it is locked. At 
http://tolstoy.newcastle.edu.au/R/help/04/05/0428.html it was suggested 
to write a new panel function. However I still need to be able to access 
grid.lines, and getFromNamespace("grid.lines","grid") is not the way to 
do this.

Any ideas? This seems simple but I'm stumped.

Thanks,
Ben



From Jeffrey.Marcus at scansoft.com  Tue Jul 12 20:09:47 2005
From: Jeffrey.Marcus at scansoft.com (Marcus, Jeffrey)
Date: Tue, 12 Jul 2005 14:09:47 -0400
Subject: [R] Anything to replace Statistical Models in S ("the white book")?
Message-ID: <F8940C21CD563F49BC884A274C4653DF01E7A79A@bn-exch1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/26f017fc/attachment.pl

From gerifalte28 at hotmail.com  Tue Jul 12 20:10:00 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 12 Jul 2005 18:10:00 +0000
Subject: [R] Complex plotting in R
In-Reply-To: <W7985627828283501121179878@kostunerix.telenet-ops.be>
Message-ID: <BAY103-F236E7F14D337F2115CDB94A6DF0@phx.gbl>

First call par(mfrow = c(2,2)) to get four plots in one panel
then plot your top figures using
plot(x, xaxt="n")#this will generate the plot withoput displaying the x axis
and then plot your bottom figures keeping the axis (without using xaxt="n")

You may want to fix the x axis limits to match between figures using the 
argument xlim=c(min,max) within plot()

More details in your note would have been helpful to give you better 
guidance

Cheers

Francisco


>From: "koen.hufkens at pandora.be" <koen.hufkens at telenet.be>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Complex plotting in R
>Date: Tue, 12 Jul 2005 14:51:18 +0000
>
>Hi list,
>
>I'm looking for a function or a combination of functions to do panel 
>plotting of mixed graph types with the same x axis.
>
>I would like to construct a panel with 3 stacked windows with on top a 
>histogram, below that 2 cdf plots. They all have the same x axis value but 
>different y axis values. Is it possible to construct something like that?
>
>I've looked into the lattice package but it doesn't seem to solve my 
>problem because I clearly want 1 x axis for all the graphs so not just a 
>panel with 3 full graphs.
>
>Something like this:
>
>http://www.ats.ucla.edu/stat/sas/faq/InfluencePlots6.gif
>
>(copyright to whom copyright is due...)
>
>but with the top and bottom graphs merged.
>
>Best regards,
>Koen Hufkens
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Tue Jul 12 20:22:57 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 12 Jul 2005 11:22:57 -0700
Subject: [R] Anything to replace Statistical Models in S ("the white
	book")?
In-Reply-To: <F8940C21CD563F49BC884A274C4653DF01E7A79A@bn-exch1>
Message-ID: <200507121823.j6CIMvLV025692@faraday.gene.com>

MASS (MODERN APPLIED STATISTICS WITH S) by Venables and Ripley. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marcus, Jeffrey
> Sent: Tuesday, July 12, 2005 11:10 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Anything to replace Statistical Models in S 
> ("the white book")?
> 
> Hello:
> 
>   I found that Amazon cannot find a copy of "Statistical 
> Models in S"? I am
> about to embark on some tree-based and perhaps ANOVA models and have
> following options:
> 
>  
> 
> (*) Find another book/online doc that covers this material 
> (perhaps one
> recommended on the R FAQ page)
> 
> (*) Use R documentation
> 
> (*) Try even harder to land the white book. 
> 
>  
> 
>    I have a decent conceptual and programming understanding 
> of trees and
> ANOVA but I'd like something that might fill in details (best way to
> cross-validate, missing value handling, other gotchas). 
> 
>  
> 
>   Thanks for any opinions on this.
> 
>  
> 
>   Jeff
> 
>  
> 
>  
> 
> ________________________________________
> 
> SpeechWorks solutions from ScanSoft. Inspired Applications, 
> Exceptional
> Results.
> 
> Jeff Marcus
> Manager, Speech Tools, SpeechWorks Division
> ScanSoft, Inc.
> p: 781.565.5281
> f: 781-565-5575
>  <http://www.scansoft.com/> www.scansoft.com
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gerifalte28 at hotmail.com  Tue Jul 12 20:34:39 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 12 Jul 2005 18:34:39 +0000
Subject: [R] Calculation of group summaries
Message-ID: <BAY103-F4019281297961230932D9EA6DF0@phx.gbl>

Take a look at ?aggregate ?ave and ?tapply

Cheers

Francisco

>From: Seeliger.Curt at epamail.epa.gov
>To: R-Help <r-help at stat.math.ethz.ch>
>Subject: [R] Calculation of group summaries
>Date: Tue, 12 Jul 2005 10:51:03 -0700
>
>I know R has a steep learning curve, but from where I stand the slope
>looks like a sheer cliff.  I'm pawing through the available docs and
>have come across examples which come close to what I want but are
>proving difficult for me to modify for my use.
>
>Calculating simple group means is fairly straight forward:
>   data(PlantGrowth)
>   attach(PlantGrowth)
>   stack(mean(unstack(PlantGrowth)))
>
>I'd like to do something slightly more complex, using a data frame and
>groups identified by unique combinations of three id variables.  There
>may be thousands of such combinations in the data.  This is easy in SQL:
>
>   select year,
>          site_id,
>          visit_no,
>          mean(undercut) AS meanUndercut,
>          count(undercut) AS nUndercut,
>          std(undercut) AS stdUndercut
>   from channelMorphology
>   group by year, site_id, visit_no
>       ;
>
>Reading a CSV written by SAS and selecting only records expected to have
>values is also straight forward in R, but getting those summary values
>for each site visit is currently beyond me:
>
>   sub<-read.csv('c:/data/channelMorphology.csv'
>                ,header=TRUE
>                ,na.strings='.'
>                ,sep=','
>                ,strip.white=TRUE
>                )
>
>   undercut<-subset(sub,
>                   ,TRANSDIR %in% c('LF','RT')
>
>,select=c('YEAR','SITE_ID','VISIT_NO','TRANSECT','TRANSDIR'
>                            ,'UNDERCUT'
>                            )
>                   ,drop=TRUE
>                   )
>
>
>Thanks all for your help.
>cur
>--
>Curt Seeliger, Data Ranger
>CSC, EPA/WED contractor
>541/754-4638
>seeliger.curt at epa.gov
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Tue Jul 12 20:44:21 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 12 Jul 2005 13:44:21 -0500
Subject: [R] getting panel.loess to use updated version of loess.smooth
In-Reply-To: <42D403C7.1060200@stat.purdue.edu>
References: <42D403C7.1060200@stat.purdue.edu>
Message-ID: <42D40F85.20401@pdf.com>



Benjamin Tyner wrote:
> I'm updating the loess routines to allow for, among other things, 
> arbitrary local polynomial degree and number of predictors. For now, 
> I've given the updated package its own namespace. The trouble is, 
> panel.loess still calls the original code in package:stats instead of 
> the new loess package, regardless of whether package:loess or 
> package:lattice comes first in the search list. If I export panel.loess 
> from the new package, then it can't see grid.lines anymore.
> 
> I've tried using fixInNamespace to change the loess.smooth in 
> package:stats to point to the updated simpleLoess, but it is locked. At 
> http://tolstoy.newcastle.edu.au/R/help/04/05/0428.html it was suggested 
> to write a new panel function. However I still need to be able to access 
> grid.lines, and getFromNamespace("grid.lines","grid") is not the way to 
> do this.
> 
> Any ideas? This seems simple but I'm stumped.
> 
> Thanks,
> Ben
> 

Can you show us what you tried? It seems as simple as defining a new 
panel function:

panel.loess2 <- function (x, y, span = 2/3, degree = 1,
                           family = c("symmetric", "gaussian"),
                           evaluation = 50, lwd = add.line$lwd,
                           lty = add.line$lty, col,
                           col.line = add.line$col, ...) {
   x <- as.numeric(x)
   y <- as.numeric(y)
   if (length(x) > 0) {
       if (!missing(col)) {
           if (missing(col.line))
               col.line <- col
       }
       add.line <- trellis.par.get("add.line")
       #smooth <- loess.smooth(x, y, span = span, family = family,
       #                       degree = degree, evaluation = evaluation)
       smooth <- simpleLoess(x, y, span = span, family = family,
                             degree = degree, evaluation = evaluation)
       grid.lines(x = smooth$x, y = smooth$y, default.units = "native",
                  gp = gpar(col = col.line, lty = lty, lwd = lwd))
   }
}


--sundar



From btyner at gmail.com  Tue Jul 12 20:53:55 2005
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 12 Jul 2005 13:53:55 -0500
Subject: [R] getting panel.loess to use updated version of loess.smooth
In-Reply-To: <42D40F85.20401@pdf.com>
References: <42D403C7.1060200@stat.purdue.edu> <42D40F85.20401@pdf.com>
Message-ID: <42D411C3.8070709@stat.purdue.edu>

A quick workaround, kudos to Deepayan Sarkar, is to use grid:: for both 
grid.lines AND gpar in panel.loess:

        grid::grid.lines(x = smooth$x, y = smooth$y, default.units = 
"native",
            gp = grid::gpar(col = col.line, lty = lty, lwd = lwd))

Then write a new panel function as you suggest (though the name can be 
the same), and to export this panel function in the package NAMESPACE 
file. loess.smooth and simpleLoess are separate functions, sorry if I 
didn't make that clear.

This works well enough that I don't require anything fancier for now. 
Thanks!

Ben

Sundar Dorai-Raj wrote:

>
>
> Benjamin Tyner wrote:
>
>> I'm updating the loess routines to allow for, among other things, 
>> arbitrary local polynomial degree and number of predictors. For now, 
>> I've given the updated package its own namespace. The trouble is, 
>> panel.loess still calls the original code in package:stats instead of 
>> the new loess package, regardless of whether package:loess or 
>> package:lattice comes first in the search list. If I export 
>> panel.loess from the new package, then it can't see grid.lines anymore.
>>
>> I've tried using fixInNamespace to change the loess.smooth in 
>> package:stats to point to the updated simpleLoess, but it is locked. 
>> At http://tolstoy.newcastle.edu.au/R/help/04/05/0428.html it was 
>> suggested to write a new panel function. However I still need to be 
>> able to access grid.lines, and getFromNamespace("grid.lines","grid") 
>> is not the way to do this.
>>
>> Any ideas? This seems simple but I'm stumped.
>>
>> Thanks,
>> Ben
>>
>
> Can you show us what you tried? It seems as simple as defining a new 
> panel function:
>
> panel.loess2 <- function (x, y, span = 2/3, degree = 1,
>                           family = c("symmetric", "gaussian"),
>                           evaluation = 50, lwd = add.line$lwd,
>                           lty = add.line$lty, col,
>                           col.line = add.line$col, ...) {
>   x <- as.numeric(x)
>   y <- as.numeric(y)
>   if (length(x) > 0) {
>       if (!missing(col)) {
>           if (missing(col.line))
>               col.line <- col
>       }
>       add.line <- trellis.par.get("add.line")
>       #smooth <- loess.smooth(x, y, span = span, family = family,
>       #                       degree = degree, evaluation = evaluation)
>       smooth <- simpleLoess(x, y, span = span, family = family,
>                             degree = degree, evaluation = evaluation)
>       grid.lines(x = smooth$x, y = smooth$y, default.units = "native",
>                  gp = gpar(col = col.line, lty = lty, lwd = lwd))
>   }
> }
>
>
> --sundar



From mschwartz at mn.rr.com  Tue Jul 12 21:00:40 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 12 Jul 2005 14:00:40 -0500
Subject: [R] simple question: reading lower triangular matrix
In-Reply-To: <42D3FFCA.1020200@gmail.com>
References: <42D3FFCA.1020200@gmail.com>
Message-ID: <1121194840.4193.29.camel@localhost.localdomain>

On Tue, 2005-07-12 at 14:37 -0300, Rogrio Rosa da Silva wrote:
> Dear list,
> 
> I will like to learn how to read a lower triangular matrix in R. The
> input file *.txt have the following format:
> 
>      A   B   C   D   E
> A   0   
> B   1    0
> C   2    5    0   
> D   3    6    8    0  
> E   4    7    9   10    0
> 
> 
> How this can be done?
> 
> Thanks in advance for your help
> 
> Rogrio


I don't know that this is the easiest way of doing it, but here is one
approach:

# I saved your data above in a file called "test.txt"

# Read the first line of test.txt to get the colnames as chars
col.names <- unlist(read.table("test.txt", nrow = 1, as.is = TRUE))

# now read the rest of the file using 'fill = TRUE' to pad lines
# with NAs 
# skip the first line
# set the row.names as the first column in the text file
# coerce to a matrix
df <- as.matrix(read.table("test.txt", fill = TRUE, skip = 1, 
                row.names = 1))

# Now set the colnames of df
colnames(df) <- col.names

> df
  A  B  C  D  E
A 0 NA NA NA NA
B 1  0 NA NA NA
C 2  5  0 NA NA
D 3  6  8  0 NA
E 4  7  9 10  0

If you should further want to set the diagonal to NA:

> diag(df) <- NA

> df
   A  B  C  D  E
A NA NA NA NA NA
B  1 NA NA NA NA
C  2  5 NA NA NA
D  3  6  8 NA NA
E  4  7  9 10 NA


See ?read.table for more information on the file reading part.

HTH,

Marc Schwartz



From spluque at gmail.com  Tue Jul 12 21:12:47 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Tue, 12 Jul 2005 14:12:47 -0500
Subject: [R] interactive plot showing label
References: <dd48e20f05071210455ac28c1d@mail.gmail.com>
Message-ID: <87d5png7o0.fsf@gmail.com>

mark salsburg <mark.salsburg at gmail.com> wrote:

[...]

> Mainly one that shows all the points, but when a point is clicked on
> it shows what gene_name it is

I think ?identify will help you.


-- 
Sebastian P. Luque



From sentientc at gmail.com  Tue Jul 12 22:02:06 2005
From: sentientc at gmail.com (simon)
Date: Wed, 13 Jul 2005 04:02:06 +0800
Subject: [R] Calling R from fortran
Message-ID: <42D421BE.5000104@gmail.com>

Hi,
The following may sound stupid so please forgive my stupidness. I have a 
question which I don't know how to name it so I have to start from the 
beginning. In an attempt to gain better understand how a photochemical 
air qaulity model works, I plotted hourly ozone concentration contour 
from the model's standard output to observe the changes in simulation. 
Because the model actually peroform simulation in time steps of 5 
minutes, it will give a better resoultion if I can plot the ozone 
concentration with time resolution close to the model. However, if I 
modify the model to output data at every time step, it will be 
impossible for the computer to store this much information( the hourly 
standard output is close to 1gb). I was wondering if it is possible to 
modify fortran to call R or something, which can just plot a contour at 
every time step rather than put it as data. I am not sure about how to 
call this act. Also, if this is possible, will output of a contour give 
a smaller file size than data?
Thanks in advance,
simon



From f.harrell at vanderbilt.edu  Tue Jul 12 21:57:06 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 12 Jul 2005 14:57:06 -0500
Subject: [R] Calculation of group summaries
In-Reply-To: <OF71BFF9FD.504E2567-ON8825703C.00572781-8825703C.00620EE8@epamail.epa.gov>
References: <OF71BFF9FD.504E2567-ON8825703C.00572781-8825703C.00620EE8@epamail.epa.gov>
Message-ID: <42D42092.80500@vanderbilt.edu>

See http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/SasByMeansExample
for one example.

Frank


Seeliger.Curt at epamail.epa.gov wrote:
> I know R has a steep learning curve, but from where I stand the slope
> looks like a sheer cliff.  I'm pawing through the available docs and
> have come across examples which come close to what I want but are
> proving difficult for me to modify for my use.
> 
> Calculating simple group means is fairly straight forward:
>   data(PlantGrowth)
>   attach(PlantGrowth)
>   stack(mean(unstack(PlantGrowth)))
> 
> I'd like to do something slightly more complex, using a data frame and
> groups identified by unique combinations of three id variables.  There
> may be thousands of such combinations in the data.  This is easy in SQL:
> 
>   select year,
>          site_id,
>          visit_no,
>          mean(undercut) AS meanUndercut,
>          count(undercut) AS nUndercut,
>          std(undercut) AS stdUndercut
>   from channelMorphology
>   group by year, site_id, visit_no
>       ;
> 
> Reading a CSV written by SAS and selecting only records expected to have
> values is also straight forward in R, but getting those summary values
> for each site visit is currently beyond me:
> 
>   sub<-read.csv('c:/data/channelMorphology.csv'
>                ,header=TRUE
>                ,na.strings='.'
>                ,sep=','
>                ,strip.white=TRUE
>                )
> 
>   undercut<-subset(sub,
>                   ,TRANSDIR %in% c('LF','RT')
> 
> ,select=c('YEAR','SITE_ID','VISIT_NO','TRANSECT','TRANSDIR'
>                            ,'UNDERCUT'
>                            )
>                   ,drop=TRUE
>                   )
> 
> 
> Thanks all for your help.
> cur
> --
> Curt Seeliger, Data Ranger
> CSC, EPA/WED contractor
> 541/754-4638
> seeliger.curt at epa.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From greg.snow at ihc.com  Tue Jul 12 23:12:39 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 12 Jul 2005 15:12:39 -0600
Subject: [R] unexpected behavior in bwplot
Message-ID: <s2d3ddfa.070@lp-msg1.co.ihc.com>

I had a similar problem when trying to modify a function from lattice. 
I second the opinion that it 
would be nice if lattice exported more things.  My soulution was to
give my function the same 
environment as the one I had copied, i.e. try:

environment(panel.mybwplot) <- environment(panel.bwplot)

I have included a line like this at the end of the script file that
defines my new/modified lattice
functions, and as long as lattice is loaded first (could wrap the above
in the body of an if 
statement that calls library or require) then everything works fine
(having the same environment
lets the new function find the non-exported functions).

Hope this helps,



Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> "Ritter, Christian C GSMCIL-GSTMS/2" <christian.ritter at shell.com>
07/12/05 05:08AM >>>
R-2.1.1 on windows XP

I just noticed something unpleasant when using bwplot (from lattice). 

In order to satisfy a wish from a client, I needed to produce sets of
boxplots conditioned by another factor. My client didn't like the look
of the boxplots (by default, they have a star to mark the median,
instead of the commonly used line). I told him "no problem" dumped
panel.bwplot, added a line at the median, commented out the star, and
sourced it back in as panel.mybwplot. 

Then I tried to call it with bwplot(...,panel="panel.mybwplot"), but
this hit two roadblocks. The first one was relatively easy: it didn't
find the function current.viewport (from grid). Explicitely loading the
package grid was sufficient (current.viewport is exported there). But
then it stopped on chooseFace. It was not obvious to find where
chooseFace was (in lattice, but not as an exported function in the
namespace) and how to make it callable. I finally clobbered it by
defining chooseFace<-lattice:::chooseFace. This certainly worked but is
very inelegant. Could anyone point me to cleaner ways to do this? 

On the other hand, I would have preferred if panel.bwplot would not use
functions not exported to the outer namespace (or if lattice would make
sure that all functions called in the standard panel functions were
exported to the outer namespace). It is very common that we have to
slightly modify standard panel functions and it is quite annoying having
to chase for evenually hidden functions called by them. Comments? 

Thanks in advance,

Christian Ritter
Functional Specialist Statistics
Shell Coordination Centre S.A.
Monnet Centre International Laboratory, Avenue Jean Monnet 1, B-1348
Louvain-La-Neuve, Belgium

Tel: +32 10 477  349 Fax: +32 10 477 219
Email: christian.ritter at shell.com 
Internet: http://www.shell.com/chemicals 



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Tue Jul 12 23:12:36 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 12 Jul 2005 22:12:36 +0100
Subject: [R] simple question: reading lower triangular matrix
In-Reply-To: <42D3FFCA.1020200@gmail.com>
References: <42D3FFCA.1020200@gmail.com>
Message-ID: <1121202756.5840.0.camel@dhcp-63.ccc.ox.ac.uk>

This has been answered at the following URL

http://tolstoy.newcastle.edu.au/~rking/R/help/04/11/6695.html


On Tue, 2005-07-12 at 14:37 -0300, Rogrio Rosa da Silva wrote:
> Dear list,
> 
> I will like to learn how to read a lower triangular matrix in R. The
> input file *.txt have the following format:
> 
>      A   B   C   D   E
> A   0   
> B   1    0
> C   2    5    0   
> D   3    6    8    0  
> E   4    7    9   10    0
> 
> 
> How this can be done?
> 
> Thanks in advance for your help
> 
> Rogrio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rogeriorosas at gmail.com  Tue Jul 12 23:36:20 2005
From: rogeriorosas at gmail.com (=?UTF-8?B?Um9nw6lyaW8gUm9zYSBkYSBTaWx2YQ==?=)
Date: Tue, 12 Jul 2005 18:36:20 -0300
Subject: [R] simple question: reading lower triangular matrix
In-Reply-To: <1121202756.5840.0.camel@dhcp-63.ccc.ox.ac.uk>
References: <42D3FFCA.1020200@gmail.com>
	<1121202756.5840.0.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <42D437D4.8050901@gmail.com>

Dear Adaikalavan and Marc,

Thanks for advices. The answer in R-help archives is exactly what I'm
needing.

Best regards,

Rogrio


Adaikalavan Ramasamy wrote:

>This has been answered at the following URL
>
>http://tolstoy.newcastle.edu.au/~rking/R/help/04/11/6695.html
>
>
>On Tue, 2005-07-12 at 14:37 -0300, Rogrio Rosa da Silva wrote:
>  
>



From sheri at atmos.colostate.edu  Tue Jul 12 23:46:57 2005
From: sheri at atmos.colostate.edu (Sheri Conner Gausepohl)
Date: Tue, 12 Jul 2005 15:46:57 -0600
Subject: [R] using its to import time series data with uneven dates
Message-ID: <f5265c49209358eb81f07957999149fa@atmos.colostate.edu>

Good day:

I am trying to use 
readcsvIts("nwr_data_qc.txt",informat=its.format("%Y%m%d%h%M 
%Y"),header=TRUE,sep="",skip=0,row.names=NULL,as.is=TRUE,dec=".")

to read in a file (nwr_data_qc.txt)  that looks like this:

Time         Y      M   D  H   Min    CO2
2000.18790   2000.  3.  9. 18. 30.    373.60
2000.20156   2000.  3. 14. 18. 30.    373.34
2000.22609   2000.  3. 23. 18.  0.    373.01

and  R returns this:

         Y  M  D  H Min    CO2
<NA> 2000  3  9 18  30 373.60
<NA> 2000  3 14 18  30 373.34
<NA> 2000  3 23 18   0 373.01

I have tried every format option on the help page.  How can I read in 
my decimal dates (e.g., 2000.18790)?

Note that these data (CO2) are irregularly spaced in time (Time).

Ultimately I would like to fit a trigonometric polynomial (first 
harmonic) to these data in order to smooth them and obtain values 
between measurements.  Any suggestions you can provide on how to do 
this would be much appreciated.

Thank you for your help.

Sheri

Sheri L. Conner Gausepohl
Graduate Research Assistant
Department of Atmospheric Science
Colorado State University



From whit at twinfieldscapital.com  Wed Jul 13 00:01:48 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Tue, 12 Jul 2005 18:01:48 -0400
Subject: [R] using its to import time series data with uneven dates
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE2C80C9@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

You would probably do better to read in your data and reformat the
dates.

What date format is 2000.18790?  The Its package uses POSIXct dates and
readcsvIts expects the formats to be "%Y-%m-%d" unless you've changed
the default format.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sheri Conner
Gausepohl
Sent: Tuesday, July 12, 2005 5:47 PM
To: r-help at stat.math.ethz.ch
Subject: [R] using its to import time series data with uneven dates

Good day:

I am trying to use
readcsvIts("nwr_data_qc.txt",informat=its.format("%Y%m%d%h%M
%Y"),header=TRUE,sep="",skip=0,row.names=NULL,as.is=TRUE,dec=".")

to read in a file (nwr_data_qc.txt)  that looks like this:

Time         Y      M   D  H   Min    CO2
2000.18790   2000.  3.  9. 18. 30.    373.60
2000.20156   2000.  3. 14. 18. 30.    373.34
2000.22609   2000.  3. 23. 18.  0.    373.01

and  R returns this:

         Y  M  D  H Min    CO2
<NA> 2000  3  9 18  30 373.60
<NA> 2000  3 14 18  30 373.34
<NA> 2000  3 23 18   0 373.01

I have tried every format option on the help page.  How can I read in my
decimal dates (e.g., 2000.18790)?

Note that these data (CO2) are irregularly spaced in time (Time).

Ultimately I would like to fit a trigonometric polynomial (first
harmonic) to these data in order to smooth them and obtain values
between measurements.  Any suggestions you can provide on how to do this
would be much appreciated.

Thank you for your help.

Sheri

Sheri L. Conner Gausepohl
Graduate Research Assistant
Department of Atmospheric Science
Colorado State University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Wed Jul 13 00:04:58 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 12 Jul 2005 17:04:58 -0500
Subject: [R] unexpected behavior in bwplot
In-Reply-To: <156CDC8CCFD1894295D2907F16337A48B2AF19@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A48B2AF19@bru-s-006.europe.shell.com>
Message-ID: <eb555e66050712150455ca9430@mail.gmail.com>

On 7/12/05, Ritter, Christian C GSMCIL-GSTMS/2
<christian.ritter at shell.com> wrote:
> R-2.1.1 on windows XP
> 
> I just noticed something unpleasant when using bwplot (from lattice).
> 
> In order to satisfy a wish from a client, I needed to produce sets of boxplots conditioned by another factor. My client didn't like the look of the boxplots (by default, they have a star to mark the median, instead of the commonly used line). I told him "no problem" dumped panel.bwplot, added a line at the median, commented out the star, and sourced it back in as panel.mybwplot.
> 
> Then I tried to call it with bwplot(...,panel="panel.mybwplot"), but this hit two roadblocks. The first one was relatively easy: it didn't find the function current.viewport (from grid). Explicitely loading the package grid was sufficient (current.viewport is exported there). But then it stopped on chooseFace. It was not obvious to find where chooseFace was (in lattice, but not as an exported function in the namespace) and how to make it callable. I finally clobbered it by defining chooseFace<-lattice:::chooseFace. This certainly worked but is very inelegant. Could anyone point me to cleaner ways to do this?
> 
> On the other hand, I would have preferred if panel.bwplot would not use functions not exported to the outer namespace (or if lattice would make sure that all functions called in the standard panel functions were exported to the outer namespace). It is very common that we have to slightly modify standard panel functions and it is quite annoying having to chase for evenually hidden functions called by them. Comments?
> 

I agree that this is a problem, but I don't know what a good solution
is. I can certainly export more functions (writing documentation is
the only hard work). However, chooseFace is a hack which definitely
isn't worth exporting.

I think the best solution in this case is for me to rewrite
panel.bwplot using lattice wrappers line panel.points and panel.lines
(and panel.rect, which doesn't exist yet). That may take a little
time. Please let me know of other similar situations if you encounter
them.

Deepayan



From deepayan.sarkar at gmail.com  Wed Jul 13 00:11:00 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 12 Jul 2005 17:11:00 -0500
Subject: [R] Complex plotting in R
In-Reply-To: <W7985627828283501121179878@kostunerix.telenet-ops.be>
References: <W7985627828283501121179878@kostunerix.telenet-ops.be>
Message-ID: <eb555e6605071215119aee64a@mail.gmail.com>

On 7/12/05, koen.hufkens at pandora.be <koen.hufkens at telenet.be> wrote:
> Hi list,
> 
> I'm looking for a function or a combination of functions to do panel plotting of mixed graph types with the same x axis.
> 
> I would like to construct a panel with 3 stacked windows with on top a histogram, below that 2 cdf plots. They all have the same x axis value but different y axis values. Is it possible to construct something like that?
> 
> I've looked into the lattice package but it doesn't seem to solve my problem because I clearly want 1 x axis for all the graphs so not just a panel with 3 full graphs.
> 

Multiple graph types in lattice would need a lot of hacking. If you
are feeling adventurous, you might want to try using grid graphics
(the 'grid' package) directly.

Deepayan



From fernando.espindola at ifop.cl  Wed Jul 13 00:42:14 2005
From: fernando.espindola at ifop.cl (=?iso-8859-1?Q?Fernando_Esp=EDndola?=)
Date: Tue, 12 Jul 2005 18:42:14 -0400
Subject: [R] Please help me.....
Message-ID: <27004DDE1590B344855CF773E1D019F115BF2A@postino.ifop.cl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050712/6b522d3a/attachment.pl

From ggrothendieck at gmail.com  Wed Jul 13 01:02:14 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 19:02:14 -0400
Subject: [R] using its to import time series data with uneven dates
In-Reply-To: <f5265c49209358eb81f07957999149fa@atmos.colostate.edu>
References: <f5265c49209358eb81f07957999149fa@atmos.colostate.edu>
Message-ID: <971536df050712160235321110@mail.gmail.com>

On 7/12/05, Sheri Conner Gausepohl <sheri at atmos.colostate.edu> wrote:
> Good day:
> 
> I am trying to use
> readcsvIts("nwr_data_qc.txt",informat=its.format("%Y%m%d%h%M
> %Y"),header=TRUE,sep="",skip=0,row.names=NULL,as.is=TRUE,dec=".")
> 
> to read in a file (nwr_data_qc.txt)  that looks like this:
> 
> Time         Y      M   D  H   Min    CO2
> 2000.18790   2000.  3.  9. 18. 30.    373.60
> 2000.20156   2000.  3. 14. 18. 30.    373.34
> 2000.22609   2000.  3. 23. 18.  0.    373.01
> 
> and  R returns this:
> 
>         Y  M  D  H Min    CO2
> <NA> 2000  3  9 18  30 373.60
> <NA> 2000  3 14 18  30 373.34
> <NA> 2000  3 23 18   0 373.01
> 
> I have tried every format option on the help page.  How can I read in
> my decimal dates (e.g., 2000.18790)?
> 
> Note that these data (CO2) are irregularly spaced in time (Time).
> 
> Ultimately I would like to fit a trigonometric polynomial (first
> harmonic) to these data in order to smooth them and obtain values
> between measurements.  Any suggestions you can provide on how to do
> this would be much appreciated.
> 
> Thank you for your help.
> 
> Sheri
> 
> Sheri L. Conner Gausepohl
> Graduate Research Assistant
> Department of Atmospheric Science
> Colorado State University

Try this:

library(its)
dd <- read.table(myfile, header = TRUE)
tt <- paste(dd$Y, "-", dd$M, "-", dd$D, " ", dd$H, ":", dd$Min, sep = "")
co2 <- its(dd$CO2, as.POSIXct(tt))
co2 # display co2



From ggrothendieck at gmail.com  Wed Jul 13 01:13:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 19:13:32 -0400
Subject: [R] using its to import time series data with uneven dates
In-Reply-To: <971536df050712160235321110@mail.gmail.com>
References: <f5265c49209358eb81f07957999149fa@atmos.colostate.edu>
	<971536df050712160235321110@mail.gmail.com>
Message-ID: <971536df050712161377869e4d@mail.gmail.com>

On 7/12/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/12/05, Sheri Conner Gausepohl <sheri at atmos.colostate.edu> wrote:
> > Good day:
> >
> > I am trying to use
> > readcsvIts("nwr_data_qc.txt",informat=its.format("%Y%m%d%h%M
> > %Y"),header=TRUE,sep="",skip=0,row.names=NULL,as.is=TRUE,dec=".")
> >
> > to read in a file (nwr_data_qc.txt)  that looks like this:
> >
> > Time         Y      M   D  H   Min    CO2
> > 2000.18790   2000.  3.  9. 18. 30.    373.60
> > 2000.20156   2000.  3. 14. 18. 30.    373.34
> > 2000.22609   2000.  3. 23. 18.  0.    373.01
> >
> > and  R returns this:
> >
> >         Y  M  D  H Min    CO2
> > <NA> 2000  3  9 18  30 373.60
> > <NA> 2000  3 14 18  30 373.34
> > <NA> 2000  3 23 18   0 373.01
> >
> > I have tried every format option on the help page.  How can I read in
> > my decimal dates (e.g., 2000.18790)?
> >
> > Note that these data (CO2) are irregularly spaced in time (Time).
> >
> > Ultimately I would like to fit a trigonometric polynomial (first
> > harmonic) to these data in order to smooth them and obtain values
> > between measurements.  Any suggestions you can provide on how to do
> > this would be much appreciated.
> >
> > Thank you for your help.
> >
> > Sheri
> >
> > Sheri L. Conner Gausepohl
> > Graduate Research Assistant
> > Department of Atmospheric Science
> > Colorado State University
> 
> Try this:
> 
> library(its)
> dd <- read.table(myfile, header = TRUE)
> tt <- paste(dd$Y, "-", dd$M, "-", dd$D, " ", dd$H, ":", dd$Min, sep = "")
> co2 <- its(dd$CO2, as.POSIXct(tt))
> co2 # display co2
> 


or even easier use ISOdatetime:

library(its)
dd <- read.table(myfile, header = TRUE)
co2 <- with(dd, its(CO2, ISOdatetime(Y, M, D, H, Min, 0)))



From jnikelski at alumni.uwaterloo.ca  Wed Jul 13 01:50:11 2005
From: jnikelski at alumni.uwaterloo.ca (EJ Nikelski)
Date: Tue, 12 Jul 2005 19:50:11 -0400
Subject: [R] write.foreign, SPSS on Mac OS X
Message-ID: <42D45733.4080905@alumni.uwaterloo.ca>

Hi all,

     I have jut installed the foreign package (v 0.8-8) on my OS X 
machine, and have a bit of a problem writing out a data frame in SPSS 
format. Specifically, the code file (the .sps format file) seems to 
write 3 unprintable hex values instead of double quotes. For example, in 
the following output ...

VALUE LABELS
/
immDel
1 ###1###
  2 ###2###
  3 ###3###

  ... emacs tells me that the left-sided ### are the hex codes E2 80 9C, 
on the right we have E2 80 9D. I am supposing that I should be seeing 
double-quotes here? Interestingly, the data file, which also contains a 
quoted field, writes out the quotes without any problem. Does anyone 
have any ideas?

Thanks,

Jim



From Mike.Prager at noaa.gov  Wed Jul 13 01:56:39 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Tue, 12 Jul 2005 19:56:39 -0400
Subject: [R] Calling R from fortran
In-Reply-To: <42D421BE.5000104@gmail.com>
References: <42D421BE.5000104@gmail.com>
Message-ID: <42D458B7.3020906@noaa.gov>

Simon,

An hour's worth of contour plots (12) should be able to be saved in far
less than 1 Gb, depending on the detail desired.

Here is one way to do your task. There are other, and probably better, ones.

At each time step (5 mins), you could have the Fortran program write a
data file and then write and execute a script file.  The script file
would call R as a batch process to make a contour graph and save it (as
PNG perhaps), and then the script would delete the data file.  You might
want to have the generated script files omit the "delete" step once per
hour, just in case the R step should fail when you are not around to
observe the failure.

I don't know if R can easily read Fortran unformatted (sometimes called
"binary") data, or how.  That would speed up the Fortran I/O and
possibly reduce file size.  If you go with formatted output, consider
limiting the number of decimal places you write to save disk space.  In
most applications, you won't need great precision for simply making
graphs. Also, for writing large amounts of data, test various block
sizes on the Fortran OPEN statement to get best I/O speed.

Another possibility would be to use a Fortran library (like the free or
inexpensive DISLIN) that can make the contour plots directly, without
using R.

...Mike


simon wrote on 7/12/2005 4:02 PM:

>Hi,
>The following may sound stupid so please forgive my stupidness. I have a 
>question which I don't know how to name it so I have to start from the 
>beginning. In an attempt to gain better understand how a photochemical 
>air qaulity model works, I plotted hourly ozone concentration contour 
>from the model's standard output to observe the changes in simulation. 
>Because the model actually peroform simulation in time steps of 5 
>minutes, it will give a better resoultion if I can plot the ozone 
>concentration with time resolution close to the model. However, if I 
>modify the model to output data at every time step, it will be 
>impossible for the computer to store this much information( the hourly 
>standard output is close to 1gb). I was wondering if it is possible to 
>modify fortran to call R or something, which can just plot a contour at 
>every time step rather than put it as data. I am not sure about how to 
>call this act. Also, if this is possible, will output of a contour give 
>a smaller file size than data?
>Thanks in advance,
>simon
>
>  
>

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/



From murdoch at stats.uwo.ca  Wed Jul 13 02:05:49 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 12 Jul 2005 20:05:49 -0400
Subject: [R] Calling R from fortran
In-Reply-To: <42D458B7.3020906@noaa.gov>
References: <42D421BE.5000104@gmail.com> <42D458B7.3020906@noaa.gov>
Message-ID: <42D45ADD.2060803@stats.uwo.ca>

Michael Prager wrote:

> I don't know if R can easily read Fortran unformatted (sometimes called
> "binary") data, or how.  

Yes, it's fairly easy.  You would use a file() connection, and 
readBin().  The only tricky part might be figuring out exactly what is 
the R equivalent to what your fortran is writing.

Duncan Murdoch



From choid at ohsu.edu  Wed Jul 13 02:29:51 2005
From: choid at ohsu.edu (Dongseok Choi)
Date: Tue, 12 Jul 2005 17:29:51 -0700
Subject: [R] How to increase memory for R on Soliars 10 with 16GB and 64bit R
Message-ID: <s2d3fe1b.033@ohsu.edu>

Dear all,

  My machine is SUN Java Workstation 2100 with 2 AMD Opteron CPUs and 16GB RAM.
  R is compiled as 64bit by using SUN compilers. 
  I trying to fit quantile smoothing on my data and I got an message as below.

> fit1<-rqss(z1~qss(cbind(x,y),lambda=la1),tau=t1)
Error in as.matrix.csr(diag(n)) : cannot allocate memory block of size 2496135168

  The lengths of vector x and y are both 17664.
  I tried and found that the same command ran with x[1:16008] and y[1:16008].
  So, it looks to me a memory related problem, but I'm not sure how I can allocate memory block.
   I read the command line option but not sure what do to with it.
   Could you help me on this?  

Thank you very much,



Dongseok Choi, Ph.D.
Assistant Professor
Division of Biostatistics
Department of Public Health & Preventive Medicine
Oregon Health & Science University
3181 SW Sam Jackson Park Road, CB-669
Portland, OR 97239-3098
TEL) 503-494-5336
FAX) 503-494-4981
choid at ohsu.edu



From Ivy_Li at smics.com  Wed Jul 13 02:52:38 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Wed, 13 Jul 2005 08:52:38 +0800
Subject: =?utf-8?Q?=E7=AD=94=E5=A4=8D=3A_=5BR=5D_fail_in_adding_library_i?=
	=?utf-8?Q?n_new_version=2E?=
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF2E@ex120.smic-sh.com>

Dear all,
I really appreciate your help. I think I have a little advancement. ^_^
Now I use the package.skeleton() function to create a template. I type:
	f <- function(x,y) x+y
	g <- function(x,y) x-y
	d <- data.frame(a=1, b=2)
	e <- rnorm(1000)
	package.skeleton(list=c("f","g","d","e"), name="example")

in R. I know it will create a folder named "example" in the path of "\R\rw2011\" I opened this folder, its format is similar as other library. Then I modify it "DESCRIPTION" file:
	Package: example
	Version: 1.0-1
	Date: 2005-07-09
	Title: My first function
	Author: Ivy <Ivy_Li at smics.com>
	Maintainer: Ivy <Ivy_Li at smics.com>
	Description: simple sum and subtract
	License: GPL version 2 or later
	Depends: R (>= 1.9), stats, graphics, utils

I don't whether I should modify other "README" file.
When I enter the Dos environment, at first, into the D:\>, I type the following code:
	cd Program Files\R\rw2011\
	bin\R CMD install /example

Well, there appeared error:
	---------- Making package example ------------
	  adding build stamp to DESCRIPTION
	  installing R files
	  installing data files
	  installing man source files
	  installing indices
	  not zipping data
	  installing help
	 >>> Building/Updating help pages for package 'example'
	     Formats: text html latex example chm
	  d                                 text    html    latex   example chm
	  e                                 text    html    latex   example chm
	  f                                 text    html    latex   example chm
	     missing link(s):  ~~fun~~
	  g                                 text    html    latex   example chm
	     missing link(s):  ~~fun~~
	hhc: not found
	cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
	directory
	make[1]: *** [chm-example] Error 1
	make: *** [pkg-example] Error 2
	*** Installation of example failed ***
	
	Removing 'D:/PROGRA~1/R/rw2011/library/example'

That's it. I have to consult every R expert. Please help to solve this issue. Thank you very much!




----------
: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
: 200578 19:34
: Ivy_Li
: r-help at stat.math.ethz.ch
: Re: [R] fail in adding library in new version.


Ivy_Li wrote:
> Dear all,
> 	I really appreciate your help. I think I have a little advancement. ^_^
> 	
> 	When I enter the Dos environment, at first, into the D:\>, I type the following code:
> cd Program Files\R\rw2011\
> bin\R CMD install /example
> 
> "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.

Read the R Extensions manual for a detailed description.  You can use 
the package.skeleton() function to create a template, but you need to 
edit it to make it acceptable.
> 
> Well, there are still aother error:
> 
> 	---------- Making package example ------------
> 	  adding build stamp to DESCRIPTION
> 	error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format

That's complaining about your bad DESCRIPTION file.

Duncan Murdoch
> 	Stop execute
> 	make[2]: *** [frontmatter] Error 1
> 	make[1]: *** [all] Error 2
> 	make: *** [pkg-example] Error 2
> 	*** Installation of example failed ***
> 	Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> Please tell me which step is wrong?
> Thanks a lot!
> 
> BG
> Ivy_Li
>  
> 
> ----------
> : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> : 200577 20:57
> : Uwe Ligges
> : Ivy_Li; r-help at stat.math.ethz.ch
> : Re: : : [R] fail in adding library in new version.
> 
> 
> On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Gabor Grothendieck wrote:
>>
>>
>>>On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>>
>>>
>>>>Ivy_Li wrote:
>>>>
>>>>
>>>>
>>>>>Dear all,
>>>>>     I have done every step as the previous mail.
>>>>>1. unpack tools.zip into c:\cygwin
>>>>>2. install Active perl in c:\Perl
>>>>>3. install the mingw32 in c:\mingwin
>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
>>>>
>>>>                  ^
>>>>such blanks are not allowed in the PATH variable
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>>     Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
>>>>>      So in the Dos environment, at first, into the D:\>, I type the following code:
>>>>>cd Program Files\R\rw2011\
>>>>
>>>>MyRpackages does not need to be here.
>>>>
>>>>
>>>>
>>>>>bin\R CMD install /MyRpackages/example
>>>>
>>>>The first slash in "/MyRPackages" sugests that this is a top level
>>>>directory, which does not exist.
>>>>Even better, cd to MyRpackages, add R's bin dir to your path variable,
>>>>and simply say:
>>>>
>>>>R CMD INSTALL example
>>>
>>>
>>>Another possibility is to put Rcmd.bat from the batch file collection
>>>
>>>   http://cran.r-project.org/contrib/extra/batchfiles/
>>>
>>>in your path.  It will use the registry to find R so you won't have
>>>to modify your path (nor would you have to remodify it every time you
>>>install a new version of R which is what you would otherwise have to do):
>>>
>>>cd \MyPackages
>>>Rcmd install example
>>
>>
>>Just for the records:
>>
>>1. "cd \MyPackages" won't work, as I have already explained above.
> 
> 
> If MyPackages is not a top level directory in the current drive
> then it will not work. Otherwise it does work.
> 
> 
>>2. I do *not* recommend this way, in particular I find it misleading to
>>provide these batch files on CRAN.
>>
> 
> 
> The alternative, at least as discussed in your post, is more work
> since one will then have to change one's path every time one
> reinstalls R.  This is just needless extra work and is error prone.  If you
> forget to do it then you will be accessing the bin directory of the
> wrong version of R.
> 
> 
>>>>>     There are some error:
>>>>>'make' is neither internal or external command, nor executable operation or batch file
>>>>>*** installation of example failed ***
>>>>
>>>>Well, make.exe is not find in your path. Please check whether the file
>>>>exists and the path has been added.
>>>>
>>>>Uwe Ligges
>>>>
>>>>
>>>>
>>>>
>>>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
>>>>>
>>>>>I think I have closed to success. heehee~~~~~
>>>>>Thank you for your help.
>>>>>I still need you and others help. Thank you very much!
>>>>>
>>>>>
>>>>>----------
>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>>>>: 2005630 19:16
>>>>>: Ivy_Li
>>>>>: r-help at stat.math.ethz.ch
>>>>>: Re: : [R] fail in adding library in new version.
>>>>>
>>>>>
>>>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>>>>
>>>>>
>>>>>
>>>>>>Dear Gabor,
>>>>>>     Thank your for helping me so much!
>>>>>>     I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
>>>>>>1. unpack tools.zip into c:\cygwin
>>>>>>2. install Active perl in c:\Perl
>>>>>>3. install the mingw32 in c:\mingwin
>>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
>>>>>
>>>>>
>>>>>If in the console you enter the command:
>>>>>
>>>>>path
>>>>>
>>>>>then it will display a semicolon separated list of folders.  You want the folder
>>>>>that contains the tools to be at the beginning so that you eliminate
>>>>>the possibility
>>>>>of finding a different program of the same name first in a folder that comes
>>>>>prior to the one where the tools are stored.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
>>>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
>>>>>>
>>>>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
>>>>>>cd \Program Files\R\rw2010
>>>>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
>>>>>
>>>>>
>>>>>I was assuming that MyRPackages and R are on the same disk.  If they are not
>>>>>then you need to specify the disk too.  That is if MyRPackages is on C and R
>>>>>is installed on D then install your package via:
>>>>>
>>>>>d:
>>>>>cd \Program Files\R\rw2010
>>>>>bin\R CMD install c:/MyRPackages/example
>>>>>
>>>>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
>>>>>using command script install and the indicated source package.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
>>>>>
>>>>>
>>>>>If you are not sure where R is installed then enter the following at the Windows
>>>>>console prompt to find out (this will work provided you let it install the key
>>>>>into the registry when you installed R initially).  The reg command is a command
>>>>>built into Windows (I used XP but I assume its the same on other versions)
>>>>>that will query the Windows registry:
>>>>>
>>>>>reg query hklm\software\r-core\r /v InstallPath
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>I still need your and others help. Thank you very much!
>>>>>>
>>>>>>
>>>>>>
>>>>>>----------
>>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
>>>>>>: 200566 10:21
>>>>>>: Ivy_Li
>>>>>>: r-help at stat.math.ethz.ch
>>>>>>: Re: [R] fail in adding library in new version.
>>>>>>
>>>>>>
>>>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
>>>>>>
>>>>>>
>>>>>>
>>>>>>>Hello everybody,
>>>>>>>     Could I consult you a question?
>>>>>>>     I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
>>>>>>
>>>>>>Getting the latest version of R is strongly recommended.  The suggestions
>>>>>>below all assume the latest version and may or may not work if you do
>>>>>>not upgrade.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
>>>>>>>*       Download the tools.zip
>>>>>>>*       Unpack tools.zip into c:\cygwin
>>>>>>>*       Install Active Perl in c:\Perl
>>>>>>>*       Install the mingw32 port of gcc in c:\mingwin
>>>>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
>>>>>>
>>>>>>You may need to put these at the beginning of the path rather than the end.
>>>>>>Also just as a check enter
>>>>>>  path
>>>>>>at the console to make sure that you have them.  You will likely
>>>>>>have to start a new console session and possibly even reboot.
>>>>>>
>>>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
>>>>>>you reread the material on which tools you need.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
>>>>>>
>>>>>>In MyRPackages you would have a folder called example, in your case,
>>>>>>that contains the package.  Within folder example, you would have the
>>>>>>DESCRIPTION file, the R folder, etc.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
>>>>>>
>>>>>>You don't have to run R first.  You do need to make sure that R.exe can
>>>>>>be found on your path or else use the absolute path name in referring to R.
>>>>>>For example, if your path does not include R you could do something like this:
>>>>>>
>>>>>>cd \Program Files\R\rw2010
>>>>>>bin\R cmd install /MyRPackages/example
>>>>>
>>>>>
>>>>>Sorry, there is an error in the above.  It should be:
>>>>>
>>>>>bin\R CMD install c:/MyRPackages/example
>>>>>
>>>>>or
>>>>>
>>>>>bin\Rcmd install c:/MyRPackages/example
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>Be sure to use forward slashes where shown above and backslashes
>>>>>>where shown.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>     So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
>>>>>>>
>>>>>>
>>>>>>Try all these suggestions including upgrading R and if that does not work
>>>>>>try posting screen dumps of the actual errors you are getting.
>>>>>>
>>>>>
>>>>>
>>>>>Also try googling for
>>>>>
>>>>> making creating R packages
>>>>>
>>>>>and you will find some privately written tutorials on all this.
>>>>>
>>>>>
>>>>>
>>>>>------------------------------------------------------------------------
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Wed Jul 13 03:31:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 21:31:51 -0400
Subject: =?GB2312?B?UmU6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZGlu?=
	=?GB2312?B?ZyBsaWJyYXJ5IGluIG5ldyB2ZXJzaW9uLg==?=
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF2E@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF2E@ex120.smic-sh.com>
Message-ID: <971536df05071218313930de59@mail.gmail.com>

hhc.exe is the Microsoft help compiler.  You have to download it and put
it somewhere in your path.

On 7/12/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Dear all,
> I really appreciate your help. I think I have a little advancement. ^_^
> Now I use the package.skeleton() function to create a template. I type:
>        f <- function(x,y) x+y
>        g <- function(x,y) x-y
>        d <- data.frame(a=1, b=2)
>        e <- rnorm(1000)
>        package.skeleton(list=c("f","g","d","e"), name="example")
> 
> in R. I know it will create a folder named "example" in the path of "\R\rw2011\" I opened this folder, its format is similar as other library. Then I modify it "DESCRIPTION" file:
>        Package: example
>        Version: 1.0-1
>        Date: 2005-07-09
>        Title: My first function
>        Author: Ivy <Ivy_Li at smics.com>
>        Maintainer: Ivy <Ivy_Li at smics.com>
>        Description: simple sum and subtract
>        License: GPL version 2 or later
>        Depends: R (>= 1.9), stats, graphics, utils
> 
> I don't whether I should modify other "README" file.
> When I enter the Dos environment, at first, into the D:\>, I type the following code:
>        cd Program Files\R\rw2011\
>        bin\R CMD install /example
> 
> Well, there appeared error:
>        ---------- Making package example ------------
>          adding build stamp to DESCRIPTION
>          installing R files
>          installing data files
>          installing man source files
>          installing indices
>          not zipping data
>          installing help
>         >>> Building/Updating help pages for package 'example'
>             Formats: text html latex example chm
>          d                                 text    html    latex   example chm
>          e                                 text    html    latex   example chm
>          f                                 text    html    latex   example chm
>             missing link(s):  ~~fun~~
>          g                                 text    html    latex   example chm
>             missing link(s):  ~~fun~~
>        hhc: not found
>        cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
>        directory
>        make[1]: *** [chm-example] Error 1
>        make: *** [pkg-example] Error 2
>        *** Installation of example failed ***
> 
>        Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> That's it. I have to consult every R expert. Please help to solve this issue. Thank you very much!
> 
> 
> 
> 
> ----------
> : Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> : 200578 19:34
> : Ivy_Li
> : r-help at stat.math.ethz.ch
> : Re: [R] fail in adding library in new version.
> 
> 
> Ivy_Li wrote:
> > Dear all,
> >       I really appreciate your help. I think I have a little advancement. ^_^
> >
> >       When I enter the Dos environment, at first, into the D:\>, I type the following code:
> > cd Program Files\R\rw2011\
> > bin\R CMD install /example
> >
> > "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.
> 
> Read the R Extensions manual for a detailed description.  You can use
> the package.skeleton() function to create a template, but you need to
> edit it to make it acceptable.
> >
> > Well, there are still aother error:
> >
> >       ---------- Making package example ------------
> >         adding build stamp to DESCRIPTION
> >       error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
> 
> That's complaining about your bad DESCRIPTION file.
> 
> Duncan Murdoch
> >       Stop execute
> >       make[2]: *** [frontmatter] Error 1
> >       make[1]: *** [all] Error 2
> >       make: *** [pkg-example] Error 2
> >       *** Installation of example failed ***
> >       Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >
> > Please tell me which step is wrong?
> > Thanks a lot!
> >
> > BG
> > Ivy_Li
> >
> >
> > ----------
> > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > : 200577 20:57
> > : Uwe Ligges
> > : Ivy_Li; r-help at stat.math.ethz.ch
> > : Re: : : [R] fail in adding library in new version.
> >
> >
> > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >
> >>Gabor Grothendieck wrote:
> >>
> >>
> >>>On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >>>
> >>>
> >>>>Ivy_Li wrote:
> >>>>
> >>>>
> >>>>
> >>>>>Dear all,
> >>>>>     I have done every step as the previous mail.
> >>>>>1. unpack tools.zip into c:\cygwin
> >>>>>2. install Active perl in c:\Perl
> >>>>>3. install the mingw32 in c:\mingwin
> >>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> >>>>
> >>>>                  ^
> >>>>such blanks are not allowed in the PATH variable
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>     Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> >>>>>      So in the Dos environment, at first, into the D:\>, I type the following code:
> >>>>>cd Program Files\R\rw2011\
> >>>>
> >>>>MyRpackages does not need to be here.
> >>>>
> >>>>
> >>>>
> >>>>>bin\R CMD install /MyRpackages/example
> >>>>
> >>>>The first slash in "/MyRPackages" sugests that this is a top level
> >>>>directory, which does not exist.
> >>>>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> >>>>and simply say:
> >>>>
> >>>>R CMD INSTALL example
> >>>
> >>>
> >>>Another possibility is to put Rcmd.bat from the batch file collection
> >>>
> >>>   http://cran.r-project.org/contrib/extra/batchfiles/
> >>>
> >>>in your path.  It will use the registry to find R so you won't have
> >>>to modify your path (nor would you have to remodify it every time you
> >>>install a new version of R which is what you would otherwise have to do):
> >>>
> >>>cd \MyPackages
> >>>Rcmd install example
> >>
> >>
> >>Just for the records:
> >>
> >>1. "cd \MyPackages" won't work, as I have already explained above.
> >
> >
> > If MyPackages is not a top level directory in the current drive
> > then it will not work. Otherwise it does work.
> >
> >
> >>2. I do *not* recommend this way, in particular I find it misleading to
> >>provide these batch files on CRAN.
> >>
> >
> >
> > The alternative, at least as discussed in your post, is more work
> > since one will then have to change one's path every time one
> > reinstalls R.  This is just needless extra work and is error prone.  If you
> > forget to do it then you will be accessing the bin directory of the
> > wrong version of R.
> >
> >
> >>>>>     There are some error:
> >>>>>'make' is neither internal or external command, nor executable operation or batch file
> >>>>>*** installation of example failed ***
> >>>>
> >>>>Well, make.exe is not find in your path. Please check whether the file
> >>>>exists and the path has been added.
> >>>>
> >>>>Uwe Ligges
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >>>>>
> >>>>>I think I have closed to success. heehee~~~~~
> >>>>>Thank you for your help.
> >>>>>I still need you and others help. Thank you very much!
> >>>>>
> >>>>>
> >>>>>----------
> >>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>>>: 2005630 19:16
> >>>>>: Ivy_Li
> >>>>>: r-help at stat.math.ethz.ch
> >>>>>: Re: : [R] fail in adding library in new version.
> >>>>>
> >>>>>
> >>>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>>>
> >>>>>
> >>>>>
> >>>>>>Dear Gabor,
> >>>>>>     Thank your for helping me so much!
> >>>>>>     I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> >>>>>>1. unpack tools.zip into c:\cygwin
> >>>>>>2. install Active perl in c:\Perl
> >>>>>>3. install the mingw32 in c:\mingwin
> >>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> >>>>>
> >>>>>
> >>>>>If in the console you enter the command:
> >>>>>
> >>>>>path
> >>>>>
> >>>>>then it will display a semicolon separated list of folders.  You want the folder
> >>>>>that contains the tools to be at the beginning so that you eliminate
> >>>>>the possibility
> >>>>>of finding a different program of the same name first in a folder that comes
> >>>>>prior to the one where the tools are stored.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> >>>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> >>>>>>
> >>>>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> >>>>>>cd \Program Files\R\rw2010
> >>>>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> >>>>>
> >>>>>
> >>>>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> >>>>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> >>>>>is installed on D then install your package via:
> >>>>>
> >>>>>d:
> >>>>>cd \Program Files\R\rw2010
> >>>>>bin\R CMD install c:/MyRPackages/example
> >>>>>
> >>>>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> >>>>>using command script install and the indicated source package.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> >>>>>
> >>>>>
> >>>>>If you are not sure where R is installed then enter the following at the Windows
> >>>>>console prompt to find out (this will work provided you let it install the key
> >>>>>into the registry when you installed R initially).  The reg command is a command
> >>>>>built into Windows (I used XP but I assume its the same on other versions)
> >>>>>that will query the Windows registry:
> >>>>>
> >>>>>reg query hklm\software\r-core\r /v InstallPath
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>I still need your and others help. Thank you very much!
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>----------
> >>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>>>>: 200566 10:21
> >>>>>>: Ivy_Li
> >>>>>>: r-help at stat.math.ethz.ch
> >>>>>>: Re: [R] fail in adding library in new version.
> >>>>>>
> >>>>>>
> >>>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>Hello everybody,
> >>>>>>>     Could I consult you a question?
> >>>>>>>     I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> >>>>>>
> >>>>>>Getting the latest version of R is strongly recommended.  The suggestions
> >>>>>>below all assume the latest version and may or may not work if you do
> >>>>>>not upgrade.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>>>>>>*       Download the tools.zip
> >>>>>>>*       Unpack tools.zip into c:\cygwin
> >>>>>>>*       Install Active Perl in c:\Perl
> >>>>>>>*       Install the mingw32 port of gcc in c:\mingwin
> >>>>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> >>>>>>
> >>>>>>You may need to put these at the beginning of the path rather than the end.
> >>>>>>Also just as a check enter
> >>>>>>  path
> >>>>>>at the console to make sure that you have them.  You will likely
> >>>>>>have to start a new console session and possibly even reboot.
> >>>>>>
> >>>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> >>>>>>you reread the material on which tools you need.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> >>>>>>
> >>>>>>In MyRPackages you would have a folder called example, in your case,
> >>>>>>that contains the package.  Within folder example, you would have the
> >>>>>>DESCRIPTION file, the R folder, etc.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> >>>>>>
> >>>>>>You don't have to run R first.  You do need to make sure that R.exe can
> >>>>>>be found on your path or else use the absolute path name in referring to R.
> >>>>>>For example, if your path does not include R you could do something like this:
> >>>>>>
> >>>>>>cd \Program Files\R\rw2010
> >>>>>>bin\R cmd install /MyRPackages/example
> >>>>>
> >>>>>
> >>>>>Sorry, there is an error in the above.  It should be:
> >>>>>
> >>>>>bin\R CMD install c:/MyRPackages/example
> >>>>>
> >>>>>or
> >>>>>
> >>>>>bin\Rcmd install c:/MyRPackages/example
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>Be sure to use forward slashes where shown above and backslashes
> >>>>>>where shown.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>     So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >>>>>>>
> >>>>>>
> >>>>>>Try all these suggestions including upgrading R and if that does not work
> >>>>>>try posting screen dumps of the actual errors you are getting.
> >>>>>>
> >>>>>
> >>>>>
> >>>>>Also try googling for
> >>>>>
> >>>>> making creating R packages
> >>>>>
> >>>>>and you will find some privately written tutorials on all this.
> >>>>>
> >>>>>
> >>>>>
> >>>>>------------------------------------------------------------------------
> >>>>>
> >>>>>______________________________________________
> >>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>>
> >>>>
> >>
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r.shengzhe at gmail.com  Wed Jul 13 03:48:06 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Wed, 13 Jul 2005 03:48:06 +0200
Subject: [R] help: how to plot a circle on the scatter plot
Message-ID: <ea57975b050712184866ade0c@mail.gmail.com>

Hello,

I have a data set with 15 variables, and use "pairs" to plot the
scatterplot of this data set. Then I want to plot some circles on the
small pictures with high correlation(e.g. > 0.9).

First, I use "cor" to obtain the corresponding correlation matrix (x)
for this scatterplot.

Second, use "seq(along = x)[x > 0.9]" to find the positions of the
small pictures with 0.9 correlation, but "seq" can just find the
sequence position row by row for the matrix, not a real position (like
"rowNumber, colNumber"). Is any function for that?

Third, use "Symbols" to plot the circle on these small pictures, but
seems it can't do that. Also, I don't know how to adjust the thickness
and radius of the circle plotted by "Symbols".

Please give me some ideas!

Thank you,
Shengzhe



From klebyn at yahoo.com.br  Wed Jul 13 04:19:55 2005
From: klebyn at yahoo.com.br (klebyn)
Date: Wed, 13 Jul 2005 00:19:55 -0200
Subject: [R] How to use the function "plot"  as Matlab?
Message-ID: <42D47A4B.7000608@yahoo.com.br>

Hello,

How to use the function plot to produce graphs as Matlab?
example in Matlab:

a = [1,2,5,3,6,8,1,7];
b = [1,7,2,9,2,3,4,5];
plot(a,'b')
hold
plot(b,'r')


How to make the same in R-package ?

I am trying something thus:

a <- c(1,2,5,3,6,8,1,7)
c(1,7,2,9,2,3,4,5) -> b

a;b

plot(a,t="l",col="blue")
plot(b,t="l",col="red")



From ggrothendieck at gmail.com  Wed Jul 13 05:30:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 23:30:22 -0400
Subject: [R] How to use the function "plot" as Matlab?
In-Reply-To: <42D47A4B.7000608@yahoo.com.br>
References: <42D47A4B.7000608@yahoo.com.br>
Message-ID: <971536df05071220307eb6ea01@mail.gmail.com>

On 7/12/05, klebyn <klebyn at yahoo.com.br> wrote:
> Hello,
> 
> How to use the function plot to produce graphs as Matlab?
> example in Matlab:
> 
> a = [1,2,5,3,6,8,1,7];
> b = [1,7,2,9,2,3,4,5];
> plot(a,'b')
> hold
> plot(b,'r')
> 
> 
> How to make the same in R-package ?
> 
> I am trying something thus:
> 
> a <- c(1,2,5,3,6,8,1,7)
> c(1,7,2,9,2,3,4,5) -> b
> 
> a;b
> 
> plot(a,t="l",col="blue")
> plot(b,t="l",col="red")
> 

In addition to the Intro manual and other usual sources (see the posting guide 
at the end of every post to this list) see:
   http://cran.r-project.org/doc/contrib/R-and-octave-2.txt



From Ivy_Li at smics.com  Wed Jul 13 05:44:01 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Wed, 13 Jul 2005 11:44:01 +0800
Subject: =?gb2312?B?tPC4tDogtPC4tDogW1JdIGZhaWwgaW4gYWRkaW5nIGxpYnJhcnkgaQ==?=
	=?gb2312?B?biBuZXcgdmVyc2lvbi4=?=
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF2F@ex120.smic-sh.com>

Dear All,
	I have downloaded the htmlhelp.exe from InterNet. Install it into the path "C:\Program Files\HTML Help Workshop\". I add this path into the "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable". 
	But it still exist error.
	---------- Making package example ------------
  	  adding build stamp to DESCRIPTION
	  installing R files
	  installing data files
	  installing man source files
	  installing indices
	  not zipping data
	  installing help
	 >>> Building/Updating help pages for package 'example'
	     Formats: text html latex example chm
	  d                                 text    html    latex   example
	  e                                 text    html    latex   example
	  f                                 text    html    latex   example
	  g                                 text    html    latex   example
	hhc: not found
	cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
	directory
	make[1]: *** [chm-example] Error 1
	make: *** [pkg-example] Error 2
	*** Installation of example failed ***
	
	Removing 'D:/PROGRA~1/R/rw2011/library/example'
	Restoring previous 'D:/PROGRA~1/R/rw2011/library/example'

I don't know why it can not find the "hhc" file. 
Please tell me which step is wrong.
Thank you for helping me!
^_^

----------
: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
: 2005713 9:32
: Ivy_Li
: Duncan Murdoch; r-help at stat.math.ethz.ch
: Re: : [R] fail in adding library in new version.


hhc.exe is the Microsoft help compiler.  You have to download it and put
it somewhere in your path.

On 7/12/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Dear all,
> I really appreciate your help. I think I have a little advancement. ^_^
> Now I use the package.skeleton() function to create a template. I type:
>        f <- function(x,y) x+y
>        g <- function(x,y) x-y
>        d <- data.frame(a=1, b=2)
>        e <- rnorm(1000)
>        package.skeleton(list=c("f","g","d","e"), name="example")
> 
> in R. I know it will create a folder named "example" in the path of "\R\rw2011\" I opened this folder, its format is similar as other library. Then I modify it "DESCRIPTION" file:
>        Package: example
>        Version: 1.0-1
>        Date: 2005-07-09
>        Title: My first function
>        Author: Ivy <Ivy_Li at smics.com>
>        Maintainer: Ivy <Ivy_Li at smics.com>
>        Description: simple sum and subtract
>        License: GPL version 2 or later
>        Depends: R (>= 1.9), stats, graphics, utils
> 
> I don't whether I should modify other "README" file.
> When I enter the Dos environment, at first, into the D:\>, I type the following code:
>        cd Program Files\R\rw2011\
>        bin\R CMD install /example
> 
> Well, there appeared error:
>        ---------- Making package example ------------
>          adding build stamp to DESCRIPTION
>          installing R files
>          installing data files
>          installing man source files
>          installing indices
>          not zipping data
>          installing help
>         >>> Building/Updating help pages for package 'example'
>             Formats: text html latex example chm
>          d                                 text    html    latex   example chm
>          e                                 text    html    latex   example chm
>          f                                 text    html    latex   example chm
>             missing link(s):  ~~fun~~
>          g                                 text    html    latex   example chm
>             missing link(s):  ~~fun~~
>        hhc: not found
>        cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
>        directory
>        make[1]: *** [chm-example] Error 1
>        make: *** [pkg-example] Error 2
>        *** Installation of example failed ***
> 
>        Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> That's it. I have to consult every R expert. Please help to solve this issue. Thank you very much!
> 
> 
> 
> 
> ----------
> : Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> : 200578 19:34
> : Ivy_Li
> : r-help at stat.math.ethz.ch
> : Re: [R] fail in adding library in new version.
> 
> 
> Ivy_Li wrote:
> > Dear all,
> >       I really appreciate your help. I think I have a little advancement. ^_^
> >
> >       When I enter the Dos environment, at first, into the D:\>, I type the following code:
> > cd Program Files\R\rw2011\
> > bin\R CMD install /example
> >
> > "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.
> 
> Read the R Extensions manual for a detailed description.  You can use
> the package.skeleton() function to create a template, but you need to
> edit it to make it acceptable.
> >
> > Well, there are still aother error:
> >
> >       ---------- Making package example ------------
> >         adding build stamp to DESCRIPTION
> >       error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
> 
> That's complaining about your bad DESCRIPTION file.
> 
> Duncan Murdoch
> >       Stop execute
> >       make[2]: *** [frontmatter] Error 1
> >       make[1]: *** [all] Error 2
> >       make: *** [pkg-example] Error 2
> >       *** Installation of example failed ***
> >       Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >
> > Please tell me which step is wrong?
> > Thanks a lot!
> >
> > BG
> > Ivy_Li
> >
> >
> > ----------
> > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > : 200577 20:57
> > : Uwe Ligges
> > : Ivy_Li; r-help at stat.math.ethz.ch
> > : Re: : : [R] fail in adding library in new version.
> >
> >
> > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >
> >>Gabor Grothendieck wrote:
> >>
> >>
> >>>On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >>>
> >>>
> >>>>Ivy_Li wrote:
> >>>>
> >>>>
> >>>>
> >>>>>Dear all,
> >>>>>     I have done every step as the previous mail.
> >>>>>1. unpack tools.zip into c:\cygwin
> >>>>>2. install Active perl in c:\Perl
> >>>>>3. install the mingw32 in c:\mingwin
> >>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> >>>>
> >>>>                  ^
> >>>>such blanks are not allowed in the PATH variable
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>     Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> >>>>>      So in the Dos environment, at first, into the D:\>, I type the following code:
> >>>>>cd Program Files\R\rw2011\
> >>>>
> >>>>MyRpackages does not need to be here.
> >>>>
> >>>>
> >>>>
> >>>>>bin\R CMD install /MyRpackages/example
> >>>>
> >>>>The first slash in "/MyRPackages" sugests that this is a top level
> >>>>directory, which does not exist.
> >>>>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> >>>>and simply say:
> >>>>
> >>>>R CMD INSTALL example
> >>>
> >>>
> >>>Another possibility is to put Rcmd.bat from the batch file collection
> >>>
> >>>   http://cran.r-project.org/contrib/extra/batchfiles/
> >>>
> >>>in your path.  It will use the registry to find R so you won't have
> >>>to modify your path (nor would you have to remodify it every time you
> >>>install a new version of R which is what you would otherwise have to do):
> >>>
> >>>cd \MyPackages
> >>>Rcmd install example
> >>
> >>
> >>Just for the records:
> >>
> >>1. "cd \MyPackages" won't work, as I have already explained above.
> >
> >
> > If MyPackages is not a top level directory in the current drive
> > then it will not work. Otherwise it does work.
> >
> >
> >>2. I do *not* recommend this way, in particular I find it misleading to
> >>provide these batch files on CRAN.
> >>
> >
> >
> > The alternative, at least as discussed in your post, is more work
> > since one will then have to change one's path every time one
> > reinstalls R.  This is just needless extra work and is error prone.  If you
> > forget to do it then you will be accessing the bin directory of the
> > wrong version of R.
> >
> >
> >>>>>     There are some error:
> >>>>>'make' is neither internal or external command, nor executable operation or batch file
> >>>>>*** installation of example failed ***
> >>>>
> >>>>Well, make.exe is not find in your path. Please check whether the file
> >>>>exists and the path has been added.
> >>>>
> >>>>Uwe Ligges
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >>>>>
> >>>>>I think I have closed to success. heehee~~~~~
> >>>>>Thank you for your help.
> >>>>>I still need you and others help. Thank you very much!
> >>>>>
> >>>>>
> >>>>>----------
> >>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>>>: 2005630 19:16
> >>>>>: Ivy_Li
> >>>>>: r-help at stat.math.ethz.ch
> >>>>>: Re: : [R] fail in adding library in new version.
> >>>>>
> >>>>>
> >>>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>>>
> >>>>>
> >>>>>
> >>>>>>Dear Gabor,
> >>>>>>     Thank your for helping me so much!
> >>>>>>     I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> >>>>>>1. unpack tools.zip into c:\cygwin
> >>>>>>2. install Active perl in c:\Perl
> >>>>>>3. install the mingw32 in c:\mingwin
> >>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> >>>>>
> >>>>>
> >>>>>If in the console you enter the command:
> >>>>>
> >>>>>path
> >>>>>
> >>>>>then it will display a semicolon separated list of folders.  You want the folder
> >>>>>that contains the tools to be at the beginning so that you eliminate
> >>>>>the possibility
> >>>>>of finding a different program of the same name first in a folder that comes
> >>>>>prior to the one where the tools are stored.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> >>>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> >>>>>>
> >>>>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> >>>>>>cd \Program Files\R\rw2010
> >>>>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> >>>>>
> >>>>>
> >>>>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> >>>>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> >>>>>is installed on D then install your package via:
> >>>>>
> >>>>>d:
> >>>>>cd \Program Files\R\rw2010
> >>>>>bin\R CMD install c:/MyRPackages/example
> >>>>>
> >>>>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> >>>>>using command script install and the indicated source package.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> >>>>>
> >>>>>
> >>>>>If you are not sure where R is installed then enter the following at the Windows
> >>>>>console prompt to find out (this will work provided you let it install the key
> >>>>>into the registry when you installed R initially).  The reg command is a command
> >>>>>built into Windows (I used XP but I assume its the same on other versions)
> >>>>>that will query the Windows registry:
> >>>>>
> >>>>>reg query hklm\software\r-core\r /v InstallPath
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>I still need your and others help. Thank you very much!
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>----------
> >>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> >>>>>>: 200566 10:21
> >>>>>>: Ivy_Li
> >>>>>>: r-help at stat.math.ethz.ch
> >>>>>>: Re: [R] fail in adding library in new version.
> >>>>>>
> >>>>>>
> >>>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>Hello everybody,
> >>>>>>>     Could I consult you a question?
> >>>>>>>     I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> >>>>>>
> >>>>>>Getting the latest version of R is strongly recommended.  The suggestions
> >>>>>>below all assume the latest version and may or may not work if you do
> >>>>>>not upgrade.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>>>>>>*       Download the tools.zip
> >>>>>>>*       Unpack tools.zip into c:\cygwin
> >>>>>>>*       Install Active Perl in c:\Perl
> >>>>>>>*       Install the mingw32 port of gcc in c:\mingwin
> >>>>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> >>>>>>
> >>>>>>You may need to put these at the beginning of the path rather than the end.
> >>>>>>Also just as a check enter
> >>>>>>  path
> >>>>>>at the console to make sure that you have them.  You will likely
> >>>>>>have to start a new console session and possibly even reboot.
> >>>>>>
> >>>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> >>>>>>you reread the material on which tools you need.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> >>>>>>
> >>>>>>In MyRPackages you would have a folder called example, in your case,
> >>>>>>that contains the package.  Within folder example, you would have the
> >>>>>>DESCRIPTION file, the R folder, etc.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> >>>>>>
> >>>>>>You don't have to run R first.  You do need to make sure that R.exe can
> >>>>>>be found on your path or else use the absolute path name in referring to R.
> >>>>>>For example, if your path does not include R you could do something like this:
> >>>>>>
> >>>>>>cd \Program Files\R\rw2010
> >>>>>>bin\R cmd install /MyRPackages/example
> >>>>>
> >>>>>
> >>>>>Sorry, there is an error in the above.  It should be:
> >>>>>
> >>>>>bin\R CMD install c:/MyRPackages/example
> >>>>>
> >>>>>or
> >>>>>
> >>>>>bin\Rcmd install c:/MyRPackages/example
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>>Be sure to use forward slashes where shown above and backslashes
> >>>>>>where shown.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>>     So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >>>>>>>
> >>>>>>
> >>>>>>Try all these suggestions including upgrading R and if that does not work
> >>>>>>try posting screen dumps of the actual errors you are getting.
> >>>>>>
> >>>>>
> >>>>>
> >>>>>Also try googling for
> >>>>>
> >>>>> making creating R packages
> >>>>>
> >>>>>and you will find some privately written tutorials on all this.
> >>>>>
> >>>>>
> >>>>>
> >>>>>------------------------------------------------------------------------
> >>>>>
> >>>>>______________________________________________
> >>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>>
> >>>>
> >>
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Tom.Mulholland at dpi.wa.gov.au  Wed Jul 13 05:58:36 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 13 Jul 2005 11:58:36 +0800
Subject: [R] help: how to plot a circle on the scatter plot
Message-ID: <4702645135092E4497088F71D9C8F51A128BC4@afhex01.dpi.wa.gov.au>

You will find previous discussion about pairs in this list. There are limits on what has been included within the functionality, but you can write your own panel functions. Here's a starting point.

x <- runif(100)
dim(x) <- c(20,5)

panel.cor1 <- function(x,y,...){
  mycor<- cor(x,y)
  if (mycor > 0){
  points(x,y, pch = 20, col = "red",cex = 0.5)
  } else {
  points(x,y, pch = 20, col = "grey",cex = 0.5)
  }
}

panel.cor2 <- function(x,y,...){
  mycor<- cor(x,y)
  if (mycor > 0){
  points(x,y, pch = 20, col = "blue",cex = 0.5)
  hpts <- chull(x,y)
  hpts <- c(hpts, hpts[1])
  lines(x[hpts],y[hpts],col = "red")
  } else {
  points(x,y, pch = 20, col = "grey",cex = 0.5)
  }
}


pairs(x,lower.panel = panel.cor1,upper.panel = panel.cor2)


Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of wu sz
> Sent: Wednesday, 13 July 2005 9:48 AM
> To: questions about R
> Subject: [R] help: how to plot a circle on the scatter plot
> 
> 
> Hello,
> 
> I have a data set with 15 variables, and use "pairs" to plot the
> scatterplot of this data set. Then I want to plot some circles on the
> small pictures with high correlation(e.g. > 0.9).
> 
> First, I use "cor" to obtain the corresponding correlation matrix (x)
> for this scatterplot.
> 
> Second, use "seq(along = x)[x > 0.9]" to find the positions of the
> small pictures with 0.9 correlation, but "seq" can just find the
> sequence position row by row for the matrix, not a real position (like
> "rowNumber, colNumber"). Is any function for that?
> 
> Third, use "Symbols" to plot the circle on these small pictures, but
> seems it can't do that. Also, I don't know how to adjust the thickness
> and radius of the circle plotted by "Symbols".
> 
> Please give me some ideas!
> 
> Thank you,
> Shengzhe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Jul 13 05:57:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Jul 2005 23:57:41 -0400
Subject: =?GB2312?B?UmU6ILTwuLQ6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZA==?=
	=?GB2312?B?aW5nIGxpYnJhcnkgaW4gbmV3IHZlcnNpb24u?=
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF2F@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF2F@ex120.smic-sh.com>
Message-ID: <971536df05071220574e150c44@mail.gmail.com>

If you start up a fresh batch console and run
  hhc
do you get the usage message?  If not you still have a path problem.  
You only need hhc.exe so just copy that file into any directory on your 
path -- issue the command:

path

to find out which directories you can copy it into (and also post the 
output of the path command if you still have problem) -- with this
approach you won't have to set your path to the 
HTML Help Workshop.

On 7/12/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Dear All,
>        I have downloaded the htmlhelp.exe from InterNet. Install it into the path "C:\Program Files\HTML Help Workshop\". I add this path into the "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable".
>        But it still exist error.
>        ---------- Making package example ------------
>          adding build stamp to DESCRIPTION
>          installing R files
>          installing data files
>          installing man source files
>          installing indices
>          not zipping data
>          installing help
>         >>> Building/Updating help pages for package 'example'
>             Formats: text html latex example chm
>          d                                 text    html    latex   example
>          e                                 text    html    latex   example
>          f                                 text    html    latex   example
>          g                                 text    html    latex   example
>        hhc: not found
>        cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
>        directory
>        make[1]: *** [chm-example] Error 1
>        make: *** [pkg-example] Error 2
>        *** Installation of example failed ***
> 
>        Removing 'D:/PROGRA~1/R/rw2011/library/example'
>        Restoring previous 'D:/PROGRA~1/R/rw2011/library/example'
> 
> I don't know why it can not find the "hhc" file.
> Please tell me which step is wrong.
> Thank you for helping me!
> ^_^
> 
> ----------
> : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> : 2005713 9:32
> : Ivy_Li
> : Duncan Murdoch; r-help at stat.math.ethz.ch
> : Re: : [R] fail in adding library in new version.
> 
> 
> hhc.exe is the Microsoft help compiler.  You have to download it and put
> it somewhere in your path.
> 
> On 7/12/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > Dear all,
> > I really appreciate your help. I think I have a little advancement. ^_^
> > Now I use the package.skeleton() function to create a template. I type:
> >        f <- function(x,y) x+y
> >        g <- function(x,y) x-y
> >        d <- data.frame(a=1, b=2)
> >        e <- rnorm(1000)
> >        package.skeleton(list=c("f","g","d","e"), name="example")
> >
> > in R. I know it will create a folder named "example" in the path of "\R\rw2011\" I opened this folder, its format is similar as other library. Then I modify it "DESCRIPTION" file:
> >        Package: example
> >        Version: 1.0-1
> >        Date: 2005-07-09
> >        Title: My first function
> >        Author: Ivy <Ivy_Li at smics.com>
> >        Maintainer: Ivy <Ivy_Li at smics.com>
> >        Description: simple sum and subtract
> >        License: GPL version 2 or later
> >        Depends: R (>= 1.9), stats, graphics, utils
> >
> > I don't whether I should modify other "README" file.
> > When I enter the Dos environment, at first, into the D:\>, I type the following code:
> >        cd Program Files\R\rw2011\
> >        bin\R CMD install /example
> >
> > Well, there appeared error:
> >        ---------- Making package example ------------
> >          adding build stamp to DESCRIPTION
> >          installing R files
> >          installing data files
> >          installing man source files
> >          installing indices
> >          not zipping data
> >          installing help
> >         >>> Building/Updating help pages for package 'example'
> >             Formats: text html latex example chm
> >          d                                 text    html    latex   example chm
> >          e                                 text    html    latex   example chm
> >          f                                 text    html    latex   example chm
> >             missing link(s):  ~~fun~~
> >          g                                 text    html    latex   example chm
> >             missing link(s):  ~~fun~~
> >        hhc: not found
> >        cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
> >        directory
> >        make[1]: *** [chm-example] Error 1
> >        make: *** [pkg-example] Error 2
> >        *** Installation of example failed ***
> >
> >        Removing 'D:/PROGRA~1/R/rw2011/library/example'
> >
> > That's it. I have to consult every R expert. Please help to solve this issue. Thank you very much!
> >
> >
> >
> >
> > ----------
> > : Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> > : 200578 19:34
> > : Ivy_Li
> > : r-help at stat.math.ethz.ch
> > : Re: [R] fail in adding library in new version.
> >
> >
> > Ivy_Li wrote:
> > > Dear all,
> > >       I really appreciate your help. I think I have a little advancement. ^_^
> > >
> > >       When I enter the Dos environment, at first, into the D:\>, I type the following code:
> > > cd Program Files\R\rw2011\
> > > bin\R CMD install /example
> > >
> > > "example" is in the d:\, which include the R folder and "DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. Actually, I don't know what I should write in it.
> >
> > Read the R Extensions manual for a detailed description.  You can use
> > the package.skeleton() function to create a template, but you need to
> > edit it to make it acceptable.
> > >
> > > Well, there are still aother error:
> > >
> > >       ---------- Making package example ------------
> > >         adding build stamp to DESCRIPTION
> > >       error happened.read_description(dfile) : file 'D:/example/DESCRIPTION' is not in valid DCF format
> >
> > That's complaining about your bad DESCRIPTION file.
> >
> > Duncan Murdoch
> > >       Stop execute
> > >       make[2]: *** [frontmatter] Error 1
> > >       make[1]: *** [all] Error 2
> > >       make: *** [pkg-example] Error 2
> > >       *** Installation of example failed ***
> > >       Removing 'D:/PROGRA~1/R/rw2011/library/example'
> > >
> > > Please tell me which step is wrong?
> > > Thanks a lot!
> > >
> > > BG
> > > Ivy_Li
> > >
> > >
> > > ----------
> > > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > > : 200577 20:57
> > > : Uwe Ligges
> > > : Ivy_Li; r-help at stat.math.ethz.ch
> > > : Re: : : [R] fail in adding library in new version.
> > >
> > >
> > > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > >
> > >>Gabor Grothendieck wrote:
> > >>
> > >>
> > >>>On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > >>>
> > >>>
> > >>>>Ivy_Li wrote:
> > >>>>
> > >>>>
> > >>>>
> > >>>>>Dear all,
> > >>>>>     I have done every step as the previous mail.
> > >>>>>1. unpack tools.zip into c:\cygwin
> > >>>>>2. install Active perl in c:\Perl
> > >>>>>3. install the mingw32 in c:\mingwin
> > >>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" , and they are in the beginning of the "Path"
> > >>>>
> > >>>>                  ^
> > >>>>such blanks are not allowed in the PATH variable
> > >>>>
> > >>>>
> > >>>>
> > >>>>
> > >>>>
> > >>>>>     Because I install R in the D drive, so I set a fold "MyRpackages" in the same drive. Into the "MyRpackages" folder I write a R library named "example", include "DESCRIPTION" file and "R" folder. In the "R" folder, the "example" file just content very simple code as the previous mail said.
> > >>>>>      So in the Dos environment, at first, into the D:\>, I type the following code:
> > >>>>>cd Program Files\R\rw2011\
> > >>>>
> > >>>>MyRpackages does not need to be here.
> > >>>>
> > >>>>
> > >>>>
> > >>>>>bin\R CMD install /MyRpackages/example
> > >>>>
> > >>>>The first slash in "/MyRPackages" sugests that this is a top level
> > >>>>directory, which does not exist.
> > >>>>Even better, cd to MyRpackages, add R's bin dir to your path variable,
> > >>>>and simply say:
> > >>>>
> > >>>>R CMD INSTALL example
> > >>>
> > >>>
> > >>>Another possibility is to put Rcmd.bat from the batch file collection
> > >>>
> > >>>   http://cran.r-project.org/contrib/extra/batchfiles/
> > >>>
> > >>>in your path.  It will use the registry to find R so you won't have
> > >>>to modify your path (nor would you have to remodify it every time you
> > >>>install a new version of R which is what you would otherwise have to do):
> > >>>
> > >>>cd \MyPackages
> > >>>Rcmd install example
> > >>
> > >>
> > >>Just for the records:
> > >>
> > >>1. "cd \MyPackages" won't work, as I have already explained above.
> > >
> > >
> > > If MyPackages is not a top level directory in the current drive
> > > then it will not work. Otherwise it does work.
> > >
> > >
> > >>2. I do *not* recommend this way, in particular I find it misleading to
> > >>provide these batch files on CRAN.
> > >>
> > >
> > >
> > > The alternative, at least as discussed in your post, is more work
> > > since one will then have to change one's path every time one
> > > reinstalls R.  This is just needless extra work and is error prone.  If you
> > > forget to do it then you will be accessing the bin directory of the
> > > wrong version of R.
> > >
> > >
> > >>>>>     There are some error:
> > >>>>>'make' is neither internal or external command, nor executable operation or batch file
> > >>>>>*** installation of example failed ***
> > >>>>
> > >>>>Well, make.exe is not find in your path. Please check whether the file
> > >>>>exists and the path has been added.
> > >>>>
> > >>>>Uwe Ligges
> > >>>>
> > >>>>
> > >>>>
> > >>>>
> > >>>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
> > >>>>>
> > >>>>>I think I have closed to success. heehee~~~~~
> > >>>>>Thank you for your help.
> > >>>>>I still need you and others help. Thank you very much!
> > >>>>>
> > >>>>>
> > >>>>>----------
> > >>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > >>>>>: 2005630 19:16
> > >>>>>: Ivy_Li
> > >>>>>: r-help at stat.math.ethz.ch
> > >>>>>: Re: : [R] fail in adding library in new version.
> > >>>>>
> > >>>>>
> > >>>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>>Dear Gabor,
> > >>>>>>     Thank your for helping me so much!
> > >>>>>>     I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> > >>>>>>1. unpack tools.zip into c:\cygwin
> > >>>>>>2. install Active perl in c:\Perl
> > >>>>>>3. install the mingw32 in c:\mingwin
> > >>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)
> > >>>>>
> > >>>>>
> > >>>>>If in the console you enter the command:
> > >>>>>
> > >>>>>path
> > >>>>>
> > >>>>>then it will display a semicolon separated list of folders.  You want the folder
> > >>>>>that contains the tools to be at the beginning so that you eliminate
> > >>>>>the possibility
> > >>>>>of finding a different program of the same name first in a folder that comes
> > >>>>>prior to the one where the tools are stored.
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>>5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> > >>>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> > >>>>>>
> > >>>>>>6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> > >>>>>>cd \Program Files\R\rw2010
> > >>>>>>But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"
> > >>>>>
> > >>>>>
> > >>>>>I was assuming that MyRPackages and R are on the same disk.  If they are not
> > >>>>>then you need to specify the disk too.  That is if MyRPackages is on C and R
> > >>>>>is installed on D then install your package via:
> > >>>>>
> > >>>>>d:
> > >>>>>cd \Program Files\R\rw2010
> > >>>>>bin\R CMD install c:/MyRPackages/example
> > >>>>>
> > >>>>>Note that bin\R means to run R.exe in the bin subfolder of the current folder
> > >>>>>using command script install and the indicated source package.
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>>I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?
> > >>>>>
> > >>>>>
> > >>>>>If you are not sure where R is installed then enter the following at the Windows
> > >>>>>console prompt to find out (this will work provided you let it install the key
> > >>>>>into the registry when you installed R initially).  The reg command is a command
> > >>>>>built into Windows (I used XP but I assume its the same on other versions)
> > >>>>>that will query the Windows registry:
> > >>>>>
> > >>>>>reg query hklm\software\r-core\r /v InstallPath
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>>I still need your and others help. Thank you very much!
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>----------
> > >>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > >>>>>>: 200566 10:21
> > >>>>>>: Ivy_Li
> > >>>>>>: r-help at stat.math.ethz.ch
> > >>>>>>: Re: [R] fail in adding library in new version.
> > >>>>>>
> > >>>>>>
> > >>>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>>Hello everybody,
> > >>>>>>>     Could I consult you a question?
> > >>>>>>>     I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> > >>>>>>
> > >>>>>>Getting the latest version of R is strongly recommended.  The suggestions
> > >>>>>>below all assume the latest version and may or may not work if you do
> > >>>>>>not upgrade.
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>>*       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> > >>>>>>>*       Download the tools.zip
> > >>>>>>>*       Unpack tools.zip into c:\cygwin
> > >>>>>>>*       Install Active Perl in c:\Perl
> > >>>>>>>*       Install the mingw32 port of gcc in c:\mingwin
> > >>>>>>>*       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> > >>>>>>
> > >>>>>>You may need to put these at the beginning of the path rather than the end.
> > >>>>>>Also just as a check enter
> > >>>>>>  path
> > >>>>>>at the console to make sure that you have them.  You will likely
> > >>>>>>have to start a new console session and possibly even reboot.
> > >>>>>>
> > >>>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
> > >>>>>>you reread the material on which tools you need.
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>>*       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> > >>>>>>
> > >>>>>>In MyRPackages you would have a folder called example, in your case,
> > >>>>>>that contains the package.  Within folder example, you would have the
> > >>>>>>DESCRIPTION file, the R folder, etc.
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>>*       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> > >>>>>>
> > >>>>>>You don't have to run R first.  You do need to make sure that R.exe can
> > >>>>>>be found on your path or else use the absolute path name in referring to R.
> > >>>>>>For example, if your path does not include R you could do something like this:
> > >>>>>>
> > >>>>>>cd \Program Files\R\rw2010
> > >>>>>>bin\R cmd install /MyRPackages/example
> > >>>>>
> > >>>>>
> > >>>>>Sorry, there is an error in the above.  It should be:
> > >>>>>
> > >>>>>bin\R CMD install c:/MyRPackages/example
> > >>>>>
> > >>>>>or
> > >>>>>
> > >>>>>bin\Rcmd install c:/MyRPackages/example
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>>Be sure to use forward slashes where shown above and backslashes
> > >>>>>>where shown.
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>>     So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> > >>>>>>>
> > >>>>>>
> > >>>>>>Try all these suggestions including upgrading R and if that does not work
> > >>>>>>try posting screen dumps of the actual errors you are getting.
> > >>>>>>
> > >>>>>
> > >>>>>
> > >>>>>Also try googling for
> > >>>>>
> > >>>>> making creating R packages
> > >>>>>
> > >>>>>and you will find some privately written tutorials on all this.
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>>------------------------------------------------------------------------
> > >>>>>
> > >>>>>______________________________________________
> > >>>>>R-help at stat.math.ethz.ch mailing list
> > >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >>>>
> > >>>>
> > >>
> > >
> > >
> > > ------------------------------------------------------------------------
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From weigand.stephen at charter.net  Wed Jul 13 06:11:59 2005
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Tue, 12 Jul 2005 23:11:59 -0500
Subject: [R] exact values for p-values
In-Reply-To: <17247091723f9a.1723f9a1724709@amc.uva.nl>
References: <17247091723f9a.1723f9a1724709@amc.uva.nl>
Message-ID: <04f28349edd3b5c1c327295f98ec94c9@charter.net>


On Jul 11, 2005, at 11:47 AM, S.O. Nyangoma wrote:

> Hi there,
> If I do an lm, I get p-vlues as
>
> p-value: < 2.2e-16
>
> Suppose am interested in exact value such as
>
> p-value = 1.6e-16 (note = and not <)
>
> How do I go about it?
>
> stephen
>

I think you're seeing a very small p-value after it has been formatted 
by format.pval.
Consider:

options(digits = 6)
format.pval(0)
# [1] "< 2.2e-16"

format.pval(0, eps = 1e-20)
# [1] "< 1e-20"

you could access the p-values you're interested following this example:

x <- 1:100
y <- x + rnorm(100, 0, 1)
f <- lm(y ~ x)
ff <- summary(f)
print(ff)
ff$coefficients
Pvals <- ff$coefficients[, 4]
print(Pvals, digits = 20)

Hope this helps,

Stephen



From ripley at stats.ox.ac.uk  Wed Jul 13 08:46:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2005 07:46:59 +0100 (BST)
Subject: [R] write.foreign, SPSS on Mac OS X
In-Reply-To: <42D45733.4080905@alumni.uwaterloo.ca>
References: <42D45733.4080905@alumni.uwaterloo.ca>
Message-ID: <Pine.LNX.4.61.0507130725350.29010@gannet.stats>

On Tue, 12 Jul 2005, EJ Nikelski wrote:

>     I have jut installed the foreign package (v 0.8-8) on my OS X
> machine, and have a bit of a problem writing out a data frame in SPSS
> format. Specifically, the code file (the .sps format file) seems to
> write 3 unprintable hex values instead of double quotes. For example, in
> the following output ...
>
> VALUE LABELS
> /
> immDel
> 1 ###1###
>  2 ###2###
>  3 ###3###
>
>  ... emacs tells me that the left-sided ### are the hex codes E2 80 9C,
> on the right we have E2 80 9D. I am supposing that I should be seeing
> double-quotes here? Interestingly, the data file, which also contains a
> quoted field, writes out the quotes without any problem. Does anyone
> have any ideas?

An idea. Those are left and right double quotes in UTF-8 and since MacOS X
is usually in a UTF-8 locale they should be printable.  However, I suspect 
that SPSS is expecting ASCII double quotation marks.

You haven't told us what you did, but I guess you used
write.foreign(package="SPSS").  That calls writeForeignSPSS which contains 
calls to dQuote(), and the latter are wrong if ASCII quotation marks are 
needed.

A quick workaround is to use a non-UTF-8 locale: how you do that on ypur 
OS depends on how you run R so please ask advice on the R-sig-mac list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hb at maths.lth.se  Wed Jul 13 08:48:46 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 13 Jul 2005 08:48:46 +0200
Subject: [R] fail in adding library in new version.
Message-ID: <42D4B94E.8040005@maths.lth.se>

I've prepared a Windows batch file that gives you a fresh Windows 
commando prompt setup for R. You can download it from:

   http://www.maths.lth.se/help/R/RCMDprompt.bat

It has instructions how and where to download necessary software such as 
'hhc'. The script setup the PATH assuming default installation of 
everything so with some luck you can build R packages right away. If 
not, follow the instructions and modify some of the variables.

/Henrik


On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
 > If you start up a fresh batch console and run
 >   hhc
 > do you get the usage message?  If not you still have a path problem.
 > You only need hhc.exe so just copy that file into any directory on your
 > path -- issue the command:
 >
 > path
 >
 > to find out which directories you can copy it into (and also post the
 > output of the path command if you still have problem) -- with this
 > approach you won't have to set your path to the
 > HTML Help Workshop.
 >
 > On 7/12/05, Ivy_Li <Ivy_Li at smics.com> wrote:
 > > Dear All,
 > >        I have downloaded the htmlhelp.exe from InterNet. Install it 
into the path "C:\Program Files\HTML Help Workshop\". I add this path 
into the "Control Panel -> System -> Advanced -> Environment Variables 
-> Path -> Variable".
 > >        But it still exist error.
 > >        ---------- Making package example ------------
 > >          adding build stamp to DESCRIPTION
 > >          installing R files
 > >          installing data files
 > >          installing man source files
 > >          installing indices
 > >          not zipping data
 > >          installing help
 > >         >>> Building/Updating help pages for package 'example'
 > >             Formats: text html latex example chm
 > >          d                                 text    html    latex 
example
 > >          e                                 text    html    latex 
example
 > >          f                                 text    html    latex 
example
 > >          g                                 text    html    latex 
example
 > >        hhc: not found
 > >        cp: cannot stat 
`D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
 > >        directory
 > >        make[1]: *** [chm-example] Error 1
 > >        make: *** [pkg-example] Error 2
 > >        *** Installation of example failed ***
 > >
 > >        Removing 'D:/PROGRA~1/R/rw2011/library/example'
 > >        Restoring previous 'D:/PROGRA~1/R/rw2011/library/example'
 > >
 > > I don't know why it can not find the "hhc" file.
 > > Please tell me which step is wrong.
 > > Thank you for helping me!
 > > ^_^
 > >
 > > ----------
 > > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
 > > : 2005713 9:32
 > > : Ivy_Li
 > > : Duncan Murdoch; r-help at stat.math.ethz.ch
 > > : Re: : [R] fail in adding library in new version.
 > >
 > >
 > > hhc.exe is the Microsoft help compiler.  You have to download it 
and put
 > > it somewhere in your path.
 > >
 > > On 7/12/05, Ivy_Li <Ivy_Li at smics.com> wrote:
 > > > Dear all,
 > > > I really appreciate your help. I think I have a little 
advancement. ^_^
 > > > Now I use the package.skeleton() function to create a template. I 
type:
 > > >        f <- function(x,y) x+y
 > > >        g <- function(x,y) x-y
 > > >        d <- data.frame(a=1, b=2)
 > > >        e <- rnorm(1000)
 > > >        package.skeleton(list=c("f","g","d","e"), name="example")
 > > >
 > > > in R. I know it will create a folder named "example" in the path 
of "\R\rw2011\" I opened this folder, its format is similar as other 
library. Then I modify it "DESCRIPTION" file:
 > > >        Package: example
 > > >        Version: 1.0-1
 > > >        Date: 2005-07-09
 > > >        Title: My first function
 > > >        Author: Ivy <Ivy_Li at smics.com>
 > > >        Maintainer: Ivy <Ivy_Li at smics.com>
 > > >        Description: simple sum and subtract
 > > >        License: GPL version 2 or later
 > > >        Depends: R (>= 1.9), stats, graphics, utils
 > > >
 > > > I don't whether I should modify other "README" file.
 > > > When I enter the Dos environment, at first, into the D:\>, I type 
the following code:
 > > >        cd Program Files\R\rw2011\
 > > >        bin\R CMD install /example
 > > >
 > > > Well, there appeared error:
 > > >        ---------- Making package example ------------
 > > >          adding build stamp to DESCRIPTION
 > > >          installing R files
 > > >          installing data files
 > > >          installing man source files
 > > >          installing indices
 > > >          not zipping data
 > > >          installing help
 > > >         >>> Building/Updating help pages for package 'example'
 > > >             Formats: text html latex example chm
 > > >          d                                 text    html    latex 
   example chm
 > > >          e                                 text    html    latex 
   example chm
 > > >          f                                 text    html    latex 
   example chm
 > > >             missing link(s):  ~~fun~~
 > > >          g                                 text    html    latex 
   example chm
 > > >             missing link(s):  ~~fun~~
 > > >        hhc: not found
 > > >        cp: cannot stat 
`D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
 > > >        directory
 > > >        make[1]: *** [chm-example] Error 1
 > > >        make: *** [pkg-example] Error 2
 > > >        *** Installation of example failed ***
 > > >
 > > >        Removing 'D:/PROGRA~1/R/rw2011/library/example'
 > > >
 > > > That's it. I have to consult every R expert. Please help to solve 
this issue. Thank you very much!
 > > >
 > > >
 > > >
 > > >
 > > > ----------
 > > > : Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
 > > > : 200578 19:34
 > > > : Ivy_Li
 > > > : r-help at stat.math.ethz.ch
 > > > : Re: [R] fail in adding library in new version.
 > > >
 > > >
 > > > Ivy_Li wrote:
 > > > > Dear all,
 > > > >       I really appreciate your help. I think I have a little 
advancement. ^_^
 > > > >
 > > > >       When I enter the Dos environment, at first, into the 
D:\>, I type the following code:
 > > > > cd Program Files\R\rw2011\
 > > > > bin\R CMD install /example
 > > > >
 > > > > "example" is in the d:\, which include the R folder and 
"DESCRIPTION" file, But I wrote nothing in the "DESCRIPTION" file. 
Actually, I don't know what I should write in it.
 > > >
 > > > Read the R Extensions manual for a detailed description.  You can use
 > > > the package.skeleton() function to create a template, but you need to
 > > > edit it to make it acceptable.
 > > > >
 > > > > Well, there are still aother error:
 > > > >
 > > > >       ---------- Making package example ------------
 > > > >         adding build stamp to DESCRIPTION
 > > > >       error happened.read_description(dfile) : file 
'D:/example/DESCRIPTION' is not in valid DCF format
 > > >
 > > > That's complaining about your bad DESCRIPTION file.
 > > >
 > > > Duncan Murdoch
 > > > >       Stop execute
 > > > >       make[2]: *** [frontmatter] Error 1
 > > > >       make[1]: *** [all] Error 2
 > > > >       make: *** [pkg-example] Error 2
 > > > >       *** Installation of example failed ***
 > > > >       Removing 'D:/PROGRA~1/R/rw2011/library/example'
 > > > >
 > > > > Please tell me which step is wrong?
 > > > > Thanks a lot!
 > > > >
 > > > > BG
 > > > > Ivy_Li
 > > > >
 > > > >
 > > > > ----------
 > > > > : Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
 > > > > : 200577 20:57
 > > > > : Uwe Ligges
 > > > > : Ivy_Li; r-help at stat.math.ethz.ch
 > > > > : Re: : : [R] fail in adding library in new version.
 > > > >
 > > > >
 > > > > On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
 > > > >
 > > > >>Gabor Grothendieck wrote:
 > > > >>
 > > > >>
 > > > >>>On 7/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
 > > > >>>
 > > > >>>
 > > > >>>>Ivy_Li wrote:
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>>Dear all,
 > > > >>>>>     I have done every step as the previous mail.
 > > > >>>>>1. unpack tools.zip into c:\cygwin
 > > > >>>>>2. install Active perl in c:\Perl
 > > > >>>>>3. install the mingw32 in c:\mingwin
 > > > >>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> 
System -> Advanced -> Environment Variables -> Path -> Variable" , and 
they are in the beginning of the "Path"
 > > > >>>>
 > > > >>>>                  ^
 > > > >>>>such blanks are not allowed in the PATH variable
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>>     Because I install R in the D drive, so I set a fold 
"MyRpackages" in the same drive. Into the "MyRpackages" folder I write a 
R library named "example", include "DESCRIPTION" file and "R" folder. In 
the "R" folder, the "example" file just content very simple code as the 
previous mail said.
 > > > >>>>>      So in the Dos environment, at first, into the D:\>, I 
type the following code:
 > > > >>>>>cd Program Files\R\rw2011\
 > > > >>>>
 > > > >>>>MyRpackages does not need to be here.
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>>bin\R CMD install /MyRpackages/example
 > > > >>>>
 > > > >>>>The first slash in "/MyRPackages" sugests that this is a top 
level
 > > > >>>>directory, which does not exist.
 > > > >>>>Even better, cd to MyRpackages, add R's bin dir to your path 
variable,
 > > > >>>>and simply say:
 > > > >>>>
 > > > >>>>R CMD INSTALL example
 > > > >>>
 > > > >>>
 > > > >>>Another possibility is to put Rcmd.bat from the batch file 
collection
 > > > >>>
 > > > >>>   http://cran.r-project.org/contrib/extra/batchfiles/
 > > > >>>
 > > > >>>in your path.  It will use the registry to find R so you won't 
have
 > > > >>>to modify your path (nor would you have to remodify it every 
time you
 > > > >>>install a new version of R which is what you would otherwise 
have to do):
 > > > >>>
 > > > >>>cd \MyPackages
 > > > >>>Rcmd install example
 > > > >>
 > > > >>
 > > > >>Just for the records:
 > > > >>
 > > > >>1. "cd \MyPackages" won't work, as I have already explained above.
 > > > >
 > > > >
 > > > > If MyPackages is not a top level directory in the current drive
 > > > > then it will not work. Otherwise it does work.
 > > > >
 > > > >
 > > > >>2. I do *not* recommend this way, in particular I find it 
misleading to
 > > > >>provide these batch files on CRAN.
 > > > >>
 > > > >
 > > > >
 > > > > The alternative, at least as discussed in your post, is more work
 > > > > since one will then have to change one's path every time one
 > > > > reinstalls R.  This is just needless extra work and is error 
prone.  If you
 > > > > forget to do it then you will be accessing the bin directory of the
 > > > > wrong version of R.
 > > > >
 > > > >
 > > > >>>>>     There are some error:
 > > > >>>>>'make' is neither internal or external command, nor 
executable operation or batch file
 > > > >>>>>*** installation of example failed ***
 > > > >>>>
 > > > >>>>Well, make.exe is not find in your path. Please check whether 
the file
 > > > >>>>exists and the path has been added.
 > > > >>>>
 > > > >>>>Uwe Ligges
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>
 > > > >>>>>Removing 'D:/PROGRA~1/R/rw2011/library/example'
 > > > >>>>>
 > > > >>>>>I think I have closed to success. heehee~~~~~
 > > > >>>>>Thank you for your help.
 > > > >>>>>I still need you and others help. Thank you very much!
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>----------
 > > > >>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
 > > > >>>>>: 2005630 19:16
 > > > >>>>>: Ivy_Li
 > > > >>>>>: r-help at stat.math.ethz.ch
 > > > >>>>>: Re: : [R] fail in adding library in new version.
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>>Dear Gabor,
 > > > >>>>>>     Thank your for helping me so much!
 > > > >>>>>>     I have loaded R the newest version 2.1.1. Then I setup 
it in the path of D:\program files\R\
 > > > >>>>>>1. unpack tools.zip into c:\cygwin
 > > > >>>>>>2. install Active perl in c:\Perl
 > > > >>>>>>3. install the mingw32 in c:\mingwin
 > > > >>>>>>4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> 
System -> Advanced -> Environment Variables -> Path -> Variable" (In 
your previous mail, you said "put these at the beginning of the path", I 
don't understand what is your meaning. Which path?)
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>If in the console you enter the command:
 > > > >>>>>
 > > > >>>>>path
 > > > >>>>>
 > > > >>>>>then it will display a semicolon separated list of folders. 
  You want the folder
 > > > >>>>>that contains the tools to be at the beginning so that you 
eliminate
 > > > >>>>>the possibility
 > > > >>>>>of finding a different program of the same name first in a 
folder that comes
 > > > >>>>>prior to the one where the tools are stored.
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>>5. I tried an library example. I set a new folder named 
"example" in the "c:\MyRpackages\". And In the "example" folder, it 
contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a 
"example" file. I just write very simple script in it:
 > > > >>>>>>a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
 > > > >>>>>>
 > > > >>>>>>6. I opened the DOS environment. Into the "D:\>"  Type the 
following code:
 > > > >>>>>>cd \Program Files\R\rw2010
 > > > >>>>>>But I don't understand the second line you writed in your 
previous mail: "bin\R cmd install /MyRPackages/example"
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>I was assuming that MyRPackages and R are on the same disk. 
  If they are not
 > > > >>>>>then you need to specify the disk too.  That is if 
MyRPackages is on C and R
 > > > >>>>>is installed on D then install your package via:
 > > > >>>>>
 > > > >>>>>d:
 > > > >>>>>cd \Program Files\R\rw2010
 > > > >>>>>bin\R CMD install c:/MyRPackages/example
 > > > >>>>>
 > > > >>>>>Note that bin\R means to run R.exe in the bin subfolder of 
the current folder
 > > > >>>>>using command script install and the indicated source package.
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>>I am not sure that I set up R in "D:\" But I do so much 
action in C:\  Did I do the correct action? Did I do the action into the 
correct path?
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>If you are not sure where R is installed then enter the 
following at the Windows
 > > > >>>>>console prompt to find out (this will work provided you let 
it install the key
 > > > >>>>>into the registry when you installed R initially).  The reg 
command is a command
 > > > >>>>>built into Windows (I used XP but I assume its the same on 
other versions)
 > > > >>>>>that will query the Windows registry:
 > > > >>>>>
 > > > >>>>>reg query hklm\software\r-core\r /v InstallPath
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>>I still need your and others help. Thank you very much!
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>----------
 > > > >>>>>>: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
 > > > >>>>>>: 200566 10:21
 > > > >>>>>>: Ivy_Li
 > > > >>>>>>: r-help at stat.math.ethz.ch
 > > > >>>>>>: Re: [R] fail in adding library in new version.
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>>Hello everybody,
 > > > >>>>>>>     Could I consult you a question?
 > > > >>>>>>>     I always use R old version 1.9.1 . Because I can not 
add my library into the new version 2.0.0 by the same method as old version.
 > > > >>>>>>
 > > > >>>>>>Getting the latest version of R is strongly recommended. 
The suggestions
 > > > >>>>>>below all assume the latest version and may or may not work 
if you do
 > > > >>>>>>not upgrade.
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>>*       I have read the webpage 
<http://www.stats.ox.ac.uk/pub/Rtools>
 > > > >>>>>>>*       Download the tools.zip
 > > > >>>>>>>*       Unpack tools.zip into c:\cygwin
 > > > >>>>>>>*       Install Active Perl in c:\Perl
 > > > >>>>>>>*       Install the mingw32 port of gcc in c:\mingwin
 > > > >>>>>>>*       Then go to "Control Panel -> System -> Advanced -> 
Environment Variables -> Path -> Variable Balue" add 
;c:\cygwin;c:\mingwin\bin
 > > > >>>>>>
 > > > >>>>>>You may need to put these at the beginning of the path 
rather than the end.
 > > > >>>>>>Also just as a check enter
 > > > >>>>>>  path
 > > > >>>>>>at the console to make sure that you have them.  You will 
likely
 > > > >>>>>>have to start a new console session and possibly even reboot.
 > > > >>>>>>
 > > > >>>>>>Also you need the Microsoft Help Compiler, hhc.  Suggest
 > > > >>>>>>you reread the material on which tools you need.
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>>*       Save my library "example" into "c:\MyRpackages\" . 
But I am not sure what type it is, is it need suffix?" And I don't what 
its content, just my function script, no special format?
 > > > >>>>>>
 > > > >>>>>>In MyRPackages you would have a folder called example, in 
your case,
 > > > >>>>>>that contains the package.  Within folder example, you 
would have the
 > > > >>>>>>DESCRIPTION file, the R folder, etc.
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>>*       Then I don't know where should I do this step: 
Type R CMD INSTALL --build example. Need I run R first?
 > > > >>>>>>
 > > > >>>>>>You don't have to run R first.  You do need to make sure 
that R.exe can
 > > > >>>>>>be found on your path or else use the absolute path name in 
referring to R.
 > > > >>>>>>For example, if your path does not include R you could do 
something like this:
 > > > >>>>>>
 > > > >>>>>>cd \Program Files\R\rw2010
 > > > >>>>>>bin\R cmd install /MyRPackages/example
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>Sorry, there is an error in the above.  It should be:
 > > > >>>>>
 > > > >>>>>bin\R CMD install c:/MyRPackages/example
 > > > >>>>>
 > > > >>>>>or
 > > > >>>>>
 > > > >>>>>bin\Rcmd install c:/MyRPackages/example
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>>Be sure to use forward slashes where shown above and 
backslashes
 > > > >>>>>>where shown.
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>
 > > > >>>>>>>     So There is a error after I do this step.  It said it 
can not find somethig. I don't which step is wrong. It costed me much time.
 > > > >>>>>>>
 > > > >>>>>>
 > > > >>>>>>Try all these suggestions including upgrading R and if that 
does not work
 > > > >>>>>>try posting screen dumps of the actual errors you are getting.
 > > > >>>>>>
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>Also try googling for
 > > > >>>>>
 > > > >>>>> making creating R packages
 > > > >>>>>
 > > > >>>>>and you will find some privately written tutorials on all this.
 > > > >>>>>
 > > > >>>>>
 > > > >>>>>
 > > > 
 >>>>>------------------------------------------------------------------------
 > > > >>>>>
 > > > >>>>>______________________________________________
 > > > >>>>>R-help at stat.math.ethz.ch mailing list
 > > > >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
 > > > >>>>>PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 > > > >>>>
 > > > >>>>
 > > > >>
 > > > >
 > > > >
 > > > > 
------------------------------------------------------------------------
 > > > >
 > > > > ______________________________________________
 > > > > R-help at stat.math.ethz.ch mailing list
 > > > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > > > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 > > >
 > > > ______________________________________________
 > > > R-help at stat.math.ethz.ch mailing list
 > > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 > >
 >
 >
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >
 >



From ripley at stats.ox.ac.uk  Wed Jul 13 09:03:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2005 08:03:44 +0100 (BST)
Subject: [R] How to increase memory for R on Soliars 10 with 16GB and
 64bit R
In-Reply-To: <s2d3fe1b.033@ohsu.edu>
References: <s2d3fe1b.033@ohsu.edu>
Message-ID: <Pine.LNX.4.61.0507130750250.29010@gannet.stats>

On Tue, 12 Jul 2005, Dongseok Choi wrote:

>  My machine is SUN Java Workstation 2100 with 2 AMD Opteron CPUs and 16GB RAM.
>  R is compiled as 64bit by using SUN compilers.
>  I trying to fit quantile smoothing on my data and I got an message as below.
>
>> fit1<-rqss(z1~qss(cbind(x,y),lambda=la1),tau=t1)
> Error in as.matrix.csr(diag(n)) :
cannot allocate memory block of size 2496135168
>
>  The lengths of vector x and y are both 17664.
>  I tried and found that the same command ran with x[1:16008] and y[1:16008].
>  So, it looks to me a memory related problem, but I'm not sure how I can allocate memory block.
>   I read the command line option but not sure what do to with it.
>   Could you help me on this?

It is trying to allocate a single memory block of size over 2^31-1 bytes. 
R internally uses ints for sizes of vectors and that is a limit (see 
help("Memory-limits") ).  However, it is intended that on 64-bit systems 
that there is a limit here of 8*(2^31-1) but there was a typo.  Please 
change line 1534 of src/main/memory.c to

#if SIZEOF_LONG > 4

and re-compile.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From David.Duffy at qimr.edu.au  Wed Jul 13 09:46:38 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 13 Jul 2005 17:46:38 +1000 (EST)
Subject: [R]  exact values for p-values
In-Reply-To: <mailman.9.1119261600.23655.r-help@stat.math.ethz.ch>
References: <mailman.9.1119261600.23655.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0507131719450.21686@orpheus.qimr.edu.au>

> This is obtained from F =39540 with df1 = 1, df2 = 7025.
> Suppose am interested in exact value such as
>

If it were really necessary, you would have to move to multiple
precision.  The gmp R package doesn't seem to yet cover this, but FMLIB
(TOMS814, DM Smith) is a multiple precision f90 library that does
include the incomplete beta -- it allows one to say for F(1,7025)=39540,
P=6.31E-2886 (evaluated using 200 sign. digit arithmetic).  Results from
R's pf() agree quite closely with the FMLIB results for less extreme values
eg
> print(pf(1500,1,7025,lower=FALSE), digits=20)
 [1] 1.3702710894887480597e-297

cf   1.37027108948832580215549799419452388134616261215463681945E-297


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From arnaud.dowkiw at orleans.inra.fr  Wed Jul 13 10:18:58 2005
From: arnaud.dowkiw at orleans.inra.fr (Arnaud Dowkiw)
Date: Wed, 13 Jul 2005 10:18:58 +0200
Subject: [R] Boxcox transformation / homogeneity of variances
Message-ID: <5.2.0.9.0.20050713100113.020cfec8@orleans.inra.fr>

Dear r-helpers,

Prior to analysis of variance, I ran the Boxcox function (MASS library) to 
find the best power transformation of my data. However, reading the Boxcox 
help file, I cannot figure out if this function (through its associated 
log-likelihood function) corrects for * normality only * or if it also 
induces * homogeneity of variances *. I found in Biometry (Sokal and Rohlf, 
p. 419) that the box-cox transformation can be extended to induce 
homogenity of variances in conjunction with Bartlett's test of homogeneity 
of variances. Does the Boxcox function implemented in R refer to this 
extension ?
Thanks a lot,


- - - - - - - - - - - - - - - - - - - - - - -
Arnaud DOWKIW
INRA
Forest Research
Avenue de la Pomme de Pin
BP 20619 ARDON
45166 OLIVET CEDEX
FRANCE
Tel. + 33 2 38 41 78 00
Fax. + 33 2 38 41 48 09
- - - - - - - - - - - - - - - - - - - - - - -



From Ted.Harding at nessie.mcc.ac.uk  Wed Jul 13 10:12:42 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Jul 2005 09:12:42 +0100 (BST)
Subject: [R] How to use the function "plot"  as Matlab?
In-Reply-To: <42D47A4B.7000608@yahoo.com.br>
Message-ID: <XFMail.050713091242.Ted.Harding@nessie.mcc.ac.uk>

On 13-Jul-05 klebyn wrote:
> Hello,
> 
> How to use the function plot to produce graphs as Matlab?
> example in Matlab:
> 
> a = [1,2,5,3,6,8,1,7];
> b = [1,7,2,9,2,3,4,5];
> plot(a,'b')
> hold
> plot(b,'r')
> 
> 
> How to make the same in R-package ?
> 
> I am trying something thus:
> 
> a <- c(1,2,5,3,6,8,1,7)
> c(1,7,2,9,2,3,4,5) -> b
> 
> a;b
> 
> plot(a,t="l",col="blue")
> plot(b,t="l",col="red")

Although this is an over-worked query -- for which an answer, given
that t="l" has been specified, is to use

  plot(a,t="l",col="blue",ylim=c(0,10))
  lines(b,t="l",col="red")

there is a more interesting issue associated with it (given that
Klebyn has come to it from a Matlab perspective).

It's a long time since I used real Matlab, but I'll illustrate
with octave which, in this respect, should be identical to Matlab.

Octave:

octave:1> x = 0.1*(0:20);
octave:2> plot(x,sin(x))

produces a graph of sin(x) with the y-axis scaled from 0 to 1.0
Next:

octave:3> hold on
octave:4> plot(x,1.5*cos(x))

superimposes a graph of 1.5*cos(x) with the y-axis automatically
re-scaled from -1 to 1.5.

This would not have happened in R with

  x = 0.1*(0:20);
  plot(x,sin(x))
  lines(x,1.5*cos(x))

where the 0 to 1.0 scaling of the first plot would be kept for
the second, in which therefore part of the additional graph of
1.5*cos(x) would be "outside the box".

No doubt like many others, I've been caught on the wrong foot
by this more than a few times. The solution, of course (as
illustrated in the reply to Klebyn above) is to anticipate
what scaling you will need for all the graphs you intend to
put on the same plot, and set up the scalings at the time
of the first one using the options "xlim" and "ylim", e.g.:

  x = 0.1*(0:20);
  plot(x,sin(x),ylim=c(-1,1.5))
  lines(x,1.5*cos(x))

This is not always feasible, and indeed should not be expected
to be feasible since part of the reason for using software
like R in the first place is to compute what you do not know!

Indeed, R will not allow you to use "xlim" or "ylim" once the
first plot has been drawn.

So in such cases I end up making a note (either on paper or,
when I do really serious planning, in auxiliary variables)
of the min's and max's for each graph, and then re-run the
plotting commands with appropriate "xlim" and "ylim" scaling
set up in the first plot so as to include all the subsequent
graphs in entirety. (Even this strategy can be defeated if
the succesive graphs represent simulations of long-tailed
distributions. Unless of course I'm sufficiently alert to
set the RNG seed first as well ... )

I'm not sufficiently acquainted with the internals of "plot"
and friends to anticipate the answer to this question; but,
anyway, the question is:

  Is it feasible to include, as a parameter to "plot", "lines"
  and "points",

    rescale=FALSE

  where this default value would maintain the existing behaviour
  of these functions, while setting

    rescale=TRUE

  would allow each succeeding plot, adding graphs using "points"
  or "lines", to be rescaled (as in Matlab/Octave) so as to
  include the entirety of each successive graph?

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jul-05                                       Time: 09:12:34
------------------------------ XFMail ------------------------------



From r.hankin at noc.soton.ac.uk  Wed Jul 13 10:33:42 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Jul 2005 09:33:42 +0100
Subject: [R] How to use the function "plot"  as Matlab?
In-Reply-To: <XFMail.050713091242.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050713091242.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <A14D4C78-F35D-4842-901B-500ADA62F792@soc.soton.ac.uk>

Hi

Ted makes a good point... matlab can dynamically rescale a plot in  
response
to plot(...,add=TRUE) statements.

For some reason which I do not understand, the rescaling issue is  
only a problem
for me when working in "matlab mode".  It's not an issue when working  
in "R mode"

Ted pointed out that the following does not behave as intended:


>   x = 0.1*(0:20);
>   plot(x,sin(x))
>   lines(x,1.5*cos(x))


and presented an alternative method in which ylim was set by hand.  I  
would suggest:

x <- 0.1*(0:20)
y1 <- sin(x)
y2 <- 1.5*cos(x)

plot(c(x,x),c(y1,y2),type="n")
lines(x,y1)
lines(x,y2)

because this way, the axes are set by the plot() statement, but  
nothing is plotted.


best wishes

rksh





On 13 Jul 2005, at 09:12, (Ted Harding) wrote:
>>
>
> Although this is an over-worked query -- for which an answer, given
> that t="l" has been specified, is to use
>
>   plot(a,t="l",col="blue",ylim=c(0,10))
>   lines(b,t="l",col="red")
>
> there is a more interesting issue associated with it (given that
> Klebyn has come to it from a Matlab perspective).
>
> It's a long time since I used real Matlab, but I'll illustrate
> with octave which, in this respect, should be identical to Matlab.
>
> Octave:
>
> octave:1> x = 0.1*(0:20);
> octave:2> plot(x,sin(x))
>
> produces a graph of sin(x) with the y-axis scaled from 0 to 1.0
> Next:
>
> octave:3> hold on
> octave:4> plot(x,1.5*cos(x))
>
> superimposes a graph of 1.5*cos(x) with the y-axis automatically
> re-scaled from -1 to 1.5.
>
> This would not have happened in R with
>
>   x = 0.1*(0:20);
>   plot(x,sin(x))
>   lines(x,1.5*cos(x))
>
> where the 0 to 1.0 scaling of the first plot would be kept for
> the second, in which therefore part of the additional graph of
> 1.5*cos(x) would be "outside the box".
>
> No doubt like many others, I've been caught on the wrong foot
> by this more than a few times. The solution, of course (as
> illustrated in the reply to Klebyn above) is to anticipate
> what scaling you will need for all the graphs you intend to
> put on the same plot, and set up the scalings at the time
> of the first one using the options "xlim" and "ylim", e.g.:
>
>   x = 0.1*(0:20);
>   plot(x,sin(x),ylim=c(-1,1.5))
>   lines(x,1.5*cos(x))
>
> This is not always feasible, and indeed should not be expected
> to be feasible since part of the reason for using software
> like R in the first place is to compute what you do not know!
>
> Indeed, R will not allow you to use "xlim" or "ylim" once the
> first plot has been drawn.
>
> So in such cases I end up making a note (either on paper or,
> when I do really serious planning, in auxiliary variables)
> of the min's and max's for each graph, and then re-run the
> plotting commands with appropriate "xlim" and "ylim" scaling
> set up in the first plot so as to include all the subsequent
> graphs in entirety. (Even this strategy can be defeated if
> the succesive graphs represent simulations of long-tailed
> distributions. Unless of course I'm sufficiently alert to
> set the RNG seed first as well ... )
>
> I'm not sufficiently acquainted with the internals of "plot"
> and friends to anticipate the answer to this question; but,
> anyway, the question is:
>
>   Is it feasible to include, as a parameter to "plot", "lines"
>   and "points",
>
>     rescale=FALSE
>
>   where this default value would maintain the existing behaviour
>   of these functions, while setting
>
>     rescale=TRUE
>
>   would allow each succeeding plot, adding graphs using "points"
>   or "lines", to be rescaled (as in Matlab/Octave) so as to
>   include the entirety of each successive graph?
>
> Best wishes to all,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 13-Jul-05                                       Time: 09:12:34
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Wed Jul 13 10:46:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2005 09:46:28 +0100 (BST)
Subject: [R] Boxcox transformation / homogeneity of variances
In-Reply-To: <5.2.0.9.0.20050713100113.020cfec8@orleans.inra.fr>
References: <5.2.0.9.0.20050713100113.020cfec8@orleans.inra.fr>
Message-ID: <Pine.LNX.4.61.0507130936150.31107@gannet.stats>

Please consult the reference on the help page of that function: it _is_ 
support software for a book.  It implements the Box-Cox procedure (as it 
says).  The original Box-Cox paper has three aims, two of which you have 
mentioned (but perhaps the most inportant one is the one you 
have not mentioned, additivity).

It is probably worth stressing that the Box-Cox procedure is about finding 
the best transformation within a specific family for fitting a particular 
_model_ to a set of data, not for the data per se.  There is a long 
history of people using an inappropriate model and finding an 
uninterpretable transformation.

On Wed, 13 Jul 2005, Arnaud Dowkiw wrote:

> Prior to analysis of variance, I ran the Boxcox function (MASS library) to
> find the best power transformation of my data. However, reading the Boxcox
> help file, I cannot figure out if this function (through its associated
> log-likelihood function) corrects for * normality only * or if it also
> induces * homogeneity of variances *. I found in Biometry (Sokal and Rohlf,
> p. 419) that the box-cox transformation can be extended to induce
> homogenity of variances in conjunction with Bartlett's test of homogeneity
> of variances. Does the Boxcox function implemented in R refer to this
> extension ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jul 13 10:55:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2005 09:55:41 +0100 (BST)
Subject: [R] How to use the function "plot"  as Matlab
In-Reply-To: <A14D4C78-F35D-4842-901B-500ADA62F792@soc.soton.ac.uk>
References: <XFMail.050713091242.Ted.Harding@nessie.mcc.ac.uk>
	<A14D4C78-F35D-4842-901B-500ADA62F792@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0507130948151.31107@gannet.stats>

For most purposes it is easiest to use matplot() to plot superimposed 
plots like this.  E.g.

x <- 0.1*(0:20)
matplot(x, cbind(sin(x), cos(x)), "pl", pch=1)


On Wed, 13 Jul 2005, Robin Hankin wrote:

> Hi
>
> Ted makes a good point... matlab can dynamically rescale a plot in
> response
> to plot(...,add=TRUE) statements.
>
> For some reason which I do not understand, the rescaling issue is
> only a problem
> for me when working in "matlab mode".  It's not an issue when working
> in "R mode"
>
> Ted pointed out that the following does not behave as intended:
>
>
>>   x = 0.1*(0:20);
>>   plot(x,sin(x))
>>   lines(x,1.5*cos(x))
>
>
> and presented an alternative method in which ylim was set by hand.  I
> would suggest:
>
> x <- 0.1*(0:20)
> y1 <- sin(x)
> y2 <- 1.5*cos(x)
>
> plot(c(x,x),c(y1,y2),type="n")
> lines(x,y1)
> lines(x,y2)
>
> because this way, the axes are set by the plot() statement, but
> nothing is plotted.
>
>
> best wishes
>
> rksh
>
>
>
>
>
> On 13 Jul 2005, at 09:12, (Ted Harding) wrote:
>>>
>>
>> Although this is an over-worked query -- for which an answer, given
>> that t="l" has been specified, is to use
>>
>>   plot(a,t="l",col="blue",ylim=c(0,10))
>>   lines(b,t="l",col="red")
>>
>> there is a more interesting issue associated with it (given that
>> Klebyn has come to it from a Matlab perspective).
>>
>> It's a long time since I used real Matlab, but I'll illustrate
>> with octave which, in this respect, should be identical to Matlab.
>>
>> Octave:
>>
>> octave:1> x = 0.1*(0:20);
>> octave:2> plot(x,sin(x))
>>
>> produces a graph of sin(x) with the y-axis scaled from 0 to 1.0
>> Next:
>>
>> octave:3> hold on
>> octave:4> plot(x,1.5*cos(x))
>>
>> superimposes a graph of 1.5*cos(x) with the y-axis automatically
>> re-scaled from -1 to 1.5.
>>
>> This would not have happened in R with
>>
>>   x = 0.1*(0:20);
>>   plot(x,sin(x))
>>   lines(x,1.5*cos(x))
>>
>> where the 0 to 1.0 scaling of the first plot would be kept for
>> the second, in which therefore part of the additional graph of
>> 1.5*cos(x) would be "outside the box".
>>
>> No doubt like many others, I've been caught on the wrong foot
>> by this more than a few times. The solution, of course (as
>> illustrated in the reply to Klebyn above) is to anticipate
>> what scaling you will need for all the graphs you intend to
>> put on the same plot, and set up the scalings at the time
>> of the first one using the options "xlim" and "ylim", e.g.:
>>
>>   x = 0.1*(0:20);
>>   plot(x,sin(x),ylim=c(-1,1.5))
>>   lines(x,1.5*cos(x))
>>
>> This is not always feasible, and indeed should not be expected
>> to be feasible since part of the reason for using software
>> like R in the first place is to compute what you do not know!
>>
>> Indeed, R will not allow you to use "xlim" or "ylim" once the
>> first plot has been drawn.
>>
>> So in such cases I end up making a note (either on paper or,
>> when I do really serious planning, in auxiliary variables)
>> of the min's and max's for each graph, and then re-run the
>> plotting commands with appropriate "xlim" and "ylim" scaling
>> set up in the first plot so as to include all the subsequent
>> graphs in entirety. (Even this strategy can be defeated if
>> the succesive graphs represent simulations of long-tailed
>> distributions. Unless of course I'm sufficiently alert to
>> set the RNG seed first as well ... )
>>
>> I'm not sufficiently acquainted with the internals of "plot"
>> and friends to anticipate the answer to this question; but,
>> anyway, the question is:
>>
>>   Is it feasible to include, as a parameter to "plot", "lines"
>>   and "points",
>>
>>     rescale=FALSE
>>
>>   where this default value would maintain the existing behaviour
>>   of these functions, while setting
>>
>>     rescale=TRUE
>>
>>   would allow each succeeding plot, adding graphs using "points"
>>   or "lines", to be rescaled (as in Matlab/Octave) so as to
>>   include the entirety of each successive graph?
>>
>> Best wishes to all,
>> Ted.
>>
>>
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 13-Jul-05                                       Time: 09:12:34
>> ------------------------------ XFMail ------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>> guide.html
>>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From S.O.Nyangoma at amc.uva.nl  Wed Jul 13 11:06:05 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Wed, 13 Jul 2005 11:06:05 +0200
Subject: [R] exact values for p-values
Message-ID: <f39622f3a5c9.f3a5c9f39622@amc.uva.nl>

Hi David, Since I am looking at very extreme values, it appears I will 
need FMLIB. Is it an R lib? if so which version? How/where can I 
download it?

Regards.


----- Original Message -----
From: David Duffy <David.Duffy at qimr.edu.au>
Date: Wednesday, July 13, 2005 9:46 am
Subject: [R]  exact values for p-values

> > This is obtained from F =39540 with df1 = 1, df2 = 7025.
> > Suppose am interested in exact value such as
> >
> 
> If it were really necessary, you would have to move to multiple
> precision.  The gmp R package doesn't seem to yet cover this, but 
> FMLIB(TOMS814, DM Smith) is a multiple precision f90 library that 
does
> include the incomplete beta -- it allows one to say for 
> F(1,7025)=39540,P=6.31E-2886 (evaluated using 200 sign. digit 
> arithmetic).  Results from
> R's pf() agree quite closely with the FMLIB results for less 
> extreme values
> eg
> > print(pf(1500,1,7025,lower=FALSE), digits=20)
> [1] 1.3702710894887480597e-297
> 
> cf   1.37027108948832580215549799419452388134616261215463681945E-297
> 
> 
> | David Duffy (MBBS PhD)                                         ,-
_|\
> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /  
>   *
> | Epidemiology Unit, Queensland Institute of Medical Research   
> \_,-._/
> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 
> 4D0B994A v
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From r.hankin at noc.soton.ac.uk  Wed Jul 13 11:04:51 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Jul 2005 10:04:51 +0100
Subject: [R] Kronecker matrix product
Message-ID: <2AE3935D-64D9-436A-A6A5-804EEAC93DC9@soc.soton.ac.uk>

Hi

I want to write a little function that takes a matrix X of size
m-by-n, and a list L of length  "m",  whose elements are matrices all  
of which have
the same number of columns but possibly a different number of rows.

I then want to get a sort of dumbed-down kronecker product in which
X[i,j] is replaced by X[i,j]*L[[j]]

where L[[j]] is the j-th of the "m" matrices.  For example, if

X = matrix(c(1,5,0,2),2,2)

and

L[[1]] = matrix(1:4,2,2)
L[[2]] = matrix(c(1,1,1,1,1,10),ncol=2)

I want


      [,1] [,2] [,3] [,4]
[1,]    1    3    0    0
[2,]    2    4    0    0
[3,]    5    5    2    2
[4,]    5    5    2    2
[5,]    5   50    2   20
 >


see how, for example, out[3:5,1:2]  == 5*L[[2]], the "5" coming from X 
[2,1].

[
I can bind L together into a single matrix with

do.call("rbind",L)

and calculate the number of rows with

sapply(L,nrow)

but I don't see how this can help.
]






--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From navarre_sabine at yahoo.fr  Wed Jul 13 11:15:21 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Wed, 13 Jul 2005 11:15:21 +0200 (CEST)
Subject: [R] delete a row from a matrix
Message-ID: <20050713091521.23435.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/1dd2aede/attachment.pl

From berwin at maths.uwa.edu.au  Wed Jul 13 11:37:37 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 13 Jul 2005 17:37:37 +0800
Subject: [R] Kronecker matrix product
In-Reply-To: <2AE3935D-64D9-436A-A6A5-804EEAC93DC9@soc.soton.ac.uk>
References: <2AE3935D-64D9-436A-A6A5-804EEAC93DC9@soc.soton.ac.uk>
Message-ID: <17108.57569.703141.191475@bossiaea.maths.uwa.edu.au>

>>>>> "RH" == Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

    RH> I want to write a little function that takes a matrix X of
    RH> size m-by-n, and a list L of length "m", whose elements are
    RH> matrices all of which have the same number of columns but
    RH> possibly a different number of rows.

    RH> I then want to get a sort of dumbed-down kronecker product in which
    RH> X[i,j] is replaced by X[i,j]*L[[j]]

    RH> where L[[j]] is the j-th of the "m" matrices.  For example, if

    RH> X = matrix(c(1,5,0,2),2,2)

    RH> and

    RH> L[[1]] = matrix(1:4,2,2)
    RH> L[[2]] = matrix(c(1,1,1,1,1,10),ncol=2)

    RH> I want


    RH> [,1] [,2] [,3] [,4]
    RH> [1,]    1    3    0    0
    RH> [2,]    2    4    0    0
    RH> [3,]    5    5    2    2
    RH> [4,]    5    5    2    2
    RH> [5,]    5   50    2   20

> tmp <- sapply(1:length(L), function(j, mat, list) kronecker(X[j,,drop=FALSE], L[[j]]), mat=X, list=L)
> do.call("rbind", tmp)
     [,1] [,2] [,3] [,4]
[1,]    1    3    0    0
[2,]    2    4    0    0
[3,]    5    5    2    2
[4,]    5    5    2    2
[5,]    5   50    2   20
> 

HTH.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From p.dalgaard at biostat.ku.dk  Wed Jul 13 11:38:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jul 2005 11:38:30 +0200
Subject: [R] delete a row from a matrix
In-Reply-To: <20050713091521.23435.qmail@web26604.mail.ukl.yahoo.com>
References: <20050713091521.23435.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <x2fyujcag9.fsf@turmalin.kubism.ku.dk>

Navarre Sabine <navarre_sabine at yahoo.fr> writes:

> Hi,
> I would like to know if it's possible to delete a rox from a matrix?
>  
> > fig
>      [,1] [,2] [,3] [,4]
> [1,]    0    1  0.0  0.2
> [2,]    0    1  0.2  0.8
> [3,]    0    1  0.8  1.0
> [4,]    0    1   NA   NA
> [5,]    0    1   NA   NA
> 
> I would like to delete the 2 rows with NA!

fig <- fig[-c(4,5),]

or, more generally

fig <- fig[complete.cases(fig),] 

or, even more generally

fig <- fig[!apply(is.na(fig), 1, any),]

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Colin.Beale at rspb.org.uk  Wed Jul 13 11:41:45 2005
From: Colin.Beale at rspb.org.uk (Beale, Colin)
Date: Wed, 13 Jul 2005 10:41:45 +0100
Subject: [R] nlme, MASS and geoRglm for spatial autocorrelation?
Message-ID: <E71A2CDB5DFFD14383BAC2E0612811BA0DFB1D@ADARA.RSPB.ORG.UK>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/8beb9d7b/attachment.pl

From berwin at maths.uwa.edu.au  Wed Jul 13 11:44:03 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 13 Jul 2005 17:44:03 +0800
Subject: [R] Kronecker matrix product
In-Reply-To: <17108.57569.703141.191475@bossiaea.maths.uwa.edu.au>
References: <2AE3935D-64D9-436A-A6A5-804EEAC93DC9@soc.soton.ac.uk>
	<17108.57569.703141.191475@bossiaea.maths.uwa.edu.au>
Message-ID: <17108.57955.546190.229643@bossiaea.maths.uwa.edu.au>

>>>>> "BT" == Berwin A Turlach <berwin at maths.uwa.edu.au> writes:

    >> tmp <- sapply(1:length(L), function(j, mat, list) kronecker(X[j,,drop=FALSE], L[[j]]), mat=X, list=L)

Uups, should proof read more carefully before hitting the send
button.  This should be, of course:

tmp <- sapply(1:length(L),
              function(j, mat, list) kronecker(mat[j,,drop=FALSE], list[[j]]),
              mat=X, list=L)

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From LIENKI at sun.ac.za  Wed Jul 13 11:44:19 2005
From: LIENKI at sun.ac.za (Viljoen L <lienki@sun.ac.za>)
Date: Wed, 13 Jul 2005 11:44:19 +0200
Subject: [R] GARCH model using fSeries
Message-ID: <18C41F6237485146904D23529B29703B13367E@STBEVS04.stb.sun.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/3ed06a5e/attachment.pl

From ghislainv at gmail.com  Wed Jul 13 12:02:51 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Wed, 13 Jul 2005 12:02:51 +0200
Subject: [R] Name for factor's levels with contr.sum
Message-ID: <ff51f022050713030254f6d6dc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/e5942f66/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Jul 13 12:06:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Jul 2005 12:06:29 +0200
Subject: [R] Please help me.....
In-Reply-To: <27004DDE1590B344855CF773E1D019F115BF2A@postino.ifop.cl>
References: <27004DDE1590B344855CF773E1D019F115BF2A@postino.ifop.cl>
Message-ID: <42D4E7A5.7040308@statistik.uni-dortmund.de>

Fernando Esp??ndola wrote:

> Hi user R,
> 
> I am try to calculate the spectrum function in two time series. But when plot a single serie, the labels in axes x is in the range 0.1 to 0.6 (frequency), but when calculate de spectrum with ts.union function, the labels x is in the range 1 to 6. I not understand why change the labels, and not know that is ralationship. Samebody can hel me in this analysis.....


Not so for me.
As the posting guide asks you to do: Can you specify a reproducible 
example, please.

Uwe Ligges


> Thank for all
> 
> fdo
> 
> Fernando Espindola R.
> Division Investigacion Pesquera
> Instituto de Fomento Pesquero
> Blanco 839
> Valparaiso - CHILE
> fono: 32 - 322442
> fernando.espindola at ifop.cl
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ecoinformatics at gmail.com  Wed Jul 13 12:08:08 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Wed, 13 Jul 2005 12:08:08 +0200
Subject: [R] delete a row from a matrix
In-Reply-To: <x2fyujcag9.fsf@turmalin.kubism.ku.dk>
References: <20050713091521.23435.qmail@web26604.mail.ukl.yahoo.com>
	<x2fyujcag9.fsf@turmalin.kubism.ku.dk>
Message-ID: <15f8e67d05071303085bd605f6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/4962e178/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Jul 13 12:01:42 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Jul 2005 11:01:42 +0100 (BST)
Subject: [R] How to use the function "plot"  as Matlab
In-Reply-To: <Pine.LNX.4.61.0507130948151.31107@gannet.stats>
Message-ID: <XFMail.050713110142.Ted.Harding@nessie.mcc.ac.uk>

On 13-Jul-05 Prof Brian Ripley wrote:
> For most purposes it is easiest to use matplot() to plot superimposed 
> plots like this.  E.g.
> 
> x <- 0.1*(0:20)
> matplot(x, cbind(sin(x), cos(x)), "pl", pch=1)

This, and Robin's suggestion, are good practical solutions especially
when only a few graphs (2 or 3 or ... ) are involved. However, their
undelying principle is to accumulate auxiliary variables encapsulating
the graphs which will eventually be plotted.

However, once in a while I like to make a really messy graph of
superimposed sample paths of a simulated stochastic process, perhaps
with several dozen replications and many points (even 5000) along
each sample path. An example where this has a real practical point
is diffusion from the chimney stack of, say, an incinerator. The
resulting plot can give a good picture of the "average plume",
allowing the viewer to form an impression of the variation in
concentration along and on the fringes of the plume.

This is definitely a case where "dynamic rescaling" could save
hassle! Brian Ripley's suggestion involves first building a
matrix whose columns are the replications and rows the time-points,
and Robin Hankin's could be easily adapted to do the same,
though I think would involve a loop over columns and some very
long vectors.

How much easier it would be with dynamic scaling!

Best wishes,
Ted.

> On Wed, 13 Jul 2005, Robin Hankin wrote:
> 
>> Hi
>>
>> Ted makes a good point... matlab can dynamically rescale a plot in
>> response
>> to plot(...,add=TRUE) statements.
>>
>> For some reason which I do not understand, the rescaling issue is
>> only a problem
>> for me when working in "matlab mode".  It's not an issue when working
>> in "R mode"
>>
>> Ted pointed out that the following does not behave as intended:
>>
>>
>>>   x = 0.1*(0:20);
>>>   plot(x,sin(x))
>>>   lines(x,1.5*cos(x))
>>
>>
>> and presented an alternative method in which ylim was set by hand.  I
>> would suggest:
>>
>> x <- 0.1*(0:20)
>> y1 <- sin(x)
>> y2 <- 1.5*cos(x)
>>
>> plot(c(x,x),c(y1,y2),type="n")
>> lines(x,y1)
>> lines(x,y2)
>>
>> because this way, the axes are set by the plot() statement, but
>> nothing is plotted.
>>
>>
>> best wishes
>>
>> rksh
>>
>>
>>
>>
>>
>> On 13 Jul 2005, at 09:12, (Ted Harding) wrote:
>>>>
>>>
>>> Although this is an over-worked query -- for which an answer, given
>>> that t="l" has been specified, is to use
>>>
>>>   plot(a,t="l",col="blue",ylim=c(0,10))
>>>   lines(b,t="l",col="red")
>>>
>>> there is a more interesting issue associated with it (given that
>>> Klebyn has come to it from a Matlab perspective).
>>>
>>> It's a long time since I used real Matlab, but I'll illustrate
>>> with octave which, in this respect, should be identical to Matlab.
>>>
>>> Octave:
>>>
>>> octave:1> x = 0.1*(0:20);
>>> octave:2> plot(x,sin(x))
>>>
>>> produces a graph of sin(x) with the y-axis scaled from 0 to 1.0
>>> Next:
>>>
>>> octave:3> hold on
>>> octave:4> plot(x,1.5*cos(x))
>>>
>>> superimposes a graph of 1.5*cos(x) with the y-axis automatically
>>> re-scaled from -1 to 1.5.
>>>
>>> This would not have happened in R with
>>>
>>>   x = 0.1*(0:20);
>>>   plot(x,sin(x))
>>>   lines(x,1.5*cos(x))
>>>
>>> where the 0 to 1.0 scaling of the first plot would be kept for
>>> the second, in which therefore part of the additional graph of
>>> 1.5*cos(x) would be "outside the box".
>>>
>>> No doubt like many others, I've been caught on the wrong foot
>>> by this more than a few times. The solution, of course (as
>>> illustrated in the reply to Klebyn above) is to anticipate
>>> what scaling you will need for all the graphs you intend to
>>> put on the same plot, and set up the scalings at the time
>>> of the first one using the options "xlim" and "ylim", e.g.:
>>>
>>>   x = 0.1*(0:20);
>>>   plot(x,sin(x),ylim=c(-1,1.5))
>>>   lines(x,1.5*cos(x))
>>>
>>> This is not always feasible, and indeed should not be expected
>>> to be feasible since part of the reason for using software
>>> like R in the first place is to compute what you do not know!
>>>
>>> Indeed, R will not allow you to use "xlim" or "ylim" once the
>>> first plot has been drawn.
>>>
>>> So in such cases I end up making a note (either on paper or,
>>> when I do really serious planning, in auxiliary variables)
>>> of the min's and max's for each graph, and then re-run the
>>> plotting commands with appropriate "xlim" and "ylim" scaling
>>> set up in the first plot so as to include all the subsequent
>>> graphs in entirety. (Even this strategy can be defeated if
>>> the succesive graphs represent simulations of long-tailed
>>> distributions. Unless of course I'm sufficiently alert to
>>> set the RNG seed first as well ... )
>>>
>>> I'm not sufficiently acquainted with the internals of "plot"
>>> and friends to anticipate the answer to this question; but,
>>> anyway, the question is:
>>>
>>>   Is it feasible to include, as a parameter to "plot", "lines"
>>>   and "points",
>>>
>>>     rescale=FALSE
>>>
>>>   where this default value would maintain the existing behaviour
>>>   of these functions, while setting
>>>
>>>     rescale=TRUE
>>>
>>>   would allow each succeeding plot, adding graphs using "points"
>>>   or "lines", to be rescaled (as in Matlab/Octave) so as to
>>>   include the entirety of each successive graph?
>>>
>>> Best wishes to all,
>>> Ted.
>>>
>>>
>>> --------------------------------------------------------------------
>>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>>> Fax-to-email: +44 (0)870 094 0861
>>> Date: 13-Jul-05                                       Time: 09:12:34
>>> ------------------------------ XFMail ------------------------------
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-
>>> guide.html
>>>
>>
>> --
>> Robin Hankin
>> Uncertainty Analyst
>> National Oceanography Centre, Southampton
>> European Way, Southampton SO14 3ZH, UK
>>  tel  023-8059-7743
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jul-05                                       Time: 11:00:01
------------------------------ XFMail ------------------------------



From stagiaire2.urc at nck.ap-hop-paris.fr  Wed Jul 13 12:10:57 2005
From: stagiaire2.urc at nck.ap-hop-paris.fr (Claude Messiaen - Urc Necker)
Date: Wed, 13 Jul 2005 12:10:57 +0200
Subject: [R]  fitting Weibull distribution on observed percentiles
Message-ID: <000c01c58793$293b6db0$39c9900a@nck.aphopparis.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/c1580cf5/attachment.pl

From murdoch at stats.uwo.ca  Wed Jul 13 12:21:57 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 13 Jul 2005 06:21:57 -0400
Subject: =?UTF-8?B?562U5aSNOiBbUl0gZmFpbCBpbiBhZGRpbmcgbGlicmFyeSBpbiA=?=
	=?UTF-8?B?bmV3IHZlcnNpb24u?=
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF2E@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF2E@ex120.smic-sh.com>
Message-ID: <42D4EB45.7080405@stats.uwo.ca>

Ivy_Li wrote:
> Dear all,
> I really appreciate your help. I think I have a little advancement. ^_^
> Now I use the package.skeleton() function to create a template. I type:
> 	f <- function(x,y) x+y
> 	g <- function(x,y) x-y
> 	d <- data.frame(a=1, b=2)
> 	e <- rnorm(1000)
> 	package.skeleton(list=c("f","g","d","e"), name="example")
> 
> in R. I know it will create a folder named "example" in the path of "\R\rw2011\" I opened this folder, its format is similar as other library. Then I modify it "DESCRIPTION" file:
> 	Package: example
> 	Version: 1.0-1
> 	Date: 2005-07-09
> 	Title: My first function
> 	Author: Ivy <Ivy_Li at smics.com>
> 	Maintainer: Ivy <Ivy_Li at smics.com>
> 	Description: simple sum and subtract
> 	License: GPL version 2 or later
> 	Depends: R (>= 1.9), stats, graphics, utils
> 
> I don't whether I should modify other "README" file.
> When I enter the Dos environment, at first, into the D:\>, I type the following code:
> 	cd Program Files\R\rw2011\
> 	bin\R CMD install /example
> 
> Well, there appeared error:
> 	---------- Making package example ------------
> 	  adding build stamp to DESCRIPTION
> 	  installing R files
> 	  installing data files
> 	  installing man source files
> 	  installing indices
> 	  not zipping data
> 	  installing help
> 	 >>> Building/Updating help pages for package 'example'
> 	     Formats: text html latex example chm
> 	  d                                 text    html    latex   example chm
> 	  e                                 text    html    latex   example chm
> 	  f                                 text    html    latex   example chm
> 	     missing link(s):  ~~fun~~
> 	  g                                 text    html    latex   example chm
> 	     missing link(s):  ~~fun~~
> 	hhc: not found
> 	cp: cannot stat `D:/PROGRA~1/R/rw2011/example/chm/example.chm': No such file or
> 	directory
> 	make[1]: *** [chm-example] Error 1
> 	make: *** [pkg-example] Error 2
> 	*** Installation of example failed ***
> 	
> 	Removing 'D:/PROGRA~1/R/rw2011/library/example'
> 
> That's it. I have to consult every R expert. Please help to solve this issue. Thank you very much!

See the appendix "The Windows Toolset" in the R Installation and 
Administration manual.  You need to install those tools.

If you've done that, but decided not to use the Help Compiler (hhc), 
then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell 
it not to try to build that kind of help.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Jul 13 12:29:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2005 11:29:31 +0100 (BST)
Subject: [R] nlme, MASS and geoRglm for spatial autocorrelation?
In-Reply-To: <E71A2CDB5DFFD14383BAC2E0612811BA0DFB1D@ADARA.RSPB.ORG.UK>
References: <E71A2CDB5DFFD14383BAC2E0612811BA0DFB1D@ADARA.RSPB.ORG.UK>
Message-ID: <Pine.LNX.4.61.0507131123200.32530@gannet.stats>

You seem to want to model spatially correlated bernoulli variables.
That's a difficult task, especially as these are bernoulli and not 
binomial(n>1).  With a much fuller description of the problem we may be 
able to help, but I at least have no idea of the aims of the analysis.

glmmPQL is designed for independent observations conditional on the 
random effects.

On Wed, 13 Jul 2005, Beale, Colin wrote:

> Hi.
>
> I'm trying to perform what should be a reasonably basic analysis of some
> spatial presence/absence data but am somewhat overwhelmed by the options
> available and could do with a helpful pointer. My researches so far
> indicate that if my data were normal, I would simply use gls() (in nlme)
> and one of the various corSpatial functions (eg. corSpher() to be
> analagous to similar analysis in SAS) with form = ~ x+y (and a nugget if
> appropriate). However, my data are binomial, so I need a different
> approach. Using various packages I could define a mixed model (eg using
> glmmPQL() in MASS) with similar correlation structure, but I seem to
> need to define a random effect to use glmmPQL(), and I don't have any.
> Could this requirement be switched off and still use the mixed model
> approach? Alternatively, it may be possible to define the variance
> appropriately in gls and use logits directly, but I'm not quite sure how
> and suspect there's a more straight-forward alternative. Looking at
> geoRglm suggests there may be solutions here, but it seems like it might
> be overkill for what is, at first appearance at least, not such a
> difficult problem. Maybe I'm just being statistically naive, but I think
> I'm looking for a function somewhere between gls() and glmmPQL() and
> would be grateful for any pointers.
>
> Thanks very much,
>
> Colin Beale
>
> ...
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From karen at biology.biol.wits.ac.za  Wed Jul 13 12:32:28 2005
From: karen at biology.biol.wits.ac.za (Karen Kotschy)
Date: Wed, 13 Jul 2005 12:32:28 +0200 (SAST)
Subject: [R] adding a factor column based on levels of another factor
In-Reply-To: <17107.59212.49475.313409@stat.math.ethz.ch>
References: <Pine.LNX.4.58.0507121157030.4900@euclea.sevenc.private>
	<17107.59212.49475.313409@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0507131223340.13617@euclea.sevenc.private>

Hello

Thanks for the replies. Merge was what I needed! But Christoph, I will
keep your email. What you described is something else I have been
wondering how to do in R...

Thanks again
Karen



From ramasamy at cancer.org.uk  Wed Jul 13 12:44:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 13 Jul 2005 11:44:00 +0100
Subject: [R] delete a row from a matrix
In-Reply-To: <15f8e67d05071303085bd605f6@mail.gmail.com>
References: <20050713091521.23435.qmail@web26604.mail.ukl.yahoo.com>
	<x2fyujcag9.fsf@turmalin.kubism.ku.dk>
	<15f8e67d05071303085bd605f6@mail.gmail.com>
Message-ID: <1121251440.5950.26.camel@ipc143004.lif.icnet.uk>

If you want to append to the first/last column or row, then use cbind or
rbind. It is a little tricky if you want to insert a row in the middle
somewhere. See insertRow in micEcon package.

Regards, Adai


On Wed, 2005-07-13 at 12:08 +0200, ecoinfo wrote:
> Hi,
> May I ask a related question, how to insert a row/column to a matrix?
>  Thanks
> Xiaohua
> 
>  On 13 Jul 2005 11:38:30 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> 
> wrote: 
> > 
> > Navarre Sabine <navarre_sabine at yahoo.fr> writes:
> > 
> > > Hi,
> > > I would like to know if it's possible to delete a rox from a matrix?
> > >
> > > > fig
> > > [,1] [,2] [,3] [,4]
> > > [1,] 0 1 0.0 0.2
> > > [2,] 0 1 0.2 0.8
> > > [3,] 0 1 0.8 1.0
> > > [4,] 0 1 NA NA
> > > [5,] 0 1 NA NA
> > >
> > > I would like to delete the 2 rows with NA!
> > 
> > fig <- fig[-c(4,5),]
> > 
> > or, more generally
> > 
> > fig <- fig[complete.cases(fig),]
> > 
> > or, even more generally
> > 
> > fig <- fig[!apply(is.na(fig), 1, any),]
> > 
> > --
> > O__ ---- Peter Dalgaard ster Farimagsgade 5, Entr.B
> > c/ /'_ --- Dept. of Biostatistics PO Box 2099, 1014 Cph. K
> > (*) \(*) -- University of Copenhagen Denmark Ph: (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 35327907
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
> 
> --
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r.hankin at noc.soton.ac.uk  Wed Jul 13 12:50:34 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Jul 2005 11:50:34 +0100
Subject: [R] How to use the function "plot"  as Matlab
In-Reply-To: <XFMail.050713110142.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050713110142.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <EE0730AE-6827-4FB4-AD21-54C96432371E@soc.soton.ac.uk>


On 13 Jul 2005, at 11:01, (Ted Harding) wrote:

> On 13-Jul-05 Prof Brian Ripley wrote:
>
>> For most purposes it is easiest to use matplot() to plot superimposed
>> plots like this.  E.g.
>>
>> x <- 0.1*(0:20)
>> matplot(x, cbind(sin(x), cos(x)), "pl", pch=1)
>>
>
> This, and Robin's suggestion, are good practical solutions especially
> when only a few graphs (2 or 3 or ... ) are involved. However, their
> undelying principle is to accumulate auxiliary variables encapsulating
> the graphs which will eventually be plotted.


Ted makes a good point here.  I would find this quite useful, for EDA  
(exploratory
data analysis) work, where one often needs to add new lines to a  
plot, one at a
time, in an ad hoc manner, just to "see what happens".

Would adding such functionality (perhaps via a new Boolean argument  
to plot(),
"rescaling", defaulting to FALSE, that enabled dynamic rescaling when  
plot(...,add=TRUE) is
executed) require quite a lot of low-level work?

best wishes

Robin



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From krcabrer at unalmed.edu.co  Wed Jul 13 13:13:59 2005
From: krcabrer at unalmed.edu.co (Kenneth Roy Cabrera Torres)
Date: Wed, 13 Jul 2005 06:13:59 -0500
Subject: [R] Memory question
In-Reply-To: <dd48e20f05071210455ac28c1d@mail.gmail.com>
References: <dd48e20f05071210455ac28c1d@mail.gmail.com>
Message-ID: <opstumtlat96pdmo@kenneth>

Hi R users and developers:

I want to know how can I save memory in R
for example:
  - saving on disk a matrix.
  - using again the matrix (changing their values)
  - saving again the matrix on disk in a different file.

The idea is that I have a process that generate several
matrices, but if I keep them all in memory it will overflow.

How can I save them in different files, so I use the same
amount of memory for each processed matrix?

Thank you for your help.

-- 
Kenneth Roy Cabrera Torres
Universidad Nacional de Colombia
Sede Medellin
Tel 430 9351
Cel 315 504 9339



From Colin.Beale at rspb.org.uk  Wed Jul 13 13:14:56 2005
From: Colin.Beale at rspb.org.uk (Beale, Colin)
Date: Wed, 13 Jul 2005 12:14:56 +0100
Subject: [R] nlme, MASS and geoRglm for spatial autocorrelation?
Message-ID: <E71A2CDB5DFFD14383BAC2E0612811BA0DFB7D@ADARA.RSPB.ORG.UK>

My data are indeed bernoulli and not binomial, as I indicated. The
dataset consists of points (grid refs) that are either locations of
events (animals) or random points (with no animal present). For each
point I have a suite of environmental covariates describing the habitat
at this point. I was anticipating some sort of function that could run:

function(present ~ env1 + env2 + env3 + x + y, correlation =
corSpher(form=~x+y), family = binomial)

where env1 to env3 are the habitat covariates, x & y the grid refs. If
my data were normal, I undertand I would use gls() with exactly this,
but drop the family requirement. As my data are bernoulli this is
clearly not possible, but I was hoping the analysis may be analagous?
The eventual aim is to firstly understand which environmental covariates
are important in determining presence and then to use habitat maps to
identify the areas expected to be most important.

Colin

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 13 July 2005 11:30
To: Beale, Colin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] nlme, MASS and geoRglm for spatial autocorrelation?

You seem to want to model spatially correlated bernoulli variables.
That's a difficult task, especially as these are bernoulli and not
binomial(n>1).  With a much fuller description of the problem we may be
able to help, but I at least have no idea of the aims of the analysis.

glmmPQL is designed for independent observations conditional on the
random effects.

On Wed, 13 Jul 2005, Beale, Colin wrote:

> Hi.
>
> I'm trying to perform what should be a reasonably basic analysis of 
> some spatial presence/absence data but am somewhat overwhelmed by the 
> options available and could do with a helpful pointer. My researches 
> so far indicate that if my data were normal, I would simply use gls() 
> (in nlme) and one of the various corSpatial functions (eg. corSpher() 
> to be analagous to similar analysis in SAS) with form = ~ x+y (and a 
> nugget if appropriate). However, my data are binomial, so I need a 
> different approach. Using various packages I could define a mixed 
> model (eg using
> glmmPQL() in MASS) with similar correlation structure, but I seem to 
> need to define a random effect to use glmmPQL(), and I don't have any.
> Could this requirement be switched off and still use the mixed model 
> approach? Alternatively, it may be possible to define the variance 
> appropriately in gls and use logits directly, but I'm not quite sure 
> how and suspect there's a more straight-forward alternative. Looking 
> at geoRglm suggests there may be solutions here, but it seems like it 
> might be overkill for what is, at first appearance at least, not such 
> a difficult problem. Maybe I'm just being statistically naive, but I 
> think I'm looking for a function somewhere between gls() and glmmPQL()

> and would be grateful for any pointers.
>
> Thanks very much,
>
> Colin Beale
>

...



From p.dalgaard at biostat.ku.dk  Wed Jul 13 13:26:11 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jul 2005 13:26:11 +0200
Subject: [R] How to use the function "plot"  as Matlab
In-Reply-To: <XFMail.050713110142.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050713110142.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x27jfvc5gs.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> 
> This is definitely a case where "dynamic rescaling" could save
> hassle! Brian Ripley's suggestion involves first building a
> matrix whose columns are the replications and rows the time-points,
> and Robin Hankin's could be easily adapted to do the same,
> though I think would involve a loop over columns and some very
> long vectors.
> 
> How much easier it would be with dynamic scaling!

Cue grid graphics... (and Paul's new book)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mkondrin at hppi.troitsk.ru  Wed Jul 13 13:30:53 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Wed, 13 Jul 2005 15:30:53 +0400
Subject: [R] finalize objects
Message-ID: <42D4FB6D.9020808@hppi.troitsk.ru>

Hello!
How to set function for the whole R-class to be executed when the object 
is no more referenced from R and garbage collection takes place? I need 
the function to be applied for the whole class (let it be "someRClass") 
like this someRClass.on.finalize<-function(...){...}. I have found 
reg.finalizer function in the manuals, but it apply for the class 
instance, the thing that I do not like.
What is the solution?



From ajayshah at mayin.org  Mon Jul 11 17:52:59 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Mon, 11 Jul 2005 21:22:59 +0530
Subject: [R] Misbehaviour of DSE
In-Reply-To: <278C2EB9-7246-465C-B5FD-869A4D337DD6@mac.com>
References: <20050711122113.EAF9415D3C5@lubyanka.local>
	<278C2EB9-7246-465C-B5FD-869A4D337DD6@mac.com>
Message-ID: <20050711155259.GJ2053@lubyanka.local>

On Mon, Jul 11, 2005 at 08:27:40AM -0700, Rob J Goedman wrote:
> Ajay,
> 
> After installing both setRNG (2004.4-1, source or binary) and dse  
> (2005.6-1, source only), it works fine.

Thanks! :-) Now dse1 works, but I get:

> library(dse2)
Warning message:
replacing previous import: acf in: namespaceImportFrom(self, asNamespace(ns)) 

Should I worry?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From dmbates at gmail.com  Wed Jul 13 15:05:57 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 13 Jul 2005 08:05:57 -0500
Subject: [R] testing for significance in random-effect factors using lmer
In-Reply-To: <2794545991emoigar@uv.es>
References: <2794545991emoigar@uv.es>
Message-ID: <40e66e0b050713060535951626@mail.gmail.com>

On 7/12/05, Eduardo.Garcia at uv.es <Eduardo.Garcia at uv.es> wrote:
> Hi, I would like to know whether it is possible to obtain a value of
> significance for random effects when aplying the lme or related
> functions. The default output in R is just a variance and standard
> deviation measurement.
> 
> I feel it would be possible to obtain the significance of these random
> effects by comparing models with and without these effects. However,
> I'm not used to perform this in R and I would thank any easy guide or
> example.

It is possible to do a likelihood ratio test on two fitted lmer models
with different specifications of the random effects.  The p-value for
such a test is calculated using the chi-squared distribution from the
asymptotic theory which does not apply in most such comparisons
because the parameter for the null hypothesis is on the boundary of
the parameter region.  The p-value shown will be conservative (that
is, it is an upper bound on the true p-value).

For example

> library(mlmRev)
Loading required package: lme4
Loading required package: Matrix
Loading required package: lattice
> options(show.signif.stars = FALSE)
> (fm1 <- lmer(normexam ~ standLRT + sex + type + (1|school), Exam))
Linear mixed-effects model fit by REML
Formula: normexam ~ standLRT + sex + type + (1 | school) 
   Data: Exam 
      AIC      BIC    logLik MLdeviance REMLdeviance
 9357.384 9395.237 -4672.692   9325.485     9345.384
Random effects:
 Groups   Name        Variance Std.Dev.
 school   (Intercept) 0.084367 0.29046 
 Residual             0.562529 0.75002 
# of obs: 4059, groups: school, 65

Fixed effects:
               Estimate  Std. Error   DF t value  Pr(>|t|)
(Intercept) -1.7233e-03  5.4982e-02 4055 -0.0313   0.97500
standLRT     5.5983e-01  1.2448e-02 4055 44.9725 < 2.2e-16
sexM        -1.6596e-01  3.2812e-02 4055 -5.0579 4.426e-07
typeSngl     1.6546e-01  7.7428e-02 4055  2.1369   0.03266
> (fm2 <- lmer(normexam ~ standLRT + sex + type + (standLRT|school), Exam))
Linear mixed-effects model fit by REML
Formula: normexam ~ standLRT + sex + type + (standLRT | school) 
   Data: Exam 
      AIC      BIC    logLik MLdeviance REMLdeviance
 9316.573 9367.043 -4650.287    9281.17     9300.573
Random effects:
 Groups   Name        Variance Std.Dev. Corr  
 school   (Intercept) 0.082477 0.28719        
          standLRT    0.015081 0.12280  0.579 
 Residual             0.550289 0.74181        
# of obs: 4059, groups: school, 65

Fixed effects:
               Estimate  Std. Error   DF t value  Pr(>|t|)
(Intercept)   -0.020727    0.052548 4055 -0.3944   0.69327
standLRT       0.554101    0.020117 4055 27.5433 < 2.2e-16
sexM          -0.167971    0.032281 4055 -5.2034 2.054e-07
typeSngl       0.176390    0.069587 4055  2.5348   0.01129
> anova(fm2, fm1)
Data: Exam
Models:
fm1: normexam ~ standLRT + sex + type + (1 | school)
fm2: normexam ~ standLRT + sex + type + (standLRT | school)
    Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
fm1  6  9357.4  9395.2 -4672.7                         
fm2  8  9316.6  9367.0 -4650.3 44.811      2  1.859e-10

At present the anova method for lmer objects does not allow comparison
with models that have no fixed effects.  Writing that code is on my
ToDo list but not currently at the top.  It is possible to use anova
to compare models fit by lme with models fit by lm (with the same
caveat about the calculated p-value being conservative).

An interesting alternative approach is to use Metropolis-Hastings
sampling for a MCMC chain based on the fitted model and create HPD
intervals from such a sample.  I have a prototype function to do this
for generalized linear mixed models in versions 0.97-3 and later of
the Matrix  package (currently hidden in the namespace and not
documented but the interested user can look at Matrix:::glmmMCMC).  It
happens that I developed the generalized linear version of this before
developing a version for linear mixed models but the lmm version will
be forthcoming.


> 
> Thanks.
> --
> ********************************
> Eduardo Mois??s Garc??a Roger
> 
> Institut Cavanilles de Biodiversitat i Biologia
> Evolutiva - ICBIBE.
> Tel. +34963543664
> Fax  +34963543670
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jserra at uchicago.edu  Wed Jul 13 15:14:31 2005
From: jserra at uchicago.edu (Joan Serra)
Date: Wed, 13 Jul 2005 08:14:31 -0500 (CDT)
Subject: [R] problems with MNP
Message-ID: <Pine.GSO.4.62.0507130809510.8825@harper.uchicago.edu>

Hi all,

Does anybody have a hint on what may be going wrong in this R code? I 
mimic the sample code from the MNP developers but I seem unable to get the 
choice specific variables right.

Thanks,
Joan Serra

> rm(list=ls())
> library(foreign)
> small<-read.spss("small.sav")
Warning message:
small.sav: Unrecognized record type 7, subtype 13 encountered in system
file.
> library(MNP)
MNP: R Package for Fitting the Multinomial Probit Models
Version: 1.3-1
URL: http://www.princeton.edu/~kimai/research/MNP.html
>
>      res1 <- mnp(PREVOTE3 ~ 1, choiceX = list(1=UCLC, 2=UDLC, 3=UPLC),
Error: syntax error
>                  cXnames = "ut", data = small, n.draws = 500, burnin =
100,
Error: syntax error
>                  verbose = TRUE)
Error: syntax error
>
> # another try giving arbitrary names to the values of the dependent
variable
>
>      res1 <- mnp(PREVOTE3 ~ 1, choiceX = list(Clinton=UCLC, Dole=UDLC,
Perot=UPLC),
+                  cXnames = "ut", data = small, n.draws = 500, burnin =
100,
+                  verbose = TRUE)

The base category is `1'.

The total number of alternatives is 3.

Error in xmatrix.mnp(formula, data = eval.parent(data), choiceX =
call$choiceX,  :
         Error: Invalid input for `choiceX.'
  Some variables do not exist.
>
> # another try using a string type dependent variable
>
>      res1 <- mnp(PRVOTE3 ~ 1, choiceX = list(Clinton=UCLC, Dole=UDLC,
Perot=UPLC),
+                  cXnames = "ut", data = small, n.draws = 500, burnin =
100,
+                  verbose = TRUE)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
         invalid variable type
>



From wilkening at censix.com  Wed Jul 13 15:15:36 2005
From: wilkening at censix.com (Soren Wilkening)
Date: Wed, 13 Jul 2005 15:15:36 +0200
Subject: [R] Is there a working XML parser for the windows R Version 2.0.1
Message-ID: <42D513F8.9090700@censix.com>

Dear all

the regular XML package does not work correctly with the R 2.0.1 windows 
version.
Can anybody indicate a suitable alternative ?
I need to dynamically read, parse and process a HTML table in R that is 
available at a certain url.

Regards
Soren Wilkening

-- 
CENSIX Consulting
wilkening at censix.com

http://www.censix.com



From ggrothendieck at gmail.com  Wed Jul 13 15:30:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 09:30:06 -0400
Subject: [R] Is there a working XML parser for the windows R Version
	2.0.1
In-Reply-To: <42D513F8.9090700@censix.com>
References: <42D513F8.9090700@censix.com>
Message-ID: <971536df0507130630591d94b4@mail.gmail.com>

On 7/13/05, Soren Wilkening <wilkening at censix.com> wrote:
> Dear all
> 
> the regular XML package does not work correctly with the R 2.0.1 windows
> version.
> Can anybody indicate a suitable alternative ?
> I need to dynamically read, parse and process a HTML table in R that is
> available at a certain url.

Is this a one-time transfer or does the information change and you have
to do it completely automatically on a repeated basis?

In the first case, select the table, copy it to the clipboard, paste it into 
Excel and then transfer it to R from there.

In the second case you could use RDCOMClient or rcom packages to 
get it via Internet Explorer -- although that would be more involved and, 
in particular, requires that you learn the IE COM interface.  There may 
or may not be some discussion in the rcom list archives:
   http://mailman.csd.univie.ac.at/pipermail/rcom-l/
on this approach.



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Jul 13 15:43:26 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 13 Jul 2005 09:43:26 -0400
Subject: [R] Is there a working XML parser for the windows R Version
	2	.0.1
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40D0@us-arlington-0668.mail.saic.com>

I do not know if current XML package suppose to work for "windows R Version
2.0.1"; however, current version of XML works fine for current version of R
(2.1.1). Also, version of XML available when R Version 2.0.1 was current,
worked just fine as well. So the answer might be to update your R version.

My System is:
- R version: R 2.1.1
- Operating System: Win XP
- Compiler: mingw32-gcc-3.4.2

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Soren Wilkening
Sent: Wednesday, July 13, 2005 9:16 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Is there a working XML parser for the windows R Version 2.0.1

Dear all

the regular XML package does not work correctly with the R 2.0.1 windows
version.
Can anybody indicate a suitable alternative ?
I need to dynamically read, parse and process a HTML table in R that is
available at a certain url.

Regards
Soren Wilkening

--
CENSIX Consulting
wilkening at censix.com

http://www.censix.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From j.van_den_hoff at fz-rossendorf.de  Wed Jul 13 16:00:58 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 13 Jul 2005 16:00:58 +0200
Subject: [R] unexpected par('pin') behaviour
Message-ID: <42D51E9A.8070702@fz-rossendorf.de>

hi everybody,

I noticed the following: in one of my scripts 'layout' is used to 
generate a (approx. square) grid of variable dimensions (depending on 
no. of input files). if the no. of subplots (grid cells) becomes 
moderately large  (say > 9) I use a construct like

   ###layout grid computation and set up occurs here###
    ...
   opar <- par(no.readonly = T);
   on.exit(par(opar))
   par(mar=c(4.1, 4.1, 1.1, .1))
   ###plotting occurs here####
    ...

to reduce the figure margins to achieve a more compact display. apart 
from 'mar' no other par() setting is modified.

this works fine until the total number of subplots becomes too large 
("large" depending on the current size of the X11() graphics device 
window, e.g. 7 x 6 subplots for the default size fo x11()).

I then get the error message (only _after_ all plots are correctly 
displayed, i.e. obviously during execution of the above on.exit() call)

Error in par(opar) :
invalid value specified for graphics parameter "pin"


and par("pin") yields:

[1]  0.34864 -0.21419


which indeed is invalid (negative 2nd component).

I'm aware of this note from ?par:

    The effect of restoring all the (settable) graphics parameters as
      in the examples is hard to predict if the device has been resized.
      Several of them are attempting to set the same things in different
      ways, and those last in the alphabet will win.  In particular, the
      settings of 'mai', 'mar', 'pin', 'plt' and 'pty' interact, as do
      the outer margin settings, the figure layout and figure region
      size.


but my problem occurs without any resizing of the x11() window prior to 
resetting par to par(opar).

any ideas, what is going on?

platform powerpc-apple-darwin7.9.0
arch     powerpc
os       darwin7.9.0
system   powerpc, darwin7.9.0
status   Patched
major    2
minor    1.0
year     2005
month    05
day      12
language R

regards,

joerg



From paulojus at est.ufpr.br  Wed Jul 13 16:03:21 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Wed, 13 Jul 2005 11:03:21 -0300 (BRT)
Subject: [R] Rd.sty, Sweave, tex4ht
In-Reply-To: <mailman.8.1121248801.19301.r-devel@r-project.org>
References: <mailman.8.1121248801.19301.r-devel@r-project.org>
Message-ID: <Pine.LNX.4.58L0.0507131052530.4817@est.ufpr.br>

Hi
I'm using Sweave with tex4th to generate xhtml documents
However there seems to be a problem with \Link defined in
Rd.sty, since this is also defined in tex4ht.sty

My workaround was to replace the lines with
\newcommand{\Link} by \providecommand{\Link}
in Rd.sty

I'm therefore wondering whether this is the best solution, and if so
whether this could be changed in the original Rd.sty shipped with R.
A possible inconvenient is that \providecommand seems to be specific to
LaTeX and apparently does not work with TeX

Thanks
P.J.

Paulo Justiniano Ribeiro Jr
LEG (Laborat??rio de Estat??stica e Geoinforma????o)
Departamento de Estat??stica
Universidade Federal do Paran??
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From dusa.adrian at gmail.com  Wed Jul 13 16:08:16 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 13 Jul 2005 17:08:16 +0300
Subject: [R] texture in barplots?
Message-ID: <200507131708.16559.dusa.adrian@gmail.com>


Dear R list,

For some reason I am unable to access  neither search.r-project.org, nor 
http://finzi.psych.upenn.edu/ so I cannot search the archives for a possible 
answer (I Googled for this but didn't find anything).

Is it possible to draw barplots using a texture instead of colors, for a black 
and white printer?

TIA,
Adrian

-- 
Adrian Dusa
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101


-- 
This message was scanned for spam and viruses by BitDefender.
For more information please visit http://linux.bitdefender.com/



From heinz.schild at caselab.info  Wed Jul 13 16:20:47 2005
From: heinz.schild at caselab.info (Heinz Schild)
Date: Wed, 13 Jul 2005 16:20:47 +0200
Subject: [R] Random fractal generator in R ?
Message-ID: <52E9AA78-2685-4A69-9B55-F81AC8CA45B0@caselab.info>

Does one of the packages of R include functions to generate random  
fractals as for instance outlined in http://classes.yale.edu/fractals ?
Heinz Schild



From admin at biostatistic.de  Wed Jul 13 16:36:50 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 13 Jul 2005 16:36:50 +0200
Subject: [R] texture in barplots?
In-Reply-To: <200507131708.16559.dusa.adrian@gmail.com>
References: <200507131708.16559.dusa.adrian@gmail.com>
Message-ID: <42D52702.8060701@biostatistic.de>



Adrian Dusa schrieb:

>Is it possible to draw barplots using a texture instead of colors, for a black 
>and white printer?
>
>  
>
  barplot(height,.....,density=c(4,6,8,10)  ...)

for each bar one number - this example is for a barplot with 4 bars.

with regards
Knut Krueger
http://www.biostatistic.de



From caobg at email.uc.edu  Wed Jul 13 16:49:41 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Wed, 13 Jul 2005 10:49:41 -0400
Subject: [R] any reference to get started clustering
Message-ID: <1e881d5e.1b445d98.8642e00@mirapoint.uc.edu>

Dear All,

  Just start to use the long expected R, my focus will be
doing clustering on microarray data, just wonder, anyone can
show me any references to conquer the steep learning curve?
Thanks!

Best regards,
 Baoqiang Cao



From adi at roda.ro  Wed Jul 13 16:56:26 2005
From: adi at roda.ro (Adrian Dusa)
Date: Wed, 13 Jul 2005 17:56:26 +0300
Subject: [R] texture in barplots?
In-Reply-To: <42D52702.8060701@biostatistic.de>
References: <200507131708.16559.dusa.adrian@gmail.com>
	<42D52702.8060701@biostatistic.de>
Message-ID: <200507131756.26393.adi@roda.ro>

On Wednesday 13 July 2005 17:36, Knut Krueger wrote:
> Adrian Dusa schrieb:
> >Is it possible to draw barplots using a texture instead of colors, for a
> > black and white printer?
>
>   barplot(height,.....,density=c(4,6,8,10)  ...)
>
> for each bar one number - this example is for a barplot with 4 bars.
>
> with regards
> Knut Krueger
> http://www.biostatistic.de

Thank you, I read about density but they only seem to draw diagonal lines 
(differing in the number of lines per inch).
I am looking for different *types* of texture (i.e. maybe I could reverse the 
shading lines, or cross-lines or something like that).

All the best,
Adrian

-- 
Adrian Dusa
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101


-- 
This message was scanned for spam and viruses by BitDefender.
For more information please visit http://linux.bitdefender.com/



From RRoa at fisheries.gov.fk  Wed Jul 13 15:03:53 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 13 Jul 2005 11:03:53 -0200
Subject: [R] nlme, MASS and geoRglm for spatial autocorrelation?
Message-ID: <03DCBBA079F2324786E8715BE538968A068E7B@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Beale, Colin
> Sent: 13 July 2005 10:15
> To: Prof Brian Ripley
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] nlme, MASS and geoRglm for spatial autocorrelation?
> 
> 
> My data are indeed bernoulli and not binomial, as I indicated. The
> dataset consists of points (grid refs) that are either locations of
> events (animals) or random points (with no animal present). For each
> point I have a suite of environmental covariates describing 
> the habitat at this point. I was anticipating some sort of function that 
> could run:
> 
> function(present ~ env1 + env2 + env3 + x + y, correlation =
> corSpher(form=~x+y), family = binomial)
> 
> where env1 to env3 are the habitat covariates, x & y the grid refs. If
> my data were normal, I undertand I would use gls() with exactly this,
> but drop the family requirement. As my data are bernoulli this is
> clearly not possible, but I was hoping the analysis may be analagous?
> The eventual aim is to firstly understand which environmental 
> covariates are important in determining presence and then to use habitat maps to
> identify the areas expected to be most important.

This could be done with geoRglm. I did something similar last week, but without
covariates, only the spatial coordinates (i.e. my spatial process had expectation 
equal to a constant). If you are willing to sacrifice some spatial resolution you 
can create cells in your spatial data (say 100 m x 100 m) and in each cell count 
the number of successes in observing your spatial process and the number of trials. 
This will be a binomial problem and it seems to me to be the spatial equivalent of 
logistic regression where the predictor continuous variable is structured in bins 
and then events are counted in those bins. You can move to the R-sig-geo list
if you have questions about geoRglm
https://stat.ethz.ch/mailman/listinfo/r-sig-geo
Btw, this can also be done in SAS using the glimmix macro.
Ruben



From ramasamy at cancer.org.uk  Wed Jul 13 17:03:26 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 13 Jul 2005 16:03:26 +0100
Subject: [R] any reference to get started clustering
In-Reply-To: <1e881d5e.1b445d98.8642e00@mirapoint.uc.edu>
References: <1e881d5e.1b445d98.8642e00@mirapoint.uc.edu>
Message-ID: <1121267006.5963.6.camel@ipc143004.lif.icnet.uk>

Welcome to R. The learning curve is well worth the benefits.

If you are used to Eisen clustering and other fancy softwares to do
clustering, then you might be a little disappointed with R's clustering
ability of thousands of genes. But then again clustering is an
exploratory tool and I see no reason why it should be the only analysis
that some papers on microarray seem to focus on. My own bias aside,
there are functions called heatmap, hclust that might useful.

You might to check out the documentation and workshop section of
BioConductor (http://www.bioconductor.org/) which have R packages
designed for the analysis of genomic data.

But you should definitely try to read the Introduction to R first
http://cran.r-project.org/doc/manuals/R-intro.html and other documents.

Regards, Adai



On Wed, 2005-07-13 at 10:49 -0400, Baoqiang Cao wrote:
> Dear All,
> 
>   Just start to use the long expected R, my focus will be
> doing clustering on microarray data, just wonder, anyone can
> show me any references to conquer the steep learning curve?
> Thanks!
> 
> Best regards,
>  Baoqiang Cao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From highstat at highstat.com  Wed Jul 13 17:12:47 2005
From: highstat at highstat.com (Highland Statistics Ltd.)
Date: Wed, 13 Jul 2005 16:12:47 +0100
Subject: [R] R and stats courses
Message-ID: <6.1.2.0.0.20050713161015.01cd46f0@pop.highstat.com>



Apologies for cross-posting

-- Final call. There are still 3 places available on each course  --

We would like to announce three statistics courses in and around Aberdeen, UK
Course 1: Regression, GLM, GAM, mixed modelling and tree models
Course 2: Multivariate analysis and multivariate time series analysis
Course 3: An introduction to R



Various modules are part of a EU MSc and UK MSc.

Course 1:
When: Monday 25 July until Friday 29 July 2005
Where: Scottish Agricultural College (SAC), Aberdeen, UK.
Course: "Analysing biological and environmental data using univariate methods".


Course 2:
When: Monday 1 August until Friday 5 August 2005
Where: Newburgh.
Course: "Analysing biological and environmental data using multivariate 
analysis and multivariate time series analysis


Course 3:
"An introduction to R".
When: 29-31 August 2005 (Monday-Wednesday).
Location: The Ythan hotel in Newburgh.
Host: Organised by Highland Statistics Ltd.


Information and registration:  www.brodgar.com/statscourse.htm

Kind regards,

Alain Zuur




Dr. Alain F. Zuur
Highland Statistics Ltd.
6 Laverock road
UK - AB41 6FN Newburgh

Tel: 0044 1358 788177
Email: highstat at highstat.com

Our statistics courses:
1. "Analysing biological and environmental data using univariate and 
multivariate methods".
2. "Analysing biological and environmental data using univariate methods"
3. "Analysing biological and environmental data using multivariate analysis 
and multivariate time series analysis"
4. "An introduction to R"

Brodgar: Software for univariate and multivariate analysis and multivariate 
time series analysis
Brodgar complies with R GNU GPL license

Statistical consultancy, courses, data analysis and software



From admin at biostatistic.de  Wed Jul 13 17:27:17 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 13 Jul 2005 17:27:17 +0200
Subject: [R] texture in barplots?
In-Reply-To: <42D52702.8060701@biostatistic.de>
References: <200507131708.16559.dusa.adrian@gmail.com>
	<42D52702.8060701@biostatistic.de>
Message-ID: <42D532D5.2030201@biostatistic.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/d5959616/attachment.pl

From buser at stat.math.ethz.ch  Wed Jul 13 17:41:13 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 13 Jul 2005 17:41:13 +0200
Subject: [R] Name for factor's levels with contr.sum
In-Reply-To: <ff51f022050713030254f6d6dc@mail.gmail.com>
References: <ff51f022050713030254f6d6dc@mail.gmail.com>
Message-ID: <17109.13849.189519.960440@stat.math.ethz.ch>

Dear Ghislain

I do not know a general elegant solution, but for some
applications the following example may be helpful:

## Artificial data for demonstration: group is fixed, species is random 
dat <- data.frame(group = c(rep("A",20),rep("B",17),rep("C",24)),
                  species = c(rep("sp1", 4), rep("sp2",5),   rep("sp3",5),
                    rep("sp4",6),  rep("sp5",2),  rep("sp6",5),  rep("sp7",3),
                    rep("sp8",3), rep("sp9",4), rep("sp10",6),  rep("sp11",6),
                    rep("sp12",6), rep("sp13",6)),
                  area = rnorm(61))

## You can attach a contrast at your fixed factor of interest "group"
## Create the contrast you like to test (in our case contr.sum for 3
## levels)
mat <- contr.sum(3)
## You can add the names you want to see in the output
## Be carefull that you give the correct names to the concerned
## column. Otherwise there is the big danger of misinterpretation.
colnames(mat) <- c(": A against rest", ": B against rest")
## Attatch the contrast at your factor "group"
dat[,"group"] <- C(dat[,"group"],mat)
## Now calculate the lme
library(nlme)
reg.lme <- lme(area ~ group, data = dat, random = ~ 1|species)
summary(reg.lme)

Maybe someone has a better idea how to do it generally.

Hope this helps

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Ghislain Vieilledent writes:
 > Good morning,
 > 
 > I used in R contr.sum for the contrast in a lme model:
 > 
 > > options(contrasts=c("contr.sum","contr.poly"))
 > > Septo5.lme<-lme(Septo~Variete+DateSemi,Data4.Iso,random=~1|LieuDit)
 > > intervals(Septo5.lme)$fixed
 > lower est. upper
 > (Intercept) 17.0644033 23.106110 29.147816
 > Variete1 9.5819873 17.335324 25.088661
 > Variete2 -3.3794907 6.816101 17.011692
 > Variete3 -0.5636915 8.452890 17.469472
 > Variete4 -22.8923812 -10.914912 1.062558
 > Variete5 -10.7152821 -1.865884 6.983515
 > Variete6 0.2743390 9.492175 18.710012
 > Variete7 -23.7943250 -15.070737 -6.347148
 > Variete8 -21.7310554 -12.380475 -3.029895
 > Variete9 -27.9782575 -17.480555 -6.982852
 > DateSemi1 -5.7903419 -1.547875 2.694592
 > DateSemi2 3.6571596 8.428417 13.199675
 > attr(,"label")
 > [1] "Fixed effects:"
 > 
 > How is it possible to obtain a return with the name of my factor's levels as 
 > with contr.treatment ?
 > 
 > Thanks for you help.
 > 
 > -- 
 > Ghislain Vieilledent
 > 30, rue Bernard Ortet 31 500 TOULOUSE
 > 06 24 62 65 07
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luis.tercero at ebi-wasser.uni-karlsruhe.de  Wed Jul 13 17:50:28 2005
From: luis.tercero at ebi-wasser.uni-karlsruhe.de (Luis Tercero)
Date: Wed, 13 Jul 2005 17:50:28 +0200
Subject: [R] High resolution plots
Message-ID: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>

Dear R-help community,

would any of you have a (preferably simple) example of a 
presentation-quality .png plot, i.e. one that looks like the .eps plots 
generated by R?  I am working with R 2.0.1 in WindowsXP and am having 
similar problems as Knut Krueger in printing high-quality plots.  I have 
looked at the help file and examples therein as well as others I have 
been able to find online but to no avail.  After many many tries I have 
to concede I cannot figure it out.

I would be very grateful for your help.

Regards,

Luis

-- 

Luis Tercero, M.Sc.

Engler-Bunte-Institut der Universit??t Karlsruhe (TH)
Bereich Wasserchemie

Engler-Bunte-Ring 1
D-76131 Karlsruhe



From jnikelski at alumni.uwaterloo.ca  Wed Jul 13 17:47:09 2005
From: jnikelski at alumni.uwaterloo.ca (EJ Nikelski)
Date: Wed, 13 Jul 2005 11:47:09 -0400
Subject: [R] write.foreign, SPSS on Mac OS X
In-Reply-To: <Pine.LNX.4.61.0507130725350.29010@gannet.stats>
References: <42D45733.4080905@alumni.uwaterloo.ca>
	<Pine.LNX.4.61.0507130725350.29010@gannet.stats>
Message-ID: <42D5377D.40008@alumni.uwaterloo.ca>

Hello,

     Thanks for your help Brian. You are correct in assuming that I am 
trying to use write.foreign to export a data frame for use in SPSS, 
using the usual format:

 >write.foreign(df, dataFile, codeFile, package="SPSS")

Your suggestion that the unprintable characters represent UTF-8 encoded 
Unicode left and right double quotes also appears correct. Now, although 
the suggested work-around may well help, the foreign package does seem 
to be creating a corrupted file. That is, an entirely 8-bit ASCII file 
containing embedded UTF-8 double quotes is not valid by any standard -- 
and is thus unreadable by any editor on any platform. Perhaps I should 
look into filing a bug report on this to the foreign package maintainer.

Thanks,

Jim


Prof Brian Ripley wrote:
> On Tue, 12 Jul 2005, EJ Nikelski wrote:
> 
>>     I have jut installed the foreign package (v 0.8-8) on my OS X
>> machine, and have a bit of a problem writing out a data frame in SPSS
>> format. Specifically, the code file (the .sps format file) seems to
>> write 3 unprintable hex values instead of double quotes. For example, in
>> the following output ...
>>
>> VALUE LABELS
>> /
>> immDel
>> 1 ###1###
>>  2 ###2###
>>  3 ###3###
>>
>>  ... emacs tells me that the left-sided ### are the hex codes E2 80 9C,
>> on the right we have E2 80 9D. I am supposing that I should be seeing
>> double-quotes here? Interestingly, the data file, which also contains a
>> quoted field, writes out the quotes without any problem. Does anyone
>> have any ideas?
> 
> 
> An idea. Those are left and right double quotes in UTF-8 and since MacOS X
> is usually in a UTF-8 locale they should be printable.  However, I 
> suspect that SPSS is expecting ASCII double quotation marks.
> 
> You haven't told us what you did, but I guess you used
> write.foreign(package="SPSS").  That calls writeForeignSPSS which 
> contains calls to dQuote(), and the latter are wrong if ASCII quotation 
> marks are needed.
> 
> A quick workaround is to use a non-UTF-8 locale: how you do that on ypur 
> OS depends on how you run R so please ask advice on the R-sig-mac list.
>



From RRoa at fisheries.gov.fk  Wed Jul 13 15:54:51 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 13 Jul 2005 11:54:51 -0200
Subject: [R] Where's iris?
Message-ID: <03DCBBA079F2324786E8715BE538968A068E7C@FIGMAIL-CLUS01.FIG.FK>

Hi:
Where is the iris data set actually
located in the R 2.1.0 folder (under W XP)?
Is it a text file or it is a binary file?
Ruben



From admin at biostatistic.de  Wed Jul 13 17:55:56 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 13 Jul 2005 17:55:56 +0200
Subject: [R] texture in barplots?
In-Reply-To: <42D52702.8060701@biostatistic.de>
References: <200507131708.16559.dusa.adrian@gmail.com>
	<42D52702.8060701@biostatistic.de>
Message-ID: <42D5398C.8050007@biostatistic.de>



Knut Krueger schrieb:

>Adrian Dusa schrieb:
>
>  
>
>>Is it possible to draw barplots using a texture instead of colors, for a black 
>>and white printer?
>>
>> 
>>
>>    
>>
>  barplot(height,.....,density=c(4,6,8,10)  ...)
>
>for each bar one number - this example is for a barplot with 4 bars.
>
>  
>
forgot something
you could also set the angle and the color of the lines
 barplot(height,.....,col=c("blue","blue","blue","green"),density=c(4,6,8,10),angle=c(15,30,60,90), ...)

with regards
Knut Krueger
http://www.biostatistic.de



From ligges at statistik.uni-dortmund.de  Wed Jul 13 18:01:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Jul 2005 18:01:32 +0200
Subject: [R] Where's iris?
In-Reply-To: <03DCBBA079F2324786E8715BE538968A068E7C@FIGMAIL-CLUS01.FIG.FK>
References: <03DCBBA079F2324786E8715BE538968A068E7C@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <42D53ADC.1010705@statistik.uni-dortmund.de>

Ruben Roa wrote:

> Hi:
> Where is the iris data set actually
> located in the R 2.1.0 folder (under W XP)?
> Is it a text file or it is a binary file?

It is a special binary file in package datasets in the binary 
distribution. Just dump() or write.table() on the data to get a text 
representation of the data.

Uwe Ligges





> Ruben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Wed Jul 13 18:01:50 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 13 Jul 2005 09:01:50 -0700
Subject: [R] Where's iris?
In-Reply-To: <03DCBBA079F2324786E8715BE538968A068E7C@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <200507131601.j6DG1orE001136@compton.gene.com>

help.search("iris") tells you.

You should always try R's built-in help resources **before** posting. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ruben Roa
> Sent: Wednesday, July 13, 2005 6:55 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Where's iris?
> 
> Hi:
> Where is the iris data set actually
> located in the R 2.1.0 folder (under W XP)?
> Is it a text file or it is a binary file?
> Ruben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Wed Jul 13 18:13:58 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Jul 2005 09:13:58 -0700
Subject: [R] exact values for p-values
In-Reply-To: <Pine.LNX.4.58.0507131719450.21686@orpheus.qimr.edu.au>
References: <mailman.9.1119261600.23655.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.58.0507131719450.21686@orpheus.qimr.edu.au>
Message-ID: <42D53DC6.6090102@pdf.com>

	  so my chi-square approximation was not very good:

 > pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)
[1] -19775.52
 > (pchisq(39540, 1, lower.tail=FALSE, log.p=TRUE)
+  /log(10))
[1] -8588.398

	  ... roughly 1e-8588.  With a few hours with Abramowitz and Stegun, I 
suspect I could do better.

	  spencer graves

David Duffy wrote:

>>This is obtained from F =39540 with df1 = 1, df2 = 7025.
>>Suppose am interested in exact value such as
>>
> 
> 
> If it were really necessary, you would have to move to multiple
> precision.  The gmp R package doesn't seem to yet cover this, but FMLIB
> (TOMS814, DM Smith) is a multiple precision f90 library that does
> include the incomplete beta -- it allows one to say for F(1,7025)=39540,
> P=6.31E-2886 (evaluated using 200 sign. digit arithmetic).  Results from
> R's pf() agree quite closely with the FMLIB results for less extreme values
> eg
> 
>>print(pf(1500,1,7025,lower=FALSE), digits=20)
> 
>  [1] 1.3702710894887480597e-297
> 
> cf   1.37027108948832580215549799419452388134616261215463681945E-297
> 
> 
> | David Duffy (MBBS PhD)                                         ,-_|\
> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
> | Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mzp3769 at yahoo.com  Wed Jul 13 18:15:33 2005
From: mzp3769 at yahoo.com (m p)
Date: Wed, 13 Jul 2005 09:15:33 -0700 (PDT)
Subject: [R] maps drawing
Message-ID: <20050713161533.58618.qmail@web51006.mail.yahoo.com>

Hello,
is there a package in R that would allow map drawing:
coastlines, country/state boundaries, maybe
topography,
rivers etc?
Thanks for any guidance,
Mark



From ligges at statistik.uni-dortmund.de  Wed Jul 13 18:25:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Jul 2005 18:25:06 +0200
Subject: [R] maps drawing
In-Reply-To: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
References: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
Message-ID: <42D54062.6010405@statistik.uni-dortmund.de>

m p wrote:

> Hello,
> is there a package in R that would allow map drawing:
> coastlines, country/state boundaries, maybe
> topography,
> rivers etc?

What about package maps?

Moreover, what about reading the posting guide and trying to search 
yourself at first. I think it is almost impossible not to find "maps" on 
CRAN in your case.

Uwe Ligges



> Thanks for any guidance,
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Wed Jul 13 18:25:44 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 13 Jul 2005 12:25:44 -0400
Subject: [R] Memory question
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9504@uswpmx00.merck.com>

One way I do this is to use Luke Tierney's "active bindings". I make an
active binding of a name to a function which either loads or saves the
object. Then the name behaves like the R object it's replacing. This works
nicely as long as I don't need lots of random accesses to the matrix.

I'd be happy to send the functions I use to do this.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kenneth Roy Cabrera
Torres
Sent: Wednesday, July 13, 2005 7:14 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Memory question
Importance: High


Hi R users and developers:

I want to know how can I save memory in R
for example:
  - saving on disk a matrix.
  - using again the matrix (changing their values)
  - saving again the matrix on disk in a different file.

The idea is that I have a process that generate several
matrices, but if I keep them all in memory it will overflow.

How can I save them in different files, so I use the same
amount of memory for each processed matrix?

Thank you for your help.

-- 
Kenneth Roy Cabrera Torres
Universidad Nacional de Colombia
Sede Medellin
Tel 430 9351
Cel 315 504 9339

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Wed Jul 13 18:30:05 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 13 Jul 2005 16:30:05 +0000
Subject: [R] maps drawing
In-Reply-To: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
Message-ID: <BAY103-F351894E8156FCCC3BCBE96A6DE0@phx.gbl>

Try RSiteSearch("map") or  help.search("map")

Cheers

Francisco

>From: m p <mzp3769 at yahoo.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] maps drawing
>Date: Wed, 13 Jul 2005 09:15:33 -0700 (PDT)
>
>Hello,
>is there a package in R that would allow map drawing:
>coastlines, country/state boundaries, maybe
>topography,
>rivers etc?
>Thanks for any guidance,
>Mark
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From stephen.cox at ttu.edu  Wed Jul 13 18:42:52 2005
From: stephen.cox at ttu.edu (Stephen B. Cox)
Date: Wed, 13 Jul 2005 11:42:52 -0500
Subject: [R] Fieller's Conf Limits and EC50's
Message-ID: <42D5448C.7000504@ttu.edu>

Folks

I have modified an existing function to calculate 'ec/ld/lc' 50 values 
and their associated Fieller's confidence limits.  It is based on 
EC50.calc (writtien by John Bailer)  - but also borrows from the dose.p 
(MASS) function.  My goal was to make the original EC50.calc function 
flexible with respect to 1) probability at which to calculate the 
expected dose, and 2) the link function.  I would appreciate comments 
about the validity of doing so!  In particular - I want to make sure 
that the confidence limit calculations are still valid when changing the 
link function.

ec.calc<-function(obj,conf.level=.95,p=.5) {

 # calculates confidence interval based upon Fieller's thm.
 # modified version of EC50.calc found in P&B Fig 7.22
 # now allows other link functions, using the calculations
 # found in dose.p (MASS)
 # SBC 19 May 05

        call <- match.call()

         coef = coef(obj)
         vcov = summary.glm(obj)$cov.unscaled
         b0<-coef[1]
         b1<-coef[2]
         var.b0<-vcov[1,1]
         var.b1<-vcov[2,2]
         cov.b0.b1<-vcov[1,2]
         alpha<-1-conf.level
         zalpha.2 <- -qnorm(alpha/2)
         gamma <- zalpha.2^2 * var.b1 / (b1^2)
         eta = family(obj)$linkfun(p)  #based on calcs in V&R's dose.p

         EC50 <- (eta-b0)/b1

         const1 <- (gamma/(1-gamma))*(EC50 + cov.b0.b1/var.b1)

         const2a <- var.b0 + 2*cov.b0.b1*EC50 + var.b1*EC50^2 -
                    gamma*(var.b0 - cov.b0.b1^2/var.b1)

         const2 <- zalpha.2/( (1-gamma)*abs(b1) )*sqrt(const2a)

         LCL <- EC50 + const1 - const2
         UCL <- EC50 + const1 + const2

         conf.pts <- c(LCL,EC50,UCL)
         names(conf.pts) <- c("Lower","EC50","Upper")

         return(conf.pts,conf.level,call=call)
 }


Thanks

Stephen Cox



From gerifalte28 at hotmail.com  Wed Jul 13 18:50:11 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 13 Jul 2005 16:50:11 +0000
Subject: [R] Please help me.....
In-Reply-To: <27004DDE1590B344855CF773E1D019F115BF2A@postino.ifop.cl>
Message-ID: <BAY103-F278170B17DDEA173F78D8DA6DE0@phx.gbl>

Dear Fernando

Please read the posting guide. If you want to get an answer to your question 
you need to be specific about your analysis, and provide examples of the 
data structure and code that you tried and didn't work.


Francisco

------------------------------------------------------------------------------------------------------------------------------------------
Espaol

Estimado Fernando

Por favor lee la gua de publicacin de preguntas en el foro.  Si quieres 
recibir una respuesta tienes que especificar los anlisis que utilizaste y 
dar ejemplos con la estructura de tus datos y el cdigo que no funcion.

Francisco

>From: Fernando Espndola <fernando.espindola at ifop.cl>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Please help me.....
>Date: Tue, 12 Jul 2005 18:42:14 -0400
>
>Hi user R,
>
>I am try to calculate the spectrum function in two time series. But when 
>plot a single serie, the labels in axes x is in the range 0.1 to 0.6 
>(frequency), but when calculate de spectrum with ts.union function, the 
>labels x is in the range 1 to 6. I not understand why change the labels, 
>and not know that is ralationship. Samebody can hel me in this 
>analysis.....
>
>Thank for all
>
>fdo
>
>Fernando Espindola R.
>Division Investigacion Pesquera
>Instituto de Fomento Pesquero
>Blanco 839
>Valparaiso - CHILE
>fono: 32 - 322442
>fernando.espindola at ifop.cl
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Jul 13 18:51:55 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Jul 2005 09:51:55 -0700
Subject: [R] maps drawing
In-Reply-To: <BAY103-F351894E8156FCCC3BCBE96A6DE0@phx.gbl>
References: <BAY103-F351894E8156FCCC3BCBE96A6DE0@phx.gbl>
Message-ID: <42D546AB.7060002@pdf.com>

	  help.search("asdf") only works if you have "asdf" in something that 
is installed.  RSiteSearch("asdf"), on the other hand, works for 
anything in the R archives.  This does NOT include, however, the 
contents of R News, which you can search via http://www.r-project.org/ 
-> Newsletter -> [Table of Contents (all issues)].

	  spencer graves

Francisco J. Zagmutt wrote:

> Try RSiteSearch("map") or  help.search("map")
> 
> Cheers
> 
> Francisco
> 
> 
>>From: m p <mzp3769 at yahoo.com>
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] maps drawing
>>Date: Wed, 13 Jul 2005 09:15:33 -0700 (PDT)
>>
>>Hello,
>>is there a package in R that would allow map drawing:
>>coastlines, country/state boundaries, maybe
>>topography,
>>rivers etc?
>>Thanks for any guidance,
>>Mark
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tlumley at u.washington.edu  Wed Jul 13 18:59:53 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 13 Jul 2005 09:59:53 -0700 (PDT)
Subject: [R] write.foreign, SPSS on Mac OS X
In-Reply-To: <42D5377D.40008@alumni.uwaterloo.ca>
References: <42D45733.4080905@alumni.uwaterloo.ca>
	<Pine.LNX.4.61.0507130725350.29010@gannet.stats>
	<42D5377D.40008@alumni.uwaterloo.ca>
Message-ID: <Pine.A41.4.61b.0507130957370.154042@homer10.u.washington.edu>

On Wed, 13 Jul 2005, EJ Nikelski wrote:
>
> Your suggestion that the unprintable characters represent UTF-8 encoded
> Unicode left and right double quotes also appears correct. Now, although
> the suggested work-around may well help, the foreign package does seem
> to be creating a corrupted file. That is, an entirely 8-bit ASCII file
> containing embedded UTF-8 double quotes is not valid by any standard --
> and is thus unreadable by any editor on any platform. Perhaps I should
> look into filing a bug report on this to the foreign package maintainer.
>

It is a bug and has been fixed, but this isn't the reason. It's a bug 
because the format is wrong for SPSS.  The file is perfectly valid UTF-8: 
all the characters other than the double quotes are 7-bit ASCII and so 
have the same representation in UTF-8.

 	-thomas



From ripley at stats.ox.ac.uk  Wed Jul 13 19:08:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2005 18:08:39 +0100 (BST)
Subject: [R] write.foreign, SPSS on Mac OS X
In-Reply-To: <42D5377D.40008@alumni.uwaterloo.ca>
References: <42D45733.4080905@alumni.uwaterloo.ca>
	<Pine.LNX.4.61.0507130725350.29010@gannet.stats>
	<42D5377D.40008@alumni.uwaterloo.ca>
Message-ID: <Pine.LNX.4.61.0507131804520.8637@gannet.stats>

On Wed, 13 Jul 2005, EJ Nikelski wrote:

> Hello,
>
>     Thanks for your help Brian. You are correct in assuming that I am
> trying to use write.foreign to export a data frame for use in SPSS,
> using the usual format:
>
> >write.foreign(df, dataFile, codeFile, package="SPSS")
>
> Your suggestion that the unprintable characters represent UTF-8 encoded
> Unicode left and right double quotes also appears correct. Now, although
> the suggested work-around may well help, the foreign package does seem
> to be creating a corrupted file. That is, an entirely 8-bit ASCII file
> containing embedded UTF-8 double quotes is not valid by any standard --
> and is thus unreadable by any editor on any platform. Perhaps I should

Not true: any editor in a UTF-8 locale should be able to read a valid 
UTF-8 file, and there seems to be a problem with your OS.  No one said 
this had to be an ASCII file, and it will not be if the labels are not 
ASCII.

BTW, `8-bit ASCII' are mutually exclusive terms in file encodings.

> look into filing a bug report on this to the foreign package maintainer.

Which is R-core, and we are already working on a fix.

> Thanks,
>
> Jim
>
>
> Prof Brian Ripley wrote:
>> On Tue, 12 Jul 2005, EJ Nikelski wrote:
>>
>>>     I have jut installed the foreign package (v 0.8-8) on my OS X
>>> machine, and have a bit of a problem writing out a data frame in SPSS
>>> format. Specifically, the code file (the .sps format file) seems to
>>> write 3 unprintable hex values instead of double quotes. For example, in
>>> the following output ...
>>>
>>> VALUE LABELS
>>> /
>>> immDel
>>> 1 ###1###
>>>  2 ###2###
>>>  3 ###3###
>>>
>>>  ... emacs tells me that the left-sided ### are the hex codes E2 80 9C,
>>> on the right we have E2 80 9D. I am supposing that I should be seeing
>>> double-quotes here? Interestingly, the data file, which also contains a
>>> quoted field, writes out the quotes without any problem. Does anyone
>>> have any ideas?
>>
>>
>> An idea. Those are left and right double quotes in UTF-8 and since MacOS X
>> is usually in a UTF-8 locale they should be printable.  However, I
>> suspect that SPSS is expecting ASCII double quotation marks.
>>
>> You haven't told us what you did, but I guess you used
>> write.foreign(package="SPSS").  That calls writeForeignSPSS which
>> contains calls to dQuote(), and the latter are wrong if ASCII quotation
>> marks are needed.
>>
>> A quick workaround is to use a non-UTF-8 locale: how you do that on ypur
>> OS depends on how you run R so please ask advice on the R-sig-mac list.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From greg.snow at ihc.com  Wed Jul 13 19:12:21 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Wed, 13 Jul 2005 11:12:21 -0600
Subject: [R] How to use the function "plot"  as Matlab?
Message-ID: <s2d4f72a.024@lp-msg1.co.ihc.com>

>>> Ted Harding <Ted.Harding at nessie.mcc.ac.uk> 07/13/05 02:12AM >>>

[snip]

>>  I'm not sufficiently acquainted with the internals of "plot"
>>  and friends to anticipate the answer to this question; but,
>>  anyway, the question is:
>>  
>>    Is it feasible to include, as a parameter to "plot", "lines"
>>    and "points",
>>  
>>      rescale=FALSE
>>  
>>    where this default value would maintain the existing behaviour
>>    of these functions, while setting
>>  
>>      rescale=TRUE
>>  
>>    would allow each succeeding plot, adding graphs using "points"
>>    or "lines", to be rescaled (as in Matlab/Octave) so as to
>>    include the entirety of each successive graph?

I tried editing the range in the result from "recordPlot" and it
crashed
R on my system, so it probably is not trivial to rescale an existing
plot
on the standard devices.  Part of the issue is what information is
saved
when the plot is made and what is recomputed each time.  Apparently
octave/matlab and R do this quite differently.

Others have suggested using matplot. you can also manually save all
the relevent information yourself to redo the plots.  

The other option is to use a different graphics device that supports
rescaling.  One option is "rgl" using the rgl package and the 
rgl.lines function (it will auto rescale, but seems overkill for this
case).

Another option is to go a similar route to octave and to have gnuplot
do the actual plotting (and keep the info to rescale when needed).

Below are some functions I wrote for passing the data to gnuplot
(I am working on windows and downloaded the win32 version of 
gnuplot from http://www.gnuplot.info).  Some editing may be 
neccessary for these to work on other systems.  If there is interest
I may debug and expand these functions and include them in a 
package.

To do the original example with these functions:

gp.open()
x <- seq(0,2,0.1)
gp.plot(x,sin(x), type='l')
gp.plot(x,1.5*cos(x), type='l', add=T)
gp.send('set yrange [-1.6:1.6]') # force my own range
gp.send()
gp.send('set yrange [*:*]') # return to autoscaling
gp.send()
gp.close()


The function definitions are:

gp.open <- function(where='c:/progra~1/GnuPlot/bin/pgnuplot.exe'){
	.gp <<- pipe(where,'w')
	.gp.tempfiles <<- character(0)
	invisible(.gp)
}


gp.close <- function(pipe=.gp){
	cat("quit\n",file=pipe)
	close(pipe)
	if(exists('.gp.tempfiles')){
		unlink(.gp.tempfiles)
		rm(.gp.tempfiles,pos=1)
	}
	rm(.gp,pos=1)
	invisible()
}

gp.send <- function(cmd='replot',pipe=.gp){
	cat(cmd, file=pipe)
	cat("\n",file=pipe)
	invisible()
}

gp.plot <- function(x,y,type='p',add=F, title=deparse(substitute(y)), 
		pipe=.gp){
	tmp <- tempfile()
	.gp.tempfiles <<- c(.gp.tempfiles, tmp)

	write.table( cbind(x,y), tmp, row.names=FALSE, col.names=FALSE
)
	w <- ifelse(type=='p', 'points', 'lines')
	r <- ifelse(add, 'replot', 'plot')

	cat( paste(r," '",tmp,"' with ",w," title
'",title,"'\n",sep=''), 
		file=pipe)
	invisible()
}

Hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111



From petzoldt at rcs.urz.tu-dresden.de  Wed Jul 13 19:11:17 2005
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 13 Jul 2005 19:11:17 +0200
Subject: [R] Help with Mahalanobis
In-Reply-To: <42CEBE7B.9020309@terra.com.br>
References: <42CEBE7B.9020309@terra.com.br>
Message-ID: <42D54B35.2090307@rcs.urz.tu-dresden.de>

Hello,

a proposed solution of Bill Venables is archieved on the S-News mailing
list:

http://www.biostat.wustl.edu/archives/html/s-news/2001-07/msg00035.html

and if I remember it correctly (and if the variance matrix is estimated
from the data), another similar way is simply to use the Euclidean
distance of rescaled scores of a pricipal component analysis, e.g.:

data(iris)

dat <- iris[1:4] # without the species names

z <- svd(scale(dat, scale=FALSE))$u
cl <- hclust(dist(z), method="ward")
plot(cl, labels=iris$Species)

#### or alternatively: ####

pc <- princomp(dat, cor=FALSE)

pcdata <- as.data.frame(scale(pc$scores))
cl <- hclust(dist(pcdata), method="ward")
plot(cl, labels=iris$Species)


Hope it helps!

Thomas P.



From kimai at Princeton.Edu  Wed Jul 13 19:49:33 2005
From: kimai at Princeton.Edu (Kosuke Imai)
Date: Wed, 13 Jul 2005 13:49:33 -0400 (EDT)
Subject: [R] problems with MNP
Message-ID: <Pine.LNX.4.44.0507131347300.11397-100000@wws-6qcbw21.Princeton.EDU>

Hi Joan,
  You need to do:
                                                                                      
 res1 <- mnp(PREVOTE3 ~ 1, choiceX = list("1"=UCLC, "2"=UDLC, "3"=UPLC),
             cXnames = "ut", data = small, verbose = TRUE)
                                                                                      
The quotations for "1" etc. are necessary because those are names of the
list elements. Also, you should check the convergence of the Markov chain
as 500 draws is typically not. In our Journal of Statistical Software
paper (available at http://www.princeton.edu/~kimai/research/MNP.html), we
show how to do this through some examples.

Best,
Kosuke
                                                                                      
---------------------------------------------------------
Kosuke Imai               Office: Corwin Hall 041
Assistant Professor       Phone: 609-258-6601
Department of Politics    eFax:  973-556-1929
Princeton University      Email: kimai at Princeton.Edu
Princeton, NJ 08544-1012  http://www.princeton.edu/~kimai
---------------------------------------------------------

> > From: Joan Serra <jserra at uchicago.edu>
> > Date: July 13, 2005 6:14:31 AM PDT
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] problems with MNP
> >
> >
> > Hi all,
> >
> > Does anybody have a hint on what may be going wrong in this R code? I
> > mimic the sample code from the MNP developers but I seem unable to  
> > get the
> > choice specific variables right.
> >
> > Thanks,
> > Joan Serra
> >
> >
> >> rm(list=ls())
> >> library(foreign)
> >> small<-read.spss("small.sav")
> >>
> > Warning message:
> > small.sav: Unrecognized record type 7, subtype 13 encountered in  
> > system
> > file.
> >
> >> library(MNP)
> >>
> > MNP: R Package for Fitting the Multinomial Probit Models
> > Version: 1.3-1
> > URL: http://www.princeton.edu/~kimai/research/MNP.html
> >
> >>
> >>      res1 <- mnp(PREVOTE3 ~ 1, choiceX = list(1=UCLC, 2=UDLC,  
> >> 3=UPLC),
> >>
> > Error: syntax error
> >
> >>                  cXnames = "ut", data = small, n.draws = 500,  
> >> burnin =
> >>
> > 100,
> > Error: syntax error
> >
> >>                  verbose = TRUE)
> >>
> > Error: syntax error
> >
> >>
> >> # another try giving arbitrary names to the values of the dependent
> >>
> > variable
> >
> >>
> >>      res1 <- mnp(PREVOTE3 ~ 1, choiceX = list(Clinton=UCLC,  
> >> Dole=UDLC,
> >>
> > Perot=UPLC),
> > +                  cXnames = "ut", data = small, n.draws = 500,  
> > burnin =
> > 100,
> > +                  verbose = TRUE)
> >
> > The base category is `1'.
> >
> > The total number of alternatives is 3.
> >
> > Error in xmatrix.mnp(formula, data = eval.parent(data), choiceX =
> > call$choiceX,  :
> >          Error: Invalid input for `choiceX.'
> >   Some variables do not exist.
> >
> >>
> >> # another try using a string type dependent variable
> >>
> >>      res1 <- mnp(PRVOTE3 ~ 1, choiceX = list(Clinton=UCLC, Dole=UDLC,
> >>
> > Perot=UPLC),
> > +                  cXnames = "ut", data = small, n.draws = 500,  
> > burnin =
> > 100,
> > +                  verbose = TRUE)
> > Error in model.frame(formula, rownames, variables, varnames, extras,
> > extranames,  :
> >          invalid variable type
> >
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting- 
> > guide.html
> >
> 
> 
> --
> Andrew D. Martin, Ph.D.
> Associate Professor of Political Science
> Director, Program in Applied Statistics and Computation
> Professor of Law (by courtesy)
> Washington University in St. Louis
> 
> (314) 935-5863 (Office)
> (314) 753-8377 (Cell)
> (314) 935-5856 (Fax)
> 
> Office: Eliot Hall 326
> Email: admartin at wustl.edu
> WWW:   http://adm.wustl.edu
> 
> 
>



From choid at ohsu.edu  Wed Jul 13 20:34:54 2005
From: choid at ohsu.edu (Dongseok Choi)
Date: Wed, 13 Jul 2005 11:34:54 -0700
Subject: [R] How to increase memory for R on Soliars 10 with 16GB and
 64bit R
Message-ID: <s2d4fc6b.066@OHSU.EDU>

Thank you very much for your help!!
Now, it runs without any problem.

Is it going to be fixed in the next release?

Thanks again,
Dongseok




Dongseok Choi, Ph.D.
Assistant Professor
Division of Biostatistics
Department of Public Health & Preventive Medicine
Oregon Health & Science University
3181 SW Sam Jackson Park Road, CB-669
Portland, OR 97239-3098
TEL) 503-494-5336
FAX) 503-494-4981
choid at ohsu.edu

>>> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> 07/13/05 12:03 AM >>>
On Tue, 12 Jul 2005, Dongseok Choi wrote:

>  My machine is SUN Java Workstation 2100 with 2 AMD Opteron CPUs and 16GB RAM.
>  R is compiled as 64bit by using SUN compilers.
>  I trying to fit quantile smoothing on my data and I got an message as below.
>
>> fit1<-rqss(z1~qss(cbind(x,y),lambda=la1),tau=t1)
> Error in as.matrix.csr(diag(n)) :
cannot allocate memory block of size 2496135168
>
>  The lengths of vector x and y are both 17664.
>  I tried and found that the same command ran with x[1:16008] and y[1:16008].
>  So, it looks to me a memory related problem, but I'm not sure how I can allocate memory block.
>   I read the command line option but not sure what do to with it.
>   Could you help me on this?

It is trying to allocate a single memory block of size over 2^31-1 bytes. 
R internally uses ints for sizes of vectors and that is a limit (see 
help("Memory-limits") ).  However, it is intended that on 64-bit systems 
that there is a limit here of 8*(2^31-1) but there was a typo.  Please 
change line 1534 of src/main/memory.c to

#if SIZEOF_LONG > 4

and re-compile.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kerryrekky at yahoo.com  Wed Jul 13 20:37:29 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Wed, 13 Jul 2005 11:37:29 -0700 (PDT)
Subject: [R] plot the number of replicates at the same point
Message-ID: <20050713183729.85172.qmail@web51805.mail.yahoo.com>

Dear R-helper,
  I want to plot the following-like data:

x y
1 1
1 1
1 2
1 3
1 3
1 4
......

In the plot that produced, I don't want to show the
usual circles or points. Instead, I want to show the
number of replicates at that point. e.g. at the
position of (1,1), there are 2 obsevations, so a
number '2' will be displayed in the plot.
Is my narrative clear? Is there a way to make the plot
in R?



From iidn01 at yahoo.com  Wed Jul 13 21:27:34 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Wed, 13 Jul 2005 12:27:34 -0700 (PDT)
Subject: [R] convert to chron objects
Message-ID: <20050713192734.80886.qmail@web31107.mail.mud.yahoo.com>

Hi,

I have a column of a dataframe which has time stamps
like:

> eh$t[1]
[1] 06/05/2005 01:15:25

and was wondering how to convert it to chron variable.
Thanks a lot.

Young.



From jeaneid at chass.utoronto.ca  Wed Jul 13 21:44:04 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 13 Jul 2005 15:44:04 -0400
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <20050713183729.85172.qmail@web51805.mail.yahoo.com>
Message-ID: <Pine.SGI.4.40.0507131542140.8483928-100000@origin.chass.utoronto.ca>

You can do the following (don't know it this is the most efficient way but
it works)

temp<-read.table("your file to read the data", header=T)
temp1<-table(temp)
plot(temp$x, temp$y, cex=0)
text(as.numeric(rownames(temp1)), as.numeric(colnames(temp1)), temp1)

HTH


On Wed, 13 Jul 2005, Kerry Bush wrote:

> Dear R-helper,
>   I want to plot the following-like data:
>
> x y
> 1 1
> 1 1
> 1 2
> 1 3
> 1 3
> 1 4
> ......
>
> In the plot that produced, I don't want to show the
> usual circles or points. Instead, I want to show the
> number of replicates at that point. e.g. at the
> position of (1,1), there are 2 obsevations, so a
> number '2' will be displayed in the plot.
> Is my narrative clear? Is there a way to make the plot
> in R?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From talih at xena.hunter.cuny.edu  Wed Jul 13 21:27:19 2005
From: talih at xena.hunter.cuny.edu (Makram Talih)
Date: Wed, 13 Jul 2005 15:27:19 -0400 (EDT)
Subject: [R] Efficient testing for +ve definiteness
Message-ID: <Pine.GSO.4.58.0507131517280.11232@xena.hunter.cuny.edu>

Dear R-users,

Is there a preferred method for testing whether a real symmetric matrix is
positive definite? [modulo machine rounding errors.]

The obvious way of computing eigenvalues via "E <- eigen(A, symmetric=T,
only.values=T)$values" and returning the result of "!any(E <= 0)" seems
less efficient than going through the LU decomposition invoked in
"determinant.matrix(A)" and checking the sign and (log) modulus of the
determinant.

I suppose this has to do with the underlying C routines. Any thoughts or
anecdotes?

Many Thanks,

Makram Talih

--
Makram Talih, Ph.D.
Assistant Professor
Department of Mathematics and Statistics
Hunter College of the City University of New York
695 Park Avenue, Room 905 HE
New York, NY 10021

Website: http://stat.hunter.cuny.edu/talih
E-mail: makram.talih at hunter.cuny.edu
Tel: 212-772-5308
Fax: 212-772-4858



From helprhelp at gmail.com  Wed Jul 13 22:38:43 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 13 Jul 2005 15:38:43 -0500
Subject: [R] read.table
Message-ID: <cdf817830507131338d1e4d8f@mail.gmail.com>

Hi,
I have a question on read.table.

I have a dataset with 273,000 lines and 195 columns. I used the
read.table to load the data into R:
trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
I found it takes forever.

then I run 1/10 of the data (test) using read.table again. And this
time it finished quickly. So, there might be something wrong in my
data format causing that problem.

then, my question is, is there a way in R to track at which line,
something wrong occurs?

Thanks,

Weiwei


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From helprhelp at gmail.com  Wed Jul 13 22:51:43 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 13 Jul 2005 15:51:43 -0500
Subject: [R] read.table
In-Reply-To: <cdf817830507131338d1e4d8f@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
Message-ID: <cdf81783050713135171a2008c@mail.gmail.com>

add:
I used
trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529, ncol=195)

it is done. 
so it seems that I just have no patience to wait for half an hour :)

but i still have that question:
is there a way to track the process if it takes too long. Could we
stop in the middle to see at which line it "hesitates" to move on?

regards,

weiwei


On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Hi,
> I have a question on read.table.
> 
> I have a dataset with 273,000 lines and 195 columns. I used the
> read.table to load the data into R:
> trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> I found it takes forever.
> 
> then I run 1/10 of the data (test) using read.table again. And this
> time it finished quickly. So, there might be something wrong in my
> data format causing that problem.
> 
> then, my question is, is there a way in R to track at which line,
> something wrong occurs?
> 
> Thanks,
> 
> Weiwei
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From ggrothendieck at gmail.com  Wed Jul 13 23:04:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:04:05 -0400
Subject: [R] convert to chron objects
In-Reply-To: <20050713192734.80886.qmail@web31107.mail.mud.yahoo.com>
References: <20050713192734.80886.qmail@web31107.mail.mud.yahoo.com>
Message-ID: <971536df050713140430dd12e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/48408880/attachment.pl

From ggrothendieck at gmail.com  Wed Jul 13 23:05:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:05:04 -0400
Subject: [R] read.table
In-Reply-To: <cdf81783050713135171a2008c@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
Message-ID: <971536df0507131405658cee9c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/87fbc350/attachment.pl

From ggrothendieck at gmail.com  Wed Jul 13 23:06:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:06:12 -0400
Subject: [R] High resolution plots
In-Reply-To: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
Message-ID: <971536df0507131406564f7700@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050713/07c6c6fd/attachment.pl

From ggrothendieck at gmail.com  Wed Jul 13 23:08:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:08:51 -0400
Subject: [R] read.table
In-Reply-To: <971536df0507131405658cee9c@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
Message-ID: <971536df0507131408fa27acf@mail.gmail.com>

[I had some email problems and am sending this again.  Sorry
if you get it twice.]

You could use the nlines= argument to scan to read in a 
portion at a time. 
 
 
> 
> 
> On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote: 
> > add:
> > I used
> > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529, ncol=195)
> > 
> > it is done.
> > so it seems that I just have no patience to wait for half an hour :)
> > 
> > but i still have that question:
> > is there a way to track the process if it takes too long. Could we
> > stop in the middle to see at which line it "hesitates" to move on? 
> > 
> > regards,
> > 
> > weiwei
> > 
> > 
> > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > Hi,
> > > I have a question on read.table.
> > >
> > > I have a dataset with 273,000 lines and 195 columns. I used the 
> > > read.table to load the data into R:
> > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > I found it takes forever.
> > >
> > > then I run 1/10 of the data (test) using read.table again. And this
> > > time it finished quickly. So, there might be something wrong in my
> > > data format causing that problem.
> > >
> > > then, my question is, is there a way in R to track at which line,
> > > something wrong occurs? 
> > >
> > > Thanks,
> > >
> > > Weiwei
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III 
> > >
> > 
> > 
> > --
> > Weiwei Shi, Ph.D
> > 
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
>



From ggrothendieck at gmail.com  Wed Jul 13 23:10:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:10:51 -0400
Subject: [R] convert to chron objects
In-Reply-To: <971536df050713140430dd12e@mail.gmail.com>
References: <20050713192734.80886.qmail@web31107.mail.mud.yahoo.com>
	<971536df050713140430dd12e@mail.gmail.com>
Message-ID: <971536df05071314105e1b77da@mail.gmail.com>

[I had some emails problems so I am sending this again.  Sorry
if you get it twice.]

On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> 
> On 7/13/05, Young Cho <iidn01 at yahoo.com> wrote: 
> > Hi,
> > 
> > I have a column of a dataframe which has time stamps
> > like:
> > 
> > > eh$t[1]
> > [1] 06/05/2005 01:15:25 
> > 
> > and was wondering how to convert it to chron variable.
> > Thanks a lot.
>  
> 
> 
>  
> 
Try this:
 
# test data frame eh containing a factor variable t
eh <- data.frame(t = c("06/05/2005 01:15:25", "06/07/2005 01:15:25"))
 
# substring converts factor to character and extracts substring
chron(dates = substring(eh$t, 1, 10), times = substring(eh$t, 12))
 
See ?chron for more info.  There is an article on dates in
R News 4/1 and although it does not specifically answer this
question it may be useful with chron and also provides a 
reference to more chron info elsewhere.



From ggrothendieck at gmail.com  Wed Jul 13 23:12:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:12:04 -0400
Subject: [R] High resolution plots
In-Reply-To: <971536df0507131406564f7700@mail.gmail.com>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
	<971536df0507131406564f7700@mail.gmail.com>
Message-ID: <971536df050713141271c95cc0@mail.gmail.com>

[I had some email problems so I am sending this again.
Sorry if you get this twice.]

On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> 
> On 7/13/05, Luis Tercero
> <luis.tercero at ebi-wasser.uni-karlsruhe.de> wrote: 
> > Dear R-help community,
> > 
> > would any of you have a (preferably simple) example of a
> > presentation-quality .png plot, i.e. one that looks like the .eps plots
> > generated by R?  I am working with R 2.0.1 in WindowsXP and am having
> > similar problems as Knut Krueger in printing high-quality plots.  I have
> > looked at the help file and examples therein as well as others I have 
> > been able to find online but to no avail.  After many many tries I have
> > to concede I cannot figure it out.
> > 
> > I would be very grateful for your help.
>  
>  
> 
> 

If you want the highest resolution use a vector format,
not a bitmapped format such as png.   See:

http://maths.newcastle.edu.au/~rking/R/help/04/02/1168.html
 
for some background.



From helprhelp at gmail.com  Wed Jul 13 23:13:10 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 13 Jul 2005 16:13:10 -0500
Subject: [R] read.table
In-Reply-To: <971536df0507131405658cee9c@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
Message-ID: <cdf8178305071314135c847984@mail.gmail.com>

that sort of works for my purpose.

btw, is there a bettter way to get data.frame by passing around
matrix(). Since I could not find data.frame() with nrow or ncol
arguments. so i have to use matrix first and then as.data.frame to
convert it.

is there any other (better) way?

weiwei

On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> You could use the nlines= argument to scan to read in a 
> portion at a time.
> 
> 
>  
> On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote: 
> > 
> > add:
> > I used
> > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529,
> ncol=195)
> > 
> > it is done.
> > so it seems that I just have no patience to wait for half an hour :)
> > 
> > but i still have that question:
> > is there a way to track the process if it takes too long. Could we
> > stop in the middle to see at which line it "hesitates" to move on? 
> > 
> > regards,
> > 
> > weiwei
> > 
> > 
> > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > Hi,
> > > I have a question on read.table.
> > >
> > > I have a dataset with 273,000 lines and 195 columns. I used the 
> > > read.table to load the data into R:
> > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > I found it takes forever.
> > >
> > > then I run 1/10 of the data (test) using read.table again. And this
> > > time it finished quickly. So, there might be something wrong in my
> > > data format causing that problem.
> > >
> > > then, my question is, is there a way in R to track at which line,
> > > something wrong occurs? 
> > >
> > > Thanks,
> > >
> > > Weiwei
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III 
> > >
> > 
> > 
> > --
> > Weiwei Shi, Ph.D
> > 
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> 
>  


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From ggrothendieck at gmail.com  Wed Jul 13 23:18:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:18:01 -0400
Subject: [R] read.table
In-Reply-To: <cdf8178305071314135c847984@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
	<cdf8178305071314135c847984@mail.gmail.com>
Message-ID: <971536df05071314185ba9f6b3@mail.gmail.com>

Maybe you don't really need a data frame in the first place?
You were concerned with speed and matrices tend to 
have higher performance than data frames.

On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> that sort of works for my purpose.
> 
> btw, is there a bettter way to get data.frame by passing around
> matrix(). Since I could not find data.frame() with nrow or ncol
> arguments. so i have to use matrix first and then as.data.frame to
> convert it.
> 
> is there any other (better) way?
> 
> weiwei
> 
> On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > You could use the nlines= argument to scan to read in a
> > portion at a time.
> >
> >
> >
> > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > >
> > > add:
> > > I used
> > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529,
> > ncol=195)
> > >
> > > it is done.
> > > so it seems that I just have no patience to wait for half an hour :)
> > >
> > > but i still have that question:
> > > is there a way to track the process if it takes too long. Could we
> > > stop in the middle to see at which line it "hesitates" to move on?
> > >
> > > regards,
> > >
> > > weiwei
> > >
> > >
> > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > Hi,
> > > > I have a question on read.table.
> > > >
> > > > I have a dataset with 273,000 lines and 195 columns. I used the
> > > > read.table to load the data into R:
> > > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > > I found it takes forever.
> > > >
> > > > then I run 1/10 of the data (test) using read.table again. And this
> > > > time it finished quickly. So, there might be something wrong in my
> > > > data format causing that problem.
> > > >
> > > > then, my question is, is there a way in R to track at which line,
> > > > something wrong occurs?
> > > >
> > > > Thanks,
> > > >
> > > > Weiwei
> > > >
> > > >
> > > > --
> > > > Weiwei Shi, Ph.D
> > > >
> > > > "Did you always know?"
> > > > "No, I did not. But I believed..."
> > > > ---Matrix III
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>



From helprhelp at gmail.com  Wed Jul 13 23:21:42 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 13 Jul 2005 16:21:42 -0500
Subject: [R] read.table
In-Reply-To: <cdf8178305071314135c847984@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
	<cdf8178305071314135c847984@mail.gmail.com>
Message-ID: <cdf8178305071314218b6dd61@mail.gmail.com>

there is another problem since last time i forgot "byrow" :(
> trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529, ncol=195, byrow=T)
Read 53338155 items
Error: cannot allocate vector of size 416704 Kb

please help with this 'simple' reading task.

weiwei

On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> that sort of works for my purpose.
> 
> btw, is there a bettter way to get data.frame by passing around
> matrix(). Since I could not find data.frame() with nrow or ncol
> arguments. so i have to use matrix first and then as.data.frame to
> convert it.
> 
> is there any other (better) way?
> 
> weiwei
> 
> On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > You could use the nlines= argument to scan to read in a
> > portion at a time.
> >
> >
> >
> > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > >
> > > add:
> > > I used
> > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529,
> > ncol=195)
> > >
> > > it is done.
> > > so it seems that I just have no patience to wait for half an hour :)
> > >
> > > but i still have that question:
> > > is there a way to track the process if it takes too long. Could we
> > > stop in the middle to see at which line it "hesitates" to move on?
> > >
> > > regards,
> > >
> > > weiwei
> > >
> > >
> > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > Hi,
> > > > I have a question on read.table.
> > > >
> > > > I have a dataset with 273,000 lines and 195 columns. I used the
> > > > read.table to load the data into R:
> > > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > > I found it takes forever.
> > > >
> > > > then I run 1/10 of the data (test) using read.table again. And this
> > > > time it finished quickly. So, there might be something wrong in my
> > > > data format causing that problem.
> > > >
> > > > then, my question is, is there a way in R to track at which line,
> > > > something wrong occurs?
> > > >
> > > > Thanks,
> > > >
> > > > Weiwei
> > > >
> > > >
> > > > --
> > > > Weiwei Shi, Ph.D
> > > >
> > > > "Did you always know?"
> > > > "No, I did not. But I believed..."
> > > > ---Matrix III
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From ggrothendieck at gmail.com  Wed Jul 13 23:26:43 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:26:43 -0400
Subject: [R] read.table
In-Reply-To: <cdf8178305071314218b6dd61@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
	<cdf8178305071314135c847984@mail.gmail.com>
	<cdf8178305071314218b6dd61@mail.gmail.com>
Message-ID: <971536df050713142686ef7da@mail.gmail.com>

Try reading it into and transposing the matrix afterwards.  Don't know if
that would work but its worth a try.  Actually if you
are having problems read it into a vector, check that its of the required
size, just in case, and then turn it into a matrix and transpose it.


On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> there is another problem since last time i forgot "byrow" :(
> > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529, ncol=195, byrow=T)
> Read 53338155 items
> Error: cannot allocate vector of size 416704 Kb
> 
> please help with this 'simple' reading task.
> 
> weiwei
> 
> On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > that sort of works for my purpose.
> >
> > btw, is there a bettter way to get data.frame by passing around
> > matrix(). Since I could not find data.frame() with nrow or ncol
> > arguments. so i have to use matrix first and then as.data.frame to
> > convert it.
> >
> > is there any other (better) way?
> >
> > weiwei
> >
> > On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > >
> > > You could use the nlines= argument to scan to read in a
> > > portion at a time.
> > >
> > >
> > >
> > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > >
> > > > add:
> > > > I used
> > > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529,
> > > ncol=195)
> > > >
> > > > it is done.
> > > > so it seems that I just have no patience to wait for half an hour :)
> > > >
> > > > but i still have that question:
> > > > is there a way to track the process if it takes too long. Could we
> > > > stop in the middle to see at which line it "hesitates" to move on?
> > > >
> > > > regards,
> > > >
> > > > weiwei
> > > >
> > > >
> > > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > > Hi,
> > > > > I have a question on read.table.
> > > > >
> > > > > I have a dataset with 273,000 lines and 195 columns. I used the
> > > > > read.table to load the data into R:
> > > > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > > > I found it takes forever.
> > > > >
> > > > > then I run 1/10 of the data (test) using read.table again. And this
> > > > > time it finished quickly. So, there might be something wrong in my
> > > > > data format causing that problem.
> > > > >
> > > > > then, my question is, is there a way in R to track at which line,
> > > > > something wrong occurs?
> > > > >
> > > > > Thanks,
> > > > >
> > > > > Weiwei
> > > > >
> > > > >
> > > > > --
> > > > > Weiwei Shi, Ph.D
> > > > >
> > > > > "Did you always know?"
> > > > > "No, I did not. But I believed..."
> > > > > ---Matrix III
> > > > >
> > > >
> > > >
> > > > --
> > > > Weiwei Shi, Ph.D
> > > >
> > > > "Did you always know?"
> > > > "No, I did not. But I believed..."
> > > > ---Matrix III
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>



From ggrothendieck at gmail.com  Wed Jul 13 23:29:44 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Jul 2005 17:29:44 -0400
Subject: [R] Where's iris?
In-Reply-To: <03DCBBA079F2324786E8715BE538968A068E7C@FIGMAIL-CLUS01.FIG.FK>
References: <03DCBBA079F2324786E8715BE538968A068E7C@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <971536df05071314293404fbca@mail.gmail.com>

On 7/13/05, Ruben Roa <RRoa at fisheries.gov.fk> wrote:
> Hi:
> Where is the iris data set actually
> located in the R 2.1.0 folder (under W XP)?
> Is it a text file or it is a binary file?
> Ruben

Uwe has already explained how to get it in text
form; however, if you are curious about its original
format in R then its actually stored in iris.R as 
R source code which you can view at:

  https://svn.r-project.org/R/trunk/src/library/datasets/data/iris.R

(or download the entire R source and get it from there).



From helprhelp at gmail.com  Wed Jul 13 23:41:00 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 13 Jul 2005 16:41:00 -0500
Subject: [R] read.table
In-Reply-To: <971536df050713142686ef7da@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
	<cdf8178305071314135c847984@mail.gmail.com>
	<cdf8178305071314218b6dd61@mail.gmail.com>
	<971536df050713142686ef7da@mail.gmail.com>
Message-ID: <cdf81783050713144128a70391@mail.gmail.com>

i think what you meant is
> trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=195, ncol=273529)
and then transpose it. However:
    Error: cannot allocate vector of size 512000 Kb

the answer is no :(

I think i am going to write my own function to split the result from
scan but not sure if it can be made into matrix or not even if I
succeed.


On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try reading it into and transposing the matrix afterwards.  Don't know if
> that would work but its worth a try.  Actually if you
> are having problems read it into a vector, check that its of the required
> size, just in case, and then turn it into a matrix and transpose it.
> 
> 
> On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > there is another problem since last time i forgot "byrow" :(
> > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529, ncol=195, byrow=T)
> > Read 53338155 items
> > Error: cannot allocate vector of size 416704 Kb
> >
> > please help with this 'simple' reading task.
> >
> > weiwei
> >
> > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > that sort of works for my purpose.
> > >
> > > btw, is there a bettter way to get data.frame by passing around
> > > matrix(). Since I could not find data.frame() with nrow or ncol
> > > arguments. so i have to use matrix first and then as.data.frame to
> > > convert it.
> > >
> > > is there any other (better) way?
> > >
> > > weiwei
> > >
> > > On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > >
> > > > You could use the nlines= argument to scan to read in a
> > > > portion at a time.
> > > >
> > > >
> > > >
> > > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > >
> > > > > add:
> > > > > I used
> > > > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529,
> > > > ncol=195)
> > > > >
> > > > > it is done.
> > > > > so it seems that I just have no patience to wait for half an hour :)
> > > > >
> > > > > but i still have that question:
> > > > > is there a way to track the process if it takes too long. Could we
> > > > > stop in the middle to see at which line it "hesitates" to move on?
> > > > >
> > > > > regards,
> > > > >
> > > > > weiwei
> > > > >
> > > > >
> > > > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > > > Hi,
> > > > > > I have a question on read.table.
> > > > > >
> > > > > > I have a dataset with 273,000 lines and 195 columns. I used the
> > > > > > read.table to load the data into R:
> > > > > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > > > > I found it takes forever.
> > > > > >
> > > > > > then I run 1/10 of the data (test) using read.table again. And this
> > > > > > time it finished quickly. So, there might be something wrong in my
> > > > > > data format causing that problem.
> > > > > >
> > > > > > then, my question is, is there a way in R to track at which line,
> > > > > > something wrong occurs?
> > > > > >
> > > > > > Thanks,
> > > > > >
> > > > > > Weiwei
> > > > > >
> > > > > >
> > > > > > --
> > > > > > Weiwei Shi, Ph.D
> > > > > >
> > > > > > "Did you always know?"
> > > > > > "No, I did not. But I believed..."
> > > > > > ---Matrix III
> > > > > >
> > > > >
> > > > >
> > > > > --
> > > > > Weiwei Shi, Ph.D
> > > > >
> > > > > "Did you always know?"
> > > > > "No, I did not. But I believed..."
> > > > > ---Matrix III
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > > >
> > > >
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From p.dalgaard at biostat.ku.dk  Wed Jul 13 23:51:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jul 2005 23:51:02 +0200
Subject: [R] texture in barplots?
In-Reply-To: <200507131756.26393.adi@roda.ro>
References: <200507131708.16559.dusa.adrian@gmail.com>
	<42D52702.8060701@biostatistic.de> <200507131756.26393.adi@roda.ro>
Message-ID: <x2pstmiddl.fsf@turmalin.kubism.ku.dk>

Adrian Dusa <adi at roda.ro> writes:

> On Wednesday 13 July 2005 17:36, Knut Krueger wrote:
> > Adrian Dusa schrieb:
> > >Is it possible to draw barplots using a texture instead of colors, for a
> > > black and white printer?
> >
> >   barplot(height,.....,density=c(4,6,8,10)  ...)
> >
> > for each bar one number - this example is for a barplot with 4 bars.
> >
> > with regards
> > Knut Krueger
> > http://www.biostatistic.de
> 
> Thank you, I read about density but they only seem to draw diagonal lines 
> (differing in the number of lines per inch).
> I am looking for different *types* of texture (i.e. maybe I could reverse the 
> shading lines, or cross-lines or something like that).


This comes up every now and then, and while it seems that everyone
thinks fill patterns would be nice to have, I suspect that every
attempt to actually implement it have gotten killed in infancy. 

The thing that is tricky to design right is the cross-device issues.
Only some devices support this at all, and when they do, the patterns
tend to be device dependent too. Probably not impossible -- there are
other bits of the device drivers that deal with missing capabilities,
like string rotation and clipping -- just, well, tricky.

 
> All the best,
> Adrian
> 
> -- 
> Adrian Dusa
> Arhiva Romana de Date Sociale
> Bd. Schitu Magureanu nr.1
> Tel./Fax: +40 21 3126618 \
>               +40 21 3120210 / int.101

Um... Romania, I suppose? What city?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From helprhelp at gmail.com  Wed Jul 13 23:52:05 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 13 Jul 2005 16:52:05 -0500
Subject: [R] read.table
In-Reply-To: <cdf81783050713144128a70391@mail.gmail.com>
References: <cdf817830507131338d1e4d8f@mail.gmail.com>
	<cdf81783050713135171a2008c@mail.gmail.com>
	<971536df0507131405658cee9c@mail.gmail.com>
	<cdf8178305071314135c847984@mail.gmail.com>
	<cdf8178305071314218b6dd61@mail.gmail.com>
	<971536df050713142686ef7da@mail.gmail.com>
	<cdf81783050713144128a70391@mail.gmail.com>
Message-ID: <cdf8178305071314522dc4a7fa@mail.gmail.com>

Sorry for last post. 
I don't know why i got the error message last time.
but if i did in the following way:
t<-scan('train1.dat',  sep='|', na.string='.')
t2<-matrix(t, nrow=195, ncol=273529)
t3<-t(t2)
t4<-as.data.frame(t3)

now I got what i needed.

Thanks a lot for Gabor's prompt help.

weiwei

On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> i think what you meant is
> > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=195, ncol=273529)
> and then transpose it. However:
>     Error: cannot allocate vector of size 512000 Kb
> 
> the answer is no :(
> 
> I think i am going to write my own function to split the result from
> scan but not sure if it can be made into matrix or not even if I
> succeed.
> 
> 
> On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Try reading it into and transposing the matrix afterwards.  Don't know if
> > that would work but its worth a try.  Actually if you
> > are having problems read it into a vector, check that its of the required
> > size, just in case, and then turn it into a matrix and transpose it.
> >
> >
> > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > there is another problem since last time i forgot "byrow" :(
> > > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529, ncol=195, byrow=T)
> > > Read 53338155 items
> > > Error: cannot allocate vector of size 416704 Kb
> > >
> > > please help with this 'simple' reading task.
> > >
> > > weiwei
> > >
> > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > that sort of works for my purpose.
> > > >
> > > > btw, is there a bettter way to get data.frame by passing around
> > > > matrix(). Since I could not find data.frame() with nrow or ncol
> > > > arguments. so i have to use matrix first and then as.data.frame to
> > > > convert it.
> > > >
> > > > is there any other (better) way?
> > > >
> > > > weiwei
> > > >
> > > > On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > >
> > > > > You could use the nlines= argument to scan to read in a
> > > > > portion at a time.
> > > > >
> > > > >
> > > > >
> > > > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > > >
> > > > > > add:
> > > > > > I used
> > > > > > trn<-matrix(scan('train1.dat',  sep='|', na.string='.'), nrow=273529,
> > > > > ncol=195)
> > > > > >
> > > > > > it is done.
> > > > > > so it seems that I just have no patience to wait for half an hour :)
> > > > > >
> > > > > > but i still have that question:
> > > > > > is there a way to track the process if it takes too long. Could we
> > > > > > stop in the middle to see at which line it "hesitates" to move on?
> > > > > >
> > > > > > regards,
> > > > > >
> > > > > > weiwei
> > > > > >
> > > > > >
> > > > > > On 7/13/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > > > > Hi,
> > > > > > > I have a question on read.table.
> > > > > > >
> > > > > > > I have a dataset with 273,000 lines and 195 columns. I used the
> > > > > > > read.table to load the data into R:
> > > > > > > trn<-read.table('train1.dat', header=F, sep='|', na.strings='.')
> > > > > > > I found it takes forever.
> > > > > > >
> > > > > > > then I run 1/10 of the data (test) using read.table again. And this
> > > > > > > time it finished quickly. So, there might be something wrong in my
> > > > > > > data format causing that problem.
> > > > > > >
> > > > > > > then, my question is, is there a way in R to track at which line,
> > > > > > > something wrong occurs?
> > > > > > >
> > > > > > > Thanks,
> > > > > > >
> > > > > > > Weiwei
> > > > > > >
> > > > > > >
> > > > > > > --
> > > > > > > Weiwei Shi, Ph.D
> > > > > > >
> > > > > > > "Did you always know?"
> > > > > > > "No, I did not. But I believed..."
> > > > > > > ---Matrix III
> > > > > > >
> > > > > >
> > > > > >
> > > > > > --
> > > > > > Weiwei Shi, Ph.D
> > > > > >
> > > > > > "Did you always know?"
> > > > > > "No, I did not. But I believed..."
> > > > > > ---Matrix III
> > > > > >
> > > > > > ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide!
> > > > > http://www.R-project.org/posting-guide.html
> > > > > >
> > > > >
> > > > >
> > > >
> > > >
> > > > --
> > > > Weiwei Shi, Ph.D
> > > >
> > > > "Did you always know?"
> > > > "No, I did not. But I believed..."
> > > > ---Matrix III
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> >
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From p.dalgaard at biostat.ku.dk  Thu Jul 14 00:21:57 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jul 2005 00:21:57 +0200
Subject: [R] High resolution plots
In-Reply-To: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
Message-ID: <x2ll4aiby2.fsf@turmalin.kubism.ku.dk>

Luis Tercero <luis.tercero at ebi-wasser.uni-karlsruhe.de> writes:

> Dear R-help community,
> 
> would any of you have a (preferably simple) example of a 
> presentation-quality .png plot, i.e. one that looks like the .eps plots 
> generated by R?  I am working with R 2.0.1 in WindowsXP and am having 
> similar problems as Knut Krueger in printing high-quality plots.  I have 
> looked at the help file and examples therein as well as others I have 
> been able to find online but to no avail.  After many many tries I have 
> to concede I cannot figure it out.
> 
> I would be very grateful for your help.

What is the real issue here? Import trouble? If you're importing to
Word/PowerPoint, why not use the Windows metafile? Perhaps they are
too ugly compared to EPS by your taste?

Bitmapped formats are a pain to deal with in general. In principle,
you could just crank up the resolution to (say) 600 dpi, but if your
software rescales the image even slightly, things look horrible. 

For web graphics, I found a reasonably working solution by plotting at
a higher resolution than you need, then smoothing the image slightly
and finally rescaling it. This method is archived at
http://tolstoy.newcastle.edu.au/R/help/04/06/1094.html, but it does
need some unixy tools like the pnm toolchain. I wouldn't know if there
are Windows versions of those.



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From laboissiere at cbs.mpg.de  Thu Jul 14 00:44:44 2005
From: laboissiere at cbs.mpg.de (Rafael Laboissiere)
Date: Thu, 14 Jul 2005 00:44:44 +0200
Subject: [R] Proportion test in three-chices experiment
Message-ID: <20050713224444.GC2885@laboiss2>

Hi,

I wish to analyze with R the results of a perception experiment in which
subjects had to recognize each stimulus among three choices (this was a
forced-choice design).  The experiment runs under two different
conditions and the data is like the following:

   N1 : count of trials in condition 1
   p11, p12, p13: proportions of choices 1, 2, and 3 in condition 1
   
   N2 : count of trials in condition 2
   p21, p22, p23: proportions of choices 1, 2, and 3 in condition 2
   
How can I test whether the triple (p11,p12,p13) is different from the
triple (p21,p22,p23)?  Clearly, prop.test does not help me here, because
it relates to two-choices tests.

I apologize if the answer is trivial, but I am relatively new to R and
could not find any pointers in the FAQ or in the mailing list archives.

Thanks in advance for any help,

-- 
Rafael Laboissiere



From r.shengzhe at gmail.com  Thu Jul 14 01:28:07 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Thu, 14 Jul 2005 01:28:07 +0200
Subject: [R] help: how to get the position of a value in a matrix
Message-ID: <ea57975b05071316286295e94c@mail.gmail.com>

Hello,

I have a data set matrix of 1200 * 15. How can I get the position of a
specific value in the matrix?

I use "seq(along = x)[x > value]" to look for the position of the
value in the matrix, but "seq" can just find the sequence position row
by row in the matrix, not a real position (like "rowNumber,
colNumber"). Is any function for that?

Thank you,
Shengzhe



From andy_liaw at merck.com  Thu Jul 14 01:33:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Jul 2005 19:33:48 -0400
Subject: [R] help: how to get the position of a value in a matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAA0@usctmx1106.Merck.com>

Use which(..., arr.ind=TRUE); e.g.,

> m <- matrix(runif(12), 3, 4)
> which(m > .8, arr.ind=TRUE)
     row col
[1,]   1   3
[2,]   2   3
[3,]   3   3
[4,]   3   4
> m
          [,1]       [,2]      [,3]      [,4]
[1,] 0.2148183 0.08251853 0.9444718 0.4487148
[2,] 0.5386863 0.49673282 0.8054240 0.5101593
[3,] 0.6252847 0.70974516 0.8858951 0.8590655

Andy

> From: wu sz
> 
> Hello,
> 
> I have a data set matrix of 1200 * 15. How can I get the position of a
> specific value in the matrix?
> 
> I use "seq(along = x)[x > value]" to look for the position of the
> value in the matrix, but "seq" can just find the sequence position row
> by row in the matrix, not a real position (like "rowNumber,
> colNumber"). Is any function for that?
> 
> Thank you,
> Shengzhe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ealaca at ucdavis.edu  Thu Jul 14 01:35:02 2005
From: ealaca at ucdavis.edu (Emilio A. Laca)
Date: Wed, 13 Jul 2005 16:35:02 -0700
Subject: [R] crossed random fx nlme lme4
Message-ID: <F0F05839-744A-4864-97A5-073F8286356B@ucdavis.edu>

I need to specify a model similar to this

lme.formula(fixed = sqrt(lbPerAc) ~ y + season + y:season, data = cy,
     random = ~y | observer/set, correlation = corARMA(q = 6))

except that observer and set are actually crossed instead of nested.

observer and set are factors
y and lbPerAc are numeric

If you know how to do it or have suggestions for reading I will be  
grateful.


eal

ps I have already read Pinheiro & Bates, the jan 05 newsletter, and  
several postings.



From blomsp at ozemail.com.au  Thu Jul 14 01:41:01 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 14 Jul 2005 09:41:01 +1000
Subject: [R] help: how to get the position of a value in a matrix
In-Reply-To: <ea57975b05071316286295e94c@mail.gmail.com>
References: <ea57975b05071316286295e94c@mail.gmail.com>
Message-ID: <6.2.1.2.0.20050714094002.01d16ea0@mail.ozemail.com.au>

See ?which Hint: arr.ind=TRUE

Simon.

At 09:28 AM 14/07/2005, wu sz wrote:
>Hello,
>
>I have a data set matrix of 1200 * 15. How can I get the position of a
>specific value in the matrix?
>
>I use "seq(along = x)[x > value]" to look for the position of the
>value in the matrix, but "seq" can just find the sequence position row
>by row in the matrix, not a real position (like "rowNumber,
>colNumber"). Is any function for that?
>
>Thank you,
>Shengzhe
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mike.rstat at gmail.com  Thu Jul 14 02:14:57 2005
From: mike.rstat at gmail.com (Mike R)
Date: Wed, 13 Jul 2005 17:14:57 -0700
Subject: [R] stripchart usage and alternatives
Message-ID: <27db823f05071317147b981a39@mail.gmail.com>

r=1:10
u=c("a","a","a","b","b","b","c","d","e","e")
uf = factor(u)
rm = tapply(r, uf, mean)

stripchart(r~u,vertical=TRUE,pch=21)
stripchart(rm~levels(uf),vertical=TRUE,pch=3,add=TRUE)

----------
the above code creates a scatter plot of nominal data

are there alternatives to generate the same or similar
"kind" of figure? 


TIA,
Mike



From blomsp at ozemail.com.au  Thu Jul 14 04:45:10 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 14 Jul 2005 12:45:10 +1000
Subject: [R] crossed random fx nlme lme4
In-Reply-To: <F0F05839-744A-4864-97A5-073F8286356B@ucdavis.edu>
References: <F0F05839-744A-4864-97A5-073F8286356B@ucdavis.edu>
Message-ID: <6.2.1.2.0.20050714122836.01c9cc20@mail.ozemail.com.au>

At 09:35 AM 14/07/2005, Emilio A. Laca wrote:
>I need to specify a model similar to this
>
>lme.formula(fixed = sqrt(lbPerAc) ~ y + season + y:season, data = cy,
>      random = ~y | observer/set, correlation = corARMA(q = 6))
>
>except that observer and set are actually crossed instead of nested.

Does this work for you? (following P&B pp 162-3 and an R-help archive 
search on "crossed random effects")...

fit <- lme(sqrt(lbPerAc) ~ y * season, random=list(pdBlocked(pdIdent(~y), 
pdIdent(observer-1), pdIdent(set-1))), correlation=corARMA(q = 6), data=cy)

lme isn't very well set up for crossed random effects. It's easier in lmer. 
I don't think lmer can handle alternative correlation structures yet, 
though. (Prof. Bates?)

HTH,

Simon.


>observer and set are factors
>y and lbPerAc are numeric
>
>If you know how to do it or have suggestions for reading I will be
>grateful.
>
>
>eal
>
>ps I have already read Pinheiro & Bates, the jan 05 newsletter, and
>several postings.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From spencer.graves at pdf.com  Thu Jul 14 05:22:48 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Jul 2005 20:22:48 -0700
Subject: [R] Efficient testing for +ve definiteness
In-Reply-To: <Pine.GSO.4.58.0507131517280.11232@xena.hunter.cuny.edu>
References: <Pine.GSO.4.58.0507131517280.11232@xena.hunter.cuny.edu>
Message-ID: <42D5DA88.9010802@pdf.com>

	  My preference is to test see if the smallest eigenvalue is less than 
something like sqrt(.Machine$double.eps) times the largest.  This may be 
too conservative, but if the ratio of the smallest to the largest is 
less than some small number like that, the inverse of such a real 
symmetric matrix will have very large eigenvalue(s) in potentially 
unstable directions.  R may have other functions beside eigen that will 
explicitly consider the symmetry of a matrix, but I'm not familiar with 
them.

	  spencer graves

Makram Talih wrote:

> Dear R-users,
> 
> Is there a preferred method for testing whether a real symmetric matrix is
> positive definite? [modulo machine rounding errors.]
> 
> The obvious way of computing eigenvalues via "E <- eigen(A, symmetric=T,
> only.values=T)$values" and returning the result of "!any(E <= 0)" seems
> less efficient than going through the LU decomposition invoked in
> "determinant.matrix(A)" and checking the sign and (log) modulus of the
> determinant.
> 
> I suppose this has to do with the underlying C routines. Any thoughts or
> anecdotes?
> 
> Many Thanks,
> 
> Makram Talih
> 
> --
> Makram Talih, Ph.D.
> Assistant Professor
> Department of Mathematics and Statistics
> Hunter College of the City University of New York
> 695 Park Avenue, Room 905 HE
> New York, NY 10021
> 
> Website: http://stat.hunter.cuny.edu/talih
> E-mail: makram.talih at hunter.cuny.edu
> Tel: 212-772-5308
> Fax: 212-772-4858
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Jul 14 05:22:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Jul 2005 20:22:53 -0700
Subject: [R] Memory question
In-Reply-To: <opstumtlat96pdmo@kenneth>
References: <dd48e20f05071210455ac28c1d@mail.gmail.com>
	<opstumtlat96pdmo@kenneth>
Message-ID: <42D5DA8D.7010308@pdf.com>

	  What kinds of matrices?  There are facilities in the Matrix and 
SparseM packages that might help for sparse matrices.  If they are N x k 
where N is large and k is not, can you compute something like the QR 
decomposition and get away with keeping only the R part for most of your 
matrices?

	  One could potentially define a class of matrices that are only kept 
in memory only when needed;  I think S-Plus may do that.  It would take 
a lot of work to make that work generally, but you might be able to 
accomplish what you need with a much smaller effort.

	  spencer graves

Kenneth Roy Cabrera Torres wrote:

> Hi R users and developers:
> 
> I want to know how can I save memory in R
> for example:
>   - saving on disk a matrix.
>   - using again the matrix (changing their values)
>   - saving again the matrix on disk in a different file.
> 
> The idea is that I have a process that generate several
> matrices, but if I keep them all in memory it will overflow.
> 
> How can I save them in different files, so I use the same
> amount of memory for each processed matrix?
> 
> Thank you for your help.
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From r.darnell at uq.edu.au  Thu Jul 14 06:26:45 2005
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Thu, 14 Jul 2005 14:26:45 +1000
Subject: [R] anova.lmlist output change
Message-ID: <42D5E985.1090808@uq.edu.au>

R-colleagues

I have adapted the anova.lmlist function to use the model object name as 
the first column in the output instead of the string "Model n".

If there is general agreement can the change be implemented into the 
stats package?

Regards

Ross Darnell
-- 
University of Queensland, Brisbane QLD 4072 AUSTRALIA
Email: <r.darnell at uq.edu.au>



From sean.oriordain at gmail.com  Thu Jul 14 06:38:16 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Thu, 14 Jul 2005 05:38:16 +0100
Subject: [R] convert to chron objects
In-Reply-To: <971536df05071314105e1b77da@mail.gmail.com>
References: <20050713192734.80886.qmail@web31107.mail.mud.yahoo.com>
	<971536df050713140430dd12e@mail.gmail.com>
	<971536df05071314105e1b77da@mail.gmail.com>
Message-ID: <8ed68eed050713213873005952@mail.gmail.com>

are those dates in m/d/y or d/m/y ?
?chron and watch out for 
format = c(dates = "d/m/y", times = "h:m:s")


On 13/07/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> [I had some emails problems so I am sending this again.  Sorry
> if you get it twice.]
> 
> On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >
> > On 7/13/05, Young Cho <iidn01 at yahoo.com> wrote:
> > > Hi,
> > >
> > > I have a column of a dataframe which has time stamps
> > > like:
> > >
> > > > eh$t[1]
> > > [1] 06/05/2005 01:15:25
> > >
> > > and was wondering how to convert it to chron variable.
> > > Thanks a lot.
> >
> >
> >
> >
> >
> Try this:
> 
> # test data frame eh containing a factor variable t
> eh <- data.frame(t = c("06/05/2005 01:15:25", "06/07/2005 01:15:25"))
> 
> # substring converts factor to character and extracts substring
> chron(dates = substring(eh$t, 1, 10), times = substring(eh$t, 12))
> 
> See ?chron for more info.  There is an article on dates in
> R News 4/1 and although it does not specifically answer this
> question it may be useful with chron and also provides a
> reference to more chron info elsewhere.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jul 14 06:46:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Jul 2005 00:46:59 -0400
Subject: [R] convert to chron objects
In-Reply-To: <8ed68eed050713213873005952@mail.gmail.com>
References: <20050713192734.80886.qmail@web31107.mail.mud.yahoo.com>
	<971536df050713140430dd12e@mail.gmail.com>
	<971536df05071314105e1b77da@mail.gmail.com>
	<8ed68eed050713213873005952@mail.gmail.com>
Message-ID: <971536df050713214640285d90@mail.gmail.com>

An easy to check what you have is to use month.day.year:

> eh <- data.frame(t = c("06/05/2005 01:15:25", "06/07/2005 01:15:25"))
> 
> # substring converts factor to character and extracts substring
> chron(dates = substring(eh$t, 1, 10), times = substring(eh$t, 12))
[1] (06/05/05 01:15:25) (06/07/05 01:15:25)
> month.day.year(.Last.value)
$month
[1] 6 6

$day
[1] 5 7

$year
[1] 2005 2005


On 7/14/05, Sean O'Riordain <sean.oriordain at gmail.com> wrote:
> are those dates in m/d/y or d/m/y ?
> ?chron and watch out for
> format = c(dates = "d/m/y", times = "h:m:s")
> 
> 
> On 13/07/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > [I had some emails problems so I am sending this again.  Sorry
> > if you get it twice.]
> >
> > On 7/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > >
> > >
> > > On 7/13/05, Young Cho <iidn01 at yahoo.com> wrote:
> > > > Hi,
> > > >
> > > > I have a column of a dataframe which has time stamps
> > > > like:
> > > >
> > > > > eh$t[1]
> > > > [1] 06/05/2005 01:15:25
> > > >
> > > > and was wondering how to convert it to chron variable.
> > > > Thanks a lot.
> > >
> > >
> > >
> > >
> > >
> > Try this:
> >
> > # test data frame eh containing a factor variable t
> > eh <- data.frame(t = c("06/05/2005 01:15:25", "06/07/2005 01:15:25"))
> >
> > # substring converts factor to character and extracts substring
> > chron(dates = substring(eh$t, 1, 10), times = substring(eh$t, 12))
> >
> > See ?chron for more info.  There is an article on dates in
> > R News 4/1 and although it does not specifically answer this
> > question it may be useful with chron and also provides a
> > reference to more chron info elsewhere.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From dinja at cs.utexas.edu  Thu Jul 14 07:04:03 2005
From: dinja at cs.utexas.edu (Denis A Ignatovich)
Date: Thu, 14 Jul 2005 01:04:03 -0400
Subject: [R] tseries & GARCH & pred_garch method
Message-ID: <200507140503.j6E53qnl010505@mail.cs.utexas.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/bad15652/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jul 14 08:26:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 14 Jul 2005 08:26:21 +0200
Subject: [R] How to increase memory for R on Soliars 10 with 16GB and
 64bit R
In-Reply-To: <s2d4fc6b.066@OHSU.EDU>
References: <s2d4fc6b.066@OHSU.EDU>
Message-ID: <42D6058D.8080209@statistik.uni-dortmund.de>

Dongseok Choi wrote:

> Thank you very much for your help!!
> Now, it runs without any problem.
> 
> Is it going to be fixed in the next release?

Of course, Brian

> Thanks again,
> Dongseok
> 
> 
> 
> 
> Dongseok Choi, Ph.D.
> Assistant Professor
> Division of Biostatistics
> Department of Public Health & Preventive Medicine
> Oregon Health & Science University
> 3181 SW Sam Jackson Park Road, CB-669
> Portland, OR 97239-3098
> TEL) 503-494-5336
> FAX) 503-494-4981
> choid at ohsu.edu
> 
> 
>>>>"Prof Brian Ripley" <ripley at stats.ox.ac.uk> 07/13/05 12:03 AM >>>
> 
> On Tue, 12 Jul 2005, Dongseok Choi wrote:
> 
> 
>> My machine is SUN Java Workstation 2100 with 2 AMD Opteron CPUs and 16GB RAM.
>> R is compiled as 64bit by using SUN compilers.
>> I trying to fit quantile smoothing on my data and I got an message as below.
>>
>>
>>>fit1<-rqss(z1~qss(cbind(x,y),lambda=la1),tau=t1)
>>
>>Error in as.matrix.csr(diag(n)) :
> 
> cannot allocate memory block of size 2496135168
> 
>> The lengths of vector x and y are both 17664.
>> I tried and found that the same command ran with x[1:16008] and y[1:16008].
>> So, it looks to me a memory related problem, but I'm not sure how I can allocate memory block.
>>  I read the command line option but not sure what do to with it.
>>  Could you help me on this?
> 
> 
> It is trying to allocate a single memory block of size over 2^31-1 bytes. 
> R internally uses ints for sizes of vectors and that is a limit (see 
> help("Memory-limits") ).  However, it is intended that on 64-bit systems 
> that there is a limit here of 8*(2^31-1) but there was a typo.  Please 
> change line 1534 of src/main/memory.c to
> 
> #if SIZEOF_LONG > 4
> 
> and re-compile.
>



From ligges at statistik.uni-dortmund.de  Thu Jul 14 08:28:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 14 Jul 2005 08:28:13 +0200
Subject: [R] How to increase memory for R on Soliars 10 with 16GB and
 64bit R
In-Reply-To: <42D6058D.8080209@statistik.uni-dortmund.de>
References: <s2d4fc6b.066@OHSU.EDU>
	<42D6058D.8080209@statistik.uni-dortmund.de>
Message-ID: <42D605FD.2090200@statistik.uni-dortmund.de>

Uwe Ligges wrote:

> Dongseok Choi wrote:
> 
>> Thank you very much for your help!!
>> Now, it runs without any problem.
>>
>> Is it going to be fixed in the next release?
> 
> 
> Of course, Brian

[hmmm, looks like some wrong shortcut has been used - and it must have 
been me who forgot to drink coffee before strating to post ...]


I meant to say: Of course, Brian already did, see the svn log:

------------------------------------------------------------------------
r34947 | ripley | 2005-07-13 03:16:18 -0400 (Wed, 13 Jul 2005) | 1 line
Changed paths:
    M /trunk/NEWS
    M /trunk/src/main/memory.c

correct typo for R_alloc on 64-bit systems




Uwe Ligges




>> Thanks again,
>> Dongseok
>>
>>
>>
>>
>> Dongseok Choi, Ph.D.
>> Assistant Professor
>> Division of Biostatistics
>> Department of Public Health & Preventive Medicine
>> Oregon Health & Science University
>> 3181 SW Sam Jackson Park Road, CB-669
>> Portland, OR 97239-3098
>> TEL) 503-494-5336
>> FAX) 503-494-4981
>> choid at ohsu.edu
>>
>>
>>>>> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> 07/13/05 12:03 AM >>>
>>
>>
>> On Tue, 12 Jul 2005, Dongseok Choi wrote:
>>
>>
>>> My machine is SUN Java Workstation 2100 with 2 AMD Opteron CPUs and 
>>> 16GB RAM.
>>> R is compiled as 64bit by using SUN compilers.
>>> I trying to fit quantile smoothing on my data and I got an message as 
>>> below.
>>>
>>>
>>>> fit1<-rqss(z1~qss(cbind(x,y),lambda=la1),tau=t1)
>>>
>>>
>>> Error in as.matrix.csr(diag(n)) :
>>
>>
>> cannot allocate memory block of size 2496135168
>>
>>> The lengths of vector x and y are both 17664.
>>> I tried and found that the same command ran with x[1:16008] and 
>>> y[1:16008].
>>> So, it looks to me a memory related problem, but I'm not sure how I 
>>> can allocate memory block.
>>>  I read the command line option but not sure what do to with it.
>>>  Could you help me on this?
>>
>>
>>
>> It is trying to allocate a single memory block of size over 2^31-1 
>> bytes. R internally uses ints for sizes of vectors and that is a limit 
>> (see help("Memory-limits") ).  However, it is intended that on 64-bit 
>> systems that there is a limit here of 8*(2^31-1) but there was a 
>> typo.  Please change line 1534 of src/main/memory.c to
>>
>> #if SIZEOF_LONG > 4
>>
>> and re-compile.
>>
> 
>



From dusa.adrian at gmail.com  Thu Jul 14 08:38:18 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 14 Jul 2005 09:38:18 +0300
Subject: [R] texture in barplots?
In-Reply-To: <x2pstmiddl.fsf@turmalin.kubism.ku.dk>
References: <200507131708.16559.dusa.adrian@gmail.com>
	<200507131756.26393.adi@roda.ro>
	<x2pstmiddl.fsf@turmalin.kubism.ku.dk>
Message-ID: <200507140938.18412.dusa.adrian@gmail.com>

On Thursday 14 July 2005 00:51, Peter Dalgaard wrote:
> Adrian Dusa <adi at roda.ro> writes:
> > <...snip...>
>
> This comes up every now and then, and while it seems that everyone
> thinks fill patterns would be nice to have, I suspect that every
> attempt to actually implement it have gotten killed in infancy.
>
> The thing that is tricky to design right is the cross-device issues.
> Only some devices support this at all, and when they do, the patterns
> tend to be device dependent too. Probably not impossible -- there are
> other bits of the device drivers that deal with missing capabilities,
> like string rotation and clipping -- just, well, tricky.

All clear then; I'll try to find some colors that are different even in B&W, 
or maybe some grays.

> Um... Romania, I suppose? What city?

It's Bucharest; I have a different signature in English but I do forget to use 
it when sending e-mails abroad (bad habit).

Thank you,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
University of Bucharest, Romania
1, Schitu Magureanu Bd.
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101


-- 
This message was scanned for spam and viruses by BitDefender.
For more information please visit http://linux.bitdefender.com/



From kar at itga.com.au  Thu Jul 14 08:46:29 2005
From: kar at itga.com.au (Kylie-Anne Richards)
Date: Thu, 14 Jul 2005 16:46:29 +1000
Subject: [R] Coxph with factors
Message-ID: <20050714064634.A705B44FA7@melmail.itga.com.au>

Hello,

I am fitting a coxph model with factors. I am running into problems when
using 'survfit'. I am unsure how R is treating the factors when I fit, say:
>        DATA<-data.frame(time.sec,done,f.pom=factor(f.pom),po,vo)
>        final<-coxph(Surv(time.sec,done)~f.pom*vo+po,data=DATA)
>         final.surv<-survfit((final), individual=T,conf.type="log-log")
>         print(final.surv)

Also, when I trying using survfit with presepecified values for the
covariates and factor (i.e. '0' to get the baseline) I get an error message,
which I think is a result of my inability to specify the factors in the
correct manner.


I would very much appreciate if somebody can help me out.

Thanks very much,


Kylie



From ripley at stats.ox.ac.uk  Thu Jul 14 09:16:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2005 08:16:31 +0100 (BST)
Subject: [R] Efficient testing for +ve definiteness
In-Reply-To: <Pine.GSO.4.58.0507131517280.11232@xena.hunter.cuny.edu>
References: <Pine.GSO.4.58.0507131517280.11232@xena.hunter.cuny.edu>
Message-ID: <Pine.LNX.4.61.0507140810170.21661@gannet.stats>

On Wed, 13 Jul 2005, Makram Talih wrote:

> Dear R-users,
>
> Is there a preferred method for testing whether a real symmetric matrix is
> positive definite? [modulo machine rounding errors.]
>
> The obvious way of computing eigenvalues via "E <- eigen(A, symmetric=T,
> only.values=T)$values" and returning the result of "!any(E <= 0)" seems
> less efficient than going through the LU decomposition invoked in
> "determinant.matrix(A)" and checking the sign and (log) modulus of the
> determinant.
>
> I suppose this has to do with the underlying C routines. Any thoughts or
> anecdotes?


It has to do with what exactly you want to test.  Knowing the determinant 
does not tell you if the matrix is close to non-positive definite or not.
For numerical work, a comparison of the smallest eigenvalue to the largest 
is usually the most useful indication of possible problems in 
computations.  An alternative is to try a Choleski decomposition, which 
may be faster but is less informative.

Given how fast eigenvalues can be computed by current algorithms (and note 
the comments in ?eigen) I would suggest not worrying about speed until you 
need to (probably never).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From luis.tercero at ebi-wasser.uni-karlsruhe.de  Thu Jul 14 09:25:41 2005
From: luis.tercero at ebi-wasser.uni-karlsruhe.de (Luis Tercero)
Date: Thu, 14 Jul 2005 09:25:41 +0200
Subject: [R] High resolution plots
In-Reply-To: <971536df0507131406564f7700@mail.gmail.com>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
	<971536df0507131406564f7700@mail.gmail.com>
Message-ID: <42D61375.7090002@ebi-wasser.uni-karlsruhe.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/3c740576/attachment.pl

From robin.smit at tno.nl  Thu Jul 14 09:26:41 2005
From: robin.smit at tno.nl (Smit, R. (Robin))
Date: Thu, 14 Jul 2005 09:26:41 +0200
Subject: [R] Pearson dispersion statistic
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F9F9@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/a8231353/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jul 14 10:19:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2005 09:19:26 +0100 (BST)
Subject: [R] Name for factor's levels with contr.sum
In-Reply-To: <17109.13849.189519.960440@stat.math.ethz.ch>
References: <ff51f022050713030254f6d6dc@mail.gmail.com>
	<17109.13849.189519.960440@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0507140910140.22535@gannet.stats>

One way to do this generally is to make a copy of contr.sum, rename it, 
and set the dimnames appropriately.

I think contr.treatment is misleading (it labels contrasts of two levels 
by just one of them), and Christoph's labels are informative but 
impractically long.  But if you want to label each contrast by the level 
it contrasts with the rest,

 	cont <- array(0, c(lenglev, lenglev - 1),
                       list(levels, levels[-lenglev]))

in a modified contr.sum will do it. E.g.

> z <- factor(letters[1:3])
> contr.treatment(z)
   b c
a 0 0
b 1 0
c 0 1
> contr.sum(z)
   [,1] [,2]
a    1    0
b    0    1
c   -1   -1
> mycontr.sum(z)
    a  b
a  1  0
b  0  1
c -1 -1




On Wed, 13 Jul 2005, Christoph Buser wrote:

> Dear Ghislain
>
> I do not know a general elegant solution, but for some
> applications the following example may be helpful:
>
> ## Artificial data for demonstration: group is fixed, species is random
> dat <- data.frame(group = c(rep("A",20),rep("B",17),rep("C",24)),
>                  species = c(rep("sp1", 4), rep("sp2",5),   rep("sp3",5),
>                    rep("sp4",6),  rep("sp5",2),  rep("sp6",5),  rep("sp7",3),
>                    rep("sp8",3), rep("sp9",4), rep("sp10",6),  rep("sp11",6),
>                    rep("sp12",6), rep("sp13",6)),
>                  area = rnorm(61))
>
> ## You can attach a contrast at your fixed factor of interest "group"
> ## Create the contrast you like to test (in our case contr.sum for 3
> ## levels)
> mat <- contr.sum(3)
> ## You can add the names you want to see in the output
> ## Be carefull that you give the correct names to the concerned
> ## column. Otherwise there is the big danger of misinterpretation.
> colnames(mat) <- c(": A against rest", ": B against rest")
> ## Attatch the contrast at your factor "group"
> dat[,"group"] <- C(dat[,"group"],mat)
> ## Now calculate the lme
> library(nlme)
> reg.lme <- lme(area ~ group, data = dat, random = ~ 1|species)
> summary(reg.lme)
>
> Maybe someone has a better idea how to do it generally.
>
> Hope this helps
>
> Christoph Buser
>
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
>
>
> Ghislain Vieilledent writes:
> > Good morning,
> >
> > I used in R contr.sum for the contrast in a lme model:
> >
> > > options(contrasts=c("contr.sum","contr.poly"))
> > > Septo5.lme<-lme(Septo~Variete+DateSemi,Data4.Iso,random=~1|LieuDit)
> > > intervals(Septo5.lme)$fixed
> > lower est. upper
> > (Intercept) 17.0644033 23.106110 29.147816
> > Variete1 9.5819873 17.335324 25.088661
> > Variete2 -3.3794907 6.816101 17.011692
> > Variete3 -0.5636915 8.452890 17.469472
> > Variete4 -22.8923812 -10.914912 1.062558
> > Variete5 -10.7152821 -1.865884 6.983515
> > Variete6 0.2743390 9.492175 18.710012
> > Variete7 -23.7943250 -15.070737 -6.347148
> > Variete8 -21.7310554 -12.380475 -3.029895
> > Variete9 -27.9782575 -17.480555 -6.982852
> > DateSemi1 -5.7903419 -1.547875 2.694592
> > DateSemi2 3.6571596 8.428417 13.199675
> > attr(,"label")
> > [1] "Fixed effects:"
> >
> > How is it possible to obtain a return with the name of my factor's levels as
> > with contr.treatment ?
> >
> > Thanks for you help.
> >
> > --
> > Ghislain Vieilledent
> > 30, rue Bernard Ortet 31 500 TOULOUSE
> > 06 24 62 65 07
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From admin at biostatistic.de  Thu Jul 14 08:28:37 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 14 Jul 2005 08:28:37 +0200
Subject: [R] High resolution plots
In-Reply-To: <x2ll4aiby2.fsf@turmalin.kubism.ku.dk>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
	<x2ll4aiby2.fsf@turmalin.kubism.ku.dk>
Message-ID: <42D60615.2080403@biostatistic.de>



Peter Dalgaard schrieb:

>What is the real issue here? Import trouble? If you're importing to
>Word/PowerPoint, why not use the Windows metafile? Perhaps they are
>too ugly compared to EPS by your taste?
>  
>

My reason was
http://www.adobe.com/support/techdocs/328541.html

I tried to plot emf files with pagemaker (not even a cheep or freeware pprogramm) and the lyaout was corrupted.
I tried to use eps files the text was corrupted
and I do not know how to transfer the *.ps file to tiff.
Maybe there is any payware but if you have a layout application for more than 700 Euro you don??t like to by any further application only to change the *.ps format.
So I am looking for any solution to increase the resolution.
Maybe I do not understand the meanings of res, width and heigh or it is not working as expected (see answer from 12 Jul 2005)
I still hope that Porf. Brian D. Ripley will answer to this question even though I got him on his nerves with a wrong answer. :-(

with regards
Knut Krueger
http://www.biostatistic.de



From azoery011388 at yahoo.com  Thu Jul 14 11:45:38 2005
From: azoery011388 at yahoo.com (Aam Sudrajat)
Date: Thu, 14 Jul 2005 02:45:38 -0700 (PDT)
Subject: [R] Ellipse Cosrrespondence Analysis(Internal Stability)
Message-ID: <20050714094538.1581.qmail@web42405.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/fe89765c/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Jul 14 11:49:21 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jul 2005 11:49:21 +0200
Subject: [R] High resolution plots
In-Reply-To: <42D60615.2080403@biostatistic.de>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
	<x2ll4aiby2.fsf@turmalin.kubism.ku.dk>
	<42D60615.2080403@biostatistic.de>
Message-ID: <x2oe95yaxq.fsf@turmalin.kubism.ku.dk>

Knut Krueger <admin at biostatistic.de> writes:

> Peter Dalgaard schrieb:
> 
> >What is the real issue here? Import trouble? If you're importing to
> >Word/PowerPoint, why not use the Windows metafile? Perhaps they are
> >too ugly compared to EPS by your taste?
> >
> 
> My reason was
> http://www.adobe.com/support/techdocs/328541.html

Oh, lovely.....
 
> I tried to plot emf files with pagemaker (not even a cheep or
> freeware pprogramm) and the lyaout was corrupted. I tried to use eps
> files the text was corrupted and I do not know how to transfer the
> *.ps file to tiff.

Stupid question: You did remember to close the graphics device when
you generated those plots (dev.off())? 

If an EPS file doesn't work with an Adobe product, there is some
reason to suspect that the file is corrupted or that there is an issue
with R's postscript driver. If you're seeing text corruption outside
the plot, then surely the E in EPS (for "encapsulated") is not working
as intended. 

Did you try the pdf() driver? That usually comes out rather nicely in
connection with Acrobat/Distiller.

> Maybe there is any payware but if you have a layout application for
> more than 700 Euro you don??t like to by any further application only
> to change the *.ps format. So I am looking for any solution to
> increase the resolution. Maybe I do not understand the meanings of
> res, width and heigh or it is not working as expected (see answer
> from 12 Jul 2005) I still hope that Porf. Brian D. Ripley will
> answer to this question even though I got him on his nerves with a
> wrong answer. :-(
> 
> with regards
> Knut Krueger
> http://www.biostatistic.de
> 
> 
> 
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From krcabrer at epm.net.co  Thu Jul 14 12:32:11 2005
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Thu, 14 Jul 2005 05:32:11 -0500
Subject: [R] Memory question
In-Reply-To: <42D5DA8D.7010308@pdf.com>
References: <dd48e20f05071210455ac28c1d@mail.gmail.com>
	<opstumtlat96pdmo@kenneth> <42D5DA8D.7010308@pdf.com>
Message-ID: <42D63F2B.90605@epm.net.co>

Thank you Dr. Spencer Graves for your answer.

What kind of matrices? They come form an image of about 3000x5000, and
I need to generate arround 1024 matrices of the same size, they are not 
sparse
matrices.

What function can I use to, once generated one matrix, I could save into 
disk
and then use the same space for the following, and so on.

Thank you very much for your help

Kenneth

Spencer Graves wrote:

>	  What kinds of matrices?  There are facilities in the Matrix and 
>SparseM packages that might help for sparse matrices.  If they are N x k 
>where N is large and k is not, can you compute something like the QR 
>decomposition and get away with keeping only the R part for most of your 
>matrices?
>
>	  One could potentially define a class of matrices that are only kept 
>in memory only when needed;  I think S-Plus may do that.  It would take 
>a lot of work to make that work generally, but you might be able to 
>accomplish what you need with a much smaller effort.
>
>	  spencer graves
>
>Kenneth Roy Cabrera Torres wrote:
>
>  
>
>>Hi R users and developers:
>>
>>I want to know how can I save memory in R
>>for example:
>>  - saving on disk a matrix.
>>  - using again the matrix (changing their values)
>>  - saving again the matrix on disk in a different file.
>>
>>The idea is that I have a process that generate several
>>matrices, but if I keep them all in memory it will overflow.
>>
>>How can I save them in different files, so I use the same
>>amount of memory for each processed matrix?
>>
>>Thank you for your help.
>>
>>    
>>
>
>  
>

From admin at biostatistic.de  Thu Jul 14 12:15:40 2005
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 14 Jul 2005 12:15:40 +0200
Subject: [R] High resolution plots
In-Reply-To: <x2oe95yaxq.fsf@turmalin.kubism.ku.dk>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>	<x2ll4aiby2.fsf@turmalin.kubism.ku.dk>	<42D60615.2080403@biostatistic.de>
	<x2oe95yaxq.fsf@turmalin.kubism.ku.dk>
Message-ID: <42D63B4C.8020609@biostatistic.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/9a324905/attachment.pl

From buser at stat.math.ethz.ch  Thu Jul 14 12:28:15 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 14 Jul 2005 12:28:15 +0200
Subject: [R] Name for factor's levels with contr.sum
In-Reply-To: <Pine.LNX.4.61.0507140910140.22535@gannet.stats>
References: <ff51f022050713030254f6d6dc@mail.gmail.com>
	<17109.13849.189519.960440@stat.math.ethz.ch>
	<Pine.LNX.4.61.0507140910140.22535@gannet.stats>
Message-ID: <17110.15935.274177.982002@stat.math.ethz.ch>

This is exactly what I've been looking for (without success)
when I was speaking about a more elegant and general solution. 

I agree with your argument that labels might be misleading.  
Nevertheless if a user is aware what contr.sum calculates, it is
practical to have an annotation. 

Thank you very much for the solution.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Prof Brian Ripley writes:
 > One way to do this generally is to make a copy of contr.sum, rename it, 
 > and set the dimnames appropriately.
 > 
 > I think contr.treatment is misleading (it labels contrasts of two levels 
 > by just one of them), and Christoph's labels are informative but 
 > impractically long.  But if you want to label each contrast by the level 
 > it contrasts with the rest,
 > 
 >  	cont <- array(0, c(lenglev, lenglev - 1),
 >                        list(levels, levels[-lenglev]))
 > 
 > in a modified contr.sum will do it. E.g.
 > 
 > > z <- factor(letters[1:3])
 > > contr.treatment(z)
 >    b c
 > a 0 0
 > b 1 0
 > c 0 1
 > > contr.sum(z)
 >    [,1] [,2]
 > a    1    0
 > b    0    1
 > c   -1   -1
 > > mycontr.sum(z)
 >     a  b
 > a  1  0
 > b  0  1
 > c -1 -1
 > 
 > 
 > 
 > 
 > On Wed, 13 Jul 2005, Christoph Buser wrote:
 > 
 > > Dear Ghislain
 > >
 > > I do not know a general elegant solution, but for some
 > > applications the following example may be helpful:
 > >
 > > ## Artificial data for demonstration: group is fixed, species is random
 > > dat <- data.frame(group = c(rep("A",20),rep("B",17),rep("C",24)),
 > >                  species = c(rep("sp1", 4), rep("sp2",5),   rep("sp3",5),
 > >                    rep("sp4",6),  rep("sp5",2),  rep("sp6",5),  rep("sp7",3),
 > >                    rep("sp8",3), rep("sp9",4), rep("sp10",6),  rep("sp11",6),
 > >                    rep("sp12",6), rep("sp13",6)),
 > >                  area = rnorm(61))
 > >
 > > ## You can attach a contrast at your fixed factor of interest "group"
 > > ## Create the contrast you like to test (in our case contr.sum for 3
 > > ## levels)
 > > mat <- contr.sum(3)
 > > ## You can add the names you want to see in the output
 > > ## Be carefull that you give the correct names to the concerned
 > > ## column. Otherwise there is the big danger of misinterpretation.
 > > colnames(mat) <- c(": A against rest", ": B against rest")
 > > ## Attatch the contrast at your factor "group"
 > > dat[,"group"] <- C(dat[,"group"],mat)
 > > ## Now calculate the lme
 > > library(nlme)
 > > reg.lme <- lme(area ~ group, data = dat, random = ~ 1|species)
 > > summary(reg.lme)
 > >
 > > Maybe someone has a better idea how to do it generally.
 > >
 > > Hope this helps
 > >
 > > Christoph Buser
 > >
 > > --------------------------------------------------------------
 > > Christoph Buser <buser at stat.math.ethz.ch>
 > > Seminar fuer Statistik, LEO C13
 > > ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
 > > phone: x-41-44-632-4673		fax: 632-1228
 > > http://stat.ethz.ch/~buser/
 > > --------------------------------------------------------------
 > >
 > >
 > > Ghislain Vieilledent writes:
 > > > Good morning,
 > > >
 > > > I used in R contr.sum for the contrast in a lme model:
 > > >
 > > > > options(contrasts=c("contr.sum","contr.poly"))
 > > > > Septo5.lme<-lme(Septo~Variete+DateSemi,Data4.Iso,random=~1|LieuDit)
 > > > > intervals(Septo5.lme)$fixed
 > > > lower est. upper
 > > > (Intercept) 17.0644033 23.106110 29.147816
 > > > Variete1 9.5819873 17.335324 25.088661
 > > > Variete2 -3.3794907 6.816101 17.011692
 > > > Variete3 -0.5636915 8.452890 17.469472
 > > > Variete4 -22.8923812 -10.914912 1.062558
 > > > Variete5 -10.7152821 -1.865884 6.983515
 > > > Variete6 0.2743390 9.492175 18.710012
 > > > Variete7 -23.7943250 -15.070737 -6.347148
 > > > Variete8 -21.7310554 -12.380475 -3.029895
 > > > Variete9 -27.9782575 -17.480555 -6.982852
 > > > DateSemi1 -5.7903419 -1.547875 2.694592
 > > > DateSemi2 3.6571596 8.428417 13.199675
 > > > attr(,"label")
 > > > [1] "Fixed effects:"
 > > >
 > > > How is it possible to obtain a return with the name of my factor's levels as
 > > > with contr.treatment ?
 > > >
 > > > Thanks for you help.
 > > >
 > > > --
 > > > Ghislain Vieilledent
 > > > 30, rue Bernard Ortet 31 500 TOULOUSE
 > > > 06 24 62 65 07
 > > >
 > > > 	[[alternative HTML version deleted]]
 > > >
 > > > ______________________________________________
 > > > R-help at stat.math.ethz.ch mailing list
 > > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 > >
 > > ______________________________________________
 > > R-help at stat.math.ethz.ch mailing list
 > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 > >
 > 
 > -- 
 > Brian D. Ripley,                  ripley at stats.ox.ac.uk
 > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
 > University of Oxford,             Tel:  +44 1865 272861 (self)
 > 1 South Parks Road,                     +44 1865 272866 (PA)
 > Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Jul 14 12:34:41 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 14 Jul 2005 12:34:41 +0200
Subject: [R] unexpected par('pin') behaviour
In-Reply-To: <42D51E9A.8070702@fz-rossendorf.de>
References: <42D51E9A.8070702@fz-rossendorf.de>
Message-ID: <17110.16321.138943.126111@stat.math.ethz.ch>

>>>>> "joerg" == joerg van den hoff <j.van_den_hoff at fz-rossendorf.de>
>>>>>     on Wed, 13 Jul 2005 16:00:58 +0200 writes:

    joerg> hi everybody,
    joerg> I noticed the following: in one of my scripts 'layout' is used to 
    joerg> generate a (approx. square) grid of variable dimensions (depending on 
    joerg> no. of input files). if the no. of subplots (grid cells) becomes 
    joerg> moderately large  (say > 9) I use a construct like

    joerg>   ###layout grid computation and set up occurs here###
    joerg>   ...
    joerg>   opar <- par(no.readonly = T);
    joerg>   on.exit(par(opar))
    joerg>   par(mar=c(4.1, 4.1, 1.1, .1))
    joerg>   ###plotting occurs here####
    joerg>   ...

    joerg> to reduce the figure margins to achieve a more
    joerg> compact display. apart from 'mar' no other par()
    joerg> setting is modified.

yet another example of using  par('no.readonly') when it's not
needed and inefficient.

Replacing the above by

    ###layout grid computation and set up occurs here###    
    ...
     op <- par(mar=c(4.1, 4.1, 1.1, .1))
     on.exit(par(op))
    ###plotting occurs here####

will be much more efficient and even solve your problem with "pin".

But then, yes, there might be another par() problem hidden in
your code / example, 
but unfortunately you have not specified reproducible code.


    joerg> this works fine until the total number of subplots becomes too large 
    joerg> ("large" depending on the current size of the X11() graphics device 
    joerg> window, e.g. 7 x 6 subplots for the default size fo x11()).

    joerg> I then get the error message (only _after_ all plots are correctly 
    joerg> displayed, i.e. obviously during execution of the above on.exit() call)

    joerg> Error in par(opar) :
    joerg> invalid value specified for graphics parameter "pin"

    joerg> and par("pin") yields:

    joerg> [1]  0.34864 -0.21419

you mean *after* all the plotting ,  not the "pin" values in
'opar', right?

    joerg> which indeed is invalid (negative 2nd component).

    joerg> I'm aware of this note from ?par:

    joerg> The effect of restoring all the (settable) graphics parameters as
    joerg> in the examples is hard to predict if the device has been resized.
    joerg> Several of them are attempting to set the same things in different
    joerg> ways, and those last in the alphabet will win.  In particular, the
    joerg> settings of 'mai', 'mar', 'pin', 'plt' and 'pty' interact, as do
    joerg> the outer margin settings, the figure layout and figure region
    joerg> size.

{{which shows you the known  but not widely known fact that
  traditional par() based graphics are ``flawed by design''
  and that's why there is the package "grid" for better 
  designed graphics
}}

    joerg> but my problem occurs without any resizing of the
    joerg> x11() window prior to resetting par to par(opar).

It still would be interesting to get a reproducible example
here, as the posting guide asks for.

Martin


    joerg> any ideas, what is going on?

    joerg> platform powerpc-apple-darwin7.9.0
    joerg> arch     powerpc
    joerg> os       darwin7.9.0
    joerg> system   powerpc, darwin7.9.0
    joerg> status   Patched
		    ^^^^^^^
I hope that this is not the basic problem 

    joerg> major    2
    joerg> minor    1.0
    joerg> year     2005
    joerg> month    05
    joerg> day      12
    joerg> language R

    joerg> regards,

    joerg> joerg



From andy_liaw at merck.com  Thu Jul 14 12:35:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Jul 2005 06:35:18 -0400
Subject: [R] How to increase memory for R on Soliars 10 with 16GB and
 64bit R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAA5@usctmx1106.Merck.com>

> From: Uwe Ligges
> 
> Uwe Ligges wrote:
> 
> > Dongseok Choi wrote:
> > 
> >> Thank you very much for your help!!
> >> Now, it runs without any problem.
> >>
> >> Is it going to be fixed in the next release?
> > 
> > 
> > Of course, Brian
> 
> [hmmm, looks like some wrong shortcut has been used - and it 
> must have 
> been me who forgot to drink coffee before strating to post ...]

Is this indication that R-help is more addictive than coffee?

Andy
 
> 
> I meant to say: Of course, Brian already did, see the svn log:
> 
> --------------------------------------------------------------
> ----------
> r34947 | ripley | 2005-07-13 03:16:18 -0400 (Wed, 13 Jul 
> 2005) | 1 line
> Changed paths:
>     M /trunk/NEWS
>     M /trunk/src/main/memory.c
> 
> correct typo for R_alloc on 64-bit systems
> 
> 
> 
> 
> Uwe Ligges
> 
> 
> 
> 
> >> Thanks again,
> >> Dongseok
> >>
> >>
> >>
> >>
> >> Dongseok Choi, Ph.D.
> >> Assistant Professor
> >> Division of Biostatistics
> >> Department of Public Health & Preventive Medicine
> >> Oregon Health & Science University
> >> 3181 SW Sam Jackson Park Road, CB-669
> >> Portland, OR 97239-3098
> >> TEL) 503-494-5336
> >> FAX) 503-494-4981
> >> choid at ohsu.edu
> >>
> >>
> >>>>> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> 07/13/05 
> 12:03 AM >>>
> >>
> >>
> >> On Tue, 12 Jul 2005, Dongseok Choi wrote:
> >>
> >>
> >>> My machine is SUN Java Workstation 2100 with 2 AMD 
> Opteron CPUs and 
> >>> 16GB RAM.
> >>> R is compiled as 64bit by using SUN compilers.
> >>> I trying to fit quantile smoothing on my data and I got 
> an message as 
> >>> below.
> >>>
> >>>
> >>>> fit1<-rqss(z1~qss(cbind(x,y),lambda=la1),tau=t1)
> >>>
> >>>
> >>> Error in as.matrix.csr(diag(n)) :
> >>
> >>
> >> cannot allocate memory block of size 2496135168
> >>
> >>> The lengths of vector x and y are both 17664.
> >>> I tried and found that the same command ran with x[1:16008] and 
> >>> y[1:16008].
> >>> So, it looks to me a memory related problem, but I'm not 
> sure how I 
> >>> can allocate memory block.
> >>>  I read the command line option but not sure what do to with it.
> >>>  Could you help me on this?
> >>
> >>
> >>
> >> It is trying to allocate a single memory block of size over 2^31-1 
> >> bytes. R internally uses ints for sizes of vectors and 
> that is a limit 
> >> (see help("Memory-limits") ).  However, it is intended 
> that on 64-bit 
> >> systems that there is a limit here of 8*(2^31-1) but there was a 
> >> typo.  Please change line 1534 of src/main/memory.c to
> >>
> >> #if SIZEOF_LONG > 4
> >>
> >> and re-compile.
> >>
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Thu Jul 14 13:05:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jul 2005 13:05:32 +0200
Subject: [R] High resolution plots
In-Reply-To: <42D63B4C.8020609@biostatistic.de>
References: <42D53844.4040802@ebi-wasser.uni-karlsruhe.de>
	<x2ll4aiby2.fsf@turmalin.kubism.ku.dk>
	<42D60615.2080403@biostatistic.de>
	<x2oe95yaxq.fsf@turmalin.kubism.ku.dk>
	<42D63B4C.8020609@biostatistic.de>
Message-ID: <x2k6jty7er.fsf@turmalin.kubism.ku.dk>

Knut Krueger <admin at biostatistic.de> writes:

> Peter Dalgaard schrieb:
> 
> >Knut Krueger <admin at biostatistic.de> writes:
> >
> >
> >>Peter Dalgaard schrieb:
> >>
> >>
> >>>What is the real issue here? Import trouble? If you're importing to
> >>>Word/PowerPoint, why not use the Windows metafile? Perhaps they are
> >>>too ugly compared to EPS by your taste?
> >>>
> >>>
> >>My reason was
> >>http://www.adobe.com/support/techdocs/328541.html
> >>
> >
> >Oh, lovely.....
> >
> isn't it ? :-(
> 
> >Stupid question: You did remember to close the graphics device when
> > you generated those plots (dev.off())?
> the first idea was no I didn??t ... but I found my script and I did :-)
> 
> >If an EPS file doesn't work with an Adobe product, there is some
> >reason to suspect that the file is corrupted or that there is an issue
> >with R's postscript driver. If you're seeing text corruption outside
> >the plot, then surely the E in EPS (for "encapsulated") is not working
> > as intended.
> That's a normal behaviour in windows surrounding.
> 
> >Did you try the pdf() driver? That usually comes out rather nicely in
> >connection with Acrobat/Distiller.
> >
> No I did not actually.
> the plots are good enough for printing with an laser printer, but they
> are not good enough for the journal.
> It is curious. one of them wants only pdf the other one wants tiff and
> wordperfect or word .. maybe the next one will receive old latin stone
> plates ;-)
> 
> Therefore  for one journal  it is enough to use the eps or bmp
> resolution with pdf file or just an pdf output, but for the next one I
> will need a tiff with 300 dpi at least.

It's still a bit weird if they want a bitmapped format, given the
difficulty of scaling such beasts, but what was ever wrong with
installing ghostscript and doing 

bitmap("my.tiff", "tiff24nc", res=300)

?



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ginters.buss at gmail.com  Thu Jul 14 13:19:20 2005
From: ginters.buss at gmail.com (Ginters)
Date: Thu, 14 Jul 2005 14:19:20 +0300
Subject: [R] memory problem
Message-ID: <7ff971d4050714041925848073@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/e0d7316f/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jul 14 13:29:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2005 12:29:25 +0100 (BST)
Subject: [R] Memory question
In-Reply-To: <42D63F2B.90605@epm.net.co>
References: <dd48e20f05071210455ac28c1d@mail.gmail.com>
	<opstumtlat96pdmo@kenneth>
	<42D5DA8D.7010308@pdf.com> <42D63F2B.90605@epm.net.co>
Message-ID: <Pine.LNX.4.61.0507141226090.4299@gannet.stats>

On Thu, 14 Jul 2005, Kenneth Cabrera wrote:

> Thank you Dr. Spencer Graves for your answer.
>
> What kind of matrices? They come form an image of about 3000x5000, and
> I need to generate arround 1024 matrices of the same size, they are not 
> sparse
> matrices.
>
> What function can I use to, once generated one matrix, I could save into disk
> and then use the same space for the following, and so on.

You can use either save or .saveRDS/serialize followed by rm() and gc(). 
You cannot use the same space, but you can free up the space.

Then when you need the data again, load/.readRDS/unserialize can pull the 
object back.  (If you arrange this right the object will only go into a 
temporary frame and so only be needed one at a time.)

>
> Thank you very much for your help
>
> Kenneth
>
> Spencer Graves wrote:
>
>> 	  What kinds of matrices?  There are facilities in the Matrix and 
>> SparseM packages that might help for sparse matrices.  If they are N x k 
>> where N is large and k is not, can you compute something like the QR 
>> decomposition and get away with keeping only the R part for most of your 
>> matrices?
>> 
>> 	  One could potentially define a class of matrices that are only kept 
>> in memory only when needed;  I think S-Plus may do that.  It would take a 
>> lot of work to make that work generally, but you might be able to 
>> accomplish what you need with a much smaller effort.
>> 
>> 	  spencer graves
>> 
>> Kenneth Roy Cabrera Torres wrote:
>> 
>> 
>>> Hi R users and developers:
>>> 
>>> I want to know how can I save memory in R
>>> for example:
>>>  - saving on disk a matrix.
>>>  - using again the matrix (changing their values)
>>>  - saving again the matrix on disk in a different file.
>>> 
>>> The idea is that I have a process that generate several
>>> matrices, but if I keep them all in memory it will overflow.
>>> 
>>> How can I save them in different files, so I use the same
>>> amount of memory for each processed matrix?
>>> 
>>> Thank you for your help.
>>> 
>>> 
>> 
>> 
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Thu Jul 14 14:16:48 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Jul 2005 08:16:48 -0400
Subject: [R] memory problem
In-Reply-To: <7ff971d4050714041925848073@mail.gmail.com>
References: <7ff971d4050714041925848073@mail.gmail.com>
Message-ID: <42D657B0.5090104@stats.uwo.ca>

On 7/14/2005 7:19 AM, Ginters wrote:
> I'm a beginner in R and, therefore, I don't know how serious my trouble is.
> After running a script:
> 
>  **
> 
> *t**<-c(14598417794,649693)*
> 
> *data**=data.frame(read.spss("C:\\Ginters\\Kalibracija\\cal_data.sav"))*
> 
> *Xs=**as.matrix(data[,1:2])
> * 
> 
> *koef**=data.frame(read.spss("C:\\Ginters\\Kalibracija\\f.sav"))
> * 
> 
> *piks=**as.matrix(koef[,1**])*
> 
> *g=regressionestimator(Xs,piks,t)*
> 
> I get:
> 
> *Error: cannot allocate vector of size 1614604 Kb
> In addition: Warning messages:
> 1: Reached total allocation of 255Mb: see help(memory.size) 
> 2: Reached total allocation of 255Mb: see help(memory.size) *
> 
> My OS is Win 2000 Proffesional.
> Those 2 objects are of sizes
> 
> *> object.size(Xs)
> [1] 805404
>> object.size(piks)
> [1] 115128*
> 
> accordingly. The 2 files use only 142KB and 60KB accordingly.
> Why does memory need so much (1.6 GB) space? How can I enlarge it? Is it 
> possible to allocate a part of memory used to the hard drive? Or, is the 
> trouble only with my script?

This sounds like a problem with the regressionestimator function, which 
I think comes from the sampling package.  You'll need to contact the 
maintainer of the package to find out why it needs so much memory, and 
whether there's a way to get what you want without it.

Duncan Murdoch



From r.shengzhe at gmail.com  Thu Jul 14 14:38:28 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Thu, 14 Jul 2005 14:38:28 +0200
Subject: [R] help: how to change the size of a window after it has been
	created
Message-ID: <ea57975b0507140538413be959@mail.gmail.com>

Hello,

I wish to plot some figures in a window in turn, but the size of these
figures is different, so how can I change the size of the window by
resetting the parameters before each plotting?

Thank you,
Shengzhe



From roy.werkman at asml.com  Thu Jul 14 14:45:14 2005
From: roy.werkman at asml.com (Roy Werkman)
Date: Thu, 14 Jul 2005 14:45:14 +0200
Subject: [R] Plotting greek symbols in plot titles, labels, etc.
Message-ID: <448071208107374B96ED90585EEBA912848FCF@NLVDHX84.sn-eu.asml.com>


Hi,

Can anyone please tell me how to use the escape sequences (e.g. \\*a for
alfa) for plotting Greek symbols in for example plot titles? I have read
the reference manual, but can't seem to get it to work.

Thanx!

Roy


-- 
The information contained in this communication and any atta...{{dropped}}



From rpeng at jhsph.edu  Thu Jul 14 14:49:57 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 14 Jul 2005 08:49:57 -0400
Subject: [R] Plotting greek symbols in plot titles, labels, etc.
In-Reply-To: <448071208107374B96ED90585EEBA912848FCF@NLVDHX84.sn-eu.asml.com>
References: <448071208107374B96ED90585EEBA912848FCF@NLVDHX84.sn-eu.asml.com>
Message-ID: <42D65F75.7020703@jhsph.edu>

Take a look at '?plotmath'.

-roger

Roy Werkman wrote:
> Hi,
> 
> Can anyone please tell me how to use the escape sequences (e.g. \\*a for
> alfa) for plotting Greek symbols in for example plot titles? I have read
> the reference manual, but can't seem to get it to work.
> 
> Thanx!
> 
> Roy
> 
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From rpeng at jhsph.edu  Thu Jul 14 14:50:44 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 14 Jul 2005 08:50:44 -0400
Subject: [R] help: how to change the size of a window after it has been
 created
In-Reply-To: <ea57975b0507140538413be959@mail.gmail.com>
References: <ea57975b0507140538413be959@mail.gmail.com>
Message-ID: <42D65FA4.4090208@jhsph.edu>

Not sure you can do this.  You might have to launch separate 
graphics windows.

-roger

wu sz wrote:
> Hello,
> 
> I wish to plot some figures in a window in turn, but the size of these
> figures is different, so how can I change the size of the window by
> resetting the parameters before each plotting?
> 
> Thank you,
> Shengzhe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From MSchwartz at mn.rr.com  Thu Jul 14 15:12:18 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 14 Jul 2005 08:12:18 -0500
Subject: [R] help: how to change the size of a window after it
	has	been	created
In-Reply-To: <ea57975b0507140538413be959@mail.gmail.com>
References: <ea57975b0507140538413be959@mail.gmail.com>
Message-ID: <1121346738.4161.20.camel@localhost.localdomain>

On Thu, 2005-07-14 at 14:38 +0200, wu sz wrote:
> Hello,
> 
> I wish to plot some figures in a window in turn, but the size of these
> figures is different, so how can I change the size of the window by
> resetting the parameters before each plotting?
> 
> Thank you,
> Shengzhe

Other than dragging a plot window with a mouse, I do not think that
there is a way to change the size of an open display device via code
(though somebody will no doubt correct me if I am wrong).

See ?Devices

Depending upon your OS, there is likely a 'width' and 'height' argument
for the screen display device for newly opened devices.

For example, under systems using X, where X11() is the screen device:

X11(width = 11, height = 8)
plot(1:10)

X11(width = 5, height = 5)
barplot(1:5)

In the above example, two separate plot windows will be created.

If you want to only have one open at a time, you can close the device
first using dev.off() before the subsequent plots. However, you lose the
current plot as soon as you close it.

X11(width = 11, height = 8)
plot(1:10)
dev.off()

X11(width = 5, height = 5)
barplot(1:5)
dev.off()

HTH,

Marc Schwartz



From bob.ohara at helsinki.fi  Thu Jul 14 15:32:34 2005
From: bob.ohara at helsinki.fi (Anon.)
Date: Thu, 14 Jul 2005 16:32:34 +0300
Subject: [R] help: how to change the size of a window after it	has	been
 created
References: <ea57975b0507140538413be959@mail.gmail.com>
	<1121346738.4161.20.camel@localhost.localdomain>
Message-ID: <42D66972.8090008@helsinki.fi>

Marc Schwartz wrote:
> On Thu, 2005-07-14 at 14:38 +0200, wu sz wrote:
> 
>>Hello,
>>
>>I wish to plot some figures in a window in turn, but the size of these
>>figures is different, so how can I change the size of the window by
>>resetting the parameters before each plotting?
>>
>>Thank you,
>>Shengzhe
> 
> 
> Other than dragging a plot window with a mouse, I do not think that
> there is a way to change the size of an open display device via code
> (though somebody will no doubt correct me if I am wrong).
> 

Perhaps some lateral thinking is called for.  Rather than change the 
size of the window, how about changing the size of the plotting region 
within the window.  For example:

split.screen(matrix(c(0,0.5, 0,1,  0.5,1,0,1), ncol=4, byrow=T))
plot(1:5)
close.screen(all=T)

split.screen(matrix(c(0,0.5, 0,0.5,  0.5,1,0.5,1), ncol=4, byrow=T))
plot(1:5)
close.screen(all=T)

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From rvaradha at jhsph.edu  Thu Jul 14 15:44:47 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 14 Jul 2005 09:44:47 -0400
Subject: [R] Fieller's Conf Limits and EC50's
In-Reply-To: <42D5448C.7000504@ttu.edu>
Message-ID: <OWA-15gKU2yfgmiCFzw00006de0@owa-1.sph.ad.jhsph.edu>

Hi,

I didn't verify your formulas for Fieller's method of computing the
confidence interval. A slightly simpler approach is to use the Delta method
to compute the CI.  It is also valid for any link function.  It yields a
simpler formula for the variance of EC50 (for any link function):

varEC50 <- 1/b1^2 * (var.b0 + EC50^2*var.b1 + 2*EC50*cov.b0.b1)

So, you can compute the CIs as:

LCL <- EC50 - zalpha.2 * sqrt(varEC50)
UCL <- EC50 + zalpha.2 * sqrt(varEC50)

This works for any EC_p, where p is the probability of getting a positive
response, and for link function.  The CI from the Delta method should be
very nearly the same as that obtained using Fieller's method for EC50.  For
smaller probabilities (e.g., p < 0.1), CIs for the EC_p values obtained
using the two methods can be slightly different.

Ravi.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Stephen B. Cox
> Sent: Wednesday, July 13, 2005 12:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Fieller's Conf Limits and EC50's
> 
> Folks
> 
> I have modified an existing function to calculate 'ec/ld/lc' 50 values
> and their associated Fieller's confidence limits.  It is based on
> EC50.calc (writtien by John Bailer)  - but also borrows from the dose.p
> (MASS) function.  My goal was to make the original EC50.calc function
> flexible with respect to 1) probability at which to calculate the
> expected dose, and 2) the link function.  I would appreciate comments
> about the validity of doing so!  In particular - I want to make sure
> that the confidence limit calculations are still valid when changing the
> link function.
> 
> ec.calc<-function(obj,conf.level=.95,p=.5) {
> 
>  # calculates confidence interval based upon Fieller's thm.
>  # modified version of EC50.calc found in P&B Fig 7.22
>  # now allows other link functions, using the calculations
>  # found in dose.p (MASS)
>  # SBC 19 May 05
> 
>         call <- match.call()
> 
>          coef = coef(obj)
>          vcov = summary.glm(obj)$cov.unscaled
>          b0<-coef[1]
>          b1<-coef[2]
>          var.b0<-vcov[1,1]
>          var.b1<-vcov[2,2]
>          cov.b0.b1<-vcov[1,2]
>          alpha<-1-conf.level
>          zalpha.2 <- -qnorm(alpha/2)
>          gamma <- zalpha.2^2 * var.b1 / (b1^2)
>          eta = family(obj)$linkfun(p)  #based on calcs in V&R's dose.p
> 
>          EC50 <- (eta-b0)/b1
> 
>          const1 <- (gamma/(1-gamma))*(EC50 + cov.b0.b1/var.b1)
> 
>          const2a <- var.b0 + 2*cov.b0.b1*EC50 + var.b1*EC50^2 -
>                     gamma*(var.b0 - cov.b0.b1^2/var.b1)
> 
>          const2 <- zalpha.2/( (1-gamma)*abs(b1) )*sqrt(const2a)
> 
>          LCL <- EC50 + const1 - const2
>          UCL <- EC50 + const1 + const2
> 
>          conf.pts <- c(LCL,EC50,UCL)
>          names(conf.pts) <- c("Lower","EC50","Upper")
> 
>          return(conf.pts,conf.level,call=call)
>  }
> 
> 
> Thanks
> 
> Stephen Cox
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From jerk_alert at hotmail.com  Thu Jul 14 15:46:01 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Thu, 14 Jul 2005 13:46:01 +0000
Subject: [R] Keeping memory usage low for a big script
Message-ID: <BAY101-F219076C06D78318BB0B0E4E8D10@phx.gbl>

Hi all,

I've got a script that I run on several computers, some with much less ram 
than others, and I would like to try and keep the memory usage as low as 
possible. The script creates variables to store intermediate results of 
calculations, and then at the end writes a text file with the results of the 
calculations.

Is a quick and dirty way to accomplish this to use rm() to delete objects no 
longer in use, and then run gc()? ...or is there a better way?

Thanks in advance,
Ken



From RRoa at fisheries.gov.fk  Thu Jul 14 13:49:27 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 14 Jul 2005 09:49:27 -0200
Subject: [R] Where's iris?
Message-ID: <03DCBBA079F2324786E8715BE538968A068E82@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: 13 July 2005 20:30
> To: Ruben Roa
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Where's iris?
> 
> 
> On 7/13/05, Ruben Roa <RRoa at fisheries.gov.fk> wrote:
> > Hi:
> > Where is the iris data set actually
> > located in the R 2.1.0 folder (under W XP)?
> > Is it a text file or it is a binary file?
> > Ruben
> 
> Uwe has already explained how to get it in text
> form; however, if you are curious about its original
> format in R then its actually stored in iris.R as 
> R source code which you can view at:
> 
>   https://svn.r-project.org/R/trunk/src/library/datasets/data/iris.R
> 
> (or download the entire R source and get it from there).

The iris.R in the website and the iris.R i get from dump("iris",file="iris.R")
from R 2.1.0 W XP are different (the file from dump() describes the factors using 
structure(as.integer(c()),.Label=c(),class="factor") 
whereas the file in the website uses 
gl=(,,label=c()),
but the difference did not matter. I wanted to emulate the file format in a txt 
*.R file with my own data and i could do it with the most helpful advice by you 
and Uwe. Thank you!
Ruben



From murdoch at stats.uwo.ca  Thu Jul 14 15:57:27 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Jul 2005 09:57:27 -0400
Subject: [R] Keeping memory usage low for a big script
In-Reply-To: <BAY101-F219076C06D78318BB0B0E4E8D10@phx.gbl>
References: <BAY101-F219076C06D78318BB0B0E4E8D10@phx.gbl>
Message-ID: <42D66F47.8090304@stats.uwo.ca>

On 7/14/2005 9:46 AM, Ken Termiso wrote:
> Hi all,
> 
> I've got a script that I run on several computers, some with much less ram 
> than others, and I would like to try and keep the memory usage as low as 
> possible. The script creates variables to store intermediate results of 
> calculations, and then at the end writes a text file with the results of the 
> calculations.
> 
> Is a quick and dirty way to accomplish this to use rm() to delete objects no 
> longer in use, and then run gc()? ...or is there a better way?

Using rm() is good.  gc() will run automatically as needed; the main 
reason to run it yourself is just to make calculation timings more 
reproducible.  It will also affect memory fragmentation, most likely 
beneficially, but not necessarily...

Duncan Murdoch



From jcm68 at cam.ac.uk  Thu Jul 14 16:01:04 2005
From: jcm68 at cam.ac.uk (J-C. Marioni)
Date: 14 Jul 2005 15:01:04 +0100
Subject: [R] Fwd: Re: Problem installing R packages
Message-ID: <Prayer.1.0.14.0507141501040.12304@hermes-1.csi.cam.ac.uk>

Hi,

I am trying to install the R libraries "rmutil" and "repeated" on a Mac OS 
X version 10.4.1 (which has the latest version of the Mac Developer tools 
installed) and I am having trouble compiling the libraries.

The error message I receive is as follows (I have only included the error 
message when I try and install the rmutil library):

............................


* Installing *source* package 'rmutil' ... ** libs gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include 
-fno-common -g -O2 -c cutil.c -o cutil.o gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include 
-fno-common -g -O2 -c dist.c -o dist.o g77 -fno-common -g -O2 -c gettvc.f 
-o gettvc.o gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include 
-fno-common -g -O2 -c romberg.c -o romberg.o gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include 
-fno-common -g -O2 -c toms614.c -o toms614.o gcc-3.3 -bundle 
-flat_namespace -undefined suppress -o rmutil.so cutil.o dist.o gettvc.o 
romberg.o toms614.o -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
-lg2c -lSystem
         package successfully installed ld: dist.o has external relocation 
entries in non-writable section (__TEXT,__text) for symbols: restFP saveFP 
make: *** [rmutil.so] Error 1 ERROR: compilation failed for package 
'rmutil'

......................

I contacted Jim Lindsey (the package author) who wasn't able to suggest a 
solution. Hence, does anyone have an idea what could be causing this error 
or does anyone have a Mac binary for these libraries?

Thanks,

John Marioni

-- 
John Marioni
PhD Student
Computational Biology Group
Department of Applied Mathematics and Theoretical Physics
University of Cambridge
Wilberforce Road, Cambridge, CB3 0WA

Email: J.Marioni at damtp.cam.ac.uk
Phone: +44 (0)1223 763391



From macq at llnl.gov  Thu Jul 14 16:04:14 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 14 Jul 2005 07:04:14 -0700
Subject: [R] Plotting greek symbols in plot titles, labels, etc.
In-Reply-To: <448071208107374B96ED90585EEBA912848FCF@NLVDHX84.sn-eu.asml.com>
	<42D65F75.7020703@jhsph.edu>
References: <448071208107374B96ED90585EEBA912848FCF@NLVDHX84.sn-eu.asml.com>
	<448071208107374B96ED90585EEBA912848FCF@NLVDHX84.sn-eu.asml.com>
	<42D65F75.7020703@jhsph.edu>
Message-ID: <p06210201befc213ee068@[128.115.153.6]>

Type
    demo(plotmath)
at the R prompt.

At 2:45 PM +0200 7/14/05, Roy Werkman wrote:
>Hi,
>
>Can anyone please tell me how to use the escape sequences (e.g. \\*a for
>alfa) for plotting Greek symbols in for example plot titles? I have read
>the reference manual, but can't seem to get it to work.
>
>Thanx!
>
>Roy

At 8:49 AM -0400 7/14/05, Roger D. Peng wrote:
>Take a look at '?plotmath'.
>
>-roger
>Roy Werkman wrote:
>  > Hi,
>>
>>  Can anyone please tell me how to use the escape sequences (e.g. \\*a for
>>  alfa) for plotting Greek symbols in for example plot titles? I have read
>>  the reference manual, but can't seem to get it to work.
>>
>>  Thanx!
>>
>>  Roy
>>
>>
>
-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Gary.Nelson at state.ma.us  Thu Jul 14 16:31:13 2005
From: Gary.Nelson at state.ma.us (Nelson, Gary (FWE))
Date: Thu, 14 Jul 2005 10:31:13 -0400
Subject: [R] read.xport
Message-ID: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650B9@ES-MSG-008.es.govt.state.ma.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/bf802da8/attachment.pl

From buser at stat.math.ethz.ch  Thu Jul 14 16:56:54 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 14 Jul 2005 16:56:54 +0200
Subject: [R] maps drawing
In-Reply-To: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
References: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
Message-ID: <17110.32054.63277.627332@stat.math.ethz.ch>

There is also the package maptools if you want or need to read
ESRI shapefiles.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


m p writes:
 > Hello,
 > is there a package in R that would allow map drawing:
 > coastlines, country/state boundaries, maybe
 > topography,
 > rivers etc?
 > Thanks for any guidance,
 > Mark
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ecoinformatics at gmail.com  Thu Jul 14 17:19:12 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Thu, 14 Jul 2005 17:19:12 +0200
Subject: [R] maps drawing
In-Reply-To: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
References: <20050713161533.58618.qmail@web51006.mail.yahoo.com>
Message-ID: <15f8e67d05071408196b264129@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/43a168fc/attachment.pl

From j.van_den_hoff at fz-rossendorf.de  Thu Jul 14 17:19:51 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Thu, 14 Jul 2005 17:19:51 +0200
Subject: [R] unexpected par('pin') behaviour
In-Reply-To: <17110.16321.138943.126111@stat.math.ethz.ch>
References: <42D51E9A.8070702@fz-rossendorf.de>
	<17110.16321.138943.126111@stat.math.ethz.ch>
Message-ID: <42D68297.8030900@fz-rossendorf.de>

Martin Maechler wrote:
>>>>>>"joerg" == joerg van den hoff <j.van_den_hoff at fz-rossendorf.de>
>>>>>>    on Wed, 13 Jul 2005 16:00:58 +0200 writes:
> 
> 
>     joerg> hi everybody,
>     joerg> I noticed the following: in one of my scripts 'layout' is used to 
>     joerg> generate a (approx. square) grid of variable dimensions (depending on 
>     joerg> no. of input files). if the no. of subplots (grid cells) becomes 
>     joerg> moderately large  (say > 9) I use a construct like
> 
>     joerg>   ###layout grid computation and set up occurs here###
>     joerg>   ...
>     joerg>   opar <- par(no.readonly = T);
>     joerg>   on.exit(par(opar))
>     joerg>   par(mar=c(4.1, 4.1, 1.1, .1))
>     joerg>   ###plotting occurs here####
>     joerg>   ...
> 
>     joerg> to reduce the figure margins to achieve a more
>     joerg> compact display. apart from 'mar' no other par()
>     joerg> setting is modified.
> 
> yet another example of using  par('no.readonly') when it's not
> needed and inefficient.

might be. but at least it is immune against modifying some more 'par' 
settings in the course of modfications  at some other place in the 
programm. inefficiency: should be at the ppm level of total cpu-usage in 
my case, :-). what's so bad with copying back and forth this moderately 
large vector?
> 
> Replacing the above by
> 
>     ###layout grid computation and set up occurs here###    
>     ...
>      op <- par(mar=c(4.1, 4.1, 1.1, .1))
>      on.exit(par(op))
>     ###plotting occurs here####
> 

> will be much more efficient and even solve your problem with "pin".
> 
right (solves the problem). I'll adopt this change for the time being. 
thank you.

> But then, yes, there might be another par() problem hidden in
> your code / example, 
> but unfortunately you have not specified reproducible code.
> 
> 
>     joerg> this works fine until the total number of subplots becomes too large 
>     joerg> ("large" depending on the current size of the X11() graphics device 
>     joerg> window, e.g. 7 x 6 subplots for the default size fo x11()).
> 
>     joerg> I then get the error message (only _after_ all plots are correctly 
>     joerg> displayed, i.e. obviously during execution of the above on.exit() call)
> 
>     joerg> Error in par(opar) :
>     joerg> invalid value specified for graphics parameter "pin"
> 
>     joerg> and par("pin") yields:
> 
>     joerg> [1]  0.34864 -0.21419
> 
> you mean *after* all the plotting ,  not the "pin" values in
> 'opar', right?

yes
> 
>     joerg> which indeed is invalid (negative 2nd component).
> 
>     joerg> I'm aware of this note from ?par:
> 
>     joerg> The effect of restoring all the (settable) graphics parameters as
>     joerg> in the examples is hard to predict if the device has been resized.
>     joerg> Several of them are attempting to set the same things in different
>     joerg> ways, and those last in the alphabet will win.  In particular, the
>     joerg> settings of 'mai', 'mar', 'pin', 'plt' and 'pty' interact, as do
>     joerg> the outer margin settings, the figure layout and figure region
>     joerg> size.
> 
> {{which shows you the known  but not widely known fact that
>   traditional par() based graphics are ``flawed by design''
>   and that's why there is the package "grid" for better 
>   designed graphics

... which seems to my simple mind a lot more complicated to come to 
terms with than the graphics package. I understand that grid is more 
powerful but the subset of functionality provided by 'graphics' seems 
more difficult to use in 'grid'. wrong or right?
> }}
> 
>     joerg> but my problem occurs without any resizing of the
>     joerg> x11() window prior to resetting par to par(opar).
> 
> It still would be interesting to get a reproducible example
> here, as the posting guide asks for.
> 
===========cut====================
graphics.off()

f <- function(n=7, m=6) {
    nm <- n*m
    layout(matrix(1:(nm),n,m))
    opar <- par(no.readonly = T)
    on.exit(par(opar))
    par(mar = c(4.1, 4.1, 1.1, 0.1))
    for (i in 1:nm) plot(i, pch=(i-1)%%25+1)
    layout(1)
}
f(5) #good
par('pin')
f()  #bad (at least for x11() default size)
par('pin')
===========cut====================
> Martin

thanks for bothering.
joerg
> 
> 
>     joerg> any ideas, what is going on?
> 
>     joerg> platform powerpc-apple-darwin7.9.0
>     joerg> arch     powerpc
>     joerg> os       darwin7.9.0
>     joerg> system   powerpc, darwin7.9.0
>     joerg> status   Patched
> 		    ^^^^^^^
> I hope that this is not the basic problem 
no, don't think so. that concerned the MacOS GUI, I believe.
> 
>     joerg> major    2
>     joerg> minor    1.0
>     joerg> year     2005
>     joerg> month    05
>     joerg> day      12
>     joerg> language R
> 
>     joerg> regards,
> 
>     joerg> joerg
> 
>



From tlumley at u.washington.edu  Thu Jul 14 17:31:42 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 14 Jul 2005 08:31:42 -0700 (PDT)
Subject: [R] memory problem
In-Reply-To: <42D657B0.5090104@stats.uwo.ca>
References: <7ff971d4050714041925848073@mail.gmail.com>
	<42D657B0.5090104@stats.uwo.ca>
Message-ID: <Pine.A41.4.61b.0507140828300.152456@homer05.u.washington.edu>

On Thu, 14 Jul 2005, Duncan Murdoch wrote:

> On 7/14/2005 7:19 AM, Ginters wrote:
>> Why does memory need so much (1.6 GB) space? How can I enlarge it? Is it
>> possible to allocate a part of memory used to the hard drive? Or, is the
>> trouble only with my script?
>
> This sounds like a problem with the regressionestimator function, which
> I think comes from the sampling package.  You'll need to contact the
> maintainer of the package to find out why it needs so much memory, and
> whether there's a way to get what you want without it.
>


You can do regression calibration of sampling weights using the 
calibrate() function in the survey package. That might use less memory.


 	-thomas



From ripley at stats.ox.ac.uk  Thu Jul 14 17:33:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2005 16:33:52 +0100 (BST)
Subject: [R] Fwd: Re: Problem installing R packages
In-Reply-To: <Prayer.1.0.14.0507141501040.12304@hermes-1.csi.cam.ac.uk>
References: <Prayer.1.0.14.0507141501040.12304@hermes-1.csi.cam.ac.uk>
Message-ID: <Pine.LNX.4.61.0507141621390.9898@gannet.stats>

What Fortran compiler (g77) are you using, obtained from where?  See

http://www.astro.gla.ac.uk/users/norman/note/2004/restFP/

Your lines are wrapped in ways that make parsing impossible, but my guess 
at the link line does not include -lcc_dynamic.

See also the list archives, e.g.

http://tolstoy.newcastle.edu.au/R/help/05/06/7345.html

and of course the r-sig-mac list for the mysteries of MacOS X.  People 
seem to be moving to gcc4/gfortran on that platform.


On Thu, 14 Jul 2005, J-C. Marioni wrote:

> Hi,
>
> I am trying to install the R libraries "rmutil" and "repeated" on a Mac OS
> X version 10.4.1 (which has the latest version of the Mac Developer tools
> installed) and I am having trouble compiling the libraries.
>
> The error message I receive is as follows (I have only included the error
> message when I try and install the rmutil library):
>
> ............................
>
>
> * Installing *source* package 'rmutil' ... ** libs gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include
> -fno-common -g -O2 -c cutil.c -o cutil.o gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include
> -fno-common -g -O2 -c dist.c -o dist.o g77 -fno-common -g -O2 -c gettvc.f
> -o gettvc.o gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include
> -fno-common -g -O2 -c romberg.c -o romberg.o gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include -I/usr/local/include
> -fno-common -g -O2 -c toms614.c -o toms614.o gcc-3.3 -bundle
> -flat_namespace -undefined suppress -o rmutil.so cutil.o dist.o gettvc.o
> romberg.o toms614.o -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
> -lg2c -lSystem
>         package successfully installed ld: dist.o has external relocation
> entries in non-writable section (__TEXT,__text) for symbols: restFP saveFP
> make: *** [rmutil.so] Error 1 ERROR: compilation failed for package
> 'rmutil'
>
> ......................
>
> I contacted Jim Lindsey (the package author) who wasn't able to suggest a
> solution. Hence, does anyone have an idea what could be causing this error
> or does anyone have a Mac binary for these libraries?
>
> Thanks,
>
> John Marioni
>
> -- 
> John Marioni
> PhD Student
> Computational Biology Group
> Department of Applied Mathematics and Theoretical Physics
> University of Cambridge
> Wilberforce Road, Cambridge, CB3 0WA
>
> Email: J.Marioni at damtp.cam.ac.uk
> Phone: +44 (0)1223 763391
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Jul 14 17:39:15 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 14 Jul 2005 08:39:15 -0700 (PDT)
Subject: [R] DSC 2005 registration
Message-ID: <Pine.A41.4.61b.0507140836301.152456@homer05.u.washington.edu>


Early registration discount for DSC 2005: Directions in Statistical 
Computing, August 13-14, in Seattle closes on July 18.

Registration is online, from the conference web page at
   http://depts.washington.edu/dsc2005/

Titles and abstracts for presentations are at
  http://depts.washington.edu/dsc2005/program.html

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From dieter.menne at menne-biomed.de  Thu Jul 14 17:36:20 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 14 Jul 2005 15:36:20 +0000 (UTC)
Subject: [R] Coxph with factors
References: <20050714064634.A705B44FA7@melmail.itga.com.au>
Message-ID: <loom.20050714T173524-264@post.gmane.org>

Kylie-Anne Richards <kar <at> itga.com.au> writes:

> 
> I am fitting a coxph model with factors. I am running into problems when
> using 'survfit'. I am unsure how R is treating the factors when I fit, say:
> >        DATA<-data.frame(time.sec,done,f.pom=factor(f.pom),po,vo)
> >        final<-coxph(Surv(time.sec,done)~f.pom*vo+po,data=DATA)
> >         final.surv<-survfit((final), individual=T,conf.type="log-log")
> >         print(final.surv)
....

Better chances to get a reply when you tell us what problems you are running 
into.

Dieter Menne



From dieter.menne at menne-biomed.de  Thu Jul 14 17:38:26 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 14 Jul 2005 15:38:26 +0000 (UTC)
Subject: [R] stripchart usage and alternatives
References: <27db823f05071317147b981a39@mail.gmail.com>
Message-ID: <loom.20050714T173646-616@post.gmane.org>

Mike R <mike.rstat <at> gmail.com> writes:

.....
> stripchart(r~u,vertical=TRUE,pch=21)
> stripchart(rm~levels(uf),vertical=TRUE,pch=3,add=TRUE)
> 
> the above code creates a scatter plot of nominal data
> 
> are there alternatives to generate the same or similar
> "kind" of figure? 

How about stripplot in lattice/trellis?


Dieter



From jcm68 at cam.ac.uk  Thu Jul 14 17:45:37 2005
From: jcm68 at cam.ac.uk (J-C. Marioni)
Date: 14 Jul 2005 16:45:37 +0100
Subject: [R] Fwd: Re: Problem installing R packages
In-Reply-To: <Pine.LNX.4.61.0507141621390.9898@gannet.stats>
References: <Prayer.1.0.14.0507141501040.12304@hermes-1.csi.cam.ac.uk>
	<Pine.LNX.4.61.0507141621390.9898@gannet.stats>
Message-ID: <Prayer.1.0.14.0507141645370.25414@hermes-1.csi.cam.ac.uk>


We used the Fortran compiler that came from the customised option when 
installing from the mac dmg

g77 --version 

GNU Fortran (GCC) 3.4.2

We will look into the links you provided.

Thanks,

John



On Jul 14 2005, Prof Brian Ripley wrote:

> What Fortran compiler (g77) are you using, obtained from where?  See
> 
> http://www.astro.gla.ac.uk/users/norman/note/2004/restFP/
> 
> Your lines are wrapped in ways that make parsing impossible, but my guess 
> at the link line does not include -lcc_dynamic.
> 
> See also the list archives, e.g.
> 
> http://tolstoy.newcastle.edu.au/R/help/05/06/7345.html
> 
> and of course the r-sig-mac list for the mysteries of MacOS X.  People 
> seem to be moving to gcc4/gfortran on that platform.
> 
> 
> On Thu, 14 Jul 2005, J-C. Marioni wrote:
> 
> > Hi,
> >
> > I am trying to install the R libraries "rmutil" and "repeated" on a 
> > Mac OS X version 10.4.1 (which has the latest version of the Mac 
> > Developer tools installed) and I am having trouble compiling the 
> > libraries.
> >
> > The error message I receive is as follows (I have only included the 
> > error message when I try and install the rmutil library):
> >
> > ............................
> >
> >
> > * Installing *source* package 'rmutil' ... ** libs gcc-3.3 
> > -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include 
> > -I/usr/local/include -fno-common -g -O2 -c cutil.c -o cutil.o gcc-3.3 
> > -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include 
> > -I/usr/local/include -fno-common -g -O2 -c dist.c -o dist.o g77 
> > -fno-common -g -O2 -c gettvc.f -o gettvc.o gcc-3.3 -no-cpp-precomp 
> > -I/Library/Frameworks/R.framework/Resources/include 
> > -I/usr/local/include -fno-common -g -O2 -c romberg.c -o romberg.o 
> > gcc-3.3 -no-cpp-precomp 
> > -I/Library/Frameworks/R.framework/Resources/include 
> > -I/usr/local/include -fno-common -g -O2 -c toms614.c -o toms614.o 
> > gcc-3.3 -bundle -flat_namespace -undefined suppress -o rmutil.so 
> > cutil.o dist.o gettvc.o romberg.o toms614.o 
> > -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 -lg2c -lSystem
> >         package successfully installed ld: dist.o has external 
> > relocation entries in non-writable section (__TEXT,__text) for symbols: 
> > restFP saveFP make: *** [rmutil.so] Error 1 ERROR: compilation failed 
> > for package 'rmutil'
> >
> > ......................
> >
> > I contacted Jim Lindsey (the package author) who wasn't able to 
> > suggest a solution. Hence, does anyone have an idea what could be 
> > causing this error or does anyone have a Mac binary for these 
> > libraries?
> >
> > Thanks,
> >
> > John Marioni
> >
> > -- 
> > John Marioni
> > PhD Student
> > Computational Biology Group
> > Department of Applied Mathematics and Theoretical Physics
> > University of Cambridge
> > Wilberforce Road, Cambridge, CB3 0WA
> >
> > Email: J.Marioni at damtp.cam.ac.uk
> > Phone: +44 (0)1223 763391
> >
> > ______________________________________________ 
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting 
> > guide! http://www.R-project.org/posting-guide.html
> >
> 
>



From greg.snow at ihc.com  Thu Jul 14 18:00:04 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 14 Jul 2005 10:00:04 -0600
Subject: [R] New Package: TeachingDemos
Message-ID: <s2d637b4.036@lp-msg1.co.ihc.com>

This is to announce the new package TeachingDemos now available on
CRAN.

This is a collection of functions that I have used to demonstrate
different ideas in various classes taught.  Some of the first ones I
wrote just to help me visualize what was happening, they latter were
usefull in teaching others (both in a 1 on 1 setting, and in classroom
type settings).  

A few examples (more are available in the package):

loess.demo:  This was one of the first I wrote, mainly for myself, but
others have liked it also.  This function creates a scatterplot and
shows the loess fit, then when you click on a point in the plot it shows
the window around that point, the weights used for the local regression
at that point and the local regression used to predict at that point. 
Additional clicks will show the same info for other x-values.

vis.gamma, vis.binom, vis.norm, and vis.t:  These functions plot the
given distribution (and related distributions) then creates a Tk slider
box that allows you to change the parameters to see how the distribution
function changes.  With overlayed plots you can also see the
relationship between related distributions.

put.points.demo:  Start with a scatterplot that shows the correlation
coefficient and least squares line, then add, delete, move points
interactively to see how the correlation and regression line change.

Hopefully others will find these demonstrations useful, let me know if
you have any suggestions for additional demonstrations or improvements
on the current ones.


Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111



From RVARADHAN at JHMI.EDU  Thu Jul 14 18:15:33 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Thu, 14 Jul 2005 12:15:33 -0400
Subject: [R] A statistical modeling problem
Message-ID: <0IJM009GJL5XOH@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/0f7ae74f/attachment.pl

From br44114 at gmail.com  Thu Jul 14 18:35:07 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 14 Jul 2005 12:35:07 -0400
Subject: [R] read.xport
Message-ID: <8d5a3635050714093544924cfb@mail.gmail.com>

How about avoiding SAS XPORT altogether and exporting everything in
the simple, clean, non-proprietary, extremely reliable,
platform-independent ... etc text format (CSV, tab delimited etc)?


> -----Original Message-----
> From: Nelson, Gary (FWE) [mailto:Gary.Nelson at state.ma.us] 
> Sent: Thursday, July 14, 2005 10:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] read.xport
> 
> 
> I am trying to import data from a SAS XPORT file that 
> contains 24 SAS files.
> When I use the "read.xport" procedure only about 16 data 
> frames (components)
> are created.  Any suggestions?
> 
>  
> 
>  
> 
> **************************************************************
> ***********
> 
> Gary A. Nelson, Ph.D
> 
> Massachusetts Division of Marine Fisheries
> 
> 30 Emerson Avenue
> 
> Gloucester, MA 01930
> 
> Phone: (978) 282-0308 x114
> 
> Fax: (617) 727-3337
> 
> Email: Gary.Nelson at state.ma.us
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Thu Jul 14 18:45:35 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 14 Jul 2005 11:45:35 -0500
Subject: [R] read.xport
In-Reply-To: <8d5a3635050714093544924cfb@mail.gmail.com>
References: <8d5a3635050714093544924cfb@mail.gmail.com>
Message-ID: <42D696AF.8000405@vanderbilt.edu>

bogdan romocea wrote:
> How about avoiding SAS XPORT altogether and exporting everything in
> the simple, clean, non-proprietary, extremely reliable,
> platform-independent ... etc text format (CSV, tab delimited etc)?

I hope the problem is fixed in the latest version of foreign (no version 
info was given).  In case it's not, you may want to look at the 
sasxport.get function in the Hmisc package.

Frank

> 
> 
> 
>>-----Original Message-----
>>From: Nelson, Gary (FWE) [mailto:Gary.Nelson at state.ma.us] 
>>Sent: Thursday, July 14, 2005 10:31 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] read.xport
>>
>>
>>I am trying to import data from a SAS XPORT file that 
>>contains 24 SAS files.
>>When I use the "read.xport" procedure only about 16 data 
>>frames (components)
>>are created.  Any suggestions?
>>
>> 
>>
>> 
>>
>>**************************************************************
>>***********
>>
>>Gary A. Nelson, Ph.D
>>
>>Massachusetts Division of Marine Fisheries
>>
>>30 Emerson Avenue
>>
>>Gloucester, MA 01930
>>
>>Phone: (978) 282-0308 x114
>>
>>Fax: (617) 727-3337
>>
>>Email: Gary.Nelson at state.ma.us
>>
>> 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From kerryrekky at yahoo.com  Thu Jul 14 19:01:46 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Thu, 14 Jul 2005 10:01:46 -0700 (PDT)
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <Pine.SGI.4.40.0507131542140.8483928-100000@origin.chass.utoronto.ca>
Message-ID: <20050714170146.93155.qmail@web51805.mail.yahoo.com>

Thank you for thinking about the problem for me.
However, I have found that your method doesn't work at
all.

You may test the following example:

x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
x1=rep(x1,4)
x2=rep(x2,4)
temp=data.frame(x1,x2)
temp1=table(temp)
plot(temp$x1,temp$x2,cex=0)
text(as.numeric(rownames(temp1)),
as.numeric(colnames(temp1)), temp1)

what I got here is not what I wanted. You may compare
with 
plot(x1,x2)

I actually want some plots similar to what SAS proc
plot produced.

Does anybody have a clue of how to do this easily in
R?

--- Jean Eid <jeaneid at chass.utoronto.ca> wrote:

> You can do the following (don't know it this is the
> most efficient way but
> it works)
> 
> temp<-read.table("your file to read the data",
> header=T)
> temp1<-table(temp)
> plot(temp$x, temp$y, cex=0)
> text(as.numeric(rownames(temp1)),
> as.numeric(colnames(temp1)), temp1)
> 
> HTH
> 
> 
> On Wed, 13 Jul 2005, Kerry Bush wrote:
> 
> > Dear R-helper,
> >   I want to plot the following-like data:
> >
> > x y
> > 1 1
> > 1 1
> > 1 2
> > 1 3
> > 1 3
> > 1 4
> > ......
> >
> > In the plot that produced, I don't want to show
> the
> > usual circles or points. Instead, I want to show
> the
> > number of replicates at that point. e.g. at the
> > position of (1,1), there are 2 obsevations, so a
> > number '2' will be displayed in the plot.
> > Is my narrative clear? Is there a way to make the
> plot
> > in R?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
>



From fzhang at doubleclick.net  Thu Jul 14 19:02:35 2005
From: fzhang at doubleclick.net (Zhang, Fan)
Date: Thu, 14 Jul 2005 13:02:35 -0400
Subject: [R] Calculate of data frame
Message-ID: <CB8E43806EA4084A9B90A3CB12DB4F60056E90B2@NYC-EXCLS1.dc1.doubleclick.corp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/ebdac05b/attachment.pl

From jsorkin at grecc.umaryland.edu  Thu Jul 14 19:05:03 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 14 Jul 2005 13:05:03 -0400
Subject: [R] Error running lme.
Message-ID: <s2d66314.007@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/451addff/attachment.pl

From Gary.Nelson at state.ma.us  Thu Jul 14 19:23:53 2005
From: Gary.Nelson at state.ma.us (Nelson, Gary (FWE))
Date: Thu, 14 Jul 2005 13:23:53 -0400
Subject: [R] read.xport
Message-ID: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>

I have the latest version of foreign, but it still doesn't work.  I
quickly tried the Hmisc package, but the same issue arose.  I will delve
into the Hmisc package further.

Thanks.
-----Original Message-----
From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu] 
Sent: Thursday, July 14, 2005 11:46 AM
To: bogdan romocea
Cc: Nelson, Gary (FWE); R-help at stat.math.ethz.ch
Subject: Re: [R] read.xport


bogdan romocea wrote:
> How about avoiding SAS XPORT altogether and exporting everything in 
> the simple, clean, non-proprietary, extremely reliable, 
> platform-independent ... etc text format (CSV, tab delimited etc)?

I hope the problem is fixed in the latest version of foreign (no version

info was given).  In case it's not, you may want to look at the 
sasxport.get function in the Hmisc package.

Frank

> 
> 
> 
>>-----Original Message-----
>>From: Nelson, Gary (FWE) [mailto:Gary.Nelson at state.ma.us]
>>Sent: Thursday, July 14, 2005 10:31 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] read.xport
>>
>>
>>I am trying to import data from a SAS XPORT file that
>>contains 24 SAS files.
>>When I use the "read.xport" procedure only about 16 data 
>>frames (components)
>>are created.  Any suggestions?
>>
>> 
>>
>> 
>>
>>**************************************************************
>>***********
>>
>>Gary A. Nelson, Ph.D
>>
>>Massachusetts Division of Marine Fisheries
>>
>>30 Emerson Avenue
>>
>>Gloucester, MA 01930
>>
>>Phone: (978) 282-0308 x114
>>
>>Fax: (617) 727-3337
>>
>>Email: Gary.Nelson at state.ma.us
>>
>> 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt
University



From efg at stowers-institute.org  Thu Jul 14 19:22:49 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 14 Jul 2005 12:22:49 -0500
Subject: [R] Partek has Dunn-Sidak Multiple Test Correction. Is this the
	same/similar to any of R's p.adjust.methods?
Message-ID: <db671a$bjl$1@sea.gmane.org>

The Partek package (www.partek.com) allows only two selections for Multiple
Test Correction:  Bonferroni and Dunn-Sidak.  Can anyone suggest why Partek
implemented Dunn-Sidak and not the other methods that R has?  Is there any
particular advantage to the Dunn-Sidak method?
R knows about these methods (in R 2.1.1):

> p.adjust.methods
[1] "holm" "hochberg" "hommel" "bonferroni" "BH" "BY" "fdr"
[8] "none"

BH is Benjamini & Hochberg (1995) and is also called "fdr" (I wish R's
documentation said this clearly).  BY is Benjamini & Yekutieli (2001).

I found a few hits from Google on Dunn-Sidak, but I'm curious if anyone can
tell me on a "conservative-liberal" scale, where the Dunn-Sidak method
falls? My guess is it's less conservative than Bonferroni (but aren't all
the other methods?), but how does it compare to the other methods?

A limited numerical experiment suggested this order to me:  bonferroni (most
conservative), hochberg and holm about the same, BY, BH (also called fdr),
and then none.

Thanks for any of  thoughts on this.

efg



From nlin at math.wustl.edu  Thu Jul 14 19:24:20 2005
From: nlin at math.wustl.edu (Nan Lin)
Date: Thu, 14 Jul 2005 12:24:20 -0500
Subject: [R] East Asian language
Message-ID: <6.1.0.6.2.20050714122036.037e38f0@math.wustl.edu>

Dear all,

I just installed R 2.1.1. The installation program automatically recognized 
my Windows XP was using Chinese language, so now my R console displays 
everything in Chinese. How can I still let R console display in English 
without modifying my Window XP language setup? Thank you so much!

Best,

Nan



From lu.yuefeng at gmail.com  Thu Jul 14 19:25:36 2005
From: lu.yuefeng at gmail.com (Lu Yuefeng)
Date: Thu, 14 Jul 2005 13:25:36 -0400
Subject: [R] Does R have ANOVA permutation tests?
Message-ID: <bc808e8d05071410253c09ddde@mail.gmail.com>

Hi list,

    Does anybody know if R has functions to do the ANOVA permutation
test?  I googled and found R has the "vegan" package to do "ANOVA like
permutation test for Constrained Correspondence Analysis". But does R
have a function for general ANOVA-like permutation tests? Thanks in
advance!

Yuefeng



From p.dalgaard at biostat.ku.dk  Thu Jul 14 19:28:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jul 2005 19:28:51 +0200
Subject: [R] Calculate of data frame
In-Reply-To: <CB8E43806EA4084A9B90A3CB12DB4F60056E90B2@NYC-EXCLS1.dc1.doubleclick.corp>
References: <CB8E43806EA4084A9B90A3CB12DB4F60056E90B2@NYC-EXCLS1.dc1.doubleclick.corp>
Message-ID: <x23bqhxpnw.fsf@turmalin.kubism.ku.dk>

"Zhang, Fan" <fzhang at doubleclick.net> writes:

> Now I want to get the total count and value for each model/date pair,
> like
> model    count    value    date
> A            6        31.8        7/1/2005
> A            3            10.2    7/2/2005
> B            7            14.2    7/1/2005
> B            10            67.4      7/2/2005
>  
> Anyone can tell me how to do this?

> aggregate(df[,c("count","value")],df[,c("date","model")],sum)
      date model count value
1 7/1/2005     A     6  31.8
2 7/2/2005     A     3  10.2
3 7/1/2005     B     7  14.2
4 7/2/2005     B    10  71.4


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From deepayan.sarkar at gmail.com  Thu Jul 14 19:30:54 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 14 Jul 2005 12:30:54 -0500
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <20050714170146.93155.qmail@web51805.mail.yahoo.com>
References: <Pine.SGI.4.40.0507131542140.8483928-100000@origin.chass.utoronto.ca>
	<20050714170146.93155.qmail@web51805.mail.yahoo.com>
Message-ID: <eb555e6605071410306685a533@mail.gmail.com>

On 7/14/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> Thank you for thinking about the problem for me.
> However, I have found that your method doesn't work at
> all.
> 
> You may test the following example:
> 
> x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> x1=rep(x1,4)
> x2=rep(x2,4)
> temp=data.frame(x1,x2)
> temp1=table(temp)
> plot(temp$x1,temp$x2,cex=0)
> text(as.numeric(rownames(temp1)),
> as.numeric(colnames(temp1)), temp1)
> 
> what I got here is not what I wanted. You may compare
> with
> plot(x1,x2)
> 
> I actually want some plots similar to what SAS proc
> plot produced.
> 
> Does anybody have a clue of how to do this easily in
> R?

Try this:


x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
x1=rep(x1,4)
x2=rep(x2,4)
temp=data.frame(x1,x2)

foo <- subset(as.data.frame(table(temp)), Freq > 0)
foo$x1 <- as.numeric(as.character(foo$x1))
foo$x2 <- as.numeric(as.character(foo$x2))

with(foo, plot(x1, x2, type = "n"))
with(foo, text(x1, x2, lab = Freq))


-Deepayan



From HDoran at air.org  Thu Jul 14 19:40:56 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 14 Jul 2005 13:40:56 -0400
Subject: [R] Error running lme.
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409993C13@dc1ex2.air.org>

John

Your model is not properly specified for lme. You have not included any
random effects or a grouping variable. Let me assume just for sake of
argument that you want to include a random effect for the intercept and
for time. Your lme specification would be 

> fm1 <- lme(Velocity~time, random=~time|ID, gate)

Where ID is a grouping variable. 

I would encourage you to work with lmer, which is now found in the
Matrix package. The call to lmer differs slightly from lme,

> fm1 <- lmer(Velocity~time + (time|ID), gate) 

I hope this is helpful,
Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
Sent: Thursday, July 14, 2005 1:05 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Error running lme.

I am trying to fit lme using R 2.1.1 under Windows 2k. 
I am getting the following message noted below. 
Any suggestions that would help me correct my error would be greatly
appreciated.
Thanks,
John
 
> fit1lme<-lme(Velocity~time,data=gate)
Error in getGroups.data.frame(dataMix, groups) : 
        Invalid formula for groups

 
John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC
 
University of Maryland School of Medicine Division of Gerontology
Baltimore VA Medical Center 10 North Greene Street GRECC (BT/18/GR)
Baltimore, MD 21201-1524
 
410-605-7119
-- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul 14 19:50:41 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 14 Jul 2005 19:50:41 +0200
Subject: [R] East Asian language
In-Reply-To: <6.1.0.6.2.20050714122036.037e38f0@math.wustl.edu>
References: <6.1.0.6.2.20050714122036.037e38f0@math.wustl.edu>
Message-ID: <42D6A5F1.5030500@statistik.uni-dortmund.de>

Nan Lin wrote:

> Dear all,
> 
> I just installed R 2.1.1. The installation program automatically recognized 
> my Windows XP was using Chinese language, so now my R console displays 
> everything in Chinese. How can I still let R console display in English 
> without modifying my Window XP language setup? Thank you so much!


E.g. by setting the envrionment variable
   LANGUAGE=en

Uwe Ligges


> Best,
> 
> Nan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Thu Jul 14 19:53:03 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 14 Jul 2005 12:53:03 -0500
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <eb555e6605071410306685a533@mail.gmail.com>
References: <Pine.SGI.4.40.0507131542140.8483928-100000@origin.chass.utoronto.ca>
	<20050714170146.93155.qmail@web51805.mail.yahoo.com>
	<eb555e6605071410306685a533@mail.gmail.com>
Message-ID: <1121363583.3381.22.camel@localhost.localdomain>

On Thu, 2005-07-14 at 12:30 -0500, Deepayan Sarkar wrote:
> On 7/14/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> > Thank you for thinking about the problem for me.
> > However, I have found that your method doesn't work at
> > all.
> > 
> > You may test the following example:
> > 
> > x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> > x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> > x1=rep(x1,4)
> > x2=rep(x2,4)
> > temp=data.frame(x1,x2)
> > temp1=table(temp)
> > plot(temp$x1,temp$x2,cex=0)
> > text(as.numeric(rownames(temp1)),
> > as.numeric(colnames(temp1)), temp1)
> > 
> > what I got here is not what I wanted. You may compare
> > with
> > plot(x1,x2)
> > 
> > I actually want some plots similar to what SAS proc
> > plot produced.
> > 
> > Does anybody have a clue of how to do this easily in
> > R?
> 
> Try this:
> 
> 
> x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> x1=rep(x1,4)
> x2=rep(x2,4)
> temp=data.frame(x1,x2)
> 
> foo <- subset(as.data.frame(table(temp)), Freq > 0)
> foo$x1 <- as.numeric(as.character(foo$x1))
> foo$x2 <- as.numeric(as.character(foo$x2))
> 
> with(foo, plot(x1, x2, type = "n"))
> with(foo, text(x1, x2, lab = Freq))
> 
> 
> -Deepayan


Great solution Deepayan. I was just in the process of working on
something similar.

One quick modification is that the two plot()/text() functions can be
combined into:

  with(foo, plot(x1, x2, pch = as.character(Freq)))

Best regards,

Marc Schwartz



From caobg at email.uc.edu  Thu Jul 14 19:56:25 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Thu, 14 Jul 2005 13:56:25 -0400
Subject: [R] Fwd: Re:  East Asian language
Message-ID: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>



---- Original message ----
>Date: Thu, 14 Jul 2005 19:50:41 +0200
>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>  
>Subject: Re: [R] East Asian language  
>To: Nan Lin <nlin at math.wustl.edu>
>Cc: r-help at stat.math.ethz.ch
>
>Nan Lin wrote:
>
>> Dear all,
>> 
>> I just installed R 2.1.1. The installation program
automatically recognized 
>> my Windows XP was using Chinese language, so now my R
console displays 
>> everything in Chinese. How can I still let R console
display in English 
>> without modifying my Window XP language setup? Thank you so
much!
>
>
>E.g. by setting the envrionment variable
>   LANGUAGE=en
>
Or, like what I did, change XP language setting back to
English from the control panel.

>Uwe Ligges
>
>
>> Best,
>> 
>> Nan
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Thu Jul 14 19:59:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Jul 2005 10:59:43 -0700
Subject: [R] Efficient testing for +ve definiteness
In-Reply-To: <Pine.LNX.4.61.0507140810170.21661@gannet.stats>
References: <Pine.GSO.4.58.0507131517280.11232@xena.hunter.cuny.edu>
	<Pine.LNX.4.61.0507140810170.21661@gannet.stats>
Message-ID: <42D6A80F.2090005@pdf.com>

	  To reinforce Prof. Ripley's comment that, "Knowing the determinant 
does not tell you if the matrix is close to non-positive definite", note 
that the determinant of the negative of the identity matrix, (-diag(k)), 
is (-1)^k;  if k is even, the determinant is positive.  This silly 
example connects to real cases, as for example a 3x3 matrix of rank 1 
with eigenvalues (1, -1e-20, -1e-21).  The last two eigenvalues are 
buried in the noise relative to the largest (under standard double 
precision arithmatec).

	  spencer graves

Prof Brian Ripley wrote:

> On Wed, 13 Jul 2005, Makram Talih wrote:
> 
> 
>>Dear R-users,
>>
>>Is there a preferred method for testing whether a real symmetric matrix is
>>positive definite? [modulo machine rounding errors.]
>>
>>The obvious way of computing eigenvalues via "E <- eigen(A, symmetric=T,
>>only.values=T)$values" and returning the result of "!any(E <= 0)" seems
>>less efficient than going through the LU decomposition invoked in
>>"determinant.matrix(A)" and checking the sign and (log) modulus of the
>>determinant.
>>
>>I suppose this has to do with the underlying C routines. Any thoughts or
>>anecdotes?
> 
> 
> 
> It has to do with what exactly you want to test.  Knowing the determinant 
> does not tell you if the matrix is close to non-positive definite or not.
> For numerical work, a comparison of the smallest eigenvalue to the largest 
> is usually the most useful indication of possible problems in 
> computations.  An alternative is to try a Choleski decomposition, which 
> may be faster but is less informative.
> 
> Given how fast eigenvalues can be computed by current algorithms (and note 
> the comments in ?eigen) I would suggest not worrying about speed until you 
> need to (probably never).
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ligges at statistik.uni-dortmund.de  Thu Jul 14 20:04:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 14 Jul 2005 20:04:36 +0200
Subject: [R] Fwd: Re:  East Asian language
In-Reply-To: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
Message-ID: <42D6A934.2080601@statistik.uni-dortmund.de>

Baoqiang Cao wrote:

> 
> ---- Original message ----
> 
>>Date: Thu, 14 Jul 2005 19:50:41 +0200
>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>  
>>Subject: Re: [R] East Asian language  
>>To: Nan Lin <nlin at math.wustl.edu>
>>Cc: r-help at stat.math.ethz.ch
>>
>>Nan Lin wrote:
>>
>>
>>>Dear all,
>>>
>>>I just installed R 2.1.1. The installation program
> 
> automatically recognized 
> 
>>>my Windows XP was using Chinese language, so now my R
> 
> console displays 
> 
>>>everything in Chinese. How can I still let R console
> 
> display in English 
> 
>>>without modifying my Window XP language setup? Thank you so
> 
> much!
> 
>>
>>E.g. by setting the envrionment variable
>>  LANGUAGE=en
>>
> 
> Or, like what I did, change XP language setting back to
> English from the control panel.

No, please read the question...

Uwe Ligges


> 
> 
>>Uwe Ligges
>>
>>
>>
>>>Best,
>>>
>>>Nan
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From drcarbon at gmail.com  Thu Jul 14 20:07:19 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Thu, 14 Jul 2005 14:07:19 -0400
Subject: [R] Using system to run a stand alone program that requires input
	in Windows
Message-ID: <e89bb7ac0507141107668de598@mail.gmail.com>

Under Windows I want to run a stand alone program that takes a number
of commands from the user. I've been running the program from the
command line using <

C:\Data\> foo.exe < params.txt

where foo is the program is params.txt is a text file with a few lines
(9) of parameters.

I want to run this from R using system a la:

system("foo.exe", input = "params.txt")

but that doesn't do it. What am I missing?

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R



From ligges at statistik.uni-dortmund.de  Thu Jul 14 20:22:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 14 Jul 2005 20:22:44 +0200
Subject: [R] Using system to run a stand alone program that requires
 input in Windows
In-Reply-To: <e89bb7ac0507141107668de598@mail.gmail.com>
References: <e89bb7ac0507141107668de598@mail.gmail.com>
Message-ID: <42D6AD74.2090704@statistik.uni-dortmund.de>

Dr Carbon wrote:

> Under Windows I want to run a stand alone program that takes a number
> of commands from the user. I've been running the program from the
> command line using <
> 
> C:\Data\> foo.exe < params.txt
> 
> where foo is the program is params.txt is a text file with a few lines
> (9) of parameters.
> 
> I want to run this from R using system a la:
> 
> system("foo.exe", input = "params.txt")
>
> but that doesn't do it. What am I missing?

You misinterpreted "input":

   system("foo.exe < params.txt")

should be sufficient. See also ?shell.

Uwe Ligges





> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fzhang at doubleclick.net  Thu Jul 14 20:26:38 2005
From: fzhang at doubleclick.net (Zhang, Fan)
Date: Thu, 14 Jul 2005 14:26:38 -0400
Subject: [R] Calculate of data frame
Message-ID: <CB8E43806EA4084A9B90A3CB12DB4F60056E90B4@NYC-EXCLS1.dc1.doubleclick.corp>

Thanks for your answer, Peter. Now I have a new data frame which is sorted by (model,date) pair. Since the sorting is done to charactors, I have 7/1/2005 followed by 7/10/2005 instead of 7/2/2005. How can I do the aggregation and at the same time, make the result sorted the way I want (i.e, 7/1/2005 followd by 7/2/2005)? 

Thanks,

Fan
-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
Sent: Thursday, July 14, 2005 1:29 PM
To: Zhang, Fan
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Calculate of data frame

"Zhang, Fan" <fzhang at doubleclick.net> writes:

> Now I want to get the total count and value for each model/date pair, 
> like
> model    count    value    date
> A            6        31.8        7/1/2005
> A            3            10.2    7/2/2005
> B            7            14.2    7/1/2005
> B            10            67.4      7/2/2005
>  
> Anyone can tell me how to do this?

> aggregate(df[,c("count","value")],df[,c("date","model")],sum)
      date model count value
1 7/1/2005     A     6  31.8
2 7/2/2005     A     3  10.2
3 7/1/2005     B     7  14.2
4 7/2/2005     B    10  71.4


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From drcarbon at gmail.com  Thu Jul 14 20:35:43 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Thu, 14 Jul 2005 14:35:43 -0400
Subject: [R] Using system to run a stand alone program that requires
	input in Windows
In-Reply-To: <42D6AD74.2090704@statistik.uni-dortmund.de>
References: <e89bb7ac0507141107668de598@mail.gmail.com>
	<42D6AD74.2090704@statistik.uni-dortmund.de>
Message-ID: <e89bb7ac05071411354e782b23@mail.gmail.com>

>    system("foo.exe < params.txt")

Alas that hangs just opens a window with foo.exe;  params.txt is not
passed to it. That's why I thought input might work. Any other ideas.
I'm trying to think of a DOS program that requires input to test this.



From p.dalgaard at biostat.ku.dk  Thu Jul 14 20:46:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jul 2005 20:46:07 +0200
Subject: [R] Partek has Dunn-Sidak Multiple Test Correction. Is this the
	same/similar to any of R's p.adjust.methods?
In-Reply-To: <db671a$bjl$1@sea.gmane.org>
References: <db671a$bjl$1@sea.gmane.org>
Message-ID: <x2y889w7io.fsf@turmalin.kubism.ku.dk>

"Earl F. Glynn" <efg at stowers-institute.org> writes:

> The Partek package (www.partek.com) allows only two selections for Multiple
> Test Correction:  Bonferroni and Dunn-Sidak.  Can anyone suggest why Partek
> implemented Dunn-Sidak and not the other methods that R has?  Is there any
> particular advantage to the Dunn-Sidak method?
> R knows about these methods (in R 2.1.1):
> 
> > p.adjust.methods
> [1] "holm" "hochberg" "hommel" "bonferroni" "BH" "BY" "fdr"
> [8] "none"
> 
> BH is Benjamini & Hochberg (1995) and is also called "fdr" (I wish R's
> documentation said this clearly).  BY is Benjamini & Yekutieli (2001).
> 
> I found a few hits from Google on Dunn-Sidak, but I'm curious if anyone can
> tell me on a "conservative-liberal" scale, where the Dunn-Sidak method
> falls? My guess is it's less conservative than Bonferroni (but aren't all
> the other methods?), but how does it compare to the other methods?

As far as I gather, D-S is exact for independent tests, conservative
for comparisons of group means, and liberal for mutually exclusive
tests (in which case Bonferroni is exact). It is always less
conservative than Bonferroni, but the difference is small for typical
significance levels: when the Bonferroni level is p, the D-S level is

   1 - (1-p/N)^N

and if you put p=0.05 and vary N you'll find that it varies from 0.05
at N=1 down to 0.04877 at N=100000. (Exercise for the students: what
is the limit as N goes to infinity?)

The three H-methods play a somewhat different game, basically by only
requiring multiple-testing adjustment for non-significant tests.  The
FDR methods play yet differently by allowing the per test level to
increase with the number of significant tests.
 
> A limited numerical experiment suggested this order to me:  bonferroni (most
> conservative), hochberg and holm about the same, BY, BH (also called fdr),
> and then none.
> 
> Thanks for any of  thoughts on this.

I'd expect the differences to be fairly small in scenarios where the
global null hypothesis is true (excluding "none"). The main difference
comes in when some of the nulls are actually false. Also, it depends
on your definitions: With the exception of "BY" and "none" the
p.adjust methods agree on the smallest adjusted p value, so have the
same familywise error rate under the global null. If you count the
total number of rejected tests, then you get a difference due to
"cascading" in the non-bonferroni cases.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From drcarbon at gmail.com  Thu Jul 14 20:47:34 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Thu, 14 Jul 2005 14:47:34 -0400
Subject: [R] Using system to run a stand alone program that requires
	input in Windows
In-Reply-To: <e89bb7ac05071411354e782b23@mail.gmail.com>
References: <e89bb7ac0507141107668de598@mail.gmail.com>
	<42D6AD74.2090704@statistik.uni-dortmund.de>
	<e89bb7ac05071411354e782b23@mail.gmail.com>
Message-ID: <e89bb7ac050714114727f4dbda@mail.gmail.com>

Well, I'm confused because I tried to cook up an exaple and it appears
to work as I think it should. If I have a simple script named
myInput.bat:
@echo off
echo Enter y
set /p Input=
if /i "%Input%"=="y" (goto Proceed)
echo y not entered
exit /b
:Proceed
echo y entered

and a text file with just a y in it called y.txt then this works:
 > system("myInput < y.txt", show.output.on.console = T)
Enter y
y entered
 > 

So, I'm baffled.



From ripley at stats.ox.ac.uk  Thu Jul 14 21:01:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2005 20:01:05 +0100 (BST)
Subject: [R] read.xport
In-Reply-To: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>
References: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>
Message-ID: <Pine.LNX.4.61.0507141958430.12253@gannet.stats>

On Thu, 14 Jul 2005, Nelson, Gary (FWE) wrote:

> I have the latest version of foreign, but it still doesn't work.  I

Are you sure: a new version was released a few hours ago?  It may not 
answer your question, but please do give actual version numbers (as the 
posting guide asks).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jul 14 21:03:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2005 20:03:47 +0100 (BST)
Subject: [R] Using system to run a stand alone program that requires
 input in Windows
In-Reply-To: <e89bb7ac05071411354e782b23@mail.gmail.com>
References: <e89bb7ac0507141107668de598@mail.gmail.com>
	<42D6AD74.2090704@statistik.uni-dortmund.de>
	<e89bb7ac05071411354e782b23@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0507142002560.12253@gannet.stats>

You want shell() not system(), I suspect, as you specified a shell command 
and I gather you are using Windows (not `DOS').

On Thu, 14 Jul 2005, Dr Carbon wrote:

>>    system("foo.exe < params.txt")
>
> Alas that hangs just opens a window with foo.exe;  params.txt is not
> passed to it. That's why I thought input might work. Any other ideas.
> I'm trying to think of a DOS program that requires input to test this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kl2176 at columbia.edu  Thu Jul 14 21:05:13 2005
From: kl2176 at columbia.edu (Kaiya Liu)
Date: Thu, 14 Jul 2005 15:05:13 -0400
Subject: [R] test for difference in the order of self generated sequence
Message-ID: <001201c588a6$f70955e0$8cac3b80@PHDXP20025C>

Hello,

I have an experiment in which we ask subjects to generate a list of thoughts
after being exposed to a stimuli. The thoughts were then coded into two
categories (e.g. A & B). The objective is to show that the order in which
thoughts are generated is affected by the experimental conditions. Can
somebody tell me what functions in R can help me do the test or point me to
the appropriate reference? Thanks!

Example data: Subject 1 in condition 1 generated 5 thoughts. The first,
second, and 5th thought is in category A, thought 3 and 4 are in category B.
Subject 2 only generated 4 thoughts.

-------Condition-------------A-----------------B--------
n1       1                                1, 2, 5                  3, 4
n2       2                                3, 4                      1, 2
-----------------------------------------------------------

Kaiya Liu



From f.harrell at vanderbilt.edu  Thu Jul 14 21:05:50 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 14 Jul 2005 14:05:50 -0500
Subject: [R] read.xport
In-Reply-To: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>
References: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>
Message-ID: <42D6B78E.6030301@vanderbilt.edu>

Nelson, Gary (FWE) wrote:
> I have the latest version of foreign, but it still doesn't work.  I
> quickly tried the Hmisc package, but the same issue arose.  I will delve
> into the Hmisc package further.

This is disappointing as we rely on read.xport quite a bit.

You can use the option with sasxport.get that reads csv files. The help 
file has a URL with a full howto.

Frank

> 
> Thanks.
> -----Original Message-----
> From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu] 
> Sent: Thursday, July 14, 2005 11:46 AM
> To: bogdan romocea
> Cc: Nelson, Gary (FWE); R-help at stat.math.ethz.ch
> Subject: Re: [R] read.xport
> 
> 
> bogdan romocea wrote:
> 
>>How about avoiding SAS XPORT altogether and exporting everything in 
>>the simple, clean, non-proprietary, extremely reliable, 
>>platform-independent ... etc text format (CSV, tab delimited etc)?
> 
> 
> I hope the problem is fixed in the latest version of foreign (no version
> 
> info was given).  In case it's not, you may want to look at the 
> sasxport.get function in the Hmisc package.
> 
> Frank
> 
> 
>>
>>
>>>-----Original Message-----
>>>From: Nelson, Gary (FWE) [mailto:Gary.Nelson at state.ma.us]
>>>Sent: Thursday, July 14, 2005 10:31 AM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] read.xport
>>>
>>>
>>>I am trying to import data from a SAS XPORT file that
>>>contains 24 SAS files.
>>>When I use the "read.xport" procedure only about 16 data 
>>>frames (components)
>>>are created.  Any suggestions?
>>>
>>>
>>>
>>>
>>>
>>>**************************************************************
>>>***********
>>>
>>>Gary A. Nelson, Ph.D
>>>
>>>Massachusetts Division of Marine Fisheries
>>>
>>>30 Emerson Avenue
>>>
>>>Gloucester, MA 01930
>>>
>>>Phone: (978) 282-0308 x114
>>>
>>>Fax: (617) 727-3337
>>>
>>>Email: Gary.Nelson at state.ma.us
>>>
>>>
>>>
>>>
>>>	[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list 
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From edgar at math.uprm.edu  Thu Jul 14 21:30:35 2005
From: edgar at math.uprm.edu (Edgar Acuna)
Date: Thu, 14 Jul 2005 16:30:35 -0300
Subject: [R] visual event programming in R
Message-ID: <20050714192756.M77090@math.uprm.edu>


 Hello, 
 One of my students is building a visual environment for the package dprep  
developed by my research group at the University of  Puerto Rico. 
 The environment will be in the style of the Orange (specifically channels 
and tokens design) and  Weka (specifically Knowledge flow environment) 
machine learning/data mining software.  She is using only the tools provided 
by tcl/tk  (active tcl). 
 I wonder if somebody else has built such kind of  environment for a R 
package, and which tools has used? 
 
 Best, 
 Edgar Acuna 
 CASTLE group 
 UPR-Mayaguez 


--
Open WebMail Project (http://openwebmail.org)



From deepayan.sarkar at gmail.com  Thu Jul 14 22:08:13 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 14 Jul 2005 15:08:13 -0500
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <1121363583.3381.22.camel@localhost.localdomain>
References: <Pine.SGI.4.40.0507131542140.8483928-100000@origin.chass.utoronto.ca>
	<20050714170146.93155.qmail@web51805.mail.yahoo.com>
	<eb555e6605071410306685a533@mail.gmail.com>
	<1121363583.3381.22.camel@localhost.localdomain>
Message-ID: <eb555e6605071413081134f3ba@mail.gmail.com>

On 7/14/05, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> On Thu, 2005-07-14 at 12:30 -0500, Deepayan Sarkar wrote:
> > On 7/14/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> > > Thank you for thinking about the problem for me.
> > > However, I have found that your method doesn't work at
> > > all.
> > >
> > > You may test the following example:
> > >
> > > x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> > > x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> > > x1=rep(x1,4)
> > > x2=rep(x2,4)
> > > temp=data.frame(x1,x2)
> > > temp1=table(temp)
> > > plot(temp$x1,temp$x2,cex=0)
> > > text(as.numeric(rownames(temp1)),
> > > as.numeric(colnames(temp1)), temp1)
> > >
> > > what I got here is not what I wanted. You may compare
> > > with
> > > plot(x1,x2)
> > >
> > > I actually want some plots similar to what SAS proc
> > > plot produced.
> > >
> > > Does anybody have a clue of how to do this easily in
> > > R?
> >
> > Try this:
> >
> >
> > x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> > x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> > x1=rep(x1,4)
> > x2=rep(x2,4)
> > temp=data.frame(x1,x2)
> >
> > foo <- subset(as.data.frame(table(temp)), Freq > 0)
> > foo$x1 <- as.numeric(as.character(foo$x1))
> > foo$x2 <- as.numeric(as.character(foo$x2))
> >
> > with(foo, plot(x1, x2, type = "n"))
> > with(foo, text(x1, x2, lab = Freq))
> >
> >
> > -Deepayan
> 
> 
> Great solution Deepayan. I was just in the process of working on
> something similar.
> 
> One quick modification is that the two plot()/text() functions can be
> combined into:
> 
>   with(foo, plot(x1, x2, pch = as.character(Freq)))

Only as long as all(Freq < 10) (I've been bitten by this before. :-) ).

Deepayan



From mschwartz at mn.rr.com  Thu Jul 14 22:16:39 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 14 Jul 2005 15:16:39 -0500
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <eb555e6605071413081134f3ba@mail.gmail.com>
References: <Pine.SGI.4.40.0507131542140.8483928-100000@origin.chass.utoronto.ca>
	<20050714170146.93155.qmail@web51805.mail.yahoo.com>
	<eb555e6605071410306685a533@mail.gmail.com>
	<1121363583.3381.22.camel@localhost.localdomain>
	<eb555e6605071413081134f3ba@mail.gmail.com>
Message-ID: <1121372199.3381.33.camel@localhost.localdomain>

On Thu, 2005-07-14 at 15:08 -0500, Deepayan Sarkar wrote:
> On 7/14/05, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> > On Thu, 2005-07-14 at 12:30 -0500, Deepayan Sarkar wrote:
> > > On 7/14/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> > > > Thank you for thinking about the problem for me.
> > > > However, I have found that your method doesn't work at
> > > > all.
> > > >
> > > > You may test the following example:
> > > >
> > > > x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> > > > x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> > > > x1=rep(x1,4)
> > > > x2=rep(x2,4)
> > > > temp=data.frame(x1,x2)
> > > > temp1=table(temp)
> > > > plot(temp$x1,temp$x2,cex=0)
> > > > text(as.numeric(rownames(temp1)),
> > > > as.numeric(colnames(temp1)), temp1)
> > > >
> > > > what I got here is not what I wanted. You may compare
> > > > with
> > > > plot(x1,x2)
> > > >
> > > > I actually want some plots similar to what SAS proc
> > > > plot produced.
> > > >
> > > > Does anybody have a clue of how to do this easily in
> > > > R?
> > >
> > > Try this:
> > >
> > >
> > > x1=c(0.6,0.4,.4,.4,.2,.2,.2,0,0)
> > > x2=c(0.4,.2,.4,.6,0,.2,.4,0,.2)
> > > x1=rep(x1,4)
> > > x2=rep(x2,4)
> > > temp=data.frame(x1,x2)
> > >
> > > foo <- subset(as.data.frame(table(temp)), Freq > 0)
> > > foo$x1 <- as.numeric(as.character(foo$x1))
> > > foo$x2 <- as.numeric(as.character(foo$x2))
> > >
> > > with(foo, plot(x1, x2, type = "n"))
> > > with(foo, text(x1, x2, lab = Freq))
> > >
> > >
> > > -Deepayan
> > 
> > 
> > Great solution Deepayan. I was just in the process of working on
> > something similar.
> > 
> > One quick modification is that the two plot()/text() functions can be
> > combined into:
> > 
> >   with(foo, plot(x1, x2, pch = as.character(Freq)))
> 
> Only as long as all(Freq < 10) (I've been bitten by this before. :-) ).
> 
> Deepayan


D'oh!

Indeed you are correct, since 'pch' is a single character:

  plot(1:20, pch = as.character(1:20))

as opposed to:

  plot(1:20, type = "n")
  text(1:20, lab = 1:20)

Thanks for the correction.

Marc



From Gary.Nelson at state.ma.us  Thu Jul 14 22:58:31 2005
From: Gary.Nelson at state.ma.us (Nelson, Gary (FWE))
Date: Thu, 14 Jul 2005 16:58:31 -0400
Subject: [R] read.xport
Message-ID: <74BDE31AFD6EC54DB026E6CD11FF0A7E1246FC@ES-MSG-008.es.govt.state.ma.us>

Sorry about the version.  I have version 0.8-8 of foreign. I downloaded
(and installed) it this afternoon.  The XPORT files were created in SAS
using PROC COPY.  I used the code:
temp<-read.xport("c:/temp/int82ag.xpt").  I still get only 16 file of
the 24 SAS placed into the XPORT file. The XPORT files come from
ftp://cusk.nmfs.noaa.gov/mrfss/intercept/ag/ if anyone wants to try it.
I used the .xpt in "int82ag.zip" and it appears they were created using
SASV5XPT.




-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Thursday, July 14, 2005 2:01 PM
To: Nelson, Gary (FWE)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] read.xport


On Thu, 14 Jul 2005, Nelson, Gary (FWE) wrote:

> I have the latest version of foreign, but it still doesn't work.  I

Are you sure: a new version was released a few hours ago?  It may not 
answer your question, but please do give actual version numbers (as the 
posting guide asks).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mike.rstat at gmail.com  Thu Jul 14 23:04:55 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 14 Jul 2005 14:04:55 -0700
Subject: [R] integer codes of factors
Message-ID: <27db823f0507141404558856e7@mail.gmail.com>

  U = c("b", "b", "b", "c", "d", "e", "e")

  F1 = factor( U, levels=c("a", "b", "c", "d", "e") )

  as.numeric(F1) 
  [1] 2 2 2 3 4 5 5 

Here, the integer code of "b" in F1 is 2

  K = factor( levels(F1) )
  as.numeric(K)
  [1] 1 2 3 4 5
  K
  [1] a b c d e
  Levels: a b c d e

And again, the integer code of "b" in K is 2. Great!

I am wondering how modify that usage such that the correspondence between 
the two numeric vectors can this be trusted.  for example, the correspondence 
can be corrupted by placing the "a" at the end:

  F2 = factor( U, levels=c("b", "c", "d", "e", "a") )
 
  as.numeric(F2) 
  [1] 1 1 1 2 3 4 4

Placing the "a" at the end changed the integer code of "b" in F2 to 1, which is 
not a problem. But ......

  K = factor( levels(F2) )
  as.numeric( K )
  [1] 2 3 4 5 1
  K
  [1] b c d e a
  Levels: a b c d e

But the integer code of "b" in K is now 2, which does not correspond to its code
in F2.

One would think that ordered=TRUE ought to avoid the corruption, but it does not
seem to accomplish that:

  K = factor(  levels(F2), ordered=TRUE ) 
  as.numeric(K)
  [1] 2 3 4 5 1
  K
  [1] b c d e a
  Levels: a < b < c < d < e

But the integer code of "b" in K is still 2.

However, corruption can be avoided with this idiom:

  K = factor(  levels(F2), levels=levels(F2) )
  as.numeric(K)
  [1] 1 2 3 4 5
  K
  [1] "b" "c" "d" "e" "a"
  Levels: b c d e a

Now the integer code of "b" in K is 1, which, as desired, is in
correspondence with
its code in F2.



From p.dalgaard at biostat.ku.dk  Thu Jul 14 23:06:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jul 2005 23:06:00 +0200
Subject: [R] read.xport
In-Reply-To: <42D6B78E.6030301@vanderbilt.edu>
References: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>
	<42D6B78E.6030301@vanderbilt.edu>
Message-ID: <x2pstldrnr.fsf@turmalin.kubism.ku.dk>

Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:

> Nelson, Gary (FWE) wrote:
> > I have the latest version of foreign, but it still doesn't work.  I
> > quickly tried the Hmisc package, but the same issue arose.  I will delve
> > into the Hmisc package further.
> 
> This is disappointing as we rely on read.xport quite a bit.

Indeed. Can SAS read everyhting in the XPORT lib? If so, what can we
do to figure out what goes wrong in read.xport. Can you paraphrase the
situation using simulated data and put the file somewhere?


<snippage>
> >>>-----Original Message-----
> >>>From: Nelson, Gary (FWE) [mailto:Gary.Nelson at state.ma.us]
> >>>Sent: Thursday, July 14, 2005 10:31 AM
> >>>To: r-help at stat.math.ethz.ch
> >>>Subject: [R] read.xport
> >>>
> >>>
> >>>I am trying to import data from a SAS XPORT file that
> >>>contains 24 SAS files.
> >>>When I use the "read.xport" procedure only about 16 data 
> >>>frames (components)
> >>>are created.  Any suggestions?
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>**************************************************************
> >>>***********
> >>>
> >>>Gary A. Nelson, Ph.D
> >>>
> >>>Massachusetts Division of Marine Fisheries
> >>>
> >>>30 Emerson Avenue
> >>>
> >>>Gloucester, MA 01930
> >>>
> >>>Phone: (978) 282-0308 x114
> >>>
> >>>Fax: (617) 727-3337
> >>>
> >>>Email: Gary.Nelson at state.ma.us
> >>>
> >>>
> >>>
> >>>
> >>>	[[alternative HTML version deleted]]
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list 
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list 
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> > 
> > 
> > 
> 
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From pantd at unlv.nevada.edu  Thu Jul 14 23:06:45 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Thu, 14 Jul 2005 14:06:45 -0700
Subject: [R] question from environmental statistics
Message-ID: <1121375205.42d6d3e5d3f33@webmail.scsv.nevada.edu>



Dear R users
I want to knw if there is a way in which a raw dataset can be modelled by some
distribution. besides the gof test is there any test involving gamma or
lognormal that would fit the data.

thank you

-dev



From p.dalgaard at biostat.ku.dk  Fri Jul 15 00:21:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jul 2005 00:21:41 +0200
Subject: [R] read.xport
In-Reply-To: <x2pstldrnr.fsf@turmalin.kubism.ku.dk>
References: <74BDE31AFD6EC54DB026E6CD11FF0A7E9650BD@ES-MSG-008.es.govt.state.ma.us>
	<42D6B78E.6030301@vanderbilt.edu>
	<x2pstldrnr.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2k6jtdo5m.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:
> 
> > Nelson, Gary (FWE) wrote:
> > > I have the latest version of foreign, but it still doesn't work.  I
> > > quickly tried the Hmisc package, but the same issue arose.  I will delve
> > > into the Hmisc package further.
> > 
> > This is disappointing as we rely on read.xport quite a bit.
> 
> Indeed. Can SAS read everyhting in the XPORT lib? If so, what can we
> do to figure out what goes wrong in read.xport. Can you paraphrase the
> situation using simulated data and put the file somewhere?

Lines crossed... Gary anwered the last bits already, and I can confirm
both that SAS will read all 24 elements, whereas lookup.xport leaves
off 7 of them - in the middle. Specifically:

> names(lookup.xport("int82ag.xpt"))
 [1] "I1_821"  "I1_822"  "I1_823" "I1_824"  "I1_825"  "I1_826"   "I2_821"
 [8] "I2_822"  "I2_823"  "I2_824"  "I2_825" "I2_826"  "GP4_822"  "GP4_823"
[15] "GP4_824" "GP4_825" "GP4_826"

whereas PROC CONTENTS shows

> grep 'Data Set Name' readxport.lst | awk '{print $4}'
FOO.I1_821
FOO.I1_822
FOO.I1_823
FOO.I1_824
FOO.I1_825
FOO.I1_826
FOO.I2_821
FOO.I2_822
FOO.I2_823
FOO.I2_824
FOO.I2_825
FOO.I2_826
FOO.I3_821
FOO.I3_822
FOO.I3_823
FOO.I3_824
FOO.I3_825
FOO.I3_826
FOO.GP4_821
FOO.GP4_822
FOO.GP4_823
FOO.GP4_824
FOO.GP4_825
FOO.GP4_826

Notice that all the I3's and GP4_821 has gone AWOL in lookup.xport

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From bitwrit at ozemail.com.au  Fri Jul 15 10:49:15 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 15 Jul 2005 08:49:15 +0000
Subject: [R] plot the number of replicates at the same point
Message-ID: <42D7788B.7070701@ozemail.com.au>

Hi all,

This is nowhere near as elegant as Deepayan's solution, but I read the 
spec as plotting symbols except where there were overlays. You are 
welcome to improve the following...

Jim
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: count.overplot.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050715/6250585e/count.overplot.pl

From steve at tenzone.u-net.com  Fri Jul 15 00:48:05 2005
From: steve at tenzone.u-net.com (Steve Ellison)
Date: Thu, 14 Jul 2005 23:48:05 +0100
Subject: [R] Variance components from lm?
Message-ID: <3.0.5.32.20050714234805.008e2a90@mail.u-net.com>

I often use simple nested random-effect models for interlaboratory data.
The variance components are important things to know.

Is there an R function or package that gets variance components from lm
objects? Or can someone point me to a method of doing so?



From ggrothendieck at gmail.com  Fri Jul 15 01:41:44 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Jul 2005 19:41:44 -0400
Subject: [R] plot the number of replicates at the same point
In-Reply-To: <42D7788B.7070701@ozemail.com.au>
References: <42D7788B.7070701@ozemail.com.au>
Message-ID: <971536df050714164170078801@mail.gmail.com>

See ?sunflowerplot for a graphic indication of 
the number of replications at each point.



From spencer.graves at pdf.com  Fri Jul 15 02:07:11 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Jul 2005 17:07:11 -0700
Subject: [R] Variance components from lm?
In-Reply-To: <3.0.5.32.20050714234805.008e2a90@mail.u-net.com>
References: <3.0.5.32.20050714234805.008e2a90@mail.u-net.com>
Message-ID: <42D6FE2F.80509@pdf.com>

	  Under normal circumstances, 'RSiteSearch("variance components")' 
would likely identify "lme" in package nlme and lmer in package lme4.  I 
recommend lme, as it comes with a fairly complete set of helper 
functions described in Pinheiro and Bates (2000) Mixed-Effect Models in 
S and S-Plus (Springer).  "lmer" is Doug Bates' next generation product 
currently under development and lacking some of the helper functions 
available with lme.

	  spencer graves
p.s.  I said, "Under normal circumstances", because when I tried 
RSiteSearch just now, it didn't work.  When I went to 
"www.r-project.org" -> Search -> "R site search", I got "Computer 
trouble, July 13, 2005".  When it works (which is 99.7% of the time), it 
is my favorite tool for finding things about R for which I don't already 
know where to look.

Steve Ellison wrote:

> I often use simple nested random-effect models for interlaboratory data.
> The variance components are important things to know.
> 
> Is there an R function or package that gets variance components from lm
> objects? Or can someone point me to a method of doing so?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Seeliger.Curt at epamail.epa.gov  Fri Jul 15 02:31:51 2005
From: Seeliger.Curt at epamail.epa.gov (Seeliger.Curt@epamail.epa.gov)
Date: Thu, 14 Jul 2005 17:31:51 -0700
Subject: [R] Calculation of group summaries
In-Reply-To: <Pine.A41.4.61b.0507130744360.61128@homer12.u.washington.edu>
Message-ID: <OF3CA755BA.09287F77-ON8825703E.0081B3B2-8825703F.0002EAD1@epamail.epa.gov>

Several people suggested specific functions (by, tapply, sapply and
others); thanks for not blowing off a simple question regarding how to
do the following SQL in R:
>   select year,
>          site_id,
>          visit_no,
>          mean(undercut) AS meanUndercut,
>          count(undercut) AS nUndercut,
>          std(undercut) AS stdUndercut
>   from channelMorphology
>   group by year, site_id, visit_no
>   ;

I'd spent quite a bit of time with the suggested functions earlier but
had no luck as I'd misread the docs and put the entire dataframe where
it only wants the columns to be processed.  Sometimes it's the simplest
of things.

This has lead to another confoundment-- sd() acts differently than
mean() for some reason, at least with R 1.9.0.  For some reason, means
generate NA results and a warning message for each group:

  argument is not numeric or logical: returning NA in:
mean.default(data[x, ], ...)

Of course, the argument is numeric, or there'd be no sd value.  Or more
likely, I'm still missing something really basic. If I wrap the value in
as.numeric() things work fine.  Why should I have to do this for mean
and median, but not sd? The code below should reproduce this error

  # Fake data for demo:
  nsites<-6
  yearList<-1999:2001
  fakesub<-as.data.frame(cbind(
                 year     =rep(yearList,nsites/length(yearList),each=11)
                ,site_id  =rep(c('site1','site2'),each=11*nsites)
                ,visit_no =rep(1,11*2*nsites)
                ,transect =rep(LETTERS[1:11],nsites,each=2)
                ,transdir =rep(c('LF','RT'),11*nsites)
                ,undercut =abs(rnorm(11*2*nsites,10))
                ,angle    =runif(11*2*nsites,0,180)
                ))

  # Create group summaries:
  sdmets<-by(fakesub$undercut
            ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
            ,sd
            )
  nmets<-by(fakesub$undercut
           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
           ,length
           )
  xmets<-by(fakesub$undercut
           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
           ,mean
           )
   xmets<-by(as.numeric(fakesub$undercut)
           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
           ,mean
           )

  # Put site id values (year, site_id and visit_no) into results:
  # List unique id combinations as a list of lists.  Then
  # reorganize that into 3 vectors for final results.
  # Certainly, there MUST be a better way...
  foo<-strsplit(unique(paste(fakesub$year
                            ,fakesub$site_id
                            ,fakesub$visit_no
                            ,sep='#'))
               ,split='#'
               )
  year<-list()
  for(i in 1:length(foo)) {year<-rbind(year,foo[[i]][1])}
  site_id<-list()
  for(i in 1:length(foo)) {site_id<-rbind(site_id,foo[[i]][2])}
  visit_no<-list()
  for(i in 1:length(foo)) {visit_no<-rbind(visit_no,foo[[i]][3])}

  # Final result, more or less
  data.frame(cbind(a=year,b=site_id,c=visit_no,sdmets,nmets,xmets))


cur

--
Curt Seeliger, Data Ranger
CSC, EPA/WED contractor
541/754-4638
seeliger.curt at epa.gov



From andy_liaw at merck.com  Fri Jul 15 02:47:21 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Jul 2005 20:47:21 -0400
Subject: [R] Variance components from lm?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAA7@usctmx1106.Merck.com>

> From: Spencer Graves
> 
> 	  Under normal circumstances, 'RSiteSearch("variance 
> components")' 
> would likely identify "lme" in package nlme and lmer in 
> package lme4.  I 
> recommend lme, as it comes with a fairly complete set of helper 
> functions described in Pinheiro and Bates (2000) Mixed-Effect 
> Models in 
> S and S-Plus (Springer).  "lmer" is Doug Bates' next 
> generation product 
> currently under development and lacking some of the helper functions 
> available with lme.
> 
> 	  spencer graves
> p.s.  I said, "Under normal circumstances", because when I tried 
> RSiteSearch just now, it didn't work.  When I went to 
> "www.r-project.org" -> Search -> "R site search", I got "Computer 
> trouble, July 13, 2005".  When it works (which is 99.7% of 
> the time), it 
> is my favorite tool for finding things about R for which I 
> don't already 
> know where to look.

The server (graciously created and maintained by Prof. Jonathan Baron)
crashed and a hard drive failure.  I was told it would likely be
back online next Monday.

I missed this search engine, too, as I've grown to depend on it quite
a bit.

Cheers,
Andy
 
> Steve Ellison wrote:
> 
> > I often use simple nested random-effect models for 
> interlaboratory data.
> > The variance components are important things to know.
> > 
> > Is there an R function or package that gets variance 
> components from lm
> > objects? Or can someone point me to a method of doing so?
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Spencer 
> Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From gerifalte28 at hotmail.com  Fri Jul 15 03:37:40 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 15 Jul 2005 01:37:40 +0000
Subject: [R] question from environmental statistics
In-Reply-To: <1121375205.42d6d3e5d3f33@webmail.scsv.nevada.edu>
Message-ID: <BAY103-F99A548459A470F38F6082A6D00@phx.gbl>

Take a look at this document by Vito Ricci: 
http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf

Did you try RSiteSearch("Fit distribution") or a Google search?  That will 
lead you to fit.dist{gnlm} and fitdistr{MASS}

Cheers

Francisco


>From: pantd at unlv.nevada.edu
>To: r-help at stat.math.ethz.ch
>Subject: [R] question from environmental statistics
>Date: Thu, 14 Jul 2005 14:06:45 -0700
>
>
>
>Dear R users
>I want to knw if there is a way in which a raw dataset can be modelled by 
>some
>distribution. besides the gof test is there any test involving gamma or
>lognormal that would fit the data.
>
>thank you
>
>-dev
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Jul 15 04:43:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Jul 2005 22:43:03 -0400
Subject: [R] Calculation of group summaries
In-Reply-To: <OF3CA755BA.09287F77-ON8825703E.0081B3B2-8825703F.0002EAD1@epamail.epa.gov>
References: <Pine.A41.4.61b.0507130744360.61128@homer12.u.washington.edu>
	<OF3CA755BA.09287F77-ON8825703E.0081B3B2-8825703F.0002EAD1@epamail.epa.gov>
Message-ID: <971536df05071419432b29e245@mail.gmail.com>

1. Try using more spaces so your code is easier to read.

2. Use data.frame to define your data frame (since the method
in your post creates data frames of factors rather than
the desired classes).  

3. Given the appropriate function, f, a single 'by' statement rbind'ed
together, as shown, will create the result.

nsites <- 6
yearList <- 1999:2001
fakesub <- data.frame(
	year = rep(yearList, nsites/length(yearList), each = 11),
	site_id  = rep(c('site1','site2'), each = 11*nsites),
	visit_no = rep(1, 11*2*nsites),
	transect = rep(LETTERS[1:11], nsites, each = 2),
	transdir = rep(c('LF','RT'), 11*nsites),
	undercut = abs(rnorm(11*2*nsites, 10)),
	angle    = runif(11*2*nsites, 0, 180)
)


f <- function(x) cbind(year = x[1,1], site_id = x[1,2], visit_no = x[1,3], 
	mean = mean(x[,6]), sd = sd(x[,6]), length = length(x[,6]))
do.call("rbind", by(fakesub, fakesub[,1:3], f))





On 7/14/05, Seeliger.Curt at epamail.epa.gov <Seeliger.Curt at epamail.epa.gov> wrote:
> Several people suggested specific functions (by, tapply, sapply and
> others); thanks for not blowing off a simple question regarding how to
> do the following SQL in R:
> >   select year,
> >          site_id,
> >          visit_no,
> >          mean(undercut) AS meanUndercut,
> >          count(undercut) AS nUndercut,
> >          std(undercut) AS stdUndercut
> >   from channelMorphology
> >   group by year, site_id, visit_no
> >   ;
> 
> I'd spent quite a bit of time with the suggested functions earlier but
> had no luck as I'd misread the docs and put the entire dataframe where
> it only wants the columns to be processed.  Sometimes it's the simplest
> of things.
> 
> This has lead to another confoundment-- sd() acts differently than
> mean() for some reason, at least with R 1.9.0.  For some reason, means
> generate NA results and a warning message for each group:
> 
>  argument is not numeric or logical: returning NA in:
> mean.default(data[x, ], ...)
> 
> Of course, the argument is numeric, or there'd be no sd value.  Or more
> likely, I'm still missing something really basic. If I wrap the value in
> as.numeric() things work fine.  Why should I have to do this for mean
> and median, but not sd? The code below should reproduce this error
> 
>  # Fake data for demo:
>  nsites<-6
>  yearList<-1999:2001
>  fakesub<-as.data.frame(cbind(
>                 year     =rep(yearList,nsites/length(yearList),each=11)
>                ,site_id  =rep(c('site1','site2'),each=11*nsites)
>                ,visit_no =rep(1,11*2*nsites)
>                ,transect =rep(LETTERS[1:11],nsites,each=2)
>                ,transdir =rep(c('LF','RT'),11*nsites)
>                ,undercut =abs(rnorm(11*2*nsites,10))
>                ,angle    =runif(11*2*nsites,0,180)
>                ))
> 
>  # Create group summaries:
>  sdmets<-by(fakesub$undercut
>            ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>            ,sd
>            )
>  nmets<-by(fakesub$undercut
>           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>           ,length
>           )
>  xmets<-by(fakesub$undercut
>           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>           ,mean
>           )
>   xmets<-by(as.numeric(fakesub$undercut)
>           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>           ,mean
>           )
> 
>  # Put site id values (year, site_id and visit_no) into results:
>  # List unique id combinations as a list of lists.  Then
>  # reorganize that into 3 vectors for final results.
>  # Certainly, there MUST be a better way...
>  foo<-strsplit(unique(paste(fakesub$year
>                            ,fakesub$site_id
>                            ,fakesub$visit_no
>                            ,sep='#'))
>               ,split='#'
>               )
>  year<-list()
>  for(i in 1:length(foo)) {year<-rbind(year,foo[[i]][1])}
>  site_id<-list()
>  for(i in 1:length(foo)) {site_id<-rbind(site_id,foo[[i]][2])}
>  visit_no<-list()
>  for(i in 1:length(foo)) {visit_no<-rbind(visit_no,foo[[i]][3])}
> 
>  # Final result, more or less
>  data.frame(cbind(a=year,b=site_id,c=visit_no,sdmets,nmets,xmets))
> 
> 
> cur
> 
> --
> Curt Seeliger, Data Ranger
> CSC, EPA/WED contractor
> 541/754-4638
> seeliger.curt at epa.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Jul 15 04:55:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Jul 2005 22:55:57 -0400
Subject: [R] Calculation of group summaries
In-Reply-To: <971536df05071419432b29e245@mail.gmail.com>
References: <Pine.A41.4.61b.0507130744360.61128@homer12.u.washington.edu>
	<OF3CA755BA.09287F77-ON8825703E.0081B3B2-8825703F.0002EAD1@epamail.epa.gov>
	<971536df05071419432b29e245@mail.gmail.com>
Message-ID: <971536df0507141955379a7e51@mail.gmail.com>

There was an error in my code (after advising you to use data.frame
rather than cbind I used it myself!).  Here it is again:


nsites <- 6
yearList <- 1999:2001
fakesub <- data.frame(
	year = rep(yearList, nsites/length(yearList), each = 11),
	site_id  = rep(c('site1','site2'), each = 11*nsites),
	visit_no = rep(1, 11*2*nsites),
	transect = rep(LETTERS[1:11], nsites, each = 2),
	transdir = rep(c('LF','RT'), 11*nsites),
	undercut = abs(rnorm(11*2*nsites, 10)),
	angle    = runif(11*2*nsites, 0, 180)
)


f <- function(x) data.frame(year = x[1,1], site_id = x[1,2], visit_no = x[1,3], 
	mean = mean(x[,6]), sd = sd(x[,6]), length = length(x[,6]))
do.call("rbind", by(fakesub, fakesub[,1:3], f))



On 7/14/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 1. Try using more spaces so your code is easier to read.
> 
> 2. Use data.frame to define your data frame (since the method
> in your post creates data frames of factors rather than
> the desired classes).
> 
> 3. Given the appropriate function, f, a single 'by' statement rbind'ed
> together, as shown, will create the result.
> 
> nsites <- 6
> yearList <- 1999:2001
> fakesub <- data.frame(
>        year = rep(yearList, nsites/length(yearList), each = 11),
>        site_id  = rep(c('site1','site2'), each = 11*nsites),
>        visit_no = rep(1, 11*2*nsites),
>        transect = rep(LETTERS[1:11], nsites, each = 2),
>        transdir = rep(c('LF','RT'), 11*nsites),
>        undercut = abs(rnorm(11*2*nsites, 10)),
>        angle    = runif(11*2*nsites, 0, 180)
> )
> 
> 
> f <- function(x) cbind(year = x[1,1], site_id = x[1,2], visit_no = x[1,3],
>        mean = mean(x[,6]), sd = sd(x[,6]), length = length(x[,6]))
> do.call("rbind", by(fakesub, fakesub[,1:3], f))
> 
> 
> 
> 
> 
> On 7/14/05, Seeliger.Curt at epamail.epa.gov <Seeliger.Curt at epamail.epa.gov> wrote:
> > Several people suggested specific functions (by, tapply, sapply and
> > others); thanks for not blowing off a simple question regarding how to
> > do the following SQL in R:
> > >   select year,
> > >          site_id,
> > >          visit_no,
> > >          mean(undercut) AS meanUndercut,
> > >          count(undercut) AS nUndercut,
> > >          std(undercut) AS stdUndercut
> > >   from channelMorphology
> > >   group by year, site_id, visit_no
> > >   ;
> >
> > I'd spent quite a bit of time with the suggested functions earlier but
> > had no luck as I'd misread the docs and put the entire dataframe where
> > it only wants the columns to be processed.  Sometimes it's the simplest
> > of things.
> >
> > This has lead to another confoundment-- sd() acts differently than
> > mean() for some reason, at least with R 1.9.0.  For some reason, means
> > generate NA results and a warning message for each group:
> >
> >  argument is not numeric or logical: returning NA in:
> > mean.default(data[x, ], ...)
> >
> > Of course, the argument is numeric, or there'd be no sd value.  Or more
> > likely, I'm still missing something really basic. If I wrap the value in
> > as.numeric() things work fine.  Why should I have to do this for mean
> > and median, but not sd? The code below should reproduce this error
> >
> >  # Fake data for demo:
> >  nsites<-6
> >  yearList<-1999:2001
> >  fakesub<-as.data.frame(cbind(
> >                 year     =rep(yearList,nsites/length(yearList),each=11)
> >                ,site_id  =rep(c('site1','site2'),each=11*nsites)
> >                ,visit_no =rep(1,11*2*nsites)
> >                ,transect =rep(LETTERS[1:11],nsites,each=2)
> >                ,transdir =rep(c('LF','RT'),11*nsites)
> >                ,undercut =abs(rnorm(11*2*nsites,10))
> >                ,angle    =runif(11*2*nsites,0,180)
> >                ))
> >
> >  # Create group summaries:
> >  sdmets<-by(fakesub$undercut
> >            ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
> >            ,sd
> >            )
> >  nmets<-by(fakesub$undercut
> >           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
> >           ,length
> >           )
> >  xmets<-by(fakesub$undercut
> >           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
> >           ,mean
> >           )
> >   xmets<-by(as.numeric(fakesub$undercut)
> >           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
> >           ,mean
> >           )
> >
> >  # Put site id values (year, site_id and visit_no) into results:
> >  # List unique id combinations as a list of lists.  Then
> >  # reorganize that into 3 vectors for final results.
> >  # Certainly, there MUST be a better way...
> >  foo<-strsplit(unique(paste(fakesub$year
> >                            ,fakesub$site_id
> >                            ,fakesub$visit_no
> >                            ,sep='#'))
> >               ,split='#'
> >               )
> >  year<-list()
> >  for(i in 1:length(foo)) {year<-rbind(year,foo[[i]][1])}
> >  site_id<-list()
> >  for(i in 1:length(foo)) {site_id<-rbind(site_id,foo[[i]][2])}
> >  visit_no<-list()
> >  for(i in 1:length(foo)) {visit_no<-rbind(visit_no,foo[[i]][3])}
> >
> >  # Final result, more or less
> >  data.frame(cbind(a=year,b=site_id,c=visit_no,sdmets,nmets,xmets))
> >
> >
> > cur
> >
> > --
> > Curt Seeliger, Data Ranger
> > CSC, EPA/WED contractor
> > 541/754-4638
> > seeliger.curt at epa.gov
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From spencer.graves at pdf.com  Fri Jul 15 05:18:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Jul 2005 20:18:26 -0700
Subject: [R] test for difference in the order of self generated sequence
In-Reply-To: <001201c588a6$f70955e0$8cac3b80@PHDXP20025C>
References: <001201c588a6$f70955e0$8cac3b80@PHDXP20025C>
Message-ID: <42D72B02.3010001@pdf.com>

	  Have you considered "glmmPQL" in library(MASS)?

	  spencer graves

Kaiya Liu wrote:

> Hello,
> 
> I have an experiment in which we ask subjects to generate a list of thoughts
> after being exposed to a stimuli. The thoughts were then coded into two
> categories (e.g. A & B). The objective is to show that the order in which
> thoughts are generated is affected by the experimental conditions. Can
> somebody tell me what functions in R can help me do the test or point me to
> the appropriate reference? Thanks!
> 
> Example data: Subject 1 in condition 1 generated 5 thoughts. The first,
> second, and 5th thought is in category A, thought 3 and 4 are in category B.
> Subject 2 only generated 4 thoughts.
> 
> -------Condition-------------A-----------------B--------
> n1       1                                1, 2, 5                  3, 4
> n2       2                                3, 4                      1, 2
> -----------------------------------------------------------
> 
> Kaiya Liu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From yl2058 at columbia.edu  Fri Jul 15 05:19:55 2005
From: yl2058 at columbia.edu (Yimeng Lu)
Date: Thu, 14 Jul 2005 23:19:55 -0400
Subject: [R] problems with nls function
Message-ID: <008901c588ec$12f172c0$97a46f9c@brianstat>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/e33a1165/attachment.pl

From spencer.graves at pdf.com  Fri Jul 15 05:26:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Jul 2005 20:26:38 -0700
Subject: [R] integer codes of factors
In-Reply-To: <27db823f0507141404558856e7@mail.gmail.com>
References: <27db823f0507141404558856e7@mail.gmail.com>
Message-ID: <42D72CEE.5040607@pdf.com>

	  What's the problem?  As suggested by the help page, the numeric codes 
are assigned in the order the names appear in the levels argument. 
Consider the following example from the help page plus a minor 
modification:

 >      (ff <- factor(substring("statistics", 1:10, 1:10), levels=letters))
  [1] s t a t i s t i c s
Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z
 >
 > as.numeric(ff)
  [1] 19 20  1 20  9 19 20  9  3 19
 > (ff <- factor(substring("statistics", 1:10, 1:10), levels=letters[3:1]))
  [1] <NA> <NA> a    <NA> <NA> <NA> <NA> <NA> c    <NA>
Levels: c b a
 > as.numeric(ff)
  [1] NA NA  3 NA NA NA NA NA  1 NA

	  I highly recommend Venable and Ripley (2002) Modern Applied 
Statistics with S (Springer) and V & R (2000) S Programming (Springer).

	  spencer graves

Mike R wrote:

>   U = c("b", "b", "b", "c", "d", "e", "e")
> 
>   F1 = factor( U, levels=c("a", "b", "c", "d", "e") )
> 
>   as.numeric(F1) 
>   [1] 2 2 2 3 4 5 5 
> 
> Here, the integer code of "b" in F1 is 2
> 
>   K = factor( levels(F1) )
>   as.numeric(K)
>   [1] 1 2 3 4 5
>   K
>   [1] a b c d e
>   Levels: a b c d e
> 
> And again, the integer code of "b" in K is 2. Great!
> 
> I am wondering how modify that usage such that the correspondence between 
> the two numeric vectors can this be trusted.  for example, the correspondence 
> can be corrupted by placing the "a" at the end:
> 
>   F2 = factor( U, levels=c("b", "c", "d", "e", "a") )
>  
>   as.numeric(F2) 
>   [1] 1 1 1 2 3 4 4
> 
> Placing the "a" at the end changed the integer code of "b" in F2 to 1, which is 
> not a problem. But ......
> 
>   K = factor( levels(F2) )
>   as.numeric( K )
>   [1] 2 3 4 5 1
>   K
>   [1] b c d e a
>   Levels: a b c d e
> 
> But the integer code of "b" in K is now 2, which does not correspond to its code
> in F2.
> 
> One would think that ordered=TRUE ought to avoid the corruption, but it does not
> seem to accomplish that:
> 
>   K = factor(  levels(F2), ordered=TRUE ) 
>   as.numeric(K)
>   [1] 2 3 4 5 1
>   K
>   [1] b c d e a
>   Levels: a < b < c < d < e
> 
> But the integer code of "b" in K is still 2.
> 
> However, corruption can be avoided with this idiom:
> 
>   K = factor(  levels(F2), levels=levels(F2) )
>   as.numeric(K)
>   [1] 1 2 3 4 5
>   K
>   [1] "b" "c" "d" "e" "a"
>   Levels: b c d e a
> 
> Now the integer code of "b" in K is 1, which, as desired, is in
> correspondence with
> its code in F2.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From yl2058 at columbia.edu  Fri Jul 15 05:31:10 2005
From: yl2058 at columbia.edu (Yimeng Lu)
Date: Thu, 14 Jul 2005 23:31:10 -0400
Subject: [R] problems with nls function
Message-ID: <009d01c588ed$a51dd6b0$97a46f9c@brianstat>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050714/6abba73d/attachment.pl

From spencer.graves at pdf.com  Fri Jul 15 05:42:55 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Jul 2005 20:42:55 -0700
Subject: [R] A statistical modeling problem
In-Reply-To: <0IJM009GJL5XOH@jhuml1.jhmi.edu>
References: <0IJM009GJL5XOH@jhuml1.jhmi.edu>
Message-ID: <42D730BF.6090506@pdf.com>

	  Let's do the simplest things first:  I'd start with normal 
probability plots of log(t_ij).  If that all looked sensible, I'd then 
use lme to model log(t_ij) with subject as a random effect and 
(probably) que as a fixed effect.

	  Then I'd probably try something like glmmPQL in library(MASS) to 
leverage whatever I got into logistic regression models with, again, 
subject as a random effect and que and log(t_ij) as possible explanatory 
variables.

	  Regarding "lme", I highly recommend Pinheiro and Bates (2000) 
Mixed-Effect Models in S and S-Plus (Springer).  For glmmPQL, see 
Venables and Ripley (2002) Modern Applied Statistics with S, 4th ed. 
(Springer).  I'd also search the archives for "generalized linear mixed 
models", as there are other R packages that offer related capabilities.

	  spencer graves

Ravi Varadhan wrote:

> Hi,
> 
>  
> 
> This is not an R related question and I apologize for that, but given the
> brain power of the R community it is hard for me to resist posting this
> here.
> 
>  
> 
> I have a problem where each participant is shown a series of visual cues
> (displayed on a computer screen in a random order) and asked to respond by
> pressing a button (from a finite number of buttons) that corresponds to the
> correct answer.  The data is of the form (r_ij , t_ij), where r_ij = 1
> indicates correct response by the i-th subject on the j-th cue, and t_ij is
> the time taken to respond.  Are there statistical models to describe this
> set up?  Any references or suggestions would be greatly appreciated.
> 
>  
> 
> Thanks very much,
> 
> Ravi.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Fri Jul 15 05:57:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Jul 2005 23:57:04 -0400
Subject: [R] problems with nls function
In-Reply-To: <009d01c588ed$a51dd6b0$97a46f9c@brianstat>
References: <009d01c588ed$a51dd6b0$97a46f9c@brianstat>
Message-ID: <971536df05071420576becfa7@mail.gmail.com>

The start list should only contain parameters, not x.  Also
your function appears to have multiple errors including
reference to temp (presumably intended as tmp?)
and b(x-m) which presumably should be b*(x-m).

On 7/14/05, Yimeng Lu <yl2058 at columbia.edu> wrote:
> Sorry that I specified the x as
> x=c(0, 0.4, 0.7, 1, 1.4, 1.6, 1.8, 2:10)
> 
> The error message is the same.
> 
> Thanks.
> 
> Hanna
> 
>  ----- Original Message -----
>  From: Yimeng Lu
>  To: r-help at stat.math.ethz.ch
>  Sent: Thursday, July 14, 2005 11:19 PM
>  Subject: problems with nls function
> 
> 
>  Hello,
> 
>  I was trying to fit a generalized logistic curve and my code for testing
>  if the function "nls" is working is like this
>  #######
>  #define the generalized logistic curve function
>  glogit=function(a, b, c, m, t, x)
> 
>  {
> 
>  tmp = 1/(1 + t * exp(-b * (x - m)))^(1/t)
> 
>  model.func = a + tmp * c
> 
>  #define the gradient vector
> 
>  Z = cbind(1, c*(x-m)*tmp^(t+1)*exp(-b*(x-m)), tmp, -c*b*exp(-b*(x-m)) *temp^(t+1), -c*tmp*(1/t)* (log(tmp) + tmp^(-t) * exp(-b(x-m))))
> 
>  attr(model.func,"gradient")=Z
> 
>  model.func
> 
>  }
> 
> 
> 
>  a= 0
> 
>  b=3
> 
>  c=6
> 
>  m=2
> 
>  t=0.75
> 
>  x=c(1:10)
> 
>  y=a + c/(1 + t * exp(-b * (x - m)))^(1/t) + rnorm(16, sd=0.01)
> 
>  plot(data.frame(x=x, y=y), type = "p", col = "red")
> 
>  nls(y ~ glogit(a, b, c, m, t, x), data=data.frame(y=y,x=x), start=list(a=1, b=2, c=3, m=1, t=0, x), trace=T)
> 
>  #####End.
> 
>  When I ran this code, I got the error "Error in assign(i, temp, envir = env) : attempt to use zero-length variable name".   What does this error mean?  How should I fix it?
> 
> 
> 
>  Many thanks.
> 
>  Hanna Lu
> 
>  Dept. of Biostatistics
> 
>  Columbia University
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Fri Jul 15 05:57:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Jul 2005 20:57:23 -0700
Subject: [R] Variance components from lm?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAA7@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAA7@usctmx1106.Merck.com>
Message-ID: <42D73423.3030504@pdf.com>

	  Yes, I'm very thankful for the support of Prof. Jonathan Baron and 
all the others who have contributed their time, creative energies and 
(for some) money to make R what it is today.  It is beyond the budgets 
of most people on this planet to purchase licenses for every piece of 
statistical software that purports to do anything useful, not to mention 
the amount of time required to install and learn how to use 
fundamentally incompatible software, overcome the sometimes near 
impossiblity of porting data between the different platforms, etc.  It 
is therefore an enormous contribution to have this common platform that 
includes capabilities to perform a increasing porportion of the 
statistical analyses that people in different disciplines have found 
useful.

	  Indeed, this is not only a service to the statistical community, it 
is an enormous contribution to the future of humanity, because it makes 
it easier for people to learn a variety of statistical techniques, 
thereby making it easier for them to get better answers to the questions 
that concern them -- and through their increased statistical literacy, 
making it easier for them effectively pressure others to improve the 
quality of their problem solving through better collection and analysis 
of data.

	  spencer graves

Liaw, Andy wrote:

>>From: Spencer Graves
>>
>>	  Under normal circumstances, 'RSiteSearch("variance 
>>components")' 
>>would likely identify "lme" in package nlme and lmer in 
>>package lme4.  I 
>>recommend lme, as it comes with a fairly complete set of helper 
>>functions described in Pinheiro and Bates (2000) Mixed-Effect 
>>Models in 
>>S and S-Plus (Springer).  "lmer" is Doug Bates' next 
>>generation product 
>>currently under development and lacking some of the helper functions 
>>available with lme.
>>
>>	  spencer graves
>>p.s.  I said, "Under normal circumstances", because when I tried 
>>RSiteSearch just now, it didn't work.  When I went to 
>>"www.r-project.org" -> Search -> "R site search", I got "Computer 
>>trouble, July 13, 2005".  When it works (which is 99.7% of 
>>the time), it 
>>is my favorite tool for finding things about R for which I 
>>don't already 
>>know where to look.
> 
> 
> The server (graciously created and maintained by Prof. Jonathan Baron)
> crashed and a hard drive failure.  I was told it would likely be
> back online next Monday.
> 
> I missed this search engine, too, as I've grown to depend on it quite
> a bit.
> 
> Cheers,
> Andy
>  
> 
>>Steve Ellison wrote:
>>
>>
>>>I often use simple nested random-effect models for 
>>
>>interlaboratory data.
>>
>>>The variance components are important things to know.
>>>
>>>Is there an R function or package that gets variance 
>>
>>components from lm
>>
>>>objects? Or can someone point me to a method of doing so?
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
>>-- 
>>Spencer 
>>Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com>
>>Tel:  408-938-4420
>>Fax: 408-280-7915
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Jul 15 08:21:57 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 15 Jul 2005 08:21:57 +0200 (CEST)
Subject: [R] Does R have ANOVA permutation tests?
In-Reply-To: <bc808e8d05071410253c09ddde@mail.gmail.com>
References: <bc808e8d05071410253c09ddde@mail.gmail.com>
Message-ID: <Pine.LNX.4.51.0507150821370.12920@artemis.imbe.med.uni-erlangen.de>


> Hi list,
>
>     Does anybody know if R has functions to do the ANOVA permutation
> test?  I googled and found R has the "vegan" package to do "ANOVA like
> permutation test for Constrained Correspondence Analysis". But does R
> have a function for general ANOVA-like permutation tests? Thanks in
> advance!

have a look at `oneway_test' in package `coin'.

Best,

Torsten

>
> Yuefeng
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Fri Jul 15 09:13:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Jul 2005 08:13:05 +0100 (BST)
Subject: [R] Variance components from lm?
In-Reply-To: <42D6FE2F.80509@pdf.com>
References: <3.0.5.32.20050714234805.008e2a90@mail.u-net.com>
	<42D6FE2F.80509@pdf.com>
Message-ID: <Pine.LNX.4.61.0507150810240.19523@gannet.stats>

In any case, the question was about lm, not lme.

The short answer is that this is easiest from aov, a wrapper for lm, and 
for balanced data will be equivalent to as using lme.  Steve can find 
worked examples for interlaboratory data in MASS (the book, see the FAQ).

On Thu, 14 Jul 2005, Spencer Graves wrote:

> 	  Under normal circumstances, 'RSiteSearch("variance components")'
> would likely identify "lme" in package nlme and lmer in package lme4.  I
> recommend lme, as it comes with a fairly complete set of helper
> functions described in Pinheiro and Bates (2000) Mixed-Effect Models in
> S and S-Plus (Springer).  "lmer" is Doug Bates' next generation product
> currently under development and lacking some of the helper functions
> available with lme.
>
> 	  spencer graves
> p.s.  I said, "Under normal circumstances", because when I tried
> RSiteSearch just now, it didn't work.  When I went to
> "www.r-project.org" -> Search -> "R site search", I got "Computer
> trouble, July 13, 2005".  When it works (which is 99.7% of the time), it
> is my favorite tool for finding things about R for which I don't already
> know where to look.
>
> Steve Ellison wrote:
>
>> I often use simple nested random-effect models for interlaboratory data.
>> The variance components are important things to know.
>>
>> Is there an R function or package that gets variance components from lm
>> objects? Or can someone point me to a method of doing so?
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Fri Jul 15 09:58:19 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 15 Jul 2005 08:58:19 +0100
Subject: [R] Coxph with factors
In-Reply-To: <loom.20050714T173524-264@post.gmane.org>
References: <20050714064634.A705B44FA7@melmail.itga.com.au>
	<loom.20050714T173524-264@post.gmane.org>
Message-ID: <1121414299.5952.4.camel@ipc143004.lif.icnet.uk>

Yes, and please show us a reproducible example or small section of the
data as well as the error output. 


On Thu, 2005-07-14 at 15:36 +0000, Dieter Menne wrote:
> Kylie-Anne Richards <kar <at> itga.com.au> writes:
> 
> > 
> > I am fitting a coxph model with factors. I am running into problems when
> > using 'survfit'. I am unsure how R is treating the factors when I fit, say:
> > >        DATA<-data.frame(time.sec,done,f.pom=factor(f.pom),po,vo)
> > >        final<-coxph(Surv(time.sec,done)~f.pom*vo+po,data=DATA)
> > >         final.surv<-survfit((final), individual=T,conf.type="log-log")
> > >         print(final.surv)
> ....
> 
> Better chances to get a reply when you tell us what problems you are running 
> into.
> 
> Dieter Menne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Fri Jul 15 10:20:38 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Jul 2005 10:20:38 +0200
Subject: [R] exact values for p-values
In-Reply-To: <f39622f3a5c9.f3a5c9f39622@amc.uva.nl>
References: <f39622f3a5c9.f3a5c9f39622@amc.uva.nl>
Message-ID: <17111.29142.892003.149959@stat.math.ethz.ch>

>>>>> "S" == S O Nyangoma <S.O.Nyangoma at amc.uva.nl>
>>>>>     on Wed, 13 Jul 2005 11:06:05 +0200 writes:

    S> Hi David, Since I am looking at very extreme values, it appears I will 
    S> need FMLIB. Is it an R lib? 

there is no such thing as an "R lib".  
Do you mean "R package"?

In any case, not it is not (yet).
As David clearly says (a few lines below), it is a 'f90'
library; which means it's "fortran 90" source code that you can
compile and then use.

I've tried with gfortran (the latest avaiable on debian amd64
sid) and it quickly ended in a compiler (!) error.

    S> if so which version? How/where can I  download it?

Netlib has all the TOMS (= Transactions Of Mathematical Software)
algorithms.  Look for '814'.

Regards,
Martin

    S> ----- Original Message -----
    S> From: David Duffy <David.Duffy at qimr.edu.au>
    S> Date: Wednesday, July 13, 2005 9:46 am
    S> Subject: [R]  exact values for p-values

    >> > This is obtained from F =39540 with df1 = 1, df2 = 7025.
    >> > Suppose am interested in exact value such as
    >> >
    >> 
    >> If it were really necessary, you would have to move to multiple
    >> precision.  The gmp R package doesn't seem to yet cover this, but 
    >> FMLIB(TOMS814, DM Smith) is a multiple precision f90 library that 
    S> does
    >> include the incomplete beta -- it allows one to say for 
    >> F(1,7025)=39540,P=6.31E-2886 (evaluated using 200 sign. digit 
    >> arithmetic).  Results from
    >> R's pf() agree quite closely with the FMLIB results for less 
    >> extreme values
    >> eg
    >> > print(pf(1500,1,7025,lower=FALSE), digits=20)
    >> [1] 1.3702710894887480597e-297
    >> 
    >> cf   1.37027108948832580215549799419452388134616261215463681945E-297
    >> 
    >> 
    >> | David Duffy (MBBS PhD)                                         ,-
    S> _|\
    >> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /  
    >> *
    >> | Epidemiology Unit, Queensland Institute of Medical Research   
    >> \_,-._/
    >> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 
    >> 4D0B994A v
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! http://www.R-project.org/posting-
    >> guide.html

    S> ______________________________________________
    S> R-help at stat.math.ethz.ch mailing list
    S> https://stat.ethz.ch/mailman/listinfo/r-help
    S> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


    S> !DSPAM:42d4da2c52151994935952!



From pingping.zheng at lancaster.ac.uk  Fri Jul 15 11:11:56 2005
From: pingping.zheng at lancaster.ac.uk (Pingping Zheng)
Date: Fri, 15 Jul 2005 10:11:56 +0100
Subject: [R] Fwd: Re:  East Asian language
In-Reply-To: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
Message-ID: <42D77DDC.3040001@lancs.ac.uk>

Dear all,

I think what Nan Lin wanted is "How to change the language used in R
console by re-setting something within R?"

For a non-english user, sometime we prefer to set the system default
language to a native one and use english in individual programs when
we think necessary.

a) setting the envrionment variable LANGUAGE=en will make an impact
     on the whole system, which has its side effects.
b) changing language setup through control panel is telling people to throw
     away their native windows system, let's all use English system.

Any better idea?

--------
Pingping Zheng
Department of Mathematics and Statistics
Fylde College
Lancaster University
Lancaster LA1 4YF
UK


Baoqiang Cao wrote:
> 
> ---- Original message ----
> 
>>Date: Thu, 14 Jul 2005 19:50:41 +0200
>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>  
>>Subject: Re: [R] East Asian language  
>>To: Nan Lin <nlin at math.wustl.edu>
>>Cc: r-help at stat.math.ethz.ch
>>
>>Nan Lin wrote:
>>
>>
>>>Dear all,
>>>
>>>I just installed R 2.1.1. The installation program
> 
> automatically recognized 
> 
>>>my Windows XP was using Chinese language, so now my R
> 
> console displays 
> 
>>>everything in Chinese. How can I still let R console
> 
> display in English 
> 
>>>without modifying my Window XP language setup? Thank you so
> 
> much!
> 
>>
>>E.g. by setting the envrionment variable
>>  LANGUAGE=en
>>
> 
> Or, like what I did, change XP language setting back to
> English from the control panel.
> 
> 
>>Uwe Ligges
>>
>>
>>
>>>Best,
>>>
>>>Nan
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Jul 15 11:22:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Jul 2005 11:22:09 +0200
Subject: [R] Fwd: Re:  East Asian language
In-Reply-To: <42D77DDC.3040001@lancs.ac.uk>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
	<42D77DDC.3040001@lancs.ac.uk>
Message-ID: <42D78041.90000@statistik.uni-dortmund.de>

Pingping Zheng wrote:

> Dear all,
> 
> I think what Nan Lin wanted is "How to change the language used in R
> console by re-setting something within R?"
> 
> For a non-english user, sometime we prefer to set the system default
> language to a native one and use english in individual programs when
> we think necessary.
> 
> a) setting the envrionment variable LANGUAGE=en will make an impact
>      on the whole system, which has its side effects.
> b) changing language setup through control panel is telling people to throw
>      away their native windows system, let's all use English system.
> 
> Any better idea?

What about setting LANGUAGE in .Renviron?
See ?.Renviron

Uwe Ligges


> --------
> Pingping Zheng
> Department of Mathematics and Statistics
> Fylde College
> Lancaster University
> Lancaster LA1 4YF
> UK
> 
> 
> Baoqiang Cao wrote:
> 
>>---- Original message ----
>>
>>
>>>Date: Thu, 14 Jul 2005 19:50:41 +0200
>>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>  
>>>Subject: Re: [R] East Asian language  
>>>To: Nan Lin <nlin at math.wustl.edu>
>>>Cc: r-help at stat.math.ethz.ch
>>>
>>>Nan Lin wrote:
>>>
>>>
>>>
>>>>Dear all,
>>>>
>>>>I just installed R 2.1.1. The installation program
>>
>>automatically recognized 
>>
>>
>>>>my Windows XP was using Chinese language, so now my R
>>
>>console displays 
>>
>>
>>>>everything in Chinese. How can I still let R console
>>
>>display in English 
>>
>>
>>>>without modifying my Window XP language setup? Thank you so
>>
>>much!
>>
>>
>>>E.g. by setting the envrionment variable
>>> LANGUAGE=en
>>>
>>
>>Or, like what I did, change XP language setting back to
>>English from the control panel.
>>
>>
>>
>>>Uwe Ligges
>>>
>>>
>>>
>>>
>>>>Best,
>>>>
>>>>Nan
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From S.O.Nyangoma at amc.uva.nl  Fri Jul 15 11:23:56 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Fri, 15 Jul 2005 11:23:56 +0200
Subject: [R] exact values for p-values
Message-ID: <f9f8cff9be7e.f9be7ef9f8cf@amc.uva.nl>



> there is no such thing as an "R lib".  
> Do you mean "R package"?

Yes. I usually refer to them R packages as as libraries, hence the use 
of lib.

Regards. Stephen.


> In any case, not it is not (yet).
> As David clearly says (a few lines below), it is a 'f90'
> library; which means it's "fortran 90" source code that you can
> compile and then use.
> 
> I've tried with gfortran (the latest avaiable on debian amd64
> sid) and it quickly ended in a compiler (!) error.
> 
>    S> if so which version? How/where can I  download it?
> 
> Netlib has all the TOMS (= Transactions Of Mathematical Software)
> algorithms.  Look for '814'.
> 
> Regards,
> Martin
> 
>    S> ----- Original Message -----
>    S> From: David Duffy <David.Duffy at qimr.edu.au>
>    S> Date: Wednesday, July 13, 2005 9:46 am
>    S> Subject: [R]  exact values for p-values
> 
>    >> > This is obtained from F =39540 with df1 = 1, df2 = 7025.
>    >> > Suppose am interested in exact value such as
>    >> >
>    >> 
>    >> If it were really necessary, you would have to move to multiple
>    >> precision.  The gmp R package doesn't seem to yet cover 
> this, but 
>    >> FMLIB(TOMS814, DM Smith) is a multiple precision f90 
> library that 
>    S> does
>    >> include the incomplete beta -- it allows one to say for 
>    >> F(1,7025)=39540,P=6.31E-2886 (evaluated using 200 sign. 
> digit 
>    >> arithmetic).  Results from
>    >> R's pf() agree quite closely with the FMLIB results for 
> less 
>    >> extreme values
>    >> eg
>    >> > print(pf(1500,1,7025,lower=FALSE), digits=20)
>    >> [1] 1.3702710894887480597e-297
>    >> 
>    >> cf   
> 1.37027108948832580215549799419452388134616261215463681945E-297
>    >> 
>    >> 
>    >> | David Duffy (MBBS PhD)                                    
>     ,-
>    S> _|\
>    >> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -
> 0101  /  
>    >> *
>    >> | Epidemiology Unit, Queensland Institute of Medical 
> Research   
>    >> \_,-._/
>    >> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 
>    >> 4D0B994A v
>    >> 
>    >> ______________________________________________
>    >> R-help at stat.math.ethz.ch mailing list
>    >> https://stat.ethz.ch/mailman/listinfo/r-help
>    >> PLEASE do read the posting guide! http://www.R-
> project.org/posting-
>    >> guide.html
> 
>    S> ______________________________________________
>    S> R-help at stat.math.ethz.ch mailing list
>    S> https://stat.ethz.ch/mailman/listinfo/r-help
>    S> PLEASE do read the posting guide! http://www.R-
> project.org/posting-guide.html
> 
> 
>    S> !DSPAM:42d4da2c52151994935952!
>



From pingping.zheng at lancaster.ac.uk  Fri Jul 15 11:40:50 2005
From: pingping.zheng at lancaster.ac.uk (Pingping Zheng)
Date: Fri, 15 Jul 2005 10:40:50 +0100
Subject: [R] Fwd: Re:  East Asian language
In-Reply-To: <42D78041.90000@statistik.uni-dortmund.de>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>	<42D77DDC.3040001@lancs.ac.uk>
	<42D78041.90000@statistik.uni-dortmund.de>
Message-ID: <42D784A2.2070504@lancs.ac.uk>

That's pretty convenient. Set LANGUAGE in .Renviron
under current directory is convenient for both Linux and
windows users.

Thanks.

Pingping Zheng
Department of Mathematics and Statistics
Fylde College
Lancaster University
Lancaster LA1 4YF
UK


Uwe Ligges wrote:
> Pingping Zheng wrote:
> 
> 
>>Dear all,
>>
>>I think what Nan Lin wanted is "How to change the language used in R
>>console by re-setting something within R?"
>>
>>For a non-english user, sometime we prefer to set the system default
>>language to a native one and use english in individual programs when
>>we think necessary.
>>
>>a) setting the envrionment variable LANGUAGE=en will make an impact
>>     on the whole system, which has its side effects.
>>b) changing language setup through control panel is telling people to throw
>>     away their native windows system, let's all use English system.
>>
>>Any better idea?
> 
> 
> What about setting LANGUAGE in .Renviron?
> See ?.Renviron
> 
> Uwe Ligges
> 
> 
> 
>>--------
>>Pingping Zheng
>>Department of Mathematics and Statistics
>>Fylde College
>>Lancaster University
>>Lancaster LA1 4YF
>>UK
>>
>>
>>Baoqiang Cao wrote:
>>
>>
>>>---- Original message ----
>>>
>>>
>>>
>>>>Date: Thu, 14 Jul 2005 19:50:41 +0200
>>>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>  
>>>>Subject: Re: [R] East Asian language  
>>>>To: Nan Lin <nlin at math.wustl.edu>
>>>>Cc: r-help at stat.math.ethz.ch
>>>>
>>>>Nan Lin wrote:
>>>>
>>>>
>>>>
>>>>
>>>>>Dear all,
>>>>>
>>>>>I just installed R 2.1.1. The installation program
>>>
>>>automatically recognized 
>>>
>>>
>>>
>>>>>my Windows XP was using Chinese language, so now my R
>>>
>>>console displays 
>>>
>>>
>>>
>>>>>everything in Chinese. How can I still let R console
>>>
>>>display in English 
>>>
>>>
>>>
>>>>>without modifying my Window XP language setup? Thank you so
>>>
>>>much!
>>>
>>>
>>>
>>>>E.g. by setting the envrionment variable
>>>>LANGUAGE=en
>>>>
>>>
>>>Or, like what I did, change XP language setting back to
>>>English from the control panel.
>>>
>>>
>>>
>>>
>>>>Uwe Ligges
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>>Best,
>>>>>
>>>>>Nan
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide!
>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Soren.Hojsgaard at agrsci.dk  Fri Jul 15 11:59:08 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 15 Jul 2005 11:59:08 +0200
Subject: [R] Calculation of group summaries
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0308404B@DJFPOST01.djf.agrsci.dk>

Perhaps I lost track of what the original question was, but on my homepage http://genetics.agrsci.dk/~sorenh/misc/ there is a package called doBy in which there is a function called summaryBy (which mimics proc summary from sas). For example
	summaryBy(cbind(Weight,Feed)~Evit+Cu,  data=dietox12, FUN=c(mean,myvar))  
S??ren


-----Oprindelig meddelelse-----
Fra: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] P?? vegne af Gabor Grothendieck
Sendt: 15. juli 2005 04:43
Til: Seeliger.Curt at epamail.epa.gov
Cc: R-Help
Emne: Re: [R] Calculation of group summaries

1. Try using more spaces so your code is easier to read.

2. Use data.frame to define your data frame (since the method in your post creates data frames of factors rather than the desired classes).  

3. Given the appropriate function, f, a single 'by' statement rbind'ed together, as shown, will create the result.

nsites <- 6
yearList <- 1999:2001
fakesub <- data.frame(
	year = rep(yearList, nsites/length(yearList), each = 11),
	site_id  = rep(c('site1','site2'), each = 11*nsites),
	visit_no = rep(1, 11*2*nsites),
	transect = rep(LETTERS[1:11], nsites, each = 2),
	transdir = rep(c('LF','RT'), 11*nsites),
	undercut = abs(rnorm(11*2*nsites, 10)),
	angle    = runif(11*2*nsites, 0, 180)
)


f <- function(x) cbind(year = x[1,1], site_id = x[1,2], visit_no = x[1,3], 
	mean = mean(x[,6]), sd = sd(x[,6]), length = length(x[,6])) do.call("rbind", by(fakesub, fakesub[,1:3], f))





On 7/14/05, Seeliger.Curt at epamail.epa.gov <Seeliger.Curt at epamail.epa.gov> wrote:
> Several people suggested specific functions (by, tapply, sapply and 
> others); thanks for not blowing off a simple question regarding how to 
> do the following SQL in R:
> >   select year,
> >          site_id,
> >          visit_no,
> >          mean(undercut) AS meanUndercut,
> >          count(undercut) AS nUndercut,
> >          std(undercut) AS stdUndercut
> >   from channelMorphology
> >   group by year, site_id, visit_no
> >   ;
> 
> I'd spent quite a bit of time with the suggested functions earlier but 
> had no luck as I'd misread the docs and put the entire dataframe where 
> it only wants the columns to be processed.  Sometimes it's the 
> simplest of things.
> 
> This has lead to another confoundment-- sd() acts differently than
> mean() for some reason, at least with R 1.9.0.  For some reason, means 
> generate NA results and a warning message for each group:
> 
>  argument is not numeric or logical: returning NA in:
> mean.default(data[x, ], ...)
> 
> Of course, the argument is numeric, or there'd be no sd value.  Or 
> more likely, I'm still missing something really basic. If I wrap the 
> value in
> as.numeric() things work fine.  Why should I have to do this for mean 
> and median, but not sd? The code below should reproduce this error
> 
>  # Fake data for demo:
>  nsites<-6
>  yearList<-1999:2001
>  fakesub<-as.data.frame(cbind(
>                 year     =rep(yearList,nsites/length(yearList),each=11)
>                ,site_id  =rep(c('site1','site2'),each=11*nsites)
>                ,visit_no =rep(1,11*2*nsites)
>                ,transect =rep(LETTERS[1:11],nsites,each=2)
>                ,transdir =rep(c('LF','RT'),11*nsites)
>                ,undercut =abs(rnorm(11*2*nsites,10))
>                ,angle    =runif(11*2*nsites,0,180)
>                ))
> 
>  # Create group summaries:
>  sdmets<-by(fakesub$undercut
>            ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>            ,sd
>            )
>  nmets<-by(fakesub$undercut
>           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>           ,length
>           )
>  xmets<-by(fakesub$undercut
>           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>           ,mean
>           )
>   xmets<-by(as.numeric(fakesub$undercut)
>           ,list(fakesub$year,fakesub$site_id,fakesub$visit_no)
>           ,mean
>           )
> 
>  # Put site id values (year, site_id and visit_no) into results:
>  # List unique id combinations as a list of lists.  Then  # reorganize 
> that into 3 vectors for final results.
>  # Certainly, there MUST be a better way...
>  foo<-strsplit(unique(paste(fakesub$year
>                            ,fakesub$site_id
>                            ,fakesub$visit_no
>                            ,sep='#'))
>               ,split='#'
>               )
>  year<-list()
>  for(i in 1:length(foo)) {year<-rbind(year,foo[[i]][1])}
>  site_id<-list()
>  for(i in 1:length(foo)) {site_id<-rbind(site_id,foo[[i]][2])}
>  visit_no<-list()
>  for(i in 1:length(foo)) {visit_no<-rbind(visit_no,foo[[i]][3])}
> 
>  # Final result, more or less
>  data.frame(cbind(a=year,b=site_id,c=visit_no,sdmets,nmets,xmets))
> 
> 
> cur
> 
> --
> Curt Seeliger, Data Ranger
> CSC, EPA/WED contractor
> 541/754-4638
> seeliger.curt at epa.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.gherardini at pigrecodata.net  Fri Jul 15 14:42:38 2005
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 15 Jul 2005 14:42:38 +0200
Subject: [R] Padding in lattice plots
Message-ID: <200507151442.39139.f.gherardini@pigrecodata.net>

Hi all,
I've used the split argument to print four lattice plots on a single page. The 
problem now is that I need to reduce the amount of white space between the 
plots. I've read other mails in this list about the new trellis parameters 
layout.heights and layout.widhts but I haven't been able to use them 
properly. I've tried to input values between 0 and 1 as the padding value 
(both left and right and top and bottom) but nothing changed. It seems I can 
only increase the padding by using values > 1. Any ideas?

Thanks in advance for your help
Federico Gherardini



From ggrothendieck at gmail.com  Fri Jul 15 12:38:39 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 15 Jul 2005 06:38:39 -0400
Subject: [R] Fwd: Re: East Asian language
In-Reply-To: <42D784A2.2070504@lancs.ac.uk>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
	<42D77DDC.3040001@lancs.ac.uk>
	<42D78041.90000@statistik.uni-dortmund.de>
	<42D784A2.2070504@lancs.ac.uk>
Message-ID: <971536df0507150338da3f83e@mail.gmail.com>

I am not sure how generally this works but I was able to get it to switch
back and forth between two languages within  a single R
session like this:

> Sys.getenv("LANGUAGE")
LANGUAGE
      ""
> q() # note English prompt; c means continue rather than exiting
Save workspace image? [y/n/c]: c
> Sys.putenv(LANGUAGE="it")
> q() # note Italian prompt
Salva area di lavoro? [y/n/c]: c
> Sys.putenv(LANGUAGE="en")
> q() # now we are back to English
Save workspace image? [y/n/c]: n


On 7/15/05, Pingping Zheng <pingping.zheng at lancaster.ac.uk> wrote:
> That's pretty convenient. Set LANGUAGE in .Renviron
> under current directory is convenient for both Linux and
> windows users.
> 
> Thanks.
> 
> Pingping Zheng
> Department of Mathematics and Statistics
> Fylde College
> Lancaster University
> Lancaster LA1 4YF
> UK
> 
> 
> Uwe Ligges wrote:
> > Pingping Zheng wrote:
> >
> >
> >>Dear all,
> >>
> >>I think what Nan Lin wanted is "How to change the language used in R
> >>console by re-setting something within R?"
> >>
> >>For a non-english user, sometime we prefer to set the system default
> >>language to a native one and use english in individual programs when
> >>we think necessary.
> >>
> >>a) setting the envrionment variable LANGUAGE=en will make an impact
> >>     on the whole system, which has its side effects.
> >>b) changing language setup through control panel is telling people to throw
> >>     away their native windows system, let's all use English system.
> >>
> >>Any better idea?
> >
> >
> > What about setting LANGUAGE in .Renviron?
> > See ?.Renviron
> >
> > Uwe Ligges
> >
> >
> >
> >>--------
> >>Pingping Zheng
> >>Department of Mathematics and Statistics
> >>Fylde College
> >>Lancaster University
> >>Lancaster LA1 4YF
> >>UK
> >>
> >>
> >>Baoqiang Cao wrote:
> >>
> >>
> >>>---- Original message ----
> >>>
> >>>
> >>>
> >>>>Date: Thu, 14 Jul 2005 19:50:41 +0200
> >>>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> >>>>Subject: Re: [R] East Asian language
> >>>>To: Nan Lin <nlin at math.wustl.edu>
> >>>>Cc: r-help at stat.math.ethz.ch
> >>>>
> >>>>Nan Lin wrote:
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>Dear all,
> >>>>>
> >>>>>I just installed R 2.1.1. The installation program
> >>>
> >>>automatically recognized
> >>>
> >>>
> >>>
> >>>>>my Windows XP was using Chinese language, so now my R
> >>>
> >>>console displays
> >>>
> >>>
> >>>
> >>>>>everything in Chinese. How can I still let R console
> >>>
> >>>display in English
> >>>
> >>>
> >>>
> >>>>>without modifying my Window XP language setup? Thank you so
> >>>
> >>>much!
> >>>
> >>>
> >>>
> >>>>E.g. by setting the envrionment variable
> >>>>LANGUAGE=en
> >>>>
> >>>
> >>>Or, like what I did, change XP language setting back to
> >>>English from the control panel.
> >>>
> >>>
> >>>
> >>>
> >>>>Uwe Ligges
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>Best,
> >>>>>
> >>>>>Nan
> >>>>>
> >>>>>______________________________________________
> >>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide!
> >>>
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide!
> >>>
> >>>http://www.R-project.org/posting-guide.html
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Jul 15 12:55:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jul 2005 12:55:16 +0200
Subject: [R] Fwd: Re: East Asian language
In-Reply-To: <971536df0507150338da3f83e@mail.gmail.com>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
	<42D77DDC.3040001@lancs.ac.uk>
	<42D78041.90000@statistik.uni-dortmund.de>
	<42D784A2.2070504@lancs.ac.uk>
	<971536df0507150338da3f83e@mail.gmail.com>
Message-ID: <x2oe949w4r.fsf@turmalin.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> I am not sure how generally this works but I was able to get it to switch
> back and forth between two languages within  a single R
> session like this:
> 
> > Sys.getenv("LANGUAGE")
> LANGUAGE
>       ""
> > q() # note English prompt; c means continue rather than exiting
> Save workspace image? [y/n/c]: c
> > Sys.putenv(LANGUAGE="it")
> > q() # note Italian prompt
> Salva area di lavoro? [y/n/c]: c
> > Sys.putenv(LANGUAGE="en")
> > q() # now we are back to English
> Save workspace image? [y/n/c]: n

Hmm, doesn't quite seem to work on Fedora:

> Sys.putenv(LANGUAGE="it")
> q()
Save workspace image? [y/n/c]: c
> Sys.setenv(LANGUAGE="it")
Errore: non trovo la funzione "Sys.setenv"
> Sys.putenv(LANGUAGE="en")
> Sys.setenv(LANGUAGE="it")
Errore: non trovo la funzione "Sys.setenv"
> q()
Save workspace image? [y/n/c]: c

So the save prompt is untranslated (but this happens with 

LANG=it_IT R

as well(?!)), but worse, it doesn't seem to want to set the language
back to English.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From bhx2 at mevik.net  Fri Jul 15 13:21:54 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 15 Jul 2005 13:21:54 +0200
Subject: [R] PLS: problem transforming scores to variable space
In-Reply-To: <30877.1120558944@www59.gmx.net> (rainer grohmann's message of
	"Tue, 5 Jul 2005 12:22:24 +0200 (MEST)")
References: <30877.1120558944@www59.gmx.net>
Message-ID: <m064vctiul.fsf@bar.nemo-project.org>

rainer grohmann writes:

> However, when I try to map the scores back to variable space, I ran into
> problems:
[...]
> cbind(t$scores[,1],(t$scores%*%(t$loadings)%*%t$projection)[,1])

You need to transpose the loadings:

> all.equal(unclass(t$scores),
+           t$scores %*% t(t$loadings) %*% t$projection)
[1] TRUE

(A tip: Since 't' is used for transposing, it is usually a Good
Thing(TM) to avoid using it as a varable name.)

-- 
Bj??rn-Helge Mevik



From michael.hopkins at hopkins-research.com  Fri Jul 15 13:59:36 2005
From: michael.hopkins at hopkins-research.com (Michael Hopkins)
Date: Fri, 15 Jul 2005 12:59:36 +0100
Subject: [R] Another simple q - removing negative values
In-Reply-To: <mailman.11.1121421601.22334.r-help@stat.math.ethz.ch>
Message-ID: <BEFD63B8.3E4B9%michael.hopkins@hopkins-research.com>


Sorry folks but again I have failed in my understanding of how to do a very
simple thing.  I've read the various texts and searched the help archives
but no positive result so far.

I want to remove all the rows in a data frame where one of the variables has
negative values.  In approx Stata pseudocode:

   drop _all, if( x < 0 )

Please either point me to relevant sections of the docs or supply me with
solution - I'm sure it's very simple.

Michael



From f.gherardini at pigrecodata.net  Fri Jul 15 16:19:03 2005
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 15 Jul 2005 16:19:03 +0200
Subject: [R] Padding in lattice plots
In-Reply-To: <200507151442.39139.f.gherardini@pigrecodata.net>
References: <200507151442.39139.f.gherardini@pigrecodata.net>
Message-ID: <200507151619.03581.f.gherardini@pigrecodata.net>

On Friday 15 July 2005 14:42, you wrote:
> Hi all,
> I've used the split argument to print four lattice plots on a single page.
> The problem now is that I need to reduce the amount of white space between
> the plots. I've read other mails in this list about the new trellis
> parameters layout.heights and layout.widhts but I haven't been able to use
> them properly. I've tried to input values between 0 and 1 as the padding
> value (both left and right and top and bottom) but nothing changed. It
> seems I can only increase the padding by using values > 1. Any ideas?
>
> Thanks in advance for your help
> Federico Gherardini
It seems like I've found an answer myself.... you have to use negative values 
to decrease the padding. I thought it was something like the cex parameter 
which acts like a multiplier but this is not the case.

Thanks anyway
Federico



From p.dalgaard at biostat.ku.dk  Fri Jul 15 14:16:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jul 2005 14:16:43 +0200
Subject: [R] Another simple q - removing negative values
In-Reply-To: <BEFD63B8.3E4B9%michael.hopkins@hopkins-research.com>
References: <BEFD63B8.3E4B9%michael.hopkins@hopkins-research.com>
Message-ID: <x2fyug9sd0.fsf@turmalin.kubism.ku.dk>

Michael Hopkins <michael.hopkins at hopkins-research.com> writes:

> Sorry folks but again I have failed in my understanding of how to do a very
> simple thing.  I've read the various texts and searched the help archives
> but no positive result so far.
> 
> I want to remove all the rows in a data frame where one of the variables has
> negative values.  In approx Stata pseudocode:
> 
>    drop _all, if( x < 0 )
> 
> Please either point me to relevant sections of the docs or supply me with
> solution - I'm sure it's very simple.

There are variations:

d1 <- subset(d, x < 0)

d1 <- d[d$x < 0,]
d1 <- d[d$x < 0 & !is.na(d$x),] # essentially same as subset()

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From matthew_wiener at merck.com  Fri Jul 15 14:19:24 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 15 Jul 2005 08:19:24 -0400
Subject: [R] Another simple q - removing negative values
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04994897@uswsmx03.merck.com>

To leave x with only the non-negative elements, you can use x[x >= 0].  Also
see the function "subset".

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Hopkins
Sent: Friday, July 15, 2005 8:00 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Another simple q - removing negative values



Sorry folks but again I have failed in my understanding of how to do a very
simple thing.  I've read the various texts and searched the help archives
but no positive result so far.

I want to remove all the rows in a data frame where one of the variables has
negative values.  In approx Stata pseudocode:

   drop _all, if( x < 0 )

Please either point me to relevant sections of the docs or supply me with
solution - I'm sure it's very simple.

Michael

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jul 15 14:25:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Jul 2005 13:25:23 +0100 (BST)
Subject: [R] Fwd: Re: East Asian language
In-Reply-To: <x2oe949w4r.fsf@turmalin.kubism.ku.dk>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
	<42D77DDC.3040001@lancs.ac.uk>
	<42D78041.90000@statistik.uni-dortmund.de>
	<42D784A2.2070504@lancs.ac.uk>
	<971536df0507150338da3f83e@mail.gmail.com>
	<x2oe949w4r.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0507151313420.17051@gannet.stats>

On Fri, 15 Jul 2005, Peter Dalgaard wrote:

> Gabor Grothendieck <ggrothendieck at gmail.com> writes:
>
>> I am not sure how generally this works but I was able to get it to switch
>> back and forth between two languages within  a single R
>> session like this:

The posting guide does ask people to read the documentation, and this is 
documented in the appropriate manual.

>>> Sys.getenv("LANGUAGE")
>> LANGUAGE
>>       ""
>>> q() # note English prompt; c means continue rather than exiting
>> Save workspace image? [y/n/c]: c
>>> Sys.putenv(LANGUAGE="it")
>>> q() # note Italian prompt
>> Salva area di lavoro? [y/n/c]: c
>>> Sys.putenv(LANGUAGE="en")
>>> q() # now we are back to English
>> Save workspace image? [y/n/c]: n
>
> Hmm, doesn't quite seem to work on Fedora:
>
>> Sys.putenv(LANGUAGE="it")
>> q()
> Save workspace image? [y/n/c]: c
>> Sys.setenv(LANGUAGE="it")
> Errore: non trovo la funzione "Sys.setenv"
>> Sys.putenv(LANGUAGE="en")
>> Sys.setenv(LANGUAGE="it")
> Errore: non trovo la funzione "Sys.setenv"
>> q()
> Save workspace image? [y/n/c]: c
>
> So the save prompt is untranslated (but this happens with
>
> LANG=it_IT R
>
> as well(?!)), but worse, it doesn't seem to want to set the language
> back to English.

This is not intended to work: it is documented that LANGUAGE should be set 
outside the process.  Does no one read the manuals?  R-admin says

   `Note that you should not expect to be able to change the language once
    R is running ...'

It is phrased that way because it _sometimes_ will work on _some_ OSes and 
on _some_ versions of gettext.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jul 15 14:31:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Jul 2005 13:31:56 +0100 (BST)
Subject: [R] Fwd: Re:  East Asian language
In-Reply-To: <42D78041.90000@statistik.uni-dortmund.de>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
	<42D77DDC.3040001@lancs.ac.uk>
	<42D78041.90000@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0507151327360.17051@gannet.stats>

On Fri, 15 Jul 2005, Uwe Ligges wrote:

> Pingping Zheng wrote:
>
>> I think what Nan Lin wanted is "How to change the language used in R
>> console by re-setting something within R?"
>>
>> For a non-english user, sometime we prefer to set the system default
>> language to a native one and use english in individual programs when
>> we think necessary.
>>
>> a) setting the envrionment variable LANGUAGE=en will make an impact
>>      on the whole system, which has its side effects.

I'd like to see your evidence for that.  gettext deliberately chose 
LANGUAGE because it was not commonly used, and I know of no application 
other than those based on gettext (like R) which makes use of it.  It is 
not at all common for Windows applications to use such environment 
variables unless they are ported from the Unix world.

>> b) changing language setup through control panel is telling people to throw
>>      away their native windows system, let's all use English system.
>>
>> Any better idea?
>
> What about setting LANGUAGE in .Renviron?
> See ?.Renviron

Or reading the rw-FAQ Q2.17 and seeing other ways to do so.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Jul 15 14:36:16 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Jul 2005 14:36:16 +0200
Subject: [R] unexpected par('pin') behaviour
In-Reply-To: <42D68297.8030900@fz-rossendorf.de>
References: <42D51E9A.8070702@fz-rossendorf.de>
	<17110.16321.138943.126111@stat.math.ethz.ch>
	<42D68297.8030900@fz-rossendorf.de>
Message-ID: <17111.44480.668877.434236@stat.math.ethz.ch>

Thank you, Joerg,
for the reproducable example 
 
>>>>> "joerg" == joerg van den hoff <j.van_den_hoff at fz-rossendorf.de>
>>>>>     on Thu, 14 Jul 2005 17:19:51 +0200 writes:

  ......................
  ......................

    joerg> ===========cut====================
    joerg> graphics.off()

    joerg> f <- function(n=7, m=6) {
    joerg> nm <- n*m
    joerg> layout(matrix(1:(nm),n,m))
    joerg> opar <- par(no.readonly = T)
    joerg> on.exit(par(opar))
    joerg> par(mar = c(4.1, 4.1, 1.1, 0.1))
    joerg> for (i in 1:nm) plot(i, pch=(i-1)%%25+1)
    joerg> layout(1)
    joerg> }
    joerg> f(5) #good
    joerg> par('pin')
    joerg> f()  #bad (at least for x11() default size)
    joerg> par('pin')
    joerg> ===========cut====================

which I can simplify to

  graphics.off()
  layout(matrix(1:42,7,6))
  par("pin")

  ## [1]  0.2918620 -0.2974408  --- when using x11() "default" 

clearly a bug in layout()  {which I'll file}

and it seems to me, one that is not shared by
par(mfrow= / mfcol=) settings.

Martin Maechler, ETH Zurich



From jsorkin at grecc.umaryland.edu  Fri Jul 15 14:48:17 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 15 Jul 2005 08:48:17 -0400
Subject: [R] Dividing a vector into ntiles
Message-ID: <s2d7786e.008@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050715/4b2331f0/attachment.pl

From pingping.zheng at lancaster.ac.uk  Fri Jul 15 14:49:37 2005
From: pingping.zheng at lancaster.ac.uk (Pingping Zheng)
Date: Fri, 15 Jul 2005 13:49:37 +0100
Subject: [R] Fwd: Re:  East Asian language
In-Reply-To: <Pine.LNX.4.61.0507151327360.17051@gannet.stats>
References: <d80a342b.1bd94b37.8251100@mirapoint.uc.edu>
	<42D77DDC.3040001@lancs.ac.uk>
	<42D78041.90000@statistik.uni-dortmund.de>
	<Pine.LNX.4.61.0507151327360.17051@gannet.stats>
Message-ID: <42D7B0E1.30907@lancs.ac.uk>

I'm not sure what other applications use environment variable like "LANGUAGE".
I'm sorry I was so assertive. However, what I said is right in the sense that
environment variables are supposed to be used NOT by one specific application.
I happen to use lots of GnuWin32 and Cygwin applications such as lots of Unix
tools though I am not sure what will happen after setting system wise variable 
"LANGUAGE".

It's a good idea to set "LANGUAGE" in .Renviron under current directory before
running R.

Thanks.


Pingping Zheng
Department of Mathematics and Statistics
Fylde College
Lancaster University
Lancaster LA1 4YF
UK


Prof Brian Ripley wrote:
> On Fri, 15 Jul 2005, Uwe Ligges wrote:
> 
>> Pingping Zheng wrote:
>>
>>> I think what Nan Lin wanted is "How to change the language used in R
>>> console by re-setting something within R?"
>>>
>>> For a non-english user, sometime we prefer to set the system default
>>> language to a native one and use english in individual programs when
>>> we think necessary.
>>>
>>> a) setting the envrionment variable LANGUAGE=en will make an impact
>>>      on the whole system, which has its side effects.
> 
> 
> I'd like to see your evidence for that.  gettext deliberately chose 
> LANGUAGE because it was not commonly used, and I know of no application 
> other than those based on gettext (like R) which makes use of it.  It is 
> not at all common for Windows applications to use such environment 
> variables unless they are ported from the Unix world.
> 
>>> b) changing language setup through control panel is telling people to 
>>> throw
>>>      away their native windows system, let's all use English system.
>>>
>>> Any better idea?
>>
>>
>> What about setting LANGUAGE in .Renviron?
>> See ?.Renviron
> 
> 
> Or reading the rw-FAQ Q2.17 and seeing other ways to do so.
>



From ccleland at optonline.net  Fri Jul 15 14:55:00 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 15 Jul 2005 08:55:00 -0400
Subject: [R] Dividing a vector into ntiles
In-Reply-To: <s2d7786e.008@grecc.umaryland.edu>
References: <s2d7786e.008@grecc.umaryland.edu>
Message-ID: <42D7B224.1020600@optonline.net>

?quantile

quantile(runif(1000), probs=seq(0,1,1/3))

John Sorkin wrote:
> R 2.1.1
> Win 2k
>  
> Would someone suggest a method (or methods) that can be used to
> determine ntile cutpoints of a vector, i.e. to determine values that can
> be used to divide a vector into thirds such as 0-33 centile, 34-66
> centile, 67-100 centile. If for example I had a vector:
> 1,2,3,4,5,6,7,8,9
> and wanted to divide the vector into thirds
> I would have cut-points of 3, and 6.
> Thanks,
> John
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Jul 15 14:57:30 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 15 Jul 2005 08:57:30 -0400
Subject: [R] Passing character strings from C code to R
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40DA@us-arlington-0668.mail.saic.com>

Hi,

I have a C code which produces array of integers and potentially a string,
and I have problems passing the string out. Here is the relevant part of the
code:

 1   PROTECT(Ret = allocVector(INTSXP, n));
 2   ret     = (int*) INTEGER(Ret);  /* get pointer to R's Ret */
 3   for(i=0; i<n; i++) ret[i] = data[i];
 4   Free(data);
 5   i=1;
 6   if (comment) { // comment was found
 7     n = strlen(comment);
 8     if(n>0) {    // and it actually have some length
 9        Rprintf(" '%s' %i\n", comment, n); 
 10       PROTECT(Str = allocString(n+1)); 
 11       str = CHAR(STRING_ELT(Str, 0));
 12       strcpy(str, comment);
 13       Rprintf(" '%s' %i\n", str, n);
 14       setAttrib(Ret, install("comm"), Str);
 15       i=2;
 16     }
 17     Free(comment);
 18   }
 20   UNPROTECT(i);  

Print statement in line 9 gives right results, but program crashes before
print statement in line 13.
Any ideas to what am I doing wrong? 

Jarek
=====================================\====                 
 Jarek Tuszynski, PhD.                               o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                        ">  \
 Jaroslaw.W.Tuszynski at saic.com                   `    \



From matthew_wiener at merck.com  Fri Jul 15 14:59:19 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 15 Jul 2005 08:59:19 -0400
Subject: [R] Dividing a vector into ntiles
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E0499489B@uswsmx03.merck.com>

You could try using "cut":
> cut(1:9, breaks = 3)
[1] (0.992,3.66] (0.992,3.66] (0.992,3.66] (3.66,6.34]  (3.66,6.34] 
[6] (3.66,6.34]  (6.34,9.01]  (6.34,9.01]  (6.34,9.01] 
Levels: (0.992,3.66] (3.66,6.34] (6.34,9.01]
>

Quantile is another possible solution to get break points:
> quantile(1:9, p = c(0, .33, .66, 1))
  0%  33%  66% 100% 
1.00 3.64 6.28 9.00 
>
You could use rounding if you want the integers, or use the given cutpoints.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
Sent: Friday, July 15, 2005 8:48 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Dividing a vector into ntiles


R 2.1.1
Win 2k
 
Would someone suggest a method (or methods) that can be used to
determine ntile cutpoints of a vector, i.e. to determine values that can
be used to divide a vector into thirds such as 0-33 centile, 34-66
centile, 67-100 centile. If for example I had a vector:
1,2,3,4,5,6,7,8,9
and wanted to divide the vector into thirds
I would have cut-points of 3, and 6.
Thanks,
John
 
John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC
 
University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
 
410-605-7119 
-- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Jul 15 15:00:12 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Jul 2005 15:00:12 +0200
Subject: [R] unexpected par('pin') behaviour
In-Reply-To: <17111.44480.668877.434236@stat.math.ethz.ch>
References: <42D51E9A.8070702@fz-rossendorf.de>
	<17110.16321.138943.126111@stat.math.ethz.ch>
	<42D68297.8030900@fz-rossendorf.de>
	<17111.44480.668877.434236@stat.math.ethz.ch>
Message-ID: <17111.45916.481293.903162@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Fri, 15 Jul 2005 14:36:16 +0200 writes:

    ............

    MM> which I can simplify to

    MM> graphics.off()
    MM> layout(matrix(1:42,7,6))
    MM> par("pin")

    MM> ## [1]  0.2918620 -0.2974408  --- when using x11() "default" 

    MM> clearly a bug in layout()  {which I'll file}

exploring more before filing a bug report,

I've now seen that this  *NO BUG*  of layout() 
but rather of the ``design bug'' of old-style graphics which
bites here.

    MM> and it seems to me, one that is not shared by
    MM> par(mfrow= / mfcol=) settings.

That's wrong, since

  postscript(paper="a4")
  par(mfrow=c(7,6))
  par("pin")
  ## [1]  1.0470855 -0.1047263

and when plot() you get message about margins being too wide.

Only after something like

  par(mar= rep(1,4))

you get both valid "pin" and can plot properly.

That's nothing new and not really astonishing,
...

Martin Maechler, ETH Zurich



From volker.schlecht at wiwi.uni-karlsruhe.de  Fri Jul 15 14:43:13 2005
From: volker.schlecht at wiwi.uni-karlsruhe.de (Volker Schlecht)
Date: Fri, 15 Jul 2005 14:43:13 +0200
Subject: [R]  Ordinal data - Regression Trees  & Proportional Odds
Message-ID: <42D7AF61.507@wiwi.uni-karlsruhe.de>

Dear Dr. Fieberg,

you used a regression tree approach to explore ordinal data set in addition to 
the proportinal odds model. I find this very interesting. I would like to know, how good the 
results of the regression tree approach turned out in comparison to the 
proportional odds model. Since people very often treat ordinal data as continuous, I would 
like to know how successfull this strategy is. I mean, I can see, what may be a reason
to do this: One could argue that ordinal data an a sufficiently large scale (say 1 to 10) are just 
coarsely measured continuous data. 

I would be delighted if you could tell me your experiences about the treatment of ordinal data with 
models for continuous data. Also I would be delighted, if you could tell me, if your results are published somewhere.

Best wishes,

Volker Schlecht



From maechler at stat.math.ethz.ch  Fri Jul 15 15:27:56 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Jul 2005 15:27:56 +0200
Subject: [R] Partek has Dunn-Sidak Multiple Test Correction. Is this
	the	same/similar to any of R's p.adjust.methods?
In-Reply-To: <db671a$bjl$1@sea.gmane.org>
References: <db671a$bjl$1@sea.gmane.org>
Message-ID: <17111.47580.681299.500224@stat.math.ethz.ch>

>>>>> "Earl" == Earl F Glynn <efg at stowers-institute.org>
>>>>>     on Thu, 14 Jul 2005 12:22:49 -0500 writes:

    Earl> The Partek package (www.partek.com) allows only two selections for Multiple
    Earl> Test Correction:  Bonferroni and Dunn-Sidak.  Can anyone suggest why Partek
    Earl> implemented Dunn-Sidak and not the other methods that R has?  Is there any
    Earl> particular advantage to the Dunn-Sidak method?
    Earl> R knows about these methods (in R 2.1.1):

    >> p.adjust.methods
    Earl> [1] "holm" "hochberg" "hommel" "bonferroni" "BH" "BY" "fdr"
    Earl> [8] "none"

    Earl> BH is Benjamini & Hochberg (1995) and is also called "fdr" (I wish R's
    Earl> documentation said this clearly).  BY is Benjamini & Yekutieli (2001).

The current R docu has

  >>   The '"BH"' and '"BY"' method of Benjamini, Hochberg, and Yekutieli
  >>   control the false discovery rate, the expected proportion of false
  >>   discoveries amongst the rejected hypotheses.  The false discovery
  >>   rate is a less stringent condition than the family wise error
  >>   rate, so these methods are more powerful than the others.

so both "BH" and "BY"   ``are FDR versions''. 
"fdr" was used - unfortunately - in some older versions of R,
so we kept it working as an *alias* for the time being.  
You should rather not know about it :-)
and use "BH" or "BY" (and maybe other methods in the future) instead.

Regards,

Martin



From dmbates at gmail.com  Fri Jul 15 15:35:29 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 15 Jul 2005 08:35:29 -0500
Subject: [R] crossed random fx nlme lme4
In-Reply-To: <6.2.1.2.0.20050714122836.01c9cc20@mail.ozemail.com.au>
References: <F0F05839-744A-4864-97A5-073F8286356B@ucdavis.edu>
	<6.2.1.2.0.20050714122836.01c9cc20@mail.ozemail.com.au>
Message-ID: <40e66e0b05071506356dde573e@mail.gmail.com>

On 7/13/05, Simon Blomberg <blomsp at ozemail.com.au> wrote:
> At 09:35 AM 14/07/2005, Emilio A. Laca wrote:
> >I need to specify a model similar to this
> >
> >lme.formula(fixed = sqrt(lbPerAc) ~ y + season + y:season, data = cy,
> >      random = ~y | observer/set, correlation = corARMA(q = 6))
> >
> >except that observer and set are actually crossed instead of nested.
> 
> Does this work for you? (following P&B pp 162-3 and an R-help archive
> search on "crossed random effects")...
> 
> fit <- lme(sqrt(lbPerAc) ~ y * season, random=list(pdBlocked(pdIdent(~y),
> pdIdent(observer-1), pdIdent(set-1))), correlation=corARMA(q = 6), data=cy)
> 
> lme isn't very well set up for crossed random effects. It's easier in lmer.
> I don't think lmer can handle alternative correlation structures yet,
> though. (Prof. Bates?)

Exactly.  Thanks for the summary.

> 
> HTH,
> 
> Simon.
> 
> 
> >observer and set are factors
> >y and lbPerAc are numeric
> >
> >If you know how to do it or have suggestions for reading I will be
> >grateful.
> >
> >
> >eal
> >
> >ps I have already read Pinheiro & Bates, the jan 05 newsletter, and
> >several postings.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
> Centre for Resource and Environmental Studies
> The Australian National University
> Canberra ACT 0200
> Australia
> T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
> F: +61 2 6125 0757
> CRICOS Provider # 00120C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kar at itga.com.au  Fri Jul 15 15:39:37 2005
From: kar at itga.com.au (Kylie-Anne Richards)
Date: Fri, 15 Jul 2005 23:39:37 +1000
Subject: [R]  Coxph with factors
Message-ID: <002c01c58942$a5379fa0$0400a8c0@KAR>

My apologies for lack of clarity.

Sample data:

      time.sec po vo done f.pom
      55 0 4402 1 -0.25
      378 -0.5 50000 1 -0.25
      51 0 1000 1 -0.25
      43 0 71581 1 -0.25

etc

>        DATA<-data.frame(time.sec,done,f.pom=factor(f.pom),po,vo)
>        final<-coxph(Surv(time.sec,done)~f.pom+vo+po,data=DATA)
                      coef exp(coef) se(coef)       z       p
f.pom-2.5  1.30e+00      3.65 1.66e-01   7.822 5.2e-15
f.pom-2    1.11e+00      3.02 1.68e-01   6.604 4.0e-11
f.pom-1.5  1.37e+00      3.95 1.59e-01   8.656 0.0e+00
f.pom-1    1.76e+00      5.79 1.56e-01  11.286 0.0e+00
f.pom-0.5  2.10e+00      8.14 1.52e-01  13.757 0.0e+00
f.pom0     4.96e+00    142.10 1.95e-01  25.433 0.0e+00
f.pom0.5   4.69e+00    108.71 1.53e-01  30.573 0.0e+00
f.pom1     4.66e+00    105.50 1.58e-01  29.575 0.0e+00
f.pom1.5   4.66e+00    105.62 1.60e-01  29.051 0.0e+00
f.pom2     4.61e+00    100.77 1.60e-01  28.792 0.0e+00
vo           -6.93e-05      1.00 6.57e-06 -10.548 0.0e+00
po           2.65e-03      1.00 1.29e-03   2.047 4.1e-02

 >final.surv<-survfit((final), individual=T,conf.type="log-log")
 >print(final.surv)

  time n.risk n.event survival  std.err lower 95% CI upper 95% CI
     0  13922    3914 8.10e-01 3.82e-03     8.02e-01      0.81712
     1   9726      91 7.98e-01 3.97e-03     7.90e-01      0.80547
     2   9457      46 7.92e-01 4.04e-03     7.84e-01      0.79949
     3   9297      36 7.87e-01 4.10e-03     7.79e-01      0.79477
etc

FIRST Q: The default uses the mean of 'vo' and mean of 'po', but what is it 
using for the factors?? Is it the sum of the coef of the factors divided by 
the number of factors??

SECOND Q: For a model with covariates I would normally specify: 
final.surv<-survfit((final), data.frame(po=0,vo=0,pom=0,individual=T)) to 
get the baseline survival prob.; what would I specify for a model with a 
factor, i.e., 'f.pom' ??

E.g. one of the many variations I have tried:
>         final.surv<-survfit((final), 
> data.frame(po=0,vo=0,f.pom=0,individual=T))
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
        variable lengths differ
In addition: Warning message:
'newdata' had 1 rows but variable(s) found have 13922 rows
Execution halted

I appreciate the time anyone can spare to help me with my problem, and 
apologies for it's simplistic nature.

Many thanks,

Kylie-Anne



Adaikalavan Ramasamy ramasamy at cancer.org.uk
Fri Jul 15 09:58:19 CEST 2005

  a.. Previous message: [R] Coxph with factors
  b.. Next message: [R] Pearson dispersion statistic
  c.. Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]

--------------------------------------------------------------------------------

Yes, and please show us a reproducible example or small section of thedata 
as well as the error output. On Thu, 2005-07-14 at 15:36 +0000, Dieter Menne 
wrote:> Kylie-Anne Richards <kar <at> itga.com.au> writes:> > > > > I am 
fitting a coxph model with factors. I am running into problems when> > using 
'survfit'. I am unsure how R is treating the factors when I fit, say:> > > 
DATA<-data.frame(time.sec,done,f.pom=factor(f.pom),po,vo)> > > 
final<-coxph(Surv(time.sec,done)~f.pom*vo+po,data=DATA)> > > 
final.surv<-survfit((final), individual=T,conf.type="log-log")> > > 
print(final.surv)> ....> > Better chances to get a reply when you tell us 
what problems you are running > into.> > Dieter Menne



From rvaradha at jhsph.edu  Fri Jul 15 15:49:46 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 15 Jul 2005 09:49:46 -0400
Subject: [R] Dividing a vector into ntiles
In-Reply-To: <s2d7786e.008@grecc.umaryland.edu>
Message-ID: <OWA-1cf3ednHM3SOlbp00007146@owa-1.sph.ad.jhsph.edu>

Here is an example that shows how to divide a vector into "quartiles" and
create a categorical (factor) variable with 4 levels ("A", "B", "C", "D")
from it:

x <- rnorm(100)
xcat <- factor(cut(x, quantile(x), include.lowest = TRUE), 
                  labels = LETTERS[1:4])
table(xcat)

Ravi.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
> Sent: Friday, July 15, 2005 8:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Dividing a vector into ntiles
> 
> R 2.1.1
> Win 2k
> 
> Would someone suggest a method (or methods) that can be used to
> determine ntile cutpoints of a vector, i.e. to determine values that can
> be used to divide a vector into thirds such as 0-33 centile, 34-66
> centile, 67-100 centile. If for example I had a vector:
> 1,2,3,4,5,6,7,8,9
> and wanted to divide the vector into thirds
> I would have cut-points of 3, and 6.
> Thanks,
> John
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From peteoutside at yahoo.com  Fri Jul 15 16:23:20 2005
From: peteoutside at yahoo.com (Pete Cap)
Date: Fri, 15 Jul 2005 07:23:20 -0700 (PDT)
Subject: [R] Newbie guide for plot & graphics functions?
In-Reply-To: <200507151619.03581.f.gherardini@pigrecodata.net>
Message-ID: <20050715142320.1718.qmail@web52401.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050715/20132d3a/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Jul 15 16:31:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Jul 2005 16:31:55 +0200
Subject: [R] Newbie guide for plot & graphics functions?
In-Reply-To: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
References: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
Message-ID: <42D7C8DB.8040700@statistik.uni-dortmund.de>

Pete Cap wrote:

> Hello all,
> 
> Can anyone point me to a decent introduction to using the plotting and assorted graphics functions in R?
>  
> I keep getting simple errrors and I can't figure out why, for example:
>  
> 
>>image(x,y,z)
> 
> Error in image.default(x, y, z) : dimensions of z are not length(x)(+1) times length(y)(+1)
> 
>>length(x)
> 
> [1] 206
> 
>>length(y)
> 
> [1] 40
> 
>>dim(z)
> 
> [1] 207  41
> 
>  
> It seems to me as if R is wrong--the matrix z is obviously of length(x)+1 times length(y)+1--but just as obviously I'm missing something essential.  Are there any "newbie" guides out there for getting started?


Can you specify a reproducible examples, please?

Uwe Ligges


> TIA,
>  
> Pete
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Fri Jul 15 16:31:17 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 15 Jul 2005 07:31:17 -0700 (PDT)
Subject: [R] Coxph with factors
In-Reply-To: <002c01c58942$a5379fa0$0400a8c0@KAR>
References: <002c01c58942$a5379fa0$0400a8c0@KAR>
Message-ID: <Pine.A41.4.61b.0507150723370.363424@homer06.u.washington.edu>

On Fri, 15 Jul 2005, Kylie-Anne Richards wrote:
>
> FIRST Q: The default uses the mean of 'vo' and mean of 'po', but what is it
> using for the factors?? Is it the sum of the coef of the factors divided by
> the number of factors??

It uses the mean of each factor variable.  The $means component of the fit 
gives the values it uses.

> SECOND Q: For a model with covariates I would normally specify:
> final.surv<-survfit((final), data.frame(po=0,vo=0,pom=0,individual=T)) to
> get the baseline survival prob.; what would I specify for a model with a
> factor, i.e., 'f.pom' ??

I would first note that the survival function at zero covariates is not a 
very useful thing and is usually numerically unstable, and it's often more 
useful to get the survival function at some reasonable set of covariates.

In any case, to specify f.pom You need it to be a factor with 
the same set of levels.  You don't say what the lowest level of pom is, 
but if it is, say, -3.

f.pom=factor(-3, levels=seq(-3,2.5, by=0.5))


 	=thomas



From christophe.pouzat at univ-paris5.fr  Fri Jul 15 16:41:40 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 15 Jul 2005 16:41:40 +0200
Subject: [R] Newbie guide for plot & graphics functions?
In-Reply-To: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
References: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
Message-ID: <42D7CB24.1020900@univ-paris5.fr>

If you have a little bit of time, check out Ross Ihaka's Lectures on 
"Information Visualisation". The specific examples he gives are built 
with R:

http://www.stat.auckland.ac.nz/~ihaka/120/

Time could be required because this stuff is so great that you want to 
read all of it...

Christophe.

Pete Cap wrote:

>Hello all,
>
>Can anyone point me to a decent introduction to using the plotting and assorted graphics functions in R?
> 
>I keep getting simple errrors and I can't figure out why, for example:
> 
>  
>
>>image(x,y,z)
>>    
>>
>Error in image.default(x, y, z) : dimensions of z are not length(x)(+1) times length(y)(+1)
>  
>
>>length(x)
>>    
>>
>[1] 206
>  
>
>>length(y)
>>    
>>
>[1] 40
>  
>
>>dim(z)
>>    
>>
>[1] 207  41
>
> 
>It seems to me as if R is wrong--the matrix z is obviously of length(x)+1 times length(y)+1--but just as obviously I'm missing something essential.  Are there any "newbie" guides out there for getting started?
>TIA,
> 
>Pete
>
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From deepayan.sarkar at gmail.com  Fri Jul 15 17:00:30 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 15 Jul 2005 10:00:30 -0500
Subject: [R] Padding in lattice plots
In-Reply-To: <200507151619.03581.f.gherardini@pigrecodata.net>
References: <200507151442.39139.f.gherardini@pigrecodata.net>
	<200507151619.03581.f.gherardini@pigrecodata.net>
Message-ID: <eb555e6605071508005fe5f93e@mail.gmail.com>

On 7/15/05, Federico Gherardini <f.gherardini at pigrecodata.net> wrote:
> On Friday 15 July 2005 14:42, you wrote:
> > Hi all,
> > I've used the split argument to print four lattice plots on a single page.
> > The problem now is that I need to reduce the amount of white space between
> > the plots. I've read other mails in this list about the new trellis
> > parameters layout.heights and layout.widhts but I haven't been able to use
> > them properly. I've tried to input values between 0 and 1 as the padding
> > value (both left and right and top and bottom) but nothing changed. It
> > seems I can only increase the padding by using values > 1. Any ideas?
> >
> > Thanks in advance for your help
> > Federico Gherardini
> It seems like I've found an answer myself.... you have to use negative values
> to decrease the padding. I thought it was something like the cex parameter
> which acts like a multiplier 

I thought so too. 

> but this is not the case.

Could you post what you used? There are several different padding
parameters you need to set to 0, did you change them all?

Deepayan



From r.hankin at noc.soton.ac.uk  Fri Jul 15 17:00:44 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 15 Jul 2005 16:00:44 +0100
Subject: [R] glm(family=binomial(link=logit))
Message-ID: <C65F84A4-7326-4AEE-A1FF-BEF577ECF46D@soc.soton.ac.uk>

Hi

I am trying to make glm() work to analyze a toy logit system.

I have a dataframe with x and y independent variables.  I have

L=1+x-y          (ie coefficients 1,1,-1)

then if I have a logit relation with L=log(p/(1-p)),
p=1/(1+exp(L)).

If I interpret "p" as the probability of  success in a Bernouilli
trial, and I can observe the result (0 for "no", 1 for "yes")
how do I retrieve the coefficients c(1,1,-1)
from the data?

n <- 300
des <- data.frame(x=(1:n)/n,y=sample(n)/n)   # experimental design
des <- cbind(des,L=1+des$x-des$y)            # L=1+x-y
des <- cbind(des,p=1/(1+exp(des$L)))         # p=1/(1+e^L)
des <- cbind(des,obs=rbinom(n,1,des$p))      # observation: prob of  
success = p.


My attempt is:

glm(obs~x+y,data=des,family=binomial(link="logit"))

But it does not retrieve the correct coefficients of c(1,1,-1) ;
I would expect a reasonably close answer with so much data.

What is the correct glm() call to perform my logit analysis?





--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From macq at llnl.gov  Fri Jul 15 17:06:55 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 15 Jul 2005 08:06:55 -0700
Subject: [R] Newbie guide for plot & graphics functions?
In-Reply-To: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
References: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
Message-ID: <p06210202befd7ddc96ae@[128.115.153.6]>

I think you have it backwards (the error message not withstanding). 
Read the documentation for image, note the beginning of the Details 
section, and then compare these two examples, both of which succeed 
on my system (R 2.1.1).

x <- 1:10
y <- 1:15
z <- outer(x,y)
image(x,y,z)

xb <- c(.5,x+.5)
yb <- c(.5,y+.5)
image(x,y,z)

x and y provide either the centers of the cells, or the boundaries of 
the cells. In the latter case, the lengths of x and y should be 1 
greater than the dim of z, not one less.

(I'm including my full version info to be complete, but I doubt that 
you problem has to do with your platform [OS]).

-Don

>  version
          _                       
platform powerpc-apple-darwin7.9.0
arch     powerpc                 
os       darwin7.9.0             
system   powerpc, darwin7.9.0    
status                           
major    2                       
minor    1.1                     
year     2005                    
month    06                      
day      20                      
language R                       


At 7:23 AM -0700 7/15/05, Pete Cap wrote:
>Hello all,
>
>Can anyone point me to a decent introduction to using the plotting 
>and assorted graphics functions in R?
>
>I keep getting simple errrors and I can't figure out why, for example:
>
>>  image(x,y,z)
>Error in image.default(x, y, z) : dimensions of z are not 
>length(x)(+1) times length(y)(+1)
>>  length(x)
>[1] 206
>>  length(y)
>[1] 40
>>  dim(z)
>[1] 207  41
>
>
>It seems to me as if R is wrong--the matrix z is obviously of 
>length(x)+1 times length(y)+1--but just as obviously I'm missing 
>something essential.  Are there any "newbie" guides out there for 
>getting started?
>TIA,
>
>Pete
>
>
>
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ftorrei2 at uiuc.edu  Fri Jul 15 17:17:36 2005
From: ftorrei2 at uiuc.edu (ftorrei2@uiuc.edu)
Date: Fri, 15 Jul 2005 10:17:36 -0500
Subject: [R] pdf() black&white; panel titles in lattice
Message-ID: <af465ba5.1c4e370c.8268600@expms2.cites.uiuc.edu>

Hello,

I have two questions: 

1. How can I get black and white graphics using the pdf()
device? I have tried pdf(Name.pdf, bg="white", fg"black"), but
I still get  full-color graphics.

2. When using lattice, how do I change the title for each
panel?  By default it uses the categories of the grouping
factor. I would like to modify the labels in the graphic
without changing the data. Also, how do I add a title above
the graphic referring to this factor (not the main title of
the graph)? The title for the xlab refers to the independent
factor within each panel. I would like to explain what each
panel represents with something similar to xlab, this title
being above the graphic if possible.

Thanking you in adavance,
Francisco 
Francisco Torreira
Spanish, Italian and Portuguese
Univ. of Illinois at Urbana-Champaign
707 South Mathews Aven.
4031 FLB
Urbana, IL, 61801



From ripley at stats.ox.ac.uk  Fri Jul 15 17:22:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Jul 2005 16:22:17 +0100 (BST)
Subject: [R] glm(family=binomial(link=logit))
In-Reply-To: <C65F84A4-7326-4AEE-A1FF-BEF577ECF46D@soc.soton.ac.uk>
References: <C65F84A4-7326-4AEE-A1FF-BEF577ECF46D@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0507151612290.18749@gannet.stats>

On Fri, 15 Jul 2005, Robin Hankin wrote:

> I am trying to make glm() work to analyze a toy logit system.
>
> I have a dataframe with x and y independent variables.  I have
>
> L=1+x-y          (ie coefficients 1,1,-1)
>
> then if I have a logit relation with L=log(p/(1-p)),
> p=1/(1+exp(L)).

Not quite, see below.

> If I interpret "p" as the probability of  success in a Bernouilli
> trial, and I can observe the result (0 for "no", 1 for "yes")
> how do I retrieve the coefficients c(1,1,-1)
> from the data?
>
> n <- 300
> des <- data.frame(x=(1:n)/n,y=sample(n)/n)   # experimental design
> des <- cbind(des,L=1+des$x-des$y)            # L=1+x-y
> des <- cbind(des,p=1/(1+exp(des$L)))         # p=1/(1+e^L)

A logit would be p = e^L/(1+e^L), so your signs for L are reversed.

> des <- cbind(des,obs=rbinom(n,1,des$p))      # observation: prob of
> success = p.
>
>
> My attempt is:
>
> glm(obs~x+y,data=des,family=binomial(link="logit"))
>
> But it does not retrieve the correct coefficients of c(1,1,-1) ;
> I would expect a reasonably close answer with so much data.

You actually have so little data.

> What is the correct glm() call to perform my logit analysis?

The call is correct, the expectation is not.  A single bernoulli 
observation provides far less information than you seem to suppose.

I got

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  -1.4747     0.3670  -4.019 5.85e-05 ***
x            -0.5549     0.4672  -1.188  0.23494
y             1.2963     0.4731   2.740  0.00614 **

and note how large the standard errors are.  With 10000 examples you will 
get closer.  Having fixed your sign change, I got

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.98711    0.06024   16.39   <2e-16 ***
x            1.00896    0.08052   12.53   <2e-16 ***
y           -0.87798    0.08031  -10.93   <2e-16 ***


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mkondrin at hppi.troitsk.ru  Fri Jul 15 17:20:48 2005
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 15 Jul 2005 19:20:48 +0400
Subject: [R] New RVTK package is available for download and testing.
In-Reply-To: <42D7CB24.1020900@univ-paris5.fr>
References: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
	<42D7CB24.1020900@univ-paris5.fr>
Message-ID: <42D7D450.7050006@hppi.troitsk.ru>

Hello!
I have just completed R language binding for VTK library from Kitware. 
The library is used for 3D modelling and data visualization. RVTK  
allows you to do that sort of things in R-session. Because RVTK is in 
quite an alpha stage it is not yet on CRAN and can be downloaded from my 
homepage: http://www.hppi.troitsk.ru/Kondrin/rvtk.htm . The link to the 
tarball is on the bottom of page after a long introduction (if you are 
intended to use RVTK better do read it).



From mkondrin at hppi.troitsk.ru  Fri Jul 15 17:32:03 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Fri, 15 Jul 2005 19:32:03 +0400
Subject: [R] Passing character strings from C code to R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40DA@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40DA@us-arlington-0668.mail.saic.com>
Message-ID: <42D7D6F3.8090904@hppi.troitsk.ru>

Tuszynski, Jaroslaw W. wrote:

>Hi,
>
>I have a C code which produces array of integers and potentially a string,
>and I have problems passing the string out. Here is the relevant part of the
>code:
>
> 1   PROTECT(Ret = allocVector(INTSXP, n));
> 2   ret     = (int*) INTEGER(Ret);  /* get pointer to R's Ret */
> 3   for(i=0; i<n; i++) ret[i] = data[i];
> 4   Free(data);
> 5   i=1;
> 6   if (comment) { // comment was found
> 7     n = strlen(comment);
> 8     if(n>0) {    // and it actually have some length
> 9        Rprintf(" '%s' %i\n", comment, n); 
> 10       PROTECT(Str = allocString(n+1)); 
> 11       str = CHAR(STRING_ELT(Str, 0));
> 12       strcpy(str, comment);
> 13       Rprintf(" '%s' %i\n", str, n);
> 14       setAttrib(Ret, install("comm"), Str);
> 15       i=2;
> 16     }
> 17     Free(comment);
> 18   }
> 20   UNPROTECT(i);  
>
>Print statement in line 9 gives right results, but program crashes before
>print statement in line 13.
>Any ideas to what am I doing wrong? 
>
>Jarek
>=====================================\====                 
> Jarek Tuszynski, PhD.                               o / \ 
> Science Applications International Corporation  <\__,|  
> (703) 676-4192                        ">  \
> Jaroslaw.W.Tuszynski at saic.com                   `    \
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
SEXP charToSexp(char * val) {
  USER_OBJECT_ ans;
  PROTECT(ans = NEW_CHARACTER(1));
  if(val)
      SET_STRING_ELT(ans, 0, COPY_TO_USER_STRING(val));
  UNPROTECT(1);

  return(ans);
}



From mkondrin at hppi.troitsk.ru  Fri Jul 15 17:40:29 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Fri, 15 Jul 2005 19:40:29 +0400
Subject: [R] Passing character strings from C code to R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40DA@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40DA@us-arlington-0668.mail.saic.com>
Message-ID: <42D7D8ED.9090205@hppi.troitsk.ru>

If you want to install comment attribute on return value set it this way
setAttrib(ans, R_CommentSymbol, charToSexp("Some comment"));



From p.dalgaard at biostat.ku.dk  Fri Jul 15 17:58:51 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jul 2005 17:58:51 +0200
Subject: [R] glm(family=binomial(link=logit))
In-Reply-To: <C65F84A4-7326-4AEE-A1FF-BEF577ECF46D@soc.soton.ac.uk>
References: <C65F84A4-7326-4AEE-A1FF-BEF577ECF46D@soc.soton.ac.uk>
Message-ID: <x23bqg9i2s.fsf@turmalin.kubism.ku.dk>

Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

> Hi
> 
> I am trying to make glm() work to analyze a toy logit system.
> 
> I have a dataframe with x and y independent variables.  I have
> 
> L=1+x-y          (ie coefficients 1,1,-1)
> 
> then if I have a logit relation with L=log(p/(1-p)),
> p=1/(1+exp(L)).
> 
> If I interpret "p" as the probability of  success in a Bernouilli
> trial, and I can observe the result (0 for "no", 1 for "yes")
> how do I retrieve the coefficients c(1,1,-1)
> from the data?
> 
> n <- 300
> des <- data.frame(x=(1:n)/n,y=sample(n)/n)   # experimental design
> des <- cbind(des,L=1+des$x-des$y)            # L=1+x-y
> des <- cbind(des,p=1/(1+exp(des$L)))         # p=1/(1+e^L)
> des <- cbind(des,obs=rbinom(n,1,des$p))      # observation: prob of  
> success = p.
> 
> 
> My attempt is:
> 
> glm(obs~x+y,data=des,family=binomial(link="logit"))
> 
> But it does not retrieve the correct coefficients of c(1,1,-1) ;
> I would expect a reasonably close answer with so much data.
> 
> What is the correct glm() call to perform my logit analysis?

Apart from a sign error, the only fault seems to be that you think
that 300 is a large number... Try upping n to 30000 and you'll see.

(The sign error is of course that log(p/(1-p)) = L implies that
p = exp(L)/(1+exp(L)) = 1/(1+exp(-L))).  

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From pbalvane at oikos.unam.mx  Fri Jul 15 18:12:26 2005
From: pbalvane at oikos.unam.mx (Patricia Balvanera)
Date: Fri, 15 Jul 2005 11:12:26 -0500
Subject: [R] nlme and spatially correlated errors
Message-ID: <5.1.0.14.2.20050715110234.0244fad0@ate.oikos.unam.mx>

Dear R users,

I am using lme and nlme to account for spatially correlated errors as 
random effects. My basic question is about being able to correct F, p, R2 
and parameters of models that do not take into account the nature of such 
errors using gls, glm or nlm and replace them for new F, p, R2 and 
parameters using lme and nlme as random effects.

I am studying distribution patterns of 50 tree species along a gradient. 
That gradient
was sampled through 27 transects, with 10 plots within each transect. For 
each plot I
have data on presence/absence, abundance and basal area of the species. I 
also have data
for 4 environmental variables related to water availability (soil water 
retention
capacity, slope, insolation, altitude) and X and Y coordinates for each 
plot. I explored
wether the relationship between any of the response variables 
(presence/absence,
abundance, basal area) and the environmental variables was linear, 
polinomial, or
non-linear.

My main interest in this question is that I proceeded to correct for spatial
autocorrelation (both within transects and overall) following the 
procedures suggest by
Crawley 2002 for linear models
e.g. (GUAMAC = a species, CRAS = soil water retention capacity, TRANSECTO = 
transect)
 > model1<-gls(GUAMAC ~ CRAS)
 > model2<-lme(GUAMAC ~ CRAS, random = ~ 1 | TRANSECTO)
 > model3<-lme(GUAMAC ~ CRAS, random = GUAMAC ~ CRAS | TRANSECTO)
 > model4<-lme(GUAMAC ~ CRAS, random = GUAMAC ~ CRAS -1 | TRANSECTO)
 > AIC(model1,model2,model3,model4)
df AIC
model1 3 3730.537
model2 4 3698.849
model3 6 3702.408
model4 4 3704.722
 > plot(Variogram(model2, form = ~ X + Y))
 > model5<-update(model2,corr=corSpher(c(30,0.8), form = ~ X + Y, nugget = T))
 > plot(Variogram(modelo7, resType = "n"))
 > summary(model5)

In this case I obtain new F for the independent variable INSOLACION, new R2 
for the whole model and new parameters for the linear model.

I have also applied this procedure to polinomial models and to glms with 
binomial errors
(presence/absence) with no problem.

I am nevertheless stuck with non-linear models. I am using the protocols 
you suggested
in the 1998 manuals by Pinheiro and Bates, and those suggested by Crawley 
2002.
Please find enclose an example with an
exponential model (which I chose for being simple). In fact the linear 
models I am using
are a bit more complicated.
(HELLOT is a species, INSOLACION = INSOLATION, basal = basal area of the 
species, TRANSECTO = transect)

 > HELLOT ~ exp(A + (B * INSOLACION))
 > basal.HELLOT <-function(A,B,INSOLACION) exp(A + (B * INSOLACION))
 > HELLOT ~ basal.HELLOT(A,B,INSOLACION)
 > basal.HELLOT<- deriv(~ exp(A + (B * INSOLACION))
+ , LETTERS [1:2], function(A, B, INSOLACION){})
 > model1<- nlme(model = HELLOT ~ exp(A + (B * INSOLACION)), fixed = A + B 
~ 1,
random = A + B ~ 1, groups = ~ TRANSECTO, start = list(fixed = c(5.23, -0.05)))

It runs perfectly and gives new values for parameters A and B, but would 
only give me F for fixed effects of A and B, while what I am really looking 
for is F for fixed effects of INSOLACION and the R2 of the new model.

Thank you so much in advance for your help



Dra. Patricia Balvanera
Centro de Investigaciones en Ecosistemas, UNAM-Campus Morelia
Apdo. Postal 27-3, Xangari
58090 Morelia, Michoac??n, Mexico
Tel. (52-443)3-22-27-07, (52-55) 56-23-27-07
FAX (52-443) 3-22-27-19, (52-55) 56-23-27-19



From mkondrin at hppi.troitsk.ru  Fri Jul 15 18:13:15 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Fri, 15 Jul 2005 20:13:15 +0400
Subject: [R] New RVTK package is available for download and testing.
In-Reply-To: <42D7CB24.1020900@univ-paris5.fr>
References: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
	<42D7CB24.1020900@univ-paris5.fr>
Message-ID: <42D7E09B.6@hppi.troitsk.ru>

Hello!
I have just completed R-language binding for VTK library from Kitware.
The library is used for 3D modelling and data visualization. RVTK
allows you to do that sort of things in R-session. Because RVTK is in
quite an alpha stage it is not yet on CRAN and can be downloaded from my
homepage: www.hppi.troitsk.ru/Kondrin/rvtk.htm . The link to the
tarball is on the bottom of page after a long introduction (if you are
intended to use RVTK better do read it).



From r.hankin at noc.soton.ac.uk  Fri Jul 15 11:01:24 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 15 Jul 2005 10:01:24 +0100
Subject: [R] [R-pkgs] new package: elliptic
Message-ID: <D1141C2F-8EE5-463D-B729-E946F0361C64@soc.soton.ac.uk>

Hi

I recently uploaded package "elliptic" to CRAN.  From the DESCRIPTION  
file, it is:


A suite of elliptic and related functions including
                Weierstrass and Jacobi forms.  Also includes various
                tools for manipulating and visualizing complex
                functions.



enjoy

rksh

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Fri Jul 15 18:51:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Jul 2005 17:51:52 +0100 (BST)
Subject: [R] Passing character strings from C code to R
In-Reply-To: <42D7D6F3.8090904@hppi.troitsk.ru>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40DA@us-arlington-0668.mail.saic.com>
	<42D7D6F3.8090904@hppi.troitsk.ru>
Message-ID: <Pine.LNX.4.61.0507151749310.24563@gannet.stats>

Why invent your own function to do the same thing as the builtin 
one mkString?

Please discuss C programming questions on the R-devel list *as the posting 
guide asks*

On Fri, 15 Jul 2005, mkondrin wrote:

> Tuszynski, Jaroslaw W. wrote:
>
>> Hi,
>>
>> I have a C code which produces array of integers and potentially a string,
>> and I have problems passing the string out. Here is the relevant part of the
>> code:
>>
>> 1   PROTECT(Ret = allocVector(INTSXP, n));
>> 2   ret     = (int*) INTEGER(Ret);  /* get pointer to R's Ret */
>> 3   for(i=0; i<n; i++) ret[i] = data[i];
>> 4   Free(data);
>> 5   i=1;
>> 6   if (comment) { // comment was found
>> 7     n = strlen(comment);
>> 8     if(n>0) {    // and it actually have some length
>> 9        Rprintf(" '%s' %i\n", comment, n);
>> 10       PROTECT(Str = allocString(n+1));
>> 11       str = CHAR(STRING_ELT(Str, 0));
>> 12       strcpy(str, comment);
>> 13       Rprintf(" '%s' %i\n", str, n);
>> 14       setAttrib(Ret, install("comm"), Str);
>> 15       i=2;
>> 16     }
>> 17     Free(comment);
>> 18   }
>> 20   UNPROTECT(i);
>>
>> Print statement in line 9 gives right results, but program crashes before
>> print statement in line 13.
>> Any ideas to what am I doing wrong?
>>
>> Jarek
>> =====================================\====
>> Jarek Tuszynski, PhD.                               o / \
>> Science Applications International Corporation  <\__,|
>> (703) 676-4192                        ">  \
>> Jaroslaw.W.Tuszynski at saic.com                   `    \
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> SEXP charToSexp(char * val) {
>  USER_OBJECT_ ans;
>  PROTECT(ans = NEW_CHARACTER(1));
>  if(val)
>      SET_STRING_ELT(ans, 0, COPY_TO_USER_STRING(val));
>  UNPROTECT(1);
>
>  return(ans);
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan.sarkar at gmail.com  Fri Jul 15 19:16:36 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 15 Jul 2005 12:16:36 -0500
Subject: [R] pdf() black&white; panel titles in lattice
In-Reply-To: <af465ba5.1c4e370c.8268600@expms2.cites.uiuc.edu>
References: <af465ba5.1c4e370c.8268600@expms2.cites.uiuc.edu>
Message-ID: <eb555e660507151016c102826@mail.gmail.com>

On 7/15/05, ftorrei2 at uiuc.edu <ftorrei2 at uiuc.edu> wrote:
> Hello,
> 
> I have two questions:
> 
> 1. How can I get black and white graphics using the pdf()
> device? I have tried pdf(Name.pdf, bg="white", fg"black"), but
> I still get  full-color graphics.

See ?trellis.device, specifically the 'color' argument.

> 2. When using lattice, how do I change the title for each
> panel?  By default it uses the categories of the grouping
> factor. I would like to modify the labels in the graphic
> without changing the data. 

See the entry for 'strip' in ?xyplot, as well as ?strip.default. For
one conditioning variable this can be as simple as

xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
    strip = strip.custom(factor.levels = c("I. setosa", 
                                  "I. versicolor", "I. verginica")))

or 

xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
    strip = strip.custom(factor.levels = 
        expression(italic("I. setosa"), 
                         italic("I. versicolor"), 
                         italic("I. verginica"))))


> Also, how do I add a title above
> the graphic referring to this factor (not the main title of
> the graph)? The title for the xlab refers to the independent
> factor within each panel. I would like to explain what each
> panel represents with something similar to xlab, this title
> being above the graphic if possible.

You may be satisfied with 

xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
          strip = strip.custom(strip.names = TRUE))

If not, use the 'key' argument (described in ?xyplot).

Deepayan



From michael.hopkins at hopkins-research.com  Fri Jul 15 19:42:56 2005
From: michael.hopkins at hopkins-research.com (Michael Hopkins)
Date: Fri, 15 Jul 2005 18:42:56 +0100
Subject: [R] 2D contour predictions
In-Reply-To: <mailman.11.1121421601.22334.r-help@stat.math.ethz.ch>
Message-ID: <BEFDB430.3E4D5%michael.hopkins@hopkins-research.com>



Hi All

I have been fitting regression models and would now like to produce some
contour & image plots from the predictors.

Is there an easy way to do this?  My current (newbie) experience with R
would suggest there is but that it's not always easy to find it!

f3 <- lm( fc ~ poly( speed, 2 ) + poly( torque, 2 ) + poly( sonl, 2 ) +
poly( p_rail, 2 ) + poly( pil_sep, 2 ) + poly( maf, 2 ) + (speed + torque +
sonl + p_rail + pil_sep + maf)^2 )

hat <- predict( f3 )

contour( sonl, maf, hat )

Error in contour.default(sonl, maf, hat) :
    increasing 'x' and 'y' values expected

image(sonl, maf, hat)

Error in image.default(sonl, maf, hat) : increasing 'x' and 'y' values
expected

I have tried na??ve sorting but no luck with that.

I suspect I may need to produce a data grid of some kind but I'm not clear
how I would use R to specify such a 2D slice in the 6D design space.

TIA

Michael

P.S.  Whilst I'm asking the list - is there an easier way of expressing a
full 2nd order model than the one I used above?  I'm sure there is but
finding it etc etc...


_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/

        _/    _/   _/_/_/             Hopkins Research Ltd
       _/    _/   _/    _/
      _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
     _/    _/   _/   _/
    _/    _/   _/     _/               'touch the future'
                   
_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/



From eric.y.hu at gmail.com  Fri Jul 15 19:48:45 2005
From: eric.y.hu at gmail.com (Eric Hu)
Date: Fri, 15 Jul 2005 10:48:45 -0700
Subject: [R] 3d scatter plot
Message-ID: <b7c1b0f64ebfd0f3c54f9ce8556b95bb@gmail.com>

Hi, I ran into a dilemma trying to plot the following data in a 3d 
scatter fashion. My data are not always increasing as persp() expects. 
For now I use scatterplot3d to get points in a 3d scatter plot. I 
wonder if I have any way to plot the surfaces. Thanks!

Eric


####sample data

-0.50 0.40 1.281
-0.50 0.45 1.795
-0.50 0.50 1.766
-0.40 0.35 1.595
-0.40 0.40 1.388
-0.40 0.45 2.344
-0.40 0.50 2.179
-0.30 0.35 1.792
-0.30 0.40 2.349
-0.30 0.45 1.682
-0.30 0.50 1.493
-0.20 0.35 1.836
-0.20 0.40 2.186
-0.20 0.45 1.863
-0.20 0.50 1.775
-0.10 0.25 2.991
-0.10 0.30 2.426
-0.10 0.35 1.954
-0.10 0.40 1.136
-0.10 0.45 1.438
-0.10 0.50 1.429
-0.05 0.30 2.163
-0.05 0.35 1.953
-0.05 0.40 1.672
-0.05 0.45 1.688
-0.05 0.50 1.963



From ftorrei2 at uiuc.edu  Fri Jul 15 19:49:13 2005
From: ftorrei2 at uiuc.edu (ftorrei2@uiuc.edu)
Date: Fri, 15 Jul 2005 12:49:13 -0500
Subject: [R] pdf() black&white; panel titles in lattice
Message-ID: <cd7c8f1c.1c5c187c.8268700@expms2.cites.uiuc.edu>

Thank you so much for your help. Now I think I can use R for
everything I need so far. However I think that I haven't found
a solution for my first question. trellis.device(color=F) only
affects the trellis device, but not the pdf device, doesn??t
it? At least that is my impression. I manage to get black and
white graphics in trellis, but whenever I switch on the pdf
device, the resulting pdf file is in color, even if I specify
 pdf(X.pdf, bg="white", fg="black") (I wonder what the use of
bg and fg is). The thing is that I need black and white
graphics for my papers. I am using R on OSX. Does this happen
to anyone else?

Thanks again 

---- Original message ----
>Date: Fri, 15 Jul 2005 12:16:36 -0500
>From: Deepayan Sarkar <deepayan.sarkar at gmail.com>  
>Subject: Re: [R] pdf() black&white; panel titles in lattice  
>To: "ftorrei2 at uiuc.edu" <ftorrei2 at uiuc.edu>
>Cc: r-help at stat.math.ethz.ch
>
>On 7/15/05, ftorrei2 at uiuc.edu <ftorrei2 at uiuc.edu> wrote:
>> Hello,
>> 
>> I have two questions:
>> 
>> 1. How can I get black and white graphics using the pdf()
>> device? I have tried pdf(Name.pdf, bg="white", fg"black"), but
>> I still get  full-color graphics.
>
>See ?trellis.device, specifically the 'color' argument.
>
>> 2. When using lattice, how do I change the title for each
>> panel?  By default it uses the categories of the grouping
>> factor. I would like to modify the labels in the graphic
>> without changing the data. 
>
>See the entry for 'strip' in ?xyplot, as well as
?strip.default. For
>one conditioning variable this can be as simple as
>
>xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
>    strip = strip.custom(factor.levels = c("I. setosa", 
>                                  "I. versicolor", "I.
verginica")))
>
>or 
>
>xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
>    strip = strip.custom(factor.levels = 
>        expression(italic("I. setosa"), 
>                         italic("I. versicolor"), 
>                         italic("I. verginica"))))
>
>
>> Also, how do I add a title above
>> the graphic referring to this factor (not the main title of
>> the graph)? The title for the xlab refers to the independent
>> factor within each panel. I would like to explain what each
>> panel represents with something similar to xlab, this title
>> being above the graphic if possible.
>
>You may be satisfied with 
>
>xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
>          strip = strip.custom(strip.names = TRUE))
>
>If not, use the 'key' argument (described in ?xyplot).
>
>Deepayan
Francisco Torreira
Spanish, Italian and Portuguese
Univ. of Illinois at Urbana-Champaign
707 South Mathews Aven.
4031 FLB
Urbana, IL, 61801



From sundar.dorai-raj at pdf.com  Fri Jul 15 19:56:15 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 15 Jul 2005 12:56:15 -0500
Subject: [R] 3d scatter plot
In-Reply-To: <b7c1b0f64ebfd0f3c54f9ce8556b95bb@gmail.com>
References: <b7c1b0f64ebfd0f3c54f9ce8556b95bb@gmail.com>
Message-ID: <42D7F8BF.6050803@pdf.com>



Eric Hu wrote:
> Hi, I ran into a dilemma trying to plot the following data in a 3d 
> scatter fashion. My data are not always increasing as persp() expects. 
> For now I use scatterplot3d to get points in a 3d scatter plot. I 
> wonder if I have any way to plot the surfaces. Thanks!
> 
> Eric
> 
> 
> ####sample data
> 
> -0.50 0.40 1.281
> -0.50 0.45 1.795
> -0.50 0.50 1.766
> -0.40 0.35 1.595
> -0.40 0.40 1.388
> -0.40 0.45 2.344
> -0.40 0.50 2.179
> -0.30 0.35 1.792
> -0.30 0.40 2.349
> -0.30 0.45 1.682
> -0.30 0.50 1.493
> -0.20 0.35 1.836
> -0.20 0.40 2.186
> -0.20 0.45 1.863
> -0.20 0.50 1.775
> -0.10 0.25 2.991
> -0.10 0.30 2.426
> -0.10 0.35 1.954
> -0.10 0.40 1.136
> -0.10 0.45 1.438
> -0.10 0.50 1.429
> -0.05 0.30 2.163
> -0.05 0.35 1.953
> -0.05 0.40 1.672
> -0.05 0.45 1.688
> -0.05 0.50 1.963
> 


?wireframe in the lattice package may help.

--sundar



From kevin.thorpe at utoronto.ca  Fri Jul 15 20:06:21 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 15 Jul 2005 14:06:21 -0400
Subject: [R] Cannot update some packages after upgrade to 2.1.1
Message-ID: <42D7FB1D.2040302@utoronto.ca>

I just upgraded to version 2.1.1 (from 2.0.1) today.

 > R.version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.1
year     2005
month    06
day      20
language R

I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
available on CRAN.  After upgrading r-base, I ran update.packages().
Four previously installed packages failed to update:

	Matrix (0.95-5 to 0.97-4)
	gam (0.93 to 0.94)
	lme4 (0.95-3 to 0.96-1)
	mgcv (1.3-1 to 1.3-4)

In the case of Matrix, gam and mgcv I get the message:

	[long path]/ld: cannot find -lf77blas

In the case of lme4 the messages are:

** preparing package for lazy loading
Error in setMethod("coef", signature(object = "lmList"), function(object,  :
         no existing definition for function 'coef'
Error: unable to load R code in package 'lme4'
Execution halted
ERROR: lazy loading failed for package 'lme4'

I have searched the SuSE repository for any package that provides
f77blas but came up empty.

I also could not identify any relevant messages in the mailing list
archives.

Is R looking for that library because it was present on the machine
the RPM was built on?  Would building R myself solve the missing library
problem or did I do something wrong?

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From ccleland at optonline.net  Fri Jul 15 20:08:12 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 15 Jul 2005 14:08:12 -0400
Subject: [R] pdf() black&white; panel titles in lattice
In-Reply-To: <cd7c8f1c.1c5c187c.8268700@expms2.cites.uiuc.edu>
References: <cd7c8f1c.1c5c187c.8268700@expms2.cites.uiuc.edu>
Message-ID: <42D7FB8C.3080501@optonline.net>

ftorrei2 at uiuc.edu wrote:
> Thank you so much for your help. Now I think I can use R for
> everything I need so far. However I think that I haven't found
> a solution for my first question. trellis.device(color=F) only
> affects the trellis device, but not the pdf device, doesn??t
> it? At least that is my impression. I manage to get black and
> white graphics in trellis, but whenever I switch on the pdf
> device, the resulting pdf file is in color, even if I specify
>  pdf(X.pdf, bg="white", fg="black") (I wonder what the use of
> bg and fg is). The thing is that I need black and white
> graphics for my papers. I am using R on OSX. Does this happen
> to anyone else?

Call trellis.device() after pdf() and use new=FALSE in trellis.device().

pdf("c:/myfolder/myfigures.pdf")
trellis.device(new=FALSE, col=FALSE)

> Thanks again 
> 
> ---- Original message ----
> 
>>Date: Fri, 15 Jul 2005 12:16:36 -0500
>>From: Deepayan Sarkar <deepayan.sarkar at gmail.com>  
>>Subject: Re: [R] pdf() black&white; panel titles in lattice  
>>To: "ftorrei2 at uiuc.edu" <ftorrei2 at uiuc.edu>
>>Cc: r-help at stat.math.ethz.ch
>>
>>On 7/15/05, ftorrei2 at uiuc.edu <ftorrei2 at uiuc.edu> wrote:
>>
>>>Hello,
>>>
>>>I have two questions:
>>>
>>>1. How can I get black and white graphics using the pdf()
>>>device? I have tried pdf(Name.pdf, bg="white", fg"black"), but
>>>I still get  full-color graphics.
>>
>>See ?trellis.device, specifically the 'color' argument.
>>
>>
>>>2. When using lattice, how do I change the title for each
>>>panel?  By default it uses the categories of the grouping
>>>factor. I would like to modify the labels in the graphic
>>>without changing the data. 
>>
>>See the entry for 'strip' in ?xyplot, as well as
> 
> ?strip.default. For
> 
>>one conditioning variable this can be as simple as
>>
>>xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
>>   strip = strip.custom(factor.levels = c("I. setosa", 
>>                                 "I. versicolor", "I.
> 
> verginica")))
> 
>>or 
>>
>>xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
>>   strip = strip.custom(factor.levels = 
>>       expression(italic("I. setosa"), 
>>                        italic("I. versicolor"), 
>>                        italic("I. verginica"))))
>>
>>
>>
>>>Also, how do I add a title above
>>>the graphic referring to this factor (not the main title of
>>>the graph)? The title for the xlab refers to the independent
>>>factor within each panel. I would like to explain what each
>>>panel represents with something similar to xlab, this title
>>>being above the graphic if possible.
>>
>>You may be satisfied with 
>>
>>xyplot(Sepal.Length ~ Sepal.Width | Species, iris, 
>>         strip = strip.custom(strip.names = TRUE))
>>
>>If not, use the 'key' argument (described in ?xyplot).
>>
>>Deepayan
> 
> Francisco Torreira
> Spanish, Italian and Portuguese
> Univ. of Illinois at Urbana-Champaign
> 707 South Mathews Aven.
> 4031 FLB
> Urbana, IL, 61801
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From deepayan.sarkar at gmail.com  Fri Jul 15 22:01:39 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 15 Jul 2005 15:01:39 -0500
Subject: [R] pdf() black&white; panel titles in lattice
In-Reply-To: <42D7FB8C.3080501@optonline.net>
References: <cd7c8f1c.1c5c187c.8268700@expms2.cites.uiuc.edu>
	<42D7FB8C.3080501@optonline.net>
Message-ID: <eb555e6605071513013950fb4c@mail.gmail.com>

On 7/15/05, Chuck Cleland <ccleland at optonline.net> wrote:

> ftorrei2 at uiuc.edu wrote:
> > Thank you so much for your help. Now I think I can use R for
> > everything I need so far. However I think that I haven't found
> > a solution for my first question. trellis.device(color=F) only
> > affects the trellis device, but not the pdf device, doesn??t
> > it? At least that is my impression. I manage to get black and
> > white graphics in trellis, but whenever I switch on the pdf
> > device, the resulting pdf file is in color, even if I specify
> >  pdf(X.pdf, bg="white", fg="black") (I wonder what the use of
> > bg and fg is). The thing is that I need black and white
> > graphics for my papers. I am using R on OSX. Does this happen
> > to anyone else?
> 
> Call trellis.device() after pdf() and use new=FALSE in trellis.device().
> 
> pdf("c:/myfolder/myfigures.pdf")
> trellis.device(new=FALSE, col=FALSE)

That will work too, but the intended usage of trellis.device as a
wrapper for any device, including pdf. E.g.,

trellis.device(pdf, file = "foo.pdf", color = FALSE) 

Deepayan



From david.groos at mpls.k12.mn.us  Fri Jul 15 22:40:01 2005
From: david.groos at mpls.k12.mn.us (David Groos)
Date: Fri, 15 Jul 2005 15:40:01 -0500
Subject: [R] Can't get sample function from "An Introduction to R" to work
Message-ID: <7f534ec248a5113eaf0dadce1ac53866@mpls.k12.mn.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050715/16d70d9b/attachment.pl

From bret at tamu.edu  Fri Jul 15 22:54:06 2005
From: bret at tamu.edu (Bret Collier)
Date: Fri, 15 Jul 2005 15:54:06 -0500
Subject: [R] Can't get sample function from "An Introduction to R"	to
	work
Message-ID: <s2d7dc3a.079@wfscgate.tamu.edu>

David,
If below is exactly what you typed, check your code again, I think you
are missing a '}' after the last 2 parentheses.

HTH,
Bret

>>> David Groos <david.groos at mpls.k12.mn.us> 7/15/2005 3:40:01 PM >>>
I'm trying to figure out R, a piece at a time, hours at a time...  I 
was trying to copy the sample function in, "An Introduction to R"  (for

version 2.1.0) by W. N. Venables, D. M. Smith, page 42.  Section 10.1 
"Simple examples" provides a sample function which I tried to duplicate

(I'm using Mac OS X 10.3.9, and "R for Mac OS X Aqua GUI v1.11).  The 
following is what I typed and the last line is R's response when I hit

the return key after the penultimate line.  I've re-checked and 
re-typed the code many times to no avail.  I wasn't able to find this 
issue using search options, either.  Any help is GREATLY appreciated!

 > twosam<-function(y1, y2) {
+ n1<-length(y1);n2 <-length(y2)
+ yb1<-mean(y1); yb2<-mean(y2)
+ s1<-var(y1);s2<-var(y2)
+ s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
+ tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
Error: syntax error

David
	[[alternative text/enriched version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Fri Jul 15 23:06:42 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 15 Jul 2005 16:06:42 -0500
Subject: [R] Can't get sample function from "An Introduction to R"
	to	work
In-Reply-To: <7f534ec248a5113eaf0dadce1ac53866@mpls.k12.mn.us>
References: <7f534ec248a5113eaf0dadce1ac53866@mpls.k12.mn.us>
Message-ID: <1121461603.4190.15.camel@localhost.localdomain>

On Fri, 2005-07-15 at 15:40 -0500, David Groos wrote:
> I'm trying to figure out R, a piece at a time, hours at a time...  I 
> was trying to copy the sample function in, "An Introduction to R"  (for 
> version 2.1.0) by W. N. Venables, D. M. Smith, page 42.  Section 10.1 
> "Simple examples" provides a sample function which I tried to duplicate 
> (I'm using Mac OS X 10.3.9, and "R for Mac OS X Aqua GUI v1.11).  The 
> following is what I typed and the last line is R's response when I hit 
> the return key after the penultimate line.  I've re-checked and 
> re-typed the code many times to no avail.  I wasn't able to find this 
> issue using search options, either.  Any help is GREATLY appreciated!
> 
>  > twosam<-function(y1, y2) {
> + n1<-length(y1);n2 <-length(y2)
> + yb1<-mean(y1); yb2<-mean(y2)
> + s1<-var(y1);s2<-var(y2)
> + s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
> + tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
> Error: syntax error
> 
> David

The code as you have above (without the "+" on each line) works for me
both in ESS and in the R console under Linux. There should be another
"+" on the next line, in anticipation of the remaining two lines:

 tst
}


Try to copy and paste the following into the console as is:

twosam<-function(y1, y2) {
 n1 <- length(y1);n2 <- length(y2)
 yb1 <- mean(y1); yb2 <- mean(y2)
 s1 <- var(y1);s2 <- var(y2)
 s <- ((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
 tst <- (yb1-yb2)/sqrt(s*(1/n1+1/n2))

and see what happens. You should be left at a:

+

on a new line again.

It is possible that there is a bug in the Aqua GUI, but not using a Mac,
I cannot replicate it.

You might want to consider subscribing and posting to the R-SIG-Mac
e-mail list, which is focused on Mac users of R. More information is
here:

https://stat.ethz.ch/mailman/listinfo/r-sig-mac

HTH,

Marc Schwartz

P.S. Greetings from Eden Prairie



From macq at llnl.gov  Fri Jul 15 23:11:26 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 15 Jul 2005 14:11:26 -0700
Subject: [R] Newbie guide for plot & graphics functions?
In-Reply-To: <p06210202befd7ddc96ae@[128.115.153.6]>
References: <20050715142320.1718.qmail@web52401.mail.yahoo.com>
	<p06210202befd7ddc96ae@[128.115.153.6]>
Message-ID: <p06210209befdd6835de6@[128.115.153.6]>

Significant goof in my first message. Here is the corrected version:

x <- 1:10
y <- 1:15
z <- outer(x,y)
image(x,y,z)

xb <- c(.5,x+.5)
yb <- c(.5,y+.5)
image(xb,yb,z)              ## xb, yb    NOT    x, y

Fortunately, corrected one also succeeds.

-Don

At 8:06 AM -0700 7/15/05, Don MacQueen wrote:
>I think you have it backwards (the error message not withstanding).
>Read the documentation for image, note the beginning of the Details
>section, and then compare these two examples, both of which succeed
>on my system (R 2.1.1).
>
>x <- 1:10
>y <- 1:15
>z <- outer(x,y)
>image(x,y,z)
>
>xb <- c(.5,x+.5)
>yb <- c(.5,y+.5)
>image(x,y,z)
>
>x and y provide either the centers of the cells, or the boundaries of
>the cells. In the latter case, the lengths of x and y should be 1
>greater than the dim of z, not one less.
>
>(I'm including my full version info to be complete, but I doubt that
>you problem has to do with your platform [OS]).
>
>-Don
>
>>   version
>           _                      
>platform powerpc-apple-darwin7.9.0
>arch     powerpc                
>os       darwin7.9.0            
>system   powerpc, darwin7.9.0   
>status                          
>major    2                      
>minor    1.1                    
>year     2005                   
>month    06                     
>day      20                     
>language R                      
>
>
>At 7:23 AM -0700 7/15/05, Pete Cap wrote:
>>Hello all,
>>
>>Can anyone point me to a decent introduction to using the plotting
>>and assorted graphics functions in R?
>>
>>I keep getting simple errrors and I can't figure out why, for example:
>>
>>>   image(x,y,z)
>>Error in image.default(x, y, z) : dimensions of z are not
>>length(x)(+1) times length(y)(+1)
>>>   length(x)
>>[1] 206
>>>   length(y)
>>[1] 40
>>>   dim(z)
>>[1] 207  41
>>
>>
>>It seems to me as if R is wrong--the matrix z is obviously of
>>length(x)+1 times length(y)+1--but just as obviously I'm missing
>>something essential.  Are there any "newbie" guides out there for
>>getting started?
>>TIA,
>>
>>Pete
>>
>>
>>
>>---------------------------------
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>--
>--------------------------------------
>Don MacQueen
>Environmental Protection Department
>Lawrence Livermore National Laboratory
>Livermore, CA, USA
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From spencer.graves at pdf.com  Fri Jul 15 23:12:07 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 15 Jul 2005 14:12:07 -0700
Subject: [R] Can't get sample function from "An Introduction to R"	to
 work
In-Reply-To: <s2d7dc3a.079@wfscgate.tamu.edu>
References: <s2d7dc3a.079@wfscgate.tamu.edu>
Message-ID: <42D826A7.1010308@pdf.com>

	  Also, are you doing this with a version of R that will allow you to 
keep your scripts in separate files and run them one line at a time? 
Rgui allows this, and many people use different editors, e.g, XEmacs 
with ESS ("EMacs Speaks Statistics").  Some editors (like ESS) help with 
parentheses matching, which makes it easier to catch errors like this. 
When that fails, I define the necessary variables and work through the 
function line by line until I find the problem.

	  spencer graves

Bret Collier wrote:

> David,
> If below is exactly what you typed, check your code again, I think you
> are missing a '}' after the last 2 parentheses.
> 
> HTH,
> Bret
> 
> 
>>>>David Groos <david.groos at mpls.k12.mn.us> 7/15/2005 3:40:01 PM >>>
> 
> I'm trying to figure out R, a piece at a time, hours at a time...  I 
> was trying to copy the sample function in, "An Introduction to R"  (for
> 
> version 2.1.0) by W. N. Venables, D. M. Smith, page 42.  Section 10.1 
> "Simple examples" provides a sample function which I tried to duplicate
> 
> (I'm using Mac OS X 10.3.9, and "R for Mac OS X Aqua GUI v1.11).  The 
> following is what I typed and the last line is R's response when I hit
> 
> the return key after the penultimate line.  I've re-checked and 
> re-typed the code many times to no avail.  I wasn't able to find this 
> issue using search options, either.  Any help is GREATLY appreciated!
> 
>  > twosam<-function(y1, y2) {
> + n1<-length(y1);n2 <-length(y2)
> + yb1<-mean(y1); yb2<-mean(y2)
> + s1<-var(y1);s2<-var(y2)
> + s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
> + tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
> Error: syntax error
> 
> David
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tolga.uzuner at csfb.com  Fri Jul 15 23:39:33 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Fri, 15 Jul 2005 22:39:33 +0100
Subject: [R] rmpi in windows
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433E0C1@elon12p32001.csfp.co.uk>

Hi Folks,
Has anyone been able to get rmpi to work under windows ?
Thanks,
Tolga

Please follow the attached hyperlink to an important disclaimer
<http://www.csfb.com/legal_terms/disclaimer_europe.shtml>



==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From p.dalgaard at biostat.ku.dk  Fri Jul 15 23:43:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jul 2005 23:43:27 +0200
Subject: [R] Can't get sample function from "An Introduction to R"	to
	work
In-Reply-To: <s2d7dc3a.079@wfscgate.tamu.edu>
References: <s2d7dc3a.079@wfscgate.tamu.edu>
Message-ID: <x2y887924g.fsf@turmalin.kubism.ku.dk>

"Bret Collier" <bret at tamu.edu> writes:

> David,
> If below is exactly what you typed, check your code again, I think you
> are missing a '}' after the last 2 parentheses.

That's not supposed to cause a syntax error, just another '+'.
I can copy and paste the code as written and not get an error:

> twosam<-function(y1, y2) {
+ n1<-length(y1);n2 <-length(y2)
+ yb1<-mean(y1); yb2<-mean(y2)
+ s1<-var(y1);s2<-var(y2)
+ s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
+ tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
+ }
>

Perhaps this was the 1st time David got his typing right? The error is
of the sort you'd get if you had 

 s<-((n1-1)*s1 + (n2-1)*s2/(n1+n2-2)

or

 tst<-(yb1-yb2)/sqrt(s*1/n1+1/n2))

 
> HTH,
> Bret
> 
> >>> David Groos <david.groos at mpls.k12.mn.us> 7/15/2005 3:40:01 PM >>>
> I'm trying to figure out R, a piece at a time, hours at a time...  I 
> was trying to copy the sample function in, "An Introduction to R"  (for
> 
> version 2.1.0) by W. N. Venables, D. M. Smith, page 42.  Section 10.1 
> "Simple examples" provides a sample function which I tried to duplicate
> 
> (I'm using Mac OS X 10.3.9, and "R for Mac OS X Aqua GUI v1.11).  The 
> following is what I typed and the last line is R's response when I hit
> 
> the return key after the penultimate line.  I've re-checked and 
> re-typed the code many times to no avail.  I wasn't able to find this 
> issue using search options, either.  Any help is GREATLY appreciated!
> 
>  > twosam<-function(y1, y2) {
> + n1<-length(y1);n2 <-length(y2)
> + yb1<-mean(y1); yb2<-mean(y2)
> + s1<-var(y1);s2<-var(y2)
> + s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
> + tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
> Error: syntax error
> 


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Alfredo.Cruz at cancercare.on.ca  Fri Jul 15 23:49:40 2005
From: Alfredo.Cruz at cancercare.on.ca (Cruz, Alfredo)
Date: Fri, 15 Jul 2005 17:49:40 -0400
Subject: [R] R on HP-UX
Message-ID: <A1BC2DA3EBA2AA40B5E009D586674F0450F54D@EXSERVER1.cco.ccods.cancercare.on.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050715/cf86d71f/attachment.pl

From eric.y.hu at gmail.com  Fri Jul 15 23:51:16 2005
From: eric.y.hu at gmail.com (Eric Hu)
Date: Fri, 15 Jul 2005 14:51:16 -0700
Subject: [R] 3d scatter plot
In-Reply-To: <42D7FF5A.80202@pdf.com>
References: <b7c1b0f64ebfd0f3c54f9ce8556b95bb@gmail.com>
	<42D7F8BF.6050803@pdf.com>
	<8fa9160fded080540e487f2403a368ce@gmail.com>
	<42D7FF5A.80202@pdf.com>
Message-ID: <4c90ac04d12e21fd70959e53d3d34198@gmail.com>

Hey Sundar

I guess it works indeed. I had made a stupid mistake. Thanks.

Eric
On Jul 15, 2005, at 11:24 AM, Sundar Dorai-Raj wrote:

> Hi, Eric,
>
> I plotted a surface using the first two columns as x and y and the 
> third column for z:
>
> library(lattice)
> v <- read.table("clipboard")
> names(v) <- c("x", "y", "z")
> wireframe(z ~ x * y, data = v)
>
> The result is attached. Am I missing something?
>
> Thanks,
>
> --sundar
>
> Eric Hu wrote:
>> Ok, I don't think x and y lay on a rectangle which explains why 
>> wireframe does not work. BTW, I am really new to R and maybe you can 
>> plot those data to prove that I am wrong. Thanks.
>> Eric
>> On Jul 15, 2005, at 10:56 AM, Sundar Dorai-Raj wrote:
>>>
>>>
>>> Eric Hu wrote:
>>>
>>>> Hi, I ran into a dilemma trying to plot the following data in a 3d 
>>>> scatter fashion. My data are not always increasing as persp() 
>>>> expects. For now I use scatterplot3d to get points in a 3d scatter 
>>>> plot. I wonder if I have any way to plot the surfaces. Thanks!
>>>> Eric
>>>> ####sample data
>>>> -0.50 0.40 1.281
>>>> -0.50 0.45 1.795
>>>> -0.50 0.50 1.766
>>>> -0.40 0.35 1.595
>>>> -0.40 0.40 1.388
>>>> -0.40 0.45 2.344
>>>> -0.40 0.50 2.179
>>>> -0.30 0.35 1.792
>>>> -0.30 0.40 2.349
>>>> -0.30 0.45 1.682
>>>> -0.30 0.50 1.493
>>>> -0.20 0.35 1.836
>>>> -0.20 0.40 2.186
>>>> -0.20 0.45 1.863
>>>> -0.20 0.50 1.775
>>>> -0.10 0.25 2.991
>>>> -0.10 0.30 2.426
>>>> -0.10 0.35 1.954
>>>> -0.10 0.40 1.136
>>>> -0.10 0.45 1.438
>>>> -0.10 0.50 1.429
>>>> -0.05 0.30 2.163
>>>> -0.05 0.35 1.953
>>>> -0.05 0.40 1.672
>>>> -0.05 0.45 1.688
>>>> -0.05 0.50 1.963
>>>
>>>
>>>
>>> ?wireframe in the lattice package may help.
>>>
>>> --sundar
>>>
>>>
>>>
> <wf.png>



From pantd at unlv.nevada.edu  Sat Jul 16 03:03:15 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Fri, 15 Jul 2005 18:03:15 -0700
Subject: [R]  question from environmental statistics
Message-ID: <1121475795.42d85cd3f1c97@webmail.scsv.nevada.edu>

thanks Fran. that was useful but Im still in a fix. its a real life data which
looks like this:
0.9
10.9
24.0
6.7
0.6
1.0
2.4
12.4
7.9
15.8
1.4
7.9
11000.0

(benzene conc. taken after WTC attacks)..its just a small chunk of data i pasted
for you to look at.
its neither normal nor lognormal. someone told me that qq plot does help in
determining the distribution. im not sure how to get it.

can someone help me in this.

thanks



Take a look at this document by Vito Ricci:
http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf

Did you try RSiteSearch("Fit distribution") or a Google search?  That will
lead you to fit.dist{gnlm} and fitdistr{MASS}

Cheers

Francisco


>From: pantd at unlv.nevada.edu
>To: r-help at stat.math.ethz.ch
>Subject: [R] question from environmental statistics
>Date: Thu, 14 Jul 2005 14:06:45 -0700
>
>
>
>Dear R users
>I want to knw if there is a way in which a raw dataset can be modelled by
>some
>distribution. besides the gof test is there any test involving gamma or
>lognormal that would fit the data.
>
>thank you
>
>-dev



From Charles.Annis at StatisticalEngineering.com  Sat Jul 16 03:08:06 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 15 Jul 2005 21:08:06 -0400
Subject: [R] question from environmental statistics
In-Reply-To: <1121475795.42d85cd3f1c97@webmail.scsv.nevada.edu>
Message-ID: <200507160108.j6G186NU004397@hypatia.math.ethz.ch>

?qqplot


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of pantd at unlv.nevada.edu
Sent: Friday, July 15, 2005 9:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] question from environmental statistics

thanks Fran. that was useful but Im still in a fix. its a real life data
which
looks like this:
0.9
10.9
24.0
6.7
0.6
1.0
2.4
12.4
7.9
15.8
1.4
7.9
11000.0

(benzene conc. taken after WTC attacks)..its just a small chunk of data i
pasted
for you to look at.
its neither normal nor lognormal. someone told me that qq plot does help in
determining the distribution. im not sure how to get it.

can someone help me in this.

thanks



Take a look at this document by Vito Ricci:
http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf

Did you try RSiteSearch("Fit distribution") or a Google search?  That will
lead you to fit.dist{gnlm} and fitdistr{MASS}

Cheers

Francisco


>From: pantd at unlv.nevada.edu
>To: r-help at stat.math.ethz.ch
>Subject: [R] question from environmental statistics
>Date: Thu, 14 Jul 2005 14:06:45 -0700
>
>
>
>Dear R users
>I want to knw if there is a way in which a raw dataset can be modelled by
>some
>distribution. besides the gof test is there any test involving gamma or
>lognormal that would fit the data.
>
>thank you
>
>-dev

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From david.groos at mpls.k12.mn.us  Sat Jul 16 05:47:01 2005
From: david.groos at mpls.k12.mn.us (David Groos)
Date: Fri, 15 Jul 2005 22:47:01 -0500
Subject: [R] Can't get sample function from "An Introduction to R"	to
	work
In-Reply-To: <16601407.1121463813237.JavaMail.root@Sniper29>
References: <s2d7dc3a.079@wfscgate.tamu.edu>
	<16601407.1121463813237.JavaMail.root@Sniper29>
Message-ID: <719c117734647fcebd5533dbdbbf1b87@mpls.k12.mn.us>

Thank you-all very much for your help, your responses and help has been 
very encouraging.  The following doesn't close the case but it tables 
it...

First I copied Ken's code into my R Console and...it worked great!  
That was baffling as it looked identical to mine.

I did not explicitly say earlier that the code I sent out I had copied 
from the console, pasted into MS Word, changed font size, then pasted 
it into the e-mail--in other words, that was a copy of one of the codes 
that didn't work.

Anyway,  I then copied the non-working, pasted it into Console, 
expecting that it would say "Error: syntax error" upon pressing the 
return key at the end of this line:
>> + tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
but it didn't!  and the code worked this time, also!
I then went about doing what I could to replicate the error from before 
and was as unsuccessful in doing that as I was in making it work, 
earlier.

>>
On Jul 15, 2005, at 4:43 PM, Peter Dalgaard wrote:

> "Bret Collier" <bret at tamu.edu> writes:
>
>> David,
>> If below is exactly what you typed, check your code again, I think you
>> are missing a '}' after the last 2 parentheses.
>
> That's not supposed to cause a syntax error, just another '+'.
(right! that's what I thought...)
> I can copy and paste the code as written and not get an error:
>
>> twosam<-function(y1, y2) {
> + n1<-length(y1);n2 <-length(y2)
> + yb1<-mean(y1); yb2<-mean(y2)
> + s1<-var(y1);s2<-var(y2)
> + s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
> + tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
> + }
>>
>
> Perhaps this was the 1st time David got his typing right? The error is
> of the sort you'd get if you had
>
>  s<-((n1-1)*s1 + (n2-1)*s2/(n1+n2-2)
Interestingly enough, I did try this line when first I was trying to 
make the code work and with just 1 ")" at the end I didn't get the 
error message, but then again I wasn't able to make the whole program 
work, either.

In conclusion, I can't explain why it didn't first work time nor why I 
couldn't replicate the error.  I think I ought to e-mail the Mac R 
folks about this.

Again, Thanks,

-David



From ripley at stats.ox.ac.uk  Sat Jul 16 09:05:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Jul 2005 08:05:34 +0100 (BST)
Subject: [R] Cannot update some packages after upgrade to 2.1.1
In-Reply-To: <42D7FB1D.2040302@utoronto.ca>
References: <42D7FB1D.2040302@utoronto.ca>
Message-ID: <Pine.LNX.4.61.0507160802370.538@gannet.stats>

-lf77blas is part of ATLAS, so I do suspect the RPM builder had ATLAS 
installed.

lme4 needs a compatible Matrix installed.

I do think installing from the sources would solve this, but probably you 
need to discuss this with the RPM maintainer as a dependency appears to 
be missing.

On Fri, 15 Jul 2005, Kevin E. Thorpe wrote:

> I just upgraded to version 2.1.1 (from 2.0.1) today.
>
> > R.version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
>
> I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
> available on CRAN.  After upgrading r-base, I ran update.packages().
> Four previously installed packages failed to update:
>
> 	Matrix (0.95-5 to 0.97-4)
> 	gam (0.93 to 0.94)
> 	lme4 (0.95-3 to 0.96-1)
> 	mgcv (1.3-1 to 1.3-4)
>
> In the case of Matrix, gam and mgcv I get the message:
>
> 	[long path]/ld: cannot find -lf77blas
>
> In the case of lme4 the messages are:
>
> ** preparing package for lazy loading
> Error in setMethod("coef", signature(object = "lmList"), function(object,  :
>         no existing definition for function 'coef'
> Error: unable to load R code in package 'lme4'
> Execution halted
> ERROR: lazy loading failed for package 'lme4'
>
> I have searched the SuSE repository for any package that provides
> f77blas but came up empty.
>
> I also could not identify any relevant messages in the mailing list
> archives.
>
> Is R looking for that library because it was present on the machine
> the RPM was built on?  Would building R myself solve the missing library
> problem or did I do something wrong?
>
> -- 
> Kevin E. Thorpe
> Biostatistician/Trialist, Knowledge Translation Program
> Assistant Professor, Department of Public Health Sciences
> Faculty of Medicine, University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kar at itga.com.au  Sat Jul 16 10:07:01 2005
From: kar at itga.com.au (Kylie-Anne Richards)
Date: Sat, 16 Jul 2005 18:07:01 +1000
Subject: [R] Coxph with factors
References: <002c01c58942$a5379fa0$0400a8c0@KAR>
	<Pine.A41.4.61b.0507150723370.363424@homer06.u.washington.edu>
Message-ID: <006701c589dd$58866e70$0400a8c0@KAR>

Thank you for your help.
____________________________________________________________
> In any case, to specify f.pom You need it to be a factor with the same set 
> of levels.  You don't say what the lowest level of pom is, but if it is, 
> say, -3.
>
> f.pom=factor(-3, levels=seq(-3,2.5, by=0.5))
____________________________________________________________

For this particular model, f.pom starts at -5.5 going to 2 in 0.5 
increments. I seem to have misunderstood your explanation, as R is still 
producing an error.

>final<-coxph(Surv(time.sec,done)~f.pom+vo+po,data=DATA)
>final.surv<-survfit((final), 
>data.frame(po=0,vo=10000,f.pom=factor(-5.5,levels=seq(-5.5,2,by=0.5)),individual=T))
>summary(final.surv)
Error in x2 %*% coef : non-conformable arguments
Execution halted

____________________________________________________________
> I would first note that the survival function at zero covariates is not a 
> very useful thing and is usually numerically unstable, and it's often more 
> useful to get the survival function at some reasonable set of covariates.
____________________________________________________________

Please correct me if I'm wrong, I was under the impression that the survival 
function at zero covariates gave the baseline distribution. I.e. if given 
the baseline prob.,S_0, at time t, one could calculate the survival prob for 
specified covariates by 
S_0^exp(beta(vo)*specified(vo)+beta(po)*specified(po)+beta(f.pom at the 
level of interest)) for time t.

Since I was unable to get survfit to work with specified covariates, I was 
using the survival probs of the 'avg' covariates, S(t), to determine the 
baseline at time t, i.e. 
S(t)^(1/exp(beta(vo)*mean(vo)+beta(po)*mean(po)+beta(f.pom-5.5)*mean(f.pom-5.5)+beta(f.pom-5.0)*mean(f.pom-5.0)+........). 
And then proceeding as mention in the above paragraph (clearly not an 
efficient way of doing things).


----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Kylie-Anne Richards" <kar at itga.com.au>
Cc: <r-help at stat.math.ethz.ch>
Sent: Saturday, July 16, 2005 12:31 AM
Subject: Re: [R] Coxph with factors


> On Fri, 15 Jul 2005, Kylie-Anne Richards wrote:
>>
>> FIRST Q: The default uses the mean of 'vo' and mean of 'po', but what is 
>> it
>> using for the factors?? Is it the sum of the coef of the factors divided 
>> by
>> the number of factors??
>
> It uses the mean of each factor variable.  The $means component of the fit 
> gives the values it uses.
>
>> SECOND Q: For a model with covariates I would normally specify:
>> final.surv<-survfit((final), data.frame(po=0,vo=0,pom=0,individual=T)) to
>> get the baseline survival prob.; what would I specify for a model with a
>> factor, i.e., 'f.pom' ??
>
> I would first note that the survival function at zero covariates is not a 
> very useful thing and is usually numerically unstable, and it's often more 
> useful to get the survival function at some reasonable set of covariates.
>
> In any case, to specify f.pom You need it to be a factor with the same set 
> of levels.  You don't say what the lowest level of pom is, but if it is, 
> say, -3.
>
> f.pom=factor(-3, levels=seq(-3,2.5, by=0.5))
>
>
>  =thomas
>



From gasparin at calvino.polito.it  Sat Jul 16 10:24:20 2005
From: gasparin at calvino.polito.it (Mauro Gasparini)
Date: Sat, 16 Jul 2005 10:24:20 +0200
Subject: [R] cbind a list of matrices
Message-ID: <42D8C434.102@calvino.polito.it>


Dear users,

I have a list of several matrices with the same number of columns,
how do I rbind them all with a vectorized command?

A related simpler question is, how do I vectorize the instruction
that rbinds together several copies of the same matrix?

Thanks.

-- 
Mauro Gasparini



From tura at centroin.com.br  Sat Jul 16 11:09:40 2005
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sat, 16 Jul 2005 06:09:40 -0300
Subject: [R] R: to the power
In-Reply-To: <1121173892.42d3c1848e058@webmail.uct.ac.za>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
Message-ID: <6.1.2.0.2.20050716060714.03927eb0@centroin.com.br>

At 10:11 12/7/2005, allan_sta_staff_sci_main_uct at mail.uct.ac.za wrote:

>hi all
>
>why does R do this:
>
>(-8)^(1/3)=NaN
>
>the answer should be : -2

Allan

In my computer:
 > (-8)^(1/3)
[1] NaN

 > -8^(1/3)
[1] -2

 > -(8^(1/3))
[1] -2


The problem is -8 or the problem is (-8) ?


[]s
Tura 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul 16 11:44:18 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 16 Jul 2005 10:44:18 +0100 (BST)
Subject: [R] cbind a list of matrices
In-Reply-To: <42D8C434.102@calvino.polito.it>
Message-ID: <XFMail.050716104418.Ted.Harding@nessie.mcc.ac.uk>

On 16-Jul-05 Mauro Gasparini wrote:
> 
> Dear users,
> 
> I have a list of several matrices with the same number of columns,
> how do I rbind them all with a vectorized command?
> 
> A related simpler question is, how do I vectorize the instruction
> that rbinds together several copies of the same matrix?

Didn't you simply try:

> A<-matrix(c(1.1,1.2,1.3,1.4,1.5,1.6),ncol=3)
> B<-matrix(c(2.1,2.2,2.3,2.4,2.5,2.6),ncol=3)
> C<-matrix(c(3.1,3.2,3.3,3.4,3.5,3.6),ncol=3)
> A
     [,1] [,2] [,3]
[1,]  1.1  1.3  1.5
[2,]  1.2  1.4  1.6
> B
     [,1] [,2] [,3]
[1,]  2.1  2.3  2.5
[2,]  2.2  2.4  2.6
> C
     [,1] [,2] [,3]
[1,]  3.1  3.3  3.5
[2,]  3.2  3.4  3.6
> rbind(A,B,C)
     [,1] [,2] [,3]
[1,]  1.1  1.3  1.5
[2,]  1.2  1.4  1.6
[3,]  2.1  2.3  2.5
[4,]  2.2  2.4  2.6
[5,]  3.1  3.3  3.5
[6,]  3.2  3.4  3.6
> rbind(A,A,A)
     [,1] [,2] [,3]
[1,]  1.1  1.3  1.5
[2,]  1.2  1.4  1.6
[3,]  1.1  1.3  1.5
[4,]  1.2  1.4  1.6
[5,]  1.1  1.3  1.5
[6,]  1.2  1.4  1.6

If there's an exception under which the above does not work,
I'd be interested to hear of it!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Jul-05                                       Time: 10:29:59
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul 16 11:44:18 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 16 Jul 2005 10:44:18 +0100 (BST)
Subject: [R] R: to the power
In-Reply-To: <6.1.2.0.2.20050716060714.03927eb0@centroin.com.br>
Message-ID: <XFMail.050716104418.Ted.Harding@nessie.mcc.ac.uk>

On 16-Jul-05 Bernardo Rangel Tura wrote:
> At 10:11 12/7/2005, allan_sta_staff_sci_main_uct at mail.uct.ac.za wrote:
>>hi all
>>
>>why does R do this:
>>
>>(-8)^(1/3)=NaN
>>
>>the answer should be : -2
> 
> Allan
> 
> In my computer:
>  > (-8)^(1/3)
> [1] NaN
> 
>  > -8^(1/3)
> [1] -2
> 
>  > -(8^(1/3))
> [1] -2
> 
> 
> The problem is -8 or the problem is (-8) ?
> 
> 
> []s
> Tura 

There's no problem. The issue lies in implicit precedence.
See ?Syntax.

In (-8)^(1/3) you have used parentheses to force evaluation
of "-3" and of "1/3" prior to applying "^", and "^" to a fraction
gives NaN for negative numbers.

In -8^(1/3) the parentheses force evaluation of (1/3) before
it is used as the "power" in "^", but implicit precedence
(see ?Syntax) causes "^" to be applied before "-" is applied.

So the result is equivalent to

  -(8^(1/3))

i.e. the same as your third case.

When you're not sure about the order in which operations will
be carried out, use parentheses to ensure that the precedence
you want is the one you will get. Otherwise you could get a
result like

  > -8^1/3
  [1] -2.666667

which equals  (-(8^1))/3, when you really wanted -(8^(1/3)).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Jul-05                                       Time: 10:43:58
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Sat Jul 16 12:20:35 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jul 2005 12:20:35 +0200
Subject: [R] cbind a list of matrices
In-Reply-To: <XFMail.050716104418.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050716104418.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2vf3bt5l8.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 16-Jul-05 Mauro Gasparini wrote:
> > 
> > Dear users,
> > 
> > I have a list of several matrices with the same number of columns,
> > how do I rbind them all with a vectorized command?
> > 
> > A related simpler question is, how do I vectorize the instruction
> > that rbinds together several copies of the same matrix?
> 
> Didn't you simply try:
> 
> > A<-matrix(c(1.1,1.2,1.3,1.4,1.5,1.6),ncol=3)
> > B<-matrix(c(2.1,2.2,2.3,2.4,2.5,2.6),ncol=3)
> > C<-matrix(c(3.1,3.2,3.3,3.4,3.5,3.6),ncol=3)
> > A
>      [,1] [,2] [,3]
> [1,]  1.1  1.3  1.5
> [2,]  1.2  1.4  1.6
> > B
>      [,1] [,2] [,3]
> [1,]  2.1  2.3  2.5
> [2,]  2.2  2.4  2.6
> > C
>      [,1] [,2] [,3]
> [1,]  3.1  3.3  3.5
> [2,]  3.2  3.4  3.6
> > rbind(A,B,C)
>      [,1] [,2] [,3]
> [1,]  1.1  1.3  1.5
> [2,]  1.2  1.4  1.6
> [3,]  2.1  2.3  2.5
> [4,]  2.2  2.4  2.6
> [5,]  3.1  3.3  3.5
> [6,]  3.2  3.4  3.6
> > rbind(A,A,A)
>      [,1] [,2] [,3]
> [1,]  1.1  1.3  1.5
> [2,]  1.2  1.4  1.6
> [3,]  1.1  1.3  1.5
> [4,]  1.2  1.4  1.6
> [5,]  1.1  1.3  1.5
> [6,]  1.2  1.4  1.6
> 
> If there's an exception under which the above does not work,
> I'd be interested to hear of it!

If the matrices literally are in a list, you might need

do.call("rbind", l)

which for multiple copies (5, say) of the same matrix might be done via

M <- matrix(1:4,2)
MM <- do.call("rbind",replicate(5, M, simplify=FALSE))

although it might be more efficient with

MM <- matrix(t(M),5*2,2,byrow=T) 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dr.mike at ntlworld.com  Sat Jul 16 13:39:19 2005
From: dr.mike at ntlworld.com (Mike Waters)
Date: Sat, 16 Jul 2005 12:39:19 +0100
Subject: [R] question from environmental statistics
In-Reply-To: <1121475795.42d85cd3f1c97@webmail.scsv.nevada.edu>
Message-ID: <20050716113936.LUEH26372.aamta12-winn.ispmail.ntl.com@d600>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> pantd at unlv.nevada.edu
> Sent: 16 July 2005 02:03
> To: r-help at stat.math.ethz.ch
> Subject: [R] question from environmental statistics
> 
> thanks Fran. that was useful but Im still in a fix. its a 
> real life data which looks like this:
> 0.9
> 10.9
> 24.0
> 6.7
> 0.6
> 1.0
> 2.4
> 12.4
> 7.9
> 15.8
> 1.4
> 7.9
> 11000.0
> 
> (benzene conc. taken after WTC attacks)..its just a small 
> chunk of data i pasted for you to look at.
> its neither normal nor lognormal. someone told me that qq 
> plot does help in determining the distribution. im not sure 
> how to get it.
> 
> can someone help me in this.
> 
> thanks
> 
> 
> 
> Take a look at this document by Vito Ricci:
> http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
> 
> Did you try RSiteSearch("Fit distribution") or a Google 
> search?  That will lead you to fit.dist{gnlm} and fitdistr{MASS}
> 
> Cheers
> 
> Francisco
> 
> 
> >From: pantd at unlv.nevada.edu
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] question from environmental statistics
> >Date: Thu, 14 Jul 2005 14:06:45 -0700
> >
> >
> >
> >Dear R users
> >I want to knw if there is a way in which a raw dataset can 
> be modelled 
> >by some distribution. besides the gof test is there any test 
> involving 
> >gamma or lognormal that would fit the data.
> >
> >thank you
> >
> >-dev
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

I think, perhaps, that we need a little more to go on, before we can help
here. What do those data represent? Are they spatial (e.g. upwind versus
other directions), temporal (e.g. small background values followed by an
emission spike), or a combination of the two? Perhaps you have a PCB release
signal superimposed on 'natural' background? In which case, maybe you want a
blind signal separation procedure to separate the two as a first step. 

Mike



From spencer.graves at pdf.com  Sat Jul 16 15:04:10 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 06:04:10 -0700
Subject: [R] Can't get sample function from "An Introduction to R"	to
 work
In-Reply-To: <719c117734647fcebd5533dbdbbf1b87@mpls.k12.mn.us>
References: <s2d7dc3a.079@wfscgate.tamu.edu>	<16601407.1121463813237.JavaMail.root@Sniper29>
	<719c117734647fcebd5533dbdbbf1b87@mpls.k12.mn.us>
Message-ID: <42D905CA.2020503@pdf.com>

	  When you copied it into MS Word, did you "Copy -> "paste special" -> 
"unformatted text"?  This trick sometimes might expose (or eliminate) 
nonprinting characters or characters with special attributes that might 
have created the problem.

	  spencer graves

David Groos wrote:

> Thank you-all very much for your help, your responses and help has been 
> very encouraging.  The following doesn't close the case but it tables 
> it...
> 
> First I copied Ken's code into my R Console and...it worked great!  
> That was baffling as it looked identical to mine.
> 
> I did not explicitly say earlier that the code I sent out I had copied 
> from the console, pasted into MS Word, changed font size, then pasted 
> it into the e-mail--in other words, that was a copy of one of the codes 
> that didn't work.
> 
> Anyway,  I then copied the non-working, pasted it into Console, 
> expecting that it would say "Error: syntax error" upon pressing the 
> return key at the end of this line:
> 
>>>+ tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
> 
> but it didn't!  and the code worked this time, also!
> I then went about doing what I could to replicate the error from before 
> and was as unsuccessful in doing that as I was in making it work, 
> earlier.
> 
> 
> On Jul 15, 2005, at 4:43 PM, Peter Dalgaard wrote:
> 
> 
>>"Bret Collier" <bret at tamu.edu> writes:
>>
>>
>>>David,
>>>If below is exactly what you typed, check your code again, I think you
>>>are missing a '}' after the last 2 parentheses.
>>
>>That's not supposed to cause a syntax error, just another '+'.
> 
> (right! that's what I thought...)
> 
>>I can copy and paste the code as written and not get an error:
>>
>>
>>>twosam<-function(y1, y2) {
>>
>>+ n1<-length(y1);n2 <-length(y2)
>>+ yb1<-mean(y1); yb2<-mean(y2)
>>+ s1<-var(y1);s2<-var(y2)
>>+ s<-((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2)
>>+ tst<-(yb1-yb2)/sqrt(s*(1/n1+1/n2))
>>+ }
>>
>>Perhaps this was the 1st time David got his typing right? The error is
>>of the sort you'd get if you had
>>
>> s<-((n1-1)*s1 + (n2-1)*s2/(n1+n2-2)
> 
> Interestingly enough, I did try this line when first I was trying to 
> make the code work and with just 1 ")" at the end I didn't get the 
> error message, but then again I wasn't able to make the whole program 
> work, either.
> 
> In conclusion, I can't explain why it didn't first work time nor why I 
> couldn't replicate the error.  I think I ought to e-mail the Mac R 
> folks about this.
> 
> Again, Thanks,
> 
> -David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ligges at statistik.uni-dortmund.de  Sat Jul 16 15:28:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Jul 2005 15:28:59 +0200
Subject: [R] rmpi in windows
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433E0C1@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433E0C1@elon12p32001.csfp.co.uk>
Message-ID: <42D90B9B.8040105@statistik.uni-dortmund.de>

Uzuner, Tolga wrote:
> Hi Folks,
> Has anyone been able to get rmpi to work under windows ?

Rmpi uses the LAM implementation of MPI.
See
http://www.lam-mpi.org/faq/category12.php3
and read FAQ 2 which implicitly tells us that there is no native port, 
hence you cannot run it under Windows.
The package maintainer may know better.

Uwe Ligges


BTW: Why do you ask twice (in private message and on R-help)? I am 
reading R-help anyway ...




> Thanks,
> Tolga
> 
> Please follow the attached hyperlink to an important disclaimer
> <http://www.csfb.com/legal_terms/disclaimer_europe.shtml>
> 
> 
> 
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer: 
> 
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sat Jul 16 15:33:18 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 06:33:18 -0700
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <20050713224444.GC2885@laboiss2>
References: <20050713224444.GC2885@laboiss2>
Message-ID: <42D90C9E.9070903@pdf.com>

	  Have you considered "BTm" in library(BradleyTerry)?  Consider the 
following example:

 > cond1 <- data.frame(winner=rep(LETTERS[1:3], e=2),
+           loser=c("B","C","A","C","A","B"),
+           Freq=1:6)
 > cond2 <- data.frame(winner=rep(LETTERS[1:3], e=2),
+           loser=c("B","C","A","C","A","B"),
+           Freq=6:1)
 > fit1 <- BTm(cond1~..)
 > fit2 <- BTm(cond2~..)
 > fit12 <- BTm(rbind(cond1, cond2)~..)
 > Dev12 <- (fit1$deviance+fit2$deviance
+           -fit12$deviance)
 > pchisq(Dev12, 2, lower=FALSE)
[1] 0.8660497

	  This says the difference between the two data sets, cond1 and cond2, 
are not statistically significant.

	  Do you present each subject with onely one pair?  If yes, then this 
model is appropriate.  If no, then the multiple judgments by the same 
subject are not statistically independent, as assumed by this model. 
However, if you don't get statistical significance via this kind of 
computation, it's unlikely that a better model would give you 
statistical significance.  If you get a p value of, say, 0.04, then the 
difference is probably NOT statistically significant.

	  The p value you get here would be an upper bound.  You could get a 
lower bound by using only one of the three pairs presented to each 
subject selected at random.  If that p value were statistically 
significant, then I think it is safe to say that your two sets of 
conditions are significantly different.  For any value in between, it 
would depend on how independent the three choices by the same subject. 
You might, for example, delete one of the three pairs at random and use 
the result of that comparison.

	  There are doubtless better techniques, but I'm not familiar with 
them.  Perhaps someone else will reply to my reply.

	  spencer graves

Rafael Laboissiere wrote:

> Hi,
> 
> I wish to analyze with R the results of a perception experiment in which
> subjects had to recognize each stimulus among three choices (this was a
> forced-choice design).  The experiment runs under two different
> conditions and the data is like the following:
> 
>    N1 : count of trials in condition 1
>    p11, p12, p13: proportions of choices 1, 2, and 3 in condition 1
>    
>    N2 : count of trials in condition 2
>    p21, p22, p23: proportions of choices 1, 2, and 3 in condition 2
>    
> How can I test whether the triple (p11,p12,p13) is different from the
> triple (p21,p22,p23)?  Clearly, prop.test does not help me here, because
> it relates to two-choices tests.
> 
> I apologize if the answer is trivial, but I am relatively new to R and
> could not find any pointers in the FAQ or in the mailing list archives.
> 
> Thanks in advance for any help,
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From S.O.Nyangoma at amc.uva.nl  Sat Jul 16 15:50:10 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Sat, 16 Jul 2005 15:50:10 +0200
Subject: [R] cbind a list of matrices
Message-ID: <17b252517b3a66.17b3a6617b2525@amc.uva.nl>


> Didn't you simply try:
> 
> > A<-matrix(c(1.1,1.2,1.3,1.4,1.5,1.6),ncol=3)
> > B<-matrix(c(2.1,2.2,2.3,2.4,2.5,2.6),ncol=3)
> > C<-matrix(c(3.1,3.2,3.3,3.4,3.5,3.6),ncol=3)
> > A
>     [,1] [,2] [,3]
> [1,]  1.1  1.3  1.5
> [2,]  1.2  1.4  1.6
> > B
>     [,1] [,2] [,3]
> [1,]  2.1  2.3  2.5
> [2,]  2.2  2.4  2.6
> > C
>     [,1] [,2] [,3]
> [1,]  3.1  3.3  3.5
> [2,]  3.2  3.4  3.6
> > rbind(A,B,C)
>     [,1] [,2] [,3]
> [1,]  1.1  1.3  1.5
> [2,]  1.2  1.4  1.6
> [3,]  2.1  2.3  2.5
> [4,]  2.2  2.4  2.6
> [5,]  3.1  3.3  3.5
> [6,]  3.2  3.4  3.6
> > rbind(A,A,A)
>     [,1] [,2] [,3]
> [1,]  1.1  1.3  1.5
> [2,]  1.2  1.4  1.6
> [3,]  1.1  1.3  1.5
> [4,]  1.2  1.4  1.6
> [5,]  1.1  1.3  1.5
> [6,]  1.2  1.4  1.6
> 


This would be adequate for low dimensional dataset e.g. the example 
you have give. What if you have a list of thousands of matrices. Is 
there an efficient way of doing this other than looping?



From f.gherardini at pigrecodata.net  Sat Jul 16 18:12:03 2005
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Sat, 16 Jul 2005 18:12:03 +0200
Subject: [R] Padding in lattice plots
In-Reply-To: <eb555e6605071508005fe5f93e@mail.gmail.com>
References: <200507151442.39139.f.gherardini@pigrecodata.net>
	<200507151619.03581.f.gherardini@pigrecodata.net>
	<eb555e6605071508005fe5f93e@mail.gmail.com>
Message-ID: <200507161812.04359.f.gherardini@pigrecodata.net>

On Friday 15 July 2005 17:00, Deepayan Sarkar wrote:
> On 7/15/05, Federico Gherardini <f.gherardini at pigrecodata.net> wrote:
> > On Friday 15 July 2005 14:42, you wrote:
> > > Hi all,
> > > I've used the split argument to print four lattice plots on a single
> > > page. The problem now is that I need to reduce the amount of white
> > > space between the plots. I've read other mails in this list about the
> > > new trellis parameters layout.heights and layout.widhts but I haven't
> > > been able to use them properly. I've tried to input values between 0
> > > and 1 as the padding value (both left and right and top and bottom) but
> > > nothing changed. It seems I can only increase the padding by using
> > > values > 1. Any ideas?
> > >
> > > Thanks in advance for your help
> > > Federico Gherardini
> >
> > It seems like I've found an answer myself.... you have to use negative
> > values to decrease the padding. I thought it was something like the cex
> > parameter which acts like a multiplier
>
> I thought so too.
>
> > but this is not the case.
>
> Could you post what you used? There are several different padding
> parameters you need to set to 0, did you change them all?
>
> Deepayan

Hi Deepayan
This is what I used.... I don't know if I did everything the "proper" way but 
at least I got the result I was seeking! :)

trellis.par.set(list(layout.heights = list(top.padding = -1)))

trellis.par.set(list(layout.heights = list(bottom.padding = -1, 
axis.xlab.padding = 1, xlab = -1.2)))

trellis.par.set(list(layout.widths = list(left.padding = -1)))

trellis.par.set(list(layout.widths = list(right.padding = -1, 
ylab.axis.padding = -0.5)))

Do these settings make any sense?

Federico



From gasparin at calvino.polito.it  Sat Jul 16 16:28:04 2005
From: gasparin at calvino.polito.it (Mauro Gasparini)
Date: Sat, 16 Jul 2005 16:28:04 +0200
Subject: [R] summary:  cbind a list of matrices
Message-ID: <42D91974.6020601@calvino.polito.it>


Thanks to all kind people who answered. Attached is a useful reply,
among many others.

-- 
Mauro Gasparini

Professore Straordinario di Statistica
Dipartimento di Matematica, Politecnico di Torino
Corso Duca degli Abruzzi 24 I-10129 Torino, Italy 

tel: +39 011 564 7546
fax: +39 011 564 7599
email: gasparini at calvino.polito.it
www: http://calvino.polito.it/~gasparin/

Coordinatore Scientifico del
Centro Universitario di Statistica per le Scienze Biomediche
dell'Universit?? Vita-Salute San Raffaele di Milano
www: http://www.cussb.unihsr.it/
-------------- next part --------------
An embedded message was scrubbed...
From: Patrick Burns <pburns at pburns.seanet.com>
Subject: Re: [R] cbind a list of matrices
Date: Sat, 16 Jul 2005 09:49:13 +0100
Size: 2274
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050716/8eb0534e/Rcbindalistofmatrices.mht

From tlumley at u.washington.edu  Sat Jul 16 16:58:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 16 Jul 2005 07:58:45 -0700 (PDT)
Subject: [R] Can't get sample function from "An Introduction to R" to
 work
In-Reply-To: <719c117734647fcebd5533dbdbbf1b87@mpls.k12.mn.us>
References: <s2d7dc3a.079@wfscgate.tamu.edu>
	<16601407.1121463813237.JavaMail.root@Sniper29>
	<719c117734647fcebd5533dbdbbf1b87@mpls.k12.mn.us>
Message-ID: <Pine.A41.4.61b.0507160755060.321378@homer12.u.washington.edu>

On Fri, 15 Jul 2005, David Groos wrote:

> Thank you-all very much for your help, your responses and help has been
> very encouraging.  The following doesn't close the case but it tables
> it...
>
> First I copied Ken's code into my R Console and...it worked great!
> That was baffling as it looked identical to mine.
>
> I did not explicitly say earlier that the code I sent out I had copied
> from the console, pasted into MS Word, changed font size, then pasted
> it into the e-mail--in other words, that was a copy of one of the codes
> that didn't work.
>

Mysterious syntax errors on OS X can result from copying and pasting into 
R. Files that look perfectly innocent can contain Windows line endings 
(\r\n) or en-dashes instead of hyphens.  These might well be transparently 
fixed in the process of converting to email.

If you actually typed into the R console then this won't explain it, of 
course.

 	-thomas



From tlumley at u.washington.edu  Sat Jul 16 17:07:28 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 16 Jul 2005 08:07:28 -0700 (PDT)
Subject: [R] Coxph with factors
In-Reply-To: <006701c589dd$58866e70$0400a8c0@KAR>
References: <002c01c58942$a5379fa0$0400a8c0@KAR>
	<Pine.A41.4.61b.0507150723370.363424@homer06.u.washington.edu>
	<006701c589dd$58866e70$0400a8c0@KAR>
Message-ID: <Pine.A41.4.61b.0507160759300.321378@homer12.u.washington.edu>

On Sat, 16 Jul 2005, Kylie-Anne Richards wrote:

> Thank you for your help.
> ____________________________________________________________
>> In any case, to specify f.pom You need it to be a factor with the same set 
>> of levels.  You don't say what the lowest level of pom is, but if it is, 
>> say, -3.
>> 
>> f.pom=factor(-3, levels=seq(-3,2.5, by=0.5))
> ____________________________________________________________
>
> For this particular model, f.pom starts at -5.5 going to 2 in 0.5 increments. 
> I seem to have misunderstood your explanation, as R is still producing an 
> error.


In the model you showed, there were no factor levels below -2.5.  You need 
to make sure that the levels are the same in the initial data and the data 
supplied to survfit.  Check this with levels().

> ____________________________________________________________
>> I would first note that the survival function at zero covariates is not a 
>> very useful thing and is usually numerically unstable, and it's often more 
>> useful to get the survival function at some reasonable set of covariates.
> ____________________________________________________________
>
> Please correct me if I'm wrong, I was under the impression that the survival 
> function at zero covariates gave the baseline distribution. I.e. if given the 
> baseline prob.,S_0, at time t, one could calculate the survival prob for 
> specified covariates by 
> S_0^exp(beta(vo)*specified(vo)+beta(po)*specified(po)+beta(f.pom at the level 
> of interest)) for time t.
>
> Since I was unable to get survfit to work with specified covariates, I was 
> using the survival probs of the 'avg' covariates, S(t), to determine the 
> baseline at time t, i.e. 
> S(t)^(1/exp(beta(vo)*mean(vo)+beta(po)*mean(po)+beta(f.pom-5.5)*mean(f.pom-5.5)+beta(f.pom-5.0)*mean(f.pom-5.0)+........). 
> And then proceeding as mention in the above paragraph (clearly not an 
> efficient way of doing things).
>

Yes, but you don't need to go via the baseline.  The survival curves for 
any two covariate vectors z1 and z2 are related by

S(t; z1)= S(t; z2)^(z1-z2)

For convenience of mathematical notation, mathematical statisticians write 
everything in terms of z2=0, and call this "the baseline". In the real 
world, though, you are better off with a baseline defined at a covariate 
value somewhere in the vicinity of the actual data. If, as if often the 
case, the zero covariate value is a long way from the observed data, both 
the computation of the survival curve at zero and the transformation to 
the covariates you want are numerically ill-conditioned.

So, you can use the "baseline" returned by survfit(z2), which is at 
z2=fit$means, to do anything you can do with the baseline at z=0, and the 
computations will be more accurate.

 	-thomas



From tlumley at u.washington.edu  Sat Jul 16 17:17:33 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 16 Jul 2005 08:17:33 -0700 (PDT)
Subject: [R] R: to the power
In-Reply-To: <6.1.2.0.2.20050716060714.03927eb0@centroin.com.br>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
	<6.1.2.0.2.20050716060714.03927eb0@centroin.com.br>
Message-ID: <Pine.A41.4.61b.0507160808220.321378@homer12.u.washington.edu>

On Sat, 16 Jul 2005, Bernardo Rangel Tura wrote:

> At 10:11 12/7/2005, allan_sta_staff_sci_main_uct at mail.uct.ac.za wrote:
>
>> hi all
>>
>> why does R do this:
>>
>> (-8)^(1/3)=NaN
>>
>> the answer should be : -2
>

Yes and no.

The problem is that the reciprocal of 3 is not exactly representable as a 
floating point number (it has an infinite binary expansion 
.010101010101...)

So the R expression 1/3 actually returns a number slightly different from 
one-third. It is a fraction with denominator a power of two (probably 
2^53).  Now, -8 to power that is a fraction with denominator a power of 2 
is not a real number, so, NaN.

It would be nice if R could realize that you meant the cube root of -8, 
but that requires either magical powers or complicated and unreliable 
heuristics.  The real solution might be a function like
   root(x,a,b)
to compute  x^(a/b), where a and b could then be exactly representable 
integers. If someone wants to write one....

 	-thomas



From tlumley at u.washington.edu  Sat Jul 16 17:19:36 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 16 Jul 2005 08:19:36 -0700 (PDT)
Subject: [R] Coxph with factors
In-Reply-To: <Pine.A41.4.61b.0507160759300.321378@homer12.u.washington.edu>
References: <002c01c58942$a5379fa0$0400a8c0@KAR>
	<Pine.A41.4.61b.0507150723370.363424@homer06.u.washington.edu>
	<006701c589dd$58866e70$0400a8c0@KAR>
	<Pine.A41.4.61b.0507160759300.321378@homer12.u.washington.edu>
Message-ID: <Pine.A41.4.61b.0507160818500.321378@homer12.u.washington.edu>

On Sat, 16 Jul 2005, Thomas Lumley wrote:
>
> Yes, but you don't need to go via the baseline.  The survival curves for
> any two covariate vectors z1 and z2 are related by
>
> S(t; z1)= S(t; z2)^(z1-z2)
>
Actually
   S(t; z1)=S(t;z2) ^(beta'(z1-z2))

of course.

 	-thomas



From baron at psych.upenn.edu  Sat Jul 16 17:49:47 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 16 Jul 2005 11:49:47 -0400
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <42D90C9E.9070903@pdf.com>
References: <20050713224444.GC2885@laboiss2> <42D90C9E.9070903@pdf.com>
Message-ID: <20050716154947.GA26263@psych>

I suspect that there are more direct ways to do this test, but it 
is unclear to me just what the issue is.  For example, if there
are many subjects and very few stimuli for each, you might want
to get some sort of measure of ability for each subject (many
possibilities here, then test the measure across subjects with a
t test.  The measure must be chosen so that you can specify a
null hypothesis.  It must be directional.

If you have a few subjects and many trials per subject, then you
could do a significance test for each subject.
You want a directional test, because you have a specific
hypothesis, namely, that the correct answer will occur more often 
than predicted from the marginal frequencies in the 3x3 table.
(I assume it is a 3x3 table with stimuli as rows and responses ad 
columns, and you want to show that the diagonal cells are higher
than predicted.) One possibility is kappa, which is in the vcd
package, and also in psy and concord, in somewhat different
forms.

Usually in this sort of experiment, though, there isn't much of
an issue about whether subjects are transmitting information at
all.  Rather the issue is testing alternative models of what they 
are doing.

Jon

On 07/16/05 06:33, Spencer Graves wrote:
> 	  Have you considered "BTm" in library(BradleyTerry)?  Consider the
> following example:
> 
>  > cond1 <- data.frame(winner=rep(LETTERS[1:3], e=2),
> +           loser=c("B","C","A","C","A","B"),
> +           Freq=1:6)
>  > cond2 <- data.frame(winner=rep(LETTERS[1:3], e=2),
> +           loser=c("B","C","A","C","A","B"),
> +           Freq=6:1)
>  > fit1 <- BTm(cond1~..)
>  > fit2 <- BTm(cond2~..)
>  > fit12 <- BTm(rbind(cond1, cond2)~..)
>  > Dev12 <- (fit1$deviance+fit2$deviance
> +           -fit12$deviance)
>  > pchisq(Dev12, 2, lower=FALSE)
> [1] 0.8660497
> 
> 	  This says the difference between the two data sets, cond1 and cond2,
> are not statistically significant.
> 
> 	  Do you present each subject with onely one pair?  If yes, then this
> model is appropriate.  If no, then the multiple judgments by the same
> subject are not statistically independent, as assumed by this model.
> However, if you don't get statistical significance via this kind of
> computation, it's unlikely that a better model would give you
> statistical significance.  If you get a p value of, say, 0.04, then the
> difference is probably NOT statistically significant.
> 
> 	  The p value you get here would be an upper bound.  You could get a
> lower bound by using only one of the three pairs presented to each
> subject selected at random.  If that p value were statistically
> significant, then I think it is safe to say that your two sets of
> conditions are significantly different.  For any value in between, it
> would depend on how independent the three choices by the same subject.
> You might, for example, delete one of the three pairs at random and use
> the result of that comparison.
> 
> 	  There are doubtless better techniques, but I'm not familiar with
> them.  Perhaps someone else will reply to my reply.
> 
> 	  spencer graves
> 
> Rafael Laboissiere wrote:
> 
> > Hi,
> >
> > I wish to analyze with R the results of a perception experiment in which
> > subjects had to recognize each stimulus among three choices (this was a
> > forced-choice design).  The experiment runs under two different
> > conditions and the data is like the following:
> >
> >    N1 : count of trials in condition 1
> >    p11, p12, p13: proportions of choices 1, 2, and 3 in condition 1
> >
> >    N2 : count of trials in condition 2
> >    p21, p22, p23: proportions of choices 1, 2, and 3 in condition 2
> >
> > How can I test whether the triple (p11,p12,p13) is different from the
> > triple (p21,p22,p23)?  Clearly, prop.test does not help me here, because
> > it relates to two-choices tests.
> >
> > I apologize if the answer is trivial, but I am relatively new to R and
> > could not find any pointers in the FAQ or in the mailing list archives.
> >
> > Thanks in advance for any help,
> >
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From rvivekrao at yahoo.com  Sat Jul 16 19:17:30 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Sat, 16 Jul 2005 10:17:30 -0700 (PDT)
Subject: [R] topical guide to R packages
Message-ID: <20050716171730.35793.qmail@web32109.mail.mud.yahoo.com>

I would like to see R packages arranged by topic. CRAN
Task Views are at 
http://lib.stat.cmu.edu/R/CRAN/src/contrib/Views/ ,
but I'd like something more detailed. For example, the
IMSL Fortran library, version 4 is easy to navigate
and has procedures arranged according to following
topics:

   Basic Statistics
   Regression
   Correlation
   Analysis of Variance
   Categorical and Discrete Data Analysis
   Nonparametric Statistics
   Tests of Goodness of FIt and Randomness
   Time Series Analysis and Forecasting
   Covariance Structures and Factor Analysis
   Discriminant Analysis
   Cluster Analysis
   Sampling
   Survival Analysis, Life Testing, and Reliability
   Multidimensional Scaling
   Density and Hazard Estimation
   Line Printer Graphics
   Probability Distribution Functions and Inverses
   Random Number Generation
   Utilities

As a start, here are the R packages I know of for time
series analysis:

dyn: Time Series Regression
dynlm: Dynamic Linear Regression
GeneTS: Microarray Time Series and Network Analysis
its: Irregular Time Series
sspir: State Space Models in R 
tseries: Time series analysis and computational
finance
urca: Unit root and cointegration tests for time
series data
uroot: Unit root tests and graphics for seasonal time
series 

I would like to see lists corresponding to other
categories of IMSL. Another classification system is
GAMS at http://gams.nist.gov/Classes.plain . Section
L, "Statistics, probability", would be relevant.

Regards,
Vivek Rao



From luk111111 at yahoo.com  Sat Jul 16 19:30:13 2005
From: luk111111 at yahoo.com (luk)
Date: Sat, 16 Jul 2005 10:30:13 -0700 (PDT)
Subject: [R] xlabel and ylabel in ROC curve ??
In-Reply-To: <20050716171730.35793.qmail@web32109.mail.mud.yahoo.com>
Message-ID: <20050716173013.20509.qmail@web30913.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050716/8f055a4e/attachment.pl

From jdnewmil at dcn.davis.ca.us  Sat Jul 16 19:47:51 2005
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 16 Jul 2005 10:47:51 -0700 (PDT)
Subject: [R] Confidence Intervals for Arbitrary Functions
Message-ID: <Pine.LNX.4.21.0507160907020.13193-100000@mirimichi.jdn.localnet>

I have a rather basic background in statistics, and am looking for
assistance in solving what I expect is a common type of problem.

I have measurements of physical processes, and mathematical models of
those processes that I want to feed the measurements into. A simple case
is using measurements of electric power entering and leaving a
power conversion device, sampled at regular intervals, and summed to
estimate energy in and out, and dividing the energy out by the energy in
to get an estimate of efficiency.  I know that power efficiency varies
with power level, but for this calculation I am interested in the
quantifying the "overall" efficiency rather than the instantaneous
efficiency.

If the energy quantities are treated as a normally-distributed random
variable (per measurement uncertainty), is there a package that simplifies
the determination of the probability distribution function for the
quotient of these values? Or, in the general sense, if I have a function
that computes a measure of interest, are such tools general enough to
handle this? (The goal being to determine a confidence interval for the
computed quantity.)

As an attempt to understand the issues, I have used SQL to generate
discrete sampled normal distributions, and then computed new abscissa
values using a function such as division and computing the joint
probability as the ordinate, and then re-partitioned the result into new
bins using GROUP BY.  This is general enough to handle non-normal
distributions as well, though I don't know how to quantify the numerical
stability/accuracy of this computational procedure. However, this is
pretty tedious... it seems like R ought to have some straightforward
solution to this problem, but I don't seem to know what search terms to
use.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k



From murdoch at stats.uwo.ca  Sat Jul 16 19:48:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 16 Jul 2005 13:48:41 -0400
Subject: [R] topical guide to R packages
In-Reply-To: <20050716171730.35793.qmail@web32109.mail.mud.yahoo.com>
References: <20050716171730.35793.qmail@web32109.mail.mud.yahoo.com>
Message-ID: <42D94879.7060308@stats.uwo.ca>

On 7/16/2005 1:17 PM, Vivek Rao wrote:
> I would like to see R packages arranged by topic. CRAN
> Task Views are at 
> http://lib.stat.cmu.edu/R/CRAN/src/contrib/Views/ ,
> but I'd like something more detailed. For example, the
> IMSL Fortran library, version 4 is easy to navigate
> and has procedures arranged according to following
> topics:
> 
>    Basic Statistics
>    Regression
>    Correlation
>    Analysis of Variance
>    Categorical and Discrete Data Analysis
>    Nonparametric Statistics
>    Tests of Goodness of FIt and Randomness
>    Time Series Analysis and Forecasting
>    Covariance Structures and Factor Analysis
>    Discriminant Analysis
>    Cluster Analysis
>    Sampling
>    Survival Analysis, Life Testing, and Reliability
>    Multidimensional Scaling
>    Density and Hazard Estimation
>    Line Printer Graphics
>    Probability Distribution Functions and Inverses
>    Random Number Generation
>    Utilities
> 
> As a start, here are the R packages I know of for time
> series analysis:
> 
> dyn: Time Series Regression
> dynlm: Dynamic Linear Regression
> GeneTS: Microarray Time Series and Network Analysis
> its: Irregular Time Series
> sspir: State Space Models in R 
> tseries: Time series analysis and computational
> finance
> urca: Unit root and cointegration tests for time
> series data
> uroot: Unit root tests and graphics for seasonal time
> series 
> 
> I would like to see lists corresponding to other
> categories of IMSL. Another classification system is
> GAMS at http://gams.nist.gov/Classes.plain . Section
> L, "Statistics, probability", would be relevant.

I don't see a Task View for time series, so why don't you write it, for 
a start?

Duncan Murdoch



From chabotd at globetrotter.net  Sat Jul 16 20:42:51 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Sat, 16 Jul 2005 14:42:51 -0400
Subject: [R] PBSmapping and shapefiles
In-Reply-To: <mailman.8.1121508001.926.r-help@stat.math.ethz.ch>
References: <mailman.8.1121508001.926.r-help@stat.math.ethz.ch>
Message-ID: <B612B1AC-D971-46BD-8B65-144245576D2B@globetrotter.net>

Hi,

Is there a way, preferably with R, to read shapefiles and transform  
them in a format that I could then use with package PBSmapping?

I have been able to read such files into R with maptools' read.shape  
and plot it with plot.Map, but I'd like to bring the data to  
PBSmapping and plot from there. I also looked at the package  
shapefile, but it does not seem to do what I want either.

Sincerely,

Denis Chabot



From ggrothendieck at gmail.com  Sat Jul 16 20:45:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Jul 2005 14:45:37 -0400
Subject: [R] Confidence Intervals for Arbitrary Functions
In-Reply-To: <Pine.LNX.4.21.0507160907020.13193-100000@mirimichi.jdn.localnet>
References: <Pine.LNX.4.21.0507160907020.13193-100000@mirimichi.jdn.localnet>
Message-ID: <971536df050716114523c68f66@mail.gmail.com>

On 7/16/05, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> I have a rather basic background in statistics, and am looking for
> assistance in solving what I expect is a common type of problem.
> 
> I have measurements of physical processes, and mathematical models of
> those processes that I want to feed the measurements into. A simple case
> is using measurements of electric power entering and leaving a
> power conversion device, sampled at regular intervals, and summed to
> estimate energy in and out, and dividing the energy out by the energy in
> to get an estimate of efficiency.  I know that power efficiency varies
> with power level, but for this calculation I am interested in the
> quantifying the "overall" efficiency rather than the instantaneous
> efficiency.
> 
> If the energy quantities are treated as a normally-distributed random
> variable (per measurement uncertainty), is there a package that simplifies
> the determination of the probability distribution function for the
> quotient of these values? Or, in the general sense, if I have a function
> that computes a measure of interest, are such tools general enough to
> handle this? (The goal being to determine a confidence interval for the
> computed quantity.)
> 
> As an attempt to understand the issues, I have used SQL to generate
> discrete sampled normal distributions, and then computed new abscissa
> values using a function such as division and computing the joint
> probability as the ordinate, and then re-partitioned the result into new
> bins using GROUP BY.  This is general enough to handle non-normal
> distributions as well, though I don't know how to quantify the numerical
> stability/accuracy of this computational procedure. However, this is
> pretty tedious... it seems like R ought to have some straightforward
> solution to this problem, but I don't seem to know what search terms to
> use.
> 

There is some discussion about the ratio of normals at:

   http://www.pitt.edu/~wpilib/statfaq.html

but you may just want to use bootstrapping:

  library(boot)
  library(simpleboot)



From pinard at iro.umontreal.ca  Sat Jul 16 22:49:21 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sat, 16 Jul 2005 16:49:21 -0400
Subject: [R] R: to the power
In-Reply-To: <Pine.A41.4.61b.0507160808220.321378@homer12.u.washington.edu>
References: <1121173892.42d3c1848e058@webmail.uct.ac.za>
	<6.1.2.0.2.20050716060714.03927eb0@centroin.com.br>
	<Pine.A41.4.61b.0507160808220.321378@homer12.u.washington.edu>
Message-ID: <20050716204921.GA18265@phenix.progiciels-bpi.ca>

[Thomas Lumley]

> It would be nice if R could realize that you meant the cube root
> of -8, but that requires either magical powers or complicated and
> unreliable heuristics.  The real solution might be a function like
> root(x,a,b) to compute x^(a/b), where a and b could then be exactly
> representable integers. If someone wants to write one...

While this could be done with moderate difficulty for the simpler cases,
one cannot reasonably ask R to be and do everything. :-)

So far, I see R more on the numerical side of things.  If you want
precise, exact solutions to various mathematical problems, you might
consider installing a Computer Algebra System on your machine, next to
R, for handling the symbolic side of things.

One such system which is both free and very capable might be Maxima.
Its convoluted story is rooted 40 years in the past.  Some may say it
lacks some chrome and be mislead; don't be, the engine is pretty solid.
Peek at http://maxima.sourceforge.net if you think you need such a
beast.  Beware: to use it, you need either GCL or Clisp pre-installed.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From zanon at ece.cmu.edu  Sat Jul 16 23:25:25 2005
From: zanon at ece.cmu.edu (Thomas Zanon)
Date: Sat, 16 Jul 2005 17:25:25 -0400
Subject: [R] xfig device - depth
Message-ID: <42D97B45.8050003@ece.cmu.edu>

Hi,
 I hope this is the right list for my posting, since I've never posted 
to any R list before.
I'm quite extensively using the xfig graphics device and as far as I 
figured out this
device writes all the objects into xfig layer 100 (based on what I saw 
in the devPS.c
file -if this is the file to output to xfig format - depth 100 is 
hardcoded). Are the any
plans to implement xfig layer depth control in the xfig graphics device 
or am I just
missing something.
Thanks a lot in advance
Thomas Zanon

-- 
*************************************************
Thomas Zanon

2116 Hamerschlag Hall
Department of Electrical and Computer Engineering
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213-3890

phone office: (412) 268-6638
fax:          (412) 268-3204
email+UID:    zanon at ece.cmu.edu
homepage:     http://www.ece.cmu.edu/~zanon

************************************************* 



-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From brgordon at gmail.com  Sun Jul 17 02:01:28 2005
From: brgordon at gmail.com (Brett Gordon)
Date: Sat, 16 Jul 2005 20:01:28 -0400
Subject: [R] Time Series Count Models
Message-ID: <acc5477a05071617016adaeee6@mail.gmail.com>

Hello,

I'm trying to model the entry of certain firms into a larger number of
distinct markets over time. I have a short time series, but a large
cross section (small T, big N).

I have both time varying and non-time varying variables. Additionally,
since I'm modeling entry of firms, it seems like the number of
existing firms in the market at time t should depend on the number of
firms at (t-1), so I would like to include the lagged cumulative count
as well.

My basic question is whether it is appropriate (in a statistical
sense) to include both the time varying variables and the lagged
cumulative count variable. The lagged count aside, I know there are
standard extensions to count models to handle time series. However,
I'm not sure if anything changes when lagged values of the cumulative
dependent variable are added (i.e. are the regular standard errors
correct, are estimates consistent, etc....)

I would greatly appreciate it if anyone can direct me to relevant
material on this. As a note, I have already looked at Cameron and
Trivedi's book.

Many thanks,
Brett



From brgordon at gmail.com  Sun Jul 17 02:12:23 2005
From: brgordon at gmail.com (Brett Gordon)
Date: Sat, 16 Jul 2005 20:12:23 -0400
Subject: [R] Time Series Count Models
Message-ID: <acc5477a05071617125f1447b8@mail.gmail.com>

Hello,

I'm trying to model the entry of certain firms into a larger number of
distinct markets over time. I have a short time series, but a large
cross section (small T, big N).

I have both time varying and non-time varying variables. Additionally,
since I'm modeling entry of firms, it seems like the number of
existing firms in the market at time t should depend on the number of
firms at (t-1), so I would like to include the lagged cumulative count.

My basic question is whether it is appropriate (in a statistical
sense) to include both the time varying variables and the lagged
cumulative count variable. The lagged count aside, I know there are
standard extensions to count models to handle time series. However,
I'm not sure if anything changes when lagged values of the cumulative
dependent variable are added (i.e. are the regular standard errors
correct, are estimates consistent, etc....).

Can I still use one of the time series count models while including
this lagged cumulative value?

I would greatly appreciate it if anyone can direct me to relevant
material on this. As a note, I have already looked at Cameron and
Trivedi's book.

Many thanks,

Brett



From yl2058 at columbia.edu  Sun Jul 17 02:34:26 2005
From: yl2058 at columbia.edu (Yimeng Lu)
Date: Sat, 16 Jul 2005 20:34:26 -0400
Subject: [R] Is it possible to coerce R to continue proceeding the next
	command in a loop after an error message ?
Message-ID: <001e01c58a67$4c369fc0$97a46f9c@brianstat>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050716/12cb30d9/attachment.pl

From murdoch at stats.uwo.ca  Sun Jul 17 02:51:48 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 16 Jul 2005 20:51:48 -0400
Subject: [R] Is it possible to coerce R to continue proceeding the next
 command in a loop after an error message ?
In-Reply-To: <001e01c58a67$4c369fc0$97a46f9c@brianstat>
References: <001e01c58a67$4c369fc0$97a46f9c@brianstat>
Message-ID: <42D9ABA4.9040009@stats.uwo.ca>

Yimeng Lu wrote:
> Hello R-users,
> 
> In a loop, if a function, such as "nls", gives an error, is it possible to
> coerce R to continue proceeding the next command with the same
> loop?
> 

Yes, see the try() function.  The basic usage is something like

value <- try( some calculation )
if (inherits(value, "try-error"))   handle the error
else  handle a correct calculation

Duncan Murdoch



From spencer.graves at pdf.com  Sun Jul 17 03:11:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 18:11:52 -0700
Subject: [R] Is it possible to coerce R to continue proceeding the next
 command in a loop after an error message ?
In-Reply-To: <001e01c58a67$4c369fc0$97a46f9c@brianstat>
References: <001e01c58a67$4c369fc0$97a46f9c@brianstat>
Message-ID: <42D9B058.3070000@pdf.com>

	  ?try

	  spencer graves

Yimeng Lu wrote:
> Hello R-users,
> 
> In a loop, if a function, such as "nls", gives an error, is it possible to
> coerce R to continue proceeding the next command with the same
> loop?
> 
> Thanks so much for your advice!
> 
> Hanna Lu
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul 17 03:14:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 18:14:09 -0700
Subject: [R] Time Series Count Models
In-Reply-To: <acc5477a05071617125f1447b8@mail.gmail.com>
References: <acc5477a05071617125f1447b8@mail.gmail.com>
Message-ID: <42D9B0E1.2080602@pdf.com>

	  Have you considered "lme" in library(nlme)?  If you want to go this 
route, I recommend Pinheiro and Bates (2000) Mixed-Effect Models in S 
and S-Plus (Springer).

	  spencer graves

Brett Gordon wrote:

> Hello,
> 
> I'm trying to model the entry of certain firms into a larger number of
> distinct markets over time. I have a short time series, but a large
> cross section (small T, big N).
> 
> I have both time varying and non-time varying variables. Additionally,
> since I'm modeling entry of firms, it seems like the number of
> existing firms in the market at time t should depend on the number of
> firms at (t-1), so I would like to include the lagged cumulative count.
> 
> My basic question is whether it is appropriate (in a statistical
> sense) to include both the time varying variables and the lagged
> cumulative count variable. The lagged count aside, I know there are
> standard extensions to count models to handle time series. However,
> I'm not sure if anything changes when lagged values of the cumulative
> dependent variable are added (i.e. are the regular standard errors
> correct, are estimates consistent, etc....).
> 
> Can I still use one of the time series count models while including
> this lagged cumulative value?
> 
> I would greatly appreciate it if anyone can direct me to relevant
> material on this. As a note, I have already looked at Cameron and
> Trivedi's book.
> 
> Many thanks,
> 
> Brett
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul 17 03:22:04 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 18:22:04 -0700
Subject: [R] nlme and spatially correlated errors
In-Reply-To: <5.1.0.14.2.20050715110234.0244fad0@ate.oikos.unam.mx>
References: <5.1.0.14.2.20050715110234.0244fad0@ate.oikos.unam.mx>
Message-ID: <42D9B2BC.6040506@pdf.com>

	  Have you tried "anova(fit1, fit2)", where

	  fit1 <- lme(one model...)
	  fit2 <- lme(a submodel ... )

This "anova" does about the best that anyone knows how to do -- or at 
lest did 7 years ago when it was written.  If the "submodel" changes the 
fixed effects, you should use "method='ML'".  If the "submodel" changes 
the noise model specification, use "method='REML'".  See Pinheiro and 
Bates (2000) Mixed-Effect Models in S and S-Plus (Springer).  If you 
need something more precise than the standard approximations, try 
"simulate.lme".

	  buena suerte!
	  spencer graves


Patricia Balvanera wrote:

> Dear R users,
> 
> I am using lme and nlme to account for spatially correlated errors as 
> random effects. My basic question is about being able to correct F, p, R2 
> and parameters of models that do not take into account the nature of such 
> errors using gls, glm or nlm and replace them for new F, p, R2 and 
> parameters using lme and nlme as random effects.
> 
> I am studying distribution patterns of 50 tree species along a gradient. 
> That gradient
> was sampled through 27 transects, with 10 plots within each transect. For 
> each plot I
> have data on presence/absence, abundance and basal area of the species. I 
> also have data
> for 4 environmental variables related to water availability (soil water 
> retention
> capacity, slope, insolation, altitude) and X and Y coordinates for each 
> plot. I explored
> wether the relationship between any of the response variables 
> (presence/absence,
> abundance, basal area) and the environmental variables was linear, 
> polinomial, or
> non-linear.
> 
> My main interest in this question is that I proceeded to correct for spatial
> autocorrelation (both within transects and overall) following the 
> procedures suggest by
> Crawley 2002 for linear models
> e.g. (GUAMAC = a species, CRAS = soil water retention capacity, TRANSECTO = 
> transect)
>  > model1<-gls(GUAMAC ~ CRAS)
>  > model2<-lme(GUAMAC ~ CRAS, random = ~ 1 | TRANSECTO)
>  > model3<-lme(GUAMAC ~ CRAS, random = GUAMAC ~ CRAS | TRANSECTO)
>  > model4<-lme(GUAMAC ~ CRAS, random = GUAMAC ~ CRAS -1 | TRANSECTO)
>  > AIC(model1,model2,model3,model4)
> df AIC
> model1 3 3730.537
> model2 4 3698.849
> model3 6 3702.408
> model4 4 3704.722
>  > plot(Variogram(model2, form = ~ X + Y))
>  > model5<-update(model2,corr=corSpher(c(30,0.8), form = ~ X + Y, nugget = T))
>  > plot(Variogram(modelo7, resType = "n"))
>  > summary(model5)
> 
> In this case I obtain new F for the independent variable INSOLACION, new R2 
> for the whole model and new parameters for the linear model.
> 
> I have also applied this procedure to polinomial models and to glms with 
> binomial errors
> (presence/absence) with no problem.
> 
> I am nevertheless stuck with non-linear models. I am using the protocols 
> you suggested
> in the 1998 manuals by Pinheiro and Bates, and those suggested by Crawley 
> 2002.
> Please find enclose an example with an
> exponential model (which I chose for being simple). In fact the linear 
> models I am using
> are a bit more complicated.
> (HELLOT is a species, INSOLACION = INSOLATION, basal = basal area of the 
> species, TRANSECTO = transect)
> 
>  > HELLOT ~ exp(A + (B * INSOLACION))
>  > basal.HELLOT <-function(A,B,INSOLACION) exp(A + (B * INSOLACION))
>  > HELLOT ~ basal.HELLOT(A,B,INSOLACION)
>  > basal.HELLOT<- deriv(~ exp(A + (B * INSOLACION))
> + , LETTERS [1:2], function(A, B, INSOLACION){})
>  > model1<- nlme(model = HELLOT ~ exp(A + (B * INSOLACION)), fixed = A + B 
> ~ 1,
> random = A + B ~ 1, groups = ~ TRANSECTO, start = list(fixed = c(5.23, -0.05)))
> 
> It runs perfectly and gives new values for parameters A and B, but would 
> only give me F for fixed effects of A and B, while what I am really looking 
> for is F for fixed effects of INSOLACION and the R2 of the new model.
> 
> Thank you so much in advance for your help
> 
> 
> 
> Dra. Patricia Balvanera
> Centro de Investigaciones en Ecosistemas, UNAM-Campus Morelia
> Apdo. Postal 27-3, Xangari
> 58090 Morelia, Michoac??n, Mexico
> Tel. (52-443)3-22-27-07, (52-55) 56-23-27-07
> FAX (52-443) 3-22-27-19, (52-55) 56-23-27-19
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jsorkin at grecc.umaryland.edu  Sun Jul 17 04:35:11 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 16 Jul 2005 22:35:11 -0400
Subject: [R] printing the name of the arguments passed to a function
Message-ID: <s2d98bcf.009@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050716/9556abfd/attachment.pl

From david.groos at MPLS.K12.MN.US  Sun Jul 17 04:40:00 2005
From: david.groos at MPLS.K12.MN.US (David Groos)
Date: Sat, 16 Jul 2005 21:40:00 -0500
Subject: [R] Can't get sample function from "An Introduction to R" to
	work
In-Reply-To: <3243958.1121525928323.JavaMail.root@sniper8>
References: <s2d7dc3a.079@wfscgate.tamu.edu>
	<16601407.1121463813237.JavaMail.root@Sniper29>
	<719c117734647fcebd5533dbdbbf1b87@mpls.k12.mn.us>
	<3243958.1121525928323.JavaMail.root@sniper8>
Message-ID: <cc7d230c761c1ba7a0e67a1e9341af17@mpls.k12.mn.us>


On Jul 16, 2005, at 9:58 AM, Thomas Lumley wrote:

> On Fri, 15 Jul 2005, David Groos wrote:
>
>> Thank you-all very much for your help, your responses and help has 
>> been
>> very encouraging.  The following doesn't close the case but it tables
>> it...
>>
>> First I copied Ken's code into my R Console and...it worked great!
>> That was baffling as it looked identical to mine.
>>
>> I did not explicitly say earlier that the code I sent out I had copied
>> from the console, pasted into MS Word, changed font size, then pasted
>> it into the e-mail--in other words, that was a copy of one of the 
>> codes
>> that didn't work.
>>
>
> Mysterious syntax errors on OS X can result from copying and pasting 
> into R. Files that look perfectly innocent can contain Windows line 
> endings (\r\n) or en-dashes instead of hyphens.  These might well be 
> transparently fixed in the process of converting to email.
I'll put this, as well as the copying/pasting as unformatted text as 
recommends Spencer, into my bag of tricks.
>
> If you actually typed into the R console then this won't explain it, 
> of course.
And indeed, I did type right into the R console, so the mystery 
remains...
Thanks,
David
>
> 	-thomas
>



From spencer.graves at pdf.com  Sun Jul 17 04:53:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 19:53:46 -0700
Subject: [R] 2D contour predictions
In-Reply-To: <BEFDB430.3E4D5%michael.hopkins@hopkins-research.com>
References: <BEFDB430.3E4D5%michael.hopkins@hopkins-research.com>
Message-ID: <42D9C83A.2010907@pdf.com>

	  It's not so simple, but consider the following:


x=rep(1:6, each=2, length=24)
y=rep(1:6, each=4)
z=rep(0:1, length=24)
set.seed(1)
tstDF <- data.frame(x=x, y=y, z=z,
            w=(x-3.5)^2+(y-3.5)+z+0.1*rnorm(24))
fit <- lm(w~x+y+z+I(x^2)+I(y^2), tstDF)
x0 <- seq(1, 5, .5)
y0 <- seq(1, 6, .5)
Grid <- expand.grid(x=x0, y=y0)
Grid$z <- rep(0.5, dim(Grid)[1])
pred <- predict(fit, Grid)

contour(x0, y0, array(pred, dim=c(length(x0), length(y0))))

	  This kind of thing is discussed in Venables and Ripley (2002) Modern 
Applied Statistics with S, 4th ed. (Springer).  I highly recommend this 
book.

	  spencer graves

Michael Hopkins wrote:

> 
> Hi All
> 
> I have been fitting regression models and would now like to produce some
> contour & image plots from the predictors.
> 
> Is there an easy way to do this?  My current (newbie) experience with R
> would suggest there is but that it's not always easy to find it!
> 
> f3 <- lm( fc ~ poly( speed, 2 ) + poly( torque, 2 ) + poly( sonl, 2 ) +
> poly( p_rail, 2 ) + poly( pil_sep, 2 ) + poly( maf, 2 ) + (speed + torque +
> sonl + p_rail + pil_sep + maf)^2 )
> 
> hat <- predict( f3 )
> 
> contour( sonl, maf, hat )
> 
> Error in contour.default(sonl, maf, hat) :
>     increasing 'x' and 'y' values expected
> 
> image(sonl, maf, hat)
> 
> Error in image.default(sonl, maf, hat) : increasing 'x' and 'y' values
> expected
> 
> I have tried na??ve sorting but no luck with that.
> 
> I suspect I may need to produce a data grid of some kind but I'm not clear
> how I would use R to specify such a 2D slice in the 6D design space.
> 
> TIA
> 
> Michael
> 
> P.S.  Whilst I'm asking the list - is there an easier way of expressing a
> full 2nd order model than the one I used above?  I'm sure there is but
> finding it etc etc...
> 
> 
> _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
> 
>         _/    _/   _/_/_/             Hopkins Research Ltd
>        _/    _/   _/    _/
>       _/_/_/_/   _/_/_/          http://www.hopkins-research.com/
>      _/    _/   _/   _/
>     _/    _/   _/     _/               'touch the future'
>                    
> _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul 17 05:01:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Jul 2005 20:01:31 -0700
Subject: [R] printing the name of the arguments passed to a function
In-Reply-To: <s2d98bcf.009@grecc.umaryland.edu>
References: <s2d98bcf.009@grecc.umaryland.edu>
Message-ID: <42D9CA0B.8070201@pdf.com>

	  How about the following:

 > tstFn <- function(x){
+   xName <- deparse(substitute(x))
+   cat(xName)
+   "done"
+ }
 > tstFn(Varname)
Varname[1] "done"
 >
 > tstF1 <- function(x){
+   tstF2 <- function(y){
+     cat(deparse(substitute(y)),
+         "doesn't work\n")
+   }
+   tstF2(x)
+   xName <- deparse(substitute(x))
+   tstF3 <- function(y, yName){
+     cat(yName, "works\n")
+   }
+   tstF3(x, xName)
+   "done"
+ }
 > tstF1(Varname)
x doesn't work
Varname works
[1] "done"

	 spencer graves

John Sorkin wrote:

> R2.1.1
> Win 2k
> I have a function, B, within a function, A.
> I would like to have B print the name of the argument passed to it (not
> the value of the arguments).
> i.e.,
>  
> A<-function()
> {
>     B<-function(x,y)
>         {
>          fit1<-lm(y~x,data=jo)
>          print(summary(fit1)
>          I want B to print the string "age" and the string "height".
>         }
> B(age,height)
> }
>  
> I would like have function B print the name of the arguments passed to
> it, i.e. I want B to print the word
> age  
> and the word
> height
>  
> I would appreciate any suggestions that would help me accomplish this
> task.
>  
> Thanks,
> John
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From wl at eimb.ru  Sun Jul 17 05:03:52 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Sat, 16 Jul 2005 19:03:52 -0800
Subject: [R] Strange problems with lattice plots
Message-ID: <412573748.20050716190352@eimb.ru>

Dear r-help,
I need to draw some lattice plots and have stuck in strange problems.
   
   I have a data frame of 405 rows:
> str(dframe)
`data.frame':   405 obs. of  4 variables:
 $ year : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...
 $ month: num  1 2 3 4 5 6 7 8 9 10 ...
 $ V_A2 : num  NA NA NA NA NA ...
 $ V_A3 : num  NA NA NA NA NA ...

Variable month changes from 1 to 27 as the data frame includes monthly
measurements and some aggregations of them.
Variable year varies from 1990 to 2004.
Variables V_A2 and V_A3 contain floating point values and several NAs (as
sometimes measurements were unavailable).

I draw plots:

xyplot((V_A3/25)~year|
       factor(month,labels=c(month.name,"Annual mean",
              "Annual integral","Integral (Jan-Mar)","Integral (Apr-Jun)","Integral (Jul-Sep)","Integral (Oct-Dec)",
              "Integral (Jan-Feb)","Integral (Jan-Mar)","Integral (Jan-Apr)","Integral (Jan-May)","Integral (Jan-Jun)",
              "Integral (Jan-Jul)","Integral (Jan-Aug)","Integral (Jan-Sep)","Integral (Jan-Oct)")),
       data=dfame,as.table=TRUE,type=c("o","g","r"),layout=c(3,9),
       ylab="Flow, Sv",xlab=NULL)

However, instead of plots I am getting the following error
Error in do.call("pmax", lapply(cond, is.na)) :
        symbol print-name too long

Shortening factor labels doesn't help.
Removing them at all solves the problem, but this doesn't meet my needs, as I have to
explain the meaning of each plot.

Could you, please, explain me, what does it mean, and what should I do
to avoid this and get my plots?

The ways to solve my task I see now are either to make subsets (I
wonder if it will work with the only addition of subset parameter to
call of xyplot) or to add captions to the plots to the picture in the
graphic editor.
I would like to avoid these.

Thank you very much in helping me to solve the problem!

---
Best regards,
Wladimir                mailto:wl at eimb.ru
==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From wl at eimb.ru  Sun Jul 17 05:55:40 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Sat, 16 Jul 2005 19:55:40 -0800
Subject: [R] Strange problems with lattice plots
In-Reply-To: <412573748.20050716190352@eimb.ru>
References: <412573748.20050716190352@eimb.ru>
Message-ID: <3810259864.20050716195540@eimb.ru>


Yes!!! I have overcome it!

The solution is

xyplot((V_A3/25)~year|factor(month),
        strip=strip.custom(factor.levels=c(month.name,"Annual mean",
              "Annual integral","Integral (Jan-Mar)","Integral (Apr-Jun)","Integral (Jul-Sep)","Integral (Oct-Dec)",
              
         and so on...


Sorry to disturb.
         
WE> Dear r-help,
WE> I need to draw some lattice plots and have stuck in strange problems.
   
WE>    I have a data frame of 405 rows:
>> str(dframe)
WE> `data.frame':   405 obs. of  4 variables:
WE>  $ year : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...
WE>  $ month: num  1 2 3 4 5 6 7 8 9 10 ...
WE>  $ V_A2 : num  NA NA NA NA NA ...
WE>  $ V_A3 : num  NA NA NA NA NA ...

WE> Variable month changes from 1 to 27 as the data frame includes monthly
WE> measurements and some aggregations of them.
WE> Variable year varies from 1990 to 2004.
WE> Variables V_A2 and V_A3 contain floating point values and several NAs (as
WE> sometimes measurements were unavailable).

WE> I draw plots:

WE> xyplot((V_A3/25)~year|
WE>        factor(month,labels=c(month.name,"Annual mean",
WE>               "Annual integral","Integral
WE> (Jan-Mar)","Integral (Apr-Jun)","Integral (Jul-Sep)","Integral
WE> (Oct-Dec)",
WE>               "Integral (Jan-Feb)","Integral
WE> (Jan-Mar)","Integral (Jan-Apr)","Integral (Jan-May)","Integral
WE> (Jan-Jun)",
WE>               "Integral (Jan-Jul)","Integral
WE> (Jan-Aug)","Integral (Jan-Sep)","Integral (Jan-Oct)")),
WE>        data=dfame,as.table=TRUE,type=c("o","g","r"),layout=c(3,9),
WE>        ylab="Flow, Sv",xlab=NULL)

WE> However, instead of plots I am getting the following error
WE> Error in do.call("pmax", lapply(cond, is.na)) :
WE>         symbol print-name too long



From ripley at stats.ox.ac.uk  Sun Jul 17 09:28:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Jul 2005 08:28:27 +0100 (BST)
Subject: [R] xfig device - depth
In-Reply-To: <42D97B45.8050003@ece.cmu.edu>
References: <42D97B45.8050003@ece.cmu.edu>
Message-ID: <Pine.LNX.4.61.0507170827170.20260@gannet.stats>

There are no plans, and the means to do so was not documented in the file 
format docs used, nor does the R graphics model have a concept of `depth'.

On Sat, 16 Jul 2005, Thomas Zanon wrote:

> Hi,
> I hope this is the right list for my posting, since I've never posted
> to any R list before.
> I'm quite extensively using the xfig graphics device and as far as I
> figured out this
> device writes all the objects into xfig layer 100 (based on what I saw
> in the devPS.c
> file -if this is the file to output to xfig format - depth 100 is
> hardcoded). Are the any
> plans to implement xfig layer depth control in the xfig graphics device
> or am I just
> missing something.
> Thanks a lot in advance
> Thomas Zanon
>
> -- 
> *************************************************
> Thomas Zanon
>
> 2116 Hamerschlag Hall
> Department of Electrical and Computer Engineering
> Carnegie Mellon University
> 5000 Forbes Avenue
> Pittsburgh, PA 15213-3890
>
> phone office: (412) 268-6638
> fax:          (412) 268-3204
> email+UID:    zanon at ece.cmu.edu
> homepage:     http://www.ece.cmu.edu/~zanon
>
> *************************************************
>
>
>
> -- 
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kar at itga.com.au  Sun Jul 17 13:27:45 2005
From: kar at itga.com.au (Kylie-Anne Richards)
Date: Sun, 17 Jul 2005 21:27:45 +1000
Subject: [R] Coxph with factors
References: <002c01c58942$a5379fa0$0400a8c0@KAR>
	<Pine.A41.4.61b.0507150723370.363424@homer06.u.washington.edu>
	<006701c589dd$58866e70$0400a8c0@KAR>
	<Pine.A41.4.61b.0507160759300.321378@homer12.u.washington.edu>
	<Pine.A41.4.61b.0507160818500.321378@homer12.u.washington.edu>
Message-ID: <002401c58ac2$8df6fdf0$0400a8c0@KAR>

Thank you again for your help.

Your suggestions worked perfect. The problems I had when using 
f.pom=factor(-3, levels=seq(-3,2.5, by=0.5)) were a silly mistake on my part 
(as suspected and dually pointed out).  Thanks for the notes on baseline. I 
was not aware of the inaccuracies when using it at z=0.

I will be back with questions on computationally efficient time dependant 
survival functions next month.

Cheers,
Kylie-Anne

----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Kylie-Anne Richards" <kar at itga.com.au>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, July 17, 2005 1:19 AM
Subject: Re: [R] Coxph with factors


> On Sat, 16 Jul 2005, Thomas Lumley wrote:
>>
>> Yes, but you don't need to go via the baseline.  The survival curves for
>> any two covariate vectors z1 and z2 are related by
>>
>> S(t; z1)= S(t; z2)^(z1-z2)
>>
> Actually
>   S(t; z1)=S(t;z2) ^(beta'(z1-z2))
>
> of course.
>
>  -thomas
>



From ferri.leberl at gmx.at  Sun Jul 17 17:09:36 2005
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Sun, 17 Jul 2005 17:09:36 +0200
Subject: [R] Contingency-Coefficient, Variance
Message-ID: <200507171709.36922.ferri.leberl@gmx.at>

Dear Everybody!
Excuse me for this newbie-questions, but I have not found answers to these 
Question:

- Is there a command calculating the variance if the whole group is known 
(thus dividing by n instead of n-1)?

- Which is the command to calculate the contingency-coefficient?

Thank you in advance.
Mag. Ferri Leberl



From spencer.graves at pdf.com  Sun Jul 17 17:58:15 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 17 Jul 2005 08:58:15 -0700
Subject: [R] Confidence Intervals for Arbitrary Functions
In-Reply-To: <971536df050716114523c68f66@mail.gmail.com>
References: <Pine.LNX.4.21.0507160907020.13193-100000@mirimichi.jdn.localnet>
	<971536df050716114523c68f66@mail.gmail.com>
Message-ID: <42DA8017.5060302@pdf.com>

	  Before you spend a lot of time worrying about the distributions of 
ratios of normal variates, bootstrapping, etc., I suggest you make 
normal probability plots of your data AND OF THEIR LOGARITHMS, e.g., using

qqnorm(y, datax=TRUE)
qqnorm(y, datax=TRUE, log="x")

	  With many electrical measurements from integrated circuits, these two 
plots will both likely be equally close to normality.  That's not true 
for leakage currents, for which I often see highly skewed images, unless 
I take logarithms, as:

qqnorm(exp(rnorm(99)), datax=TRUE)
qqnorm(exp(rnorm(99)), datax=TRUE, log="x")

	  I routinely see distributions of this nature in estimates of Poisson 
defect rates in wafer fab as well as in leakage currents, distributions 
of income, etc.

	  This is important, because the distribution of a ratio of normally 
distributed random variables has known pathologies, while the 
distribtuion of a ratio of lognormal variates is lognormal.  (With a 
ratio of normals, if the denominator has mean zero, the ratio follows 
the Cauchy disribution, also known as Student's t with one degree of 
freedom.  This distribution has infinite variance, and the mean is not 
even defined, being Inf-Inf.)

	  In sum, you will likely get better answers in less time if you can 
find it politically acceptable in your professional efforts to work in 
decibels or logaritms, where ratios become differences.

	  spencer graves

Gabor Grothendieck wrote:

> On 7/16/05, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
>>I have a rather basic background in statistics, and am looking for
>>assistance in solving what I expect is a common type of problem.
>>
>>I have measurements of physical processes, and mathematical models of
>>those processes that I want to feed the measurements into. A simple case
>>is using measurements of electric power entering and leaving a
>>power conversion device, sampled at regular intervals, and summed to
>>estimate energy in and out, and dividing the energy out by the energy in
>>to get an estimate of efficiency.  I know that power efficiency varies
>>with power level, but for this calculation I am interested in the
>>quantifying the "overall" efficiency rather than the instantaneous
>>efficiency.
>>
>>If the energy quantities are treated as a normally-distributed random
>>variable (per measurement uncertainty), is there a package that simplifies
>>the determination of the probability distribution function for the
>>quotient of these values? Or, in the general sense, if I have a function
>>that computes a measure of interest, are such tools general enough to
>>handle this? (The goal being to determine a confidence interval for the
>>computed quantity.)
>>
>>As an attempt to understand the issues, I have used SQL to generate
>>discrete sampled normal distributions, and then computed new abscissa
>>values using a function such as division and computing the joint
>>probability as the ordinate, and then re-partitioned the result into new
>>bins using GROUP BY.  This is general enough to handle non-normal
>>distributions as well, though I don't know how to quantify the numerical
>>stability/accuracy of this computational procedure. However, this is
>>pretty tedious... it seems like R ought to have some straightforward
>>solution to this problem, but I don't seem to know what search terms to
>>use.
>>
> 
> 
> There is some discussion about the ratio of normals at:
> 
>    http://www.pitt.edu/~wpilib/statfaq.html
> 
> but you may just want to use bootstrapping:
> 
>   library(boot)
>   library(simpleboot)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From kerryrekky at yahoo.com  Sun Jul 17 20:09:42 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Sun, 17 Jul 2005 11:09:42 -0700 (PDT)
Subject: [R] How to set the background of xyplot() to be white?
Message-ID: <20050717180942.33732.qmail@web51810.mail.yahoo.com>

The usual background of xyplot is grey. Is there any
advantage of using this color instead of white? I
don't know how to change this specification into other
colors. Anybody knows?



From laboissiere at cbs.mpg.de  Sun Jul 17 20:12:08 2005
From: laboissiere at cbs.mpg.de (Rafael Laboissiere)
Date: Sun, 17 Jul 2005 20:12:08 +0200
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <20050716154947.GA26263@psych>
References: <20050713224444.GC2885@laboiss2> <42D90C9E.9070903@pdf.com>
	<20050716154947.GA26263@psych>
Message-ID: <20050717181208.GH3043@laboiss2>

* Jonathan Baron <baron at psych.upenn.edu> [2005-07-16 11:49]:

> I suspect that there are more direct ways to do this test, but it 
> is unclear to me just what the issue is.  For example, if there
> are many subjects and very few stimuli for each, you might want
> to get some sort of measure of ability for each subject (many
> possibilities here, then test the measure across subjects with a
> t test.  The measure must be chosen so that you can specify a
> null hypothesis.  It must be directional.
> 
> If you have a few subjects and many trials per subject, then you
> could do a significance test for each subject.
> You want a directional test, because you have a specific
> hypothesis, namely, that the correct answer will occur more often 
> than predicted from the marginal frequencies in the 3x3 table.
> (I assume it is a 3x3 table with stimuli as rows and responses ad 
> columns, and you want to show that the diagonal cells are higher
> than predicted.) One possibility is kappa, which is in the vcd
> package, and also in psy and concord, in somewhat different
> forms.

Thanks for your reply, Jonathan.  Thanks also to Spencer, who suggested
using the BTm function.  I realize that my description of both the
experiment and the involved issue was not clear.  Let me try again:

My subjects do a recognition task where I present stimuli belonging to
three different classes (let us say A, B, and C).  There are many of
them.  Subjects are asked to recognize each stimulus as belonging to one
of the three classes (forced-choice design).  This is done under two
different conditions (say conditions 1 and 2).  I end up with matrices of
counts like this (in R notation):

# under condition 1
c1 <- t (matrix (c (c1AA, c1AB, c1AC, 
                    c1BA, c1BB, c1BC, 
		    c1CA, c1CB, c1CC), nc = 3))
# under condition 2
c2 <- t (matrix (c (c2AA, c2AB, c2AC, 
                    c2BA, c2BB, c2BC, 
		    c2CA, c2CB, c2CC), nc = 3))

where "cijk" is the number of times the subject gave answer k when
presented with a stimulus of class j, under condition i.

The issue is to test whether subjects perform better (in the sense of a
higher recognition score) in condition 1 compared with condition 2.  My
first idea was to test the global recognition rate, which could be
computed as:

# under condition 1
r1 <- sum (diag (c1)) / sum (c1)
# under condition 2
r2 <- sum (diag (c2)) / sum (c2)

The null hypothesis is that r1 is not different from r2. I guess that I
could test it with the chisq.test function, like this:

p1 <- sum (diag (c1))
q1 <- sum (c1) - p1
p2 <- sum (diag (c2))
q2 <- sum (c2) - p2
chisq.test (matrix (c(p1, q1, p2, q2), nc = 2))

What do you think?

I also thought about testing the triples like [c1AA, c1AB, c1AC] against
[c2AA, c2AB, c2AC], hence my original question.

-- 
Rafael



From kerryrekky at yahoo.com  Sun Jul 17 20:23:38 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Sun, 17 Jul 2005 11:23:38 -0700 (PDT)
Subject: [R] Where to learn how to deal with time class variable?
Message-ID: <20050717182338.83597.qmail@web51802.mail.yahoo.com>

Dear R-helpers,
  In my data set, I have a time variable 'RecordTime'
whose class property is 'times'. When I list my data
set, I see the values of RecordTime is like 10:20:30
in a 'h:m:s' format. Suppose I want to choose all the
data after 10 o'clock, then use

subset(data,RecordTime>10:20:30)

just doesn't work. I noticed that all the values of
RecordTime seem to take some decimals when using
str(data). But how can I easily find the
correspondence between those decimals and the exact
times? I have looked at the help file of POSIXlt and
related functions. Yet I couldn't find any useful
instruction on this issue.



From ggrothendieck at gmail.com  Sun Jul 17 20:35:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Jul 2005 14:35:57 -0400
Subject: [R] Where to learn how to deal with time class variable?
In-Reply-To: <20050717182338.83597.qmail@web51802.mail.yahoo.com>
References: <20050717182338.83597.qmail@web51802.mail.yahoo.com>
Message-ID: <971536df05071711357e625dc@mail.gmail.com>

On 7/17/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> Dear R-helpers,
>  In my data set, I have a time variable 'RecordTime'
> whose class property is 'times'. When I list my data
> set, I see the values of RecordTime is like 10:20:30
> in a 'h:m:s' format. Suppose I want to choose all the
> data after 10 o'clock, then use
> 
> subset(data,RecordTime>10:20:30)
> 
> just doesn't work. I noticed that all the values of
> RecordTime seem to take some decimals when using
> str(data). But how can I easily find the
> correspondence between those decimals and the exact
> times? I have looked at the help file of POSIXlt and
> related functions. Yet I couldn't find any useful
> instruction on this issue.

Are you using the chron library?  Assuming that is the case, you
want to make sure that the object you are comparing your times
object to is also a times object:

> library(chron)
> example(chron) # this defines a times object tms (output omitted)
> tms
[1] 23:03:20 22:29:56 01:03:30 18:21:03 16:56:26
> subset(tms, tms > times("20:00:00"))
[1] 23:03:20 22:29:56



From baron at psych.upenn.edu  Sun Jul 17 21:05:52 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 17 Jul 2005 15:05:52 -0400
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <20050717181208.GH3043@laboiss2>
References: <20050713224444.GC2885@laboiss2> <42D90C9E.9070903@pdf.com>
	<20050716154947.GA26263@psych> <20050717181208.GH3043@laboiss2>
Message-ID: <20050717190552.GA18801@psych>

On 07/17/05 20:12, Rafael Laboissiere wrote:

> Thanks for your reply, Jonathan.  Thanks also to Spencer, who suggested
> using the BTm function.  I realize that my description of both the
> experiment and the involved issue was not clear.  Let me try again:
> 
> My subjects do a recognition task where I present stimuli belonging to
> three different classes (let us say A, B, and C).  There are many of
> them.  Subjects are asked to recognize each stimulus as belonging to one
> of the three classes (forced-choice design).  This is done under two
> different conditions (say conditions 1 and 2).  I end up with matrices of
> counts like this (in R notation):
> 
> # under condition 1
> c1 <- t (matrix (c (c1AA, c1AB, c1AC,
>                     c1BA, c1BB, c1BC,
> 		    c1CA, c1CB, c1CC), nc = 3))
> # under condition 2
> c2 <- t (matrix (c (c2AA, c2AB, c2AC,
>                     c2BA, c2BB, c2BC,
> 		    c2CA, c2CB, c2CC), nc = 3))
> 
> where "cijk" is the number of times the subject gave answer k when
> presented with a stimulus of class j, under condition i.
> 
> The issue is to test whether subjects perform better (in the sense of a
> higher recognition score) in condition 1 compared with condition 2.  My
> first idea was to test the global recognition rate, which could be
> computed as:
> 
> # under condition 1
> r1 <- sum (diag (c1)) / sum (c1)
> # under condition 2
> r2 <- sum (diag (c2)) / sum (c2)
> 
> The null hypothesis is that r1 is not different from r2. I guess that I
> could test it with the chisq.test function, like this:
> 
> p1 <- sum (diag (c1))
> q1 <- sum (c1) - p1
> p2 <- sum (diag (c2))
> q2 <- sum (c2) - p2
> chisq.test (matrix (c(p1, q1, p2, q2), nc = 2))
> 
> What do you think?
> 
> I also thought about testing the triples like [c1AA, c1AB, c1AC] against
> [c2AA, c2AB, c2AC], hence my original question.

You still aren't saying whether you are doing this for each
subject for the entire data set summed over subjects.  If the
latter, are you worried about subject variance?  Do you think it
possible that some subjects might show better performance in
condition 2?  Would you be happy if you tested a single subject
and got the result?  If subject variance is an issue, then you
need to test "across subjects."  One way to do that is to
compute some performance measure for each subject and each
condition and then do a matched-pairs t test across subjects.

The method you suggest requires several assumptions, and I don't
know if these are reasonable.  The problem is in using a sum of
the diagonal (p1) and off-diagonal entries (q1) in the table.
This may work if you have no reason to think that c2 is better,
ever.  In that case, all you need is a measure that varies
monotonically with the true measure, whatever it is.  You need
also to assume that c1 and c2 do not differ in response biases,
and that it could not be the case that one of the diagonal cells
is better in c1 and another is better in c2.

I have not studied these issues much since my PhD thesis (1970!), 
but then the usual approach was to develop a sensible model of
the task and then use some parameter of the model as the
measure.  Perhaps this is over-kill for what you are doing, but I 
don't know.  For example, one model says that the subject either
knows the answer or guesses, and the guesses are distributed
across the three categories according to biases that are specific 
to the condition, but knowing the answer is independent of the
category.  (You can test the assumptions of this model.)  Another 
model (popular in 1970) is Luce's choice theory, which is similar 
to the first but uses multiplication.  If I remember correctly
(which I probably don't) you would exactly what you propose but
after taking the logs of the frequencies.

It is possible to get different, even opposite, results using
logs than you would get with your proposal.  Likewise, it is
possible to get opposite results if you ignore response bias, and 
if the conditions differ in response bias.

The suggestion I made based on the idea of inter-rater agreement
implies a rough-and-ready model similar to the first.  It does
take response bias into account.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From rashmimathur at shaw.ca  Sun Jul 17 21:11:36 2005
From: rashmimathur at shaw.ca (Rashmi Mathur)
Date: Sun, 17 Jul 2005 12:11:36 -0700
Subject: [R] indexing from 0
Message-ID: <000601c58b03$5d4346a0$583b5418@RashmisComputer>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050717/9ffed104/attachment.pl

From ggrothendieck at gmail.com  Sun Jul 17 21:31:39 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Jul 2005 15:31:39 -0400
Subject: [R] indexing from 0
In-Reply-To: <000601c58b03$5d4346a0$583b5418@RashmisComputer>
References: <000601c58b03$5d4346a0$583b5418@RashmisComputer>
Message-ID: <971536df05071712315fe1d7fc@mail.gmail.com>

On 7/17/05, Rashmi Mathur <rashmimathur at shaw.ca> wrote:
> Hello,
> 
> How would one index vectors and matrices starting from 0 (or some other
> value other than the default of 1) in R?
> 

Check out the Oarray package and also the list archives where threads
on Oarray mention other approaches as well.



From spencer.graves at pdf.com  Sun Jul 17 21:47:16 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 17 Jul 2005 12:47:16 -0700
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <20050717190552.GA18801@psych>
References: <20050713224444.GC2885@laboiss2>
	<42D90C9E.9070903@pdf.com>	<20050716154947.GA26263@psych>
	<20050717181208.GH3043@laboiss2> <20050717190552.GA18801@psych>
Message-ID: <42DAB5C4.9020503@pdf.com>

Hi, Rafael:

	  At this point, it might help if you try the posting guide! 
"http://www.R-project.org/posting-guide.html", especially the part about 
constructing a toy example with real numbers, try some of the R 
facilities discussed, and explain why you aren't sure they will solve 
your problem.  You may answer your own question in the course of working 
through that guide, and if you don't, the exercise could make it easier 
for someone else to suggest something you actually find useful.

	  spencer graves

Jonathan Baron wrote:

> On 07/17/05 20:12, Rafael Laboissiere wrote:
> 
> 
>>Thanks for your reply, Jonathan.  Thanks also to Spencer, who suggested
>>using the BTm function.  I realize that my description of both the
>>experiment and the involved issue was not clear.  Let me try again:
>>
>>My subjects do a recognition task where I present stimuli belonging to
>>three different classes (let us say A, B, and C).  There are many of
>>them.  Subjects are asked to recognize each stimulus as belonging to one
>>of the three classes (forced-choice design).  This is done under two
>>different conditions (say conditions 1 and 2).  I end up with matrices of
>>counts like this (in R notation):
>>
>># under condition 1
>>c1 <- t (matrix (c (c1AA, c1AB, c1AC,
>>                    c1BA, c1BB, c1BC,
>>		    c1CA, c1CB, c1CC), nc = 3))
>># under condition 2
>>c2 <- t (matrix (c (c2AA, c2AB, c2AC,
>>                    c2BA, c2BB, c2BC,
>>		    c2CA, c2CB, c2CC), nc = 3))
>>
>>where "cijk" is the number of times the subject gave answer k when
>>presented with a stimulus of class j, under condition i.
>>
>>The issue is to test whether subjects perform better (in the sense of a
>>higher recognition score) in condition 1 compared with condition 2.  My
>>first idea was to test the global recognition rate, which could be
>>computed as:
>>
>># under condition 1
>>r1 <- sum (diag (c1)) / sum (c1)
>># under condition 2
>>r2 <- sum (diag (c2)) / sum (c2)
>>
>>The null hypothesis is that r1 is not different from r2. I guess that I
>>could test it with the chisq.test function, like this:
>>
>>p1 <- sum (diag (c1))
>>q1 <- sum (c1) - p1
>>p2 <- sum (diag (c2))
>>q2 <- sum (c2) - p2
>>chisq.test (matrix (c(p1, q1, p2, q2), nc = 2))
>>
>>What do you think?
>>
>>I also thought about testing the triples like [c1AA, c1AB, c1AC] against
>>[c2AA, c2AB, c2AC], hence my original question.
> 
> 
> You still aren't saying whether you are doing this for each
> subject for the entire data set summed over subjects.  If the
> latter, are you worried about subject variance?  Do you think it
> possible that some subjects might show better performance in
> condition 2?  Would you be happy if you tested a single subject
> and got the result?  If subject variance is an issue, then you
> need to test "across subjects."  One way to do that is to
> compute some performance measure for each subject and each
> condition and then do a matched-pairs t test across subjects.
> 
> The method you suggest requires several assumptions, and I don't
> know if these are reasonable.  The problem is in using a sum of
> the diagonal (p1) and off-diagonal entries (q1) in the table.
> This may work if you have no reason to think that c2 is better,
> ever.  In that case, all you need is a measure that varies
> monotonically with the true measure, whatever it is.  You need
> also to assume that c1 and c2 do not differ in response biases,
> and that it could not be the case that one of the diagonal cells
> is better in c1 and another is better in c2.
> 
> I have not studied these issues much since my PhD thesis (1970!), 
> but then the usual approach was to develop a sensible model of
> the task and then use some parameter of the model as the
> measure.  Perhaps this is over-kill for what you are doing, but I 
> don't know.  For example, one model says that the subject either
> knows the answer or guesses, and the guesses are distributed
> across the three categories according to biases that are specific 
> to the condition, but knowing the answer is independent of the
> category.  (You can test the assumptions of this model.)  Another 
> model (popular in 1970) is Luce's choice theory, which is similar 
> to the first but uses multiplication.  If I remember correctly
> (which I probably don't) you would exactly what you propose but
> after taking the logs of the frequencies.
> 
> It is possible to get different, even opposite, results using
> logs than you would get with your proposal.  Likewise, it is
> possible to get opposite results if you ignore response bias, and 
> if the conditions differ in response bias.
> 
> The suggestion I made based on the idea of inter-rater agreement
> implies a rough-and-ready model similar to the first.  It does
> take response bias into account.
> 
> Jon

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From laboissiere at cbs.mpg.de  Sun Jul 17 22:56:41 2005
From: laboissiere at cbs.mpg.de (Rafael Laboissiere)
Date: Sun, 17 Jul 2005 22:56:41 +0200
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <20050717190552.GA18801@psych>
References: <20050713224444.GC2885@laboiss2> <42D90C9E.9070903@pdf.com>
	<20050716154947.GA26263@psych> <20050717181208.GH3043@laboiss2>
	<20050717190552.GA18801@psych>
Message-ID: <20050717205641.GJ3043@laboiss2>

Jon and Spencer,

First of all, thanks for your insightful comments on my questions.  I am
quite impressed by the level of support one finds in the r-help mailing
list.  In particular, as Spencer pointed out in another post, I did not
do my homework and you have been overly kind in discussing the issue.  I
promised Spencer privately that I will read the Posting Guide before
getting farther in the thread, but I would like just to give a short
answer: 

* Jonathan Baron <baron at psych.upenn.edu> [2005-07-17 15:05]:

> You still aren't saying whether you are doing this for each subject for
> the entire data set summed over subjects. If the latter, are you
> worried about subject variance? Do you think it possible that some
> subjects might show better performance in condition 2?  Would you be
> happy if you tested a single subject and got the result?  If subject
> variance is an issue, then you need to test "across subjects." One way
> to do that is to compute some performance measure for each subject and
> each condition and then do a matched-pairs t test across subjects.

Yes, I intend to do the test across subjects and subject variance is
indeed an issue in my case.

Thanks for your further suggestions, I will look at them carefully.

-- 
Rafael



From ealaca at ucdavis.edu  Sun Jul 17 23:32:38 2005
From: ealaca at ucdavis.edu (Emilio A. Laca)
Date: Sun, 17 Jul 2005 14:32:38 -0700
Subject: [R] crossed random fx nlme lme4
In-Reply-To: <40e66e0b05071506356dde573e@mail.gmail.com>
References: <F0F05839-744A-4864-97A5-073F8286356B@ucdavis.edu>
	<6.2.1.2.0.20050714122836.01c9cc20@mail.ozemail.com.au>
	<40e66e0b05071506356dde573e@mail.gmail.com>
Message-ID: <EB04C994-1B10-49A6-AD7D-A41F111D7957@ucdavis.edu>

random = pdBlocked( list( pdIdent( ~ y), pdIdent( ~ observer - 1),  
pdIdent( ~ set - 1) )

gave me output only when the data was a groupedData. The results are  
different depending on whether I specify observer or set as grouping  
factor.
I am missing something ...

Simon, Prof Bates, thanks for taking the time to reply.

eal

On Jul 15, 2005, at 6:35 AM, Douglas Bates wrote:

> On 7/13/05, Simon Blomberg <blomsp at ozemail.com.au> wrote:
>
>> At 09:35 AM 14/07/2005, Emilio A. Laca wrote:
>>
>>> I need to specify a model similar to this
>>>
>>> lme.formula(fixed = sqrt(lbPerAc) ~ y + season + y:season, data =  
>>> cy,
>>>      random = ~y | observer/set, correlation = corARMA(q = 6))
>>>
>>> except that observer and set are actually crossed instead of nested.
>>>
>>
>> Does this work for you? (following P&B pp 162-3 and an R-help archive
>> search on "crossed random effects")...
>>
>> fit <- lme(sqrt(lbPerAc) ~ y * season, random=list(pdBlocked 
>> (pdIdent(~y),
>> pdIdent(observer-1), pdIdent(set-1))), correlation=corARMA(q = 6),  
>> data=cy)
>>
>> lme isn't very well set up for crossed random effects. It's easier  
>> in lmer.
>> I don't think lmer can handle alternative correlation structures yet,
>> though. (Prof. Bates?)
>>
>
> Exactly.  Thanks for the summary.
>
>
>>
>> HTH,
>>
>> Simon.
>>
>>
>>
>>> observer and set are factors
>>> y and lbPerAc are numeric
>>>
>>> If you know how to do it or have suggestions for reading I will be
>>> grateful.
>>>
>>>
>>> eal
>>>
>>> ps I have already read Pinheiro & Bates, the jan 05 newsletter, and
>>> several postings.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/ 
>>> posting-guide.html
>>>
>>
>> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
>> Centre for Resource and Environmental Studies
>> The Australian National University
>> Canberra ACT 0200
>> Australia
>> T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
>> F: +61 2 6125 0757
>> CRICOS Provider # 00120C
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>



From deepayan.sarkar at gmail.com  Mon Jul 18 01:05:40 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 17 Jul 2005 18:05:40 -0500
Subject: [R] How to set the background of xyplot() to be white?
In-Reply-To: <20050717180942.33732.qmail@web51810.mail.yahoo.com>
References: <20050717180942.33732.qmail@web51810.mail.yahoo.com>
Message-ID: <eb555e660507171605192232b9@mail.gmail.com>

On 7/17/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> The usual background of xyplot is grey. Is there any
> advantage of using this color instead of white? I

It's the S-PLUS default.

> don't know how to change this specification into other
> colors. Anybody knows?

See  ?trellis.device

Deepayan



From yl2058 at columbia.edu  Mon Jul 18 01:38:18 2005
From: yl2058 at columbia.edu (Yimeng Lu)
Date: Sun, 17 Jul 2005 19:38:18 -0400
Subject: [R] how to solve the step halving factor problems in gnls and nls
Message-ID: <003601c58b28$9d8a33c0$97a46f9c@brianstat>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050717/e0c665dc/attachment.pl

From p.murrell at auckland.ac.nz  Mon Jul 18 02:18:37 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 18 Jul 2005 12:18:37 +1200
Subject: [R] How to use the function "plot"  as Matlab
References: <XFMail.050713110142.Ted.Harding@nessie.mcc.ac.uk>
	<x27jfvc5gs.fsf@turmalin.kubism.ku.dk>
Message-ID: <42DAF55D.3060501@stat.auckland.ac.nz>

Hi


Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> 
> 
>>This is definitely a case where "dynamic rescaling" could save
>>hassle! Brian Ripley's suggestion involves first building a
>>matrix whose columns are the replications and rows the time-points,
>>and Robin Hankin's could be easily adapted to do the same,
>>though I think would involve a loop over columns and some very
>>long vectors.
>>
>>How much easier it would be with dynamic scaling!
> 
> 
> Cue grid graphics... (and Paul's new book)


... which will give you the basic tools to produce something like this. 
  Here's a very simple start at one possible way to do it (no argument 
checking, assumes x-values are 1:length(y-values), always plots points, 
quickly runs out of different symbols to use, pays no heed to 
efficiency, ...):

plotVPs <- function(x) {
   vpStack(plotViewport(c(5, 4, 4, 2), name="pvp"),
           # Calculate scale ranges based on ALL data
           dataViewport(1:max(unlist(lapply(x, length))),
                        unlist(x), name="dvp"))
}

drawDetails.scalePlot <- function(x, recording) {
   pushViewport(plotVPs(x$data))
   grid.xaxis()
   grid.yaxis()
   grid.rect()
   # Plot ALL data
   for (i in 1:length(x$data)) {
     xx <- 1:length(x$data[[i]])
     yy <- x$data[[i]]
     grid.lines(xx, yy, default.units="native",
                gp=gpar(col="grey"))
     grid.points(xx, yy, pch=i,
                 gp=gpar(cex=0.5))
   }
   upViewport(2)
}

scalePlot <- function(x, name=NULL, newpage=TRUE) {
   if (newpage)
     grid.newpage()
   grid.draw(grob(data=list(x), name=name,
                  cl="scalePlot"))
}

addPoints <- function(x, plot) {
   grid.edit(plot, data=c(grid.get(plot)$data, list(x)))
}

# Testing
scalePlot(1:10, "myplot")
addPoints(2*1:100, "myplot")
addPoints(20*sin(seq(0, 3*pi, length=50)), "myplot")

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From paddison at museum.vic.gov.au  Mon Jul 18 05:59:58 2005
From: paddison at museum.vic.gov.au (Addison, Prue)
Date: Mon, 18 Jul 2005 13:59:58 +1000
Subject: [R] Nested ANOVA with a random nested factor (how to use the lme
	function?)
Message-ID: <C5FB48F42359D54CA441D616F8D29D8504077196@exgcg10.mv.vic.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/48487466/attachment.pl

From gerifalte28 at hotmail.com  Mon Jul 18 07:32:17 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 18 Jul 2005 05:32:17 +0000
Subject: [R] question from environmental statistics
In-Reply-To: <1121475795.42d85cd3f1c97@webmail.scsv.nevada.edu>
Message-ID: <BAY103-F31715AD557A5C5AA6B96DBA6D50@phx.gbl>

Dear Dev

I believe you may need some conceptual guidance in probability theory and 
statistics rather than help with a specific R function to solve your 
analysis.  May I recomend you to visit the statistic consulting office in 
your campus or perhaps review a textbook on applied probability.

Best wishes

Francisco


>From: pantd at unlv.nevada.edu
>To: r-help at stat.math.ethz.ch
>CC: gerifalte28 at hotmail.com
>Subject: [R] question from environmental statistics
>Date: Fri, 15 Jul 2005 18:03:15 -0700
>
>thanks Fran. that was useful but Im still in a fix. its a real life data 
>which
>looks like this:
>0.9
>10.9
>24.0
>6.7
>0.6
>1.0
>2.4
>12.4
>7.9
>15.8
>1.4
>7.9
>11000.0
>
>(benzene conc. taken after WTC attacks)..its just a small chunk of data i 
>pasted
>for you to look at.
>its neither normal nor lognormal. someone told me that qq plot does help in
>determining the distribution. im not sure how to get it.
>
>can someone help me in this.
>
>thanks
>
>
>
>Take a look at this document by Vito Ricci:
>http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
>
>Did you try RSiteSearch("Fit distribution") or a Google search?  That will
>lead you to fit.dist{gnlm} and fitdistr{MASS}
>
>Cheers
>
>Francisco
>
>
> >From: pantd at unlv.nevada.edu
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] question from environmental statistics
> >Date: Thu, 14 Jul 2005 14:06:45 -0700
> >
> >
> >
> >Dear R users
> >I want to knw if there is a way in which a raw dataset can be modelled by
> >some
> >distribution. besides the gof test is there any test involving gamma or
> >lognormal that would fit the data.
> >
> >thank you
> >
> >-dev



From szlevine at nana.co.il  Mon Jul 18 08:46:52 2005
From: szlevine at nana.co.il (Stephen)
Date: Mon, 18 Jul 2005 08:46:52 +0200
Subject: [R] Survival dummy variables and some questions
Message-ID: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/88345b82/attachment.pl

From dieter.menne at menne-biomed.de  Mon Jul 18 08:42:40 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 18 Jul 2005 06:42:40 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Is_it_possible_to_coerce_R_to_continue_proceeding_t?=
	=?utf-8?q?he_next=09command_in_a_loop_after_an_error_message_=3F?=
References: <001e01c58a67$4c369fc0$97a46f9c@brianstat>
Message-ID: <loom.20050718T084039-417@post.gmane.org>

Yimeng Lu <yl2058 <at> columbia.edu> writes:

> In a loop, if a function, such as "nls", gives an error, is it possible to
> coerce R to continue proceeding the next command with the same
> loop?

You may also have a look at nlsList in package(nlme). In handles errors returns 
a nice list of result.

Dieter



From blomsp at ozemail.com.au  Mon Jul 18 08:50:22 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 18 Jul 2005 16:50:22 +1000
Subject: [R] Nested ANOVA with a random nested factor (how to use the
 lme function?)
In-Reply-To: <C5FB48F42359D54CA441D616F8D29D8504077196@exgcg10.mv.vic.go v.au>
References: <C5FB48F42359D54CA441D616F8D29D8504077196@exgcg10.mv.vic.gov.au>
Message-ID: <6.2.1.2.0.20050718150748.01bd4f48@mail.ozemail.com.au>

At 01:59 PM 18/07/2005, Addison, Prue wrote:
>Hi,
>
>I am having trouble using the lme function to perform a nested ANOVA
>with a random nested factor.
>
>My design is as follows:

>Location (n=6) (Random)
>
>Site nested within each Location (n=12) (2 Sites nested within each
>Location) (Random)
>
>Dependent variable: sp (species abundance)
>
>By using the aov function I can generate a nested ANOVA, however this
>assumes that my nested factor is fixed.
> > summary(aov(sp~Location/Site, data=mavric))
>
>Df Sum Sq Mean Sq F value Pr(>F)
>
>Location           4 112366   28092  1.2742 0.2962
>
>Location:Transect  5 121690   24338  1.1039 0.3736
>
>Residuals         40 881875   22047

Yes, this is equivalent to aov(sp ~ Location + Location:Site,...)

>I have tried the following lme function to specify that Site is random:
>
> > lme1 <- lme(sp~Location, random=~1|Site, data=mavric)

Here, Location is fixed, and Site is a grouping factor. There are fixed and 
random components to the intercept.


> > lme2 <- lme(sp~Location, random=~1|Location/Site, data=mavric)

Here you have Location as a fixed effect, the intercept is random and the 
grouping is Site %in % Location.

> > anova(lme1)
>
>             numDF denDF  F-value p-value
>
>(Intercept)     1    40 3.418077  0.0719
>
>Location        4     5 1.152505  0.4294

How can you have 6 sites, but only 4 df for Location (should be 5?)


>This gives me the correct F-value for Location from
>MSLocation/MSLocation:Transect, but the p-value doesn't seem to be
>correct (by my calculations in Microsoft Excel it should be 0.345)

I don't know your data, or your calculations. Microsoft Excel does not fill 
me with confidence.

> > anova(lme2)
>
>Warning in pf(q, df1, df2, lower.tail, log.p) :
>
>          NaNs produced
>
>Warning: NAs introduced by coercion
>
>             numDF denDF  F-value p-value
>
>(Intercept)     1    40 0.459966  0.5015
>
>Location        4     0 0.155091     NaN
>
>? I don't know what this output means

Division by zero, somewhere? :-)

> > anova(lme1,lme2)
>
>      Model df      AIC      BIC    logLik   Test      L.Ratio p-value
>
>lme1     1  7 603.7534 616.4000 -294.8767
>
>lme2     2  8 605.7534 620.2067 -294.8767 1 vs 2 1.815674e-05  0.9966
>
>
>? I also don't know what this output means.

you are testing the difference between the models, using a likelihood ratio 
test. The difference is not significant, so the conventional wisdom is to 
choose the simpler model (lme1). Note that the AIC and BIC are lower 
(better) for lme1, too.


>Can anyone tell me if there is a way to use the lme() function in order
>to obtain the same output as the aov() function (above), but so it
>correctly calculates the MS, F and p values for my main Location factor?

The following code shows agreement:

dat <- data.frame(Location=factor(rep(1:6, each=2)), Site=factor(rep(1:2, 
12)), sp=rnorm(12))

fit <- aov(sp ~ Location + Error(Site), data=dat)
fit2 <- lme(sp ~ Location, random=~1|Site, data=dat)

summary(fit)
anova(fit2)

HTH,

Simon.

>Thanks,
>
>
>
>Prue
>
>This e-mail is solely for the named addressee and may be confidential.
>You should only read, disclose, transmit, copy, distribute, act in reliance
>on or commercialise the contents if you are authorised to do so.  If you
>are not the intended recipient of this e-mail, please notify
>postmaster at museum.vic.gov.au by e-mail immediately, or notify the sender
>and then destroy any copy of this message.  Views expressed in this e-mail
>are those of the individual sender, except where specifically stated to be
>those of an officer of Museum Victoria.  Museum Victoria does not represent,
>warrant or guarantee that the integrity of this communication has been
>maintained nor that it is free from errors, virus or interference.
>
>Museum Victoria
>+61 3 8341 7777
>11 Nicholson St
>Carlton
>Victoria
>www.museum.vic.gov.au
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From buser at stat.math.ethz.ch  Mon Jul 18 09:34:03 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 18 Jul 2005 09:34:03 +0200
Subject: [R] Contingency-Coefficient, Variance
In-Reply-To: <200507171709.36922.ferri.leberl@gmx.at>
References: <200507171709.36922.ferri.leberl@gmx.at>
Message-ID: <17115.23403.319608.870579@stat.math.ethz.ch>

Hi

You can easily implement it by:

Mag. Ferri Leberl writes:
 > Dear Everybody!
 > Excuse me for this newbie-questions, but I have not found answers to these 
 > Question:
 > 
 > - Is there a command calculating the variance if the whole group is known 
 > (thus dividing by n instead of n-1)?

var(x)*(n-1)/n

 > 
 > - Which is the command to calculate the
 > - contingency-coefficient?

Have a look at ?chisq.test
There you can calculate and extract the chi-square statistic and
then it is easy to program the needed formula.
Please note that there is an argument "exact" in
chisq.test(). You can set it on FALSE if you do not want to
apply Yates continuity correction.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


 > 
 > Thank you in advance.
 > Mag. Ferri Leberl
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From renukas at gmail.com  Mon Jul 18 11:38:05 2005
From: renukas at gmail.com (Renuka Sane)
Date: Mon, 18 Jul 2005 15:08:05 +0530
Subject: [R] dataframes of unequal size
Message-ID: <f7dc8e940507180238726e595f@mail.gmail.com>

I have two dataframes C and C1. Each has three columns viz. state, psu
and weight. The dataframes are of unequal size i.e. C1 could be
2/25/50 rows and C has 42000 rows.  C1 is the master table i.e.
C1$state, C1$psu and C1$weight are never the same. ThisA. P., Urban, 0
is not so for C.

For example
C
state, psu,weight
A. P., Urban, 0
Mah., Rural, 0
W.B., Rural,0
Ass., Rural,0
M. P., Urban,0
A. P., Urban, 0
...

C1
state, psu, weight
A. P., Urban, 1.3
A. P., Rural, 1.2
M. P., Urban, 0.8
......

For every row of C, I want to check if C$state==C1$state and
C$psu==C1$psu. If it is, I want C$weight <- C1$weight, else C$weight
should be zero.

I am doing the following
for( i in 1:length(C$weight)) {
 C$w[C$state[i]==C1$state & C$psu[i]==C1$psu] <- C1$w[C$state[i] ==
C1$state & C$psu[i] == C1$psu]
}

This gives me the correct replacements for the number of rows in C1
and then just repeats the same weights for the remaning rows in C.

Can someone point out the error in what I am doing or show the correct
way of doing this?

Thanks,
Renuka



From tolga.uzuner at csfb.com  Mon Jul 18 11:45:49 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Mon, 18 Jul 2005 10:45:49 +0100
Subject: [R] rmpi in windows
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433E0C6@elon12p32001.csfp.co.uk>

Hi Uwe,

Thanks for your kind response.

best,
Tolga

Please follow the attached hyperlink to an important disclaimer
http://www.csfb.com/legal_terms/disclaimer_europe.shtml



-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: 16 July 2005 14:29
To: Uzuner, Tolga
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] rmpi in windows


Uzuner, Tolga wrote:
> Hi Folks,
> Has anyone been able to get rmpi to work under windows ?

Rmpi uses the LAM implementation of MPI.
See
http://www.lam-mpi.org/faq/category12.php3
and read FAQ 2 which implicitly tells us that there is no native port, 
hence you cannot run it under Windows.
The package maintainer may know better.

Uwe Ligges


BTW: Why do you ask twice (in private message and on R-help)? I am 
reading R-help anyway ...




> Thanks,
> Tolga
> 
> Please follow the attached hyperlink to an important disclaimer
> <http://www.csfb.com/legal_terms/disclaimer_europe.shtml>
> 
> 
> 
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer: 
> 
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From stefan_hoderlein at yahoo.com  Mon Jul 18 11:54:00 2005
From: stefan_hoderlein at yahoo.com (Stefan Hoderlein)
Date: Mon, 18 Jul 2005 02:54:00 -0700 (PDT)
Subject: [R] Quantile Regression, S-Function "Rreg"
Message-ID: <20050718095400.82390.qmail@web50612.mail.yahoo.com>

I have the following problem: I would like to do a
nonparametric quatile regression. Thus far I have used
the quantreg package and done a local quadratic, but
it does not seem to work well.

Alternatively, I have tried with an older S version I
have the function rreg, and used 

rreg(datax,datay,method=function(u) 
           {(abs(u)+(2*alpha-1)*u)},iter=100)

which gave me pretty acceptable results. What I would
like to do now is to have a similar command in R, but
with the functions

rlm  and   lqs

I do not seem to be able to get somewhere. Can anybody
help? 

I found in the archive under 

Message-ID:
<Pine.GSO.4.31.0101311456130.4395-100000 at toucan.stats>

a reply from Brian Ripley on a similar question, but
was not able to download the experimental file from
his website...

Thanks

Stefan



From yvonnick.noel at uhb.fr  Mon Jul 18 13:34:11 2005
From: yvonnick.noel at uhb.fr (NOEL Yvonnick)
Date: Mon, 18 Jul 2005 13:34:11 +0200
Subject: [R] Proportion test in three-chices experiment
In-Reply-To: <mailman.9.1121680802.17124.r-help@stat.math.ethz.ch>
References: <mailman.9.1121680802.17124.r-help@stat.math.ethz.ch>
Message-ID: <42DB93B3.3080006@uhb.fr>

Rafael,

when testing binomial hypotheses with both repeated measures and 
inter-group factors, you should make explicit your model on the 
intra-subject part of the data. You can't do Chi-square comparisons on 
count data that mix independent and dependent measures.

But you can define a Bernoulli logistic model at the individual single 
response level, and define a proper subject factor, and a stimulus-type 
factor, and possibly an item factor (nested within the stimulus-type 
category). This may be viewed as a Rasch model of measurement.

Within this model, coefficient estimates on the subject factor are 
measures of individual ability that can be a posteriori introduced in an 
ANOVA.

In some cases, you can model the intra-subject part of the data as a 
temporal model, assuming for instance that subjects are getting more 
efficient as time goes on.

HTH,

Yvonnick.

From jonathenwu at hotmail.com  Mon Jul 18 14:23:36 2005
From: jonathenwu at hotmail.com (=?gb2312?B?zuIg6rs=?=)
Date: Mon, 18 Jul 2005 12:23:36 +0000
Subject: [R] about 3d surface plot
Message-ID: <BAY20-F926962EDE04D7E8CB878ED0D50@phx.gbl>

Hi 
I have a data format like this:
x coordinates   y coordinates             z value
3.77E+002	  7.13E+002	0,000000000
1.27E+003	  5.52E+002	2,756785261
1.06E+003	  4.76E+002	2,583918174
3.86E+002	  7.15E+002	0,158626133
3.60E+002	  1.77E+002	2,007595908
a pair of x and y corresponds to a z value. Can I use these data to draw a 
3d surface plot, in which surface constists of z values?if can, how?
thanks
Hao Wu



From jhainm at fas.harvard.edu  Mon Jul 18 14:38:31 2005
From: jhainm at fas.harvard.edu (jhainm@fas.harvard.edu)
Date: Mon, 18 Jul 2005 08:38:31 -0400
Subject: [R] column-wise deletion in data-frames
Message-ID: <1121690311.42dba2c745865@webmail.fas.harvard.edu>

Hi,

I have a huge dataframe and like to delete all those variables from it that that
have NAs. The deletion of vars should be done column-wise, and not row-wise as
na.omit would do it, because I have some vars that have NAs for all rows thus
using na.omit I would end up with no obs. Is there a convenient way to do this
R?

To make the question more explicit. Imagine a dataset that looks something like
this (but much bigger)

X1 <- rnorm(1000)
X2 <- c(rep(NA,1000))
X3 <- rnorm(1000)
X4 <- c(rep(NA,499),1,44,rep(NA,499))
X5 <- rnorm(1000)

data <- as.data.frame(cbind(X1,X2,X3,X4,X5))

So only X1, X3 and X5 are vars without any NAs and there are some vars (X2 and
X4 stacked in between that have NAs). Now, how can I extract those former vars
in a new dataset or remove all those latter vars in between that have NAs
(without missing a single row)?

Thank you very much!

Best,
jens



From f.harrell at vanderbilt.edu  Mon Jul 18 14:46:13 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 18 Jul 2005 08:46:13 -0400
Subject: [R] Survival dummy variables and some questions
In-Reply-To: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
Message-ID: <42DBA495.3030505@vanderbilt.edu>

Stephen wrote:
> Hi All,
> 
>  
> 
> I am currently conducting some survival analyses. I would like to
> extract coefficients at each level of the IVs. 
> 
> I read on a previous posting that dummy regression using coxph was not
> possible. 

I'm not sure what that means.

> 
> Therefore I though, hey why not categorize the variables 
> 
> (I realize some folks object to categorization but the paper I am
> replicating appears to have done so ...)

The fact that some people murder doesn't mean we should copy them.  And 
murdering data, though not as serious, should also be avoided.

Make sure that eventbefore is a "pre time zero" measurement.

Frank

> 
> and turn the variables into factors and then try the analysis.
> 
>  
> 
> E.g., 
> 
> Dataset <- read.table("categ.dat", header=TRUE)
> 
> Dataset$eventbefore2c <- factor(Dataset$eventbefore)
> 
> .. other IVs here
> 
> ...
> 
> surv.mod1 <- coxph(Surv(start, stop, event) ~ sex2 + ageonset2c +
> eventbefore2c  + daysbefore2c, data=Dataset)
> 
>  
> 
> Strangely enough, I receive a warning message when the variables are
> treated in this way: X matrix deemed to be singular; variable 11 in:
> coxph(Surv(start, stop, event) ~ sex2 + ageonset2c + eventbefore2c +  
> 
>  
> 
> I don?t receive any warnings just treating the variables in their
> initial continuous format.
> 
> I am currently using version
> 
> platform i386-pc-mingw32
> 
> arch     i386           
> 
> os       mingw32        
> 
> system   i386, mingw32  
> 
> status                  
> 
> major    2              
> 
> minor    1.1            
> 
> year     2005           
> 
> month    06             
> 
> day      20             
> 
> language R     
> 
>  
> 
>  
> 
> Is this approach to dummy variable using coxph erroneous?
> 
>  
> 
> Is there another way to conduct dummy variable regression with coxph?
> 
>  
> 
> Also, if I include frailty (id) does anyone know of a useful way to
> investigate frailty?
> 
>  
> 
> If one were to plot recurrent events does anyone know of a way of
> interpreting them?
> 
>  
> 
> References & code appreciated.
> 
>  
> 
> BTW. not too familiar with R, less so with survival analysis .... but
> well worth the effort.
> 
>  
> 
> Many thanks in advance...
> 
>  
> 
> Regards
> 
>  
> 
> Stephen
> 
> 
> ???? ?"? ???? ????
> http://mail.nana.co.il
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ccleland at optonline.net  Mon Jul 18 14:50:56 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 18 Jul 2005 08:50:56 -0400
Subject: [R] column-wise deletion in data-frames
In-Reply-To: <1121690311.42dba2c745865@webmail.fas.harvard.edu>
References: <1121690311.42dba2c745865@webmail.fas.harvard.edu>
Message-ID: <42DBA5B0.20200@optonline.net>

jhainm at fas.harvard.edu wrote:
> Hi,
> 
> I have a huge dataframe and like to delete all those variables from it that that
> have NAs. The deletion of vars should be done column-wise, and not row-wise as
> na.omit would do it, because I have some vars that have NAs for all rows thus
> using na.omit I would end up with no obs. Is there a convenient way to do this
> R?
> 
> To make the question more explicit. Imagine a dataset that looks something like
> this (but much bigger)
> 
> X1 <- rnorm(1000)
> X2 <- c(rep(NA,1000))
> X3 <- rnorm(1000)
> X4 <- c(rep(NA,499),1,44,rep(NA,499))
> X5 <- rnorm(1000)
> 
> data <- as.data.frame(cbind(X1,X2,X3,X4,X5))
> 
> So only X1, X3 and X5 are vars without any NAs and there are some vars (X2 and
> X4 stacked in between that have NAs). Now, how can I extract those former vars
> in a new dataset or remove all those latter vars in between that have NAs
> (without missing a single row)?
> ...

   Someone else will probably suggest something more elegant, but how 
about this:

newdata <- data[,-which(apply(data, 2, function(x){all(is.na(x))}))]

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From p.dalgaard at biostat.ku.dk  Mon Jul 18 15:33:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jul 2005 15:33:45 +0200
Subject: [R] column-wise deletion in data-frames
In-Reply-To: <42DBA5B0.20200@optonline.net>
References: <1121690311.42dba2c745865@webmail.fas.harvard.edu>
	<42DBA5B0.20200@optonline.net>
Message-ID: <x24qasb5mu.fsf@turmalin.kubism.ku.dk>

Chuck Cleland <ccleland at optonline.net> writes:

> > data <- as.data.frame(cbind(X1,X2,X3,X4,X5))
> > 
> > So only X1, X3 and X5 are vars without any NAs and there are some vars (X2 and
> > X4 stacked in between that have NAs). Now, how can I extract those former vars
> > in a new dataset or remove all those latter vars in between that have NAs
> > (without missing a single row)?
> > ...
> 
>    Someone else will probably suggest something more elegant, but how 
> about this:
> 
> newdata <- data[,-which(apply(data, 2, function(x){all(is.na(x))}))]

(I think that's supposed to be any(), not all(), and which() is
crossing the creek to fetch water.)

This should do it:

data[,apply(!is.na(data),2,all)]

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From reid_huntsinger at merck.com  Mon Jul 18 15:47:26 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 18 Jul 2005 09:47:26 -0400
Subject: [R] about 3d surface plot
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9525@uswpmx00.merck.com>

Have a look at the "rgl" package on CRAN.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
jonathenwu at hotmail.com
Sent: Monday, July 18, 2005 8:24 AM
To: r-help at stat.math.ethz.ch
Subject: [R] about 3d surface plot


Hi 
I have a data format like this:
x coordinates   y coordinates             z value
3.77E+002	  7.13E+002	0,000000000
1.27E+003	  5.52E+002	2,756785261
1.06E+003	  4.76E+002	2,583918174
3.86E+002	  7.15E+002	0,158626133
3.60E+002	  1.77E+002	2,007595908
a pair of x and y corresponds to a z value. Can I use these data to draw a 
3d surface plot, in which surface constists of z values?if can, how?
thanks
Hao Wu



From tuechler at gmx.at  Mon Jul 18 16:25:14 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 18 Jul 2005 16:25:14 +0200
Subject: [R] levels() deletes other attributes
Message-ID: <3.0.6.32.20050718162514.007af930@pop.gmx.net>

Dear All,

it seems to me that levels() deletes other attributes. See the following
example:

## example with levels
f1 <- factor(c('level c','level b','level a','level c'), ordered=TRUE)
attr(f1, 'testattribute') <- 'teststring'
attributes(f1)
levels(f1) <- c('L-A', 'L-B', 'L-C')
attributes(f1)

If I run it, after assigning new levels, the class is only "factor" instead
of "ordered" "factor" and the $testattribute "teststring" is gone.

The same happens to the label() attribute of Hmisc.

## example with levels and label
library(Hmisc)
f1 <- factor(c('level c','level b','level a','level c'), ordered=TRUE)
label(f1) <- 'factor f1'
attr(f1, 'testattribute') <- 'teststring'
attributes(f1)
levels(f1) <- c('L-A', 'L-B', 'L-C')
attributes(f1)

Should I expect this behaviour?

Thanks

Heinz

# R-Version 2.1.0 Patched (2005-05-30)
# Windows98



From uctqmkt at ucl.ac.uk  Mon Jul 18 16:24:08 2005
From: uctqmkt at ucl.ac.uk (Michael Townsley)
Date: Mon, 18 Jul 2005 15:24:08 +0100
Subject: [R] how to change bar colours in plot.stl
Message-ID: <5.2.1.1.0.20050718151752.02265a90@imap-server.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/599f1bf5/attachment.pl

From szlevine at nana.co.il  Mon Jul 18 17:36:44 2005
From: szlevine at nana.co.il (Stephen)
Date: Mon, 18 Jul 2005 17:36:44 +0200
Subject: [R] Survival dummy variables and some questions
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
Message-ID: <000601c58bae$80ea8270$287716ac@IBM4FC21148802>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/0ea16dd0/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jul 18 16:43:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Jul 2005 15:43:38 +0100 (BST)
Subject: [R] column-wise deletion in data-frames
In-Reply-To: <x24qasb5mu.fsf@turmalin.kubism.ku.dk>
References: <1121690311.42dba2c745865@webmail.fas.harvard.edu>
	<42DBA5B0.20200@optonline.net> <x24qasb5mu.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0507181539320.19788@gannet.stats>

On Mon, 18 Jul 2005, Peter Dalgaard wrote:

> Chuck Cleland <ccleland at optonline.net> writes:
>
>>> data <- as.data.frame(cbind(X1,X2,X3,X4,X5))
>>>
>>> So only X1, X3 and X5 are vars without any NAs and there are some vars (X2 and
>>> X4 stacked in between that have NAs). Now, how can I extract those former vars
>>> in a new dataset or remove all those latter vars in between that have NAs
>>> (without missing a single row)?
>>> ...
>>
>>    Someone else will probably suggest something more elegant, but how
>> about this:
>>
>> newdata <- data[,-which(apply(data, 2, function(x){all(is.na(x))}))]
>
> (I think that's supposed to be any(), not all(), and which() is
> crossing the creek to fetch water.)
>
> This should do it:
>
> data[,apply(!is.na(data),2,all)]

If `data' is a data frame, apply will coerce it to a matrix.  I would do
something like

keep <- sapply(data, function(x) all(!is.na(x)))
data[keep]

to use the list-like structure of a data frame and make the fewest 
possible copies.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 18 16:46:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Jul 2005 15:46:44 +0100 (BST)
Subject: [R] Survival dummy variables and some questions
In-Reply-To: <000601c58bae$80ea8270$287716ac@IBM4FC21148802>
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
Message-ID: <Pine.LNX.4.61.0507181544000.19788@gannet.stats>

This is almost unreadable.

On Mon, 18 Jul 2005, Stephen wrote:

> Hi 1. To clarify: There is a posting saying that dummy regression using
> the coxph function is not possible... That posting may be outdated....

Clarification needs

1) an explanation of what you mean by `dummy regression' and

2) a link to the URL of that posting in the archives.

I would normally use RSiteSearch for the latter, but it is temporarily 
unavailable.

> 2. Q. You say 'Make sure that eventbefore is a "pre time zero"
> measurement' please explain: Do you mean that if someone who is not left
> censored and has no eventbefore then their value is zero? So John is in
> my study from 1978 to 1992 and has no events prior to 1980 then John's
> eventbefore is 0 prior to 1980. 3. .... just kidding... Thanks S -----
> Original Message ----- From: "Frank E Harrell Jr" To: "Stephen" Cc:
> Sent: Monday, July 18, 2005 2:46 PM Subject: Re: [R] Survival dummy
> variables and some questions > Stephen wrote: >> Hi All, >> >> I am
> currently conducting some survival analyses. I would like to >> extract
> coefficients at each level of the IVs. I read on a previous >> posting
> that dummy regression using coxph was not >> possible. > > I'm not sure
> what that means. > >> >> Therefore I though, hey why not categorize the
> variables (I realize some >> folks object to categorization but the
> paper I am >> replicating appears to have done so ...) > > The fact that
> some people murder doesn't mean we should copy them. And > murdering
> data, though not as serious, should also be avoided. > > Make sure that
> eventbefore is a "pre time zero" measurement. > > Frank > >> >> and turn
> the variables into factors and then try the analysis. >> >> E.g.,
> Dataset <- read.table("categ.dat", header=TRUE) >> >>
> Dataset$eventbefore2c <- factor(Dataset$eventbefore) >> >> .. other IVs
> here >> >> ... >> >> surv.mod1 <- coxph(Surv(start, stop, event) ~ sex2
> + ageonset2c + >> eventbefore2c + daysbefore2c, data=Dataset) >> >>
> Strangely enough, I receive a warning message when the variables are >>
> treated in this way: X matrix deemed to be singular; variable 11 in: >>
> coxph(Surv(start, stop, event) ~ sex2 + ageonset2c + eventbefore2c + I
>>> don't receive any warnings just treating the variables in their >>
> initial continuous format. >> >> I am currently using version >> >>
> platform i386-pc-mingw32 >> >> arch i386 os mingw32 system i386, mingw32
>>> status major 2 minor 1.1 >> year 2005 month 06 day 20 >> language R
> Is this approach to dummy variable using coxph erroneous? >> >> Is there
> another way to conduct dummy variable regression with coxph? >> >> Also,
> if I include frailty (id) does anyone know of a useful way to >>
> investigate frailty? >> >> If one were to plot recurrent events does
> anyone know of a way of >> interpreting them? >> >> References & code
> appreciated. >> >> BTW. not too familiar with R, less so with survival
> analysis .... but >> well worth the effort. >> >> Many thanks in
> advance... >> >> Regards >> >> Stephen >> >> >> ???? ?"? ???? ???? >>
> http://mail.nana.co.il >> >> [[alternative HTML version deleted]] >> >>
>>>>>
> ------------------------------------------------------------------------
>>>>> ______________________________________________ >>
> R-help at stat.math.ethz.ch mailing list >>
> https://stat.ethz.ch/mailman/listinfo/r-help >> PLEASE do read the
> posting guide! >> http://www.R-project.org/posting-guide.html > > > -- >
> Frank E Harrell Jr Professor and Chair School of Medicine > Department
> of Biostatistics Vanderbilt University >
>
> ???? ?"? ???? ????
> http://mail.nana.co.il
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tuechler at gmx.at  Mon Jul 18 16:53:35 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 18 Jul 2005 16:53:35 +0200
Subject: [R] levels() deletes other attributes
Message-ID: <3.0.6.32.20050718165335.007aade0@pop.gmx.net>

At 09:29 18.07.2005 -0500, Frank E Harrell Jr wrote:
>Heinz Tuechler wrote:
>> Dear All,
>> 
>> it seems to me that levels() deletes other attributes. See the following
>> example:
>> 
>> ## example with levels
>> f1 <- factor(c('level c','level b','level a','level c'), ordered=TRUE)
>> attr(f1, 'testattribute') <- 'teststring'
>> attributes(f1)
>> levels(f1) <- c('L-A', 'L-B', 'L-C')
>> attributes(f1)
>> 
>> If I run it, after assigning new levels, the class is only "factor" instead
>> of "ordered" "factor" and the $testattribute "teststring" is gone.
>> 
>> The same happens to the label() attribute of Hmisc.
>> 
>> ## example with levels and label
>> library(Hmisc)
>> f1 <- factor(c('level c','level b','level a','level c'), ordered=TRUE)
>> label(f1) <- 'factor f1'
>> attr(f1, 'testattribute') <- 'teststring'
>> attributes(f1)
>> levels(f1) <- c('L-A', 'L-B', 'L-C')
>> attributes(f1)
>> 
>> Should I expect this behaviour?
>
>Does the same thing happen when you do
>
>  attr(f1,'levels') <- c('L-A',...)
>
>Frank

No, it does not. With attr(f1,'levels') <- c('L-A', 'L-B', 'L-C') only the
levels are changed, all other attributes remain as before.
Heinz

>> 
>> Thanks
>> 
>> Heinz
>> 
>> # R-Version 2.1.0 Patched (2005-05-30)
>> # Windows98
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>> 
>
>
>-- 
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
>



From ripley at stats.ox.ac.uk  Mon Jul 18 16:54:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Jul 2005 15:54:19 +0100 (BST)
Subject: [R] how to change bar colours in plot.stl
In-Reply-To: <5.2.1.1.0.20050718151752.02265a90@imap-server.ucl.ac.uk>
References: <5.2.1.1.0.20050718151752.02265a90@imap-server.ucl.ac.uk>
Message-ID: <Pine.LNX.4.61.0507181552480.19788@gannet.stats>

On Mon, 18 Jul 2005, Michael Townsley wrote:

> Dear helpeRs,
>
> Is it possible to change the shading colour of the range bars in the plot
> generated by plot.stl?  By default they are grey, but I would prefer them
> white (I am preparing some graphics for a powerpoint presentation so I'm
> inverting all colours).
>
> As far as I can see plot.stl allows you to turn off the range bars, but
> nothing about the shading colour.  I tried to look at the function by typing:
>
> > plot.stl
> Error: Object "plot.stl" not found
>
> but received an error message.

So use

 	getS3method("plot", "stl")

to see it.  "light gray" is hardcoded, so you will need to make a copy, 
edit it, and use the edited copy.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jul 18 16:57:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jul 2005 16:57:18 +0200
Subject: [R] column-wise deletion in data-frames
In-Reply-To: <Pine.LNX.4.61.0507181539320.19788@gannet.stats>
References: <1121690311.42dba2c745865@webmail.fas.harvard.edu>
	<42DBA5B0.20200@optonline.net> <x24qasb5mu.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0507181539320.19788@gannet.stats>
Message-ID: <x2zmsk9n75.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 18 Jul 2005, Peter Dalgaard wrote:
> 
> > Chuck Cleland <ccleland at optonline.net> writes:
> >
> >>> data <- as.data.frame(cbind(X1,X2,X3,X4,X5))
> >>>
> >>> So only X1, X3 and X5 are vars without any NAs and there are some vars (X2 and
> >>> X4 stacked in between that have NAs). Now, how can I extract those former vars
> >>> in a new dataset or remove all those latter vars in between that have NAs
> >>> (without missing a single row)?
> >>> ...
> >>
> >>    Someone else will probably suggest something more elegant, but how
> >> about this:
> >>
> >> newdata <- data[,-which(apply(data, 2, function(x){all(is.na(x))}))]
> >
> > (I think that's supposed to be any(), not all(), and which() is
> > crossing the creek to fetch water.)
> >
> > This should do it:
> >
> > data[,apply(!is.na(data),2,all)]
> 
> If `data' is a data frame, apply will coerce it to a matrix. 

So will is.na()...

> I would do
> something like
> 
> keep <- sapply(data, function(x) all(!is.na(x)))
> data[keep]
> 
> to use the list-like structure of a data frame and make the fewest
> possible copies.

I think the amount of copying is the same, but your version doesn't
need to store the entire is.na(data) at once.

Nitpick: !any(is.na(x)) should be marginally faster than all(!is.na(x)).

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tuechler at gmx.at  Mon Jul 18 17:10:30 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 18 Jul 2005 17:10:30 +0200
Subject: [R] levels() deletes other attributes
In-Reply-To: <3.0.6.32.20050718165335.007aade0@pop.gmx.net>
Message-ID: <3.0.6.32.20050718171030.007aedb0@pop.gmx.net>

At 16:53 18.07.2005 +0200, Heinz Tuechler wrote:
>At 09:29 18.07.2005 -0500, Frank E Harrell Jr wrote:
>>Heinz Tuechler wrote:
>>> Dear All,
>>> 
>>> it seems to me that levels() deletes other attributes. See the following
>>> example:
>>> 
>>> ## example with levels
>>> f1 <- factor(c('level c','level b','level a','level c'), ordered=TRUE)
>>> attr(f1, 'testattribute') <- 'teststring'
>>> attributes(f1)
>>> levels(f1) <- c('L-A', 'L-B', 'L-C')
>>> attributes(f1)
>>> 
>>> If I run it, after assigning new levels, the class is only "factor"
instead
>>> of "ordered" "factor" and the $testattribute "teststring" is gone.
>>> 
>>> The same happens to the label() attribute of Hmisc.
>>> 
>>> ## example with levels and label
>>> library(Hmisc)
>>> f1 <- factor(c('level c','level b','level a','level c'), ordered=TRUE)
>>> label(f1) <- 'factor f1'
>>> attr(f1, 'testattribute') <- 'teststring'
>>> attributes(f1)
>>> levels(f1) <- c('L-A', 'L-B', 'L-C')
>>> attributes(f1)
>>> 
>>> Should I expect this behaviour?
>>
>>Does the same thing happen when you do
>>
>>  attr(f1,'levels') <- c('L-A',...)
>>
>>Frank
>
>No, it does not. With attr(f1,'levels') <- c('L-A', 'L-B', 'L-C') only the
>levels are changed, all other attributes remain as before.
>Heinz
>
I think, I know why attr(f1,'levels') behaves different from levels(f1) <- .
As far as I see, the method of levels for factor does not use attr() but
factor().
Heinz
>>> 
>>> Thanks
>>> 
>>> Heinz
>>> 
>>> # R-Version 2.1.0 Patched (2005-05-30)
>>> # Windows98
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>>> 
>>
>>
>>-- 
>>Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                      Department of Biostatistics   Vanderbilt University
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From gilbert.wu at sabrefund.com  Mon Jul 18 17:11:12 2005
From: gilbert.wu at sabrefund.com (Gilbert Wu)
Date: Mon, 18 Jul 2005 16:11:12 +0100
Subject: [R] colnames
Message-ID: <C7FF4EF92D5A794EA5820C75CFB938F9630354@MAILSERVER.sabrefund.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/ea1ce2e0/attachment.pl

From stefan_hoderlein at yahoo.com  Mon Jul 18 17:31:17 2005
From: stefan_hoderlein at yahoo.com (Stefan Hoderlein)
Date: Mon, 18 Jul 2005 08:31:17 -0700 (PDT)
Subject: [R] Quantile Regression, S-Function "Rreg"
In-Reply-To: <OFEA24ECEA.9B782B01-ON87257042.004DEAF3-87257042.004E670F@usgs.gov>
Message-ID: <20050718153117.63919.qmail@web50601.mail.yahoo.com>

Dear Brian,

thanks for your mail. For other reasons I need a local
polynomial. The nonparametric regression code is very
scetchy, but I have used it as base anyway.

Best

Stefan



From siwulayid at gmail.com  Mon Jul 18 17:34:30 2005
From: siwulayid at gmail.com (Luwis Tapiwa Diya)
Date: Mon, 18 Jul 2005 17:34:30 +0200
Subject: [R] Picking a subset of explanatory variables corresponding to a
	subset or a transformed subsert of the response
In-Reply-To: <ddf1e2bc0505270056171c9709@mail.gmail.com>
References: <ddf1e2bc0505270056171c9709@mail.gmail.com>
Message-ID: <ddf1e2bc05071808347319cf8d@mail.gmail.com>

I have a dataframe in which I have used a cerain procedure to select
the response a subset of the response and obtained a vector y.Now I
need to fit a glm of the response y given x(the covariates) but now I
get an error that the variables are not of equal length.How can I come
up with a way to pick the x values corresponding to only the subset of
the response I am interested in.

Regards,

Luwis



From helprhelp at gmail.com  Mon Jul 18 17:34:51 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 18 Jul 2005 10:34:51 -0500
Subject: [R] read large amount of data
Message-ID: <cdf8178305071808347b8f18e6@mail.gmail.com>

Hi,
I have a dataset with 2194651x135, in which all the numbers are 0,1,2,
and is bar-delimited.

I used the following approach which can handle 100,000 lines:
t<-scan('fv', sep='|', nlines=100000)
t1<-matrix(t, nrow=135, ncol=100000)
t2<-t(t1)
t3<-as.data.frame(t2)

I changed my plan into using stratified sampling with replacement (col
2 is my class variable: 1 or 2). The class distr is like:
awk -F\| '{print $2}' fv | sort | uniq -c
2162792 1
  31859 2

Is it possible to use R to read the whole dataset and do the
stratified sampling? Is it really dependent on my memory size?
Mem:   3111736k total,  1023040k used,  2088696k free,   150160k buffers
Swap:  4008208k total,    19040k used,  3989168k free,   668892k cached


Thanks,

weiwei

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From szlevine at nana.co.il  Mon Jul 18 18:43:05 2005
From: szlevine at nana.co.il (Stephen)
Date: Mon, 18 Jul 2005 18:43:05 +0200
Subject: [R] Survival dummy variables and some questions
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
	<Pine.LNX.4.61.0507181544000.19788@gannet.stats>
Message-ID: <001001c58bb7$c7162340$287716ac@IBM4FC21148802>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/40fb0ae7/attachment.pl

From tlumley at u.washington.edu  Mon Jul 18 17:53:56 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Jul 2005 08:53:56 -0700 (PDT)
Subject: [R] read large amount of data
In-Reply-To: <cdf8178305071808347b8f18e6@mail.gmail.com>
References: <cdf8178305071808347b8f18e6@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0507180845200.24316@homer12.u.washington.edu>

On Mon, 18 Jul 2005, Weiwei Shi wrote:

> Hi,
> I have a dataset with 2194651x135, in which all the numbers are 0,1,2,
> and is bar-delimited.
>
> I used the following approach which can handle 100,000 lines:
> t<-scan('fv', sep='|', nlines=100000)
> t1<-matrix(t, nrow=135, ncol=100000)
> t2<-t(t1)
> t3<-as.data.frame(t2)
>
> I changed my plan into using stratified sampling with replacement (col
> 2 is my class variable: 1 or 2). The class distr is like:
> awk -F\| '{print $2}' fv | sort | uniq -c
> 2162792 1
>  31859 2
>
> Is it possible to use R to read the whole dataset and do the
> stratified sampling? Is it really dependent on my memory size?

You may well not be able to read the whole data set into memory at once: 
it would take a bit more than 2Gb memory even to store it.

You can use readLines to read it in chunks of, say, 10000 lines.

To do stratified sampling I would suggest bernoulli sampling of slightly 
more than you want. Eg if you want 10000 from class 1, keeping each 
elements with probability 10500/2162792 will get you Poisson(10500) 
elements, which will be more than 10000 elements with better than 99.999% 
probability. You can then choose 10000 at random from these. I can't think 
of an approach that it is guaranteed to work in one pass over the data, 
but 99.999% is pretty close.

 	-thomas



From ramasamy at cancer.org.uk  Mon Jul 18 18:03:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 18 Jul 2005 17:03:00 +0100
Subject: [R] colnames
In-Reply-To: <C7FF4EF92D5A794EA5820C75CFB938F9630354@MAILSERVER.sabrefund.com>
References: <C7FF4EF92D5A794EA5820C75CFB938F9630354@MAILSERVER.sabrefund.com>
Message-ID: <1121702580.6080.11.camel@ipc143004.lif.icnet.uk>

This normally happens to me when I read in a table where the rownames
will be appended by an "X". Read help(make.names) for more information. 
Remember that R is primarily a statistical software and thus likes
colnames classes to be characters.

 mat1 <- matrix( 1:12, nc=3, dimnames=list(NULL, c(0,1,2)) )
 mat1
     0 1  2
[1,] 1 5  9
[2,] 2 6 10
[3,] 3 7 11
[4,] 4 8 12
 colnames(mat1)
[1] "0" "1" "2"
 is.character( colnames(mat1) )
[1] TRUE

However I am not able to reproduce what your problem

  mat2 <- matrix( 101:108, nc=2, dimnames=list(NULL, c("A", "B")) )
  mat2
        A   B
 [1,] 101 105
 [2,] 102 106
 [3,] 103 107
 [4,] 104 108

 cbind(mat1, mat2)
      0 1  2   A   B
 [1,] 1 5  9 101 105
 [2,] 2 6 10 102 106
 [3,] 3 7 11 103 107
 [4,] 4 8 12 104 108

I tried other operation such as mat1[ , 1:2] + mat2 and 
mat1[ ,1] <- mat2[ ,2] but it does not add a preceding "X".
Can you give a reproducible example please ?

If you want to get rid of the preceding "X", try

 colnames( mat1 ) <- c("X0", "X1", "X2") 
 colnames( mat1 ) <- gsub("^X", "", colnames(mat1))

Why do want to do this anyway ?

Regards, Adai


On Mon, 2005-07-18 at 16:11 +0100, Gilbert Wu wrote:
> Hi,
>  
> I have a matrix with column names starting with a character in [0-9]. After some matrix operations (e.g. copy to another matrix), R seems to add a character 'X' in front of the column name. Is this a normal default behaviour of R? Why has it got this behaviour? Can it be changed? What would be the side effect?
>  
> Thank you.
>  
> Regards,
>  
> Gilbert
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Jul 18 18:05:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Jul 2005 17:05:17 +0100 (BST)
Subject: [R] column-wise deletion in data-frames
In-Reply-To: <x2zmsk9n75.fsf@turmalin.kubism.ku.dk>
References: <1121690311.42dba2c745865@webmail.fas.harvard.edu>
	<42DBA5B0.20200@optonline.net> <x24qasb5mu.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0507181539320.19788@gannet.stats>
	<x2zmsk9n75.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0507181659520.21657@gannet.stats>

On Mon, 18 Jul 2005, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> On Mon, 18 Jul 2005, Peter Dalgaard wrote:
>>
>>> Chuck Cleland <ccleland at optonline.net> writes:
>>>
>>>>> data <- as.data.frame(cbind(X1,X2,X3,X4,X5))
>>>>>
>>>>> So only X1, X3 and X5 are vars without any NAs and there are some vars (X2 and
>>>>> X4 stacked in between that have NAs). Now, how can I extract those former vars
>>>>> in a new dataset or remove all those latter vars in between that have NAs
>>>>> (without missing a single row)?
>>>>> ...
>>>>
>>>>    Someone else will probably suggest something more elegant, but how
>>>> about this:
>>>>
>>>> newdata <- data[,-which(apply(data, 2, function(x){all(is.na(x))}))]
>>>
>>> (I think that's supposed to be any(), not all(), and which() is
>>> crossing the creek to fetch water.)
>>>
>>> This should do it:
>>>
>>> data[,apply(!is.na(data),2,all)]
>>
>> If `data' is a data frame, apply will coerce it to a matrix.
>
> So will is.na()...

Not quite.  is.na on a data frame will create a matrix by cbind-ing 
columns.   I was mainly commenting on Chuck Cleland's version, which 
coerces a data frame to a matrix then pulls out each column of the matrix, 
something that is quite wasteful of space.  Forming the logical matrix 
is.na(data) is also I think wasteful.

>> I would do
>> something like
>>
>> keep <- sapply(data, function(x) all(!is.na(x)))
>> data[keep]
>>
>> to use the list-like structure of a data frame and make the fewest
>> possible copies.
>
> I think the amount of copying is the same, but your version doesn't
> need to store the entire is.na(data) at once.
>
> Nitpick: !any(is.na(x)) should be marginally faster than all(!is.na(x)).

I doubt it is measurably so.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Mon Jul 18 18:12:12 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 18 Jul 2005 11:12:12 -0500
Subject: [R] Survival dummy variables and some questions
In-Reply-To: <000601c58bae$80ea8270$287716ac@IBM4FC21148802>
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
Message-ID: <42DBD4DC.2010105@vanderbilt.edu>

Stephen wrote:
> Hi 1. To clarify: There is a posting saying that dummy regression using 
> the coxph function is not possible... That posting may be outdated.... 

That does not make sense.

> 2. Q. You say 'Make sure that eventbefore is a "pre time zero" 
> measurement' please explain: Do you mean that if someone who is not left 
> censored and has no eventbefore then their value is zero? So John is in 
> my study from 1978 to 1992 and has no events prior to 1980 then John's 
> eventbefore is 0 prior to 1980. 3. .... just kidding... Thanks S ----- 

I'm not sure you can handle left censoring by constructing something on 
the right hand side of the model.  But if you have a simple historical 
covariate such as "previous history of disease" before the start of 
follow-up time that's ok.

Frank

> Original Message ----- From: "Frank E Harrell Jr" To: "Stephen" Cc: 
> Sent: Monday, July 18, 2005 2:46 PM Subject: Re: [R] Survival dummy 
> variables and some questions > Stephen wrote: >> Hi All, >> >> I am 
> currently conducting some survival analyses. I would like to >> extract 
> coefficients at each level of the IVs. I read on a previous >> posting 
> that dummy regression using coxph was not >> possible. > > I'm not sure 
> what that means. > >> >> Therefore I though, hey why not categorize the 
> variables (I realize some >> folks object to categorization but the 
> paper I am >> replicating appears to have done so ...) > > The fact that 
> some people murder doesn't mean we should copy them. And > murdering 
> data, though not as serious, should also be avoided. > > Make sure that 
> eventbefore is a "pre time zero" measurement. > > Frank > >> >> and turn 
> the variables into factors and then try the analysis. >> >> E.g., 
> Dataset <- read.table("categ.dat", header=TRUE) >> >> 
> Dataset$eventbefore2c <- factor(Dataset$eventbefore) >> >> .. other IVs 
> here >> >> ... >> >> surv.mod1 <- coxph(Surv(start, stop, event) ~ sex2 
> + ageonset2c + >> eventbefore2c + daysbefore2c, data=Dataset) >> >> 
> Strangely enough, I receive a warning message when the variables are >> 
> treated in this way: X matrix deemed to be singular; variable 11 in: >> 
> coxph(Surv(start, stop, event) ~ sex2 + ageonset2c + eventbefore2c + I 
>  >> don't receive any warnings just treating the variables in their >> 
> initial continuous format. >> >> I am currently using version >> >> 
> platform i386-pc-mingw32 >> >> arch i386 os mingw32 system i386, mingw32 
>  >> status major 2 minor 1.1 >> year 2005 month 06 day 20 >> language R 
> Is this approach to dummy variable using coxph erroneous? >> >> Is there 
> another way to conduct dummy variable regression with coxph? >> >> Also, 
> if I include frailty (id) does anyone know of a useful way to >> 
> investigate frailty? >> >> If one were to plot recurrent events does 
> anyone know of a way of >> interpreting them? >> >> References & code 
> appreciated. >> >> BTW. not too familiar with R, less so with survival 
> analysis .... but >> well worth the effort. >> >> Many thanks in 
> advance... >> >> Regards >> >> Stephen >> >> >> ???? ?"? ???? ???? >> 
> http://mail.nana.co.il >> >> [[alternative HTML version deleted]] >> >> 
>  >> >> 
> ------------------------------------------------------------------------ 
>  >> >> ______________________________________________ >> 
> R-help at stat.math.ethz.ch mailing list >> 
> https://stat.ethz.ch/mailman/listinfo/r-help >> PLEASE do read the 
> posting guide! >> http://www.R-project.org/posting-guide.html > > > -- > 
> Frank E Harrell Jr Professor and Chair School of Medicine > Department 
> of Biostatistics Vanderbilt University >
> 
>  "  
> http://mail.nana.co.il


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From f.harrell at vanderbilt.edu  Mon Jul 18 18:13:50 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 18 Jul 2005 11:13:50 -0500
Subject: [R] Survival dummy variables and some questions
In-Reply-To: <001001c58bb7$c7162340$287716ac@IBM4FC21148802>
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
	<Pine.LNX.4.61.0507181544000.19788@gannet.stats>
	<001001c58bb7$c7162340$287716ac@IBM4FC21148802>
Message-ID: <42DBD53E.7010100@vanderbilt.edu>

Stephen wrote:
> Hi 1. Right perhaps this should clarify. I would like to extract 
> coefficeints for different levels of the IVs (covariate). So for 
> instance, age of onset I would want Hazards etc for every 5 years and so 
> on... The approach I took was to categorize the variables (e.g., age of 
> onset) and then turn the resultant categorical variable into a factor as 
> opposed to a variable... that is when the problems began.... An 
> alternative approach to pulling out different values at different levels 
> of the variable is what I seek. 2. I looked for the link, but can't 

Your needs don't require categorization.  You can request predicted 
values at any sequence of ages.  If you want hazard ratios you can take 
differences in predicted log hazards and antilog them.

Frank


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Mon Jul 18 18:34:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Jul 2005 17:34:50 +0100 (BST)
Subject: [R] read large amount of data
In-Reply-To: <Pine.A41.4.61b.0507180845200.24316@homer12.u.washington.edu>
References: <cdf8178305071808347b8f18e6@mail.gmail.com>
	<Pine.A41.4.61b.0507180845200.24316@homer12.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0507181730430.21876@gannet.stats>

On Mon, 18 Jul 2005, Thomas Lumley wrote:

> On Mon, 18 Jul 2005, Weiwei Shi wrote:
>
>> Hi,
>> I have a dataset with 2194651x135, in which all the numbers are 0,1,2,
>> and is bar-delimited.
>>
>> I used the following approach which can handle 100,000 lines:
>> t<-scan('fv', sep='|', nlines=100000)
>> t1<-matrix(t, nrow=135, ncol=100000)
>> t2<-t(t1)
>> t3<-as.data.frame(t2)
>>
>> I changed my plan into using stratified sampling with replacement (col
>> 2 is my class variable: 1 or 2). The class distr is like:
>> awk -F\| '{print $2}' fv | sort | uniq -c
>> 2162792 1
>>  31859 2
>>
>> Is it possible to use R to read the whole dataset and do the
>> stratified sampling? Is it really dependent on my memory size?
>
> You may well not be able to read the whole data set into memory at once:
> it would take a bit more than 2Gb memory even to store it.

About 1.2G if stored as an integer (not double) vector.

> You can use readLines to read it in chunks of, say, 10000 lines.
>
> To do stratified sampling I would suggest bernoulli sampling of slightly
> more than you want. Eg if you want 10000 from class 1, keeping each
> elements with probability 10500/2162792 will get you Poisson(10500)
> elements, which will be more than 10000 elements with better than 99.999%
> probability. You can then choose 10000 at random from these. I can't think
> of an approach that it is guaranteed to work in one pass over the data,
> but 99.999% is pretty close.

Reservoir sampling methods will work in one pass.  See e.g. my 1987 book 
on Stochastic Simulation.  But Thomas' idea will be easier to implement in 
R, and I would have chosen 20000 not 10500 and be sure I would get enough.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From caobg at email.uc.edu  Mon Jul 18 21:02:05 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Mon, 18 Jul 2005 15:02:05 -0400
Subject: [R] how to get dissimilarity matrix
Message-ID: <200507181902.COQ02358@mirapoint.uc.edu>

Hello All,
   I'm learning R. Just wonder, any package or function that I can use to get the dissimilarity matrix? Thanks.

Best regards, 
  Baoqiang Cao



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Jul 18 22:00:43 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 18 Jul 2005 16:00:43 -0400
Subject: [R] New functions supporting GIF file format in R
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40E3@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050718/418615b0/attachment.pl

From friedm69 at msu.edu  Mon Jul 18 22:11:18 2005
From: friedm69 at msu.edu (Steven K Friedman)
Date: Mon, 18 Jul 2005 16:11:18 -0400
Subject: [R] Package Vegan: species accumlumation functions
Message-ID: <E1DubxP-0004qD-O5@sys31.mail.msu.edu>


Hi everyone, 

I am working with a data frame consisting of 1009 sampling locations, 138 
species incidence and abundance data, and eight forest community types. 

My goal is to develop species acumulation curves and extrapolated estimates 
for each community type. 

I am using the following approach:
attach(forest_plots)
library(vegan)
# calcuate species abundance and species incidence (presence/absence)per 
plot
# where Point_ID = plot sample unit. 

sp.abund <- table(forest_plots$Latin_Name, forest_plots$Point_ID)
sp.incid <- matrix(ifelse(sp.abund > 0, 1, sp.abund)) 

## now quantify richness using community type as "pool" 

richness.pool <- specpool(sp.incid, type) 

> richness.pool
richness.pool
  Species     Chao  Chao.SE   Jack.1 Jack1.SE   Jack.2     Boot  Boot.SE   n
5       NA       NA       NA       NA       NA       NA       NA       NA  
52
8      546 718.3939 28.70646 787.3462 139.0467 858.6446 662.9829 97.92155  
26
9       NA       NA       NA       NA       NA       NA       NA       NA  
93
10      NA       NA       NA       NA       NA       NA       NA       NA 
126
11      NA       NA       NA       NA       NA       NA       NA       NA  
36
13      NA       NA       NA       NA       NA       NA       NA       NA 
122
15      NA       NA       NA       NA       NA       NA       NA       NA  
36
18     364 687.1809 55.49003 591.5556 135.2185 722.6111 463.3126 83.16079   
9 

Ok, I do not understand this output.  Why is NA reported for all community
types other than type 8 and 18? 

Thanks for helping.
Steve F.



From epurdom at stanford.edu  Mon Jul 18 23:17:46 2005
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Mon, 18 Jul 2005 14:17:46 -0700
Subject: [R] listing datasets from all my packages
Message-ID: <6.1.2.0.2.20050718141034.04800df0@epurdom.pobox.stanford.edu>

Hi,
I am using R 2.1.0 on Windows XP and when I type data() to list the 
datasets in R, there is a helpful hint to type 'data(package = 
.packages(all.available = TRUE))' to see the datasets in all of the 
packages -- not just the active ones.

However, when I do this, I get the following message:
 > data(package = .packages(all.available = TRUE))
Error in rbind(...) : number of columns of matrices must match (see arg 2)
In addition: Warning messages:
1: datasets have been moved from package 'base' to package 'datasets' in: 
data(package = .packages(all.available = TRUE))
2: datasets have been moved from package 'stats' to package 'datasets' in: 
data(package = .packages(all.available = TRUE))

I possibly have old libraries in my R libraries because I copy them forward 
and update them with new versions of R, rather than redownload them. Is 
there a way to fix this or do the same another way? (I saw something in 
archives about a problem similar to this with .packages(), but I got the 
impression it was fixed for 2.1.0)

Thanks,
Elizabeth



From jobrien at ucdavis.edu  Mon Jul 18 23:46:46 2005
From: jobrien at ucdavis.edu (Obrien, Josh)
Date: Mon, 18 Jul 2005 14:46:46 -0700
Subject: [R] definition of index.array and boot.return in the code for boot
Message-ID: <5E498E35EFA5A844A18972DAD3289B757530D9@mothra.des.ucdavis.edu>

Dear R friends,

I am reading the code for the function boot in package:boot in an attempt to learn how and where it implements the random resampling used by the non-parametric bootstraps.

The code contains two (apparent) functions - 'index.array'  and  'boot.return' - for which I can find no documentation, and which don't even seem to exist anywhere on the search path.  What are they?

Also, if the meanings of those two don't answer my larger question, could you point me to the code that implements the random resampling?

Thanks very much for your help,

Josh O'Brien
UC Davis



From brgordon at gmail.com  Mon Jul 18 23:53:42 2005
From: brgordon at gmail.com (Brett Gordon)
Date: Mon, 18 Jul 2005 17:53:42 -0400
Subject: [R] Time Series Count Models
In-Reply-To: <42D9B0E1.2080602@pdf.com>
References: <acc5477a05071617125f1447b8@mail.gmail.com>
	<42D9B0E1.2080602@pdf.com>
Message-ID: <acc5477a050718145321da6daf@mail.gmail.com>

Thanks for the suggestion. Is such a model appropriate for count data?
The library you reference seems to just be form standard regressions
(ie those with continuous dependent variables).

Thanks,
Brett

On 7/16/05, Spencer Graves <spencer.graves at pdf.com> wrote:
>           Have you considered "lme" in library(nlme)?  If you want to go this
> route, I recommend Pinheiro and Bates (2000) Mixed-Effect Models in S
> and S-Plus (Springer).
> 
>           spencer graves
> 
> Brett Gordon wrote:
> 
> > Hello,
> >
> > I'm trying to model the entry of certain firms into a larger number of
> > distinct markets over time. I have a short time series, but a large
> > cross section (small T, big N).
> >
> > I have both time varying and non-time varying variables. Additionally,
> > since I'm modeling entry of firms, it seems like the number of
> > existing firms in the market at time t should depend on the number of
> > firms at (t-1), so I would like to include the lagged cumulative count.
> >
> > My basic question is whether it is appropriate (in a statistical
> > sense) to include both the time varying variables and the lagged
> > cumulative count variable. The lagged count aside, I know there are
> > standard extensions to count models to handle time series. However,
> > I'm not sure if anything changes when lagged values of the cumulative
> > dependent variable are added (i.e. are the regular standard errors
> > correct, are estimates consistent, etc....).
> >
> > Can I still use one of the time series count models while including
> > this lagged cumulative value?
> >
> > I would greatly appreciate it if anyone can direct me to relevant
> > material on this. As a note, I have already looked at Cameron and
> > Trivedi's book.
> >
> > Many thanks,
> >
> > Brett
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>



From spencer.graves at pdf.com  Tue Jul 19 00:10:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 18 Jul 2005 15:10:13 -0700
Subject: [R] definition of index.array and boot.return in the code for
 boot
In-Reply-To: <5E498E35EFA5A844A18972DAD3289B757530D9@mothra.des.ucdavis.edu>
References: <5E498E35EFA5A844A18972DAD3289B757530D9@mothra.des.ucdavis.edu>
Message-ID: <42DC28C5.9070208@pdf.com>

	  Excellent question.  Try 'getAnywhere("index.array")'.  It's hidden 
in "namespace:boot".  Ditto for "boot.return".

	  spencer graves

Obrien, Josh wrote:

> Dear R friends,
> 
> I am reading the code for the function boot in package:boot in an attempt to learn how and where it implements the random resampling used by the non-parametric bootstraps.
> 
> The code contains two (apparent) functions - 'index.array'  and  'boot.return' - for which I can find no documentation, and which don't even seem to exist anywhere on the search path.  What are they?
> 
> Also, if the meanings of those two don't answer my larger question, could you point me to the code that implements the random resampling?
> 
> Thanks very much for your help,
> 
> Josh O'Brien
> UC Davis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From pauljohn at ku.edu  Tue Jul 19 00:48:02 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Mon, 18 Jul 2005 17:48:02 -0500
Subject: [R] Time Series Count Models
In-Reply-To: <acc5477a05071617125f1447b8@mail.gmail.com>
References: <acc5477a05071617125f1447b8@mail.gmail.com>
Message-ID: <42DC31A2.1050205@ku.edu>

Dear Brett:

There are books for this topic that are more narrowly tailored to your 
question. Lindsey's Models for Repeated Measurements and Diggle, et al's 
Analysis of Longitudinal Data.  Lindsey offers an R package on his web 
site. If you dig around, you will find many modeling papers on this, 
although in my mind none coalesced into a completely clear path such as 
"throw in these variables and you will get the right estimates".

The problem, as you will see, is that there are many possible 
mathematical descriptions of the idea that there is time dependence in a 
count model.

My political science colleagues John Williams and Pat Brandt published 2 
articles on time series with counts.  My favorite is the second one 
here.  There is R code for the Pests model. 
http://www.utdallas.edu/~pbrandt/pests/pests.htm

Brandt, Patrick T., John T. Williams, Benjamin O. Fordham and Brian 
Pollins. 2000. "Dynamic Modelling For Persistent Event Count Time 
Series." American Journal of Political Science 44(4): 823-843.

Brandt, Patrick T. and John T. Williams. 2001. "A Linear Poisson 
Autoregressive Model: the Poisson AR(p) Model." Political Analysis 9(2): 
164-184.

I worked really hard on TS counts a while ago because a student was 
trying that.  If you look at J Lindsay's book Models for Repeated 
Measures you will make some progress on understanding his method 
kalcount. That's in the repeated library you get from his web site.

Here are the notes I made a couple of years ago

http://lark.cc.ku.edu/~pauljohn/stats/TimeSeries/

Look for files called TSCountData*.pdf.


It all boils down to the fact that you can't just act like it is an OLS 
model and throw Y_t-1 or something like that on the right had side. 
Instead, you have to think in a more delicate way about the process you 
are modeling and hit it from that other direction.

Here are some of the articles for which I kept copies.

U. Bokenholt, "Mixed INAR(1) Poisson regression models" Journal of 
Econometrics, 89 (1999): 317-338

A.C. Harvey and C. Fernandes, "Time Series Models for Count or 
Qualitative Observations, " Journal of Business & Economic Statistics, 4 
(1989): 407-


I recall liking this one a lot

J E Kelsall and Scott Zeger and J M Samet "Frequency Domain Log-linear 
Models; air pollution and mortality" Appl. Statis 48 1999 331-344.

Good luck, let me know what you find out.

pj

Brett Gordon wrote:
> Hello,
> 
> I'm trying to model the entry of certain firms into a larger number of
> distinct markets over time. I have a short time series, but a large
> cross section (small T, big N).
> 
> I have both time varying and non-time varying variables. Additionally,
> since I'm modeling entry of firms, it seems like the number of
> existing firms in the market at time t should depend on the number of
> firms at (t-1), so I would like to include the lagged cumulative count.
> 
> My basic question is whether it is appropriate (in a statistical
> sense) to include both the time varying variables and the lagged
> cumulative count variable. The lagged count aside, I know there are
> standard extensions to count models to handle time series. However,
> I'm not sure if anything changes when lagged values of the cumulative
> dependent variable are added (i.e. are the regular standard errors
> correct, are estimates consistent, etc....).
> 
> Can I still use one of the time series count models while including
> this lagged cumulative value?
> 
> I would greatly appreciate it if anyone can direct me to relevant
> material on this. As a note, I have already looked at Cameron and
> Trivedi's book.
> 
> Many thanks,
> 
> Brett
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From spencer.graves at pdf.com  Tue Jul 19 01:19:07 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 18 Jul 2005 16:19:07 -0700
Subject: [R] Time Series Count Models
In-Reply-To: <acc5477a050718145321da6daf@mail.gmail.com>
References: <acc5477a05071617125f1447b8@mail.gmail.com>	<42D9B0E1.2080602@pdf.com>
	<acc5477a050718145321da6daf@mail.gmail.com>
Message-ID: <42DC38EB.5050605@pdf.com>

	  We are leveraging too far on speculation, at least from what I can 
see.  PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  In particular, try the 
simplest example you can find that illustrates your question, and 
explain your concerns to us in terms of a short series of R commands and 
the resulting output.

	  With counts, especially if there were only a few zeros, I'd start by 
taking logarithms (after replacing 0's by something like 0.5 or by 
adding something like 0.5 to avoid sending 0's to (-Inf)) and use "lme", 
if that seemed appropriate.  Then if I got drastically different answers 
from other software, I would suspect a problem.

	  Other possibilities for count data are the following:

	  * "lmer" library(lme4) [see Douglas Bates. Fitting linear mixed 
models in R. R News, 5(1):27-30, May 2005, www.r-project.org -> 
Newsletter -> "Volume 5/1, May 2005: PDF".

	  * "glmmPQL" in library(MASS).

	  * "glmmML" in library(glmmML)

	  However, I don't know if any of these as the capability now to handle 
short time series like you described.

	  You might also consider the IEKS package by Bjarke Mirner Klein 
(http://www.stat.sdu.dk/publications/monographs/m001/KleinPhdThesis.pdf and
http://genetics.agrsci.dk/~bmk/IEKS.R).

	  spencer graves

Brett Gordon wrote:

> Thanks for the suggestion. Is such a model appropriate for count data?
> The library you reference seems to just be form standard regressions
> (ie those with continuous dependent variables).
> 
> Thanks,
> Brett
> 
> On 7/16/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>          Have you considered "lme" in library(nlme)?  If you want to go this
>>route, I recommend Pinheiro and Bates (2000) Mixed-Effect Models in S
>>and S-Plus (Springer).
>>
>>          spencer graves
>>
>>Brett Gordon wrote:
>>
>>
>>>Hello,
>>>
>>>I'm trying to model the entry of certain firms into a larger number of
>>>distinct markets over time. I have a short time series, but a large
>>>cross section (small T, big N).
>>>
>>>I have both time varying and non-time varying variables. Additionally,
>>>since I'm modeling entry of firms, it seems like the number of
>>>existing firms in the market at time t should depend on the number of
>>>firms at (t-1), so I would like to include the lagged cumulative count.
>>>
>>>My basic question is whether it is appropriate (in a statistical
>>>sense) to include both the time varying variables and the lagged
>>>cumulative count variable. The lagged count aside, I know there are
>>>standard extensions to count models to handle time series. However,
>>>I'm not sure if anything changes when lagged values of the cumulative
>>>dependent variable are added (i.e. are the regular standard errors
>>>correct, are estimates consistent, etc....).
>>>
>>>Can I still use one of the time series count models while including
>>>this lagged cumulative value?
>>>
>>>I would greatly appreciate it if anyone can direct me to relevant
>>>material on this. As a note, I have already looked at Cameron and
>>>Trivedi's book.
>>>
>>>Many thanks,
>>>
>>>Brett
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>--
>>Spencer Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com>
>>Tel:  408-938-4420
>>Fax: 408-280-7915
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From brgordon at gmail.com  Tue Jul 19 01:40:08 2005
From: brgordon at gmail.com (Brett Gordon)
Date: Mon, 18 Jul 2005 19:40:08 -0400
Subject: [R] Time Series Count Models
In-Reply-To: <42DC31A2.1050205@ku.edu>
References: <acc5477a05071617125f1447b8@mail.gmail.com>
	<42DC31A2.1050205@ku.edu>
Message-ID: <acc5477a05071816404f54b66b@mail.gmail.com>

Paul,

Thank you so much for your thoughtful reply. I agree - there are many
possible descriptions for my data, and I realize that I don't want to
get bogged down with figuring out the 'best' model if something simple
will work well. For me, I think the difficulty is going to be handling
the cumulative aspect of the lagged variable.

To be clear, suppose that y_1, ..., y_T are the counts. At time t=3, I
want to include the quantity (y_1 + y_2) as an independent variable,
and so on. I wonder if this is as simple as solving a conditional ML
problem......I'll have to look more deeply into it.

Again, thanks for the references.

-Brett 

On 7/18/05, Paul Johnson <pauljohn at ku.edu> wrote:
> Dear Brett:
> 
> There are books for this topic that are more narrowly tailored to your
> question. Lindsey's Models for Repeated Measurements and Diggle, et al's
> Analysis of Longitudinal Data.  Lindsey offers an R package on his web
> site. If you dig around, you will find many modeling papers on this,
> although in my mind none coalesced into a completely clear path such as
> "throw in these variables and you will get the right estimates".
> 
> The problem, as you will see, is that there are many possible
> mathematical descriptions of the idea that there is time dependence in a
> count model.
> 
> My political science colleagues John Williams and Pat Brandt published 2
> articles on time series with counts.  My favorite is the second one
> here.  There is R code for the Pests model.
> http://www.utdallas.edu/~pbrandt/pests/pests.htm
> 
> Brandt, Patrick T., John T. Williams, Benjamin O. Fordham and Brian
> Pollins. 2000. "Dynamic Modelling For Persistent Event Count Time
> Series." American Journal of Political Science 44(4): 823-843.
> 
> Brandt, Patrick T. and John T. Williams. 2001. "A Linear Poisson
> Autoregressive Model: the Poisson AR(p) Model." Political Analysis 9(2):
> 164-184.
> 
> I worked really hard on TS counts a while ago because a student was
> trying that.  If you look at J Lindsay's book Models for Repeated
> Measures you will make some progress on understanding his method
> kalcount. That's in the repeated library you get from his web site.
> 
> Here are the notes I made a couple of years ago
> 
> http://lark.cc.ku.edu/~pauljohn/stats/TimeSeries/
> 
> Look for files called TSCountData*.pdf.
> 
> 
> It all boils down to the fact that you can't just act like it is an OLS
> model and throw Y_t-1 or something like that on the right had side.
> Instead, you have to think in a more delicate way about the process you
> are modeling and hit it from that other direction.
> 
> Here are some of the articles for which I kept copies.
> 
> U. Bokenholt, "Mixed INAR(1) Poisson regression models" Journal of
> Econometrics, 89 (1999): 317-338
> 
> A.C. Harvey and C. Fernandes, "Time Series Models for Count or
> Qualitative Observations, " Journal of Business & Economic Statistics, 4
> (1989): 407-
> 
> 
> I recall liking this one a lot
> 
> J E Kelsall and Scott Zeger and J M Samet "Frequency Domain Log-linear
> Models; air pollution and mortality" Appl. Statis 48 1999 331-344.
> 
> Good luck, let me know what you find out.
> 
> pj
> 
> Brett Gordon wrote:
> > Hello,
> >
> > I'm trying to model the entry of certain firms into a larger number of
> > distinct markets over time. I have a short time series, but a large
> > cross section (small T, big N).
> >
> > I have both time varying and non-time varying variables. Additionally,
> > since I'm modeling entry of firms, it seems like the number of
> > existing firms in the market at time t should depend on the number of
> > firms at (t-1), so I would like to include the lagged cumulative count.
> >
> > My basic question is whether it is appropriate (in a statistical
> > sense) to include both the time varying variables and the lagged
> > cumulative count variable. The lagged count aside, I know there are
> > standard extensions to count models to handle time series. However,
> > I'm not sure if anything changes when lagged values of the cumulative
> > dependent variable are added (i.e. are the regular standard errors
> > correct, are estimates consistent, etc....).
> >
> > Can I still use one of the time series count models while including
> > this lagged cumulative value?
> >
> > I would greatly appreciate it if anyone can direct me to relevant
> > material on this. As a note, I have already looked at Cameron and
> > Trivedi's book.
> >
> > Many thanks,
> >
> > Brett
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> --
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
>



From jyzz88 at gmail.com  Tue Jul 19 02:14:48 2005
From: jyzz88 at gmail.com (Luke)
Date: Mon, 18 Jul 2005 20:14:48 -0400
Subject: [R] svmlight running error
Message-ID: <27583b4005071817146eda84a8@mail.gmail.com>

Dear R Users,

When I used svmlight, I got below error:

my command is:
foo <- svmlight(y~., data= myData)

the results:
Error in file(con, "r") : unable to open connection
In addition: Warning messages:
1: svm_learn not found 
2: cannot open file '_model_1.txt'

> myData[1:2,]
  y X1 X2 X3 X4  X5  X6 X7 X8 X9 X10 X11 X12 X13 X14 X15 X16 X17
1 1 63  1  0  0 145 233  1  1  0 150   0 2.3   1   0   0   1   0
2 0 67  0  1  0 160 286  0  1  0 108   1 1.5   0   1   3   0   1

I wonder what is the possible reason for this error.

-Luke



From gerifalte28 at hotmail.com  Tue Jul 19 04:16:53 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 19 Jul 2005 02:16:53 +0000
Subject: [R] Obtaining argument name within a function
Message-ID: <BAY103-F1775259EC18CD2197ADC35A6D40@phx.gbl>

Dear all

How can I obtain the name of the argument passed in a function?  Here is a 
simplistic example of what I would like to obtain:

myfunction= function(name) {
     print(paste("The parameter name was",unknownFunction(name))
     }

myfunction(myobject)
[1] "The parameter name was myobject"

Thanks

Francisco



From spluque at gmail.com  Tue Jul 19 05:23:08 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Mon, 18 Jul 2005 22:23:08 -0500
Subject: [R] Obtaining argument name within a function
References: <BAY103-F1775259EC18CD2197ADC35A6D40@phx.gbl>
Message-ID: <87r7dveaxv.fsf@gmail.com>

"Francisco J. Zagmutt" <gerifalte28 at hotmail.com> wrote:
> Dear all
>
> How can I obtain the name of the argument passed in a function? Here is a
> simplistic example of what I would like to obtain:
>
> myfunction= function(name) {
> print(paste("The parameter name was",unknownFunction(name))
> }
>
> myfunction(myobject)
> [1] "The parameter name was myobject"


?substitute

myfunction <- function(obj) {
  paste("The parameter name was", deparse(substitute(obj)))
}

myfunction(myobject)

-- 
Sebastian P. Luque



From jsorkin at grecc.umaryland.edu  Tue Jul 19 06:12:23 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 19 Jul 2005 00:12:23 -0400
Subject: [R] Obtaining argument name within a function
Message-ID: <s2dc4577.016@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/de95c4c3/attachment.pl

From r.shengzhe at gmail.com  Tue Jul 19 07:49:05 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 19 Jul 2005 07:49:05 +0200
Subject: [R] help: how to change the column name of data.frame
Message-ID: <ea57975b0507182249796ea618@mail.gmail.com>

Hello,

I have a data frame with 15 variables, and want to exchange the data
of 4th column and 6th column. First I append a column in the data
frame, copy the 4th column data there, then copy the 6th column data
to 4th column, and copy the appended column data to 6th column, but
the names of the 4th and 6th column are still unchanged. How can I
exchange them?

Thank you,
Shengzhe



From szlevine at nana.co.il  Tue Jul 19 08:59:05 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 19 Jul 2005 08:59:05 +0200
Subject: [R] Survival dummy variables and some questions
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
	<Pine.LNX.4.61.0507181544000.19788@gannet.stats>
	<001001c58bb7$c7162340$287716ac@IBM4FC21148802>
	<42DBD53E.7010100@vanderbilt.edu>
Message-ID: <001001c58c2f$5acaf850$287716ac@IBM4FC21148802>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/6bef33c7/attachment.pl

From dieter.menne at menne-biomed.de  Tue Jul 19 08:42:55 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 19 Jul 2005 06:42:55 +0000 (UTC)
Subject: [R] how to solve the step halving factor problems in gnls and
	nls
References: <003601c58b28$9d8a33c0$97a46f9c@brianstat>
Message-ID: <loom.20050719T083854-188@post.gmane.org>

Yimeng Lu <yl2058 <at> columbia.edu> writes:

> Could you give me some advice in 
> solving the problem of such error message from gnls and nls?
> ## begin error message
> 
> "Problem in gnls(y1 ~ glogit4(b, c, m, t, x), data.frame(x..: Step halving 
> factor reduced below minimum in NLS step "
> 

Try to set nlsTol in the optional control parameter (gnlsControl) to a larger 
value, e.g. 0.1 instead of default 0.001, but be sure to check your results 
make sense. If this should help, you can try intermediate values.

Dieter Menne



From maechler at stat.math.ethz.ch  Tue Jul 19 08:51:34 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Jul 2005 08:51:34 +0200
Subject: [R] how to get dissimilarity matrix
In-Reply-To: <200507181902.COQ02358@mirapoint.uc.edu>
References: <200507181902.COQ02358@mirapoint.uc.edu>
Message-ID: <17116.41718.708000.314475@stat.math.ethz.ch>

>>>>> "Baoqiang" == Baoqiang Cao <caobg at email.uc.edu>
>>>>>     on Mon, 18 Jul 2005 15:02:05 -0400 writes:

    Baoqiang> Hello All, I'm learning R. Just wonder, any
    Baoqiang> package or function that I can use to get the
    Baoqiang> dissimilarity matrix? Thanks.

Yes,
learn to use help.search()  {also read the docu :  ?help.search}

help.search("dissimilarity")

and find daisy() in recommended package 'cluster'.
There's also  dist() in 'stats' which is a bit less versatile.



From maechler at stat.math.ethz.ch  Tue Jul 19 08:52:59 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Jul 2005 08:52:59 +0200
Subject: [R] New functions supporting GIF file format in R
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40E3@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40E3@us-arlington-0668.mail.saic.com>
Message-ID: <17116.41803.488407.945189@stat.math.ethz.ch>

>>>>> "JarekT" == Tuszynski, Jaroslaw W <JAROSLAW.W.TUSZYNSKI at saic.com>
>>>>>     on Mon, 18 Jul 2005 16:00:43 -0400 writes:

    JarekT> Hi, A minor announcement. I just added two functions
    JarekT> for reading and writing GIF files to my caTools
    JarekT> package. Input and output is in the form of standard
    JarekT> R matrices or arrays, and standard R color-maps
    JarekT> (palettes). The functions can read and write both
    JarekT> regular GIF images, as well as, multi-frame animated
    JarekT> GIFs. Most of the work is done in C level code
    JarekT> (included), so functions do not use any external
    JarekT> libraries.

 

    JarekT> For more info and examples go to
    JarekT> http://cran.r-project.org/doc/packages/caTools.pdf
    JarekT> <http://cran.r-project.org/doc/packages/caTools.pdf>
    JarekT> and click GIF.

Wouldn't it make sense to donate these to the 'pixmap' package
which is dedicated to such objects and has been in place for a
very long time?

Regards,
Martin



From ripley at stats.ox.ac.uk  Tue Jul 19 09:39:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2005 08:39:45 +0100 (BST)
Subject: [R] listing datasets from all my packages
In-Reply-To: <6.1.2.0.2.20050718141034.04800df0@epurdom.pobox.stanford.edu>
References: <6.1.2.0.2.20050718141034.04800df0@epurdom.pobox.stanford.edu>
Message-ID: <Pine.LNX.4.61.0507190818520.1227@gannet.stats>

I did manage eventually to reproduce this via

data(package="makecdfenv")

which is a BioC package with a non-standard use of the data directory.
(There are no objects in the single R file in the data directory, which 
from its name I suggest is a dummy and the data directory should be 
removed entirely.  The indexing code fails to handle this correctly.)

I had no difficulty on a system that has almost all CRAN packages 
installed.

On Mon, 18 Jul 2005, Elizabeth Purdom wrote:

> Hi,
> I am using R 2.1.0 on Windows XP and when I type data() to list the
> datasets in R, there is a helpful hint to type 'data(package =
> .packages(all.available = TRUE))' to see the datasets in all of the
> packages -- not just the active ones.
>
> However, when I do this, I get the following message:
> > data(package = .packages(all.available = TRUE))
> Error in rbind(...) : number of columns of matrices must match (see arg 2)
> In addition: Warning messages:
> 1: datasets have been moved from package 'base' to package 'datasets' in:
> data(package = .packages(all.available = TRUE))
> 2: datasets have been moved from package 'stats' to package 'datasets' in:
> data(package = .packages(all.available = TRUE))
>
> I possibly have old libraries in my R libraries because I copy them forward
> and update them with new versions of R, rather than redownload them. Is
> there a way to fix this or do the same another way? (I saw something in
> archives about a problem similar to this with .packages(), but I got the
> impression it was fixed for 2.1.0)
>
> Thanks,
> Elizabeth
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul 19 09:45:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2005 08:45:27 +0100 (BST)
Subject: [R] definition of index.array and boot.return in the code for
 boot
In-Reply-To: <42DC28C5.9070208@pdf.com>
References: <5E498E35EFA5A844A18972DAD3289B757530D9@mothra.des.ucdavis.edu>
	<42DC28C5.9070208@pdf.com>
Message-ID: <Pine.LNX.4.61.0507190845000.1227@gannet.stats>

Or read the *sources* for package boot, and find the original code with 
all the comments.

On Mon, 18 Jul 2005, Spencer Graves wrote:

> 	  Excellent question.  Try 'getAnywhere("index.array")'.  It's hidden
> in "namespace:boot".  Ditto for "boot.return".
>
> 	  spencer graves
>
> Obrien, Josh wrote:
>
>> Dear R friends,
>>
>> I am reading the code for the function boot in package:boot in an attempt to learn how and where it implements the random resampling used by the non-parametric bootstraps.
>>
>> The code contains two (apparent) functions - 'index.array'  and  'boot.return' - for which I can find no documentation, and which don't even seem to exist anywhere on the search path.  What are they?
>>
>> Also, if the meanings of those two don't answer my larger question, could you point me to the code that implements the random resampling?
>>
>> Thanks very much for your help,
>>
>> Josh O'Brien
>> UC Davis
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul 19 09:49:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2005 08:49:13 +0100 (BST)
Subject: [R] help: how to change the column name of data.frame
In-Reply-To: <ea57975b0507182249796ea618@mail.gmail.com>
References: <ea57975b0507182249796ea618@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0507190846530.1227@gannet.stats>

Just change the names!  E.g.

names(DF)[c(4,6)] <- names(DF)[c(6,4)]

Strictly a data frame has names, not column names, hence the use the 
names() and names<-() functions here.

On Tue, 19 Jul 2005, wu sz wrote:

> I have a data frame with 15 variables, and want to exchange the data
> of 4th column and 6th column. First I append a column in the data
> frame, copy the 4th column data there, then copy the 6th column data
> to 4th column, and copy the appended column data to 6th column, but
> the names of the 4th and 6th column are still unchanged. How can I
> exchange them?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uctqmkt at ucl.ac.uk  Tue Jul 19 10:06:15 2005
From: uctqmkt at ucl.ac.uk (Michael Townsley)
Date: Tue, 19 Jul 2005 09:06:15 +0100
Subject: [R] how to change bar colours in plot.stl
In-Reply-To: <Pine.LNX.4.61.0507181552480.19788@gannet.stats>
References: <5.2.1.1.0.20050718151752.02265a90@imap-server.ucl.ac.uk>
	<5.2.1.1.0.20050718151752.02265a90@imap-server.ucl.ac.uk>
Message-ID: <5.2.1.1.0.20050719090303.00a14ea8@imap-server.ucl.ac.uk>

My thanks to Achim Zeileis and Prof Ripley for their responses.  In a very 
short time I not only had an answer and solved my problem, but also learned 
something about R that I can employ in other situations.

Much appreciated,

MT



From gilbert.wu at sabrefund.com  Tue Jul 19 10:17:03 2005
From: gilbert.wu at sabrefund.com (Gilbert Wu)
Date: Tue, 19 Jul 2005 09:17:03 +0100
Subject: [R] colnames
Message-ID: <C7FF4EF92D5A794EA5820C75CFB938F9630355@MAILSERVER.sabrefund.com>

Hi Adai,

Many Thanks for the examples.

I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.

The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.

I would welcome any suggestion to improve the script.


Regards,

Gilbert

> "p.RIs2Returns" <-
+ function (RIm)
+ {
+ x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
+ y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
+ RReturns <- (y/x -1)
+ RReturns
+ }
> 
> 
> channel<-odbcConnect("ourSQLDB")
> result<-sqlQuery(channel,paste("select * from equityRIs;"))
> odbcClose(channel)
> result
   stockid    sdate  dbPrice
1   899188 20050713  7.59500
2   899188 20050714  7.60500
3   899188 20050715  7.48000
4   899188 20050718  7.41500
5   902232 20050713 10.97000
6   902232 20050714 10.94000
7   902232 20050715 10.99000
8   902232 20050718 11.05000
9   901714 20050713 17.96999
10  901714 20050714 18.00999
11  901714 20050715 17.64999
12  901714 20050718 17.64000
13  28176U 20050713  5.19250
14  28176U 20050714  5.25000
15  28176U 20050715  5.25000
16  28176U 20050718  5.22500
17  15322M 20050713 11.44000
18  15322M 20050714 11.50000
19  15322M 20050715 11.33000
20  15322M 20050718 11.27000
> r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> r1
     sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
1 20050713          7.595          10.97       17.96999         5.1925          11.44
2 20050714          7.605          10.94       18.00999         5.2500          11.50
3 20050715          7.480          10.99       17.64999         5.2500          11.33
4 20050718          7.415          11.05       17.64000         5.2250          11.27
> #Set sdate as the rownames
> rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> #Get rid of the first column
> r1 <- r1[1:nrow(r1),2:ncol(r1)]
> r1
         dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
20050713          7.595          10.97       17.96999         5.1925          11.44
20050714          7.605          10.94       18.00999         5.2500          11.50
20050715          7.480          10.99       17.64999         5.2500          11.33
20050718          7.415          11.05       17.64000         5.2250          11.27
> colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> r1
         899188 902232   901714 28176U 15322M
20050713  7.595  10.97 17.96999 5.1925  11.44
20050714  7.605  10.94 18.00999 5.2500  11.50
20050715  7.480  10.99 17.64999 5.2500  11.33
20050718  7.415  11.05 17.64000 5.2250  11.27
> RRs<-p.RIs2Returns(r1)
> RRs
              X899188      X902232      X901714      X28176U      X15322M
20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
>



From ripley at stats.ox.ac.uk  Tue Jul 19 10:20:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2005 09:20:55 +0100 (BST)
Subject: [R] help: how to change the column name of data.frame
In-Reply-To: <Pine.LNX.4.61.0507190846530.1227@gannet.stats>
References: <ea57975b0507182249796ea618@mail.gmail.com>
	<Pine.LNX.4.61.0507190846530.1227@gannet.stats>
Message-ID: <Pine.LNX.4.61.0507190915350.3594@gannet.stats>

On Tue, 19 Jul 2005, Prof Brian Ripley wrote:

> Just change the names!  E.g.
>
> names(DF)[c(4,6)] <- names(DF)[c(6,4)]
>
> Strictly a data frame has names, not column names, hence the use the
> names() and names<-() functions here.

That answered the subject line.  If you want to exchange the columns (not 
just the data but also the names) you can just use

DF[c(4,6)] <- DF[c(6,4)]

Please do try to be more precise as to what you want to do, for example by 
giving an example.

> On Tue, 19 Jul 2005, wu sz wrote:
>
>> I have a data frame with 15 variables, and want to exchange the data
>> of 4th column and 6th column. First I append a column in the data
>> frame, copy the 4th column data there, then copy the 6th column data
>> to 4th column, and copy the appended column data to 6th column, but
>> the names of the 4th and 6th column are still unchanged. How can I
>> exchange them?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From u9370004 at cc.kmu.edu.tw  Tue Jul 19 10:47:35 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Tue, 19 Jul 2005 16:47:35 +0800
Subject: [R] Michaelis-menten equation
Message-ID: <20050719084736.M53999@cc.kmu.edu.tw>

Dear R users:
   I encountered difficulties in michaelis-menten equation. I found 
that when I use right model definiens, I got wrong Km vlaue, 
and I got right Km value when i use wrong model definiens. 
The value of Vd and Vmax are correct in these two models. 

#-----right model definiens--------
PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
       conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
mm.model <- function(time, y, parms) { 
       dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]+y[1]) 
       list(dCpdt)}
Dose<-300
modfun <- function(time,Vm,Km,Vd) { 
       out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
              rtol=1e-8,atol=1e-8)
          out[,2] } 
objfun <- function(par) { 
   out <- modfun(PKindex$time,par[1],par[2],par[3]) 
   sum((PKindex$conc-out)^2) } 
fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
print(fit$par)
[1] 10.0390733  0.1341544 34.9891829  #--Km=0.1341544,wrong value--


#-----wrong model definiens--------
#-----Km should not divided by Vd--
PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
       conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
mm.model <- function(time, y, parms) { 
   dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]/parms["Vd"]+y[1]) 
   list(dCpdt)}
Dose<-300
modfun <- function(time,Vm,Km,Vd) { 
out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
            rtol=1e-8,atol=1e-8)
       out[,2] 
} 
objfun <- function(par) { 
    out <- modfun(PKindex$time,par[1],par[2],par[3]) 
    sum((PKindex$conc-out)^2)} 
fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
print(fit$par)
[1] 10.038821  4.690267 34.989239  #--Km=4.690267,right value--

What did I do wrong, and how to fix it?
Any suggestions would be greatly appreciated.
Thanks in advance!!



From p.dalgaard at biostat.ku.dk  Tue Jul 19 11:34:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jul 2005 11:34:43 +0200
Subject: [R] Michaelis-menten equation
In-Reply-To: <20050719084736.M53999@cc.kmu.edu.tw>
References: <20050719084736.M53999@cc.kmu.edu.tw>
Message-ID: <x24qarqguk.fsf@turmalin.kubism.ku.dk>

"Chun-Ying Lee" <u9370004 at cc.kmu.edu.tw> writes:

> Dear R users:
>    I encountered difficulties in michaelis-menten equation. I found 
> that when I use right model definiens, I got wrong Km vlaue, 
> and I got right Km value when i use wrong model definiens. 
> The value of Vd and Vmax are correct in these two models. 

How do you know what the correct value is? Are you sure that the other
values are right?

I'm a bit rusty on MM, but are you sure your "right" model is right?
Try doing a dimensional analysis on the ODE. I kind of suspect that
Vd is entering in the wrong way. Since you're dealing in
concentrations, should it enter at all (except via the conc. at time
0, of course)?

Not knowing the context, I can't be quite sure, but generally, I'd
expect Vm*Km/(Km+y) to be the reaction rate, so that Vm is the maximum
rate, attained when y is zero and Km is the conc. at half-maximum
rate. This doesn't look quit like what you have. 
 
> #-----right model definiens--------
> PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
>        conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
> mm.model <- function(time, y, parms) { 
>        dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]+y[1]) 
>        list(dCpdt)}
> Dose<-300
> modfun <- function(time,Vm,Km,Vd) { 
>        out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
>               rtol=1e-8,atol=1e-8)
>           out[,2] } 
> objfun <- function(par) { 
>    out <- modfun(PKindex$time,par[1],par[2],par[3]) 
>    sum((PKindex$conc-out)^2) } 
> fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
> print(fit$par)
> [1] 10.0390733  0.1341544 34.9891829  #--Km=0.1341544,wrong value--
> 
> 
> #-----wrong model definiens--------
> #-----Km should not divided by Vd--
> PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
>        conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
> mm.model <- function(time, y, parms) { 
>    dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]/parms["Vd"]+y[1]) 
>    list(dCpdt)}
> Dose<-300
> modfun <- function(time,Vm,Km,Vd) { 
> out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
>             rtol=1e-8,atol=1e-8)
>        out[,2] 
> } 
> objfun <- function(par) { 
>     out <- modfun(PKindex$time,par[1],par[2],par[3]) 
>     sum((PKindex$conc-out)^2)} 
> fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
> print(fit$par)
> [1] 10.038821  4.690267 34.989239  #--Km=4.690267,right value--
> 
> What did I do wrong, and how to fix it?
> Any suggestions would be greatly appreciated.
> Thanks in advance!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From r.shengzhe at gmail.com  Tue Jul 19 12:42:14 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 19 Jul 2005 12:42:14 +0200
Subject: [R] help: how to change the column name of data.frame
In-Reply-To: <Pine.LNX.4.61.0507190915350.3594@gannet.stats>
References: <ea57975b0507182249796ea618@mail.gmail.com>
	<Pine.LNX.4.61.0507190846530.1227@gannet.stats>
	<Pine.LNX.4.61.0507190915350.3594@gannet.stats>
Message-ID: <ea57975b050719034213932ff7@mail.gmail.com>

Thanks a lot! But  DF[c(4,6)] <- DF[c(6,4)]   seems to just exchange
the data, not the names. If exchanging the columns(both data and
names) needs two steps?

DF[c(4,6)] <- DF[c(6,4)]
names(DF)[c(4,6)] <- names(DF)[c(6,4)]

Shengzhe



From ripley at stats.ox.ac.uk  Tue Jul 19 12:49:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2005 11:49:35 +0100 (BST)
Subject: [R] help: how to change the column name of data.frame
In-Reply-To: <ea57975b050719034213932ff7@mail.gmail.com>
References: <ea57975b0507182249796ea618@mail.gmail.com> 
	<Pine.LNX.4.61.0507190846530.1227@gannet.stats>
	<Pine.LNX.4.61.0507190915350.3594@gannet.stats>
	<ea57975b050719034213932ff7@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0507191147050.6037@gannet.stats>

On Tue, 19 Jul 2005, wu sz wrote:

> Thanks a lot! But  DF[c(4,6)] <- DF[c(6,4)]   seems to just exchange
> the data, not the names. If exchanging the columns(both data and
> names) needs two steps?
>
> DF[c(4,6)] <- DF[c(6,4)]
> names(DF)[c(4,6)] <- names(DF)[c(6,4)]

Yes, it does.  You did however say in your first message that you wanted 
to exchange the data, despite the subject line.

As I asked before *PLEASE* do try to be precise in what you want to do.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From j.van_den_hoff at fz-rossendorf.de  Tue Jul 19 12:51:36 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Tue, 19 Jul 2005 12:51:36 +0200
Subject: [R] Michaelis-menten equation
In-Reply-To: <20050719084736.M53999@cc.kmu.edu.tw>
References: <20050719084736.M53999@cc.kmu.edu.tw>
Message-ID: <42DCDB38.4050705@fz-rossendorf.de>

Chun-Ying Lee wrote:
> Dear R users:
>    I encountered difficulties in michaelis-menten equation. I found 
> that when I use right model definiens, I got wrong Km vlaue, 
> and I got right Km value when i use wrong model definiens. 
> The value of Vd and Vmax are correct in these two models. 
> 
> #-----right model definiens--------
> PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
>        conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
> mm.model <- function(time, y, parms) { 
>        dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]+y[1]) 
>        list(dCpdt)}
> Dose<-300
> modfun <- function(time,Vm,Km,Vd) { 
>        out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
>               rtol=1e-8,atol=1e-8)
>           out[,2] } 
> objfun <- function(par) { 
>    out <- modfun(PKindex$time,par[1],par[2],par[3]) 
>    sum((PKindex$conc-out)^2) } 
> fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
> print(fit$par)
> [1] 10.0390733  0.1341544 34.9891829  #--Km=0.1341544,wrong value--
> 
> 
> #-----wrong model definiens--------
> #-----Km should not divided by Vd--
> PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
>        conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
> mm.model <- function(time, y, parms) { 
>    dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]/parms["Vd"]+y[1]) 
>    list(dCpdt)}
> Dose<-300
> modfun <- function(time,Vm,Km,Vd) { 
> out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
>             rtol=1e-8,atol=1e-8)
>        out[,2] 
> } 
> objfun <- function(par) { 
>     out <- modfun(PKindex$time,par[1],par[2],par[3]) 
>     sum((PKindex$conc-out)^2)} 
> fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
> print(fit$par)
> [1] 10.038821  4.690267 34.989239  #--Km=4.690267,right value--
> 
> What did I do wrong, and how to fix it?
> Any suggestions would be greatly appreciated.
> Thanks in advance!!
> 
> 
> 

it is not clear to me what you are trying to do:
you seem to have a time-concentration-curve in PKindex and you seem to
set up a derivative of this time dependency according
to some model in dCpdt. AFAIKS this scenario is  not directly related to
the situation described by the Michaelis-Menten-Equation which relates
some "input" concentration with some "product" concentration. If Vm and
Km are meant to be the canonical symbols,
what is Vd, a volume of distribution? it is impossible to see (at least
for me) what exactly you want to achieve.

(and in any case, I would prefer "nls" for a least squares fit instead
of 'optim').

joerg
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Tue Jul 19 13:19:47 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 19 Jul 2005 12:19:47 +0100
Subject: [R] colnames
In-Reply-To: <C7FF4EF92D5A794EA5820C75CFB938F9630355@MAILSERVER.sabrefund.com>
References: <C7FF4EF92D5A794EA5820C75CFB938F9630355@MAILSERVER.sabrefund.com>
Message-ID: <1121771987.5947.31.camel@ipc143004.lif.icnet.uk>

First, your problem could be boiled down to the following example. See
how the colnames of the two outputs vary.

df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
df/df
  X100 X200
1    1    1
2    1    1

m  <- as.matrix( df )   # coerce to matrix class
m/m
  100 200
1   1   1
2   1   1

It appears that whenever R has to create a new dataframe automatically,
it tries to get nice colnames. See help(data.frame). I am not exactly
sure why this behaviour is different when creating a matrix. But I do
not think this is a major problem for most people. If you coerce your
input to matrix, the problem goes away.


Next, note the following points :
 a) "mat[ 1:3, 1:ncol(mat) ]" is equivalent to simply "mat[ 1:3,  ]". 
 b) "mat[ 2:nrow(mat), ]" is equivalent to simply "mat[ -1,  ]"
See help(subset) for more information.

Using the points above, we can simplify your function as 

 p.RIs2Returns <- function (mat){

   mat <- as.matrix(mat)
   x <- mat[ -nrow(mat), ]
   y <- mat[ -1, ]
  
   return( y/x -1 )
 }

If your data contains only numerical data, it is probably good idea to
work with matrices as matrix operations are faster.


Finally, we can shorten your function. You can use the diff (which works
column-wise if input is a matrix) and apply function if you know that 

	y/x  =  exp(log(y/x))  =  exp( log(y) - log(x) )

which could be coded in R as

	exp( diff( log(r1) ) )

and then subtract 1 from above to get your returns.

Regards, Adai



On Tue, 2005-07-19 at 09:17 +0100, Gilbert Wu wrote:
> Hi Adai,
> 
> Many Thanks for the examples.
> 
> I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.
> 
> The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.
> 
> I would welcome any suggestion to improve the script.
> 
> 
> Regards,
> 
> Gilbert
> 
> > "p.RIs2Returns" <-
> + function (RIm)
> + {
> + x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
> + y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
> + RReturns <- (y/x -1)
> + RReturns
> + }
> > 
> > 
> > channel<-odbcConnect("ourSQLDB")
> > result<-sqlQuery(channel,paste("select * from equityRIs;"))
> > odbcClose(channel)
> > result
>    stockid    sdate  dbPrice
> 1   899188 20050713  7.59500
> 2   899188 20050714  7.60500
> 3   899188 20050715  7.48000
> 4   899188 20050718  7.41500
> 5   902232 20050713 10.97000
> 6   902232 20050714 10.94000
> 7   902232 20050715 10.99000
> 8   902232 20050718 11.05000
> 9   901714 20050713 17.96999
> 10  901714 20050714 18.00999
> 11  901714 20050715 17.64999
> 12  901714 20050718 17.64000
> 13  28176U 20050713  5.19250
> 14  28176U 20050714  5.25000
> 15  28176U 20050715  5.25000
> 16  28176U 20050718  5.22500
> 17  15322M 20050713 11.44000
> 18  15322M 20050714 11.50000
> 19  15322M 20050715 11.33000
> 20  15322M 20050718 11.27000
> > r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> > r1
>      sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> 1 20050713          7.595          10.97       17.96999         5.1925          11.44
> 2 20050714          7.605          10.94       18.00999         5.2500          11.50
> 3 20050715          7.480          10.99       17.64999         5.2500          11.33
> 4 20050718          7.415          11.05       17.64000         5.2250          11.27
> > #Set sdate as the rownames
> > rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> > #Get rid of the first column
> > r1 <- r1[1:nrow(r1),2:ncol(r1)]
> > r1
>          dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> 20050713          7.595          10.97       17.96999         5.1925          11.44
> 20050714          7.605          10.94       18.00999         5.2500          11.50
> 20050715          7.480          10.99       17.64999         5.2500          11.33
> 20050718          7.415          11.05       17.64000         5.2250          11.27
> > colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> > r1
>          899188 902232   901714 28176U 15322M
> 20050713  7.595  10.97 17.96999 5.1925  11.44
> 20050714  7.605  10.94 18.00999 5.2500  11.50
> 20050715  7.480  10.99 17.64999 5.2500  11.33
> 20050718  7.415  11.05 17.64000 5.2250  11.27
> > RRs<-p.RIs2Returns(r1)
> > RRs
>               X899188      X902232      X901714      X28176U      X15322M
> 20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
> 20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
> 20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
> > 
>



From megg at gmx.ch  Tue Jul 19 13:22:35 2005
From: megg at gmx.ch (Matthias Eggenberger)
Date: Tue, 19 Jul 2005 13:22:35 +0200 (MEST)
Subject: [R] =?iso-8859-1?q?Predict?=
Message-ID: <28795.1121772155@www68.gmx.net>

When I callculate a linear model, then I can compute via confint the
confidencial intervals. the interval level can be chosen. as result, I get
the parameter of the model according to the interval level. 

On the other hand, I can compute the prediction-values for my model as well
with predict(object, type=c("response") etc.). Here I have also the
possibility to chose a level for the confidential intervals. the output are
the calculatet values for the fit, the lower and upper level. 

the problem now is, that when I calculate the values through the linear
model function with the parameter values I get from confint() an I compare
them with the values I get from predict() these values differ extremely. Why
is that so? Does the command predict() calculate the values through an other
routine? That means the command predict() doesn't use the same parameters to
calculate the prediction-values than the ones given by confint()?

Greetings Matthias

-- 
GMX DSL = Maximale Leistung zum minimalen Preis!



From ramasamy at cancer.org.uk  Tue Jul 19 13:30:03 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 19 Jul 2005 12:30:03 +0100
Subject: [R] Survival dummy variables and some questions
In-Reply-To: <001001c58c2f$5acaf850$287716ac@IBM4FC21148802>
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
	<Pine.LNX.4.61.0507181544000.19788@gannet.stats>
	<001001c58bb7$c7162340$287716ac@IBM4FC21148802>
	<42DBD53E.7010100@vanderbilt.edu>
	<001001c58c2f$5acaf850$287716ac@IBM4FC21148802>
Message-ID: <1121772603.5947.35.camel@ipc143004.lif.icnet.uk>

Stephen,

This has nothing to do with your R but to do with your email settings. 

Are you sure you are sending mails in plain text ? Your email on the
R-help mailing archive (see link below) appears to be unreadable 
https://stat.ethz.ch/pipermail/r-help/2005-July/074210.html

Please try to use plain text. (See http://expita.com/nomime.html).

Regards, Adai



On Tue, 2005-07-19 at 08:59 +0200, Stephen wrote:
> Many thanks I follow you what you say.... You can request predicted
> values at any sequence of ages - I guess there are plenty of postings on
> how to do that .... Regards, Stephen ----- Original Message ----- From:
> "Frank E Harrell Jr" To: "Stephen" Cc: "Prof Brian Ripley" ; Sent:
> Monday, July 18, 2005 6:13 PM Subject: Re: [R] Survival dummy variables
> and some questions > Stephen wrote: >> Hi 1. Right perhaps this should
> clarify. I would like to extract >> coefficeints for different levels of
> the IVs (covariate). So for >> instance, age of onset I would want
> Hazards etc for every 5 years and so >> on... The approach I took was to
> categorize the variables (e.g., age of >> onset) and then turn theu
> resultant categorical variable into a factor as >> opposed to a
> variable... that is when the problems began.... An >> alternative
> approach to pulling out different values at different levels >> of the
> variable is what I seek. 2. I looked for the link, but can't > > Your
> needs don't require categorization. You can request predicted values >
> at any sequence of ages. If you want hazard ratios you can take >
> differences in predicted log hazards and antilog them. > > Frank > > >
> -- > Frank E Harrell Jr Professor and Chair School of Medicine >
> Department of Biostatistics Vanderbilt University > 
> 
> ???? ?"? ???? ????
> http://mail.nana.co.il
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Allan at STATS.uct.ac.za  Tue Jul 19 13:35:26 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 19 Jul 2005 13:35:26 +0200
Subject: [R] R: expression
Message-ID: <42DCE57E.186FD875@STATS.uct.ac.za>

hi all

i am having a problem with the expression/paste command

say we estimate a variable, named PHI

it contains the value of say 2

and we want to display this value as " hat(phi) = PHI" onto a graphic

i.e.	 " hat(phi)=2 "

how does one do this?

i've tried the following:

1.	legend(-5,.3,expression(hat(phi)*"="*PHI))

2.	legend(-5,.3,paste(expression(phi),"=",PHI))

but they do not work.

any help?

/
allan

From ligges at statistik.uni-dortmund.de  Tue Jul 19 13:43:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Jul 2005 13:43:44 +0200
Subject: [R] R: expression
In-Reply-To: <42DCE57E.186FD875@STATS.uct.ac.za>
References: <42DCE57E.186FD875@STATS.uct.ac.za>
Message-ID: <42DCE770.5000105@statistik.uni-dortmund.de>

Clark Allan wrote:

> hi all
> 
> i am having a problem with the expression/paste command
> 
> say we estimate a variable, named PHI
> 
> it contains the value of say 2
> 
> and we want to display this value as " hat(phi) = PHI" onto a graphic
> 
> i.e.	 " hat(phi)=2 "
> 
> how does one do this?
> 
> i've tried the following:
> 
> 1.	legend(-5,.3,expression(hat(phi)*"="*PHI))
> 
> 2.	legend(-5,.3,paste(expression(phi),"=",PHI))


See ?plotmath or the Help Desk Article "Automation of Mathematical 
Annotation in Plots" in R News 2 (3), 32-34.

     legend(-5, .3, substitute(hat(phi) == PHI, list(PHI = PHI)))

Uwe Ligges


> but they do not work.
> 
> any help?
> 
> /
> allan
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Tue Jul 19 13:49:38 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 19 Jul 2005 12:49:38 +0100
Subject: [R] R: expression
In-Reply-To: <42DCE57E.186FD875@STATS.uct.ac.za>
References: <42DCE57E.186FD875@STATS.uct.ac.za>
Message-ID: <1121773778.5947.45.camel@ipc143004.lif.icnet.uk>

Something like this :

 x <- 0.5
 plot( 1:10, main=substitute( hat(Phi) ~ "=" ~ x, list(x=x) ) )

Also see http://tolstoy.newcastle.edu.au/R/help/04/09/3371.html

Regards, Adai



On Tue, 2005-07-19 at 13:35 +0200, Clark Allan wrote:
> hi all
> 
> i am having a problem with the expression/paste command
> 
> say we estimate a variable, named PHI
> 
> it contains the value of say 2
> 
> and we want to display this value as " hat(phi) = PHI" onto a graphic
> 
> i.e.	 " hat(phi)=2 "
> 
> how does one do this?
> 
> i've tried the following:
> 
> 1.	legend(-5,.3,expression(hat(phi)*"="*PHI))
> 
> 2.	legend(-5,.3,paste(expression(phi),"=",PHI))
> 
> but they do not work.
> 
> any help?
> 
> /
> allan
> ______________________________________________ R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jul 19 14:50:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Jul 2005 08:50:18 -0400
Subject: [R] dataframes of unequal size
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAC8@usctmx1106.Merck.com>

Seems like no one had responded to this one yet, so I'll take a stab:

## Generate some bogus data:
set.seed(45)
dat <- cbind(expand.grid(LETTERS[1:2], 1:3), round(runif(6), 2))
names(dat) <- c("state", "psu", "weight")
dat2 <- data.frame(state=sample(c("A", "B"), 100, replace=TRUE),
                   psu=sample(3, 100, replace=TRUE),
                   weight=rep(0, 100))

## The actual work:
split(dat2$weight, interaction(dat2$state, dat2$psu)) <-
    split(dat$weight, interaction(dat$state, dat$psu))

This, I think, will only work correctly if all state/psu combinations in
your "C" are also present in "C1".  If not, you can just augment "C1" to
include them.

HTH,
Andy


> From: Renuka Sane
> 
> I have two dataframes C and C1. Each has three columns viz. state, psu
> and weight. The dataframes are of unequal size i.e. C1 could be
> 2/25/50 rows and C has 42000 rows.  C1 is the master table i.e.
> C1$state, C1$psu and C1$weight are never the same. ThisA. P., Urban, 0
> is not so for C.
> 
> For example
> C
> state, psu,weight
> A. P., Urban, 0
> Mah., Rural, 0
> W.B., Rural,0
> Ass., Rural,0
> M. P., Urban,0
> A. P., Urban, 0
> ...
> 
> C1
> state, psu, weight
> A. P., Urban, 1.3
> A. P., Rural, 1.2
> M. P., Urban, 0.8
> ......
> 
> For every row of C, I want to check if C$state==C1$state and
> C$psu==C1$psu. If it is, I want C$weight <- C1$weight, else C$weight
> should be zero.
> 
> I am doing the following
> for( i in 1:length(C$weight)) {
>  C$w[C$state[i]==C1$state & C$psu[i]==C1$psu] <- C1$w[C$state[i] ==
> C1$state & C$psu[i] == C1$psu]
> }
> 
> This gives me the correct replacements for the number of rows in C1
> and then just repeats the same weights for the remaning rows in C.
> 
> Can someone point out the error in what I am doing or show the correct
> way of doing this?
> 
> Thanks,
> Renuka
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From faceasec at uapar.edu  Tue Jul 19 15:12:48 2005
From: faceasec at uapar.edu (secretario academico FACEA)
Date: Tue, 19 Jul 2005 10:12:48 -0300
Subject: [R] data mining
Message-ID: <42DCFC50.20502@uapar.edu>

Dear all,
I'm looking for some material on data mining with R. I have something 
from Luis Torgo but I'd like to see something else.
If anybody could help me I'll be thankful
Adri??n

From faceasec at uapar.edu  Tue Jul 19 15:13:42 2005
From: faceasec at uapar.edu (secretario academico FACEA)
Date: Tue, 19 Jul 2005 10:13:42 -0300
Subject: [R] reading data
Message-ID: <42DCFC86.4040807@uapar.edu>

Dear all,
How can I read data from posgresql?
Thanks
Adri??n

From yclough at gwdg.de  Tue Jul 19 15:15:31 2005
From: yclough at gwdg.de (yann clough)
Date: Tue, 19 Jul 2005 15:15:31 +0200
Subject: [R] lme outer
Message-ID: <42DCFCF3.4080706@gwdg.de>

Dear R users

a question about "outer" explanatory variables in lme:

I have measured size of a population of insects in fields.
These fields were spread out over a large region.
The fields are grouped (spatially) in pairs: one with fertiliser "high", 
the other one "low".
I want to test effect of mean temperature and fertiliser on popsize.
Meantemp was measured for each field, but measurements are correlated 
within pairs,
and this should be taken into account to avoid pseudoreplication (in 
other words, I d like meantemp to be considered an "outer" variable).

Do I need to replace the temperature measurements by the means for each 
pair?
Or can I leave in the measurements per field pair?

this is my data and model (with meantemp values for each field):
popsize=c(8,19,13,28,30,29,45,41,21,30,20,32,44,52,65,45)
meantemp=c(10,10.4,11.2,11.4,12,12.25,12.5,12.7,10.1,10.7,11.5,11.3,11.7,12.3,12.9,12.8)
fertiliser=as.factor(rep(c("low","high"),each=8))
pair=as.factor(rep(c(1:8),times=2))

model1=lme(popsize~meantemp+fertiliser, random=~1|pair)

I now create a vector with the values of meantemp averaged per pair

meantemp2=tapply(meantemp,pair,mean)
meantemp2=meantemp2[pair]

rerun a model with that explanatory variable:
model2=lme(popsize~meantemp2+fertiliser, random=~1|pair)

summary.lme and anova.lme suggest minute differences in the estimated 
parameters and DF (!) between model1 and model2.
How do I explain these differences, especially in the DF?
Is there a model to prefer?

Sincerely,
Yann
-- 
Yann Clough
Agroecology
Georg-August University
Waldweg 26
D-37073 Goettingen
Tel: 0551/39-2358
email: yclough at gwdg.de
www: http://wwwuser.gwdg.de/~uaoe/mitarbeiter/y_clough_e.htm



From Achim.Zeileis at wu-wien.ac.at  Tue Jul 19 15:14:54 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 19 Jul 2005 15:14:54 +0200
Subject: [R] data mining
In-Reply-To: <42DCFC50.20502@uapar.edu>
References: <42DCFC50.20502@uapar.edu>
Message-ID: <20050719151454.45873248.Achim.Zeileis@wu-wien.ac.at>

On Tue, 19 Jul 2005 10:12:48 -0300 secretario academico FACEA wrote:

> Dear all,
> I'm looking for some material on data mining with R. I have something 
> from Luis Torgo but I'd like to see something else.

There have been several discussions on the list, so browsing the
archives will probably bring up some helpful pointers. Also have a look
at the "MachineLearning" view at
  http://CRAN.R-project.org/src/contrib/Views/MachineLearning.html
Z

> If anybody could help me I'll be thankful
> Adri??n
>



From Allan at STATS.uct.ac.za  Tue Jul 19 15:28:33 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 19 Jul 2005 15:28:33 +0200
Subject: [R] R: stats
Message-ID: <42DD0001.4DB7F7BD@STATS.uct.ac.za>

for the stats gurus

does anyone know if there exists a general formula relating the median
of a continuous distribution to its moments. the distribution could be
skewed or symmetric and is definitely not normal.

the reason for asking is since the median of the particular distribution
that i am interested in is difficult (probably impossible) to obtain.
the median depends depends on an incomplete gamma distribution. the
moments however can be obtained fairly easily.

/
allan

From murdoch at stats.uwo.ca  Tue Jul 19 16:08:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Jul 2005 10:08:11 -0400
Subject: [R] R: stats
In-Reply-To: <42DD0001.4DB7F7BD@STATS.uct.ac.za>
References: <42DD0001.4DB7F7BD@STATS.uct.ac.za>
Message-ID: <42DD094B.7010200@stats.uwo.ca>

On 7/19/2005 9:28 AM, Clark Allan wrote:
> for the stats gurus
> 
> does anyone know if there exists a general formula relating the median
> of a continuous distribution to its moments. the distribution could be
> skewed or symmetric and is definitely not normal.

Not in general, and probably not any more practically useful than 
F^(-1)(0.5).

> the reason for asking is since the median of the particular distribution
> that i am interested in is difficult (probably impossible) to obtain.
> the median depends depends on an incomplete gamma distribution. the
> moments however can be obtained fairly easily.

R can calculate the incomplete gamma function (see ?pgamma), so that's 
not necessarily a stumbling block.

Duncan Murdoch



From tlumley at u.washington.edu  Tue Jul 19 16:26:29 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 19 Jul 2005 07:26:29 -0700 (PDT)
Subject: [R] R: expression
In-Reply-To: <42DCE770.5000105@statistik.uni-dortmund.de>
References: <42DCE57E.186FD875@STATS.uct.ac.za>
	<42DCE770.5000105@statistik.uni-dortmund.de>
Message-ID: <Pine.A41.4.61b.0507190725230.66584@homer08.u.washington.edu>

On Tue, 19 Jul 2005, Uwe Ligges wrote:

> Clark Allan wrote:
>
>> hi all
>> i am having a problem with the expression/paste command
>> say we estimate a variable, named PHI
>> it contains the value of say 2
>> and we want to display this value as " hat(phi) = PHI" onto a graphic
>> i.e.	 " hat(phi)=2 "
>> how does one do this?
>>

>
>     legend(-5, .3, substitute(hat(phi) == PHI, list(PHI = PHI)))
>

or
      legend(-5, .3, bquote(hat(phi) == .(PHI)))


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From baron at psych.upenn.edu  Tue Jul 19 16:29:18 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 19 Jul 2005 10:29:18 -0400
Subject: [R] R Site Search is back up
Message-ID: <20050719142918.GA22669@psych>

The search site at
http://finzi.psych.upenn.edu/
is back up.  This allows you to search mailing list archives,
R functions from most packages, and documents.

You can also use the R function RSiteSearch() in R itself, which
opens the results in your browser.

I'm sorry the site was down for so long.  I plan to work out a
system that will allow faster restoration if this happens again
(mutual complete backups with a colleague).  Backups are useless
if you don't have a machine to put them on, and it took 5 days to
get one, then one day to restore.

Some details that you don't need to read:

The site was reconstructed from scratch.  This means that links
from one message in the archives to another message in the
archives will not work anymore.  They never worked very well,
since the message numbers have been changed a few times over the
years.  (Threads work.  Just links don't work.)

In my rush to get this back up, I left out a few packages that
would not install.  I plan to install at least the help files
from these in due course.  I'm also not sure about Bioconductor
packages that are not already on CRAN.  I left out all of these.
Let me know if you want them.  Same with Jim Lindsey's packages.

I also left out all the mail from r-sig-geo, which I had been
including, although it seems hardly worth the effort since volume
is so low now on that list.  If you want it back, let me know.

What happened is still not clear.  There were several things
wrong with the computer, still not fully diagnosed.  The symptoms 
are consistent with overheating, but the fan was working fine,
and so has the air conditioning in the building (which has been
working very hard, global warming having finally hit
Philadelphia).  Recovery from the disk was possible but very
expensive.

What I learned from this is that backup systems do no good unless 
you have a computer to use.  And also I could have backed up in a 
way that made it easier for myself to reconstruct, but that was
not the major source of the delay.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From ripley at stats.ox.ac.uk  Tue Jul 19 16:31:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2005 15:31:27 +0100 (BST)
Subject: [R] colnames
In-Reply-To: <1121771987.5947.31.camel@ipc143004.lif.icnet.uk>
References: <C7FF4EF92D5A794EA5820C75CFB938F9630355@MAILSERVER.sabrefund.com>
	<1121771987.5947.31.camel@ipc143004.lif.icnet.uk>
Message-ID: <Pine.LNX.4.61.0507191518580.11205@gannet.stats>

On Tue, 19 Jul 2005, Adaikalavan Ramasamy wrote:

> First, your problem could be boiled down to the following example. See
> how the colnames of the two outputs vary.
>
> df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
> df/df
>  X100 X200
> 1    1    1
> 2    1    1

That one is probably unintentional.

> m  <- as.matrix( df )   # coerce to matrix class
> m/m
>  100 200
> 1   1   1
> 2   1   1
>
> It appears that whenever R has to create a new dataframe automatically,
> it tries to get nice colnames. See help(data.frame). I am not exactly
> sure why this behaviour is different when creating a matrix. But I do
> not think this is a major problem for most people. If you coerce your
> input to matrix, the problem goes away.

A data frame is column-oriented, and can be used as a source of variables, 
e.g. by attach() and the data= argument of all the model-fitting 
functions.  That is not the purpose of matrices, but is why data.frames 
are made to have syntactic names for columns by default.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gilbert.wu at sabrefund.com  Tue Jul 19 16:49:05 2005
From: gilbert.wu at sabrefund.com (Gilbert Wu)
Date: Tue, 19 Jul 2005 15:49:05 +0100
Subject: [R] colnames
Message-ID: <C7FF4EF92D5A794EA5820C75CFB938F9630356@MAILSERVER.sabrefund.com>

Hi Adai,

Thank you very much for your suggestions. Your optimized function would come in very handy cause I will need to generate a matrix of size around 2250 * 1000.

Regards,

Gilbert


-----Original Message-----
From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
Sent: 19 July 2005 12:20
To: Gilbert Wu
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] colnames


First, your problem could be boiled down to the following example. See
how the colnames of the two outputs vary.

df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
df/df
  X100 X200
1    1    1
2    1    1

m  <- as.matrix( df )   # coerce to matrix class
m/m
  100 200
1   1   1
2   1   1

It appears that whenever R has to create a new dataframe automatically,
it tries to get nice colnames. See help(data.frame). I am not exactly
sure why this behaviour is different when creating a matrix. But I do
not think this is a major problem for most people. If you coerce your
input to matrix, the problem goes away.


Next, note the following points :
 a) "mat[ 1:3, 1:ncol(mat) ]" is equivalent to simply "mat[ 1:3,  ]". 
 b) "mat[ 2:nrow(mat), ]" is equivalent to simply "mat[ -1,  ]"
See help(subset) for more information.

Using the points above, we can simplify your function as 

 p.RIs2Returns <- function (mat){

   mat <- as.matrix(mat)
   x <- mat[ -nrow(mat), ]
   y <- mat[ -1, ]
  
   return( y/x -1 )
 }

If your data contains only numerical data, it is probably good idea to
work with matrices as matrix operations are faster.


Finally, we can shorten your function. You can use the diff (which works
column-wise if input is a matrix) and apply function if you know that 

	y/x  =  exp(log(y/x))  =  exp( log(y) - log(x) )

which could be coded in R as

	exp( diff( log(r1) ) )

and then subtract 1 from above to get your returns.

Regards, Adai



On Tue, 2005-07-19 at 09:17 +0100, Gilbert Wu wrote:
> Hi Adai,
> 
> Many Thanks for the examples.
> 
> I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.
> 
> The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.
> 
> I would welcome any suggestion to improve the script.
> 
> 
> Regards,
> 
> Gilbert
> 
> > "p.RIs2Returns" <-
> + function (RIm)
> + {
> + x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
> + y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
> + RReturns <- (y/x -1)
> + RReturns
> + }
> > 
> > 
> > channel<-odbcConnect("ourSQLDB")
> > result<-sqlQuery(channel,paste("select * from equityRIs;"))
> > odbcClose(channel)
> > result
>    stockid    sdate  dbPrice
> 1   899188 20050713  7.59500
> 2   899188 20050714  7.60500
> 3   899188 20050715  7.48000
> 4   899188 20050718  7.41500
> 5   902232 20050713 10.97000
> 6   902232 20050714 10.94000
> 7   902232 20050715 10.99000
> 8   902232 20050718 11.05000
> 9   901714 20050713 17.96999
> 10  901714 20050714 18.00999
> 11  901714 20050715 17.64999
> 12  901714 20050718 17.64000
> 13  28176U 20050713  5.19250
> 14  28176U 20050714  5.25000
> 15  28176U 20050715  5.25000
> 16  28176U 20050718  5.22500
> 17  15322M 20050713 11.44000
> 18  15322M 20050714 11.50000
> 19  15322M 20050715 11.33000
> 20  15322M 20050718 11.27000
> > r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> > r1
>      sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> 1 20050713          7.595          10.97       17.96999         5.1925          11.44
> 2 20050714          7.605          10.94       18.00999         5.2500          11.50
> 3 20050715          7.480          10.99       17.64999         5.2500          11.33
> 4 20050718          7.415          11.05       17.64000         5.2250          11.27
> > #Set sdate as the rownames
> > rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> > #Get rid of the first column
> > r1 <- r1[1:nrow(r1),2:ncol(r1)]
> > r1
>          dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> 20050713          7.595          10.97       17.96999         5.1925          11.44
> 20050714          7.605          10.94       18.00999         5.2500          11.50
> 20050715          7.480          10.99       17.64999         5.2500          11.33
> 20050718          7.415          11.05       17.64000         5.2250          11.27
> > colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> > r1
>          899188 902232   901714 28176U 15322M
> 20050713  7.595  10.97 17.96999 5.1925  11.44
> 20050714  7.605  10.94 18.00999 5.2500  11.50
> 20050715  7.480  10.99 17.64999 5.2500  11.33
> 20050718  7.415  11.05 17.64000 5.2250  11.27
> > RRs<-p.RIs2Returns(r1)
> > RRs
>               X899188      X902232      X901714      X28176U      X15322M
> 20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
> 20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
> 20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
> > 
>



From szlevine at nana.co.il  Tue Jul 19 18:11:15 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 19 Jul 2005 18:11:15 +0200
Subject: [R] Survival dummy variables and some questions
References: <000e01c58b64$7b39c060$287716ac@IBM4FC21148802>
	<42DBA495.3030505@vanderbilt.edu>
	<000601c58bae$80ea8270$287716ac@IBM4FC21148802>
	<Pine.LNX.4.61.0507181544000.19788@gannet.stats>
	<001001c58bb7$c7162340$287716ac@IBM4FC21148802>
	<42DBD53E.7010100@vanderbilt.edu>
	<001001c58c2f$5acaf850$287716ac@IBM4FC21148802>
	<1121772603.5947.35.camel@ipc143004.lif.icnet.uk>
Message-ID: <000801c58c7c$7dee26c0$287716ac@IBM4FC21148802>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/d464ed04/attachment.pl

From abunn at whrc.org  Tue Jul 19 17:13:55 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 19 Jul 2005 11:13:55 -0400
Subject: [R] extracting row means from a list
Message-ID: <NEBBIPHDAMMOKDKPOFFIMECADIAA.abunn@whrc.org>

Hello: I'm reading in a series of text files (100 files that are each 2000
rows by 6 columns). I wish to combine the columns (6) of each file (100) and
get the row mean. I'd like to end up with a data.frame of 2000 rows by 6
columns.

foo <- list()
for(i in 1:10){
     # The real data are read in from a series of numbered text files
     foo[[i]] <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 =
rnorm(100),
                            x4 = rnorm(100), x5 = rnorm(100), x6 =
rnorm(100))

}

str(foo)
# by hand
mean.x1 <-
rowMeans(cbind(foo[[1]][,1],foo[[2]][,1],foo[[3]][,1],foo[[4]][,1],foo[[5]][
,1]),
                          foo[[6]][,1],foo[[7]][,1],foo[[8]][,1],foo[[9]][,1
],foo[[10]][,1]))
mean.x2 <-
rowMeans(cbind(foo[[1]][,2],foo[[2]][,2],foo[[3]][,2],foo[[4]][,2],foo[[5]][
,2]),
                          foo[[6]][,2],foo[[7]][,2],foo[[8]][,2],foo[[9]][,2
],foo[[10]][,2]))
# and so on to column 6
mean.x6 <-
rowMeans(cbind(foo[[1]][,6],foo[[2]][,6],foo[[3]][,6],foo[[4]][,6],foo[[5]][
,6]),
                          foo[[6]][,6],foo[[7]][,6],foo[[8]][,6],foo[[9]][,6
],foo[[10]][,6]))


I've implemented this with nested loops that create temporary variables and
calc the mean, but the approach is clunky. E.g.,

# nested loops
for(i in 1:ncol(foo[[1]])){
  for(j in 1:length(foo)){
    # etc ...
  }
}

Is there a way to build a better mouse trap?

TIA, Andy

Thanks, Andy



From dirk.enzmann at jura.uni-hamburg.de  Tue Jul 19 17:29:25 2005
From: dirk.enzmann at jura.uni-hamburg.de (Dirk Enzmann)
Date: Tue, 19 Jul 2005 17:29:25 +0200
Subject: [R] using argument names (of indeterminate number) within a function
Message-ID: <42DD1C55.8040200@jura.uni-hamburg.de>

Although I tried to find an answer in the manuals and archives, I cannot 
solve this (please excuse that my English and/or R programming skills 
are not good enough to state my problem more clearly):

I want to write a function with an indeterminate (not pre-defined) 
number of arguments and think that I should use the "..." construct and 
the match.call() function. The goal is to write a function that (among 
other things) uses cbind() to combine a not pre-defined number of 
vectors specified in the function call. For example, if my vectors are 
x1, x2, x3, ... xn, within the function I want to use cbind(x1, x2) or 
cbind(x1, x3, x5) or ... depending on the vector names I use in the 
funcion call. Additionally, the function has other arguments.

In the archives I found the following thread (followed by Marc Schwartz)

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15186.html
[R] returning argument names from Peter Dalgaard BSA on 2003-04-10 (stdin)

that seems to contain the solution to my problem, but I am stuck because 
  sapply(match.call()[-1], deparse) gives me a vector of strings and I 
don't know how to use the names in this vector in the cbind() function.

Up to now my (clearly deficit) function looks like:

test <- function(..., mvalid=1)
{
   args = sapply(match.call()[-1], deparse)
# and here, I don't know how the vector names in args
# can be used in the cbind() function to follow:
#
# temp <- cbind( ???
   if (mvalid > 1)
   {
#  here it goes on
   }
}

Ultimately, I want that the function can be called like
test(x1,x2,mvalid=1)
or
test(x1,x3,x5,mavlid=2)
and that within the function
cbind(x1,x2)
or cbind(x1,x3,x5)
will be used.

Can someone give and explain an example / a solution on how to proceed?

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-040-42838.7498 (office)
        +49-040-42838.4591 (Billon)
fax:   +49-040-42838.2344
email: dirk.enzmann at jura.uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html



From gilbert.wu at sabrefund.com  Tue Jul 19 17:30:12 2005
From: gilbert.wu at sabrefund.com (Gilbert Wu)
Date: Tue, 19 Jul 2005 16:30:12 +0100
Subject: [R] colnames
Message-ID: <C7FF4EF92D5A794EA5820C75CFB938F9630357@MAILSERVER.sabrefund.com>

Hi Adai,

When I tried the optimized routine, I got the following error message:

r1
         899188 902232   901714 28176U 15322M
20050713  7.595  10.97 17.96999 5.1925  11.44
20050714  7.605  10.94 18.00999 5.2500  11.50
20050715  7.480  10.99 17.64999 5.2500  11.33
20050718  7.415  11.05 17.64000 5.2250  11.27
> exp(diff(log(r1))) -1
Error in r[i1] - r[-length(r):-(length(r) - lag + 1)] : 
        non-numeric argument to binary operator
>

Any idea?

Many Thanks.

Gilbert
-----Original Message-----
From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
Sent: 19 July 2005 12:20
To: Gilbert Wu
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] colnames


First, your problem could be boiled down to the following example. See
how the colnames of the two outputs vary.

df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
df/df
  X100 X200
1    1    1
2    1    1

m  <- as.matrix( df )   # coerce to matrix class
m/m
  100 200
1   1   1
2   1   1

It appears that whenever R has to create a new dataframe automatically,
it tries to get nice colnames. See help(data.frame). I am not exactly
sure why this behaviour is different when creating a matrix. But I do
not think this is a major problem for most people. If you coerce your
input to matrix, the problem goes away.


Next, note the following points :
 a) "mat[ 1:3, 1:ncol(mat) ]" is equivalent to simply "mat[ 1:3,  ]". 
 b) "mat[ 2:nrow(mat), ]" is equivalent to simply "mat[ -1,  ]"
See help(subset) for more information.

Using the points above, we can simplify your function as 

 p.RIs2Returns <- function (mat){

   mat <- as.matrix(mat)
   x <- mat[ -nrow(mat), ]
   y <- mat[ -1, ]
  
   return( y/x -1 )
 }

If your data contains only numerical data, it is probably good idea to
work with matrices as matrix operations are faster.


Finally, we can shorten your function. You can use the diff (which works
column-wise if input is a matrix) and apply function if you know that 

	y/x  =  exp(log(y/x))  =  exp( log(y) - log(x) )

which could be coded in R as

	exp( diff( log(r1) ) )

and then subtract 1 from above to get your returns.

Regards, Adai



On Tue, 2005-07-19 at 09:17 +0100, Gilbert Wu wrote:
> Hi Adai,
> 
> Many Thanks for the examples.
> 
> I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.
> 
> The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.
> 
> I would welcome any suggestion to improve the script.
> 
> 
> Regards,
> 
> Gilbert
> 
> > "p.RIs2Returns" <-
> + function (RIm)
> + {
> + x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
> + y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
> + RReturns <- (y/x -1)
> + RReturns
> + }
> > 
> > 
> > channel<-odbcConnect("ourSQLDB")
> > result<-sqlQuery(channel,paste("select * from equityRIs;"))
> > odbcClose(channel)
> > result
>    stockid    sdate  dbPrice
> 1   899188 20050713  7.59500
> 2   899188 20050714  7.60500
> 3   899188 20050715  7.48000
> 4   899188 20050718  7.41500
> 5   902232 20050713 10.97000
> 6   902232 20050714 10.94000
> 7   902232 20050715 10.99000
> 8   902232 20050718 11.05000
> 9   901714 20050713 17.96999
> 10  901714 20050714 18.00999
> 11  901714 20050715 17.64999
> 12  901714 20050718 17.64000
> 13  28176U 20050713  5.19250
> 14  28176U 20050714  5.25000
> 15  28176U 20050715  5.25000
> 16  28176U 20050718  5.22500
> 17  15322M 20050713 11.44000
> 18  15322M 20050714 11.50000
> 19  15322M 20050715 11.33000
> 20  15322M 20050718 11.27000
> > r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> > r1
>      sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> 1 20050713          7.595          10.97       17.96999         5.1925          11.44
> 2 20050714          7.605          10.94       18.00999         5.2500          11.50
> 3 20050715          7.480          10.99       17.64999         5.2500          11.33
> 4 20050718          7.415          11.05       17.64000         5.2250          11.27
> > #Set sdate as the rownames
> > rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> > #Get rid of the first column
> > r1 <- r1[1:nrow(r1),2:ncol(r1)]
> > r1
>          dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> 20050713          7.595          10.97       17.96999         5.1925          11.44
> 20050714          7.605          10.94       18.00999         5.2500          11.50
> 20050715          7.480          10.99       17.64999         5.2500          11.33
> 20050718          7.415          11.05       17.64000         5.2250          11.27
> > colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> > r1
>          899188 902232   901714 28176U 15322M
> 20050713  7.595  10.97 17.96999 5.1925  11.44
> 20050714  7.605  10.94 18.00999 5.2500  11.50
> 20050715  7.480  10.99 17.64999 5.2500  11.33
> 20050718  7.415  11.05 17.64000 5.2250  11.27
> > RRs<-p.RIs2Returns(r1)
> > RRs
>               X899188      X902232      X901714      X28176U      X15322M
> 20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
> 20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
> 20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
> > 
>



From sundar.dorai-raj at pdf.com  Tue Jul 19 17:35:11 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 19 Jul 2005 10:35:11 -0500
Subject: [R] extracting row means from a list
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIMECADIAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIMECADIAA.abunn@whrc.org>
Message-ID: <42DD1DAF.9040102@pdf.com>



Andy Bunn wrote:
> Hello: I'm reading in a series of text files (100 files that are each 2000
> rows by 6 columns). I wish to combine the columns (6) of each file (100) and
> get the row mean. I'd like to end up with a data.frame of 2000 rows by 6
> columns.
> 
> foo <- list()
> for(i in 1:10){
>      # The real data are read in from a series of numbered text files
>      foo[[i]] <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 =
> rnorm(100),
>                             x4 = rnorm(100), x5 = rnorm(100), x6 =
> rnorm(100))
> 
> }
> 
> str(foo)
> # by hand
> mean.x1 <-
> rowMeans(cbind(foo[[1]][,1],foo[[2]][,1],foo[[3]][,1],foo[[4]][,1],foo[[5]][
> ,1]),
>                           foo[[6]][,1],foo[[7]][,1],foo[[8]][,1],foo[[9]][,1
> ],foo[[10]][,1]))
> mean.x2 <-
> rowMeans(cbind(foo[[1]][,2],foo[[2]][,2],foo[[3]][,2],foo[[4]][,2],foo[[5]][
> ,2]),
>                           foo[[6]][,2],foo[[7]][,2],foo[[8]][,2],foo[[9]][,2
> ],foo[[10]][,2]))
> # and so on to column 6
> mean.x6 <-
> rowMeans(cbind(foo[[1]][,6],foo[[2]][,6],foo[[3]][,6],foo[[4]][,6],foo[[5]][
> ,6]),
>                           foo[[6]][,6],foo[[7]][,6],foo[[8]][,6],foo[[9]][,6
> ],foo[[10]][,6]))
> 
> 
> I've implemented this with nested loops that create temporary variables and
> calc the mean, but the approach is clunky. E.g.,
> 
> # nested loops
> for(i in 1:ncol(foo[[1]])){
>   for(j in 1:length(foo)){
>     # etc ...
>   }
> }
> 
> Is there a way to build a better mouse trap?
> 
> TIA, Andy


I don't know of a way of getting around at least one for loop, but the 
following might be want you need:

r <- matrix(, NROW(foo[[1]]), length(foo))
for(i in 1:NCOL(foo[[1]]))
   r[, i] <- rowMeans(do.call("cbind", lapply(foo, "[", i)))
dim(r)

This assumes each element of foo has identical dimensions. Otherwise 
you'll get an error.

HTH,

--sundar



From mschwartz at mn.rr.com  Tue Jul 19 18:00:17 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 19 Jul 2005 11:00:17 -0500
Subject: [R] using argument names (of indeterminate number) within
	a	function
In-Reply-To: <42DD1C55.8040200@jura.uni-hamburg.de>
References: <42DD1C55.8040200@jura.uni-hamburg.de>
Message-ID: <1121788817.4124.10.camel@localhost.localdomain>

On Tue, 2005-07-19 at 17:29 +0200, Dirk Enzmann wrote:
> Although I tried to find an answer in the manuals and archives, I cannot 
> solve this (please excuse that my English and/or R programming skills 
> are not good enough to state my problem more clearly):
> 
> I want to write a function with an indeterminate (not pre-defined) 
> number of arguments and think that I should use the "..." construct and 
> the match.call() function. The goal is to write a function that (among 
> other things) uses cbind() to combine a not pre-defined number of 
> vectors specified in the function call. For example, if my vectors are 
> x1, x2, x3, ... xn, within the function I want to use cbind(x1, x2) or 
> cbind(x1, x3, x5) or ... depending on the vector names I use in the 
> funcion call. Additionally, the function has other arguments.
> 
> In the archives I found the following thread (followed by Marc Schwartz)
> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15186.html
> [R] returning argument names from Peter Dalgaard BSA on 2003-04-10 (stdin)
> 
> that seems to contain the solution to my problem, but I am stuck because 
>   sapply(match.call()[-1], deparse) gives me a vector of strings and I 
> don't know how to use the names in this vector in the cbind() function.
> 
> Up to now my (clearly deficit) function looks like:
> 
> test <- function(..., mvalid=1)
> {
>    args = sapply(match.call()[-1], deparse)
> # and here, I don't know how the vector names in args
> # can be used in the cbind() function to follow:
> #
> # temp <- cbind( ???
>    if (mvalid > 1)
>    {
> #  here it goes on
>    }
> }
> 
> Ultimately, I want that the function can be called like
> test(x1,x2,mvalid=1)
> or
> test(x1,x3,x5,mavlid=2)
> and that within the function
> cbind(x1,x2)
> or cbind(x1,x3,x5)
> will be used.
> 
> Can someone give and explain an example / a solution on how to proceed?

Hi Dirk,

How about this:

my.cbind <- function(...)
{
  do.call("cbind", list(...))
}

> a <- 1:10
> b <- 11:20
> c <- 21:30
> d <- 31:40

> my.cbind(a, b)
      [,1] [,2]
 [1,]    1   11
 [2,]    2   12
 [3,]    3   13
 [4,]    4   14
 [5,]    5   15
 [6,]    6   16
 [7,]    7   17
 [8,]    8   18
 [9,]    9   19
[10,]   10   20

> my.cbind(b, c, d)
      [,1] [,2] [,3]
 [1,]   11   21   31
 [2,]   12   22   32
 [3,]   13   23   33
 [4,]   14   24   34
 [5,]   15   25   35
 [6,]   16   26   36
 [7,]   17   27   37
 [8,]   18   28   38
 [9,]   19   29   39
[10,]   20   30   40

> my.cbind(a, b, c, d)
      [,1] [,2] [,3] [,4]
 [1,]    1   11   21   31
 [2,]    2   12   22   32
 [3,]    3   13   23   33
 [4,]    4   14   24   34
 [5,]    5   15   25   35
 [6,]    6   16   26   36
 [7,]    7   17   27   37
 [8,]    8   18   28   38
 [9,]    9   19   29   39
[10,]   10   20   30   40


The use of list(...) in the function allows you to use list based
functions such as do.call() or lapply() against the argument objects
directly without having to deparse and re-parse the character names of
the arguments, which is the approach that Peter used in his response in
the thread you referenced. 

The OP in that thread wanted the argument names as character vectors, as
opposed to the argument objects themselves, which is what you need here.

HTH,

Marc Schwartz



From isubirana at imim.es  Tue Jul 19 18:00:33 2005
From: isubirana at imim.es (SUBIRANA CACHINERO, ISAAC)
Date: Tue, 19 Jul 2005 18:00:33 +0200
Subject: [R] ROC curve with survival data
Message-ID: <FAD63B0F78D0224A9AF352A4A98F1E2A588C36@jupiter.imim.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/e2531061/attachment.pl

From mark.salsburg at gmail.com  Tue Jul 19 18:10:03 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 19 Jul 2005 12:10:03 -0400
Subject: [R] .gct file
Message-ID: <dd48e20f050719091034c8dca9@mail.gmail.com>

I have two files to compare, one is a regular txt file that I can read
in no prob.

The other is a .gct file (How do I read in this one?)

I tried a simple

read.table("data.gct", header = T)

How do you suggest reading in this file??

thank you.



From murdoch at stats.uwo.ca  Tue Jul 19 18:28:32 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Jul 2005 12:28:32 -0400
Subject: [R] .gct file
In-Reply-To: <dd48e20f050719091034c8dca9@mail.gmail.com>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
Message-ID: <42DD2A30.3000104@stats.uwo.ca>

On 7/19/2005 12:10 PM, mark salsburg wrote:
> I have two files to compare, one is a regular txt file that I can read
> in no prob.
> 
> The other is a .gct file (How do I read in this one?)
> 
> I tried a simple
> 
> read.table("data.gct", header = T)
> 
> How do you suggest reading in this file??
> 

.gct is not a standard filename extension.  You need to know what is in 
that file.  Where did you get it?  What program created it?

Chances are the easiest thing to do is to get the program that created 
it to export in a well known format, e.g. .csv.

Duncan Murdoch



From buser at stat.math.ethz.ch  Tue Jul 19 18:35:37 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 19 Jul 2005 18:35:37 +0200
Subject: [R] =?iso-8859-1?q?Predict?=
In-Reply-To: <28795.1121772155@www68.gmx.net>
References: <28795.1121772155@www68.gmx.net>
Message-ID: <17117.11225.931638.729372@stat.math.ethz.ch>

Dear Matthias

Can you provide an example to demonstrate what you did? Two
remarks to your email. Maybe that answers already your question.

1) Using predict() you will get the estimated value for each
   observation or for new data. You can reproduce this value by
   using the coefficients from your estimated model (see the
   example below). 
   For the interval you can get a confidence interval for the
   expected value under fixed conditions of the explanatory
   variables or you can obtain a prediction interval for a
   single new observation. The latter is of course wider, since
   you try to catch a single observation and not the expected
   value. 

2) Using confint() you will get the estimated parameters (which
   are random variables, too) and their confidence interval. 
   You can use the estimated values to calculate the predicted
   values.

But you can NOT use the upper values from confint to
estimate the upper values from predict by just putting them into
your regression model. Thats not the way how confidence
intervals are constructed.
(I am not sure if this was your intention. Maybe if you show a
reproducible example you can correct me if you meant something
different) 

## R Code
## Creation of a dataframe 
set.seed(1)
x1 <- runif(40)
f1 <- rep(c("a", "b", "c","d"), each = 10)
y <- 2*x1 + rep(c(0.5, 0.1, -0.6, 1.5), each = 10) + rnorm(40, 0, 2)
dat <- data.frame(y = y, f1 = f1, x1 = x1)
## regression model
reg <- lm(y~ x1 + f1, data = dat)
summary(reg)

confint(reg)
predict(reg, type=c("response"), interval = "confidence")

## caluclation of predicted values using the estimated
## coefficients 

## estimated coefficients
co <- summary(reg)$coefficients[,"Estimate"]
## Using the regression model with that coefficients
## for observation 11 
co["(Intercept)"] + dat[11,"x1"]*co["x1"] + co["f1b"]
## prediction of observation 11 
predict(reg, type=c("response"))[11]


Regards,

Christoph 

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Matthias Eggenberger writes:
 > When I callculate a linear model, then I can compute via confint the
 > confidencial intervals. the interval level can be chosen. as result, I get
 > the parameter of the model according to the interval level. 
 > 
 > On the other hand, I can compute the prediction-values for my model as well
 > with predict(object, type=c("response") etc.). Here I have also the
 > possibility to chose a level for the confidential intervals. the output are
 > the calculatet values for the fit, the lower and upper level. 
 > 
 > the problem now is, that when I calculate the values through the linear
 > model function with the parameter values I get from confint() an I compare
 > them with the values I get from predict() these values differ extremely. Why
 > is that so? Does the command predict() calculate the values through an other
 > routine? That means the command predict() doesn't use the same parameters to
 > calculate the prediction-values than the ones given by confint()?
 > 
 > Greetings Matthias
 > 
 > -- 
 > GMX DSL = Maximale Leistung zum minimalen Preis!
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Tue Jul 19 18:50:00 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 19 Jul 2005 11:50:00 -0500
Subject: [R] Padding in lattice plots
In-Reply-To: <200507161812.04359.f.gherardini@pigrecodata.net>
References: <200507151442.39139.f.gherardini@pigrecodata.net>
	<200507151619.03581.f.gherardini@pigrecodata.net>
	<eb555e6605071508005fe5f93e@mail.gmail.com>
	<200507161812.04359.f.gherardini@pigrecodata.net>
Message-ID: <eb555e66050719095019f5c80e@mail.gmail.com>

On 7/16/05, Federico Gherardini <f.gherardini at pigrecodata.net> wrote:
> On Friday 15 July 2005 17:00, Deepayan Sarkar wrote:
> > On 7/15/05, Federico Gherardini <f.gherardini at pigrecodata.net> wrote:
> > > On Friday 15 July 2005 14:42, you wrote:
> > > > Hi all,
> > > > I've used the split argument to print four lattice plots on a single
> > > > page. The problem now is that I need to reduce the amount of white
> > > > space between the plots. I've read other mails in this list about the
> > > > new trellis parameters layout.heights and layout.widhts but I haven't
> > > > been able to use them properly. I've tried to input values between 0
> > > > and 1 as the padding value (both left and right and top and bottom) but
> > > > nothing changed. It seems I can only increase the padding by using
> > > > values > 1. Any ideas?
> > > >
> > > > Thanks in advance for your help
> > > > Federico Gherardini
> > >
> > > It seems like I've found an answer myself.... you have to use negative
> > > values to decrease the padding. I thought it was something like the cex
> > > parameter which acts like a multiplier
> >
> > I thought so too.
> >
> > > but this is not the case.
> >
> > Could you post what you used? There are several different padding
> > parameters you need to set to 0, did you change them all?
> >
> > Deepayan
> 
> Hi Deepayan
> This is what I used.... I don't know if I did everything the "proper" way but
> at least I got the result I was seeking! :)
> 
> trellis.par.set(list(layout.heights = list(top.padding = -1)))
> 
> trellis.par.set(list(layout.heights = list(bottom.padding = -1,
> axis.xlab.padding = 1, xlab = -1.2)))
> 
> trellis.par.set(list(layout.widths = list(left.padding = -1)))
> 
> trellis.par.set(list(layout.widths = list(right.padding = -1,
> ylab.axis.padding = -0.5)))
> 
> Do these settings make any sense?

Yes, but there are other padding parameters. For example, at the top, there's 

 $ top.padding      : num 1
 $ main.key.padding : num 1
 $ key.axis.padding : num 1

The total amount of space left at the top (in the default case, with
no main label and no key) is the sum of all three, so just setting one
to 0 wouldn't be enough for what you want. (This is probably not very
simple, but I couldn't think of anything that's simpler yet as
flexible.)

Deepayan



From buser at stat.math.ethz.ch  Tue Jul 19 19:03:21 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 19 Jul 2005 19:03:21 +0200
Subject: [R] initial points for arms in package HI
Message-ID: <17117.12889.773438.382880@stat.math.ethz.ch>

Dear R-users

I have a problem choosing initial points for the function arms()
in the package HI
I intend to implement a Gibbs sampler and one of my conditional
distributions is nonstandard and not logconcave.
Therefore I'd like to use arms.

But there seem to be a strong influence of the initial point
y.start. To show the effect I constructed a demonstration
example. It is reproducible without further information.  

Please note that my target density is not logconcave.

Thanks for all comments or ideas.

Christoph Buser

## R Code:

library(HI)
## parameter for the distribution
para <- 0.1

## logdensity
logDichteGam <- function(x, u = para, v = para) {
  -(u*x + v*1/x) - log(x)
}
## density except for the constant
propDichteGam <- function(x, u = para, v = para) {
  exp(-(u*x + v*1/x) - log(x))
}
## calculating the constant 
(c <- integrate(propDichteGam, 0, 1000, rel.tol = 10^(-12))$value)
## density
DichteGam <- function(x, u = para, v = para) {
  exp(-(u*x + v*1/x) - log(x))/c
}

## calculating 1000 values by repeating a call of arms (this would
## be the situation in an Gibbs Sample. Of course in a Gibbs sampler
## the distribution would change. This is only for demonstration
res1 <- NULL
for(i in 1:1000)
  res1[i] <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1)

## Generating a sample of thousand observations with 1 call of arms
res2 <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1000)

## Plot of the samples 
mult.fig(4)
plot(res1, log = "y")
plot(res2, log = "y")
hist(res1, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
     ylim = c(0,1))
curve(DichteGam, 0,4, add = TRUE, col = 2)
hist(res2, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
     ylim = c(0,1))
curve(DichteGam, 0,4, add = TRUE, col = 2)


## If we repeat the procedure, using the fix intial value 1,
## the situation is even worse
res3 <- NULL
for(i in 1:1000)
  res3[i] <- arms(1, logDichteGam, function(x) (x>0)&(x<100), 1)

## Generating a sample of thousand observations with 1 call of arms
res4 <- arms(1, logDichteGam, function(x) (x>0)&(x<100), 1000)

## Plot of the samples 
par(mfrow = c(2,2))
plot(res3, log = "y")
plot(res4, log = "y")
hist(res3, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
     ylim = c(0,1))
curve(DichteGam, 0,4, add = TRUE, col = 2)
hist(res4, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
     ylim = c(0,1))
curve(DichteGam, 0,4, add = TRUE, col = 2)


## If I generate the sample in a for-loop (one by one) I do not
## get the correct density. But this is exactly the situation in 
## my Gibbs Sampler. Therfore I am concerned about the correct 
## application of arms

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/



From mschwartz at mn.rr.com  Tue Jul 19 19:06:36 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 19 Jul 2005 12:06:36 -0500
Subject: [R] .gct file
In-Reply-To: <42DD2A30.3000104@stats.uwo.ca>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
Message-ID: <1121792796.4124.15.camel@localhost.localdomain>

On Tue, 2005-07-19 at 12:28 -0400, Duncan Murdoch wrote:
> On 7/19/2005 12:10 PM, mark salsburg wrote:
> > I have two files to compare, one is a regular txt file that I can read
> > in no prob.
> > 
> > The other is a .gct file (How do I read in this one?)
> > 
> > I tried a simple
> > 
> > read.table("data.gct", header = T)
> > 
> > How do you suggest reading in this file??
> > 
> 
> .gct is not a standard filename extension.  You need to know what is in 
> that file.  Where did you get it?  What program created it?
> 
> Chances are the easiest thing to do is to get the program that created 
> it to export in a well known format, e.g. .csv.
> 
> Duncan Murdoch

A quick Google search would suggest "Gene Cluster Text" file:

http://www.broad.mit.edu/cancer/software/genepattern/tutorial/gp_tutorial_fileformats.html#gct

produced by Gene Pattern:

http://www.broad.mit.edu/cancer/software/genepattern/

If correct, I would point Mark to the Bioconductor folks for more
information and assistance:

http://www.bioconductor.org/

HTH,

Marc Schwartz



From mark.salsburg at gmail.com  Tue Jul 19 19:16:58 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 19 Jul 2005 13:16:58 -0400
Subject: [R] .gct file
In-Reply-To: <42DD2A30.3000104@stats.uwo.ca>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
Message-ID: <dd48e20f050719101623a60189@mail.gmail.com>

ok so the gct file looks like this:

#1.2  (version number)
7283 19   (matrix size)
Name Description Values
....      .......          ......

How can I tell R to disregard the first two lines and start reading
the 3rd line in this gct file. I would just delete them, but I do not
know how to open a gct. file

thank you

On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/19/2005 12:10 PM, mark salsburg wrote:
> > I have two files to compare, one is a regular txt file that I can read
> > in no prob.
> >
> > The other is a .gct file (How do I read in this one?)
> >
> > I tried a simple
> >
> > read.table("data.gct", header = T)
> >
> > How do you suggest reading in this file??
> >
> 
> .gct is not a standard filename extension.  You need to know what is in
> that file.  Where did you get it?  What program created it?
> 
> Chances are the easiest thing to do is to get the program that created
> it to export in a well known format, e.g. .csv.
> 
> Duncan Murdoch
>



From rjohnson at ncifcrf.gov  Tue Jul 19 19:18:19 2005
From: rjohnson at ncifcrf.gov (Randy Johnson)
Date: Tue, 19 Jul 2005 13:18:19 -0400
Subject: [R] .gct file
In-Reply-To: <1121792796.4124.15.camel@localhost.localdomain>
Message-ID: <BF02AE1B.2FE0%rjohnson@ncifcrf.gov>

If it is a text file ?read.table should provide enough details to read the
file into R. Based on the file format referenced below it shouldn't be too
hard to get at the parts you want.

Randy


On 7/19/05 1:06 PM, "Marc Schwartz (via MN)" <mschwartz at mn.rr.com> wrote:

> On Tue, 2005-07-19 at 12:28 -0400, Duncan Murdoch wrote:
>> On 7/19/2005 12:10 PM, mark salsburg wrote:
>>> I have two files to compare, one is a regular txt file that I can read
>>> in no prob.
>>> 
>>> The other is a .gct file (How do I read in this one?)
>>> 
>>> I tried a simple
>>> 
>>> read.table("data.gct", header = T)
>>> 
>>> How do you suggest reading in this file??
>>> 
>> 
>> .gct is not a standard filename extension.  You need to know what is in
>> that file.  Where did you get it?  What program created it?
>> 
>> Chances are the easiest thing to do is to get the program that created
>> it to export in a well known format, e.g. .csv.
>> 
>> Duncan Murdoch
> 
> A quick Google search would suggest "Gene Cluster Text" file:
> 
> http://www.broad.mit.edu/cancer/software/genepattern/tutorial/gp_tutorial_file
> formats.html#gct
> 
> produced by Gene Pattern:
> 
> http://www.broad.mit.edu/cancer/software/genepattern/
> 
> If correct, I would point Mark to the Bioconductor folks for more
> information and assistance:
> 
> http://www.bioconductor.org/
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Randy Johnson
Laboratory of Genomic Diversity
NCI-Frederick
Bldg 560, Rm 11-85
Frederick, MD 21702
(301)846-1304
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From murdoch at stats.uwo.ca  Tue Jul 19 19:21:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Jul 2005 13:21:14 -0400
Subject: [R] .gct file
In-Reply-To: <dd48e20f050719101623a60189@mail.gmail.com>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
Message-ID: <42DD368A.5070904@stats.uwo.ca>

On 7/19/2005 1:16 PM, mark salsburg wrote:
> ok so the gct file looks like this:
> 
> #1.2  (version number)
> 7283 19   (matrix size)
> Name Description Values
> ....      .......          ......
> 
> How can I tell R to disregard the first two lines and start reading
> the 3rd line in this gct file. I would just delete them, but I do not
> know how to open a gct. file

Use skip=2.  See ?read.table.

Duncan Murdoch



From thchung at tgen.org  Tue Jul 19 19:26:01 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Tue, 19 Jul 2005 10:26:01 -0700
Subject: [R] Library mclust in 64bit compiled R
Message-ID: <BF0285B9.67EA%thchung@tgen.org>

Hi, All;

I tried to use library mclust in 64-bit compiled R 2.0.1 but failed.
Installation went smoothly without any warning or error. However, when I
tried to use them with the following simple code, it crashed.

Library(mclust)
Dat <- c(rnorm(20, mean=0, sd=0.2), rnorm(30, mean=1, sd=0.2))
Ind <- Mclust(dat, 1, 5)$classification
cbind(Dat, Ind)

The error message was:

/usr/local/R-2.0.1_64bit/lib/R/bin/BATCH: line 55: 18097 Done
( echo "invisible(options(echo = TRUE))"; cat ${in}; echo "proc.time()" )
     18099 Segmentation fault      | ${R_HOME}/bin/R ${opts} >${out} 2>&1

Can anybody help me with this?
Thanks in advance,

Tae-Hoon Chung



From spencer.graves at pdf.com  Tue Jul 19 19:27:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 19 Jul 2005 10:27:00 -0700
Subject: [R] .gct file
In-Reply-To: <dd48e20f050719101623a60189@mail.gmail.com>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
Message-ID: <42DD37E4.5010904@pdf.com>

	  Try ?read.table or args(read.table).  Might "skip=2" do what you want?

	  spencer graves
p.s.  I routinely "readLines(File, n=11)" to see how many headers there 
are AND identify the "sep" character.  Then I 
"quantile(count.fields(File, ...))" to see if all records have the same 
number of fields.  Then I call something like "read.table" with the 
appropriate arguments.  Then I print the first 2 or so rows of the 
result to make sure I read the file correctly.

mark salsburg wrote:

> ok so the gct file looks like this:
> 
> #1.2  (version number)
> 7283 19   (matrix size)
> Name Description Values
> ....      .......          ......
> 
> How can I tell R to disregard the first two lines and start reading
> the 3rd line in this gct file. I would just delete them, but I do not
> know how to open a gct. file
> 
> thank you
> 
> On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> 
>>On 7/19/2005 12:10 PM, mark salsburg wrote:
>>
>>>I have two files to compare, one is a regular txt file that I can read
>>>in no prob.
>>>
>>>The other is a .gct file (How do I read in this one?)
>>>
>>>I tried a simple
>>>
>>>read.table("data.gct", header = T)
>>>
>>>How do you suggest reading in this file??
>>>
>>
>>.gct is not a standard filename extension.  You need to know what is in
>>that file.  Where did you get it?  What program created it?
>>
>>Chances are the easiest thing to do is to get the program that created
>>it to export in a well known format, e.g. .csv.
>>
>>Duncan Murdoch
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From pantd at unlv.nevada.edu  Tue Jul 19 19:28:50 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Tue, 19 Jul 2005 10:28:50 -0700
Subject: [R] Code Verification
Message-ID: <1121794130.42dd3852f127f@webmail.scsv.nevada.edu>

Hi R Users
I have a code which I am running for my thesis work. Just want to make sure that
its ok. Its a t test I am conducting between two gamma distributions with
different shape parameters.

the code looks like:

sink("a1.txt");

for (i in 1:1000)
{
x<-rgamma(40, 2.5, 10)  # n = 40, shape = 2.5, Scale = 10
y<-rgamma(40, 2.8, 10)  # n = 40, shape = 2.8, Scale = 10
z<-t.test(x, y)
print(z)
}


I will appreciate it if someone could tell me if its alrite or not.

thanks

-dev



From mschwartz at mn.rr.com  Tue Jul 19 19:30:15 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 19 Jul 2005 12:30:15 -0500
Subject: [R] .gct file
In-Reply-To: <dd48e20f050719101623a60189@mail.gmail.com>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
Message-ID: <1121794215.4124.22.camel@localhost.localdomain>

On Tue, 2005-07-19 at 13:16 -0400, mark salsburg wrote:
> ok so the gct file looks like this:
> 
> #1.2  (version number)
> 7283 19   (matrix size)
> Name Description Values
> ....      .......          ......
> 
> How can I tell R to disregard the first two lines and start reading
> the 3rd line in this gct file. I would just delete them, but I do not
> know how to open a gct. file
> 
> thank you
> 
> On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > On 7/19/2005 12:10 PM, mark salsburg wrote:
> > > I have two files to compare, one is a regular txt file that I can read
> > > in no prob.
> > >
> > > The other is a .gct file (How do I read in this one?)
> > >
> > > I tried a simple
> > >
> > > read.table("data.gct", header = T)
> > >
> > > How do you suggest reading in this file??
> > >
> > 
> > .gct is not a standard filename extension.  You need to know what is in
> > that file.  Where did you get it?  What program created it?
> > 
> > Chances are the easiest thing to do is to get the program that created
> > it to export in a well known format, e.g. .csv.
> > 
> > Duncan Murdoch


The above would be consistent with the info in my reply.

I guess if the format is consistent, as per Mark's example above, you
can use:

read.table("data.gct", skip = 2, header = TRUE)

which will start by skipping the first two lines and then reading in the
header row and then the data.

See ?read.table

HTH,

Marc Schwartz



From herodote at oreka.com  Tue Jul 19 18:11:37 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Tue, 19 Jul 2005 17:11:37 +0100
Subject: [R] =?iso-8859-1?q?integrate_fails_with_errors?=
Message-ID: <IJVUBD$ADFCAAA305AC514FF6F7B22126F6973A@oreka.com>

Hi all,
i'm new to R,
I need to modelize in R a statistic algorithm,
This algo use Weibull, normal law, linear regression, normalisation, root mean square, to find eta and beta fitting the weibull model (to analyse few results) and further when we will get more information apply bayes model .

the problem is when When i try to integrate it fails with errors.

by the way i like to integrate something like that :

beta0,eta0,n are initialized as single integer, temp is a 1 dimension array containing 9 integer.

integrate(function(beta) ((beta/(eta0)^beta)^n)*prod(temp^(1-beta)*exp(-sum(temp^beta)/(eta^beta)))*(1/(sqrt(2*pi))*exp(((beta-beta0)^2,0,Inf)

R says : The longuest object isn't a multiple of the shortest object in temp^beta , the same for temp^(1-beta).

I don't understand why R fails to take calculate each temp^beta then sum it over again for each beta values.

don't know if my post is explicit my head is burned out with these things today

thks.

////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From temiz at deprem.gov.tr  Tue Jul 19 19:41:30 2005
From: temiz at deprem.gov.tr (orkun)
Date: Tue, 19 Jul 2005 20:41:30 +0300
Subject: [R] problem in Krig function of "Fields" package
Message-ID: <42DD3B4A.80607@deprem.gov.tr>

hello

I try to build DEM  using Krig function of fields package.
And I get this error message.
here is the procedure I followed:

 > dt<-read.table("/usr/local/bartin/stat/topostat1",header=F,sep="|")
 > names(dt) <-c("x","y","z")
 > coord<-cbind(dt$x,dt$y)
 > elevation<-cbind(dt$z)
 > fit1<-Krig(coord,elevation,cov.function=exp.cov,scale.type="range")
[1] "condition number is 907405545.878743"
Error in Krig(kord, el, cov.function = exp.cov, scale.type = "range") :
        Covariance matrix is close
to
singular

here is the structure  of my data:

`data.frame':   3911 obs. of  3 variables:
 $ x: num  50139 50169 50214 50223 50227 ...
 $ y: num  4592999 4593322 4593054 4593151 4593246 ...
 $ z: int  320 320 320 320 320 320 320 320 320 320 ...

what do you suggest to solve the problem ?

kind regards



______________________________________
XamimeLT - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr
______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From mark.salsburg at gmail.com  Tue Jul 19 19:52:55 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 19 Jul 2005 13:52:55 -0400
Subject: [R] .gct file
In-Reply-To: <1121794215.4124.22.camel@localhost.localdomain>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
	<1121794215.4124.22.camel@localhost.localdomain>
Message-ID: <dd48e20f0507191052396d468d@mail.gmail.com>

This is all extremely helpful.

The data turns out is a little atypical, the columns are tab-delemited
except for the description columns


DATA1.gct looks like this

#1.2
23 3423
NAME DESCRIPTION VALUE
gene1 "a protein inducer" 1123
.....          .................     ......

How do I get R to read the data as tab delemited, but read in the 2nd
coloumn as one value based on the quotation marks..

thanks..

On 7/19/05, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> On Tue, 2005-07-19 at 13:16 -0400, mark salsburg wrote:
> > ok so the gct file looks like this:
> >
> > #1.2  (version number)
> > 7283 19   (matrix size)
> > Name Description Values
> > ....      .......          ......
> >
> > How can I tell R to disregard the first two lines and start reading
> > the 3rd line in this gct file. I would just delete them, but I do not
> > know how to open a gct. file
> >
> > thank you
> >
> > On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > > On 7/19/2005 12:10 PM, mark salsburg wrote:
> > > > I have two files to compare, one is a regular txt file that I can read
> > > > in no prob.
> > > >
> > > > The other is a .gct file (How do I read in this one?)
> > > >
> > > > I tried a simple
> > > >
> > > > read.table("data.gct", header = T)
> > > >
> > > > How do you suggest reading in this file??
> > > >
> > >
> > > .gct is not a standard filename extension.  You need to know what is in
> > > that file.  Where did you get it?  What program created it?
> > >
> > > Chances are the easiest thing to do is to get the program that created
> > > it to export in a well known format, e.g. .csv.
> > >
> > > Duncan Murdoch
> 
> 
> The above would be consistent with the info in my reply.
> 
> I guess if the format is consistent, as per Mark's example above, you
> can use:
> 
> read.table("data.gct", skip = 2, header = TRUE)
> 
> which will start by skipping the first two lines and then reading in the
> header row and then the data.
> 
> See ?read.table
> 
> HTH,
> 
> Marc Schwartz
> 
> 
>



From tlumley at u.washington.edu  Tue Jul 19 19:53:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 19 Jul 2005 10:53:46 -0700 (PDT)
Subject: [R] =?iso-8859-1?q?integrate_fails_with_errors?=
In-Reply-To: <IJVUBD$ADFCAAA305AC514FF6F7B22126F6973A@oreka.com>
References: <IJVUBD$ADFCAAA305AC514FF6F7B22126F6973A@oreka.com>
Message-ID: <Pine.A41.4.61b.0507191052460.33352@homer06.u.washington.edu>

On Tue, 19 Jul 2005, [iso-8859-1] herodote at oreka.com wrote:

>
> beta0,eta0,n are initialized as single integer, temp is a 1 dimension array containing 9 integer.
>
> integrate(function(beta) ((beta/(eta0)^beta)^n)*prod(temp^(1-beta)*exp(-sum(temp^beta)/(eta^beta)))*(1/(sqrt(2*pi))*exp(((beta-beta0)^2,0,Inf)
>
> R says : The longuest object isn't a multiple of the shortest object in temp^beta , the same for temp^(1-beta).
>
> I don't understand why R fails to take calculate each temp^beta then sum it over again for each beta values.
>

The help page for integrate() says
     f: an R function taking a numeric first argument and returning a
           numeric vector of the same length.

Your f is not of that form.

 	-thomas



From ggrothendieck at gmail.com  Tue Jul 19 20:07:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Jul 2005 14:07:11 -0400
Subject: [R] using argument names (of indeterminate number) within a
	function
In-Reply-To: <42DD1C55.8040200@jura.uni-hamburg.de>
References: <42DD1C55.8040200@jura.uni-hamburg.de>
Message-ID: <971536df0507191107454f444d@mail.gmail.com>

On 7/19/05, Dirk Enzmann <dirk.enzmann at jura.uni-hamburg.de> wrote:
> Although I tried to find an answer in the manuals and archives, I cannot
> solve this (please excuse that my English and/or R programming skills
> are not good enough to state my problem more clearly):
> 
> I want to write a function with an indeterminate (not pre-defined)
> number of arguments and think that I should use the "..." construct and
> the match.call() function. The goal is to write a function that (among
> other things) uses cbind() to combine a not pre-defined number of
> vectors specified in the function call. For example, if my vectors are
> x1, x2, x3, ... xn, within the function I want to use cbind(x1, x2) or
> cbind(x1, x3, x5) or ... depending on the vector names I use in the
> funcion call. Additionally, the function has other arguments.
> 
> In the archives I found the following thread (followed by Marc Schwartz)
> 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/15186.html
> [R] returning argument names from Peter Dalgaard BSA on 2003-04-10 (stdin)
> 
> that seems to contain the solution to my problem, but I am stuck because
>  sapply(match.call()[-1], deparse) gives me a vector of strings and I
> don't know how to use the names in this vector in the cbind() function.
> 
> Up to now my (clearly deficit) function looks like:
> 
> test <- function(..., mvalid=1)
> {
>   args = sapply(match.call()[-1], deparse)
> # and here, I don't know how the vector names in args
> # can be used in the cbind() function to follow:
> #
> # temp <- cbind( ???
>   if (mvalid > 1)
>   {
> #  here it goes on
>   }
> }
> 
> Ultimately, I want that the function can be called like
> test(x1,x2,mvalid=1)
> or
> test(x1,x3,x5,mavlid=2)
> and that within the function
> cbind(x1,x2)
> or cbind(x1,x3,x5)
> will be used.
> 

If you just need to pass them to cbind then just use cbind(...), e.g.

  test <- function(..., m) if (m > 1) cbind(...) else m

otherwise, use list(...) as shown by a previous answer or here

  test2 <- function(..., m) if (m > 1) length(list(...)) else m



From mschwartz at mn.rr.com  Tue Jul 19 20:08:15 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 19 Jul 2005 13:08:15 -0500
Subject: [R] .gct file
In-Reply-To: <dd48e20f0507191052396d468d@mail.gmail.com>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
	<1121794215.4124.22.camel@localhost.localdomain>
	<dd48e20f0507191052396d468d@mail.gmail.com>
Message-ID: <1121796495.4124.30.camel@localhost.localdomain>

For the TAB delimited columns, adjust the 'sep' argument to:

read.table("data.gct", skip = 2, header = TRUE, sep = "\t")

The 'quote' argument is by default:

quote = "\"'"

which should take care of the quoted strings and bring them in as a
single value.

The above presumes that the header row is also TAB delimited. If not,
you may have to set 'skip = 3' to skip over the header row and manually
set the column names.

HTH,

Marc Schwartz


On Tue, 2005-07-19 at 13:52 -0400, mark salsburg wrote:
> This is all extremely helpful.
> 
> The data turns out is a little atypical, the columns are tab-delemited
> except for the description columns
> 
> 
> DATA1.gct looks like this
> 
> #1.2
> 23 3423
> NAME DESCRIPTION VALUE
> gene1 "a protein inducer" 1123
> .....          .................     ......
> 
> How do I get R to read the data as tab delemited, but read in the 2nd
> coloumn as one value based on the quotation marks..
> 
> thanks..
> 
> On 7/19/05, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> > On Tue, 2005-07-19 at 13:16 -0400, mark salsburg wrote:
> > > ok so the gct file looks like this:
> > >
> > > #1.2  (version number)
> > > 7283 19   (matrix size)
> > > Name Description Values
> > > ....      .......          ......
> > >
> > > How can I tell R to disregard the first two lines and start reading
> > > the 3rd line in this gct file. I would just delete them, but I do not
> > > know how to open a gct. file
> > >
> > > thank you
> > >
> > > On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > > > On 7/19/2005 12:10 PM, mark salsburg wrote:
> > > > > I have two files to compare, one is a regular txt file that I can read
> > > > > in no prob.
> > > > >
> > > > > The other is a .gct file (How do I read in this one?)
> > > > >
> > > > > I tried a simple
> > > > >
> > > > > read.table("data.gct", header = T)
> > > > >
> > > > > How do you suggest reading in this file??
> > > > >
> > > >
> > > > .gct is not a standard filename extension.  You need to know what is in
> > > > that file.  Where did you get it?  What program created it?
> > > >
> > > > Chances are the easiest thing to do is to get the program that created
> > > > it to export in a well known format, e.g. .csv.
> > > >
> > > > Duncan Murdoch
> > 
> > 
> > The above would be consistent with the info in my reply.
> > 
> > I guess if the format is consistent, as per Mark's example above, you
> > can use:
> > 
> > read.table("data.gct", skip = 2, header = TRUE)
> > 
> > which will start by skipping the first two lines and then reading in the
> > header row and then the data.
> > 
> > See ?read.table
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> > 
> >



From p.dalgaard at biostat.ku.dk  Tue Jul 19 20:08:40 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jul 2005 20:08:40 +0200
Subject: [R] Michaelis-menten equation
In-Reply-To: <x24qarqguk.fsf@turmalin.kubism.ku.dk>
References: <20050719084736.M53999@cc.kmu.edu.tw>
	<x24qarqguk.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2fyua4qjb.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> "Chun-Ying Lee" <u9370004 at cc.kmu.edu.tw> writes:
> 
> > Dear R users:
> >    I encountered difficulties in michaelis-menten equation. I found 
> > that when I use right model definiens, I got wrong Km vlaue, 
> > and I got right Km value when i use wrong model definiens. 
> > The value of Vd and Vmax are correct in these two models. 
> 
> How do you know what the correct value is? Are you sure that the other
> values are right?
> 
> I'm a bit rusty on MM, but are you sure your "right" model is right?
> Try doing a dimensional analysis on the ODE. I kind of suspect that
> Vd is entering in the wrong way. Since you're dealing in
> concentrations, should it enter at all (except via the conc. at time
> 0, of course)?
> 
> Not knowing the context, I can't be quite sure, but generally, I'd
> expect Vm*Km/(Km+y) to be the reaction rate, so that Vm is the maximum
> rate, attained when y is zero and Km is the conc. at half-maximum
> rate. This doesn't look quit like what you have. 

Hmm, sorry, no. I'm talking through a hole in my head there.

Vm*y/(Km+y) makes OK sense. Vm is what you get for large y - passing
from 1st order to 0th order kinetics. However, looking at the data

 plot(PKindex)
 abline(lm(conc~time,data=PKindex))

shows that they are pretty much on a straight line, i.e. you are 
in the domain of 0-order kinetics. So why are you expecting the rate
of decrease to have changed by roughly 3/4 (from 2/3*Vm/Vd at y=2*Km
to 1/2*Vm/Vd at y=Km when you reach 4.67)??
  
> > #-----right model definiens--------
> > PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
> >        conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
> > mm.model <- function(time, y, parms) { 
> >        dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]+y[1]) 
> >        list(dCpdt)}
> > Dose<-300
> > modfun <- function(time,Vm,Km,Vd) { 
> >        out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
> >               rtol=1e-8,atol=1e-8)
> >           out[,2] } 
> > objfun <- function(par) { 
> >    out <- modfun(PKindex$time,par[1],par[2],par[3]) 
> >    sum((PKindex$conc-out)^2) } 
> > fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
> > print(fit$par)
> > [1] 10.0390733  0.1341544 34.9891829  #--Km=0.1341544,wrong value--
> > 
> > 
> > #-----wrong model definiens--------
> > #-----Km should not divided by Vd--
> > PKindex<-data.frame(time=c(0,1,2,4,6,8,10,12,16,20,24),
> >        conc=c(8.57,8.30,8.01,7.44,6.88,6.32,5.76,5.20,4.08,2.98,1.89))
> > mm.model <- function(time, y, parms) { 
> >    dCpdt <- -(parms["Vm"]/parms["Vd"])*y[1]/(parms["Km"]/parms["Vd"]+y[1]) 
> >    list(dCpdt)}
> > Dose<-300
> > modfun <- function(time,Vm,Km,Vd) { 
> > out <- lsoda(Dose/Vd,time,mm.model,parms=c(Vm=Vm,Km=Km,Vd=Vd),
> >             rtol=1e-8,atol=1e-8)
> >        out[,2] 
> > } 
> > objfun <- function(par) { 
> >     out <- modfun(PKindex$time,par[1],par[2],par[3]) 
> >     sum((PKindex$conc-out)^2)} 
> > fit <- optim(c(10,1,80),objfun, method="Nelder-Mead)
> > print(fit$par)
> > [1] 10.038821  4.690267 34.989239  #--Km=4.690267,right value--
> > 
> > What did I do wrong, and how to fix it?
> > Any suggestions would be greatly appreciated.
> > Thanks in advance!!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From MSchwartz at mednetstudy.com  Tue Jul 19 20:11:21 2005
From: MSchwartz at mednetstudy.com (Marc Schwartz)
Date: Tue, 19 Jul 2005 13:11:21 -0500
Subject: [R] .gct file
In-Reply-To: <1121796495.4124.30.camel@localhost.localdomain>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
	<1121794215.4124.22.camel@localhost.localdomain>
	<dd48e20f0507191052396d468d@mail.gmail.com>
	<1121796495.4124.30.camel@localhost.localdomain>
Message-ID: <1121796681.4124.33.camel@localhost.localdomain>

On Tue, 2005-07-19 at 13:08 -0500, Marc Schwartz (via MN) wrote:
> For the TAB delimited columns, adjust the 'sep' argument to:
> 
> read.table("data.gct", skip = 2, header = TRUE, sep = "\t")
> 
> The 'quote' argument is by default:
> 
> quote = "\"'"
> 
> which should take care of the quoted strings and bring them in as a
> single value.
> 
> The above presumes that the header row is also TAB delimited. If not,
> you may have to set 'skip = 3' to skip over the header row and manually
> set the column names.

One correction. If the final para applies and you need to use 'skip =
3', you would also need to leave out the 'header = TRUE' argument, which
defaults to FALSE.

Marc



From mschwartz at mn.rr.com  Tue Jul 19 20:11:32 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 19 Jul 2005 13:11:32 -0500
Subject: [R] .gct file
In-Reply-To: <1121796495.4124.30.camel@localhost.localdomain>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
	<1121794215.4124.22.camel@localhost.localdomain>
	<dd48e20f0507191052396d468d@mail.gmail.com>
	<1121796495.4124.30.camel@localhost.localdomain>
Message-ID: <1121796692.4124.35.camel@localhost.localdomain>

On Tue, 2005-07-19 at 13:08 -0500, Marc Schwartz (via MN) wrote:
> For the TAB delimited columns, adjust the 'sep' argument to:
> 
> read.table("data.gct", skip = 2, header = TRUE, sep = "\t")
> 
> The 'quote' argument is by default:
> 
> quote = "\"'"
> 
> which should take care of the quoted strings and bring them in as a
> single value.
> 
> The above presumes that the header row is also TAB delimited. If not,
> you may have to set 'skip = 3' to skip over the header row and manually
> set the column names.

One correction. If the final para applies and you need to use 'skip =
3', you would also need to leave out the 'header = TRUE' argument, which
defaults to FALSE.

Marc



From mgreene at PRAC.com  Tue Jul 19 20:28:33 2005
From: mgreene at PRAC.com (Greene, Michael)
Date: Tue, 19 Jul 2005 14:28:33 -0400
Subject: [R] CPU Usage with R 2.1.0 in Windows
Message-ID: <4D2300C2F8092C45AFC70A1D04CD9A05137298@PRCMAILNE.prcins.net>


Hi,

I'm using a fairly simple HP Compaq desktop PC running Windows 2K.  When
running a large process in R, the process "RGUI.exe" will never exceed 50%
of the CPU usage.

The program used to be able to use more of the computer, but does not now.
I don't believe this is a multiple processor machine.

Can anyone give any advice on how to solve the problem?  

Thanks,

Michael Greene

Product Management
Plymouth Rock Assurance Corp
617-951-1682



From f.harrell at vanderbilt.edu  Tue Jul 19 20:42:52 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 19 Jul 2005 13:42:52 -0500
Subject: [R] ROC curve with survival data
In-Reply-To: <FAD63B0F78D0224A9AF352A4A98F1E2A588C36@jupiter.imim.es>
References: <FAD63B0F78D0224A9AF352A4A98F1E2A588C36@jupiter.imim.es>
Message-ID: <42DD49AC.7070506@vanderbilt.edu>

SUBIRANA CACHINERO, ISAAC wrote:
> Hi everyone, 
> 
> I am doing 5 years mortality predictive index score with survival analysis using a Cox proportional hazard model where I have a continous predictive variable and a right censored response which is the mortality, and the individuals were followed a maximum of 7 years.
> 
> I'd like to asses the discrimination ability of survival analysis Cox model by computing a ROC curve and area under the curve for a fixed time (5 years), taking into account that the response is not binary but right censored.
> 
> So, is there a function that computes a ROC curve under right censored survival data?
> 
> Thank you in advance.

I don't find ROC curves themselves very useful but the area under them 
is useful and is a simple translation of the Somers' Dxy rank 
correlation between predicted survival probability (or anything 
monotonically related to it just is log hazard) and observed survival 
time.  Dxy = 2*(C-.5) where C is the concordance index, a generalization 
of ROC area not requiring choosing a specific time point.  You can get 
this in the rcorr.cens function in the Hmisc package.

Frank

> 
>  
> 
>  
> 
> Isaac Subirana: isubirana at imim.es
> 
> Institut Municipal d'Investigaci?? M??dica (IMIM),
> 
> Barcelona (Spain)
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jp.mcdermott at gmail.com  Tue Jul 19 20:53:55 2005
From: jp.mcdermott at gmail.com (James McDermott)
Date: Tue, 19 Jul 2005 14:53:55 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
Message-ID: <33871de605071911531e399f4d@mail.gmail.com>

Hello,

I have been trying to take the derivative of a quadratic B-spline
obtained by using the COBS library.  What I would like to do is
similar to what one can do by using

fit<-smooth.spline(cdf)
xx<-seq(-10,10,.1)
predict(fit, xx, deriv = 1)

The goal is to fit the spline to data that is approximating a
cumulative distribution function (e.g. in my example, cdf is a
2-column matrix with x values in column 1 and the estimate of the cdf
evaluated at x in column 2) and then take the first derivative over a
range of values to get density estimates.

The reason I don't want to use smooth.spline is that there is no way
to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
with COBS.  However, since COBS doesn't have the 'deriv =' option, the
only way I can think of doing it with COBS is to evaluate the
derivatives numerically.

Regards,
Jim McDermott



From HDoran at air.org  Tue Jul 19 20:59:34 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 19 Jul 2005 14:59:34 -0400
Subject: [R] CPU Usage with R 2.1.0 in Windows
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409993F97@dc1ex2.air.org>

Dear Michael:

Why is it a problem that R is not using more CPU space than it seems to
need? 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greene, Michael
Sent: Tuesday, July 19, 2005 2:29 PM
To: 'R-help at lists.R-project.org'
Subject: [R] CPU Usage with R 2.1.0 in Windows


Hi,

I'm using a fairly simple HP Compaq desktop PC running Windows 2K.  When
running a large process in R, the process "RGUI.exe" will never exceed
50% of the CPU usage.

The program used to be able to use more of the computer, but does not
now.
I don't believe this is a multiple processor machine.

Can anyone give any advice on how to solve the problem?  

Thanks,

Michael Greene

Product Management
Plymouth Rock Assurance Corp
617-951-1682

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Tue Jul 19 21:07:23 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Jul 2005 15:07:23 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
In-Reply-To: <33871de605071911531e399f4d@mail.gmail.com>
References: <33871de605071911531e399f4d@mail.gmail.com>
Message-ID: <42DD4F6B.7090600@stats.uwo.ca>

On 7/19/2005 2:53 PM, James McDermott wrote:
> Hello,
> 
> I have been trying to take the derivative of a quadratic B-spline
> obtained by using the COBS library.  What I would like to do is
> similar to what one can do by using
> 
> fit<-smooth.spline(cdf)
> xx<-seq(-10,10,.1)
> predict(fit, xx, deriv = 1)
> 
> The goal is to fit the spline to data that is approximating a
> cumulative distribution function (e.g. in my example, cdf is a
> 2-column matrix with x values in column 1 and the estimate of the cdf
> evaluated at x in column 2) and then take the first derivative over a
> range of values to get density estimates.
> 
> The reason I don't want to use smooth.spline is that there is no way
> to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
> with COBS.  However, since COBS doesn't have the 'deriv =' option, the
> only way I can think of doing it with COBS is to evaluate the
> derivatives numerically.

Numerical estimates of the derivatives of a quadratic should be easy to 
obtain accurately.  For example, if the quadratic ax^2 + bx + c is 
defined on [-1, 1], then the derivative 2ax + b, has 2a = f(1) - f(0) + 
f(-1), and b = (f(1) - f(-1))/2.

You should be able to generalize this to the case where the spline is 
quadratic between knots k1 and k2 pretty easily.

Duncan Murdoch



From gregory_gentlemen at yahoo.ca  Tue Jul 19 21:11:42 2005
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Tue, 19 Jul 2005 15:11:42 -0400 (EDT)
Subject: [R] Question about creating unique factor labels with the factor
	function
Message-ID: <20050719191143.32637.qmail@web31213.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/e88f5bdf/attachment.pl

From luke at novum.am.lublin.pl  Tue Jul 19 21:20:54 2005
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Tue, 19 Jul 2005 21:20:54 +0200
Subject: [R] CPU Usage with R 2.1.0 in Windows
In-Reply-To: <4D2300C2F8092C45AFC70A1D04CD9A05137298@PRCMAILNE.prcins.net>
References: <4D2300C2F8092C45AFC70A1D04CD9A05137298@PRCMAILNE.prcins.net>
Message-ID: <42DD5296.4020509@novum.am.lublin.pl>

Dnia 2005-07-19 20:28, Uytkownik Greene, Michael napisa:

> Hi,
> 
> I'm using a fairly simple HP Compaq desktop PC running Windows 2K.  When
> running a large process in R, the process "RGUI.exe" will never exceed 50%
> of the CPU usage.

If you have hyperthreading, R catches only one virtual processor (from
two available), being not able to exceed half of total power (100% of
one only). If you want to use full power, you should turn hyperthreading
off, if your BIOS supports such option.

Regards,

-- 
Lukasz Komsta
Department of Medicinal Chemistry
Medical University of Lublin
6 Chodzki, 20-093 Lublin, Poland
Fax +48 81 7425165



From jp.mcdermott at gmail.com  Tue Jul 19 21:34:52 2005
From: jp.mcdermott at gmail.com (James McDermott)
Date: Tue, 19 Jul 2005 15:34:52 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
In-Reply-To: <42DD4F6B.7090600@stats.uwo.ca>
References: <33871de605071911531e399f4d@mail.gmail.com>
	<42DD4F6B.7090600@stats.uwo.ca>
Message-ID: <33871de605071912341b778fdb@mail.gmail.com>

I wish it were that simple (perhaps it is and I am just not seeing
it).  The output from cobs( ) includes the B-spline coefficients and
the knots.  These coefficients are not the same as the a, b, and c
coefficients in a quadratic polynomial.  Rather, they are the
coefficients of the quadratic B-spline representation of the fitted
curve.  I need to evaluate a linear combination of basis functions and
it is not clear to me how to accomplish this easily.  I was hoping to
find an alternative way of getting the derivatives.

Jim McDermott

On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/19/2005 2:53 PM, James McDermott wrote:
> > Hello,
> >
> > I have been trying to take the derivative of a quadratic B-spline
> > obtained by using the COBS library.  What I would like to do is
> > similar to what one can do by using
> >
> > fit<-smooth.spline(cdf)
> > xx<-seq(-10,10,.1)
> > predict(fit, xx, deriv = 1)
> >
> > The goal is to fit the spline to data that is approximating a
> > cumulative distribution function (e.g. in my example, cdf is a
> > 2-column matrix with x values in column 1 and the estimate of the cdf
> > evaluated at x in column 2) and then take the first derivative over a
> > range of values to get density estimates.
> >
> > The reason I don't want to use smooth.spline is that there is no way
> > to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
> > with COBS.  However, since COBS doesn't have the 'deriv =' option, the
> > only way I can think of doing it with COBS is to evaluate the
> > derivatives numerically.
> 
> Numerical estimates of the derivatives of a quadratic should be easy to
> obtain accurately.  For example, if the quadratic ax^2 + bx + c is
> defined on [-1, 1], then the derivative 2ax + b, has 2a = f(1) - f(0) +
> f(-1), and b = (f(1) - f(-1))/2.
> 
> You should be able to generalize this to the case where the spline is
> quadratic between knots k1 and k2 pretty easily.
> 
> Duncan Murdoch
>



From reid_huntsinger at merck.com  Tue Jul 19 21:43:13 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 19 Jul 2005 15:43:13 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A952D@uswpmx00.merck.com>

The derivative of a quadratic B-spline is the centered finite difference of
a linear B-spline, so if you have access to the underlying coefficients of
the B-spline expansion you can do this easily. I believe the coefficients
are passed as the $coef component of the return value.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of James McDermott
Sent: Tuesday, July 19, 2005 2:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Taking the derivative of a quadratic B-spline


Hello,

I have been trying to take the derivative of a quadratic B-spline
obtained by using the COBS library.  What I would like to do is
similar to what one can do by using

fit<-smooth.spline(cdf)
xx<-seq(-10,10,.1)
predict(fit, xx, deriv = 1)

The goal is to fit the spline to data that is approximating a
cumulative distribution function (e.g. in my example, cdf is a
2-column matrix with x values in column 1 and the estimate of the cdf
evaluated at x in column 2) and then take the first derivative over a
range of values to get density estimates.

The reason I don't want to use smooth.spline is that there is no way
to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
with COBS.  However, since COBS doesn't have the 'deriv =' option, the
only way I can think of doing it with COBS is to evaluate the
derivatives numerically.

Regards,
Jim McDermott

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Tue Jul 19 21:58:48 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Jul 2005 15:58:48 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
In-Reply-To: <33871de605071912341b778fdb@mail.gmail.com>
References: <33871de605071911531e399f4d@mail.gmail.com>	
	<42DD4F6B.7090600@stats.uwo.ca>
	<33871de605071912341b778fdb@mail.gmail.com>
Message-ID: <42DD5B78.40906@stats.uwo.ca>

On 7/19/2005 3:34 PM, James McDermott wrote:
> I wish it were that simple (perhaps it is and I am just not seeing
> it).  The output from cobs( ) includes the B-spline coefficients and
> the knots.  These coefficients are not the same as the a, b, and c
> coefficients in a quadratic polynomial.  Rather, they are the
> coefficients of the quadratic B-spline representation of the fitted
> curve.  I need to evaluate a linear combination of basis functions and
> it is not clear to me how to accomplish this easily.  I was hoping to
> find an alternative way of getting the derivatives.

I don't know COBS, but doesn't predict just evaluate the B-spline?  The 
point of what I posted is that the particular basis doesn't matter if 
you can evaluate the quadratic at 3 points.

Duncan Murdoch

> 
> Jim McDermott
> 
> On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 7/19/2005 2:53 PM, James McDermott wrote:
>> > Hello,
>> >
>> > I have been trying to take the derivative of a quadratic B-spline
>> > obtained by using the COBS library.  What I would like to do is
>> > similar to what one can do by using
>> >
>> > fit<-smooth.spline(cdf)
>> > xx<-seq(-10,10,.1)
>> > predict(fit, xx, deriv = 1)
>> >
>> > The goal is to fit the spline to data that is approximating a
>> > cumulative distribution function (e.g. in my example, cdf is a
>> > 2-column matrix with x values in column 1 and the estimate of the cdf
>> > evaluated at x in column 2) and then take the first derivative over a
>> > range of values to get density estimates.
>> >
>> > The reason I don't want to use smooth.spline is that there is no way
>> > to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
>> > with COBS.  However, since COBS doesn't have the 'deriv =' option, the
>> > only way I can think of doing it with COBS is to evaluate the
>> > derivatives numerically.
>> 
>> Numerical estimates of the derivatives of a quadratic should be easy to
>> obtain accurately.  For example, if the quadratic ax^2 + bx + c is
>> defined on [-1, 1], then the derivative 2ax + b, has 2a = f(1) - f(0) +
>> f(-1), and b = (f(1) - f(-1))/2.
>> 
>> You should be able to generalize this to the case where the spline is
>> quadratic between knots k1 and k2 pretty easily.
>> 
>> Duncan Murdoch
>>



From abunn at whrc.org  Tue Jul 19 21:58:46 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 19 Jul 2005 15:58:46 -0400
Subject: [R] extracting row means from a list
In-Reply-To: <42DD1DAF.9040102@pdf.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEECCDIAA.abunn@whrc.org>

I think about half of my question in R can be solved with a judicious
do.call.

Thanks, Andy



From sethjpruitt at gmail.com  Tue Jul 19 22:10:13 2005
From: sethjpruitt at gmail.com (Seth Pruitt)
Date: Tue, 19 Jul 2005 13:10:13 -0700
Subject: [R] Using BRugs, in FUN: .C(..) 'type' not "real"
Message-ID: <ed55c06b0507191310ab753b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/8ba76def/attachment.pl

From LI at nsabp.pitt.edu  Tue Jul 19 22:37:20 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Tue, 19 Jul 2005 16:37:20 -0400
Subject: [R] A warning message when using mix package.
Message-ID: <3D0B2434377E984E9C85CAA316F8B18301B35936@nsabpmail>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/8a7121d5/attachment.pl

From ghjmora at gmail.com  Tue Jul 19 22:50:20 2005
From: ghjmora at gmail.com (ghjmora g mail)
Date: Tue, 19 Jul 2005 22:50:20 +0200
Subject: [R] sciviews installation
Message-ID: <42DD678C.2030700@gmail.com>

Hello


1. a few months ago, I had sciviews working fine with R (rw2001) under 
windows XP
2. now, upgrading to rw2011, the stuff seems fine (every package 
installed),but I find a conflict when launching sciviews:
- it runs, apparently
- but when I try to work ("import/export In: text" for instance), it 
asks for Rcmdr ("Would you like to install it now?")

3. Rcmdr is already installed (with all dependencies) and works well 
when called directly in R gui
4. and it's impossible to make it reconized or to install it under sciviews


I have all the latest packages, and I am going to get mad.

what do you suggest to solve my problem ?

Thanks

Georges Moracchini



From plummer at iarc.fr  Tue Jul 19 23:18:27 2005
From: plummer at iarc.fr (plummer@iarc.fr)
Date: Tue, 19 Jul 2005 23:18:27 +0200
Subject: [R] initial points for arms in package HI
In-Reply-To: <17117.12889.773438.382880@stat.math.ethz.ch>
References: <17117.12889.773438.382880@stat.math.ethz.ch>
Message-ID: <1121807907.42dd6e23f2e8e@webmail.iarc.fr>

Quoting Christoph Buser <buser at stat.math.ethz.ch>:

> Dear R-users
>
> I have a problem choosing initial points for the function arms()
> in the package HI
> I intend to implement a Gibbs sampler and one of my conditional
> distributions is nonstandard and not logconcave.
> Therefore I'd like to use arms.
>
> But there seem to be a strong influence of the initial point
> y.start. To show the effect I constructed a demonstration
> example. It is reproducible without further information.
>
> Please note that my target density is not logconcave.
>
> Thanks for all comments or ideas.
>
> Christoph Buser

Dear Christoph,

There is a Metropolis step at each iteration of the ARMS sampler, in
which it may choose to reject the proposed move to a new point and stick
at the current point (This is what the "M" in "ARMS" stands for)  If you
do repeated calls to arms with the same starting point, then the
iterations where the Metropolis step rejects a move will create a spike
in the sample density at your initial value. If you use a uniform random
starting point, then your sample density will be a mixture of the
target distribution (Metropolis accepts move) and a uniform distribution
(Metropolis rejects move).

You should be doing something like this:

res1 <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1)
for(i in 2:1000)
  res1[i] <- arms(res1[i-1], logDichteGam, function(x) (x>0)&(x<100), 1)

i.e., using each sampled point as the starting value for the next
iteration.  The sequence of values in res1 will then be a correlated
sample from the given distribution:

acf(res1)

The bottom line is that you can't use ARMS to draw a single sample
from a non-log-concave density.

If you are still worried about using ARMS, you can verify your results
using the random walk Metropolis sampler (MCMCmetrop1R) in the package
MCMCpack.

Martyn

> ## R Code:
>
> library(HI)
> ## parameter for the distribution
> para <- 0.1
>
> ## logdensity
> logDichteGam <- function(x, u = para, v = para) {
>   -(u*x + v*1/x) - log(x)
> }
> ## density except for the constant
> propDichteGam <- function(x, u = para, v = para) {
>   exp(-(u*x + v*1/x) - log(x))
> }
> ## calculating the constant
> (c <- integrate(propDichteGam, 0, 1000, rel.tol = 10^(-12))$value)
> ## density
> DichteGam <- function(x, u = para, v = para) {
>   exp(-(u*x + v*1/x) - log(x))/c
> }
>
> ## calculating 1000 values by repeating a call of arms (this would
> ## be the situation in an Gibbs Sample. Of course in a Gibbs sampler
> ## the distribution would change. This is only for demonstration
> res1 <- NULL
> for(i in 1:1000)
>   res1[i] <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1)
>
> ## Generating a sample of thousand observations with 1 call of arms
> res2 <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1000)
>
> ## Plot of the samples
> mult.fig(4)
> plot(res1, log = "y")
> plot(res2, log = "y")
> hist(res1, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
>      ylim = c(0,1))
> curve(DichteGam, 0,4, add = TRUE, col = 2)
> hist(res2, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
>      ylim = c(0,1))
> curve(DichteGam, 0,4, add = TRUE, col = 2)
>
>
> ## If we repeat the procedure, using the fix intial value 1,
> ## the situation is even worse
> res3 <- NULL
> for(i in 1:1000)
>   res3[i] <- arms(1, logDichteGam, function(x) (x>0)&(x<100), 1)
>
> ## Generating a sample of thousand observations with 1 call of arms
> res4 <- arms(1, logDichteGam, function(x) (x>0)&(x<100), 1000)
>
> ## Plot of the samples
> par(mfrow = c(2,2))
> plot(res3, log = "y")
> plot(res4, log = "y")
> hist(res3, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
>      ylim = c(0,1))
> curve(DichteGam, 0,4, add = TRUE, col = 2)
> hist(res4, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
>      ylim = c(0,1))
> curve(DichteGam, 0,4, add = TRUE, col = 2)
>
>
> ## If I generate the sample in a for-loop (one by one) I do not
> ## get the correct density. But this is exactly the situation in
> ## my Gibbs Sampler. Therfore I am concerned about the correct
> ## application of arms
>



-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From liuwensui at gmail.com  Tue Jul 19 23:22:32 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 19 Jul 2005 17:22:32 -0400
Subject: [R] Is it possible to create highly customized report in *.xls
	format by using R/S+?
Message-ID: <1115a2b005071914229f14377@mail.gmail.com>

I remember in one slide of Prof. Ripley's presentation overhead, he
said the most popular data analysis software is excel.

So is there any resource or tutorial on this topic? 

Thank you so much!



From carsten.steinhoff at stud.uni-goettingen.de  Tue Jul 19 23:59:14 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Tue, 19 Jul 2005 23:59:14 +0200
Subject: [R] Problems with date-format (R 2.1.1 + chron)
Message-ID: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>

Hello,
 
today I've updated on the newest R-Version. But sadly a function I needed
didnt want to work:
The input is e.g.
 
days(as.Date("21-07-2005","%d-%m-%y"))

the error is: Fehler in Math.Date(dts): floor nicht definiert f??r Date
Objekte
(Error in Math.Date(dts): floor not defined for date objects)

Same for year. Only months gives me the correct output.
In Version 2.01 it worked very well, with the same chron library.
Whats wrong ?
 
Carsten



From rhodesju at ohsu.edu  Wed Jul 20 00:09:39 2005
From: rhodesju at ohsu.edu (Justin Rhodes)
Date: Tue, 19 Jul 2005 15:09:39 -0700
Subject: [R] Indexing within scan
Message-ID: <s2dd17bf.043@ohsu.edu>

Dear R-help subscribers,

Can some one please help me figure out how to write code that will allow me to use a for loop to scan a number of files one by one, and then save a summary of each file as the for loop progresses.  For example I have 24 files named a1 through a24, and I want to do something like:


results<-numeric(24)
for (i in 1:24)
{
 
p<-scan("ai.txt")  # where the i is an index for each of the 24 files

results[i]<-mean(p)
}

Thanks for any help,





Justin Rhodes
Behavioral Neuroscience
Oregon Health & Science University
VA Medical Center (R & D 12)
3710 SW US Veterans Hospital Rd
Portland, OR  97239
Phone: (503) 220-8262 extn 54392
Fax: (503) 721-1029
E-mail: rhodesju at ohsu.edu



From p.dalgaard at biostat.ku.dk  Wed Jul 20 00:22:35 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 00:22:35 +0200
Subject: [R] Indexing within scan
In-Reply-To: <s2dd17bf.043@ohsu.edu>
References: <s2dd17bf.043@ohsu.edu>
Message-ID: <x27jfm4es4.fsf@turmalin.kubism.ku.dk>

"Justin Rhodes" <rhodesju at ohsu.edu> writes:

> Dear R-help subscribers,
> 
> Can some one please help me figure out how to write code that will allow me to use a for loop to scan a number of files one by one, and then save a summary of each file as the for loop progresses.  For example I have 24 files named a1 through a24, and I want to do something like:
> 
> 
> results<-numeric(24)
> for (i in 1:24)
> {
>  
> p<-scan("ai.txt")  # where the i is an index for each of the 24 files
> 
> results[i]<-mean(p)
> }
> 
> Thanks for any help,
> 

paste("a", i, ".txt", sep="") 

should get you in the right direction. However, I don't think you
*really* want a for loop. Use vectorization and sapply instead:

names <- paste("a", 1:24, ".txt", sep="")
results <- sapply(names, function(n) mean(scan(n)) )

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From andy_liaw at merck.com  Wed Jul 20 00:24:56 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Jul 2005 18:24:56 -0400
Subject: [R] extracting row means from a list
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAD0@usctmx1106.Merck.com>

Justone more comment in addition to Sundar's solution:

If these are all numeric matrices, I would read them into R
as such, instead of data frames.  Actually, I would read them
all into a 3-dimensional array (2000 x 6 x # of files).
Assuming you have such an array, then you can do something
like:

> ## get your example into an array: need the abind package.
> bar <- do.call("abind", c(lapply(foo, as.matrix), along=3))
> str(bar)
 num [1:100, 1:6, 1:10] -0.78981  0.31939 -0.00819  1.59346  1.20498 ...
 - attr(*, "dimnames")=List of 3
  ..$ : chr [1:100] "1" "2" "3" "4" ...
  ..$ : chr [1:6] "x1" "x2" "x3" "x4" ...
  ..$ : NULL
> str(m <- rowMeans(bar, dims=2))
 num [1:100, 1:6] -0.4401  0.5463 -0.0572 -0.1314  0.5177 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:100] "1" "2" "3" "4" ...
  ..$ : chr [1:6] "x1" "x2" "x3" "x4" ...

Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andy Bunn
> Sent: Tuesday, July 19, 2005 3:59 PM
> To: Sundar Dorai-Raj
> Cc: R-Help
> Subject: Re: [R] extracting row means from a list
> 
> 
> I think about half of my question in R can be solved with a judicious
> do.call.
> 
> Thanks, Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From rotella at montana.edu  Wed Jul 20 01:11:46 2005
From: rotella at montana.edu (Rotella, Jay)
Date: Tue, 19 Jul 2005 17:11:46 -0600
Subject: [R] deriv - accessing numeric output for gradient
Message-ID: <CCAEE0D3A0105A41BD601F222EBEDB6C0BD59E@GEMSTONES.msu.montana.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050719/c31272d9/attachment.pl

From jayrotella at msn.com  Wed Jul 20 01:18:51 2005
From: jayrotella at msn.com (Jay Rotella)
Date: Tue, 19 Jul 2005 17:18:51 -0600
Subject: [R] deriv - accessing numeric output listed under gradient attribute
Message-ID: <BAY11-DAV5B8DEB772D50052CD4619DAD40@phx.gbl>

Hi,
I am interested in using the numeric output from the "gradient" attribute of 
deriv's output in subsequent analyses.
But, I have so far been unable to determine how to do so.

I will use the example from the deriv help to illustrate.

> ## function with defaulted arguments:
>    (fx <- deriv(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
                  function(b0, b1, th, x = 1:7){} ) )
>     fx(2,3,4)

This yields
[1] 4.522689 4.121320 3.783811 3.500000 3.261345 3.060660 2.891905
attr(,"gradient")
     b0        b1        th
[1,]  1 0.8408964 0.1092872
[2,]  1 0.7071068 0.1837984
[3,]  1 0.5946036 0.2318331
[4,]  1 0.5000000 0.2599302
[5,]  1 0.4204482 0.2732180
[6,]  1 0.3535534 0.2756976
[7,]  1 0.2973018 0.2704720

I would greatly appreciate it if anyone could tell me how to convert the 
numbers listed under "b0", "b1", and "th" into a matrix.

Thanks!

Jay Rotella
Ecology Department
Montana State University
Bozeman, MT 59717



From marxlau1 at msu.edu  Wed Jul 20 01:53:23 2005
From: marxlau1 at msu.edu (Laura M Marx)
Date: Tue, 19 Jul 2005 19:53:23 -0400
Subject: [R] Regression lines for differently-sized groups on the same plot
Message-ID: <E1Dv1ts-00085Q-7O@sys25.mail.msu.edu>

Hi there,
  I've looked through the very helpful advice about adding fitted lines to 
plots in the r-help archive, and can't find a post where someone has offered 
a solution for my specific problem.  I need to plot logistic regression fits 
from three differently-sized data subsets on a plot of the entire dataset.  
A description and code are below:
  I have an unbalanced dataset consisting of three different species (hem, 
yb, and sm), with unequal numbers of wood pieces in each species group.  I 
am trying to generate a plot that will show the size of the wood piece on 
the X axis, the probability of it having tree seedlings growing on it on the 
Y (a binomial yes or no variable), and three fitted curves showing how the 
probability of having tree seedlings changes with increasing wood piece size 
for each species.
  I have no problem generating fits using GLM, and no problem creating the 
plot.  However, if I try to add a fitted curve based only on the hem data 
subset to a plot that shows the entire dataset, I get an error message that 
the lengths of those data sets differ. "Error in xy.coords(x,y) : x and y 
lengths differ".  I could see R's point -- you can't plot a regression line 
of babies born as a function of stork abundance on a graph of cherries 
produced (Y) versus rainfall (X), which for all the program knows, I'm 
trying to do.  As a temporary fix, I added NAs to the end of the hem, yb, 
and sm subsets to make them the same length as the entire dataset.  I can 
now add my fitted curves to the plot, but the lines are not connected.  That 
is, if the hem group only contains wood pieces that are 1, 4, and 10 meters 
long, the plot has an X axis that ranges from 1 to 10, but line segments for 
the hem group regression line only appear above 1, 4, and 10.  How can I fix 
this?  An ideal solution would not require me to make the hem subset of my 
data the same length as the full dataset, either (although the summaries of 
regressions with the NAs (or zeroes) added and taken away are identical).  
I'd also settle for a work-around that would have R connect the pieces of 
the curve so that I get a solid line rather than small dots and dashes where 
actual data exist.  Thanks so much for your help!
  Laura Marx
  Michigan State University, Dept. of Forestry 

#Note: hemdata has all the rows that are not hemlock species replaced with 
#"NA"s.
hemhem=glm(hempresence~logarea, family=binomial(logit), data=hemdata)
hemyb=glm(hempresence~logarea, family=binomial(logit), data=birchdata)
hemsm=glm(hempresence~logarea, family=binomial(logit), data=mapledata) 

attach(logreg) #logreg is the full dataset
plot(logarea, hempresence, xlab = "Surface area of log (m2)", 
ylab="Probability of hemlock seedling presence", type="n", font.lab=2, 
cex.lab=1.5, axes=TRUE)
lines(logarea,fitted(hemhem), lty=1, lwd=2)
lines(logarea,fitted(hemyb), lty="dashed", lwd=2)
lines(logarea,fitted(hemsm), lty="dotted", lwd=2)



From sundar.dorai-raj at pdf.com  Wed Jul 20 01:57:33 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 19 Jul 2005 18:57:33 -0500
Subject: [R] deriv - accessing numeric output listed under gradient
	attribute
In-Reply-To: <BAY11-DAV5B8DEB772D50052CD4619DAD40@phx.gbl>
References: <BAY11-DAV5B8DEB772D50052CD4619DAD40@phx.gbl>
Message-ID: <42DD936D.7020900@pdf.com>



Jay Rotella wrote:
> Hi,
> I am interested in using the numeric output from the "gradient" attribute of 
> deriv's output in subsequent analyses.
> But, I have so far been unable to determine how to do so.
> 
> I will use the example from the deriv help to illustrate.
> 
> 
>>## function with defaulted arguments:
>>   (fx <- deriv(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
> 
>                   function(b0, b1, th, x = 1:7){} ) )
> 
>>    fx(2,3,4)
> 
> 
> This yields
> [1] 4.522689 4.121320 3.783811 3.500000 3.261345 3.060660 2.891905
> attr(,"gradient")
>      b0        b1        th
> [1,]  1 0.8408964 0.1092872
> [2,]  1 0.7071068 0.1837984
> [3,]  1 0.5946036 0.2318331
> [4,]  1 0.5000000 0.2599302
> [5,]  1 0.4204482 0.2732180
> [6,]  1 0.3535534 0.2756976
> [7,]  1 0.2973018 0.2704720
> 
> I would greatly appreciate it if anyone could tell me how to convert the 
> numbers listed under "b0", "b1", and "th" into a matrix.
> 
> Thanks!
> 
> Jay Rotella
> Ecology Department
> Montana State University
> Bozeman, MT 59717
> 


Try:

attr(fx(2, 3, 4), "gradient")

--sundar



From ldimitro at wfubmc.edu  Wed Jul 20 03:46:50 2005
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Tue, 19 Jul 2005 21:46:50 -0400
Subject: [R] Memory limits using read.table on Windows XP Pro
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF08DA8370@EXCHVS1.medctr.ad.wfubmc.edu>

Hello everyone,

Would you please somebody explain me what my sin is (please see the code
and timing bellow)? And how to improve myself and the following piece of
R code? BTW, the code works.

This is R 64-bit built by myself on Sun SPARC Solaris 9 with gcc-4.0.1
(64-bit) also built by yours truly. The machine is Sun Fire-V880 with
32GB memory and 8 cpu's. Nobody else was using it.


Thank you very much

Latchezar Dimitrov

PS. Prof. Brian D. Ripley, thank you very much for not wasting your
invaluable time responding to the above. I know your answer anyway.
Sorry for making you read it though ...

>  system.time(
+ haplo[i,2*j-1]<-substr(as.character(geno[i,j]),1,1)
+  ,TRUE)
[1] 66.27 13.67 80.02  0.00  0.00
> ls()
[1] "P"     "geno"  "haplo" "i"     "j"     "lr"    "model" "pheno"
> i
[1] 1
> j
[1] 1
> dim(P)
[1]      1 125000
> str(P)
 num [1, 1:125000] 0.6188 0.0533 0.0893 0.8994 0.0316 ...
> str(pheno)
`data.frame':   2500 obs. of  11 variables:
 $ pca    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ her    : int  0 1 0 0 0 0 0 0 0 0 ...
 $ age    : num  67.1 70.4 64.9 60.8 64.3 ...
 $ t      : int  1 3 1 2 1 2 1 3 9 1 ...
 $ n      : int  9 9 9 9 9 9 9 0 9 9 ...
 $ m      : int  0 1 0 0 0 1 9 9 1 9 ...
 $ diffgrd: int  9 9 9 2 9 9 9 9 3 9 ...
 $ gs     : int  6 7 6 7 7 7 5 6 NA 6 ...
 $ psa    : num  13 75 11.3 13 9.1 51 4.3 10.7 NA 93 ...
 $ geo    : int  2 2 1 1 1 1 1 1 1 1 ...
 $ ageg   : int  6 7 5 5 5 5 3 5 4 5 ...
> str(geno)
`data.frame':   2500 obs. of  125000 variables:
^C

> 
> dim(geno)
[1]   2500 125000
> dim(haplo)
[1]   2500 250000
> version()
Error: attempt to apply non-function
> version  
         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status   Patched             
major    2                   
minor    1.1                 
year     2005                
month    07                  
day      09                  
language R                   
>



From ldimitro at wfubmc.edu  Wed Jul 20 03:48:51 2005
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Tue, 19 Jul 2005 21:48:51 -0400
Subject: [R] Memory limits using read.table on Windows XP Pro
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF08DA8371@EXCHVS1.medctr.ad.wfubmc.edu>

Really sorry for the wrong addressing. It was intended to the list only.
I apologize.
Latchezar 

> -----Original Message-----
> From: Latchezar Dimitrov 
> Sent: Tuesday, July 19, 2005 9:47 PM
> To: Latchezar Dimitrov; 'Prof Brian Ripley'
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: RE: [R] Memory limits using read.table on Windows XP Pro
> 
> Hello everyone,
> 
> Would you please somebody explain me what my sin is (please 
> see the code and timing bellow)? And how to improve myself 
> and the following piece of R code? BTW, the code works.
> 
> This is R 64-bit built by myself on Sun SPARC Solaris 9 with 
> gcc-4.0.1 (64-bit) also built by yours truly. The machine is 
> Sun Fire-V880 with 32GB memory and 8 cpu's. Nobody else was using it.
> 
> 
> Thank you very much
> 
> Latchezar Dimitrov
> 
> PS. Prof. Brian D. Ripley, thank you very much for not 
> wasting your invaluable time responding to the above. I know 
> your answer anyway. Sorry for making you read it though ...
> 
> >  system.time(
> + haplo[i,2*j-1]<-substr(as.character(geno[i,j]),1,1)
> +  ,TRUE)
> [1] 66.27 13.67 80.02  0.00  0.00
> > ls()
> [1] "P"     "geno"  "haplo" "i"     "j"     "lr"    "model" "pheno"
> > i
> [1] 1
> > j
> [1] 1
> > dim(P)
> [1]      1 125000
> > str(P)
>  num [1, 1:125000] 0.6188 0.0533 0.0893 0.8994 0.0316 ...
> > str(pheno)
> `data.frame':   2500 obs. of  11 variables:
>  $ pca    : int  1 1 1 1 1 1 1 1 1 1 ...
>  $ her    : int  0 1 0 0 0 0 0 0 0 0 ...
>  $ age    : num  67.1 70.4 64.9 60.8 64.3 ...
>  $ t      : int  1 3 1 2 1 2 1 3 9 1 ...
>  $ n      : int  9 9 9 9 9 9 9 0 9 9 ...
>  $ m      : int  0 1 0 0 0 1 9 9 1 9 ...
>  $ diffgrd: int  9 9 9 2 9 9 9 9 3 9 ...
>  $ gs     : int  6 7 6 7 7 7 5 6 NA 6 ...
>  $ psa    : num  13 75 11.3 13 9.1 51 4.3 10.7 NA 93 ...
>  $ geo    : int  2 2 1 1 1 1 1 1 1 1 ...
>  $ ageg   : int  6 7 5 5 5 5 3 5 4 5 ...
> > str(geno)
> `data.frame':   2500 obs. of  125000 variables:
> ^C
> 
> > 
> > dim(geno)
> [1]   2500 125000
> > dim(haplo)
> [1]   2500 250000
> > version()
> Error: attempt to apply non-function
> > version
>          _                   
> platform sparc-sun-solaris2.9
> arch     sparc               
> os       solaris2.9          
> system   sparc, solaris2.9   
> status   Patched             
> major    2                   
> minor    1.1                 
> year     2005                
> month    07                  
> day      09                  
> language R                   
> >



From ggrothendieck at gmail.com  Wed Jul 20 03:49:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Jul 2005 21:49:30 -0400
Subject: [R] Problems with date-format (R 2.1.1 + chron)
In-Reply-To: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>
References: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>
Message-ID: <971536df0507191849374d951f@mail.gmail.com>

I think its likely that you are using different versions of chron.
I noticed that version "2.2-33" of chron had the statement:

   tms <- dts - trunc(dts)

but version "2.2-35" seems to have replaced it with:

  tms <- dts - floor(dts)

and that seems to be causing the problem.

As a workaround:

    floor.Date <- floor.trunc
    days(as.Date("21-07-2005", "%d-%m-%y"))


On 7/19/05, Carsten Steinhoff <carsten.steinhoff at stud.uni-goettingen.de> wrote:
> Hello,
> 
> today I've updated on the newest R-Version. But sadly a function I needed
> didnt want to work:
> The input is e.g.
> 
> days(as.Date("21-07-2005","%d-%m-%y"))
> 
> the error is: Fehler in Math.Date(dts): floor nicht definiert f??r Date
> Objekte
> (Error in Math.Date(dts): floor not defined for date objects)
> 
> Same for year. Only months gives me the correct output.
> In Version 2.01 it worked very well, with the same chron library.
> Whats wrong ?
> 
> Carsten
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jp.mcdermott at gmail.com  Wed Jul 20 03:50:15 2005
From: jp.mcdermott at gmail.com (James McDermott)
Date: Tue, 19 Jul 2005 21:50:15 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
In-Reply-To: <42DD5B78.40906@stats.uwo.ca>
References: <33871de605071911531e399f4d@mail.gmail.com>
	<42DD4F6B.7090600@stats.uwo.ca>
	<33871de605071912341b778fdb@mail.gmail.com>
	<42DD5B78.40906@stats.uwo.ca>
Message-ID: <33871de6050719185047ade71e@mail.gmail.com>

Would the unique quadratic defined by the three points be the same
curve as the curve predicted by a quadratic B-spline (fit to all of
the data) through those same three points?

Jim

On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/19/2005 3:34 PM, James McDermott wrote:
> > I wish it were that simple (perhaps it is and I am just not seeing
> > it).  The output from cobs( ) includes the B-spline coefficients and
> > the knots.  These coefficients are not the same as the a, b, and c
> > coefficients in a quadratic polynomial.  Rather, they are the
> > coefficients of the quadratic B-spline representation of the fitted
> > curve.  I need to evaluate a linear combination of basis functions and
> > it is not clear to me how to accomplish this easily.  I was hoping to
> > find an alternative way of getting the derivatives.
> 
> I don't know COBS, but doesn't predict just evaluate the B-spline?  The
> point of what I posted is that the particular basis doesn't matter if
> you can evaluate the quadratic at 3 points.
> 
> Duncan Murdoch
> 
> >
> > Jim McDermott
> >
> > On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >> On 7/19/2005 2:53 PM, James McDermott wrote:
> >> > Hello,
> >> >
> >> > I have been trying to take the derivative of a quadratic B-spline
> >> > obtained by using the COBS library.  What I would like to do is
> >> > similar to what one can do by using
> >> >
> >> > fit<-smooth.spline(cdf)
> >> > xx<-seq(-10,10,.1)
> >> > predict(fit, xx, deriv = 1)
> >> >
> >> > The goal is to fit the spline to data that is approximating a
> >> > cumulative distribution function (e.g. in my example, cdf is a
> >> > 2-column matrix with x values in column 1 and the estimate of the cdf
> >> > evaluated at x in column 2) and then take the first derivative over a
> >> > range of values to get density estimates.
> >> >
> >> > The reason I don't want to use smooth.spline is that there is no way
> >> > to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
> >> > with COBS.  However, since COBS doesn't have the 'deriv =' option, the
> >> > only way I can think of doing it with COBS is to evaluate the
> >> > derivatives numerically.
> >>
> >> Numerical estimates of the derivatives of a quadratic should be easy to
> >> obtain accurately.  For example, if the quadratic ax^2 + bx + c is
> >> defined on [-1, 1], then the derivative 2ax + b, has 2a = f(1) - f(0) +
> >> f(-1), and b = (f(1) - f(-1))/2.
> >>
> >> You should be able to generalize this to the case where the spline is
> >> quadratic between knots k1 and k2 pretty easily.
> >>
> >> Duncan Murdoch
> >>
> 
>



From ekhous at po-box.mcgill.ca  Wed Jul 20 03:57:08 2005
From: ekhous at po-box.mcgill.ca (ekhous@po-box.mcgill.ca)
Date: Tue, 19 Jul 2005 21:57:08 -0400
Subject: [R] nls
Message-ID: <1121824628.42ddaf74da751@webmail.mcgill.ca>

Dear R-helpers,

I am trying to estimate a model that I am proposing, which consists of putting
an extra hidden layer in the Markov switching models. In the simplest case the
S(t) - Markov states - and w(t) - the extra hidden variables - are independent,
and w(t) is constant. Formally the model looks like this:
y(t)=c(1,y[t-1])%*%beta0*w+c(1,y[t-1])%*%beta1*(1-w). So I ran some simulations
to obtain the y's, and I am putting it into the nls:

res<-nls(y~(a+b*x)*w+(c+d*x)*(1-w),start=list(a=1,b=0.3,c=-1,d=-0.2,w=0.5))

and the starting parameter values are similar to the ones I used for
simulations, however I am getting

Error in nlsModel(formula, mf, start) : singular gradient matrix at initial
parameter estimates

What am I doing wrong? I tried many different parameter values to no avail.
Thank you so much in advance,
Sincerely,

Eugene
McGill University



From u9370004 at cc.kmu.edu.tw  Wed Jul 20 04:21:22 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Wed, 20 Jul 2005 10:21:22 +0800
Subject: [R] Michaelis-menten equation
In-Reply-To: <42DCDB38.4050705@fz-rossendorf.de>
References: <20050719084736.M53999@cc.kmu.edu.tw>
	<42DCDB38.4050705@fz-rossendorf.de>
Message-ID: <20050720020944.M51196@cc.kmu.edu.tw>

Hi,

   We are doing a pharmaockinetic modeling.  This model is
described as intravenous injection of a certain drug into
the body.  Then the drug molecule will be eliminated (or decayed)
from the body.  We here used a MM eq. to describe the elimination
process and the changes of the drug conc..  So the diff. eq. would
be: dCp/dt = -Vm/Vd * Cp/(Km+Cp).  Vd is a volume of distribution.
We used lsoda to solve the diff. eq. first and fit the diff. eq.
with optim first (Nelder-Mead simplex) and followed by using nls 
to take over the fitting process of optim.  However, we can not
obtain the correct value for Km if we used the above model.  The
correct Km can be obtained only when we modeled the diff eq. with
dCp/dt= -Vm/Vd * Cp/(Km/vd + Cp).  Now we lost.  The data were
from simulation with known Vm and Km.  Any idea?  Thanks.

regards,
--- Chun-ying Lee
> 
> it is not clear to me what you are trying to do:
> you seem to have a time-concentration-curve in PKindex and you seem 
> to set up a derivative of this time dependency according to some 
> model in dCpdt. AFAIKS this scenario is  not directly related to the 
> situation described by the Michaelis-Menten-Equation which relates 
> some "input" concentration with some "product" concentration. If Vm and
> Km are meant to be the canonical symbols,
> what is Vd, a volume of distribution? it is impossible to see (at least
> for me) what exactly you want to achieve.
> 
> (and in any case, I would prefer "nls" for a least squares fit 
> instead of 'optim').
> 
> joerg
> > ------------------------------------------------------------------------
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html



From ggrothendieck at gmail.com  Wed Jul 20 04:25:14 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Jul 2005 22:25:14 -0400
Subject: [R] nls
In-Reply-To: <1121824628.42ddaf74da751@webmail.mcgill.ca>
References: <1121824628.42ddaf74da751@webmail.mcgill.ca>
Message-ID: <971536df0507191925257319e9@mail.gmail.com>

On 7/19/05, ekhous at po-box.mcgill.ca <ekhous at po-box.mcgill.ca> wrote:
> Dear R-helpers,
> 
> I am trying to estimate a model that I am proposing, which consists of putting
> an extra hidden layer in the Markov switching models. In the simplest case the
> S(t) - Markov states - and w(t) - the extra hidden variables - are independent,
> and w(t) is constant. Formally the model looks like this:
> y(t)=c(1,y[t-1])%*%beta0*w+c(1,y[t-1])%*%beta1*(1-w). So I ran some simulations
> to obtain the y's, and I am putting it into the nls:
> 
> res<-nls(y~(a+b*x)*w+(c+d*x)*(1-w),start=list(a=1,b=0.3,c=-1,d=-0.2,w=0.5))
> 
> and the starting parameter values are similar to the ones I used for
> simulations, however I am getting
> 
> Error in nlsModel(formula, mf, start) : singular gradient matrix at initial
> parameter estimates
> 

Your model is not identifiable.  You are using 5 parameters to describe 
a two dimensional model -- in fact, y is linear in x so anything beyond 
the intercept and slope are redundant, viz. a singular gradient.



From u9370004 at cc.kmu.edu.tw  Wed Jul 20 04:26:25 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Wed, 20 Jul 2005 10:26:25 +0800
Subject: [R] Michaelis-menten equation
In-Reply-To: <x2fyua4qjb.fsf@turmalin.kubism.ku.dk>
References: <20050719084736.M53999@cc.kmu.edu.tw>
	<x24qarqguk.fsf@turmalin.kubism.ku.dk>
	<x2fyua4qjb.fsf@turmalin.kubism.ku.dk>
Message-ID: <20050720022136.M87120@cc.kmu.edu.tw>

Hi,

   We used known Vm and Km to simulate the data set (time, Cp) without
adding random error in there.  Yes, the line looks like very close
to a straight line.  But why can't we obtain the correct values with
fitting process?  We used optim first and then followed by using nls
to fit the model.  Thanks.

regards,
---Chun-ying Lee
> 
> Hmm, sorry, no. I'm talking through a hole in my head there.
> 
> Vm*y/(Km+y) makes OK sense. Vm is what you get for large y - passing
> from 1st order to 0th order kinetics. However, looking at the data
> 
>  plot(PKindex)
>  abline(lm(conc~time,data=PKindex))
> 
> shows that they are pretty much on a straight line, i.e. you are 
> in the domain of 0-order kinetics. So why are you expecting the rate
> of decrease to have changed by roughly 3/4 (from 2/3*Vm/Vd at y=2*Km
> to 1/2*Vm/Vd at y=Km when you reach 4.67)??
>



From ggrothendieck at gmail.com  Wed Jul 20 04:27:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Jul 2005 22:27:05 -0400
Subject: [R] Problems with date-format (R 2.1.1 + chron)
In-Reply-To: <971536df0507191849374d951f@mail.gmail.com>
References: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>
	<971536df0507191849374d951f@mail.gmail.com>
Message-ID: <971536df0507191927cbdac17@mail.gmail.com>

On 7/19/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I think its likely that you are using different versions of chron.
> I noticed that version "2.2-33" of chron had the statement:
> 
>   tms <- dts - trunc(dts)
> 
> but version "2.2-35" seems to have replaced it with:
> 
>  tms <- dts - floor(dts)
> 
> and that seems to be causing the problem.
> 
> As a workaround:
> 
>    floor.Date <- floor.trunc

That should have been:

      floor.Date <- trunc.Date

>    days(as.Date("21-07-2005", "%d-%m-%y"))
> 
> 
> On 7/19/05, Carsten Steinhoff <carsten.steinhoff at stud.uni-goettingen.de> wrote:
> > Hello,
> >
> > today I've updated on the newest R-Version. But sadly a function I needed
> > didnt want to work:
> > The input is e.g.
> >
> > days(as.Date("21-07-2005","%d-%m-%y"))
> >
> > the error is: Fehler in Math.Date(dts): floor nicht definiert f??r Date
> > Objekte
> > (Error in Math.Date(dts): floor not defined for date objects)
> >
> > Same for year. Only months gives me the correct output.
> > In Version 2.01 it worked very well, with the same chron library.
> > Whats wrong ?
> >
> > Carsten
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From sundar.dorai-raj at pdf.com  Wed Jul 20 04:56:06 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 19 Jul 2005 21:56:06 -0500
Subject: [R] Regression lines for differently-sized groups on the same
 plot
In-Reply-To: <E1Dv1ts-00085Q-7O@sys25.mail.msu.edu>
References: <E1Dv1ts-00085Q-7O@sys25.mail.msu.edu>
Message-ID: <42DDBD46.2060209@pdf.com>



Laura M Marx wrote:
> Hi there,
>   I've looked through the very helpful advice about adding fitted lines to 
> plots in the r-help archive, and can't find a post where someone has offered 
> a solution for my specific problem.  I need to plot logistic regression fits 
> from three differently-sized data subsets on a plot of the entire dataset.  
> A description and code are below:
>   I have an unbalanced dataset consisting of three different species (hem, 
> yb, and sm), with unequal numbers of wood pieces in each species group.  I 
> am trying to generate a plot that will show the size of the wood piece on 
> the X axis, the probability of it having tree seedlings growing on it on the 
> Y (a binomial yes or no variable), and three fitted curves showing how the 
> probability of having tree seedlings changes with increasing wood piece size 
> for each species.
>   I have no problem generating fits using GLM, and no problem creating the 
> plot.  However, if I try to add a fitted curve based only on the hem data 
> subset to a plot that shows the entire dataset, I get an error message that 
> the lengths of those data sets differ. "Error in xy.coords(x,y) : x and y 
> lengths differ".  I could see R's point -- you can't plot a regression line 
> of babies born as a function of stork abundance on a graph of cherries 
> produced (Y) versus rainfall (X), which for all the program knows, I'm 
> trying to do.  As a temporary fix, I added NAs to the end of the hem, yb, 
> and sm subsets to make them the same length as the entire dataset.  I can 
> now add my fitted curves to the plot, but the lines are not connected.  That 
> is, if the hem group only contains wood pieces that are 1, 4, and 10 meters 
> long, the plot has an X axis that ranges from 1 to 10, but line segments for 
> the hem group regression line only appear above 1, 4, and 10.  How can I fix 
> this?  An ideal solution would not require me to make the hem subset of my 
> data the same length as the full dataset, either (although the summaries of 
> regressions with the NAs (or zeroes) added and taken away are identical).  
> I'd also settle for a work-around that would have R connect the pieces of 
> the curve so that I get a solid line rather than small dots and dashes where 
> actual data exist.  Thanks so much for your help!
>   Laura Marx
>   Michigan State University, Dept. of Forestry 
> 
> #Note: hemdata has all the rows that are not hemlock species replaced with 
> #"NA"s.
> hemhem=glm(hempresence~logarea, family=binomial(logit), data=hemdata)
> hemyb=glm(hempresence~logarea, family=binomial(logit), data=birchdata)
> hemsm=glm(hempresence~logarea, family=binomial(logit), data=mapledata) 
> 
> attach(logreg) #logreg is the full dataset
> plot(logarea, hempresence, xlab = "Surface area of log (m2)", 
> ylab="Probability of hemlock seedling presence", type="n", font.lab=2, 
> cex.lab=1.5, axes=TRUE)
> lines(logarea,fitted(hemhem), lty=1, lwd=2)
> lines(logarea,fitted(hemyb), lty="dashed", lwd=2)
> lines(logarea,fitted(hemsm), lty="dotted", lwd=2)
> 

Hi, Laura,

Would ?predict.glm be better?

plot(logarea, hempresence,
      xlab = "Surface area of log (m2)",
      ylab="Probability of hemlock seedling presence",
      type="n", font.lab=2, cex.lab=1.5, axes=TRUE)
lines(logarea, predict(hemhem, logreg, "response"), lty=1, lwd=2)
lines(logarea, predict(hemyb, logreg, "response"), lty="dashed", lwd=2)
lines(logarea, predict(hemsm, logreg, "response"), lty="dotted", lwd=2)

Without seeing more description of your data, this is still a guess.

--sundar



From s.su at qut.edu.au  Wed Jul 20 06:07:45 2005
From: s.su at qut.edu.au (Steve Su)
Date: Wed, 20 Jul 2005 14:07:45 +1000
Subject: [R] Turning off return warning messages.
Message-ID: <000601c58ce0$95aad150$2032b583@qut.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050720/e2362779/attachment.pl

From tmchoi at ris.chonnam.ac.kr  Wed Jul 20 07:13:50 2005
From: tmchoi at ris.chonnam.ac.kr (Taemyong Choi)
Date: Wed, 20 Jul 2005 14:13:50 +0900
Subject: [R] Question about Installing SJava package
Message-ID: <42DDDD8E.4070407@ris.chonnam.ac.kr>

Dear all,

I have an error message installing SJava package.
So I searched web site(google) and R-mailing list to find a similar error message.
But I couldn't find it.

I installed R-2.1.1 like this on Fedora Core4

1) /configure --enable-R-shlib --with-libpng --with-jpeglib
2) make -> make check -> make install

and then issuing on shell prompt (red lines are error messages)

R CMD INSTALL -c /usr/local/src/R/SJava_0.68-0.tar.gz

* Installing *source* package 'SJava' ...
checking for java... /usr/java/jdk1.5.0_04//bin/java
Java VM /usr/java/jdk1.5.0_04//bin/java
checking for javah... /usr/java/jdk1.5.0_04//bin/javah
Looking in /usr/java/jdk1.5.0_04/include
Looking in /usr/java/jdk1.5.0_04/include/linux
checking for g++... g++
checking for C++ compiler default output... a.out
checking whether the C++ compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking for gcc... gcc
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking for Rf_initEmbeddedR in -lR... no
No R shared library found
configure: creating ./config.status
config.status: creating Makevars
config.status: creating src/Makevars
config.status: creating src/RSJava/Makefile
config.status: creating Makefile_rules
config.status: creating inst/scripts/RJava.bsh
config.status: creating inst/scripts/RJava.csh
config.status: creating R/zzz.R
config.status: creating cleanup
config.status: creating inst/scripts/RJava
Copying the cleanup script to the scripts/ directory
Building libRSNativeJava.so in /tmp/R.INSTALL.tf2988/SJava/src/RSJava
if test ! -d /usr/local/lib/R/library/SJava/libs ; then \
mkdir /usr/local/lib/R/library/SJava/libs ; \
fi
gcc -g -O2 -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-c CtoJava.c
CtoJava.cweb:215: error: static declaration of 'std_env' follows
non-static declaration
CtoJava.cweb:195: error: previous declaration of 'std_env' was here
make: *** [CtoJava.o] Error 1
Generating JNI header files from Java classes.
RForeignReference, RManualFunctionActionListener, ROmegahatInterpreter &
REvaluator
*************
Warning:
At present, to use the library you must set the
LD_LIBRARY_PATH environment variable
to
/usr/local/lib/R/library/SJava/libs:/usr/java/jdk1.5.0_04/jre/lib/i386/client:/usr/java/jdk1.5.0_04/jre/lib/i386:/usr/java/jdk1.5.0_04/jre/../lib/i386:
or use one of the RJava.bsh or RJava.csh scripts
*************
** libs
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c ConverterExamples.c -o
ConverterExamples.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c Converters.c -o Converters.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c REmbed.c -o REmbed.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c REmbedWin.c -o REmbedWin.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c REval.c -o REval.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c RFunctionListener.c -o
RFunctionListener.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c RReferenceCall.c -o RReferenceCall.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c RStoJava.c -o RStoJava.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c Reflectance.c -o Reflectance.o
gcc -I/usr/local/lib/R/include -D_R_ -I/usr/local/lib/R/include
-I/usr/local/lib/R/include/R_ext
-I/tmp/R.INSTALL.tf2988/SJava/src/RSJava -I.
-I/tmp/R.INSTALL.tf2988/SJava/inst/include -IRSJava
-I/usr/java/jdk1.5.0_04//include -I/usr/java/jdk1.5.0_04//include/linux
-I/usr/local/include -fPIC -g -O2 -c RtoJava.c -o RtoJava.o
gcc -shared -L/usr/local/lib -o SJava.so ConverterExamples.o
Converters.o REmbed.o REmbedWin.o REval.o RFunctionListener.o
RReferenceCall.o RStoJava.o Reflectance.o RtoJava.o
-L/tmp/R.INSTALL.tf2988/SJava/inst/libs
-L/usr/java/jdk1.5.0_04/jre/lib/i386/client
-L/usr/java/jdk1.5.0_04/jre/lib/i386
-L/usr/java/jdk1.5.0_04/jre/../lib/i386
-L/usr/local/lib/R/library/SJava/libs -lRSNativeJava -ljvm
-L/usr/local/lib/R/lib -lR
/usr/bin/ld: cannot find -lRSNativeJava
collect2: ld returned 1 exit status
make: *** [SJava.so] Error 1
ERROR: compilation failed for package 'SJava'
** Removing '/usr/local/lib/R/library/SJava'
** Restoring previous '/usr/local/lib/R/library/SJava'

I wonder what's wrong.

Thanks in advance,
taemyong choi.

---------------------------------------------------------------------
Taemyong Choi, tmhoi at ris.chonnam.ac.kr
MA.D. Candidate
Department of statistics, Tel: +82-62-530-0442
Chonnam National University, Fax: +82-62-530-3449
300, Yongbong-dong, Buk-gu,Gwangju, 500-757, KOREA



From uofiowa at gmail.com  Wed Jul 20 07:16:29 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 20 Jul 2005 01:16:29 -0400
Subject: [R] class in apply
Message-ID: <3f87cc6d05071922162867e91d@mail.gmail.com>

Numeric data that is part of a mixed type data frame is converted into
character. How can I tell apply to maintain the original class of a
column and not convert it into character. I would like to do this of
the vector and not inside the apply function individually over each
element. Consider the following two scenarios, in the second column
'a' maintained its class while it lost its numeric type in the first
case.

> df = data.frame(a=c(1,2), b=c('A','B'))
> df
  a b
1 1 A
2 2 B
> a=apply(df, 1, function(r) print(class(r['a'])))
[1] "character"
[1] "character"
> a=apply(df, 1, function(r) print(class(r['b'])))
[1] "character"
[1] "character"


> df = data.frame(a=c(1,2))
> df
  a
1 1
2 2
> a=apply(df, 1, function(r) print(class(r['a'])))
[1] "numeric"
[1] "numeric"



From roebuck at odin.mdacc.tmc.edu  Wed Jul 20 07:49:33 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 20 Jul 2005 00:49:33 -0500 (CDT)
Subject: [R] Turning off return warning messages.
In-Reply-To: <000601c58ce0$95aad150$2032b583@qut.edu.au>
References: <000601c58ce0$95aad150$2032b583@qut.edu.au>
Message-ID: <Pine.OSF.4.58.0507200038480.118374@odin.mdacc.tmc.edu>

On Wed, 20 Jul 2005, Steve Su wrote:

> Is there a way I can turn off the following warning message
> for using multi-argument returns?
>
> multi-argument returns are deprecated in: return(p1, p2, p3, p4)

doubleEm <- function(p1, p2, p3, p4) {
    return(list(p1 = p1*p1,
                p2 = p2*p2,
                p3 = p3*p3,
                p4 = p4*p4))
}

res <- doubleEm(1, 2, 3, 4)
cat("p3 =", res$p3, "\n")

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ligges at statistik.uni-dortmund.de  Wed Jul 20 08:37:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Jul 2005 08:37:13 +0200
Subject: [R] Using BRugs, in FUN: .C(..) 'type' not "real"
In-Reply-To: <ed55c06b0507191310ab753b1@mail.gmail.com>
References: <ed55c06b0507191310ab753b1@mail.gmail.com>
Message-ID: <42DDF119.2030704@statistik.uni-dortmund.de>

Seth Pruitt wrote:
> To All,
> 
> I am using the BRugs package. In running the meta file BRugsFit() with a 
> syntactically correct model .txt file, I see the message:
> 
> Error in FUN(X[[1]], ...) : .C(..): 'type' must be "real" for this format


Hi,

can you send me in a private message a reproducible example (including 
you R calls, data, inits and model file) please?

Thanks,
Uwe


> I haven't found information on this kind of error either here or on the 
> OpenBUGS/WinBUGS/BRugs bulletin boards. If there is a better place to post 
> this message, please let me know.
> 
> I am running Windows XP Professional (build 2600) Service Pack 2.0 on Intel 
> Centrino, and using R version 2.1.1.
> 
> Thank you,
>



From carsten.dormann at ufz.de  Wed Jul 20 08:40:46 2005
From: carsten.dormann at ufz.de (Carsten Dormann)
Date: Wed, 20 Jul 2005 08:40:46 +0200
Subject: [R] analysing non-normal spatially autocorrelated data
Message-ID: <42DDF1EE.2030109@ufz.de>

Dear fellow R-users,

I wish to analyse a lattice of presence-absence data which are spatially 
autocorrelated.

For normally distributed errors I used gls {nlme} with the "appropriate" 
corStruct-method.
Is there any method for other families (binomial and poisson)?

A method that look suitable to me as a non-statistician is called gllamm 
(generalised linear latent mixed model), by Rabe-Hesketh et al (2001), 
available apparently only for Stata.
In R, I found the gamm {Matrix} function doing what I want, but I am 
interested in the parameter values of the covariates, using the model 
for prediction, hence gamm is no option.
Finally, Dan Bebber posted a similar question to the R-help list in 
September 2004 (about using corStruct in glmmPQL), but there is no reply 
in the thread (http://tolstoy.newcastle.edu.au/R/help/04/09/3103.html).

Any suggestions are highly welcome.

Many thanks,

Carsten

-- 
Dr. Carsten F. Dormann
Department of Applied Landscape Ecology
UFZ Centre of Environmental Research
Permoserstr. 15
04318 Leipzig
Germany

Tel: ++49(0)341 2352953
Fax: ++49(0)341 2352534
Email: carsten.dormann at ufz.de
internet: http://www.ufz.de/index.php?de=4205



From dray at biomserv.univ-lyon1.fr  Wed Jul 20 08:45:37 2005
From: dray at biomserv.univ-lyon1.fr (=?UTF-8?B?U3TDqXBoYW5lIERyYXk=?=)
Date: Wed, 20 Jul 2005 08:45:37 +0200
Subject: [R] class in apply
In-Reply-To: <3f87cc6d05071922162867e91d@mail.gmail.com>
References: <3f87cc6d05071922162867e91d@mail.gmail.com>
Message-ID: <42DDF311.9060504@biomserv.univ-lyon1.fr>

apply transorm your data.frame into a matrix which can contains only one 
type of values (character or numeric or logical).
data.frame are list, so they can contain various kind of data, and a 
solution to your problem is given by lapply:

 > b=lapply(df, function(r) print(class(r['a'])))
[1] "numeric"
[1] "factor"

>Numeric data that is part of a mixed type data frame is converted into
>character. How can I tell apply to maintain the original class of a
>column and not convert it into character. I would like to do this of
>the vector and not inside the apply function individually over each
>element. Consider the following two scenarios, in the second column
>'a' maintained its class while it lost its numeric type in the first
>case.
>
>  
>
>>df = data.frame(a=c(1,2), b=c('A','B'))
>>df
>>    
>>
>  a b
>1 1 A
>2 2 B
>  
>
>>a=apply(df, 1, function(r) print(class(r['a'])))
>>    
>>
>[1] "character"
>[1] "character"
>  
>
>>a=apply(df, 1, function(r) print(class(r['b'])))
>>    
>>
>[1] "character"
>[1] "character"
>
>
>  
>
>>df = data.frame(a=c(1,2))
>>df
>>    
>>
>  a
>1 1
>2 2
>  
>
>>a=apply(df, 1, function(r) print(class(r['a'])))
>>    
>>
>[1] "numeric"
>[1] "numeric"
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 
Stphane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://www.steph280.freesurf.fr/



From Tom.Mulholland at dpi.wa.gov.au  Wed Jul 20 09:01:39 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 20 Jul 2005 15:01:39 +0800
Subject: [R] Turning off return warning messages.
Message-ID: <4702645135092E4497088F71D9C8F51A128BCB@afhex01.dpi.wa.gov.au>

doubleEm <- function(p1,p2,p3,p4){return(p1 *p1,p2 * p2, p3 * p3, p4 * p4)}
suppressWarnings(doubleEm(1,2,3,4)) 
[[1]]
[1] 1

[[2]]
[1] 4

[[3]]
[1] 9

[[4]]
[1] 16

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Paul Roebuck
> Sent: Wednesday, 20 July 2005 1:50 PM
> To: R Help Mailing List
> Subject: Re: [R] Turning off return warning messages.
> 
> 
> On Wed, 20 Jul 2005, Steve Su wrote:
> 
> > Is there a way I can turn off the following warning message
> > for using multi-argument returns?
> >
> > multi-argument returns are deprecated in: return(p1, p2, p3, p4)
> 
> doubleEm <- function(p1, p2, p3, p4) {
>     return(list(p1 = p1*p1,
>                 p2 = p2*p2,
>                 p3 = p3*p3,
>                 p4 = p4*p4))
> }
> 
> res <- doubleEm(1, 2, 3, 4)
> cat("p3 =", res$p3, "\n")
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Bernd.Ebersberger at isi.fraunhofer.de  Wed Jul 20 09:07:10 2005
From: Bernd.Ebersberger at isi.fraunhofer.de (Ebersberger, Bernd)
Date: Wed, 20 Jul 2005 09:07:10 +0200
Subject: [R] maps and data for german federal states
Message-ID: <4AFB92BB8CCDEB4087821B5A69541D9130E40A@MAILGW.isi26.isi.fhg.de>

dear R-tists,

i want to graph information for the German Federal States (Bundeslaender) using the maps package. unfortunately there is no maps for the German Bundeslaender.

does anyone have an idea / a source where to get map data that can be used in the maps package that graphs structures below the country level. 

in the long run it would also be interesting to integrate Swiss Cantons and Austrian Bundeslnder... 

any suggestions are highly appreciated.

thanks

bernd

++++++++++++++++++++++++++++++++++++++++++++++
bernd ebersberger
fhg isi
b.ebersberger (AT) isi.fhg.de



From David.Ruau at rwth-aachen.de  Wed Jul 20 09:18:38 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Wed, 20 Jul 2005 09:18:38 +0200
Subject: [R] (no subject)
Message-ID: <40d962e17d20eb7fd6d5508c3d94628a@rwth-aachen.de>

Hi All,

I want to print a square matrix of 7000 x 7000 into a text file. But I 
got a error after few hours of computation...
--------
 > write.table(MyDistMxDF, file = "temp.csv", sep=",", quote=F)
*** malloc: vm_allocate(size=8421376) failed (error code=3)
*** malloc[2889]: error: Can't allocate region
Error: vector memory exhausted (limit reached?)
*** malloc: vm_allocate(size=8421376) failed (error code=3)
*** malloc[2889]: error: Can't allocate region
 >  q()
*** malloc: vm_allocate(size=8421376) failed (error code=3)
*** malloc[2889]: error: Can't allocate region
*** malloc: vm_allocate(size=8421376) failed (error code=3)
*** malloc[2889]: error: Can't allocate region
Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
         internal error in decompress1
 >
------
I am running R 2.0.1 on MacOS X 10.3 with 1Gb ram.
How could I write such a matrix to a text file.

David



From maechler at stat.math.ethz.ch  Wed Jul 20 09:19:25 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 20 Jul 2005 09:19:25 +0200
Subject: [R] package (!) mclust in 64bit compiled R
In-Reply-To: <BF0285B9.67EA%thchung@tgen.org>
References: <BF0285B9.67EA%thchung@tgen.org>
Message-ID: <17117.64253.661741.230867@stat.math.ethz.ch>

>>>>> "TaeHC" == Tae-Hoon Chung <thchung at tgen.org>
>>>>>     on Tue, 19 Jul 2005 10:26:01 -0700 writes:

    TaeHC> Hi, All; I tried to use library mclust in 64-bit
    TaeHC> compiled R 2.0.1 but failed.  

you mean   "package  mclust"
{a package is installed in a library full of packages}

    TaeHC> Installation went smoothly without any warning or
    TaeHC> error. However, when I tried to use them with the
    TaeHC> following simple code, it crashed.

 >      library(mclust)
 >      Dat <- c(rnorm(20, mean=0, sd=0.2), rnorm(30, mean=1, sd=0.2))
 >      Ind <- Mclust(dat, 1, 5)$classification
 >      cbind(Dat, Ind)

My desktop has been a 64-bit (AMD Athlon 64) for more than a
year now, and I do note small differences to 32-bit some times,
and even have found (and not yet solved) quite severe problem
with the akima package.

Not with 'mclust' though.
I've just run several 1000 of Mclust() fits like the one above
and not seen any problem.

    TaeHC> The error message was:

    TaeHC> /usr/local/R-2.0.1_64bit/lib/R/bin/BATCH: line 55:
    TaeHC> 18097 Done ( echo "invisible(options(echo = TRUE))";
    TaeHC> cat ${in}; echo "proc.time()" ) 18099 Segmentation
    TaeHC> fault | ${R_HOME}/bin/R ${opts} >${out} 2>&1
	  

    TaeHC> Can anybody help me with this?  Thanks in advance,

- Does *every* call to Mclust() give a segmentation fault?

- I assume you -- or someone in your group has compiled this R version
from source.  Did you (or he/she) run  "make check" after
building from source?  Could you (or ..) asks to run it?

- What output do you get when you run 
  
  R -d gdb
  r

  then the above R code,
  and then maybe type 'bt' (backtrace) ?


Regards,
Martin Maechler, ETH Zurich



From vito_ricci at yahoo.com  Wed Jul 20 10:02:08 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 20 Jul 2005 10:02:08 +0200 (CEST)
Subject: [R] data mining
Message-ID: <20050720080208.15671.qmail@web41211.mail.yahoo.com>

Hi Adrian,
give a look to these links concerning R & DM:

http://www.togaware.com/datamining/survivor/R.html
http://sawww.epfl.ch/SIC/SA/publications/FI01/fi-sp-1/sp-1-page45.html
http://www.stats.ox.ac.uk/~vos/DataMining.html

Hoping I helped you.

Regards,

Vito



secretario academico FACEA faceasec at uapar.edu 
wrote:

Dear all,
I'm looking for some material on data mining with R. I
have something 
from Luis Torgo but I'd like to see something else.
If anybody could help me I'll be thankful
Adri??n


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From ripley at stats.ox.ac.uk  Wed Jul 20 10:05:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 09:05:09 +0100 (BST)
Subject: [R] writing matrices (no subject)
In-Reply-To: <40d962e17d20eb7fd6d5508c3d94628a@rwth-aachen.de>
References: <40d962e17d20eb7fd6d5508c3d94628a@rwth-aachen.de>
Message-ID: <Pine.LNX.4.61.0507200854170.28666@gannet.stats>

Please use a current version of R: they are much better at this!
write.table was rewritten in R 2.1.0 to use *much* less memory.

There _is_ a `R Data Import/Export Manual' that dicsusses this.
write.table was designed to write `tables' (data frames) not matrices.
write.matrix (package MASS) does a much better job of the latter, 
memory-wise, in particular allowing the use of blocks.

That _is_ on the help page for write.table in your version of R.

Assuming this is a numeric matrix, it is taking up 373MB of storage.
It is not too surprising that you are having problems manipulating it on a 
1GB machine: my 1GB machine ended up swapping just creating such a matrix.


On Wed, 20 Jul 2005, David Ruau wrote:

> Hi All,
>
> I want to print a square matrix of 7000 x 7000 into a text file. But I
> got a error after few hours of computation...
> --------
> > write.table(MyDistMxDF, file = "temp.csv", sep=",", quote=F)
> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> *** malloc[2889]: error: Can't allocate region
> Error: vector memory exhausted (limit reached?)
> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> *** malloc[2889]: error: Can't allocate region
> >  q()
> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> *** malloc[2889]: error: Can't allocate region
> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> *** malloc[2889]: error: Can't allocate region
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
>         internal error in decompress1
> >
> ------
> I am running R 2.0.1 on MacOS X 10.3 with 1Gb ram.
> How could I write such a matrix to a text file.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jul 20 10:12:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 09:12:07 +0100 (BST)
Subject: [R] CPU Usage with R 2.1.0 in Windows
In-Reply-To: <42DD5296.4020509@novum.am.lublin.pl>
References: <4D2300C2F8092C45AFC70A1D04CD9A05137298@PRCMAILNE.prcins.net>
	<42DD5296.4020509@novum.am.lublin.pl>
Message-ID: <Pine.LNX.4.61.0507200908090.28666@gannet.stats>

It probably is not a problem to leave hyperthreading on: we found little 
performance difference on a P4 either way.

The Windows task manager is misleading, as `50%' is about as much as a 
P4-class processor with hyperthreading can actually deliver.

On Tue, 19 Jul 2005, Lukasz Komsta wrote:

> Dnia 2005-07-19 20:28, U??ytkownik Greene, Michael napisa??:
>> Hi,> > I'm using a fairly simple HP Compaq desktop PC running Windows 2K.  When> running a large process in R, the process "RGUI.exe" will never exceed 50%> of the CPU usage.
> If you have hyperthreading, R catches only one virtual processor (fromtwo available), being not able to exceed half of total power (100% ofone only). If you want to use full power, you should turn hyperthreadingoff, if your BIOS supports such option.
> Regards,
> -- Lukasz KomstaDepartment of Medicinal ChemistryMedical University of Lublin6 Chodzki, 20-093 Lublin, PolandFax +48 81 7425165
> ______________________________________________R-help at stat.math.ethz.ch mailing listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Jul 20 10:25:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 09:25:09 +0100 (BST)
Subject: [R] Problems with date-format (R 2.1.1 + chron)
In-Reply-To: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>
References: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>
Message-ID: <Pine.LNX.4.61.0507200919090.31797@gannet.stats>

x <- as.Date("21-07-2005","%d-%m-%y")

is base R, and gives you a object of class "Date".

days() is not part of R, and I guess from your subject is part of chron. 
Its help page does not say what it actually does, but my guess is that it 
finds days of the month, *for a "dates" object not a "Date" object*.  Try

as.POSIXlt(x)$mday

to get that.

On Tue, 19 Jul 2005, Carsten Steinhoff wrote:

> today I've updated on the newest R-Version. But sadly a function I needed
> didnt want to work:

It should never have worked, as you were using contrary to its 
documentation.

> The input is e.g.
>
> days(as.Date("21-07-2005","%d-%m-%y"))
>
> the error is: Fehler in Math.Date(dts): floor nicht definiert f?r Date
> Objekte
> (Error in Math.Date(dts): floor not defined for date objects)
>
> Same for year. Only months gives me the correct output.
> In Version 2.01 it worked very well, with the same chron library.
> Whats wrong ?

There is no version 2.01 of R: do see the posting guide!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From p.dalgaard at biostat.ku.dk  Wed Jul 20 10:50:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 10:50:18 +0200
Subject: [R] Michaelis-menten equation
In-Reply-To: <20050720022136.M87120@cc.kmu.edu.tw>
References: <20050719084736.M53999@cc.kmu.edu.tw>
	<x24qarqguk.fsf@turmalin.kubism.ku.dk>
	<x2fyua4qjb.fsf@turmalin.kubism.ku.dk>
	<20050720022136.M87120@cc.kmu.edu.tw>
Message-ID: <x2mzoh3lpx.fsf@turmalin.kubism.ku.dk>

"Chun-Ying Lee" <u9370004 at cc.kmu.edu.tw> writes:

> Hi,
> 
>    We used known Vm and Km to simulate the data set (time, Cp) without
> adding random error in there.  Yes, the line looks like very close
> to a straight line.  But why can't we obtain the correct values with
> fitting process?  We used optim first and then followed by using nls
> to fit the model.  Thanks.
> 
> regards,


Check your simulation. There is no way that curve is consistent with
Km in the middle of the y range!

> ---Chun-ying Lee
> > 
> > Hmm, sorry, no. I'm talking through a hole in my head there.
> > 
> > Vm*y/(Km+y) makes OK sense. Vm is what you get for large y - passing
> > from 1st order to 0th order kinetics. However, looking at the data
> > 
> >  plot(PKindex)
> >  abline(lm(conc~time,data=PKindex))
> > 
> > shows that they are pretty much on a straight line, i.e. you are 
> > in the domain of 0-order kinetics. So why are you expecting the rate
> > of decrease to have changed by roughly 3/4 (from 2/3*Vm/Vd at y=2*Km
> > to 1/2*Vm/Vd at y=Km when you reach 4.67)??
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jul 20 10:52:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 09:52:51 +0100 (BST)
Subject: [R] .gct file
In-Reply-To: <1121796495.4124.30.camel@localhost.localdomain>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
	<1121794215.4124.22.camel@localhost.localdomain>
	<dd48e20f0507191052396d468d@mail.gmail.com>
	<1121796495.4124.30.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0507200913050.28666@gannet.stats>

On Tue, 19 Jul 2005, Marc Schwartz (via MN) wrote:

> For the TAB delimited columns, adjust the 'sep' argument to:
>
> read.table("data.gct", skip = 2, header = TRUE, sep = "\t")
>
> The 'quote' argument is by default:
>
> quote = "\"'"
>
> which should take care of the quoted strings and bring them in as a
> single value.
>
> The above presumes that the header row is also TAB delimited. If not,
> you may have to set 'skip = 3' to skip over the header row and manually
> set the column names.

Not quite.  You can open a connection, skip 2 rows and read one to get the 
column names, then read the rest of the file using read.table on the open 
connection using the column names you just read.

However, based on what we have been shown

read.table("data.gct", skip = 2, header = TRUE)

ought to work as the file looks as if it is white-space delimited (a tab 
is white space).

>
> HTH,
>
> Marc Schwartz
>
>
> On Tue, 2005-07-19 at 13:52 -0400, mark salsburg wrote:
>> This is all extremely helpful.
>>
>> The data turns out is a little atypical, the columns are tab-delemited
>> except for the description columns
>>
>>
>> DATA1.gct looks like this
>>
>> #1.2
>> 23 3423
>> NAME DESCRIPTION VALUE
>> gene1 "a protein inducer" 1123
>> .....          .................     ......
>>
>> How do I get R to read the data as tab delemited, but read in the 2nd
>> coloumn as one value based on the quotation marks..
>>
>> thanks..
>>
>> On 7/19/05, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
>>> On Tue, 2005-07-19 at 13:16 -0400, mark salsburg wrote:
>>>> ok so the gct file looks like this:
>>>>
>>>> #1.2  (version number)
>>>> 7283 19   (matrix size)
>>>> Name Description Values
>>>> ....      .......          ......
>>>>
>>>> How can I tell R to disregard the first two lines and start reading
>>>> the 3rd line in this gct file. I would just delete them, but I do not
>>>> know how to open a gct. file
>>>>
>>>> thank you
>>>>
>>>> On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>>>>> On 7/19/2005 12:10 PM, mark salsburg wrote:
>>>>>> I have two files to compare, one is a regular txt file that I can read
>>>>>> in no prob.
>>>>>>
>>>>>> The other is a .gct file (How do I read in this one?)
>>>>>>
>>>>>> I tried a simple
>>>>>>
>>>>>> read.table("data.gct", header = T)
>>>>>>
>>>>>> How do you suggest reading in this file??
>>>>>>
>>>>>
>>>>> .gct is not a standard filename extension.  You need to know what is in
>>>>> that file.  Where did you get it?  What program created it?
>>>>>
>>>>> Chances are the easiest thing to do is to get the program that created
>>>>> it to export in a well known format, e.g. .csv.
>>>>>
>>>>> Duncan Murdoch
>>>
>>>
>>> The above would be consistent with the info in my reply.
>>>
>>> I guess if the format is consistent, as per Mark's example above, you
>>> can use:
>>>
>>> read.table("data.gct", skip = 2, header = TRUE)
>>>
>>> which will start by skipping the first two lines and then reading in the
>>> header row and then the data.
>>>
>>> See ?read.table
>>>
>>> HTH,
>>>
>>> Marc Schwartz
>>>
>>>
>>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jul 20 10:56:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 09:56:00 +0100 (BST)
Subject: [R] problem in Krig function of "Fields" package
In-Reply-To: <42DD3B4A.80607@deprem.gov.tr>
References: <42DD3B4A.80607@deprem.gov.tr>
Message-ID: <Pine.LNX.4.61.0507200953060.32168@gannet.stats>

My guess is that you have two data points which are very close together.
The normal way out is to included a nugget effect in your model, but it 
can also be appropriate to leave out one of a pair of points.

On Tue, 19 Jul 2005, orkun wrote:

> hello
>
> I try to build DEM  using Krig function of fields package.
> And I get this error message.
> here is the procedure I followed:
>
> > dt<-read.table("/usr/local/bartin/stat/topostat1",header=F,sep="|")
> > names(dt) <-c("x","y","z")
> > coord<-cbind(dt$x,dt$y)
> > elevation<-cbind(dt$z)
> > fit1<-Krig(coord,elevation,cov.function=exp.cov,scale.type="range")
> [1] "condition number is 907405545.878743"
> Error in Krig(kord, el, cov.function = exp.cov, scale.type = "range") :
>        Covariance matrix is close
> to
> singular
>
> here is the structure  of my data:
>
> `data.frame':   3911 obs. of  3 variables:
> $ x: num  50139 50169 50214 50223 50227 ...
> $ y: num  4592999 4593322 4593054 4593151 4593246 ...
> $ z: int  320 320 320 320 320 320 320 320 320 320 ...
>
> what do you suggest to solve the problem ?
>
> kind regards
>
>
>
> ______________________________________
> XamimeLT - installed on mailserver for domain @deprem.gov.tr
> Queries to: postmaster at deprem.gov.tr
> ______________________________________
> The views and opinions expressed in this e-mail message are ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From herodote at oreka.com  Wed Jul 20 11:23:23 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Wed, 20 Jul 2005 10:23:23 +0100
Subject: [R] =?iso-8859-1?q?normalise_a_function?=
Message-ID: <IJX62Z$24C8F521D3E6B77543637449A612F000@oreka.com>

hi all,

I need to normalize a function, did something exist in R who can do it for me, instead of integrate the function then divide it by the result?

thks, i'm sorry i didn't found any information in the documentations and statistic vocabulary in english is a pain for me.

////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From b.jhaveri at decisioncraft.com  Wed Jul 20 11:42:24 2005
From: b.jhaveri at decisioncraft.com (bhumir jhaveri)
Date: Wed, 20 Jul 2005 15:12:24 +0530
Subject: [R] Need R Help
Message-ID: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>



Hi,

I am newbie in R. I have R installed on Solaris machine and I am connected
to Solaris machine from Windows using telnet. I want to create png image
file using R. But when I issue R command from telnet:-

> png("test.png")

Following error occurs:

Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
:
        unable to start device PNG
In addition: Warning message:
unable to open connection to X11 display ''

  Also, when I checked DISPLAY environment variable, it shows
67.145.62.110:0.0

(67.145.62.110) is windows machine's ip address

Please suggest any solutions for the stated problem.

Thanks,

Bhumir



From ramasamy at cancer.org.uk  Wed Jul 20 00:37:41 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 19 Jul 2005 23:37:41 +0100
Subject: [R] colnames
In-Reply-To: <C7FF4EF92D5A794EA5820C75CFB938F9630357@MAILSERVER.sabrefund.com>
References: <C7FF4EF92D5A794EA5820C75CFB938F9630357@MAILSERVER.sabrefund.com>
Message-ID: <1121812661.5936.4.camel@dhcppc3>

What does class(r1) give you ? If it is "data.frame", then try 
  exp( diff( log( as.matrix( df ) ) ) )

BTW, I made the assumption that both x and y are positive values only.

Regards, Adai



On Tue, 2005-07-19 at 16:30 +0100, Gilbert Wu wrote:
> Hi Adai,
> 
> When I tried the optimized routine, I got the following error message:
> 
> r1
>          899188 902232   901714 28176U 15322M
> 20050713  7.595  10.97 17.96999 5.1925  11.44
> 20050714  7.605  10.94 18.00999 5.2500  11.50
> 20050715  7.480  10.99 17.64999 5.2500  11.33
> 20050718  7.415  11.05 17.64000 5.2250  11.27
> > exp(diff(log(r1))) -1
> Error in r[i1] - r[-length(r):-(length(r) - lag + 1)] : 
>         non-numeric argument to binary operator
> >
> 
> Any idea?
> 
> Many Thanks.
> 
> Gilbert
> -----Original Message-----
> From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> Sent: 19 July 2005 12:20
> To: Gilbert Wu
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] colnames
> 
> 
> First, your problem could be boiled down to the following example. See
> how the colnames of the two outputs vary.
> 
> df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
> df/df
>   X100 X200
> 1    1    1
> 2    1    1
> 
> m  <- as.matrix( df )   # coerce to matrix class
> m/m
>   100 200
> 1   1   1
> 2   1   1
> 
> It appears that whenever R has to create a new dataframe automatically,
> it tries to get nice colnames. See help(data.frame). I am not exactly
> sure why this behaviour is different when creating a matrix. But I do
> not think this is a major problem for most people. If you coerce your
> input to matrix, the problem goes away.
> 
> 
> Next, note the following points :
>  a) "mat[ 1:3, 1:ncol(mat) ]" is equivalent to simply "mat[ 1:3,  ]". 
>  b) "mat[ 2:nrow(mat), ]" is equivalent to simply "mat[ -1,  ]"
> See help(subset) for more information.
> 
> Using the points above, we can simplify your function as 
> 
>  p.RIs2Returns <- function (mat){
> 
>    mat <- as.matrix(mat)
>    x <- mat[ -nrow(mat), ]
>    y <- mat[ -1, ]
>   
>    return( y/x -1 )
>  }
> 
> If your data contains only numerical data, it is probably good idea to
> work with matrices as matrix operations are faster.
> 
> 
> Finally, we can shorten your function. You can use the diff (which works
> column-wise if input is a matrix) and apply function if you know that 
> 
> 	y/x  =  exp(log(y/x))  =  exp( log(y) - log(x) )
> 
> which could be coded in R as
> 
> 	exp( diff( log(r1) ) )
> 
> and then subtract 1 from above to get your returns.
> 
> Regards, Adai
> 
> 
> 
> On Tue, 2005-07-19 at 09:17 +0100, Gilbert Wu wrote:
> > Hi Adai,
> > 
> > Many Thanks for the examples.
> > 
> > I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.
> > 
> > The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.
> > 
> > I would welcome any suggestion to improve the script.
> > 
> > 
> > Regards,
> > 
> > Gilbert
> > 
> > > "p.RIs2Returns" <-
> > + function (RIm)
> > + {
> > + x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
> > + y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
> > + RReturns <- (y/x -1)
> > + RReturns
> > + }
> > > 
> > > 
> > > channel<-odbcConnect("ourSQLDB")
> > > result<-sqlQuery(channel,paste("select * from equityRIs;"))
> > > odbcClose(channel)
> > > result
> >    stockid    sdate  dbPrice
> > 1   899188 20050713  7.59500
> > 2   899188 20050714  7.60500
> > 3   899188 20050715  7.48000
> > 4   899188 20050718  7.41500
> > 5   902232 20050713 10.97000
> > 6   902232 20050714 10.94000
> > 7   902232 20050715 10.99000
> > 8   902232 20050718 11.05000
> > 9   901714 20050713 17.96999
> > 10  901714 20050714 18.00999
> > 11  901714 20050715 17.64999
> > 12  901714 20050718 17.64000
> > 13  28176U 20050713  5.19250
> > 14  28176U 20050714  5.25000
> > 15  28176U 20050715  5.25000
> > 16  28176U 20050718  5.22500
> > 17  15322M 20050713 11.44000
> > 18  15322M 20050714 11.50000
> > 19  15322M 20050715 11.33000
> > 20  15322M 20050718 11.27000
> > > r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> > > r1
> >      sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> > 1 20050713          7.595          10.97       17.96999         5.1925          11.44
> > 2 20050714          7.605          10.94       18.00999         5.2500          11.50
> > 3 20050715          7.480          10.99       17.64999         5.2500          11.33
> > 4 20050718          7.415          11.05       17.64000         5.2250          11.27
> > > #Set sdate as the rownames
> > > rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> > > #Get rid of the first column
> > > r1 <- r1[1:nrow(r1),2:ncol(r1)]
> > > r1
> >          dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> > 20050713          7.595          10.97       17.96999         5.1925          11.44
> > 20050714          7.605          10.94       18.00999         5.2500          11.50
> > 20050715          7.480          10.99       17.64999         5.2500          11.33
> > 20050718          7.415          11.05       17.64000         5.2250          11.27
> > > colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> > > r1
> >          899188 902232   901714 28176U 15322M
> > 20050713  7.595  10.97 17.96999 5.1925  11.44
> > 20050714  7.605  10.94 18.00999 5.2500  11.50
> > 20050715  7.480  10.99 17.64999 5.2500  11.33
> > 20050718  7.415  11.05 17.64000 5.2250  11.27
> > > RRs<-p.RIs2Returns(r1)
> > > RRs
> >               X899188      X902232      X901714      X28176U      X15322M
> > 20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
> > 20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
> > 20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
> > > 
> > 
> 
>



From petr.pikal at precheza.cz  Wed Jul 20 11:55:07 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 20 Jul 2005 11:55:07 +0200
Subject: [R] Question about creating unique factor labels with the
	factor	function
In-Reply-To: <20050719191143.32637.qmail@web31213.mail.mud.yahoo.com>
Message-ID: <42DE3B9B.17362.F001E5@localhost>

Hallo

I am not sure what you want to achieve.

your factor has 3 levels but with only 2 different labels

> hb
[1] 1 1 1 1 1 1 2 2 2
Levels: 1 1 2

but

> str(hb)
 Factor w/ 3 levels "1","1","2": 1 1 1 2 2 2 3 3 3

so you gave only one label to level 1 and 2. You can give the same 
label to any level you choose but I can not see a reason in it.

>  hb <- factor(c(1,1,1,2,2,2,3,3,3), 
levels=c(1,2,3),labels=c(1,1,1)) 
> hb
[1] 1 1 1 1 1 1 1 1 1
Levels: 1 1 1

or

>  hb <- factor(c(1,1,1,2,2,2,3,3,3), 
levels=c(1,2,3),labels=c(3,100,100)) 
> hb
[1] 3   3   3   100 100 100 100 100 100
Levels: 3 100 100
> str(hb)
 Factor w/ 3 levels "3","100","100": 1 1 1 2 2 2 3 3 3
-------------------------------------------------------------------------
but surprising for me is an ordered behaviour which I find a bit 
odd.

> hb
[1] 3   3   3   100 100 100 100 100 100
Levels: 3 100 100

> ordered(hb)
[1] 3   3   3   100 100 100 100 100 100
Levels: 3 < 100 < 100

> str(hb)
 Factor w/ 3 levels "3","100","100": 1 1 1 2 2 2 3 3 3

> str(ordered(hb))
 Ord.factor w/ 3 levels "3"<"100"<"100": 1 1 1 2 2 2 2 2 2

> unique((hb))
[1] 3   100 100     	#three levels and three values
Levels: 3 100 100

> unique(ordered(hb))
[1] 3   100		# three levels but only 2 values?
Levels: 3 < 100 < 100
*************************************************
Can anybody explain why unique(ordered()) results in only 2 
displayed levels although this

> str(unique(ordered(hb)))
 Ord.factor w/ 3 levels "3"<"100"<"100": 1 2

says that the factor still have 3 levels?

Best regards
Petr




On 19 Jul 2005 at 15:11, Gregory Gentlemen wrote:

> Hi guys,
> 
> I ran into a problem of not being able to create unique labels when
> creating a factor. Consider an example below:
> 
>  hb <- factor(c(1,1,1,2,2,2,3,3,3), levels=c(1,2,3),labels=c(1,1,2)) >
> hb [1] 1 1 1 1 1 1 2 2 2 Levels: 1 1 2 
> unique(hb) [1] 1 1 2 Levels:
> 1 1 2
> 
> How come there are three unique levels, I thought this would only
> create one unique level?
> 
> > unique(as.ordered(hb))
> [1] 1 2
> Levels: 1 < 1 < 2
> 
> Is as.ordered the only solution?
> 
> Thanks in advance,
> Greg
> 
> 
> __________________________________________________
> 
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From devens8765 at yahoo.com  Wed Jul 20 11:57:35 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Wed, 20 Jul 2005 02:57:35 -0700 (PDT)
Subject: [R] lapply question
Message-ID: <20050720095735.21392.qmail@web61312.mail.yahoo.com>

Dear members, 

I have numerous arrays that are organised in a list.
For example, suppose I have 2 arrays in a list called
alist 

alist <- list(array(rpois(12,5), 6:8) ,
array(rpois(15,5), 10:12)) 

with array dimnames 

dimnames(alist[[1]]) <- list(LETTERS[1:6],
paste("namesd", 1:7, sep=""), paste("namese", 1:8,
sep="")) 

dimnames(alist[[2]]) <- list(LETTERS[7:16],
paste("namesf", 1:11, sep=""), paste("namesg", 1:12,
sep="")) 

I would like to use the lapply function to produce a
report with:
____________________________________________________
Array 1 

Dimension name: namese1 

Row Value              Value-Average(excluding Value) 
A    alist[[1]][1,1,1]
alist[[1]][1,1,1]-mean(alist[[1]][1,-1,1]) 
...etc for all elements in the first row on the array 
B    alist[[1]][2,1,1]
alist[[1]][2,1,1]-mean(alist[[1]][2,-1,1]) 
...etc 

Dimension name: namese2 
.... 
....
....
Dimension name: namese8 
... 



Array 2 

Dimension name: namesg1 
....
....
....
Dimension name: namesg12 
....
______________________ 

Can I use the apply to do this, something like 
lapply(alist, function(k), apply(k, c(1,3), ... 

but how do I layout the report using the array names,
dimension names etc and with each observation on a
separate line? Is it possible to give apply an array
and output a list?

Thanks for any help in advance. 

Dave



From ramasamy at cancer.org.uk  Wed Jul 20 12:00:57 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 20 Jul 2005 11:00:57 +0100
Subject: [R] .gct file
In-Reply-To: <Pine.LNX.4.61.0507200913050.28666@gannet.stats>
References: <dd48e20f050719091034c8dca9@mail.gmail.com>
	<42DD2A30.3000104@stats.uwo.ca>
	<dd48e20f050719101623a60189@mail.gmail.com>
	<1121794215.4124.22.camel@localhost.localdomain>
	<dd48e20f0507191052396d468d@mail.gmail.com>
	<1121796495.4124.30.camel@localhost.localdomain>
	<Pine.LNX.4.61.0507200913050.28666@gannet.stats>
Message-ID: <1121853657.6004.5.camel@ipc143004.lif.icnet.uk>

Those wondering what gtc file stands for, might be interested in [1].

The original poster can see if the package 'ctc' [2] supports reading in
this format but I think Prof. Ripley's solution works too.

[1]http://www.broad.mit.edu/cancer/software/genepattern/tutorial/gp_tutorial_fileformats.html
[2]http://www.bioconductor.org/packages/bioc/stable/src/contrib/html/ctc.html
 


On Wed, 2005-07-20 at 09:52 +0100, Prof Brian Ripley wrote:
> On Tue, 19 Jul 2005, Marc Schwartz (via MN) wrote:
> 
> > For the TAB delimited columns, adjust the 'sep' argument to:
> >
> > read.table("data.gct", skip = 2, header = TRUE, sep = "\t")
> >
> > The 'quote' argument is by default:
> >
> > quote = "\"'"
> >
> > which should take care of the quoted strings and bring them in as a
> > single value.
> >
> > The above presumes that the header row is also TAB delimited. If not,
> > you may have to set 'skip = 3' to skip over the header row and manually
> > set the column names.
> 
> Not quite.  You can open a connection, skip 2 rows and read one to get the 
> column names, then read the rest of the file using read.table on the open 
> connection using the column names you just read.
> 
> However, based on what we have been shown
> 
> read.table("data.gct", skip = 2, header = TRUE)
> 
> ought to work as the file looks as if it is white-space delimited (a tab 
> is white space).
> 
> >
> > HTH,
> >
> > Marc Schwartz
> >
> >
> > On Tue, 2005-07-19 at 13:52 -0400, mark salsburg wrote:
> >> This is all extremely helpful.
> >>
> >> The data turns out is a little atypical, the columns are tab-delemited
> >> except for the description columns
> >>
> >>
> >> DATA1.gct looks like this
> >>
> >> #1.2
> >> 23 3423
> >> NAME DESCRIPTION VALUE
> >> gene1 "a protein inducer" 1123
> >> .....          .................     ......
> >>
> >> How do I get R to read the data as tab delemited, but read in the 2nd
> >> coloumn as one value based on the quotation marks..
> >>
> >> thanks..
> >>
> >> On 7/19/05, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> >>> On Tue, 2005-07-19 at 13:16 -0400, mark salsburg wrote:
> >>>> ok so the gct file looks like this:
> >>>>
> >>>> #1.2  (version number)
> >>>> 7283 19   (matrix size)
> >>>> Name Description Values
> >>>> ....      .......          ......
> >>>>
> >>>> How can I tell R to disregard the first two lines and start reading
> >>>> the 3rd line in this gct file. I would just delete them, but I do not
> >>>> know how to open a gct. file
> >>>>
> >>>> thank you
> >>>>
> >>>> On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >>>>> On 7/19/2005 12:10 PM, mark salsburg wrote:
> >>>>>> I have two files to compare, one is a regular txt file that I can read
> >>>>>> in no prob.
> >>>>>>
> >>>>>> The other is a .gct file (How do I read in this one?)
> >>>>>>
> >>>>>> I tried a simple
> >>>>>>
> >>>>>> read.table("data.gct", header = T)
> >>>>>>
> >>>>>> How do you suggest reading in this file??
> >>>>>>
> >>>>>
> >>>>> .gct is not a standard filename extension.  You need to know what is in
> >>>>> that file.  Where did you get it?  What program created it?
> >>>>>
> >>>>> Chances are the easiest thing to do is to get the program that created
> >>>>> it to export in a well known format, e.g. .csv.
> >>>>>
> >>>>> Duncan Murdoch
> >>>
> >>>
> >>> The above would be consistent with the info in my reply.
> >>>
> >>> I guess if the format is consistent, as per Mark's example above, you
> >>> can use:
> >>>
> >>> read.table("data.gct", skip = 2, header = TRUE)
> >>>
> >>> which will start by skipping the first two lines and then reading in the
> >>> header row and then the data.
> >>>
> >>> See ?read.table
> >>>
> >>> HTH,
> >>>
> >>> Marc Schwartz
> >>>
> >>>
> >>>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ripley at stats.ox.ac.uk  Wed Jul 20 12:12:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 11:12:17 +0100 (BST)
Subject: [R] Need R Help
In-Reply-To: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>
References: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>
Message-ID: <Pine.LNX.4.61.0507201105230.467@gannet.stats>

You are probably not running a X server on your Windows machine, or if you 
are, you are not forwarding X connections.  Even then, this is likely to 
be unsatisfactory unless you have a very good X server and a fast 
connection.

Please read the help page for png, and note the See Also section which 
suggests a good solution to your dilemma.

On Wed, 20 Jul 2005, bhumir jhaveri wrote:

> I am newbie in R.

Have you read the posting guide?

> I have R installed on Solaris machine and I am connected
> to Solaris machine from Windows using telnet. I want to create png image
> file using R. But when I issue R command from telnet:-
>
>> png("test.png")
>
> Following error occurs:
>
> Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> :
>        unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display ''
>
>  Also, when I checked DISPLAY environment variable, it shows
> 67.145.62.110:0.0
>
> (67.145.62.110) is windows machine's ip address
>
> Please suggest any solutions for the stated problem.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jul 20 12:16:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 12:16:33 +0200
Subject: [R] Question about creating unique factor labels with the
	factor	function
In-Reply-To: <42DE3B9B.17362.F001E5@localhost>
References: <42DE3B9B.17362.F001E5@localhost>
Message-ID: <x2ek9t3hq6.fsf@turmalin.kubism.ku.dk>

"Petr Pikal" <petr.pikal at precheza.cz> writes:

> Hallo
> 
> I am not sure what you want to achieve.
> 
> your factor has 3 levels but with only 2 different labels
> 
> > hb
> [1] 1 1 1 1 1 1 2 2 2
> Levels: 1 1 2
> 
> but
> 
> > str(hb)
>  Factor w/ 3 levels "1","1","2": 1 1 1 2 2 2 3 3 3
> 
> so you gave only one label to level 1 and 2. You can give the same 
> label to any level you choose but I can not see a reason in it.

Well, it does look a bit like a bug compared to the semantics of e.g.
levels<-  In particular:

> hb <- factor(c(1,1,1,2,2,2,3,3,3), levels=c(1,2,3),labels=c(1,1,2))
> hb
[1] 1 1 1 1 1 1 2 2 2
Levels: 1 1 2
> levels(hb) <- levels(hb)
> hb
[1] 1 1 1 1 1 1 2 2 2
Levels: 1 2

Conceptually, one might want to replace 

    attr(f, "levels") <-

with 

    levels(f) <- 

inside factor(). However, f does not have class "factor" at that point
and if it did, we'd have a recursive call to factor() and the thing
would blow up... 


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From phgrosjean at sciviews.org  Wed Jul 20 12:24:28 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 20 Jul 2005 12:24:28 +0200
Subject: [R] sciviews installation
In-Reply-To: <42DD678C.2030700@gmail.com>
References: <42DD678C.2030700@gmail.com>
Message-ID: <42DE265C.1090305@sciviews.org>

Hello,

Several changes in the latest Rcmdr broken the SciViews compatibility. I 
am working on it... but it takes longer than expected. Sorry for the 
inconvenience.
Best,

Philippe

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

ghjmora g mail wrote:
> Hello
> 
> 
> 1. a few months ago, I had sciviews working fine with R (rw2001) under 
> windows XP
> 2. now, upgrading to rw2011, the stuff seems fine (every package 
> installed),but I find a conflict when launching sciviews:
> - it runs, apparently
> - but when I try to work ("import/export In: text" for instance), it 
> asks for Rcmdr ("Would you like to install it now?")
> 
> 3. Rcmdr is already installed (with all dependencies) and works well 
> when called directly in R gui
> 4. and it's impossible to make it reconized or to install it under sciviews
> 
> 
> I have all the latest packages, and I am going to get mad.
> 
> what do you suggest to solve my problem ?
> 
> Thanks
> 
> Georges Moracchini
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From murdoch at stats.uwo.ca  Wed Jul 20 12:27:48 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Jul 2005 06:27:48 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
In-Reply-To: <33871de6050719185047ade71e@mail.gmail.com>
References: <33871de605071911531e399f4d@mail.gmail.com>	
	<42DD4F6B.7090600@stats.uwo.ca>	
	<33871de605071912341b778fdb@mail.gmail.com>	
	<42DD5B78.40906@stats.uwo.ca>
	<33871de6050719185047ade71e@mail.gmail.com>
Message-ID: <42DE2724.2020209@stats.uwo.ca>

James McDermott wrote:
> Would the unique quadratic defined by the three points be the same
> curve as the curve predicted by a quadratic B-spline (fit to all of
> the data) through those same three points?

Yes, if you restrict attention to an interval between knots.  You'll 
need to re-evaluate it for each such interval (but since quadratic 
splines are continuous, you can reuse evaluations at the knots, and you 
just need one new point in each interval).

 From a practical point of view, you need to make sure that COBS really 
is giving you a quadratic spline and really is reporting all of the 
knots correctly.  Watch out for coincident knots (zero length 
intervals); you don't care about the derivative on those, but they might 
cause overflows in some calculations.

Duncan Murdoch
> 
> Jim
> 
> On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> 
>>On 7/19/2005 3:34 PM, James McDermott wrote:
>>
>>>I wish it were that simple (perhaps it is and I am just not seeing
>>>it).  The output from cobs( ) includes the B-spline coefficients and
>>>the knots.  These coefficients are not the same as the a, b, and c
>>>coefficients in a quadratic polynomial.  Rather, they are the
>>>coefficients of the quadratic B-spline representation of the fitted
>>>curve.  I need to evaluate a linear combination of basis functions and
>>>it is not clear to me how to accomplish this easily.  I was hoping to
>>>find an alternative way of getting the derivatives.
>>
>>I don't know COBS, but doesn't predict just evaluate the B-spline?  The
>>point of what I posted is that the particular basis doesn't matter if
>>you can evaluate the quadratic at 3 points.
>>
>>Duncan Murdoch
>>
>>
>>>Jim McDermott
>>>
>>>On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>>>
>>>>On 7/19/2005 2:53 PM, James McDermott wrote:
>>>>
>>>>>Hello,
>>>>>
>>>>>I have been trying to take the derivative of a quadratic B-spline
>>>>>obtained by using the COBS library.  What I would like to do is
>>>>>similar to what one can do by using
>>>>>
>>>>>fit<-smooth.spline(cdf)
>>>>>xx<-seq(-10,10,.1)
>>>>>predict(fit, xx, deriv = 1)
>>>>>
>>>>>The goal is to fit the spline to data that is approximating a
>>>>>cumulative distribution function (e.g. in my example, cdf is a
>>>>>2-column matrix with x values in column 1 and the estimate of the cdf
>>>>>evaluated at x in column 2) and then take the first derivative over a
>>>>>range of values to get density estimates.
>>>>>
>>>>>The reason I don't want to use smooth.spline is that there is no way
>>>>>to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
>>>>>with COBS.  However, since COBS doesn't have the 'deriv =' option, the
>>>>>only way I can think of doing it with COBS is to evaluate the
>>>>>derivatives numerically.
>>>>
>>>>Numerical estimates of the derivatives of a quadratic should be easy to
>>>>obtain accurately.  For example, if the quadratic ax^2 + bx + c is
>>>>defined on [-1, 1], then the derivative 2ax + b, has 2a = f(1) - f(0) +
>>>>f(-1), and b = (f(1) - f(-1))/2.
>>>>
>>>>You should be able to generalize this to the case where the spline is
>>>>quadratic between knots k1 and k2 pretty easily.
>>>>
>>>>Duncan Murdoch
>>>>
>>
>>



From ramasamy at cancer.org.uk  Wed Jul 20 12:27:52 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 20 Jul 2005 11:27:52 +0100
Subject: [R] Need R Help
In-Reply-To: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>
References: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>
Message-ID: <1121855272.6004.12.camel@ipc143004.lif.icnet.uk>

Your alternatives are :
a) Use pdf if you can.
b) See help(bitmap) and especially the details section which tells you
about setting the R_GSCMD path to ghostscript
c) Try Xvfb. I do not know much about this. Try a google search.

Regards, Adai


On Wed, 2005-07-20 at 15:12 +0530, bhumir jhaveri wrote:
> 
> Hi,
> 
> I am newbie in R. I have R installed on Solaris machine and I am connected
> to Solaris machine from Windows using telnet. I want to create png image
> file using R. But when I issue R command from telnet:-
> 
> > png("test.png")
> 
> Following error occurs:
> 
> Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> :
>         unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display ''
> 
>   Also, when I checked DISPLAY environment variable, it shows
> 67.145.62.110:0.0
> 
> (67.145.62.110) is windows machine's ip address
> 
> Please suggest any solutions for the stated problem.
> 
> Thanks,
> 
> Bhumir
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From A.Robinson at ms.unimelb.edu.au  Wed Jul 20 12:47:02 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 20 Jul 2005 20:47:02 +1000
Subject: [R] Need R Help
In-Reply-To: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>
References: <LLEEIDEALBPCCGMALPCLMEFACAAA.b.jhaveri@decisioncraft.com>
Message-ID: <20050720104702.GE97353@ms.unimelb.edu.au>

Hi Bhumir

Do you happen to have an X client installed on your Windows machine?

e.g. cygwin, winaXe, eXceed, ... 

I've used winaXe with some success.

Andrew


On Wed, Jul 20, 2005 at 03:12:24PM +0530, bhumir jhaveri wrote:
> 
> 
> Hi,
> 
> I am newbie in R. I have R installed on Solaris machine and I am connected
> to Solaris machine from Windows using telnet. I want to create png image
> file using R. But when I issue R command from telnet:-
> 
> > png("test.png")
> 
> Following error occurs:
> 
> Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,
> :
>         unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display ''
> 
>   Also, when I checked DISPLAY environment variable, it shows
> 67.145.62.110:0.0
> 
> (67.145.62.110) is windows machine's ip address
> 
> Please suggest any solutions for the stated problem.
> 
> Thanks,
> 
> Bhumir
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From ggrothendieck at gmail.com  Wed Jul 20 12:47:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Jul 2005 06:47:24 -0400
Subject: [R] Problems with date-format (R 2.1.1 + chron)
In-Reply-To: <971536df0507191927cbdac17@mail.gmail.com>
References: <E1Dv07c-0008Lh-Fz@s2.stud.uni-goettingen.de>
	<971536df0507191849374d951f@mail.gmail.com>
	<971536df0507191927cbdac17@mail.gmail.com>
Message-ID: <971536df05072003475d2d8c57@mail.gmail.com>

Here are some different approaches to this problem:

#1 just convert it to chron without using Date class.
days(chron("21-07-2005", format = "d-m-y"))

#2 after converting it to Date, convert it to numeric and then
# to chron to avoid the problem
dd <- as.Date("21-07-2005", "%d-%m-%y")
days(chron(unclass(dd)))

#3 do not use chron in the first place, just Date
dd <- as.Date("21-07-2005", "%d-%m-%y")
ordered(format(dd, "%d"), lev = 1:31)

Note that days in the chron package gives the answer as an ordered 
factor, which is what is assumed is desired in these examples as
well as pointers to further information in the references.

R News 4/1 Help Desk has some examples of date manipulations
in Date, chron and POSIXct in the table at the end of the article.

On 7/19/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/19/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > I think its likely that you are using different versions of chron.
> > I noticed that version "2.2-33" of chron had the statement:
> >
> >   tms <- dts - trunc(dts)
> >
> > but version "2.2-35" seems to have replaced it with:
> >
> >  tms <- dts - floor(dts)
> >
> > and that seems to be causing the problem.
> >
> > As a workaround:
> >
> >    floor.Date <- floor.trunc
> 
> That should have been:
> 
>      floor.Date <- trunc.Date
> 
> >    days(as.Date("21-07-2005", "%d-%m-%y"))
> >
> >
> > On 7/19/05, Carsten Steinhoff <carsten.steinhoff at stud.uni-goettingen.de> wrote:
> > > Hello,
> > >
> > > today I've updated on the newest R-Version. But sadly a function I needed
> > > didnt want to work:
> > > The input is e.g.
> > >
> > > days(as.Date("21-07-2005","%d-%m-%y"))
> > >
> > > the error is: Fehler in Math.Date(dts): floor nicht definiert f??r Date
> > > Objekte
> > > (Error in Math.Date(dts): floor not defined for date objects)
> > >
> > > Same for year. Only months gives me the correct output.
> > > In Version 2.01 it worked very well, with the same chron library.
> > > Whats wrong ?
> > >
> > > Carsten
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
>



From j.van_den_hoff at fz-rossendorf.de  Wed Jul 20 12:56:25 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 20 Jul 2005 12:56:25 +0200
Subject: [R] Michaelis-menten equation
In-Reply-To: <20050720020944.M51196@cc.kmu.edu.tw>
References: <20050719084736.M53999@cc.kmu.edu.tw>	<42DCDB38.4050705@fz-rossendorf.de>
	<20050720020944.M51196@cc.kmu.edu.tw>
Message-ID: <42DE2DD9.4040002@fz-rossendorf.de>

I believe the following is correct:
1.
first of all, as peter daalgaard already pointed out, your data Cp(t)
are following a straight line
very closely, i.e. 0.-order kinetics
2.
for your diff. eq. this means that you are permanently in the range cp
>> Km so that
dCp/dt = - Vm/Vd = const. =: -b and, therefore, Cp = a - b*t
3.
you can't get any reliable information concerning Km from the fit. the
solution of the diff. eq (according to Maxima),
namely t + const. = -(Km*log(Cp) + Cp)/(Vm/Vd) tells you the same:
Km*log(cp) << Cp in your data.
4.
in any case (even in the range Km ~= Cp) you can't determine Vm _and_ Vd
separately according to your diff. eq., you only get the ratio b =
Vm/Vd. this does make sense:
what you are measuring  is the decreasing plasma concentration, you
don't have any information
concerning the "relevant" volume fraction, i.e. the Vd, in you data.
therefore any variation in the effective
max. velocity can either be ascribed to a variation of Vm or to a
modified Vd. in other words:
you should view Vm* = Vm/Vd as your  "effective" Vm.
5.
x<-PKindex[,1]
y<-PKindex[,2]
res <- lm ( y~x ) yields a=8.561, b=Vm*= 0.279
summary(abs(residuals(r)/y)*100)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
0.00863 0.09340 0.13700 0.26500 0.22900 1.37000

i.e. 0.265 percent deviation on average between data and fit. I believe
that is the max. information you can get from your data
((the result for "b" is  accidently is not so far away
from the ratio of your results Vm = 10.04, Vd = 34.99 which actually
must be completely unstable
(double both parameters and nothing happens to the fit).
6.
you state you simulated the data with km=4.69? using the above Vm and
Vd, the resulting data are (not unexpectedly)
quite different from those you used as input to the fit. maybe you made
an error somewhere "upstream"?
7.
in conclusion: don't try to fit Vm _and_ Vd separately, check whether
your simulated data are correct, keep in mind that if km<<Cp, you can't
fit Km (at least not reliably).



Chun-Ying Lee wrote:
> Hi,
> 
>    We are doing a pharmaockinetic modeling.  This model is
> described as intravenous injection of a certain drug into
> the body.  Then the drug molecule will be eliminated (or decayed)
> from the body.  We here used a MM eq. to describe the elimination
> process and the changes of the drug conc..  So the diff. eq. would
> be: dCp/dt = -Vm/Vd * Cp/(Km+Cp).  Vd is a volume of distribution.
> We used lsoda to solve the diff. eq. first and fit the diff. eq.
> with optim first (Nelder-Mead simplex) and followed by using nls 
> to take over the fitting process of optim.  However, we can not
> obtain the correct value for Km if we used the above model.  The
> correct Km can be obtained only when we modeled the diff eq. with
> dCp/dt= -Vm/Vd * Cp/(Km/vd + Cp).  Now we lost.  The data were
> from simulation with known Vm and Km.  Any idea?  Thanks.
> 
> regards,
> --- Chun-ying Lee
> 
>>it is not clear to me what you are trying to do:
>>you seem to have a time-concentration-curve in PKindex and you seem 
>>to set up a derivative of this time dependency according to some 
>>model in dCpdt. AFAIKS this scenario is  not directly related to the 
>>situation described by the Michaelis-Menten-Equation which relates 
>>some "input" concentration with some "product" concentration. If Vm and
>>Km are meant to be the canonical symbols,
>>what is Vd, a volume of distribution? it is impossible to see (at least
>>for me) what exactly you want to achieve.
>>
>>(and in any case, I would prefer "nls" for a least squares fit 
>>instead of 'optim').
>>
>>joerg
>>
>>>------------------------------------------------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-
> 
> guide.html
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Wed Jul 20 13:23:53 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 20 Jul 2005 13:23:53 +0200
Subject: [R] initial points for arms in package HI
In-Reply-To: <1121807907.42dd6e23f2e8e@webmail.iarc.fr>
References: <17117.12889.773438.382880@stat.math.ethz.ch>
	<1121807907.42dd6e23f2e8e@webmail.iarc.fr>
Message-ID: <17118.13385.262621.784818@stat.math.ethz.ch>

Dear Martyn

Thank you for your fast and helpful answer. It showed me to
my wrong thinking.

I confused the previous value of the Markov Chain with the
initial values to construct the envelope.

In the original C code by W. Gilks there are the arguments
"xinit" and "xprev". The first is used to construct the
envelope, the second is the previous value of the Markov chain.
There are comments from the author how to set the initial values
"xinit". It is important to choose these values independent from
the current parameter, being updated (in the case of not
log-concave functions).

In the R code and the C code behind the function arms the
argument "xinit" is missing, since it is set in the function
itself and the user can not change it.

Since I confused the two arguments I didn't realize that
"y.start" is the value "xprev", the state of the Markov chain
and set this value 1 or runif(...). But for an implementation of
a Gibbs Sampler this makes no sense.

My example was a little bit misleading, since I simulated 1000
points from the same distribution. In reality my distribution
is changing at each step and I always need only 1 new value in
my Gibbs Sampler. 
But by implementing it correctly with the previous value of the
chain it should work in my context.

Best regards,

Christoph


plummer at iarc.fr writes:
 > Quoting Christoph Buser <buser at stat.math.ethz.ch>:
 > 
 > > Dear R-users
 > >
 > > I have a problem choosing initial points for the function arms()
 > > in the package HI
 > > I intend to implement a Gibbs sampler and one of my conditional
 > > distributions is nonstandard and not logconcave.
 > > Therefore I'd like to use arms.
 > >
 > > But there seem to be a strong influence of the initial point
 > > y.start. To show the effect I constructed a demonstration
 > > example. It is reproducible without further information.
 > >
 > > Please note that my target density is not logconcave.
 > >
 > > Thanks for all comments or ideas.
 > >
 > > Christoph Buser
 > 
 > Dear Christoph,
 > 
 > There is a Metropolis step at each iteration of the ARMS sampler, in
 > which it may choose to reject the proposed move to a new point and stick
 > at the current point (This is what the "M" in "ARMS" stands for)  If you
 > do repeated calls to arms with the same starting point, then the
 > iterations where the Metropolis step rejects a move will create a spike
 > in the sample density at your initial value. If you use a uniform random
 > starting point, then your sample density will be a mixture of the
 > target distribution (Metropolis accepts move) and a uniform distribution
 > (Metropolis rejects move).
 > 
 > You should be doing something like this:
 > 
 > res1 <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1)
 > for(i in 2:1000)
 >   res1[i] <- arms(res1[i-1], logDichteGam, function(x) (x>0)&(x<100), 1)
 > 
 > i.e., using each sampled point as the starting value for the next
 > iteration.  The sequence of values in res1 will then be a correlated
 > sample from the given distribution:
 > 
 > acf(res1)
 > 
 > The bottom line is that you can't use ARMS to draw a single sample
 > from a non-log-concave density.
 > 
 > If you are still worried about using ARMS, you can verify your results
 > using the random walk Metropolis sampler (MCMCmetrop1R) in the package
 > MCMCpack.
 > 
 > Martyn
 > 
 > > ## R Code:
 > >
 > > library(HI)
 > > ## parameter for the distribution
 > > para <- 0.1
 > >
 > > ## logdensity
 > > logDichteGam <- function(x, u = para, v = para) {
 > >   -(u*x + v*1/x) - log(x)
 > > }
 > > ## density except for the constant
 > > propDichteGam <- function(x, u = para, v = para) {
 > >   exp(-(u*x + v*1/x) - log(x))
 > > }
 > > ## calculating the constant
 > > (c <- integrate(propDichteGam, 0, 1000, rel.tol = 10^(-12))$value)
 > > ## density
 > > DichteGam <- function(x, u = para, v = para) {
 > >   exp(-(u*x + v*1/x) - log(x))/c
 > > }
 > >
 > > ## calculating 1000 values by repeating a call of arms (this would
 > > ## be the situation in an Gibbs Sample. Of course in a Gibbs sampler
 > > ## the distribution would change. This is only for demonstration
 > > res1 <- NULL
 > > for(i in 1:1000)
 > >   res1[i] <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1)
 > >
 > > ## Generating a sample of thousand observations with 1 call of arms
 > > res2 <- arms(runif(1,0,100), logDichteGam, function(x) (x>0)&(x<100), 1000)
 > >
 > > ## Plot of the samples
 > > mult.fig(4)
 > > plot(res1, log = "y")
 > > plot(res2, log = "y")
 > > hist(res1, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
 > >      ylim = c(0,1))
 > > curve(DichteGam, 0,4, add = TRUE, col = 2)
 > > hist(res2, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
 > >      ylim = c(0,1))
 > > curve(DichteGam, 0,4, add = TRUE, col = 2)
 > >
 > >
 > > ## If we repeat the procedure, using the fix intial value 1,
 > > ## the situation is even worse
 > > res3 <- NULL
 > > for(i in 1:1000)
 > >   res3[i] <- arms(1, logDichteGam, function(x) (x>0)&(x<100), 1)
 > >
 > > ## Generating a sample of thousand observations with 1 call of arms
 > > res4 <- arms(1, logDichteGam, function(x) (x>0)&(x<100), 1000)
 > >
 > > ## Plot of the samples
 > > par(mfrow = c(2,2))
 > > plot(res3, log = "y")
 > > plot(res4, log = "y")
 > > hist(res3, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
 > >      ylim = c(0,1))
 > > curve(DichteGam, 0,4, add = TRUE, col = 2)
 > > hist(res4, freq = FALSE, xlim = c(0,4), breaks = seq(0,100,by = 0.1),
 > >      ylim = c(0,1))
 > > curve(DichteGam, 0,4, add = TRUE, col = 2)
 > >
 > >
 > > ## If I generate the sample in a for-loop (one by one) I do not
 > > ## get the correct density. But this is exactly the situation in
 > > ## my Gibbs Sampler. Therfore I am concerned about the correct
 > > ## application of arms
 > >
 > 
 > 
 > 
 > -----------------------------------------------------------------------
 > This message and its attachments are strictly confidential. If you are
 > not the intended recipient of this message, please immediately notify 
 > the sender and delete it. Since its integrity cannot be guaranteed, 
 > its content cannot involve the sender's responsibility. Any misuse, 
 > any disclosure or publication of its content, either whole or partial, 
 > is prohibited, exception made of formally approved use
 > -----------------------------------------------------------------------



From ggrothendieck at gmail.com  Wed Jul 20 13:25:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Jul 2005 07:25:11 -0400
Subject: [R] R function to kill a process
Message-ID: <971536df05072004252b0d2336@mail.gmail.com>

Is there an R function to kill a process?  I found one in package
fork but it is specific to UNIX and I want something that also
works on Windows.   The XP console command, taskkill,
will do it so I can easily get the effect but it won't work
on other Windows systems, even 2000 and NT.  I found a free utility
pskill.exe by googling around that does work across 2000/NT/XP
but was still wondering about something more general in R.



From fooms at euroscreen.com  Wed Jul 20 13:44:27 2005
From: fooms at euroscreen.com (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Wed, 20 Jul 2005 13:44:27 +0200
Subject: [R] Chemoinformatic people
Message-ID: <5198ADA420721246BC35BFA666E24F16D743B4@euromail.euroscreen.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050720/8e829441/attachment.pl

From buser at stat.math.ethz.ch  Wed Jul 20 13:44:57 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 20 Jul 2005 13:44:57 +0200
Subject: [R] Code Verification
In-Reply-To: <1121794130.42dd3852f127f@webmail.scsv.nevada.edu>
References: <1121794130.42dd3852f127f@webmail.scsv.nevada.edu>
Message-ID: <17118.14649.93562.642606@stat.math.ethz.ch>

Hi

"t.test" assumes that your data within each group has a normal
distribution. This is not the case in your example.
I would recommend you a non parametric test like "wilcox.test" if
you want to compare the mean of two samples that are not normal
distributed. 
see ?wilcox.test

Be careful. Your example produces two gamma distributed samples
with rate = 10, not scale = 10. 
rate = 1/scale.
If you want to use scale, you need to specify this argument 
x<-rgamma(40, 2.5, scale = 10)
see ?rgamma

I do not see the interpretation of your result. Since you do
know the distribution and the parameters of your sample, you
know the true means and that they are different. It is only a
question of the sample size and the power of your test, if this
difference is detected.
Is that something you are investigating? Maybe a power
calculation or something similar.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

pantd at unlv.nevada.edu writes:
 > Hi R Users
 > I have a code which I am running for my thesis work. Just want to make sure that
 > its ok. Its a t test I am conducting between two gamma distributions with
 > different shape parameters.
 > 
 > the code looks like:
 > 
 > sink("a1.txt");
 > 
 > for (i in 1:1000)
 > {
 > x<-rgamma(40, 2.5, 10)  # n = 40, shape = 2.5, Scale = 10
 > y<-rgamma(40, 2.8, 10)  # n = 40, shape = 2.8, Scale = 10
 > z<-t.test(x, y)
 > print(z)
 > }
 > 
 > 
 > I will appreciate it if someone could tell me if its alrite or not.
 > 
 > thanks
 > 
 > -dev
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Jul 20 13:52:23 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 20 Jul 2005 07:52:23 -0400
Subject: [R] sciviews installation
In-Reply-To: <42DE265C.1090305@sciviews.org>
Message-ID: <20050720115215.SLZJ27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Philippe,

The development version 1.1-0 of the Rcmdr package (i.e., the one with
localization facilities) isn't yet on CRAN. Georges doesn't say which
version of the Rcmdr package he's using, but I assume 1.0-2 from CRAN, which
I believe should work with SciViews.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Philippe Grosjean
> Sent: Wednesday, July 20, 2005 5:24 AM
> To: ghjmora at gmail.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] sciviews installation
> 
> Hello,
> 
> Several changes in the latest Rcmdr broken the SciViews 
> compatibility. I am working on it... but it takes longer than 
> expected. Sorry for the inconvenience.
> Best,
> 
> Philippe
> 
> ..............................................<??}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>   ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>   ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>   ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>   ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
> 
> ghjmora g mail wrote:
> > Hello
> > 
> > 
> > 1. a few months ago, I had sciviews working fine with R 
> (rw2001) under 
> > windows XP 2. now, upgrading to rw2011, the stuff seems fine (every 
> > package installed),but I find a conflict when launching sciviews:
> > - it runs, apparently
> > - but when I try to work ("import/export In: text" for 
> instance), it 
> > asks for Rcmdr ("Would you like to install it now?")
> > 
> > 3. Rcmdr is already installed (with all dependencies) and 
> works well 
> > when called directly in R gui 4. and it's impossible to make it 
> > reconized or to install it under sciviews
> > 
> > 
> > I have all the latest packages, and I am going to get mad.
> > 
> > what do you suggest to solve my problem ?
> > 
> > Thanks
> > 
> > Georges Moracchini
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From S.O.Nyangoma at amc.uva.nl  Wed Jul 20 13:52:35 2005
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Wed, 20 Jul 2005 13:52:35 +0200
Subject: [R] Chemoinformatic people
Message-ID: <11f504711edf2c.11edf2c11f5047@amc.uva.nl>

Do you mean people working in areas of mass spectrometry (eg. SELDI, 
LC-MS, MALDI etc) data analysis?

----- Original Message -----
From: Fr??d??ric Ooms <fooms at euroscreen.com>
Date: Wednesday, July 20, 2005 1:44 pm
Subject: [R] Chemoinformatic people

> Dear colleague,
> Just an e-mail to know if they are people working in the field of 
> chemoinformatic that are using R in their work. If yes I was 
> wondering if we couldn't exchange tips and tricks about the use of 
> R in this area ?
> Best regards
> Fred Ooms
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From fooms at euroscreen.com  Wed Jul 20 13:57:19 2005
From: fooms at euroscreen.com (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Wed, 20 Jul 2005 13:57:19 +0200
Subject: [R] Chemoinformatic people
Message-ID: <5198ADA420721246BC35BFA666E24F16D743B5@euromail.euroscreen.be>

I am more lloking at people working in the field of drug discovery performing clustering analysis, MDS, LDA in order to classify chemicals based on  computed properties.
Fred

-----Original Message-----
From: S.O. Nyangoma [mailto:S.O.Nyangoma at amc.uva.nl] 
Sent: Wednesday, July 20, 2005 1:53 PM
To: Fr??d??ric Ooms
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Chemoinformatic people


Do you mean people working in areas of mass spectrometry (eg. SELDI, 
LC-MS, MALDI etc) data analysis?

----- Original Message -----
From: Fr??d??ric Ooms <fooms at euroscreen.com>
Date: Wednesday, July 20, 2005 1:44 pm
Subject: [R] Chemoinformatic people

> Dear colleague,
> Just an e-mail to know if they are people working in the field of
> chemoinformatic that are using R in their work. If yes I was 
> wondering if we couldn't exchange tips and tricks about the use of 
> R in this area ?
> Best regards
> Fred Ooms
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From murdoch at stats.uwo.ca  Wed Jul 20 14:03:51 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Jul 2005 08:03:51 -0400
Subject: [R] R function to kill a process
In-Reply-To: <971536df05072004252b0d2336@mail.gmail.com>
References: <971536df05072004252b0d2336@mail.gmail.com>
Message-ID: <42DE3DA7.5090901@stats.uwo.ca>

On 7/20/2005 7:25 AM, Gabor Grothendieck wrote:
> Is there an R function to kill a process?  I found one in package
> fork but it is specific to UNIX and I want something that also
> works on Windows.   The XP console command, taskkill,
> will do it so I can easily get the effect but it won't work
> on other Windows systems, even 2000 and NT.  I found a free utility
> pskill.exe by googling around that does work across 2000/NT/XP
> but was still wondering about something more general in R.

There's no such thing in base R, as far as I know.  Killing a process is 
complicated, and I think the procedure is quite different in Unix and 
Windows.  You want to give the process a chance at a clean shutdown if 
possible.

Duncan Murdoch



From pbarros at ualg.pt  Wed Jul 20 14:32:36 2005
From: pbarros at ualg.pt (Pedro de Barros)
Date: Wed, 20 Jul 2005 13:32:36 +0100
Subject: [R] Randomization test for interaction effect
Message-ID: <6.1.2.0.2.20050720131129.02fb02c8@pop.ualg.pt>

Dear All,

I am trying to build a randomization test for interaction

The problem is as follows: I have a set of stations where the ocurrence and 
biomass of each species being investigated was recorded.

For each group of stations, the relative importance of a set of species was 
calculated, as the Index of Relative Importance (IRI), given as

IRI(i)=(FO(i)*RW(i))/sum(FO(j)*RW(j))

FO(i) is the relative frequency of ocurrence of species i and RW(i) is the 
relative Biomass of species i (Total biomass of this species/Total biomass 
of all species). Stations are grouped according to 3 different criteria, 
say A, B and C, with 3 levels of A, 5 of B, and 3 of C.

The null hypothesis of interest is that the distribution of the IRI among 
the species is independent of these classification levels. I suggest using 
the average difference in IRI(i) of each grouping to the overall IRI(i) as 
the test statistic.

I have no difficulty building the randomization test for the main effects: 
Simply randomize the corresponding group membership factor. However, I am 
having difficulty building the test for the interaction effect: As far as I 
understand, the test should randomize the levels of say B, within the 
levels of A, so that the main effects should not be affected (all 
observations classified as level A1 would still be A1, and all observations 
classified as level B1 should stay as B1). My difficulty is how to 
calculate the "overall" distribution of the IRI in this case. This should 
consider all main effects, but no interaction.

I would really appreciate any pointer to a solution of this problem. I 
believe it is not complicated (and probably quite obvious) but the solution 
keeps out of reach, even though I have been searching for over a week.

Thanks,
Pedro

Pedro de Barros
Faculdade de Ci??ncias do Mar e do Ambiente
Universidade do Algarve ,Campus de Gambelas
8000-117 FARO, Portugal
Tel: + 351 289 800 034
Fax: +351 289 818 353
e-mail: pbarros at ualg.pt



From andy_liaw at merck.com  Wed Jul 20 14:35:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Jul 2005 08:35:14 -0400
Subject: [R] Chemoinformatic people
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAD4@usctmx1106.Merck.com>

You might want to check out Rajarshi Guha's work on connecting CDK to R.  He
has an article in the recent issue of CDK News.

Andy

> From: Fr??d??ric Ooms
> 
> I am more lloking at people working in the field of drug 
> discovery performing clustering analysis, MDS, LDA in order 
> to classify chemicals based on  computed properties.
> Fred
> 
> -----Original Message-----
> From: S.O. Nyangoma [mailto:S.O.Nyangoma at amc.uva.nl] 
> Sent: Wednesday, July 20, 2005 1:53 PM
> To: Fr??d??ric Ooms
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Chemoinformatic people
> 
> 
> Do you mean people working in areas of mass spectrometry (eg. SELDI, 
> LC-MS, MALDI etc) data analysis?
> 
> ----- Original Message -----
> From: Fr??d??ric Ooms <fooms at euroscreen.com>
> Date: Wednesday, July 20, 2005 1:44 pm
> Subject: [R] Chemoinformatic people
> 
> > Dear colleague,
> > Just an e-mail to know if they are people working in the field of
> > chemoinformatic that are using R in their work. If yes I was 
> > wondering if we couldn't exchange tips and tricks about the use of 
> > R in this area ?
> > Best regards
> > Fred Ooms
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting- 
> > guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From RRoa at fisheries.gov.fk  Wed Jul 20 12:37:43 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 20 Jul 2005 08:37:43 -0200
Subject: [R] analysing non-normal spatially autocorrelated data
Message-ID: <03DCBBA079F2324786E8715BE538968A068EA9@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Carsten Dormann
> Sent: 20 July 2005 05:41
> To: r-help at stat.math.ethz.ch
> Subject: [R] analysing non-normal spatially autocorrelated data
> 
> 
> Dear fellow R-users,
> 
> I wish to analyse a lattice of presence-absence data which 
> are spatially autocorrelated.
> 
> For normally distributed errors I used gls {nlme} with the 
> "appropriate" corStruct-method.
> Is there any method for other families (binomial and poisson)?
> 
> A method that look suitable to me as a non-statistician is 
> called gllamm  (generalised linear latent mixed model), by Rabe-Hesketh et 
> al (2001), available apparently only for Stata.
> In R, I found the gamm {Matrix} function doing what I want, but I am 
> interested in the parameter values of the covariates, using the model 
> for prediction, hence gamm is no option.
> Finally, Dan Bebber posted a similar question to the R-help list in 
> September 2004 (about using corStruct in glmmPQL), but there 
> is no reply in the thread 
> (http://tolstoy.newcastle.edu.au/R/help/04/09/3103.html).
> 
> Any suggestions are highly welcome.
> 
> Many thanks,
> 
> Carsten

Check the package geoRglm, which fits by maximum likelihood a generalized 
linear mixed spatial model in the binomial or Poisson families, allowing for
covariates. geoRglm will also need the package geoR which fits the 
spatial model for continuous processes.
The authors of the packages have published several papers on theory
and applications.
Ruben



From David.Ruau at rwth-aachen.de  Wed Jul 20 15:37:38 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Wed, 20 Jul 2005 15:37:38 +0200
Subject: [R] writing matrices (no subject)
In-Reply-To: <644e1f320507200607639159d0@mail.gmail.com>
References: <40d962e17d20eb7fd6d5508c3d94628a@rwth-aachen.de>
	<644e1f320507200607639159d0@mail.gmail.com>
Message-ID: <39566da74190f596a626e9a479753b28@rwth-aachen.de>

Thanks for your answers,
I need to print this data frame into a .csv file to import it in WEKA.
Do you have better solution?
I quite a new user of WEKA I don't know if you can give it a binary 
file. I think you can but it will be complicated...

David

On Jul 20, 2005, at 15:07, jim holtman wrote:

> You might have better luck if you have a loop that is processing one
> row at a time.  That means you will have to determine what the
> formatting should be.  With write.table it is trying to process the
> entire array at once to determine the best formatting and it taking a
> lot of memory (my guess at least 500MB).
>
> What are you going to do with a text file that large?  Can you write
> it out in binary if you are reading it in with another program?  If
> you are going to reread it with R, then 'save' would be a better
> choice.
>
> On 7/20/05, David Ruau <David.Ruau at rwth-aachen.de> wrote:
>> Hi All,
>>
>> I want to print a square matrix of 7000 x 7000 into a text file. But I
>> got a error after few hours of computation...
>> --------
>>> write.table(MyDistMxDF, file = "temp.csv", sep=",", quote=F)
>> *** malloc: vm_allocate(size=8421376) failed (error code=3)
>> *** malloc[2889]: error: Can't allocate region
>> Error: vector memory exhausted (limit reached?)
>> *** malloc: vm_allocate(size=8421376) failed (error code=3)
>> *** malloc[2889]: error: Can't allocate region
>>>  q()
>> *** malloc: vm_allocate(size=8421376) failed (error code=3)
>> *** malloc[2889]: error: Can't allocate region
>> *** malloc: vm_allocate(size=8421376) failed (error code=3)
>> *** malloc[2889]: error: Can't allocate region
>> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
>>         internal error in decompress1
>>>
>> ------
>> I am running R 2.0.1 on MacOS X 10.3 with 1Gb ram.
>> How could I write such a matrix to a text file.
>>
>> David
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>
> -- 
> Jim Holtman
>
> What the problem you are trying to solve?
>
>



From andy_liaw at merck.com  Wed Jul 20 15:49:57 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Jul 2005 09:49:57 -0400
Subject: [R] writing matrices (no subject)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAD5@usctmx1106.Merck.com>

Simply gooling for "writing ARFF file in R" gave the following as first hit,
which is right on the WEKA page:

Miscellaneous code
[...]
Function for reading ARFF files into the R statistical package (kindly
provided by Dr Craig Struble). 
Function for writing ARFF files from the R statistical package (kindly
provided by Nigel Sim). 
http://www.cs.waikato.ac.nz/~ml/weka/example_code/writearff.R

I have not tried it myself.

Andy

PS: AFAIK, ARFF is not a binary format, but simply csv with header
information.

> From: David Ruau
> 
> Thanks for your answers,
> I need to print this data frame into a .csv file to import it in WEKA.
> Do you have better solution?
> I quite a new user of WEKA I don't know if you can give it a binary 
> file. I think you can but it will be complicated...
> 
> David
> 
> On Jul 20, 2005, at 15:07, jim holtman wrote:
> 
> > You might have better luck if you have a loop that is processing one
> > row at a time.  That means you will have to determine what the
> > formatting should be.  With write.table it is trying to process the
> > entire array at once to determine the best formatting and 
> it taking a
> > lot of memory (my guess at least 500MB).
> >
> > What are you going to do with a text file that large?  Can you write
> > it out in binary if you are reading it in with another program?  If
> > you are going to reread it with R, then 'save' would be a better
> > choice.
> >
> > On 7/20/05, David Ruau <David.Ruau at rwth-aachen.de> wrote:
> >> Hi All,
> >>
> >> I want to print a square matrix of 7000 x 7000 into a text 
> file. But I
> >> got a error after few hours of computation...
> >> --------
> >>> write.table(MyDistMxDF, file = "temp.csv", sep=",", quote=F)
> >> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> >> *** malloc[2889]: error: Can't allocate region
> >> Error: vector memory exhausted (limit reached?)
> >> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> >> *** malloc[2889]: error: Can't allocate region
> >>>  q()
> >> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> >> *** malloc[2889]: error: Can't allocate region
> >> *** malloc: vm_allocate(size=8421376) failed (error code=3)
> >> *** malloc[2889]: error: Can't allocate region
> >> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
> >>         internal error in decompress1
> >>>
> >> ------
> >> I am running R 2.0.1 on MacOS X 10.3 with 1Gb ram.
> >> How could I write such a matrix to a text file.
> >>
> >> David
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> >
> > -- 
> > Jim Holtman
> >
> > What the problem you are trying to solve?
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Wed Jul 20 15:52:55 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Jul 2005 09:52:55 -0400
Subject: [R] Code Verification
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAD6@usctmx1106.Merck.com>

> From: Christoph Buser
> 
> Hi
> 
> "t.test" assumes that your data within each group has a normal
> distribution. This is not the case in your example.

Eh?  What happen to the CLT?

Andy


> I would recommend you a non parametric test like "wilcox.test" if
> you want to compare the mean of two samples that are not normal
> distributed. 
> see ?wilcox.test
> 
> Be careful. Your example produces two gamma distributed samples
> with rate = 10, not scale = 10. 
> rate = 1/scale.
> If you want to use scale, you need to specify this argument 
> x<-rgamma(40, 2.5, scale = 10)
> see ?rgamma
> 
> I do not see the interpretation of your result. Since you do
> know the distribution and the parameters of your sample, you
> know the true means and that they are different. It is only a
> question of the sample size and the power of your test, if this
> difference is detected.
> Is that something you are investigating? Maybe a power
> calculation or something similar.
> 
> Regards,
> 
> Christoph Buser
> 
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
> 
> pantd at unlv.nevada.edu writes:
>  > Hi R Users
>  > I have a code which I am running for my thesis work. Just 
> want to make sure that
>  > its ok. Its a t test I am conducting between two gamma 
> distributions with
>  > different shape parameters.
>  > 
>  > the code looks like:
>  > 
>  > sink("a1.txt");
>  > 
>  > for (i in 1:1000)
>  > {
>  > x<-rgamma(40, 2.5, 10)  # n = 40, shape = 2.5, Scale = 10
>  > y<-rgamma(40, 2.8, 10)  # n = 40, shape = 2.8, Scale = 10
>  > z<-t.test(x, y)
>  > print(z)
>  > }
>  > 
>  > 
>  > I will appreciate it if someone could tell me if its alrite or not.
>  > 
>  > thanks
>  > 
>  > -dev
>  > 
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jhainm at fas.harvard.edu  Wed Jul 20 15:54:04 2005
From: jhainm at fas.harvard.edu (jhainm@fas.harvard.edu)
Date: Wed, 20 Jul 2005 09:54:04 -0400
Subject: [R] Prefix for colnames
Message-ID: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>


Hi,

I would like to add a prefix to colnames in a matrix but I can't get the prefix
option in colnames to work. What am I doing wrong?

> X<-matrix(NA,3,4)
> colnames(X)<-c("test","test","test","test")
> colnames(X)<-colnames(X,prefix="PREFIX.")
> X
     test test test test
[1,]   NA   NA   NA   NA
[2,]   NA   NA   NA   NA
[3,]   NA   NA   NA   NA

?? I also tried with do.NULL set to FALSE but it still does not work.

Thank you very much for your help!

Best,
Jens



From kevin.thorpe at utoronto.ca  Wed Jul 20 15:57:17 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Wed, 20 Jul 2005 09:57:17 -0400
Subject: [R] Cannot update some packages after upgrade to 2.1.1
In-Reply-To: <Pine.LNX.4.61.0507160802370.538@gannet.stats>
References: <42D7FB1D.2040302@utoronto.ca>
	<Pine.LNX.4.61.0507160802370.538@gannet.stats>
Message-ID: <42DE583D.3040802@utoronto.ca>

Thank you for the information. I have contacted the RPM maintainer and 
am awaiting a response.

It occurs to me that my "problem" could also be fixed by putting ATLAS 
on my system. Are there advantages to doing that or any reasons not to?

Prof Brian Ripley wrote:

> -lf77blas is part of ATLAS, so I do suspect the RPM builder had ATLAS 
> installed.
>
> lme4 needs a compatible Matrix installed.
>
> I do think installing from the sources would solve this, but probably 
> you need to discuss this with the RPM maintainer as a dependency 
> appears to be missing.
>
> On Fri, 15 Jul 2005, Kevin E. Thorpe wrote:
>
>> I just upgraded to version 2.1.1 (from 2.0.1) today.
>>
>> > R.version
>> _
>> platform i686-pc-linux-gnu
>> arch i686
>> os linux-gnu
>> system i686, linux-gnu
>> status
>> major 2
>> minor 1.1
>> year 2005
>> month 06
>> day 20
>> language R
>>
>> I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
>> available on CRAN. After upgrading r-base, I ran update.packages().
>> Four previously installed packages failed to update:
>>
>> Matrix (0.95-5 to 0.97-4)
>> gam (0.93 to 0.94)
>> lme4 (0.95-3 to 0.96-1)
>> mgcv (1.3-1 to 1.3-4)
>>
>> In the case of Matrix, gam and mgcv I get the message:
>>
>> [long path]/ld: cannot find -lf77blas
>>
>> In the case of lme4 the messages are:
>>
>> ** preparing package for lazy loading
>> Error in setMethod("coef", signature(object = "lmList"), 
>> function(object, :
>> no existing definition for function 'coef'
>> Error: unable to load R code in package 'lme4'
>> Execution halted
>> ERROR: lazy loading failed for package 'lme4'
>>
>> I have searched the SuSE repository for any package that provides
>> f77blas but came up empty.
>>
>> I also could not identify any relevant messages in the mailing list
>> archives.
>>
>> Is R looking for that library because it was present on the machine
>> the RPM was built on? Would building R myself solve the missing library
>> problem or did I do something wrong?
>>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From murdoch at stats.uwo.ca  Wed Jul 20 16:03:20 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Jul 2005 10:03:20 -0400
Subject: [R] Prefix for colnames
In-Reply-To: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>
References: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>
Message-ID: <42DE59A8.5030207@stats.uwo.ca>

On 7/20/2005 9:54 AM, jhainm at fas.harvard.edu wrote:
> Hi,
> 
> I would like to add a prefix to colnames in a matrix but I can't get the prefix
> option in colnames to work. What am I doing wrong?

The prefix argument is only used when there are no names and colnames is 
generating some.  You just want to use paste().

> 
>> X<-matrix(NA,3,4)
>> colnames(X)<-c("test","test","test","test")

 > colnames(X) <- paste("PREFIX.", colnames(X), sep="")
 > X
      PREFIX.test PREFIX.test PREFIX.test PREFIX.test
[1,]          NA          NA          NA          NA
[2,]          NA          NA          NA          NA
[3,]          NA          NA          NA          NA

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Jul 20 16:04:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 15:04:35 +0100 (BST)
Subject: [R] Cannot update some packages after upgrade to 2.1.1
In-Reply-To: <42DE583D.3040802@utoronto.ca>
References: <42D7FB1D.2040302@utoronto.ca>
	<Pine.LNX.4.61.0507160802370.538@gannet.stats>
	<42DE583D.3040802@utoronto.ca>
Message-ID: <Pine.LNX.4.61.0507201500490.27355@gannet.stats>

On Wed, 20 Jul 2005, Kevin E. Thorpe wrote:

> Thank you for the information. I have contacted the RPM maintainer and am 
> awaiting a response.
>
> It occurs to me that my "problem" could also be fixed by putting ATLAS on my 
> system. Are there advantages to doing that or any reasons not to?

It is a good idea unless you share R systems between different 
architectures (even down to the vintage of P4 or Xeon chips). We do and so 
tend to avoid ATLAS (which gets statically compiled in by default).

>
> Prof Brian Ripley wrote:
>
>> -lf77blas is part of ATLAS, so I do suspect the RPM builder had ATLAS 
>> installed.
>> 
>> lme4 needs a compatible Matrix installed.
>> 
>> I do think installing from the sources would solve this, but probably you 
>> need to discuss this with the RPM maintainer as a dependency appears to be 
>> missing.
>> 
>> On Fri, 15 Jul 2005, Kevin E. Thorpe wrote:
>> 
>>> I just upgraded to version 2.1.1 (from 2.0.1) today.
>>> 
>>> > R.version
>>> _
>>> platform i686-pc-linux-gnu
>>> arch i686
>>> os linux-gnu
>>> system i686, linux-gnu
>>> status
>>> major 2
>>> minor 1.1
>>> year 2005
>>> month 06
>>> day 20
>>> language R
>>> 
>>> I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
>>> available on CRAN. After upgrading r-base, I ran update.packages().
>>> Four previously installed packages failed to update:
>>> 
>>> Matrix (0.95-5 to 0.97-4)
>>> gam (0.93 to 0.94)
>>> lme4 (0.95-3 to 0.96-1)
>>> mgcv (1.3-1 to 1.3-4)
>>> 
>>> In the case of Matrix, gam and mgcv I get the message:
>>> 
>>> [long path]/ld: cannot find -lf77blas
>>> 
>>> In the case of lme4 the messages are:
>>> 
>>> ** preparing package for lazy loading
>>> Error in setMethod("coef", signature(object = "lmList"), function(object, 
>>> :
>>> no existing definition for function 'coef'
>>> Error: unable to load R code in package 'lme4'
>>> Execution halted
>>> ERROR: lazy loading failed for package 'lme4'
>>> 
>>> I have searched the SuSE repository for any package that provides
>>> f77blas but came up empty.
>>> 
>>> I also could not identify any relevant messages in the mailing list
>>> archives.
>>> 
>>> Is R looking for that library because it was present on the machine
>>> the RPM was built on? Would building R myself solve the missing library
>>> problem or did I do something wrong?
>>> 
>
>
> -- 
> Kevin E. Thorpe
> Biostatistician/Trialist, Knowledge Translation Program
> Assistant Professor, Department of Public Health Sciences
> Faculty of Medicine, University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From davidr at rhotrading.com  Wed Jul 20 16:05:52 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 20 Jul 2005 09:05:52 -0500
Subject: [R] CPU Usage with R 2.1.0 in Windows
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FBC94@rhosvr02.rhotrading.com>

If you set the affinity of the R process to processor 0, you can run another (R or other) process with affinity set to processor 1 and get 100% usage.
Most applications can't take advantage of hyperthreading (or multiprocessors), since they have to be specially written to do so.

It seems that parts of Windows are single threaded, though, so, for example, starting up another instance of Excel when one is already using all of a 'processor' can take a very long time, but if the processes are already started, you can take advantage of the gross parallelism.

David L. Reiner
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
> Sent: Wednesday, July 20, 2005 3:12 AM
> To: Lukasz Komsta
> Cc: Greene, Michael; 'R-help at lists.R-project.org'
> Subject: Re: [R] CPU Usage with R 2.1.0 in Windows
> 
> It probably is not a problem to leave hyperthreading on: we found little
> performance difference on a P4 either way.
> 
> The Windows task manager is misleading, as `50%' is about as much as a
> P4-class processor with hyperthreading can actually deliver.
> 
> On Tue, 19 Jul 2005, Lukasz Komsta wrote:
> 
> > Dnia 2005-07-19 20:28, U????ytkownik Greene, Michael napisa????:
> >> Hi,> > I'm using a fairly simple HP Compaq desktop PC running Windows
> 2K.  When> running a large process in R, the process "RGUI.exe" will never
> exceed 50%> of the CPU usage.
> > If you have hyperthreading, R catches only one virtual processor
> (fromtwo available), being not able to exceed half of total power (100%
> ofone only). If you want to use full power, you should turn
> hyperthreadingoff, if your BIOS supports such option.
> > Regards,
> > -- Lukasz KomstaDepartment of Medicinal ChemistryMedical University of
> Lublin6 Chodzki, 20-093 Lublin, PolandFax +48 81 7425165
> > ______________________________________________R-help at stat.math.ethz.ch
> mailing listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jul 20 16:10:40 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 16:10:40 +0200
Subject: [R] Prefix for colnames
In-Reply-To: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>
References: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>
Message-ID: <x2k6jl1sbj.fsf@turmalin.kubism.ku.dk>

jhainm at fas.harvard.edu writes:

> Hi,
> 
> I would like to add a prefix to colnames in a matrix but I can't get the prefix
> option in colnames to work. What am I doing wrong?
> 
> > X<-matrix(NA,3,4)
> > colnames(X)<-c("test","test","test","test")
> > colnames(X)<-colnames(X,prefix="PREFIX.")
> > X
>      test test test test
> [1,]   NA   NA   NA   NA
> [2,]   NA   NA   NA   NA
> [3,]   NA   NA   NA   NA
> 
> ?? I also tried with do.NULL set to FALSE but it still does not work.
> 
> Thank you very much for your help!

Well

> colnames
function (x, do.NULL = TRUE, prefix = "col")
{
    dn <- dimnames(x)
    if (!is.null(dn[[2]]))
        dn[[2]]
    else {
        if (do.NULL)
            NULL
        else paste(prefix, seq(length = NCOL(x)), sep = "")
    }
}

So if x has colnames that's what you get, irrespective of "prefix".

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jul 20 16:12:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 15:12:35 +0100 (BST)
Subject: [R] Prefix for colnames
In-Reply-To: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>
References: <1121867644.42de577cdd8c4@webmail.fas.harvard.edu>
Message-ID: <Pine.LNX.4.61.0507201507410.27355@gannet.stats>

On Wed, 20 Jul 2005 jhainm at fas.harvard.edu wrote:

>
> Hi,
>
> I would like to add a prefix to colnames in a matrix but I can't get the prefix
> option in colnames to work. What am I doing wrong?
>
>> X<-matrix(NA,3,4)
>> colnames(X)<-c("test","test","test","test")
>> colnames(X)<-colnames(X,prefix="PREFIX.")
>> X
>     test test test test
> [1,]   NA   NA   NA   NA
> [2,]   NA   NA   NA   NA
> [3,]   NA   NA   NA   NA
>
> ?? I also tried with do.NULL set to FALSE but it still does not work.

The help page is your friend here:

  do.NULL: logical.  Should this create names if they are 'NULL'?

   prefix: for created names.

      If 'do.NULL' is 'FALSE', a character vector (of length 'NROW(x)'
      or 'NCOL(x)') is returned in any case, prepending 'prefix' to
      simple numbers, if there are no dimnames or the corresponding
                      ^^^^^^^^^^^^^^^^^^^^^^^^
      component of the dimnames is 'NULL'.

so

> X < -matrix(NA,3,4)
> colnames(X,prefix="PREFIX.", do.NULL=FALSE)
[1] "PREFIX.1" "PREFIX.2" "PREFIX.3" "PREFIX.4"

works as documented.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uofiowa at gmail.com  Wed Jul 20 16:36:52 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 20 Jul 2005 10:36:52 -0400
Subject: [R] class in apply
In-Reply-To: <42DDF311.9060504@biomserv.univ-lyon1.fr>
References: <3f87cc6d05071922162867e91d@mail.gmail.com>
	<42DDF311.9060504@biomserv.univ-lyon1.fr>
Message-ID: <3f87cc6d05072007363b9c1936@mail.gmail.com>

Thank you for teh helpful answer. I do have one aother related
question; If I am not interested in teh result of the lapply, as I am
using it instaid of a for loop, example

lapply(df, function(r) sqlQuery(conn, paste("insert into t
values(",r['a'],',',r['b'],')')))

and I am not interested in teh result of the insert sql statement. Is
it, for memory and other concerns better to do it this way or to
implement a for loop?

for (i in seq(length= nrow(df)) {
   sqlQuery(conn, paste("insert into t values(",df[i,'a'],',',df[i,'b'],')')))
}

On 7/20/05, St??phane Dray <dray at biomserv.univ-lyon1.fr> wrote:
> apply transorm your data.frame into a matrix which can contains only one
> type of values (character or numeric or logical).
> data.frame are list, so they can contain various kind of data, and a
> solution to your problem is given by lapply:
> 
>  > b=lapply(df, function(r) print(class(r['a'])))
> [1] "numeric"
> [1] "factor"
> 
> >Numeric data that is part of a mixed type data frame is converted into
> >character. How can I tell apply to maintain the original class of a
> >column and not convert it into character. I would like to do this of
> >the vector and not inside the apply function individually over each
> >element. Consider the following two scenarios, in the second column
> >'a' maintained its class while it lost its numeric type in the first
> >case.
> >
> >
> >
> >>df = data.frame(a=c(1,2), b=c('A','B'))
> >>df
> >>
> >>
> >  a b
> >1 1 A
> >2 2 B
> >
> >
> >>a=apply(df, 1, function(r) print(class(r['a'])))
> >>
> >>
> >[1] "character"
> >[1] "character"
> >
> >
> >>a=apply(df, 1, function(r) print(class(r['b'])))
> >>
> >>
> >[1] "character"
> >[1] "character"
> >
> >
> >
> >
> >>df = data.frame(a=c(1,2))
> >>df
> >>
> >>
> >  a
> >1 1
> >2 2
> >
> >
> >>a=apply(df, 1, function(r) print(class(r['a'])))
> >>
> >>
> >[1] "numeric"
> >[1] "numeric"
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >
> >
> 
> 
> --
> St??phane DRAY (dray at biomserv.univ-lyon1.fr )
> Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
> 43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
> Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
> http://www.steph280.freesurf.fr/
> 
>



From lobrien at icoria.com  Wed Jul 20 16:41:41 2005
From: lobrien at icoria.com (O'Brien, Laura)
Date: Wed, 20 Jul 2005 10:41:41 -0400
Subject: [R] unable to call R t-test from Java
Message-ID: <9F19C13E789D6E45A144CF22EDA28E3E062969@rtp-exchng02.paradigm.paragen.com>

Hello,

My colleague and I would like to write Java code that invokes R to do a simple TTest.  I've included my sample java code below.  I tried various alternatives and am unable to pass a vector to the TTest method.  In my investigation, I tried to call other R methods that take vectors and also ran into various degrees of failure.   Any insight you can provide or other Web references you can point me to would be appreciated.

Thank you,
Laura O'Brien
Application Architect



---------------------------  code   ------------------------------


package org.omegahat.R.Java.Examples;

import org.omegahat.R.Java.ROmegahatInterpreter;
import org.omegahat.R.Java.REvaluator;


public class JavaRCall2
{

    /**
     * want to see if I can eval a t.test command like what I would run in the
     * R command line
     */

    static public void runTTestByEval_cores(REvaluator e, ROmegahatInterpreter interp)
    {
        /* produces a core */
        System.err.println("eval a t.test");
        Object value = e.eval("t.test (c(1,2,3), c(4,5,6))");
        if (value != null)
            interp.show(value);
    }

    
    /**
     * want to see if I can eval anything that takes a vector, e.g. mean, 
     * like what I would run in the R command line
     */

    static public void runMeanByEval_works(REvaluator e, ROmegahatInterpreter interp)
    {
        System.err.println("\r\n  evaluation string mean command");
        Object value = e.eval("mean(c(1,2,3))");
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }    
    }

/**
 *  if I pass mean a org.omegahat.Environment.DataStructures.numeric what do I get?  NaN
 */
 
    static public void runMeanByNumericList_nan(REvaluator e, ROmegahatInterpreter interp)
    {    
         Object[] funArgs = new Object[1];
         // given argument is not numeric or logical
         
         org.omegahat.Environment.DataStructures.numeric rList1 = new org.omegahat.Environment.DataStructures.numeric(3);       
    
         double[] dList = new double[3];
         dList[0] = (double) 1.1;
         dList[1] = (double) 2.2; 
         dList[2] = (double) 3.3;
         rList1.setData(dList, true);
         System.err.println(rList1.toString());

         funArgs[0] = rList1 ;
         
         System.err.println("\r\n Calling mean and passing an omegahat vector");

    
         Object value =  e.call("mean", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }

    }
    
    /**
     * let's run some tests on the vector passed in and see what R thinks I'm handing it
     * 
     * it returns 
     * isnumeric:     false
     * mode:          list
     * length:        2
     */

    public static void runTestsOnOmegahatNumeric(REvaluator e, ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[1];
        // given argument is not numeric or logical
         
        org.omegahat.Environment.DataStructures.numeric rList1 = new org.omegahat.Environment.DataStructures.numeric(3);       
    
        double[] dList = new double[3];
        dList[0] = (double) 1.1;
        dList[1] = (double) 2.2; 
        dList[2] = (double) 3.3;
        rList1.setData(dList, true);
        System.err.println(rList1.toString());

        funArgs[0] = rList1 ;
         
        System.err.println("\r\n Calling isnumeric and passing an omegahat vector");
   
        Object value =  e.call("is.numeric", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

        // mode is list

        System.err.println("\r\n Calling mode and passing an omegahat vector");
   
        value =  e.call("mode", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

        System.err.println("\r\n Calling length and passing an omegahat vector");
        System.err.println("\r\n");
        System.err.println("INTERESTING:  thinks the length is 2!");

   
        value =  e.call("length", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

    }

    /**
     * run the mean on a java array.  Does it also return NAN?  
     * It returns a different value than the mean -- 2.19999... instead of 2.2
     */

    static public void runMeanOnJavaArray(REvaluator e, ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[1];
        double[] d = { 1.1, 2.2, 3.3};
        funArgs[0] = d;
        System.err.println("\r\n Calling mean of (1.1, 2.2, 3.3) and passing a java array\r\n");
        System.err.println("INTERSTING:  thinks the mean is 2.19999... it should be 2.2 \r\n");
   
        Object value =  e.call("mean", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }
    }

    /**
     *    This is what I really want! 
     */

    static public void runTTestOnJavaArray_cores(REvaluator e, ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[2];
        double[] d0 = { 1.1, 2.2, 3.3};
        double[] d1 = { 9.9, 8.8, 7.7};

        funArgs[0] = d0;
        funArgs[1] = d1;
        
         System.err.println("\r\n Calling t.test and passing a java array");
   
         Object value =  e.call("t.test", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }
        
    }

    /**
     * 
     */
    static public void main(String[] args) 
    {
        ROmegahatInterpreter interp = new ROmegahatInterpreter(ROmegahatInterpreter.fixArgs(args), false);
        REvaluator e = new REvaluator();

        Object[] funArgs;
        String[] objects;
        Object value;
        int i;

        /** GOAL:  pass a vector of numbers into a t-test
        */
         
            // unfortunately it core dumps
            runTTestOnJavaArray_cores(e, interp);   


        /*         since I've been unable to get that work, 
        *         I try various evaluation commands to see if I 
        *         can get any sort of R commands that take vectors to work
        *         in any sort of fashion
        */
          

        //
        //   SAMPLE eval based calls
        //
          
        //  can I successfully invoke a t.test command similar to what I can
        //  do via the command line, e.g. t.test (c(1,2,3), c(4,5,6))
        //  NO -- this core dumps
        // runTTestByEval_cores(e, interp);
            
        // can I eval anything that takes a vector -- let's try the mean method
        // yes this works!
            //runMeanByEval_works(e, interp);
            
          
        //
        //    SAMPLE org.omegahat.Environment.DataStructures.numeric 
        //

        // this returns Nan
          
        // runMeanByNumericList_nan(e, interp);

        // given that the above returns NaN, what does R think I'm passing it for a vector?
        // let's run some tests
        runTestsOnOmegahatNumeric(e, interp);
          
        // now let's try some tests on a java array instead of an omegahat numeric
        runMeanOnJavaArray(e, interp);
    }
}



From uofiowa at gmail.com  Wed Jul 20 16:41:36 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 20 Jul 2005 10:41:36 -0400
Subject: [R] is.Date ?!!!
Message-ID: <3f87cc6d050720074126108abc@mail.gmail.com>

Is there a way to test if a variable is a date?



From greg.snow at ihc.com  Wed Jul 20 16:43:05 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Wed, 20 Jul 2005 08:43:05 -0600
Subject: [R] Is it possible to create highly customized report in *.xls
 format by using R/S+?
Message-ID: <s2de0ea8.035@lp-msg1.co.ihc.com>

See:

http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
and
http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
I remember in one slide of Prof. Ripley's presentation overhead, he
said the most popular data analysis software is excel.

So is there any resource or tutorial on this topic? 

Thank you so much!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jul 20 16:43:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Jul 2005 16:43:48 +0200
Subject: [R] Cannot update some packages after upgrade to 2.1.1
In-Reply-To: <Pine.LNX.4.61.0507201500490.27355@gannet.stats>
References: <42D7FB1D.2040302@utoronto.ca>	<Pine.LNX.4.61.0507160802370.538@gannet.stats>	<42DE583D.3040802@utoronto.ca>
	<Pine.LNX.4.61.0507201500490.27355@gannet.stats>
Message-ID: <42DE6324.4080207@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:

> On Wed, 20 Jul 2005, Kevin E. Thorpe wrote:
> 
> 
>>Thank you for the information. I have contacted the RPM maintainer and am 
>>awaiting a response.
>>
>>It occurs to me that my "problem" could also be fixed by putting ATLAS on my 
>>system. Are there advantages to doing that or any reasons not to?
> 
> 
> It is a good idea unless you share R systems between different 
> architectures (even down to the vintage of P4 or Xeon chips). We do and so 
> tend to avoid ATLAS (which gets statically compiled in by default).


We are using you ATLAS-optimized Rblas.dll files for our facutly wide 
Windows installation.

Additional to the regular .../bin directory, we have copies called 
.../bin-At, .../bin-P3, .../bin-P4 but the regular Rblas.dll replaced by 
those optimized Rblas.dll files.
Hence one can simply call .../bin-P4/RGui.exe on any P4/Xeon machine and 
do not need a seperate installation for another Athlon machine using the 
.../bin-At/RGui.exe, for example.


Uwe


> 
>>Prof Brian Ripley wrote:
>>
>>
>>>-lf77blas is part of ATLAS, so I do suspect the RPM builder had ATLAS 
>>>installed.
>>>
>>>lme4 needs a compatible Matrix installed.
>>>
>>>I do think installing from the sources would solve this, but probably you 
>>>need to discuss this with the RPM maintainer as a dependency appears to be 
>>>missing.
>>>
>>>On Fri, 15 Jul 2005, Kevin E. Thorpe wrote:
>>>
>>>
>>>>I just upgraded to version 2.1.1 (from 2.0.1) today.
>>>>
>>>>
>>>>>R.version
>>>>
>>>>_
>>>>platform i686-pc-linux-gnu
>>>>arch i686
>>>>os linux-gnu
>>>>system i686, linux-gnu
>>>>status
>>>>major 2
>>>>minor 1.1
>>>>year 2005
>>>>month 06
>>>>day 20
>>>>language R
>>>>
>>>>I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
>>>>available on CRAN. After upgrading r-base, I ran update.packages().
>>>>Four previously installed packages failed to update:
>>>>
>>>>Matrix (0.95-5 to 0.97-4)
>>>>gam (0.93 to 0.94)
>>>>lme4 (0.95-3 to 0.96-1)
>>>>mgcv (1.3-1 to 1.3-4)
>>>>
>>>>In the case of Matrix, gam and mgcv I get the message:
>>>>
>>>>[long path]/ld: cannot find -lf77blas
>>>>
>>>>In the case of lme4 the messages are:
>>>>
>>>>** preparing package for lazy loading
>>>>Error in setMethod("coef", signature(object = "lmList"), function(object, 
>>>>:
>>>>no existing definition for function 'coef'
>>>>Error: unable to load R code in package 'lme4'
>>>>Execution halted
>>>>ERROR: lazy loading failed for package 'lme4'
>>>>
>>>>I have searched the SuSE repository for any package that provides
>>>>f77blas but came up empty.
>>>>
>>>>I also could not identify any relevant messages in the mailing list
>>>>archives.
>>>>
>>>>Is R looking for that library because it was present on the machine
>>>>the RPM was built on? Would building R myself solve the missing library
>>>>problem or did I do something wrong?
>>>>
>>
>>
>>-- 
>>Kevin E. Thorpe
>>Biostatistician/Trialist, Knowledge Translation Program
>>Assistant Professor, Department of Public Health Sciences
>>Faculty of Medicine, University of Toronto
>>email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462
>>
>>
> 
>



From liuwensui at gmail.com  Wed Jul 20 16:55:39 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 20 Jul 2005 10:55:39 -0400
Subject: [R] Is it possible to create highly customized report in *.xls
	format by using R/S+?
In-Reply-To: <s2de0ea8.033@lp-msg1.co.ihc.com>
References: <s2de0ea8.033@lp-msg1.co.ihc.com>
Message-ID: <1115a2b005072007554e1ba1f1@mail.gmail.com>

I appreciate your reply and understand your point completely. But at
times we can't change the rule, the only choice is to follow the rule.
Most deliverables in my work are in excel format.

On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> See:
> 
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> and
> http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> 
> Greg Snow, Ph.D.
> Statistical Data Center, LDS Hospital
> Intermountain Health Care
> greg.snow at ihc.com
> (801) 408-8111
> 
> >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> I remember in one slide of Prof. Ripley's presentation overhead, he
> said the most popular data analysis software is excel.
> 
> So is there any resource or tutorial on this topic?
> 
> Thank you so much!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From ripley at stats.ox.ac.uk  Wed Jul 20 16:56:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 15:56:27 +0100 (BST)
Subject: [R] Cannot update some packages after upgrade to 2.1.1
In-Reply-To: <42DE6324.4080207@statistik.uni-dortmund.de>
References: <42D7FB1D.2040302@utoronto.ca>
	<Pine.LNX.4.61.0507160802370.538@gannet.stats>
	<42DE583D.3040802@utoronto.ca>
	<Pine.LNX.4.61.0507201500490.27355@gannet.stats>
	<42DE6324.4080207@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0507201550130.27862@gannet.stats>

On Wed, 20 Jul 2005, Uwe Ligges wrote:

> Prof Brian Ripley wrote:
>
>> On Wed, 20 Jul 2005, Kevin E. Thorpe wrote:
>> 
>> 
>>> Thank you for the information. I have contacted the RPM maintainer and am 
>>> awaiting a response.
>>> 
>>> It occurs to me that my "problem" could also be fixed by putting ATLAS on 
>>> my system. Are there advantages to doing that or any reasons not to?
>> 
>> 
>> It is a good idea unless you share R systems between different 
>> architectures (even down to the vintage of P4 or Xeon chips). We do and so 
>> tend to avoid ATLAS (which gets statically compiled in by default).
>
>
> We are using you ATLAS-optimized Rblas.dll files for our facutly wide Windows 
> installation.
>
> Additional to the regular .../bin directory, we have copies called 
> .../bin-At, .../bin-P3, .../bin-P4 but the regular Rblas.dll replaced by 
> those optimized Rblas.dll files.
> Hence one can simply call .../bin-P4/RGui.exe on any P4/Xeon machine and do 
> not need a seperate installation for another Athlon machine using the 
> .../bin-At/RGui.exe, for example.

That is fine on Windows, which has a separate Rblas.dll exactly to enable 
this to be done.

You can also do it on Debian Linux, which has a dynamic libatlas (as I 
understand it).

On other systems with a static ATLAS library, the BLAS is compiled into 
the R installation in many (up to hundreds) of places, including each 
package that uses a BLAS.   It's all too easy to build R on, say, an 
Athlon and discover it segfaults on a Xeon because an Athlon-optimized 
ATLAS has been compiled in. (I've done it more than once, and it needs the 
whole setup to be reinstalled, including many of the packages without it 
being obvious which ones).

>
>
> Uwe
>
>
>> 
>>> Prof Brian Ripley wrote:
>>> 
>>> 
>>>> -lf77blas is part of ATLAS, so I do suspect the RPM builder had ATLAS 
>>>> installed.
>>>> 
>>>> lme4 needs a compatible Matrix installed.
>>>> 
>>>> I do think installing from the sources would solve this, but probably you 
>>>> need to discuss this with the RPM maintainer as a dependency appears to 
>>>> be missing.
>>>> 
>>>> On Fri, 15 Jul 2005, Kevin E. Thorpe wrote:
>>>> 
>>>> 
>>>>> I just upgraded to version 2.1.1 (from 2.0.1) today.
>>>>> 
>>>>> 
>>>>>> R.version
>>>>> 
>>>>> _
>>>>> platform i686-pc-linux-gnu
>>>>> arch i686
>>>>> os linux-gnu
>>>>> system i686, linux-gnu
>>>>> status
>>>>> major 2
>>>>> minor 1.1
>>>>> year 2005
>>>>> month 06
>>>>> day 20
>>>>> language R
>>>>> 
>>>>> I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
>>>>> available on CRAN. After upgrading r-base, I ran update.packages().
>>>>> Four previously installed packages failed to update:
>>>>> 
>>>>> Matrix (0.95-5 to 0.97-4)
>>>>> gam (0.93 to 0.94)
>>>>> lme4 (0.95-3 to 0.96-1)
>>>>> mgcv (1.3-1 to 1.3-4)
>>>>> 
>>>>> In the case of Matrix, gam and mgcv I get the message:
>>>>> 
>>>>> [long path]/ld: cannot find -lf77blas
>>>>> 
>>>>> In the case of lme4 the messages are:
>>>>> 
>>>>> ** preparing package for lazy loading
>>>>> Error in setMethod("coef", signature(object = "lmList"), 
>>>>> function(object, :
>>>>> no existing definition for function 'coef'
>>>>> Error: unable to load R code in package 'lme4'
>>>>> Execution halted
>>>>> ERROR: lazy loading failed for package 'lme4'
>>>>> 
>>>>> I have searched the SuSE repository for any package that provides
>>>>> f77blas but came up empty.
>>>>> 
>>>>> I also could not identify any relevant messages in the mailing list
>>>>> archives.
>>>>> 
>>>>> Is R looking for that library because it was present on the machine
>>>>> the RPM was built on? Would building R myself solve the missing library
>>>>> problem or did I do something wrong?
>>>>> 
>>> 
>>> 
>>> -- 
>>> Kevin E. Thorpe
>>> Biostatistician/Trialist, Knowledge Translation Program
>>> Assistant Professor, Department of Public Health Sciences
>>> Faculty of Medicine, University of Toronto
>>> email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462
>>> 
>>> 
>> 
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Wed Jul 20 16:57:52 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Jul 2005 10:57:52 -0400
Subject: [R] is.Date ?!!!
In-Reply-To: <3f87cc6d050720074126108abc@mail.gmail.com>
References: <3f87cc6d050720074126108abc@mail.gmail.com>
Message-ID: <42DE6670.2080508@stats.uwo.ca>

On 7/20/2005 10:41 AM, Omar Lakkis wrote:
> Is there a way to test if a variable is a date?

Yes, but there are several different things people might call dates. 
Which do you mean?

For example,

 > today <- Sys.Date()
 > today
[1] "2005-07-20"
 > inherits(today, "Date")
[1] TRUE

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Jul 20 16:59:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 15:59:18 +0100 (BST)
Subject: [R] is.Date ?!!!
In-Reply-To: <3f87cc6d050720074126108abc@mail.gmail.com>
References: <3f87cc6d050720074126108abc@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0507201558330.27862@gannet.stats>

If you mean of class "Date",  use inherits(x, "Date").

On Wed, 20 Jul 2005, Omar Lakkis wrote:

> Is there a way to test if a variable is a date?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From buser at stat.math.ethz.ch  Wed Jul 20 17:06:41 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 20 Jul 2005 17:06:41 +0200
Subject: [R] Code Verification
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAD6@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAD6@usctmx1106.Merck.com>
Message-ID: <17118.26753.876811.332462@stat.math.ethz.ch>

Dear Andy

Since the power of the t-test decreases when there are
discrepancies in the data to the normal distribution and there
is only a small loss of power if the data is normal distributed,
the only reason to use the t.test is his simplicity and the
easier interpretation.
Generally I'd prefer the wilcoxon test even if the data is
normal distributed.
But I agree that for a gamma distribution there is no huge loss
of power.

## example simulation:

n <- 1000
z1 <- numeric(n)
z2 <- numeric(n)

## gamma distribution
for (i in 1:n)
{
x<-rgamma(40, 2.5, 0.1)  
y<-rgamma(40, 3.5, 0.1)  
z1[i]<-t.test(x, y)$p.value
z2[i]<-wilcox.test(x, y)$p.value
}
## Power
1 - sum(z1>0.05)/1000  ## 0.71
1 - sum(z2>0.05)/1000  ## 0.76

##################################

## t distribution
for (i in 1:n)
{
x<-rt(40, df = 3)      
y<-1 + rt(40, df = 3)  
z1[i]<-t.test(x, y)$p.value
z2[i]<-wilcox.test(x, y)$p.value
}
## Power
1 - sum(z1>0.05)/1000  ## 0.76
1 - sum(z2>0.05)/1000  ## 0.91

##################################

## normal distribution
for (i in 1:n)
{
x<-rnorm(40, 0, 3)      
y<-1 + rnorm(40, 1, 3)  
z1[i]<-t.test(x, y)$p.value
z2[i]<-wilcox.test(x, y)$p.value
}
## Power
1 - sum(z1>0.05)/1000  ## 0.83
1 - sum(z2>0.05)/1000  ## 0.81

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Liaw, Andy writes:
 > > From: Christoph Buser
 > > 
 > > Hi
 > > 
 > > "t.test" assumes that your data within each group has a normal
 > > distribution. This is not the case in your example.
 > 
 > Eh?  What happen to the CLT?
 > 
 > Andy
 > 
 > 
 > > I would recommend you a non parametric test like "wilcox.test" if
 > > you want to compare the mean of two samples that are not normal
 > > distributed. 
 > > see ?wilcox.test
 > > 
 > > Be careful. Your example produces two gamma distributed samples
 > > with rate = 10, not scale = 10. 
 > > rate = 1/scale.
 > > If you want to use scale, you need to specify this argument 
 > > x<-rgamma(40, 2.5, scale = 10)
 > > see ?rgamma
 > > 
 > > I do not see the interpretation of your result. Since you do
 > > know the distribution and the parameters of your sample, you
 > > know the true means and that they are different. It is only a
 > > question of the sample size and the power of your test, if this
 > > difference is detected.
 > > Is that something you are investigating? Maybe a power
 > > calculation or something similar.
 > > 
 > > Regards,
 > > 
 > > Christoph Buser
 > > 
 > > --------------------------------------------------------------
 > > Christoph Buser <buser at stat.math.ethz.ch>
 > > Seminar fuer Statistik, LEO C13
 > > ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
 > > phone: x-41-44-632-4673		fax: 632-1228
 > > http://stat.ethz.ch/~buser/
 > > --------------------------------------------------------------
 > > 
 > > pantd at unlv.nevada.edu writes:
 > >  > Hi R Users
 > >  > I have a code which I am running for my thesis work. Just 
 > > want to make sure that
 > >  > its ok. Its a t test I am conducting between two gamma 
 > > distributions with
 > >  > different shape parameters.
 > >  > 
 > >  > the code looks like:
 > >  > 
 > >  > sink("a1.txt");
 > >  > 
 > >  > for (i in 1:1000)
 > >  > {
 > >  > x<-rgamma(40, 2.5, 10)  # n = 40, shape = 2.5, Scale = 10
 > >  > y<-rgamma(40, 2.8, 10)  # n = 40, shape = 2.8, Scale = 10
 > >  > z<-t.test(x, y)
 > >  > print(z)
 > >  > }
 > >  > 
 > >  > 
 > >  > I will appreciate it if someone could tell me if its alrite or not.
 > >  > 
 > >  > thanks
 > >  > 
 > >  > -dev
 > >  > 
 > >  > ______________________________________________
 > >  > R-help at stat.math.ethz.ch mailing list
 > >  > https://stat.ethz.ch/mailman/listinfo/r-help
 > >  > PLEASE do read the posting guide! 
 > > http://www.R-project.org/posting-guide.html
 > > 
 > > 
 > > ______________________________________________
 > > R-help at stat.math.ethz.ch mailing list
 > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > PLEASE do read the posting guide! 
 > > http://www.R-project.org/posting-guide.html
 > > 
 > > 
 > > 
 > 
 > 
 > 
 > ------------------------------------------------------------------------------
 > Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
 > ------------------------------------------------------------------------------



From ligges at statistik.uni-dortmund.de  Wed Jul 20 17:07:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Jul 2005 17:07:15 +0200
Subject: [R] is.Date ?!!!
In-Reply-To: <3f87cc6d050720074126108abc@mail.gmail.com>
References: <3f87cc6d050720074126108abc@mail.gmail.com>
Message-ID: <42DE68A3.9030202@statistik.uni-dortmund.de>

Omar Lakkis wrote:

> Is there a way to test if a variable is a date?


  is(variable, "Date")

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Iyue.Sung at lm.mmc.com  Wed Jul 20 17:19:45 2005
From: Iyue.Sung at lm.mmc.com (Sung, Iyue)
Date: Wed, 20 Jul 2005 11:19:45 -0400
Subject: [R] Is it possible to create highly customized report in
	*.xlsformat by using R/S+?
Message-ID: <B01A505EA0E3124998DF4661D313B206DEA6F7@mmci-bos03fs01.mmci.ad.root>

 Your surest bet is to look into S+, not R, since the former does
'integrate' with Microsoft applications. To what extent, I don' know.
This is a desirable feature (IMO) and a reason to use S+ rather then R
(already discussed in different thread).

Try the S+ newsgroup.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
> Sent: Wednesday, July 20, 2005 10:56 AM
> To: Greg Snow
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Is it possible to create highly customized 
> report in *.xlsformat by using R/S+?
> 
> I appreciate your reply and understand your point completely. 
> But at times we can't change the rule, the only choice is to 
> follow the rule.
> Most deliverables in my work are in excel format.
> 
> On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> > See:
> > 
> > http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> > and
> > http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> > 
> > Greg Snow, Ph.D.
> > Statistical Data Center, LDS Hospital
> > Intermountain Health Care
> > greg.snow at ihc.com
> > (801) 408-8111
> > 
> > >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> > I remember in one slide of Prof. Ripley's presentation overhead, he 
> > said the most popular data analysis software is excel.
> > 
> > So is there any resource or tutorial on this topic?
> > 
> > Thank you so much!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --
> WenSui Liu, MS MA
> Senior Decision Support Analyst
> Division of Health Policy and Clinical Effectiveness 
> Cincinnati Children Hospital Medical Center
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> --------------------------------------------------------------
> --------------
> This e-mail and any attachments may be confidential or\ > ...{{dropped}}



From uofiowa at gmail.com  Wed Jul 20 17:18:07 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 20 Jul 2005 11:18:07 -0400
Subject: [R] is.Date ?!!!
In-Reply-To: <42DE68A3.9030202@statistik.uni-dortmund.de>
References: <3f87cc6d050720074126108abc@mail.gmail.com>
	<42DE68A3.9030202@statistik.uni-dortmund.de>
Message-ID: <3f87cc6d05072008185bfbc4e4@mail.gmail.com>

The type of date I am asking about is the result of an as.Date(). 
inherits(x, "Date")
and 
is(variable, "Date")
works for me. Thank you all.

On 7/20/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Omar Lakkis wrote:
> 
> > Is there a way to test if a variable is a date?
> 
> 
>   is(variable, "Date")
> 
> Uwe Ligges
> 
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From greg.snow at ihc.com  Wed Jul 20 17:20:22 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Wed, 20 Jul 2005 09:20:22 -0600
Subject: [R] maps and data for german federal states
Message-ID: <s2de175b.030@lp-msg1.co.ihc.com>

There are shapefiles for Europe (and other places) at: http://www.vdstech.com/map_data.htm (there are 16 polygons withing the one for Germany, is that enough detail).  You can read and plot the shapefiles using the maptools package.

hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> "Ebersberger, Bernd" <Bernd.Ebersberger at isi.fraunhofer.de> 07/20/05 01:07AM >>>
dear R-tists,

i want to graph information for the German Federal States (Bundeslaender) using the maps package. unfortunately there is no maps for the German Bundeslaender.

does anyone have an idea / a source where to get map data that can be used in the maps package that graphs structures below the country level. 

in the long run it would also be interesting to integrate Swiss Cantons and Austrian Bundesl??nder... 

any suggestions are highly appreciated.

thanks

bernd

++++++++++++++++++++++++++++++++++++++++++++++
bernd ebersberger
fhg isi
b.ebersberger (AT) isi.fhg.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From whit at twinfieldscapital.com  Wed Jul 20 17:27:29 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Wed, 20 Jul 2005 11:27:29 -0400
Subject: [R] Is it possible to create highly customized report in
	*.xlsformat by using R/S+?
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE2C8168@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

I have a package which I use to create excel files from R.

I have not been able to produce a configure script general enough for me
to post it to cran, but I will send it to you if you like.

I use it for production jobs on our linux servers.  You may have to
tinker a bit to get it to compile on windows.

Have a look at the examples below.  If it suits your needs, I will send
it to you.

Cheers,
Whit


Here are the examples from the help page:

     nc <- 4
     nr <- 20
     x <- matrix(rnorm(nc*nr),ncol=nc,nrow=nr)

     rownames(x) <- rep(letters,length=nr)
     colnames(x) <- rep(letters,length=nc)

     # write a matrix
     write.xls(x,"matrixFromR.xls","matrix")
     write.xls(x,"matrixFromR.no.colnms.xls","matrix",writeColNms=FALSE)
     write.xls(x,"matrix.no.rownms.xls","matrix",writeRowNms=FALSE)
 
write.xls(x,"matrix.no.nms.xls","matrix",writeColNms=FALSE,writeRowNms=F
ALSE)

     # add some formats
 
write.xls(x,"matrixFromR_formatted.xls","matrix",formats=rep("0.0",nc))
 
write.xls(x,"matrixFromR_formatted2.xls","matrix",formats=rep(c("0.0","0
"),nc/2))
 
write.xls(x,"matrixFromR_formatted3.xls","matrix",formats=rep("#,##0.0;[
Red]#-##0.0;\"\"",nc))

     write.xls(x[,1],"vectorFromR.xls","vector")
 
write.xls(x[,1],"vectorFromR.no.colnms.xls","vector",writeColNms=FALSE)
 
write.xls(x[,1],"vectorFromR.no.rownms.xls","vector",writeRowNms=FALSE)
 
write.xls(x[,1],"vectorFromR.no.nms.xls","vector",writeColNms=FALSE,writ
eRowNms=FALSE)

     # char data test
     y <- matrix(rep(letters,each=26),ncol=26)
     rownames(y) <- rep(letters,length=26)
     colnames(y) <- rep(letters,length=26)
     write.xls(y,"char.data.xls","alphabet")

     # logical data test
     z <- matrix(as.logical(round(runif(nc*nr),0)),ncol=nc)
     rownames(z) <- rep(letters,length=nr)
     colnames(z) <- rep(letters,length=nc)

     write.xls(z,"logical.data.xls","myLogic")
     write.xls(z[,1],"logical.data.vector.xls","myLogicVector") 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
Sent: Wednesday, July 20, 2005 10:56 AM
To: Greg Snow
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Is it possible to create highly customized report in
*.xlsformat by using R/S+?

I appreciate your reply and understand your point completely. But at
times we can't change the rule, the only choice is to follow the rule.
Most deliverables in my work are in excel format.

On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> See:
> 
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> and
> http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> 
> Greg Snow, Ph.D.
> Statistical Data Center, LDS Hospital
> Intermountain Health Care
> greg.snow at ihc.com
> (801) 408-8111
> 
> >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> I remember in one slide of Prof. Ripley's presentation overhead, he 
> said the most popular data analysis software is excel.
> 
> So is there any resource or tutorial on this topic?
> 
> Thank you so much!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


--
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness Cincinnati Children
Hospital Medical Center

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kehler at mathstat.dal.ca  Wed Jul 20 17:28:50 2005
From: kehler at mathstat.dal.ca (kehler@mathstat.dal.ca)
Date: Wed, 20 Jul 2005 12:28:50 -0300 (ADT)
Subject: [R] predict.lm  - standard error of predicted means?
Message-ID: <47307.198.103.196.130.1121873330.squirrel@198.103.196.130>

Simple question.

For a simple linear regression, I obtained the "standard error of
predicted means", for both a confidence and prediction interval:

x<-1:15
y<-x + rnorm(n=15)
model<-lm(y~x)
predict.lm(model,newdata=data.frame(x=c(10,20)),se.fit=T,interval="confidence")$se.fit
        1         2
0.2708064 0.7254615

predict.lm(model,newdata=data.frame(x=c(10,20)),se.fit=T,interval="prediction")$se.fit
        1         2
0.2708064 0.7254615


I was surprised to find that the standard errors returned were in fact the
standard errors of the sampling distribution of Y_hat:

sqrt(MSE(1/n + (x-x_bar)^2/SS_x)),

not the standard errors of Y_new (predicted value):

sqrt(MSE(1 + 1/n + (x-x_bar)^2/SS_x)).

Is there a reason this quantity is called the "standard error of predicted
means" if it doesn't relate to the prediction distribution?

Turning to Neter et al.'s Applied Linear Statistical Models, I note that
if we have multiple observations, then the standard error of the mean of
the predicted value:

sqrt(MSE(1/m + 1/n + (x-x_bar)^2/SS_x)),

reverts to the standard error of the sampling distribution of Y-hat, as m,
the number of samples, gets large. Still, this doesn't explain the result
for small sample sizes.

Using R.2.1 for Windows



From isen at plantpath.wisc.edu  Wed Jul 20 17:40:51 2005
From: isen at plantpath.wisc.edu (Thomas Isenbarger)
Date: Wed, 20 Jul 2005 10:40:51 -0500
Subject: [R] poisson fit for histogram
Message-ID: <6DE2C8B4-3C9E-47F9-BB3D-87D6E0967898@plantpath.wisc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050720/c2699202/attachment.pl

From matthew_wiener at merck.com  Wed Jul 20 17:41:35 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 20 Jul 2005 11:41:35 -0400
Subject: [R] unable to call R t-test from Java
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049948D4@uswsmx03.merck.com>

This does not address the question of how to do this the way you're trying
to do it, using Omegahat.  But I've had good results using Simon Urbanek's
program Rserve:  http://stats.math.uni-augsburg.de/Rserve/

Hope this helps,

Matthew Wiener
Applied Computer Science & Mathematics
Merck Research Laboratories
Rahway, NJ 07065
732-594-5303 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of O'Brien, Laura
Sent: Wednesday, July 20, 2005 10:42 AM
To: r-help at stat.math.ethz.ch
Subject: [R] unable to call R t-test from Java


Hello,

My colleague and I would like to write Java code that invokes R to do a
simple TTest.  I've included my sample java code below.  I tried various
alternatives and am unable to pass a vector to the TTest method.  In my
investigation, I tried to call other R methods that take vectors and also
ran into various degrees of failure.   Any insight you can provide or other
Web references you can point me to would be appreciated.

Thank you,
Laura O'Brien
Application Architect



---------------------------  code   ------------------------------


package org.omegahat.R.Java.Examples;

import org.omegahat.R.Java.ROmegahatInterpreter;
import org.omegahat.R.Java.REvaluator;


public class JavaRCall2
{

    /**
     * want to see if I can eval a t.test command like what I would run in
the
     * R command line
     */

    static public void runTTestByEval_cores(REvaluator e,
ROmegahatInterpreter interp)
    {
        /* produces a core */
        System.err.println("eval a t.test");
        Object value = e.eval("t.test (c(1,2,3), c(4,5,6))");
        if (value != null)
            interp.show(value);
    }

    
    /**
     * want to see if I can eval anything that takes a vector, e.g. mean, 
     * like what I would run in the R command line
     */

    static public void runMeanByEval_works(REvaluator e,
ROmegahatInterpreter interp)
    {
        System.err.println("\r\n  evaluation string mean command");
        Object value = e.eval("mean(c(1,2,3))");
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }    
    }

/**
 *  if I pass mean a org.omegahat.Environment.DataStructures.numeric what do
I get?  NaN
 */
 
    static public void runMeanByNumericList_nan(REvaluator e,
ROmegahatInterpreter interp)
    {    
         Object[] funArgs = new Object[1];
         // given argument is not numeric or logical
         
         org.omegahat.Environment.DataStructures.numeric rList1 = new
org.omegahat.Environment.DataStructures.numeric(3);       
    
         double[] dList = new double[3];
         dList[0] = (double) 1.1;
         dList[1] = (double) 2.2; 
         dList[2] = (double) 3.3;
         rList1.setData(dList, true);
         System.err.println(rList1.toString());

         funArgs[0] = rList1 ;
         
         System.err.println("\r\n Calling mean and passing an omegahat
vector");

    
         Object value =  e.call("mean", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }

    }
    
    /**
     * let's run some tests on the vector passed in and see what R thinks
I'm handing it
     * 
     * it returns 
     * isnumeric:     false
     * mode:          list
     * length:        2
     */

    public static void runTestsOnOmegahatNumeric(REvaluator e,
ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[1];
        // given argument is not numeric or logical
         
        org.omegahat.Environment.DataStructures.numeric rList1 = new
org.omegahat.Environment.DataStructures.numeric(3);       
    
        double[] dList = new double[3];
        dList[0] = (double) 1.1;
        dList[1] = (double) 2.2; 
        dList[2] = (double) 3.3;
        rList1.setData(dList, true);
        System.err.println(rList1.toString());

        funArgs[0] = rList1 ;
         
        System.err.println("\r\n Calling isnumeric and passing an omegahat
vector");
   
        Object value =  e.call("is.numeric", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

        // mode is list

        System.err.println("\r\n Calling mode and passing an omegahat
vector");
   
        value =  e.call("mode", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

        System.err.println("\r\n Calling length and passing an omegahat
vector");
        System.err.println("\r\n");
        System.err.println("INTERESTING:  thinks the length is 2!");

   
        value =  e.call("length", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

    }

    /**
     * run the mean on a java array.  Does it also return NAN?  
     * It returns a different value than the mean -- 2.19999... instead of
2.2
     */

    static public void runMeanOnJavaArray(REvaluator e, ROmegahatInterpreter
interp)
    {
        Object[] funArgs = new Object[1];
        double[] d = { 1.1, 2.2, 3.3};
        funArgs[0] = d;
        System.err.println("\r\n Calling mean of (1.1, 2.2, 3.3) and passing
a java array\r\n");
        System.err.println("INTERSTING:  thinks the mean is 2.19999... it
should be 2.2 \r\n");
   
        Object value =  e.call("mean", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }
    }

    /**
     *    This is what I really want! 
     */

    static public void runTTestOnJavaArray_cores(REvaluator e,
ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[2];
        double[] d0 = { 1.1, 2.2, 3.3};
        double[] d1 = { 9.9, 8.8, 7.7};

        funArgs[0] = d0;
        funArgs[1] = d1;
        
         System.err.println("\r\n Calling t.test and passing a java array");
   
         Object value =  e.call("t.test", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }
        
    }

    /**
     * 
     */
    static public void main(String[] args) 
    {
        ROmegahatInterpreter interp = new
ROmegahatInterpreter(ROmegahatInterpreter.fixArgs(args), false);
        REvaluator e = new REvaluator();

        Object[] funArgs;
        String[] objects;
        Object value;
        int i;

        /** GOAL:  pass a vector of numbers into a t-test
        */
         
            // unfortunately it core dumps
            runTTestOnJavaArray_cores(e, interp);   


        /*         since I've been unable to get that work, 
        *         I try various evaluation commands to see if I 
        *         can get any sort of R commands that take vectors to work
        *         in any sort of fashion
        */
          

        //
        //   SAMPLE eval based calls
        //
          
        //  can I successfully invoke a t.test command similar to what I can
        //  do via the command line, e.g. t.test (c(1,2,3), c(4,5,6))
        //  NO -- this core dumps
        // runTTestByEval_cores(e, interp);
            
        // can I eval anything that takes a vector -- let's try the mean
method
        // yes this works!
            //runMeanByEval_works(e, interp);
            
          
        //
        //    SAMPLE org.omegahat.Environment.DataStructures.numeric 
        //

        // this returns Nan
          
        // runMeanByNumericList_nan(e, interp);

        // given that the above returns NaN, what does R think I'm passing
it for a vector?
        // let's run some tests
        runTestsOnOmegahatNumeric(e, interp);
          
        // now let's try some tests on a java array instead of an omegahat
numeric
        runMeanOnJavaArray(e, interp);
    }
}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Wed Jul 20 17:50:44 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 20 Jul 2005 17:50:44 +0200
Subject: [R] sciviews installation
In-Reply-To: <20050720115215.SLZJ27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <20050720115215.SLZJ27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <42DE72D4.1000609@sciviews.org>

John Fox wrote:
> Dear Philippe,
> 
> The development version 1.1-0 of the Rcmdr package (i.e., the one with
> localization facilities) isn't yet on CRAN. Georges doesn't say which
> version of the Rcmdr package he's using, but I assume 1.0-2 from CRAN, which
> I believe should work with SciViews.

That is true, he doesn't say which version. However, SciViews 
compatibility of Rcmdr is broken since version 1.0-0. The last Rcmdr I 
can run properly under SciViews is Rcmdr 0.9-18. So, the current version 
on CRAN is also broken. For those using Rcmdr under SciViews, I would 
suggest to download version 0.9-18 of Rcmdr from the web site 
http://www.sciviews.org/SciViews-R, just for the time I fix the problem.

Sorry for this long delay before I fix this problem. R commander is 
evolving at an incredible speed. This is excellent, of course... but not 
easy to catch up with when you are away from your work for a couple of 
months (indeed, just taking care of my baby and finishing my new house, 
but I am not seeking for excuses) ;-)

Best,

Philippe Grosjean

> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>Philippe Grosjean
>>Sent: Wednesday, July 20, 2005 5:24 AM
>>To: ghjmora at gmail.com
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] sciviews installation
>>
>>Hello,
>>
>>Several changes in the latest Rcmdr broken the SciViews 
>>compatibility. I am working on it... but it takes longer than 
>>expected. Sorry for the inconvenience.
>>Best,
>>
>>Philippe
>>
>>..............................................<??}))><........
>>  ) ) ) ) )
>>( ( ( ( (    Prof. Philippe Grosjean
>>  ) ) ) ) )
>>( ( ( ( (    Numerical Ecology of Aquatic Systems
>>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
>>( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
>>( ( ( ( (
>>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
>>( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>>  ) ) ) ) )
>>( ( ( ( (    web:   http://www.umh.ac.be/~econum
>>  ) ) ) ) )          http://www.sciviews.org
>>( ( ( ( (
>>..............................................................
>>
>>ghjmora g mail wrote:
>>
>>>Hello
>>>
>>>
>>>1. a few months ago, I had sciviews working fine with R 
>>
>>(rw2001) under 
>>
>>>windows XP 2. now, upgrading to rw2011, the stuff seems fine (every 
>>>package installed),but I find a conflict when launching sciviews:
>>>- it runs, apparently
>>>- but when I try to work ("import/export In: text" for 
>>
>>instance), it 
>>
>>>asks for Rcmdr ("Would you like to install it now?")
>>>
>>>3. Rcmdr is already installed (with all dependencies) and 
>>
>>works well 
>>
>>>when called directly in R gui 4. and it's impossible to make it 
>>>reconized or to install it under sciviews
>>>
>>>
>>>I have all the latest packages, and I am going to get mad.
>>>
>>>what do you suggest to solve my problem ?
>>>
>>>Thanks
>>>
>>>Georges Moracchini
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From edd at debian.org  Wed Jul 20 17:53:00 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 20 Jul 2005 15:53:00 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Is_it_possible_to_create_highly_customized_report_i?=
	=?utf-8?q?n=09*=2Exlsformat_by_using_R/S+=3F?=
References: <726FC6DD09DE1046AF81B499D70C3BCE2C8168@twinfields02.CORP.TWINFIELDSCAPITAL.COM>
Message-ID: <loom.20050720T174803-629@post.gmane.org>

Whit Armstrong <whit <at> twinfieldscapital.com> writes:
> I have a package which I use to create excel files from R.
> 
> I have not been able to produce a configure script general enough for me
> to post it to cran, but I will send it to you if you like.
> 
> I use it for production jobs on our linux servers.  You may have to
> tinker a bit to get it to compile on windows.
> 
> Have a look at the examples below.  If it suits your needs, I will send
> it to you.

Looks straightforward, but does not fancy-pants formatting.  If that is 
sufficient, could one not borrow Greg's approach from read.xls() [ in package 
gdata, part of the unbundled gregmisc bundle ] and create a simpler write.xls()
that uses Perl's SpreadSheet::WriteExcel to write xls files -- just like Greg
uses the related SpreadSheet::ReadExcel to read xls files ?  That way you avoid
the configure hassles of the Java module you use here [ if I recall correctly,
you borrowed this from an Apache sub-project, no ? ]

Cheers, Dirk



From p.dalgaard at biostat.ku.dk  Wed Jul 20 18:09:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 18:09:03 +0200
Subject: [R] predict.lm  - standard error of predicted means?
In-Reply-To: <47307.198.103.196.130.1121873330.squirrel@198.103.196.130>
References: <47307.198.103.196.130.1121873330.squirrel@198.103.196.130>
Message-ID: <x2fyu91mu8.fsf@turmalin.kubism.ku.dk>

kehler at mathstat.dal.ca writes:

> Simple question.
> 
> For a simple linear regression, I obtained the "standard error of
> predicted means", for both a confidence and prediction interval:
> 
> x<-1:15
> y<-x + rnorm(n=15)
> model<-lm(y~x)
> predict.lm(model,newdata=data.frame(x=c(10,20)),se.fit=T,interval="confidence")$se.fit
>         1         2
> 0.2708064 0.7254615
> 
> predict.lm(model,newdata=data.frame(x=c(10,20)),se.fit=T,interval="prediction")$se.fit
>         1         2
> 0.2708064 0.7254615
> 
> 
> I was surprised to find that the standard errors returned were in fact the
> standard errors of the sampling distribution of Y_hat:
> 
> sqrt(MSE(1/n + (x-x_bar)^2/SS_x)),
> 
> not the standard errors of Y_new (predicted value):
> 
> sqrt(MSE(1 + 1/n + (x-x_bar)^2/SS_x)).
> 
> Is there a reason this quantity is called the "standard error of predicted
> means" if it doesn't relate to the prediction distribution?

Yes. Yhat is the predicted mean and se.fit is its standard deviation.
It doesn't change its meaning because you desire another kind of
prediction interval.

 
> Turning to Neter et al.'s Applied Linear Statistical Models, I note that
> if we have multiple observations, then the standard error of the mean of
> the predicted value:
> 
> sqrt(MSE(1/m + 1/n + (x-x_bar)^2/SS_x)),
> 
> reverts to the standard error of the sampling distribution of Y-hat, as m,
> the number of samples, gets large. Still, this doesn't explain the result
> for small sample sizes.

You can make completely similar considerations regarding the standard
errors of and about an estimated mean: sigma*sqrt(1+1/n) vs.
sigma*sqrt(1/m + 1/n) vs. sigma*sqrt(1/n). SEM is still the latter
quantity even if you are interested in another kind of prediction limit. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From vincent.goulet at act.ulaval.ca  Wed Jul 20 18:52:16 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 20 Jul 2005 12:52:16 -0400
Subject: [R] Issues with convolve
Message-ID: <200507201252.16499.vincent.goulet@act.ulaval.ca>

We obtained some disturbing results from convolve() (inaccuracies and negative 
probabilities). We'll try to make the context clear in as few lines as 
possible...

Our function panjer() (code below) basically computes recursively the 
probability mass function of a compound Poisson distribution. When the 
Poisson parameter lambda is very large, the starting value of the recursive 
scheme --- the mass at 0 --- is 0 and the recursion fails. One way to solve 
this problem is to divide lambda by 2^n, apply the panjer() function and then 
convolve the result with itself n times.

We applied the panjer() function with a lambda such that the mass at 0 is just 
larger than .Machine$double.xmin. We thus know that once this pmf is 
convoluted with itself, the first probabilities will be 0 (for the computer).

Here are the two issues we have with convolve():

1. The probabilities we know should be 0 are rather in the vicinity of 1E-19, 
as if convolve() could not "go lower". Using a hand made convolution function 
(not given here), we obtained the correct values. When probabilities get 
around 1E-12, results from convolve() and our home made function are 
essentially identical.

2. We obtained negative probabilities. More accurately, the same example 
returns negative probabilities under Windows, but not under Linux. We also 
obtained negative probabilities for another example under Linux, though.

We understand that convolve() computes the convolutions using fft(), but we 
are not familiar enough with the latter to assess if the above issues are 
some sort of bugs or normal behavior. In the latter case, is there is any 
workaround?

Any help/comments/ideas appreciated.

=== EXAMPLES ===
panjer <- function(fx, lambda)
{
    fx0 <- fx[1]
    fx <- fx[-1]
    r <- length(fx)
    cumul <- fs <- exp(-lambda * (1 - fx0))
    repeat
    {
        x <- length(fs)
        m <- min(x, r)
        fs <- c(fs, sum((lambda * 1:m / x) * head(fx, m) * rev(tail(fs, m))))
        if ((1-1E-8) < (cumul <- cumul + tail(fs, 1)))
            break
    }
    fs
}

### Linux example ###
> R.version
         _                
platform i386-pc-linux-gnu
arch     i386             
os       linux-gnu        
system   i386, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R   
> fs <- panjer(rep(0.2, 5), 600)          # compute pmf
> fs[1:10]                                # small values
 [1] 3.456597e-209 4.147916e-207 2.530229e-205 1.045690e-203
 [5] 3.292657e-202 8.422924e-201 1.822768e-199 3.431181e-198
 [9] 5.733481e-197 8.637025e-196
> fsc <- convolve(fs, rev(fs), type="o")  # convolution
> sum(fsc < 0)                            # no negatives under Linux
[1] 0
> fsc[1:10]                               # these should be 0
 [1] 4.819870e-19 4.135471e-19 4.511455e-19 3.994485e-19 3.820177e-19
 [6] 4.738272e-19 3.885447e-19 3.167399e-19 3.695636e-19 3.701792e-19
> sum(fsc)                                # no impact on the sum
[1] 1

### Windows example ###
> R.version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.1            
year     2005           
month    06             
day      20             
language R              
> fs <- panjer(rep(0.2, 5), 600)          # compute pmf
> fs[1:10]                                # small values
 [1] 3.456597e-209 4.147916e-207 2.530229e-205 1.045690e-203
 [5] 3.292657e-202 8.422924e-201 1.822768e-199 3.431181e-198
 [9] 5.733481e-197 8.637025e-196
> fsc <- convolve(fs, rev(fs), type="o")  # convolution
> sum(fsc < 0)                            # negatives under Windows
[1] 39
> fsc[1:10]                               # these should be 0
 [1] 4.049552e-19 3.345584e-19 4.126913e-19 3.351211e-19 2.758626e-19
 [6] 3.530111e-19 2.735041e-19 2.376711e-19 2.591287e-19 3.196405e-19
> sum(fsc)                                # no impact on the sum
[1] 1

--
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From mark.salsburg at gmail.com  Wed Jul 20 18:58:00 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Wed, 20 Jul 2005 12:58:00 -0400
Subject: [R] predict.lm - standard error of predicted means?
In-Reply-To: <x2fyu91mu8.fsf@turmalin.kubism.ku.dk>
References: <47307.198.103.196.130.1121873330.squirrel@198.103.196.130>
	<x2fyu91mu8.fsf@turmalin.kubism.ku.dk>
Message-ID: <dd48e20f0507200958838d84f@mail.gmail.com>

Can someone please refer me to a function or method that resolves this
structuring issue:

I have two matrices with identical colnames (89), but varying number
of observations:

matrix A                                matrix B 

217 x 89                              16063 x 89

I want to creat one matrix C that has both matrices adjacent to one
another, where matrix A is duplicated many times to create the same
row number for matrix B, i.e. 16063.

matrixA matrix B
matrixA
matrixA

so matrix C will be 16063 x 178

I've tried cbind() and merge() with no success..



From whit at twinfieldscapital.com  Wed Jul 20 19:09:30 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Wed, 20 Jul 2005 13:09:30 -0400
Subject: [R] Is it possible to create highly customized report
	in	*.xlsformat by using R/S+?
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE2C816D@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

It _does_ do formatting:

write.xls(x,"matrixFromR_formatted3.xls","matrix",formats=rep("#,##0.0;[
Red]#-##0.0;\"\"",nc))

And also writes multiple sheets to a workbook (I didn't post all the
examples).

I need those features for the reports I generate, but if the perl module
has the same features, then I'm happy to contribute to a solution for
Greg's package that would use perl instead.  The R -> C++ -> Java ->
file method is a huge pain.

On the other hand, there may be no reason to continue to work on this
project as MS has suggested that it will use xml formats for the next
version of office: http://www.theregister.co.uk/2005/06/03/_office_xml/

btw, here's the list of alternatives suggested by the POI team:
http://jakarta.apache.org/poi/hssf/alternatives.html

fyi, here is the project homepage:
http://jakarta.apache.org/poi/index.html

Thx,
Whit

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dirk Eddelbuettel
Sent: Wednesday, July 20, 2005 11:53 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R]Is it possible to create highly customized report in
*.xlsformat by using R/S+?

Whit Armstrong <whit <at> twinfieldscapital.com> writes:
> I have a package which I use to create excel files from R.
> 
> I have not been able to produce a configure script general enough for 
> me to post it to cran, but I will send it to you if you like.
> 
> I use it for production jobs on our linux servers.  You may have to 
> tinker a bit to get it to compile on windows.
> 
> Have a look at the examples below.  If it suits your needs, I will 
> send it to you.

Looks straightforward, but does not fancy-pants formatting.  If that is
sufficient, could one not borrow Greg's approach from read.xls() [ in
package gdata, part of the unbundled gregmisc bundle ] and create a
simpler write.xls() that uses Perl's SpreadSheet::WriteExcel to write
xls files -- just like Greg uses the related SpreadSheet::ReadExcel to
read xls files ?  That way you avoid the configure hassles of the Java
module you use here [ if I recall correctly, you borrowed this from an
Apache sub-project, no ? ]

Cheers, Dirk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mark.salsburg at gmail.com  Wed Jul 20 19:03:27 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Wed, 20 Jul 2005 13:03:27 -0400
Subject: [R] Combining two matrices
Message-ID: <dd48e20f05072010034e97c916@mail.gmail.com>

Can someone please refer me to a function or method that resolves this
structuring issue:

I have two matrices with identical colnames (89), but varying number
of observations:

matrix A                                matrix B

217 x 89                              16063 x 89

I want to creat one matrix C that has both matrices adjacent to one
another, where matrix A is duplicated many times to create the same
row number for matrix B, i.e. 16063.

matrixA matrix B
matrixA
matrixA

so matrix C will be 16063 x 178

I've tried cbind() and merge() with no success..



From p.dalgaard at biostat.ku.dk  Wed Jul 20 19:06:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 19:06:26 +0200
Subject: [R] predict.lm - standard error of predicted means?
In-Reply-To: <dd48e20f0507200958838d84f@mail.gmail.com>
References: <47307.198.103.196.130.1121873330.squirrel@198.103.196.130>
	<x2fyu91mu8.fsf@turmalin.kubism.ku.dk>
	<dd48e20f0507200958838d84f@mail.gmail.com>
Message-ID: <x27jfl1k6l.fsf@turmalin.kubism.ku.dk>

mark salsburg <mark.salsburg at gmail.com> writes:

> Can someone please refer me to a function or method that resolves this
> structuring issue:
> 
> I have two matrices with identical colnames (89), but varying number
> of observations:
> 
> matrix A                                matrix B 
> 
> 217 x 89                              16063 x 89
> 
> I want to creat one matrix C that has both matrices adjacent to one
> another, where matrix A is duplicated many times to create the same
> row number for matrix B, i.e. 16063.
> 
> matrixA matrix B
> matrixA
> matrixA
> 
> so matrix C will be 16063 x 178
> 
> I've tried cbind() and merge() with no success..

A: What the !!##"?? does this have to do with the subject line?

B: This should do it:

cbind(A[rep(1:217,length=16063),], B)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Jul 20 19:09:52 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Jul 2005 13:09:52 -0400
Subject: [R] Issues with convolve
In-Reply-To: <200507201252.16499.vincent.goulet@act.ulaval.ca>
References: <200507201252.16499.vincent.goulet@act.ulaval.ca>
Message-ID: <42DE8560.7060203@stats.uwo.ca>

On 7/20/2005 12:52 PM, Vincent Goulet wrote:
> We obtained some disturbing results from convolve() (inaccuracies and negative 
> probabilities). We'll try to make the context clear in as few lines as 
> possible...
> 
> Our function panjer() (code below) basically computes recursively the 
> probability mass function of a compound Poisson distribution. When the 
> Poisson parameter lambda is very large, the starting value of the recursive 
> scheme --- the mass at 0 --- is 0 and the recursion fails. One way to solve 
> this problem is to divide lambda by 2^n, apply the panjer() function and then 
> convolve the result with itself n times.
> 
> We applied the panjer() function with a lambda such that the mass at 0 is just 
> larger than .Machine$double.xmin. We thus know that once this pmf is 
> convoluted with itself, the first probabilities will be 0 (for the computer).
> 
> Here are the two issues we have with convolve():
> 
> 1. The probabilities we know should be 0 are rather in the vicinity of 1E-19, 
> as if convolve() could not "go lower". Using a hand made convolution function 
> (not given here), we obtained the correct values. When probabilities get 
> around 1E-12, results from convolve() and our home made function are 
> essentially identical.
> 
> 2. We obtained negative probabilities. More accurately, the same example 
> returns negative probabilities under Windows, but not under Linux. We also 
> obtained negative probabilities for another example under Linux, though.
> 
> We understand that convolve() computes the convolutions using fft(), but we 
> are not familiar enough with the latter to assess if the above issues are 
> some sort of bugs or normal behavior. In the latter case, is there is any 
> workaround?

Rounding will depend on the hardware and the compiler, so this might be 
normal behaviour.  It's a little disturbing, but you shouldn't expect 
calculations to be accurate to more digits than your platform supports.
Convolving using an FFT is essentially rotating the vectors in a high 
dimensional space, multiplying terms, and then rotating back.  Unless 
those rotations are unrealistically accurate very small numbers won't 
necessarily show up as zeros.

I'd suggest re-thinking your panjer function to work in log 
probabilities instead of probabilities, so that you can handle a larger 
dynamic range before you run into underflow problems, and you don't need 
to use convolve at all. Convert them back to probabilities at the very 
end.

Duncan Murdoch



From pbarros at ualg.pt  Wed Jul 20 19:14:53 2005
From: pbarros at ualg.pt (Pedro de Barros)
Date: Wed, 20 Jul 2005 18:14:53 +0100
Subject: [R] Combining two matrices
In-Reply-To: <dd48e20f05072010034e97c916@mail.gmail.com>
References: <dd48e20f05072010034e97c916@mail.gmail.com>
Message-ID: <6.1.2.0.2.20050720181141.02fdc1a8@pop.ualg.pt>

Please clarify:
Is there a column of common values that you want to use to merge the two 
matrices together? Or do you just want to replicate matrix A enough times 
to have the same number of rows as B? I could think this was the case, but 
16063 is not a multiple of 217.
Anyway, you will have to change the colnames of at least one of them.

Pedro
At 18:03 20/07/2005, you wrote:
>Can someone please refer me to a function or method that resolves this
>structuring issue:
>
>I have two matrices with identical colnames (89), but varying number
>of observations:
>
>matrix A                                matrix B
>
>217 x 89                              16063 x 89
>
>I want to creat one matrix C that has both matrices adjacent to one
>another, where matrix A is duplicated many times to create the same
>row number for matrix B, i.e. 16063.
>
>matrixA matrix B
>matrixA
>matrixA
>
>so matrix C will be 16063 x 178
>
>I've tried cbind() and merge() with no success..
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From greg.snow at ihc.com  Wed Jul 20 19:15:40 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Wed, 20 Jul 2005 11:15:40 -0600
Subject: [R] Is it possible to create highly customized report in *.xls
 format by using R/S+?
Message-ID: <s2de3261.021@lp-msg1.co.ihc.com>


When you are forced to use excel (but want to really use R and just 
give the result to others in excel), then there are a few options 
depending on what you are trying to do.  We may be able to give
better help if you can give a specific problem you are trying to
solve.

Some Ideas:

To quickly copy data from excel (highlight a region in excel and
choose
copy then:)

mydata <- read.delim(file('clipboard'))

to send a dataframe or matrix to excel:

write.table(mydata, file('clipboard'), sep="\t")
(now switch to excel, select a cell and choose paste).

You can also look at the RODBC package for other ways to transfer
information back and forth between R and excel.

For more complicated output you may want to look at the R2HTML package
or the LaTeX functions in Hmisc and other packages (then use a latex
to
rtf converter so you end users can read the output in word or copy it
over
to latex).

Another place to look
is:http://cran.us.r-project.org/contrib/extra/dcom/RSrv135.html

This has examples of building functions in excel that will take the
data from
excel, analyze it in R, then bring the results back to excel.

More detail on what you are trying to do would help us help you.

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Wensui Liu <liuwensui at gmail.com> 07/20/05 08:55AM >>>
I appreciate your reply and understand your point completely. But at
times we can't change the rule, the only choice is to follow the rule.
Most deliverables in my work are in excel format.

On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> See:
> 
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html 
> and
> http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf 
> 
> Greg Snow, Ph.D.
> Statistical Data Center, LDS Hospital
> Intermountain Health Care
> greg.snow at ihc.com 
> (801) 408-8111
> 
> >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> I remember in one slide of Prof. Ripley's presentation overhead, he
> said the most popular data analysis software is excel.
> 
> So is there any resource or tutorial on this topic?
> 
> Thank you so much!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html 
> 
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From mark.salsburg at gmail.com  Wed Jul 20 19:24:15 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Wed, 20 Jul 2005 13:24:15 -0400
Subject: [R] Combining two matrices
In-Reply-To: <6.1.2.0.2.20050720181141.02fdc1a8@pop.ualg.pt>
References: <dd48e20f05072010034e97c916@mail.gmail.com>
	<6.1.2.0.2.20050720181141.02fdc1a8@pop.ualg.pt>
Message-ID: <dd48e20f05072010241375e136@mail.gmail.com>

The latter, I want to replicate A enough times to have the same rows
as B. I realize that 16063 is not a mutliple 217, that doesn't really
matter for this problem
Assuming I change the colnames of one of them, how can I create a
matrix C that is the composite of the replicated A with B.

thank you again.

On 7/20/05, Pedro de Barros <pbarros at ualg.pt> wrote:
> Please clarify:
> Is there a column of common values that you want to use to merge the two
> matrices together? Or do you just want to replicate matrix A enough times
> to have the same number of rows as B? I could think this was the case, but
> 16063 is not a multiple of 217.
> Anyway, you will have to change the colnames of at least one of them.
> 
> Pedro
> At 18:03 20/07/2005, you wrote:
> >Can someone please refer me to a function or method that resolves this
> >structuring issue:
> >
> >I have two matrices with identical colnames (89), but varying number
> >of observations:
> >
> >matrix A                                matrix B
> >
> >217 x 89                              16063 x 89
> >
> >I want to creat one matrix C that has both matrices adjacent to one
> >another, where matrix A is duplicated many times to create the same
> >row number for matrix B, i.e. 16063.
> >
> >matrixA matrix B
> >matrixA
> >matrixA
> >
> >so matrix C will be 16063 x 178
> >
> >I've tried cbind() and merge() with no success..
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Wed Jul 20 19:25:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2005 18:25:15 +0100 (BST)
Subject: [R] predict.lm - standard error of predicted means?
In-Reply-To: <x27jfl1k6l.fsf@turmalin.kubism.ku.dk>
References: <47307.198.103.196.130.1121873330.squirrel@198.103.196.130>
	<x2fyu91mu8.fsf@turmalin.kubism.ku.dk>
	<dd48e20f0507200958838d84f@mail.gmail.com>
	<x27jfl1k6l.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0507201822500.29814@gannet.stats>

On Wed, 20 Jul 2005, Peter Dalgaard wrote:

> mark salsburg <mark.salsburg at gmail.com> writes:
>
>> Can someone please refer me to a function or method that resolves this
>> structuring issue:
>>
>> I have two matrices with identical colnames (89), but varying number
>> of observations:
>>
>> matrix A                                matrix B
>>
>> 217 x 89                              16063 x 89
>>
>> I want to creat one matrix C that has both matrices adjacent to one
>> another, where matrix A is duplicated many times to create the same
>> row number for matrix B, i.e. 16063.
>>
>> matrixA matrix B
>> matrixA
>> matrixA
>>
>> so matrix C will be 16063 x 178
>>
>> I've tried cbind() and merge() with no success..
>
> A: What the !!##"? does this have to do with the subject line?
>
> B: This should do it:
>
> cbind(A[rep(1:217,length=16063),], B)

But note that makes 74 + 5/217 copies of A, and I did wonder if that was 
the intention (or if not, what was intended).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From reid_huntsinger at merck.com  Wed Jul 20 19:27:21 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 20 Jul 2005 13:27:21 -0400
Subject: [R] Combining two matrices
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A952F@uswpmx00.merck.com>

If 16063 were an integer multiple of 217 you could do this in two steps;
first construct the left half with rep() or

L <- kronecker(rep(1,16063/217),A)

and then use cbind(L,B).

However, 16063 is not a multiple of 217, so I don't know what you want to do
with the leftover 5 rows you need in L. If you want to recycle rows, you
could use ceiling(16063/217) in the call to rep and then use
cbind(L[1:16063,],B).

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of mark salsburg
Sent: Wednesday, July 20, 2005 1:03 PM
To: R-help at stat.math.ethz.ch
Subject: [R] Combining two matrices


Can someone please refer me to a function or method that resolves this
structuring issue:

I have two matrices with identical colnames (89), but varying number
of observations:

matrix A                                matrix B

217 x 89                              16063 x 89

I want to creat one matrix C that has both matrices adjacent to one
another, where matrix A is duplicated many times to create the same
row number for matrix B, i.e. 16063.

matrixA matrix B
matrixA
matrixA

so matrix C will be 16063 x 178

I've tried cbind() and merge() with no success..

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pbarros at ualg.pt  Wed Jul 20 19:42:19 2005
From: pbarros at ualg.pt (Pedro de Barros)
Date: Wed, 20 Jul 2005 18:42:19 +0100
Subject: [R] Combining two matrices
In-Reply-To: <dd48e20f05072010241375e136@mail.gmail.com>
References: <dd48e20f05072010034e97c916@mail.gmail.com>
	<6.1.2.0.2.20050720181141.02fdc1a8@pop.ualg.pt>
	<dd48e20f05072010241375e136@mail.gmail.com>
Message-ID: <6.1.2.0.2.20050720184105.02eb3ec0@pop.ualg.pt>

Hi,

Se if this will solve your question (quick and dirty)...
A is the first matrix, B is the second...

"F.CombineMat" <-
function (A, B)
{
ind<-rep(1:nrow(A),ceiling(nrow(B)/nrow(A)))
ind<-ind[1:nrow(B)]
A<-A[ind,]
colnames(A)<-paste('A', colnames(A), sep='')
R<-cbind(A,B)
R
}
Greetings,
Pedro

At 18:24 20/07/2005, you wrote:
>The latter, I want to replicate A enough times to have the same rows
>as B. I realize that 16063 is not a mutliple 217, that doesn't really
>matter for this problem
>Assuming I change the colnames of one of them, how can I create a
>matrix C that is the composite of the replicated A with B.
>
>thank you again.
>
>On 7/20/05, Pedro de Barros <pbarros at ualg.pt> wrote:
> > Please clarify:
> > Is there a column of common values that you want to use to merge the two
> > matrices together? Or do you just want to replicate matrix A enough times
> > to have the same number of rows as B? I could think this was the case, but
> > 16063 is not a multiple of 217.
> > Anyway, you will have to change the colnames of at least one of them.
> >
> > Pedro
> > At 18:03 20/07/2005, you wrote:
> > >Can someone please refer me to a function or method that resolves this
> > >structuring issue:
> > >
> > >I have two matrices with identical colnames (89), but varying number
> > >of observations:
> > >
> > >matrix A                                matrix B
> > >
> > >217 x 89                              16063 x 89
> > >
> > >I want to creat one matrix C that has both matrices adjacent to one
> > >another, where matrix A is duplicated many times to create the same
> > >row number for matrix B, i.e. 16063.
> > >
> > >matrixA matrix B
> > >matrixA
> > >matrixA
> > >
> > >so matrix C will be 16063 x 178
> > >
> > >I've tried cbind() and merge() with no success..
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >



From marxlau1 at msu.edu  Wed Jul 20 19:58:37 2005
From: marxlau1 at msu.edu (Laura M Marx)
Date: Wed, 20 Jul 2005 13:58:37 -0400
Subject: [R] Regression lines for differently-sized groups on the same
	plot
In-Reply-To: <42DDBD46.2060209@pdf.com>
References: <E1Dv1ts-00085Q-7O@sys25.mail.msu.edu> <42DDBD46.2060209@pdf.com>
Message-ID: <E1DvIq5-0001Uz-O6@sys14.mail.msu.edu>

Sundar Dorai-Raj writes: 

> Hi, Laura, 
> 
> Would ?predict.glm be better? 
> 
> plot(logarea, hempresence,
>      xlab = "Surface area of log (m2)",
>      ylab="Probability of hemlock seedling presence",
>      type="n", font.lab=2, cex.lab=1.5, axes=TRUE)
> lines(logarea, predict(hemhem, logreg, "response"), lty=1, lwd=2)
> lines(logarea, predict(hemyb, logreg, "response"), lty="dashed", lwd=2)
> lines(logarea, predict(hemsm, logreg, "response"), lty="dotted", lwd=2) 
> 
> Without seeing more description of your data, this is still a guess. 
> 
> --sundar 
> 

YES!  Thank you.  That solves all of the problems I was having.  Thanks also 
to Tom Mulholland and Bill Venables for their replies.
     Laura
The final code is now: 

logreg <- read.table("C:/Documents and Settings/Laura/Desktop/logreg.txt", 
header=TRUE, sep=",", na.strings="NA", dec=".", strip.white=TRUE)
#This is the full dataset.  A hemlock row followed by a birch row might look 
#like:
#Hemybsm     logarea    hempresence
#1           0.054      0
#2           1.370      1 

hemhem=glm(hempresence~logarea, family=binomial(logit), subset=Hemybsm<2)
#This pulls out only the hemlocks from the full dataset and calculates the 
#regression.
hemyb=glm(hempresence~logarea, family=binomial(logit), subset=Hemybsm==2)
#Same code, with only the birches from the full dataset.
hemsm=glm(hempresence~logarea, family=binomial(logit), subset=Hemybsm>2) 

plot(logarea, hempresence, xlab = "Surface area of log (m2)",
    ylab="Probability of hemlock seedling presence",
    type="n", font.lab=2, cex.lab=1.5, axes=TRUE)
lines(logarea, predict(hemhem, logreg, "response"), lty=1, lwd=2)
lines(logarea, predict(hemyb, logreg, "response"), lty="dashed", lwd=2)
lines(logarea, predict(hemsm, logreg, "response"), lty="dotted", lwd=2)



From ghjmora at gmail.com  Wed Jul 20 20:54:29 2005
From: ghjmora at gmail.com (ghjmora g mail)
Date: Wed, 20 Jul 2005 20:54:29 +0200
Subject: [R] sciviews installation
In-Reply-To: <42DE72D4.1000609@sciviews.org>
References: <20050720115215.SLZJ27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>
	<42DE72D4.1000609@sciviews.org>
Message-ID: <42DE9DE5.8070801@gmail.com>

Thanks for the replies

1. about Rcmdr versions
Sorry, I did'nt say wich versions because I tried a lot... before asking 
for help
First I tried the version of the site www.sciviews.org  (0.9-14), after 
I tried the Cran version (1.0-2) and at last I got the 1.1-0 version on 
John Fox's web.
After that, I wrote the message for help...
2. my mistake
 Excuse me, I met the first problem a few months ago with sciviews when 
upgrading the packages on line, and I was hoping for a new version of R 
with correct bindings. I never tried to downgrade the package (shame on me).

3.
Now, I run R2.1.1 patched, with Rcmdr 0.9-17 which  was always here in a 
directory on my computer, because I was not able  to find 0.9-18. The 
result is that SciViews runs, with a little disagreement, but I am so 
happy that I take it like a kind of joke (obliged to load ADE4 in 
SciViews with the command line before use it)


Regards

Georges Moracchini
University of Corsica



From rvalliant at survey.umd.edu  Wed Jul 20 21:44:00 2005
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Wed, 20 Jul 2005 15:44:00 -0400
Subject: [R] Installation error on Sun Solaris
Message-ID: <s2de714a.087@SURVEYGWIA.UMD.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050720/b795bb85/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Jul 20 22:00:47 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2005 22:00:47 +0200
Subject: [R] Installation error on Sun Solaris
In-Reply-To: <s2de714a.087@SURVEYGWIA.UMD.EDU>
References: <s2de714a.087@SURVEYGWIA.UMD.EDU>
Message-ID: <x23bq91c40.fsf@turmalin.kubism.ku.dk>

"Richard Valliant" <rvalliant at survey.umd.edu> writes:

> This is from a colleague who is attempting to install R 2.1.1 on a Sun
> machine. I hope this is sufficient information for someone to give us
> some pointers.
>  
> thanks,
> Richard
>  
> Our system is a Sunfire 480 running Solaris V2.8
> The LD_LIBRARY_PATH is set
> to:/usr/local/bin:/usr/lib:/usr/openwin/libL/opt/SUNWspro/libThe
> path:/usr/local/lib     does not exist on our server.The configure
> results in several errors. The final one states:"Error:
> --with-readlines=yes (default) and headers/libs are not available"

Either install the readline library/headerfiles, or use
--with-readline=no (and live without command line editing).

The
> install instructions next indicate that the command "make" is to be ran.
> This results in the following message:"make: Fatal error: No arguments
> to build" 

Yes, this assumes that the configure step succeeded. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From chrysopa at insecta.ufv.br  Wed Jul 20 22:44:54 2005
From: chrysopa at insecta.ufv.br (Ronaldo Reis-Jr.)
Date: Wed, 20 Jul 2005 17:44:54 -0300
Subject: [R] help with a xyplot legend
Message-ID: <200507201744.55343.chrysopa@insecta.ufv.br>

Hi,

I try to put a legend in a xyplot graphic.

xyplot(y~x|g,ylim=c(0,80),xlim=c(10,40),as.table=T,layout=c(2,3), ylab="N??mero 
de machos capturados",xlab=expression(paste("Temperatura (",degree,"C)")), 
key=list(corner=c(0,0),x=0, y=0, text=list(legenda),lines=list(col=cor, lwd = 
espessura, lty=linha),columns=7,between=0.5,betwen.columns=0.5,cex=0.8))

The problem is that legend is very close do xlab. I try change 
corner=c(0,0),x=0, y=0, to corner=c(0,0),x=0, y=1, but in this way the legend 
dont appear.

How to make the vertical bottom area of the plot bigger to put the legend a 
bit separated of the xlabel?

Thanks
Ronaldo
-- 

O pre??o da sabedoria ?? detestar tudo

--Mill??r Fernandes
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From iidn01 at yahoo.com  Wed Jul 20 22:58:21 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Wed, 20 Jul 2005 13:58:21 -0700 (PDT)
Subject: [R] aregImpute in Hmisc
Message-ID: <20050720205821.36315.qmail@web31108.mail.mud.yahoo.com>

Hi,

I have a dataframe ds1.2 - 503 categorial variables
and 1 continuous response variables.  I ran aregImpute
to deal with NA's and got the followig error:

> fmla = terms( Response ~ . ,data=ds1.2)
> ds.i = aregImpute(fmla,data=ds1.2)
Error in matrix(as.double(1), nrow = n, ncol = p,
dimnames = list(rnam,  : 
        length of dimnames [2] not equal to array
extent

Could you explain what I should do? Thanks a lot.

Young.



From AdamsL at thenfl.com  Wed Jul 20 23:46:57 2005
From: AdamsL at thenfl.com (Lorena Adams)
Date: Wed, 20 Jul 2005 14:46:57 -0700
Subject: [R] TURF (total unreplicated reach and frequency) Analysis program
Message-ID: <s2de63f7.060@mail.thenfl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050720/a1636a6f/attachment.pl

From bolker at ufl.edu  Thu Jul 21 00:48:31 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 20 Jul 2005 22:48:31 +0000 (UTC)
Subject: [R] poisson fit for histogram
References: <6DE2C8B4-3C9E-47F9-BB3D-87D6E0967898@plantpath.wisc.edu>
Message-ID: <loom.20050721T003534-34@post.gmane.org>

Thomas Isenbarger <isen <at> plantpath.wisc.edu> writes:

> 
> I haven't been an R lister for a bit, but I hope to enlist someone's  
> help here.  I think this is a simple question, so I hope the answer  
> is not much trouble.  Can you please respond directly to this email  
> address in addition to the list (if responding to the list is  
> warranted)?
> 
> I have a histogram and I want to see if the data fit a Poisson  
> distribution.  How do I do this?  It is preferable if it could be  
> done without having to install any or many packages.
> 
> I use R Version 1.12 (1622) on OS X
> 
> Thank-you very much,
> Tom Isenbarger
> 
> --
> Tom Isenbarger PhD
> isen <at> plantpath.wisc.edu
> 608.265.0850
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

  by "histogram" do you mean that you have counts for each
non-negative integer?  or are the data more binned than that?
(I'll assume the former since it's slightly easier to deal with.)
Say you have vectors "number" and "count".  The mean of
the sample is meanval <- sum(number*count).  The _expected_ number
of counts in each bin if the distribution is
Poisson is expval <- sum(count)*dpois(number,meanval).
The chi-square statistic is csq <- sum((expval-count)^2/expval), with
df <- length(count)-2 degrees of freedom (for the mean and the total
number of observations).  pchisq(csq,df=df,lower.tail=FALSE)
should give you the chi-squared probability.

  A couple of minor issues: (1) beware, this is shooting from
the hip -- haven't tested at all; (2) you may have to deal
with lumping categories (rule of thumb is that expected number
of counts in a bin should not be < 5).

  Other ways to tackle this: use fitdistr() from MASS with
different candidate distributions (Poisson, neg. bin.) and
do a likelihood ratio test or compare AICs; compare variance and
mean of distribution (much cruder).

See
http://www.zoo.ufl.edu/bolker/emd/probs/ED-4.1.html
for a worked example of a similar problem.



From ggrothendieck at gmail.com  Thu Jul 21 01:27:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Jul 2005 19:27:38 -0400
Subject: [R] Is it possible to create highly customized report in *.xls
	format by using R/S+?
In-Reply-To: <1115a2b005072007554e1ba1f1@mail.gmail.com>
References: <s2de0ea8.033@lp-msg1.co.ihc.com>
	<1115a2b005072007554e1ba1f1@mail.gmail.com>
Message-ID: <971536df05072016271c69539d@mail.gmail.com>

Here is an example where R is the client and Excel is the server 
so that R is issuing commands to Excel.  This example uses the 
RDCOMClient package from www.omegahat.org:

	library(RDCOMClient)
	xl <- COMCreate("Excel.Application")  # starts up Excel
	xl[["Visible"]] <- TRUE                       # Excel becomes visible
	wkbk <- xl$Workbooks()$Add()          # new workbook

	# set some cells

	sh <- xl$ActiveSheet()

	x12 <- sh$Cells(1,2)
	x12[["Value"]] <- 123

	x22 <- sh$Cells(2,2)
	x22[["Value"]] <- 100

	x31 <- sh$Cells(3,1)
	x31[["Value"]] <- "Total"

	B3R <- sh$Range("B3")
	B3R[["Formula"]] <- "=Sum(R1C2:R2C2)"
	B3R[["NumberFormat"]] <- "_($* #,##0.00_)"
	B3RF <- B3R$Font()
	B3RF[["Bold"]] <- TRUE


	# save and exit
	wkbk$SaveAs("\\test.xls")
	xl$Quit()

Code using the rcom package at (second link is mailing list):

	http://sunsite.univie.ac.at/rcom/download/
	http://mailman.csd.univie.ac.at/pipermail/rcom-l/

would be nearly identical once the upcoming version of rcom comes out.  
rcom and omegahat both provide the possibility of having
Excel as the client and R as the server; however, in that setup the
user would have to have R running whereas in the above setup only you do.

On 7/20/05, Wensui Liu <liuwensui at gmail.com> wrote:
> I appreciate your reply and understand your point completely. But at
> times we can't change the rule, the only choice is to follow the rule.
> Most deliverables in my work are in excel format.
> 
> On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> > See:
> >
> > http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> > and
> > http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> >
> > Greg Snow, Ph.D.
> > Statistical Data Center, LDS Hospital
> > Intermountain Health Care
> > greg.snow at ihc.com
> > (801) 408-8111
> >
> > >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> > I remember in one slide of Prof. Ripley's presentation overhead, he
> > said the most popular data analysis software is excel.
> >
> > So is there any resource or tutorial on this topic?
> >
> > Thank you so much!



From ggrothendieck at gmail.com  Thu Jul 21 01:28:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Jul 2005 19:28:49 -0400
Subject: [R] poisson fit for histogram
In-Reply-To: <6DE2C8B4-3C9E-47F9-BB3D-87D6E0967898@plantpath.wisc.edu>
References: <6DE2C8B4-3C9E-47F9-BB3D-87D6E0967898@plantpath.wisc.edu>
Message-ID: <971536df05072016283cd52eee@mail.gmail.com>

See ?goodfit in package vcd.


On 7/20/05, Thomas Isenbarger <isen at plantpath.wisc.edu> wrote:
> I haven't been an R lister for a bit, but I hope to enlist someone's
> help here.  I think this is a simple question, so I hope the answer
> is not much trouble.  Can you please respond directly to this email
> address in addition to the list (if responding to the list is
> warranted)?
> 
> I have a histogram and I want to see if the data fit a Poisson
> distribution.  How do I do this?  It is preferable if it could be
> done without having to install any or many packages.
> 
> I use R Version 1.12 (1622) on OS X
> 
> Thank-you very much,
> Tom Isenbarger
> 
> 
> --
> Tom Isenbarger PhD
> isen at plantpath.wisc.edu
> 608.265.0850
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jul 21 01:29:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Jul 2005 19:29:24 -0400
Subject: [R] Issues with convolve
In-Reply-To: <200507201252.16499.vincent.goulet@act.ulaval.ca>
References: <200507201252.16499.vincent.goulet@act.ulaval.ca>
Message-ID: <971536df050720162925632a25@mail.gmail.com>

Check out sum.exact in the caTools package.


On 7/20/05, Vincent Goulet <vincent.goulet at act.ulaval.ca> wrote:
> We obtained some disturbing results from convolve() (inaccuracies and negative
> probabilities). We'll try to make the context clear in as few lines as
> possible...
> 
> Our function panjer() (code below) basically computes recursively the
> probability mass function of a compound Poisson distribution. When the
> Poisson parameter lambda is very large, the starting value of the recursive
> scheme --- the mass at 0 --- is 0 and the recursion fails. One way to solve
> this problem is to divide lambda by 2^n, apply the panjer() function and then
> convolve the result with itself n times.
> 
> We applied the panjer() function with a lambda such that the mass at 0 is just
> larger than .Machine$double.xmin. We thus know that once this pmf is
> convoluted with itself, the first probabilities will be 0 (for the computer).
> 
> Here are the two issues we have with convolve():
> 
> 1. The probabilities we know should be 0 are rather in the vicinity of 1E-19,
> as if convolve() could not "go lower". Using a hand made convolution function
> (not given here), we obtained the correct values. When probabilities get
> around 1E-12, results from convolve() and our home made function are
> essentially identical.
> 
> 2. We obtained negative probabilities. More accurately, the same example
> returns negative probabilities under Windows, but not under Linux. We also
> obtained negative probabilities for another example under Linux, though.
> 
> We understand that convolve() computes the convolutions using fft(), but we
> are not familiar enough with the latter to assess if the above issues are
> some sort of bugs or normal behavior. In the latter case, is there is any
> workaround?
> 
> Any help/comments/ideas appreciated.
> 
> === EXAMPLES ===
> panjer <- function(fx, lambda)
> {
>    fx0 <- fx[1]
>    fx <- fx[-1]
>    r <- length(fx)
>    cumul <- fs <- exp(-lambda * (1 - fx0))
>    repeat
>    {
>        x <- length(fs)
>        m <- min(x, r)
>        fs <- c(fs, sum((lambda * 1:m / x) * head(fx, m) * rev(tail(fs, m))))
>        if ((1-1E-8) < (cumul <- cumul + tail(fs, 1)))
>            break
>    }
>    fs
> }
> 
> ### Linux example ###
> > R.version
>         _
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> > fs <- panjer(rep(0.2, 5), 600)          # compute pmf
> > fs[1:10]                                # small values
>  [1] 3.456597e-209 4.147916e-207 2.530229e-205 1.045690e-203
>  [5] 3.292657e-202 8.422924e-201 1.822768e-199 3.431181e-198
>  [9] 5.733481e-197 8.637025e-196
> > fsc <- convolve(fs, rev(fs), type="o")  # convolution
> > sum(fsc < 0)                            # no negatives under Linux
> [1] 0
> > fsc[1:10]                               # these should be 0
>  [1] 4.819870e-19 4.135471e-19 4.511455e-19 3.994485e-19 3.820177e-19
>  [6] 4.738272e-19 3.885447e-19 3.167399e-19 3.695636e-19 3.701792e-19
> > sum(fsc)                                # no impact on the sum
> [1] 1
> 
> ### Windows example ###
> > R.version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
> > fs <- panjer(rep(0.2, 5), 600)          # compute pmf
> > fs[1:10]                                # small values
>  [1] 3.456597e-209 4.147916e-207 2.530229e-205 1.045690e-203
>  [5] 3.292657e-202 8.422924e-201 1.822768e-199 3.431181e-198
>  [9] 5.733481e-197 8.637025e-196
> > fsc <- convolve(fs, rev(fs), type="o")  # convolution
> > sum(fsc < 0)                            # negatives under Windows
> [1] 39
> > fsc[1:10]                               # these should be 0
>  [1] 4.049552e-19 3.345584e-19 4.126913e-19 3.351211e-19 2.758626e-19
>  [6] 3.530111e-19 2.735041e-19 2.376711e-19 2.591287e-19 3.196405e-19
> > sum(fsc)                                # no impact on the sum
> [1] 1
> 
> --
>  Vincent Goulet, Associate Professor
>  ??cole d'actuariat
>  Universit?? Laval, Qu??bec
>  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jp.mcdermott at gmail.com  Thu Jul 21 01:31:33 2005
From: jp.mcdermott at gmail.com (James McDermott)
Date: Wed, 20 Jul 2005 19:31:33 -0400
Subject: [R] Taking the derivative of a quadratic B-spline
In-Reply-To: <42DE2724.2020209@stats.uwo.ca>
References: <33871de605071911531e399f4d@mail.gmail.com>
	<42DD4F6B.7090600@stats.uwo.ca>
	<33871de605071912341b778fdb@mail.gmail.com>
	<42DD5B78.40906@stats.uwo.ca>
	<33871de6050719185047ade71e@mail.gmail.com>
	<42DE2724.2020209@stats.uwo.ca>
Message-ID: <33871de6050720163142a74d12@mail.gmail.com>

Thanks a lot for the input on this Professor Murdoch.  I really
appreciate all the help.

Regards,
Jim

On 7/20/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> James McDermott wrote:
> > Would the unique quadratic defined by the three points be the same
> > curve as the curve predicted by a quadratic B-spline (fit to all of
> > the data) through those same three points?
> 
> Yes, if you restrict attention to an interval between knots.  You'll
> need to re-evaluate it for each such interval (but since quadratic
> splines are continuous, you can reuse evaluations at the knots, and you
> just need one new point in each interval).
> 
>  From a practical point of view, you need to make sure that COBS really
> is giving you a quadratic spline and really is reporting all of the
> knots correctly.  Watch out for coincident knots (zero length
> intervals); you don't care about the derivative on those, but they might
> cause overflows in some calculations.
> 
> Duncan Murdoch
> >
> > Jim
> >
> > On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >
> >>On 7/19/2005 3:34 PM, James McDermott wrote:
> >>
> >>>I wish it were that simple (perhaps it is and I am just not seeing
> >>>it).  The output from cobs( ) includes the B-spline coefficients and
> >>>the knots.  These coefficients are not the same as the a, b, and c
> >>>coefficients in a quadratic polynomial.  Rather, they are the
> >>>coefficients of the quadratic B-spline representation of the fitted
> >>>curve.  I need to evaluate a linear combination of basis functions and
> >>>it is not clear to me how to accomplish this easily.  I was hoping to
> >>>find an alternative way of getting the derivatives.
> >>
> >>I don't know COBS, but doesn't predict just evaluate the B-spline?  The
> >>point of what I posted is that the particular basis doesn't matter if
> >>you can evaluate the quadratic at 3 points.
> >>
> >>Duncan Murdoch
> >>
> >>
> >>>Jim McDermott
> >>>
> >>>On 7/19/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >>>
> >>>>On 7/19/2005 2:53 PM, James McDermott wrote:
> >>>>
> >>>>>Hello,
> >>>>>
> >>>>>I have been trying to take the derivative of a quadratic B-spline
> >>>>>obtained by using the COBS library.  What I would like to do is
> >>>>>similar to what one can do by using
> >>>>>
> >>>>>fit<-smooth.spline(cdf)
> >>>>>xx<-seq(-10,10,.1)
> >>>>>predict(fit, xx, deriv = 1)
> >>>>>
> >>>>>The goal is to fit the spline to data that is approximating a
> >>>>>cumulative distribution function (e.g. in my example, cdf is a
> >>>>>2-column matrix with x values in column 1 and the estimate of the cdf
> >>>>>evaluated at x in column 2) and then take the first derivative over a
> >>>>>range of values to get density estimates.
> >>>>>
> >>>>>The reason I don't want to use smooth.spline is that there is no way
> >>>>>to impose constraints (e.g. >=0, <=1, and monotonicity) as there is
> >>>>>with COBS.  However, since COBS doesn't have the 'deriv =' option, the
> >>>>>only way I can think of doing it with COBS is to evaluate the
> >>>>>derivatives numerically.
> >>>>
> >>>>Numerical estimates of the derivatives of a quadratic should be easy to
> >>>>obtain accurately.  For example, if the quadratic ax^2 + bx + c is
> >>>>defined on [-1, 1], then the derivative 2ax + b, has 2a = f(1) - f(0) +
> >>>>f(-1), and b = (f(1) - f(-1))/2.
> >>>>
> >>>>You should be able to generalize this to the case where the spline is
> >>>>quadratic between knots k1 and k2 pretty easily.
> >>>>
> >>>>Duncan Murdoch
> >>>>
> >>
> >>
> 
>



From davidoff at haas.berkeley.edu  Thu Jul 21 02:50:22 2005
From: davidoff at haas.berkeley.edu (Thomas Davidoff)
Date: Wed, 20 Jul 2005 17:50:22 -0700
Subject: [R] Clustered standard errors in a panel
Message-ID: <D82F4D50-13B7-4B73-8658-1BBA5A53EF9C@haas.berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050720/bbdb4412/attachment.pl

From f.harrell at vanderbilt.edu  Thu Jul 21 03:09:29 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 20 Jul 2005 21:09:29 -0400
Subject: [R] aregImpute in Hmisc
In-Reply-To: <20050720205821.36315.qmail@web31108.mail.mud.yahoo.com>
References: <20050720205821.36315.qmail@web31108.mail.mud.yahoo.com>
Message-ID: <42DEF5C9.708@vanderbilt.edu>

Young Cho wrote:
> Hi,
> 
> I have a dataframe ds1.2 - 503 categorial variables
> and 1 continuous response variables.  I ran aregImpute
> to deal with NA's and got the followig error:
> 
> 
>>fmla = terms( Response ~ . ,data=ds1.2)
>>ds.i = aregImpute(fmla,data=ds1.2)
> 
> Error in matrix(as.double(1), nrow = n, ncol = p,
> dimnames = list(rnam,  : 
>         length of dimnames [2] not equal to array
> extent
> 
> Could you explain what I should do? Thanks a lot.
> 
> Young.

Without a small reproducible example (using for example simulated data) 
I can't solve it.  You may have trouble using aregImpute with that many 
variables.  -Frank Harrell

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From lyhin at netvigator.com  Thu Jul 21 04:07:15 2005
From: lyhin at netvigator.com (Dr L. Y Hin)
Date: Thu, 21 Jul 2005 10:07:15 +0800
Subject: [R] family
Message-ID: <001001c58d98$eae46830$104efea9@yourgk68c57jh8>

Dear all,

I am in the process of migrating an S programme library to R, and it 
involves the family entity.

I have checked ?family but it does not give much detail of its components.
I will be very grateful if anyone can point towards sources/ways to look up 
on this areas
with an aim to find the equivalance of the followings in S:
family()$inverse
family()$deriv
family()$variance
family()$deviance t


Thank you
Lin



From Ivy_Li at smics.com  Thu Jul 21 05:25:04 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Thu, 21 Jul 2005 11:25:04 +0800
Subject: [R] The steps of building library in R 2.1.1
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>

Dear All,

With the warm support of every R expert, I have built my R library successfully. 
Especially thanks: Duncan Murdoch
		    Gabor Grothendieck   
		    Henrik Bengtsson
		    Uwe Ligges
Without your help, I will lower efficiency.
I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.

1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools> 
2. Download the "rw2011.exe"; Install the newest version of R
3. Download the "tools.zip"; Unpack it into c:\cygwin
4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl
5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin
6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin" 
	The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";". 
	Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.

7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
I type in R:
	>f <- function(x,y) x+y
	>g <- function(x,y) x-y
	>d <- data.frame(a=1, b=2)
	>e <- rnorm(1000)
	>package.skeleton(list=c("f","g","d","e"), name="example")
Then modify the 'DESCRIPTION':
	Package: example
	Version: 1.0-1
	Date: 2005-07-09
	Title: My first function
	Author: Ivy <Ivy_Li at smics.com>
	Maintainer: Ivy <Ivy_Li at smics.com>
	Description: simple sum and subtract
	License: GPL version 2 or later
	Depends: R (>= 1.9), stats, graphics, utils
You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference. 

8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
    I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
    However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file

9. In the DOS environment. Into the "D:\>"  Type the following code: 
	cd \Program Files\R\rw2010 
	bin\R CMD INSTALL "/Program Files/R/rw2011/example"
Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.

10.Finally, this package is successfully built up.

	  ---------- Making package example ------------
  	  adding build stamp to DESCRIPTION
	  installing R files
	  installing data files
	  installing man source files
	  installing indices
	  not zipping data
	  installing help
	 >>> Building/Updating help pages for package 'example'
	     Formats: text html latex example chm
	  d                                 text    html    latex   example
	  e                                 text    html    latex   example
	  f                                 text    html    latex   example
	  g                                 text    html    latex   example
	  adding MD5 sums
	
	* DONE (example)

I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.


Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



From klebyn at yahoo.com.br  Thu Jul 21 04:59:13 2005
From: klebyn at yahoo.com.br (klebyn)
Date: Thu, 21 Jul 2005 00:59:13 -0200
Subject: [R] about confounded variances in NESTED and SPLIT PLOT designs
Message-ID: <42DF0F81.9010209@yahoo.com.br>

All R users

We observed that for the NESTED DESIGN there is a junction of the 
variable effects randomic that impede the separation of the values of 
these variance components.

Does this also happen in the SPLIT-PLOT method?

Is there junction between main plot error and replicate error?

There is a junction among the main-plot error and the replicatas that 
would impede the determination of these components separately? Is there 
the same effect for SPD?

How to simulate data with values control for each error contribution?

Thanks a lot

Kleber Borges and Jo??o Bort



From s.su at qut.edu.au  Thu Jul 21 07:01:28 2005
From: s.su at qut.edu.au (Steve Su)
Date: Thu, 21 Jul 2005 15:01:28 +1000
Subject: [R] cut in R
Message-ID: <000701c58db1$410128d0$2032b583@qut.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050721/afcee5f4/attachment.pl

From vito_ricci at yahoo.com  Thu Jul 21 08:26:03 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 21 Jul 2005 08:26:03 +0200 (CEST)
Subject: [R] poisson fit for histogram
Message-ID: <20050721062603.2045.qmail@web41201.mail.yahoo.com>

Hi,

see: 

http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf

Regards,

Vito


Thomas Isenbarger isen at plantpath.wisc.edu 
wrote:
I haven't been an R lister for a bit, but I hope to
enlist someone's  
help here.  I think this is a simple question, so I
hope the answer  
is not much trouble.  Can you please respond directly
to this email  
address in addition to the list (if responding to the
list is  
warranted)?

I have a histogram and I want to see if the data fit a
Poisson  
distribution.  How do I do this?  It is preferable if
it could be  
done without having to install any or many packages.

I use R Version 1.12 (1622) on OS X

Thank-you very much,
Tom Isenbarger


--
Tom Isenbarger PhD
isen at plantpath.wisc.edu
608.265.0850


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From spencer.graves at pdf.com  Thu Jul 21 08:28:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Jul 2005 23:28:13 -0700
Subject: [R] Clustered standard errors in a panel
In-Reply-To: <D82F4D50-13B7-4B73-8658-1BBA5A53EF9C@haas.berkeley.edu>
References: <D82F4D50-13B7-4B73-8658-1BBA5A53EF9C@haas.berkeley.edu>
Message-ID: <42DF407D.3050203@pdf.com>

	  Have you considered "lmer" in library(lme4)?  If you are interested 
in this, you may want to check the article by Doug Bates in the latest R 
news, www.r-project.org -> Documentation:  Newsletter.

	  spencer graves

Thomas Davidoff wrote:

> I want to do the following:
> 
> glm(y ~ x1 + x2 +...)
> within a panel.  Hence y, x1, and x2 all vary at the individual  
> level.  However, there is likely correlation of these variables  
> within an individual, so standard errors need adjustment.
> I do not want to estimate fixed effects, but do want to cluster  
> standard errors at the individual level.
> Is there an automated way to do this?  Nothing in the cluster  
> documentation makes it clear that there is.
> 
> (An alternative is to do this by hand.  In that case, I would need to  
> be able to calculate weighted sums of x1 and x2... at the individual  
> level.  I can do this at the variable level [with lapply,split and  
> unsplit], but would love to be able to do so over the matrix of x's.   
> Of course, doing by hand is less easy than an automated solution if  
> it exists.)
> 
> 
> Thomas Davidoff
> Assistant Professor
> Haas School of Business
> UC Berkeley
> Berkeley, CA 94720
> phone:     (510) 643-1425
> fax:        (510) 643-7357
> davidoff at haas.berkeley.edu
> http://faculty.haas.berkeley.edu/davidoff
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ripley at stats.ox.ac.uk  Thu Jul 21 08:41:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Jul 2005 07:41:33 +0100 (BST)
Subject: [R] family
In-Reply-To: <001001c58d98$eae46830$104efea9@yourgk68c57jh8>
References: <001001c58d98$eae46830$104efea9@yourgk68c57jh8>
Message-ID: <Pine.LNX.4.61.0507210736360.5999@gannet.stats>

You want to aim to write a family for R, not find the equivalents of S 
constructs -- they are different and so exact equivalents do not exist. 
In particular, an R family has several components which an S family does 
not.

There are lots of example families for you to follow (e.g. see ?family and 
the negative binomial families in the MASS package).  Others have found 
reading the sources sufficient to write new families.

On Thu, 21 Jul 2005, Dr L. Y Hin wrote:

> I am in the process of migrating an S programme library to R, and it involves 
> the family entity.
>
> I have checked ?family but it does not give much detail of its components.
> I will be very grateful if anyone can point towards sources/ways to look up 
> on this areas
> with an aim to find the equivalance of the followings in S:
> family()$inverse
> family()$deriv
> family()$variance
> family()$deviance t


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jordi.Molins at drkw.com  Thu Jul 21 09:25:20 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Thu, 21 Jul 2005 09:25:20 +0200
Subject: [R] again, a question between R and C++
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC8BFE17@ibfftce502.de.ad.drkw.net>


Dear R Users,

I want to make a call from R into C++. My inputs are List1, List2, List3,
IntegerID. The amount of elements of the lists and their type depend on
IntegerID. Typical elements of a given list can be vectors, doubles, and
even other lists. I want to return also a list (whose nature will depend
also, possibly, on IntegerID).

What I want to do is to call these 4 inputs from C++ and then use a factory
pattern (depending on IntegerID) that will perform different calculations on
the lists depending on the IntegerID (of course, I could also do this with a
simple switch statement).

I have been reading the documentation, especially the one regarding .Call
and .External, and it seems that my algorithm could be implemented, but the
examples I have seen up to now are such that what occupies the place of my
lists are just vectors (like in convolve4 example).

Is there an example where I could see how instead of a vector, a set of
lists (with an unkown number of arguments, as well as unkown types) are used
as inputs? I guess that the ideal would be that in the equivalent of the
convolve4 function, my args would be "variant" type of lists, and then,
after the factory pattern is called, and the correct class is registered
(via IntegerID), this variant type is really "decomposed" into the
individual types that compose the list (ie, vectors, doubles, ...). Of
course, in the factory there should be as many "decomposing" algorithms as
IntegerIDs, each creating a particular decomposition.

Also, how returning a list (whose nature will depend also, possibly, on
IntegerID) should be handled?

Thank you in advance

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From p.dalgaard at biostat.ku.dk  Thu Jul 21 10:04:57 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jul 2005 10:04:57 +0200
Subject: [R] cut in R
In-Reply-To: <000701c58db1$410128d0$2032b583@qut.edu.au>
References: <000701c58db1$410128d0$2032b583@qut.edu.au>
Message-ID: <x2ackg7ffa.fsf@turmalin.kubism.ku.dk>

"Steve Su" <s.su at qut.edu.au> writes:

> Dear All, 
> 
> I wonder whether it is still valid to use the following R code for cut. All I have done is changed:
> 
>    if (is.na(breaks) | breaks < 2) 
> 
> to:
>  
>    if (is.na(breaks) | breaks < 1)
> 
> so that it covers interval of 1?  
> 
> It seems okay for my purposes but I am not sure why R specifically does not allow break<2 to happen.
> 
> Steve.

What do you need it for? It gives you a factor with only one group, so
I suppose that the idea is that this is more likely to be due to a
programming error.

(However, I spot a bit of a bug in that we don't set
include.lowest=TRUE when using breaks as a number:

> x <- round(rnorm(20),2)
> x
 [1]  0.66 -2.22 -0.70 -1.68  0.38 -0.23 -0.43 -0.72  0.30 -0.22 -1.36
> 0.60
[13]  0.44 -0.40 -0.61  1.08 -0.41 -0.02 -1.41 -0.49
> cut(x,breaks=3)
 [1] (-0.0189,1.08]  (-2.22,-1.12]   (-1.12,-0.0189] (-2.22,-1.12]
 [5] (-0.0189,1.08]  (-1.12,-0.0189] (-1.12,-0.0189] (-1.12,-0.0189]
 [9] (-0.0189,1.08]  (-1.12,-0.0189] (-2.22,-1.12]   (-0.0189,1.08]
[13] (-0.0189,1.08]  (-1.12,-0.0189] (-1.12,-0.0189] (-0.0189,1.08]
[17] (-1.12,-0.0189] (-1.12,-0.0189] (-2.22,-1.12]   (-1.12,-0.0189]
Levels: (-2.22,-1.12] (-1.12,-0.0189] (-0.0189,1.08]

Notice how -2.22 appears to be inside the interval (-2.22,-1.12] .)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From uctpgde at ucl.ac.uk  Thu Jul 21 12:07:27 2005
From: uctpgde at ucl.ac.uk (Giacomo De Giorgi )
Date: Thu, 21 Jul 2005 11:07:27 +0100
Subject: [R] dpill in KernSmooth package
Message-ID: <000b01c58ddc$000164a0$e05a2880@economics.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050721/f662ad1b/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Thu Jul 21 12:20:24 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 21 Jul 2005 11:20:24 +0100 (BST)
Subject: [R] Question about 'text' (add lm summary to a plot)
Message-ID: <Pine.LNX.4.21.0507211112480.17314-100000@mail.mrc-dunn.cam.ac.uk>


I would like to annotate my plot with a little box containing the slope,
intercept and R^2 of a lm on the data.

I would like it to look like...

 +----------------------------+
 | Slope     :   3.45 +- 0.34 |
 | Intercept : -10.43 +- 1.42 |
 | R^2       :   0.78         |
 +----------------------------+

However I can't make anything this neat, and I can't find out how to
combine this with symbols for R^2 / +- (plus minus).

Below is my best attempt (which is franky quite pour). Can anyone
improve on the below?

Specifically, 

aligned text and numbers, 
aligned decimal places, 
symbol for R^2 in the text (expression(R^2) seems to fail with
'paste') and +- 



Cheers,
Dan.


dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)

abline(coef(dat.lm),lty=2,lwd=1.5)


dat.lm.sum <- summary(dat.lm)
dat.lm.sum

attributes(dat.lm.sum)

my.text.1 <-
  paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
        "+/-",          round(dat.lm.sum$coefficients[4],2))

my.text.2 <-
  paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
        "+/-",          round(dat.lm.sum$coefficients[3],2))

my.text.3 <-
  paste("R^2 : ",       round(dat.lm.sum$r.squared,2))

my.text.1
my.text.2
my.text.3


## Add legend
text(x=3,
     y=300,
     paste(my.text.1,
           my.text.2,
           my.text.3,
           sep="\n"),
     adj=c(0,0),
     cex=1)



From danbebber at yahoo.co.uk  Thu Jul 21 13:05:45 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Thu, 21 Jul 2005 12:05:45 +0100 (BST)
Subject: [R] bubble.plot() - standardize size of unit circle
Message-ID: <20050721110545.67741.qmail@web26301.mail.ukl.yahoo.com>

Hello,

I wrote a wrapper for symbols() that produces a
bivariate bubble plot, for use when plot(x,y) hides
multiple occurrences of the same x,y combination (e.g.
if x,y are integers).
Circle area ~ counts per bin, and circle size is
controlled by 'scale'.
Question: how can I automatically make the smallest
circle the same size as a standard plot character,
rather than having to approximate it using 'scale'?

#Function:
bubble.plot<-function(x,y,scale=0.1,xlab=substitute(x),ylab=substitute(y),...){
z<-table(x,y)
xx<-rep(as.numeric(rownames(z)),ncol(z))
yy<-sort(rep(as.numeric(colnames(z)),nrow(z)))
id<-which(z!=0)
symbols(xx[id],yy[id],inches=F,circles=sqrt(z[id])*scale,xlab=xlab,ylab=ylab,...)}

#Example:
x<-rpois(100,3)
y<-x+rpois(100,2)
bubble.plot(x,y)


		
___________________________________________________________ 
How much free photo storage do you get? Store your holiday



From liuwensui at gmail.com  Thu Jul 21 14:07:43 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 21 Jul 2005 08:07:43 -0400
Subject: [R] Is it possible to create highly customized report in *.xls
	format by using R/S+?
In-Reply-To: <971536df05072016271c69539d@mail.gmail.com>
References: <s2de0ea8.033@lp-msg1.co.ihc.com>
	<1115a2b005072007554e1ba1f1@mail.gmail.com>
	<971536df05072016271c69539d@mail.gmail.com>
Message-ID: <1115a2b0050721050716e4d0d7@mail.gmail.com>

Thank you all for the replies. It is very eye-opening for me.

I probably need something like RDCOMClient. I've tried it last night.
Very nice package!!!

On 7/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here is an example where R is the client and Excel is the server
> so that R is issuing commands to Excel.  This example uses the
> RDCOMClient package from www.omegahat.org:
> 
>         library(RDCOMClient)
>         xl <- COMCreate("Excel.Application")  # starts up Excel
>         xl[["Visible"]] <- TRUE                       # Excel becomes visible
>         wkbk <- xl$Workbooks()$Add()          # new workbook
> 
>         # set some cells
> 
>         sh <- xl$ActiveSheet()
> 
>         x12 <- sh$Cells(1,2)
>         x12[["Value"]] <- 123
> 
>         x22 <- sh$Cells(2,2)
>         x22[["Value"]] <- 100
> 
>         x31 <- sh$Cells(3,1)
>         x31[["Value"]] <- "Total"
> 
>         B3R <- sh$Range("B3")
>         B3R[["Formula"]] <- "=Sum(R1C2:R2C2)"
>         B3R[["NumberFormat"]] <- "_($* #,##0.00_)"
>         B3RF <- B3R$Font()
>         B3RF[["Bold"]] <- TRUE
> 
> 
>         # save and exit
>         wkbk$SaveAs("\\test.xls")
>         xl$Quit()
> 
> Code using the rcom package at (second link is mailing list):
> 
>         http://sunsite.univie.ac.at/rcom/download/
>         http://mailman.csd.univie.ac.at/pipermail/rcom-l/
> 
> would be nearly identical once the upcoming version of rcom comes out.
> rcom and omegahat both provide the possibility of having
> Excel as the client and R as the server; however, in that setup the
> user would have to have R running whereas in the above setup only you do.
> 
> On 7/20/05, Wensui Liu <liuwensui at gmail.com> wrote:
> > I appreciate your reply and understand your point completely. But at
> > times we can't change the rule, the only choice is to follow the rule.
> > Most deliverables in my work are in excel format.
> >
> > On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> > > See:
> > >
> > > http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> > > and
> > > http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> > >
> > > Greg Snow, Ph.D.
> > > Statistical Data Center, LDS Hospital
> > > Intermountain Health Care
> > > greg.snow at ihc.com
> > > (801) 408-8111
> > >
> > > >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> > > I remember in one slide of Prof. Ripley's presentation overhead, he
> > > said the most popular data analysis software is excel.
> > >
> > > So is there any resource or tutorial on this topic?
> > >
> > > Thank you so much!
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From whit at twinfieldscapital.com  Thu Jul 21 14:25:25 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Thu, 21 Jul 2005 08:25:25 -0400
Subject: [R] again, a question between R and C++
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE2C8182@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

Jordi,

The place to ask this question is probably the r-devel list; it's a
little too heavy for r-help.

This is fairly easy to do using the .Call interface.

Have a look at lapply2 in the Writing R Extensions manual.
http://cran.r-project.org/doc/manuals/R-exts.html#Evaluating-R-expressio
ns-from-C

or just follow this short example.


write a function in C++ as follows:
 
SEXP myFunc(SEXP list1, SEXP list2, SEXP list3, SEXP list4, SEXP
intID_SEXP) {

	// obtain the list length as follows:
	int list1_len = length(list1);

	// to access your integer (I assume it's a scalar, not a vector)
	// you need to grab the first element of this integer vector
	int INTEGER(intID_SEXP)[0]

	// you will want to add some checks to make sure the arguments
are of the right type
	
	...
	...

	SEXP ans = (whatever)
	return ans;
}

you can call it in R as follows:

.Call("myFunc", list1, list2, list3, list4, intID)



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Molins, Jordi
Sent: Thursday, July 21, 2005 3:25 AM
To: r-help at stat.math.ethz.ch
Cc: Jordi Molins
Subject: [R] again, a question between R and C++


Dear R Users,

I want to make a call from R into C++. My inputs are List1, List2,
List3, IntegerID. The amount of elements of the lists and their type
depend on IntegerID. Typical elements of a given list can be vectors,
doubles, and even other lists. I want to return also a list (whose
nature will depend also, possibly, on IntegerID).

What I want to do is to call these 4 inputs from C++ and then use a
factory pattern (depending on IntegerID) that will perform different
calculations on the lists depending on the IntegerID (of course, I could
also do this with a simple switch statement).

I have been reading the documentation, especially the one regarding
.Call and .External, and it seems that my algorithm could be
implemented, but the examples I have seen up to now are such that what
occupies the place of my lists are just vectors (like in convolve4
example).

Is there an example where I could see how instead of a vector, a set of
lists (with an unkown number of arguments, as well as unkown types) are
used as inputs? I guess that the ideal would be that in the equivalent
of the
convolve4 function, my args would be "variant" type of lists, and then,
after the factory pattern is called, and the correct class is registered
(via IntegerID), this variant type is really "decomposed" into the
individual types that compose the list (ie, vectors, doubles, ...). Of
course, in the factory there should be as many "decomposing" algorithms
as IntegerIDs, each creating a particular decomposition.

Also, how returning a list (whose nature will depend also, possibly, on
IntegerID) should be handled?

Thank you in advance

Jordi



------------------------------------------------------------------------
--------
The information contained herein is confidential and is\ int...{{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Jul 21 14:57:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 14:57:45 +0200
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
Message-ID: <42DF9BC9.9050508@statistik.uni-dortmund.de>

Ivy_Li wrote:

> Dear All,
> 
> With the warm support of every R expert, I have built my R library successfully. 
> Especially thanks: Duncan Murdoch
> 		    Gabor Grothendieck   
> 		    Henrik Bengtsson
> 		    Uwe Ligges

You are welcome.


The following is intended for the records in the archive in order to
protect readers.


> Without your help, I will lower efficiency.
> I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.
> 
> 1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools> 

Do you mean http://www.murdoch-sutherland.com/Rtools/ ?

> 2. Download the "rw2011.exe"; Install the newest version of R
> 3. Download the "tools.zip"; Unpack it into c:\cygwin

Not required to call it "cygwin" - also a bit misleading...

> 4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl

Why in C:\Perl ?

> 5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin

Why in c:\mingwin ?


> 6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin" 
> 	The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";". 
> 	Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.


OK, this (1-6) is all described in the R Administration and Installation
manual, hence I do not see why we have to repeat it here.


> 7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
> I type in R:
> 	>f <- function(x,y) x+y
> 	>g <- function(x,y) x-y
> 	>d <- data.frame(a=1, b=2)
> 	>e <- rnorm(1000)
> 	>package.skeleton(list=c("f","g","d","e"), name="example")
> Then modify the 'DESCRIPTION':
> 	Package: example
> 	Version: 1.0-1
> 	Date: 2005-07-09
> 	Title: My first function
> 	Author: Ivy <Ivy_Li at smics.com>
> 	Maintainer: Ivy <Ivy_Li at smics.com>
> 	Description: simple sum and subtract
> 	License: GPL version 2 or later
> 	Depends: R (>= 1.9), stats, graphics, utils
> You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference. 


This is described in Writing R Extension and is not related to the setup
of you system in 1-6.


> 
> 8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
>     I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
>     However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file


This is described in the R Administration and Installation manual
and I do not see why we should put the html compiler to the other tools.


> 9. In the DOS environment. Into the "D:\>"  Type the following code: 

There is no DOS environment in Windows NT based operating systems.


> 	cd \Program Files\R\rw2010 
> 	bin\R CMD INSTALL "/Program Files/R/rw2011/example"

I do not see why anybody would like to contaminate the binary
installation of R with some development source packages.
I'd rather use a separate directory.

I think reading the two mentioned manuals shoul be sufficient. You have
not added relevant information. By adding irrelevant information and
omitting some relevant information, I guess we got something that is
misleading if the reader does NOT read the manuals as well.

Best,
Uwe Ligges


> Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.
> 
> 10.Finally, this package is successfully built up.
> 
> 	  ---------- Making package example ------------
>   	  adding build stamp to DESCRIPTION
> 	  installing R files
> 	  installing data files
> 	  installing man source files
> 	  installing indices
> 	  not zipping data
> 	  installing help
> 	 >>> Building/Updating help pages for package 'example'
> 	     Formats: text html latex example chm
> 	  d                                 text    html    latex   example
> 	  e                                 text    html    latex   example
> 	  f                                 text    html    latex   example
> 	  g                                 text    html    latex   example
> 	  adding MD5 sums
> 	
> 	* DONE (example)
> 
> I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.
> 
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kgleditsch at ucsd.edu  Thu Jul 21 15:11:36 2005
From: kgleditsch at ucsd.edu (Kristian Skrede Gleditsch)
Date: Thu, 21 Jul 2005 14:11:36 +0100
Subject: [R] Problem with read.table()
Message-ID: <42DF9F08.3080407@ucsd.edu>

Dear all,

I have encountered a strange problem with read.table(). When I try to 
read a tab delimited file I get an error message for line 260 not being 
equal to 14 (see below).

Using count.fields() suggests that a number of lines have length not 
equal to 14, but not 260.

Looking at the actual file, however, I cannot see anything wrong with 
any lines. They all seem to have length 14, there are no double tabs 
etc., and the file reads correctly in other programs. Does anyone have 
any suggestions as to what this might stem from?

I have placed a copy of the file at 
http://dss.ucsd.edu/~kgledits/archigos_v.1.9.asc

regards,
Kristian Skrede Gleditsch


 > archigos1.9 <- read.table("c:/work/work12/archigos/archigos_v.1.9.asc",
+     sep="\t",header=T,as.is=T,row.names=NULL)
Error in scan(file = file, what = what, sep = sep, quote = quote, dec = 
dec,  :
         line 260 did not have 14 elements
 > a <- count.fields("c:/work/work12/archigos/archigos_v.1.9.asc",sep="\t")
 > a <- data.frame(c(1:length(a)),a)
 > a[a[,2]!=14,]
      c.1.length.a..  a
150             150 10
313             313 10
424             424 10
1189           1189  5
1510           1510 10
1514           1514 10
1590           1590  5
1600           1600 10
1612           1612 10
1618           1618 10
1619           1619 10
1709           1709 10
1722           1722 10
1981           1981 10
1985           1985 10
2112           2112 10
2178           2178 10
2208           2208 10
2224           2224 10
2530           2530  5
2536           2536  5
2573           2573  5
2928           2928  5
-- 
Kristian Skrede Gleditsch
Department of Political Science, UCSD
(On leave, University of Essex, 2005-6)
Tel: +44 1206 872499, Fax: +44 1206 873234
Email: kgleditsch at ucsd.edu or ksg at essex.ac.uk
http://weber.ucsd.edu/~kgledits/



From s.j.baxter at reading.ac.uk  Thu Jul 21 15:15:58 2005
From: s.j.baxter at reading.ac.uk (Sam Baxter)
Date: Thu, 21 Jul 2005 14:15:58 +0100
Subject: [R] R graphics
Message-ID: <42DFA00E.2090506@rdg.ac.uk>


Hi

I am trying to set up 16 graphs on one graphics page in R. I have used 
the mfrow=c(4,4) command. However I get a lot of white space between 
each graph. Does anyone know how I can reduce this?

Thanks

Sam



From br44114 at gmail.com  Thu Jul 21 15:15:53 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 21 Jul 2005 09:15:53 -0400
Subject: [R] Is it possible to create highly customized report in *.xls
	format by using R/S+?
Message-ID: <8d5a363505072106154ed3a82f@mail.gmail.com>

So your conclusion is that the only choice is to make mistakes and get
in trouble. (That's what Excel excels at.)

Two options I haven't seen mentioned are:
1. Create your deliverables in HTML format, and change the extension
from .htm to .xls; Excel will import them automatically. The way the
file looks in Excel is determined by .CSS settings (I've seen this
happen) and I presume HTML tags.
2. For the real spreadsheet thing, switch to OpenOffice.org. Their
format is XML compressed with ZIP which you can easily work with since
the format specifications are not proprietary. See
http://xml.openoffice.org/ for details.



> -----Original Message-----
> From: Wensui Liu [mailto:liuwensui at gmail.com] 
> Sent: Wednesday, July 20, 2005 10:56 AM
> To: Greg Snow
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Is it possible to create highly customized 
> report in *.xls format by using R/S+?
> 
> 
> I appreciate your reply and understand your point completely. But at
> times we can't change the rule, the only choice is to follow the rule.
> Most deliverables in my work are in excel format.
> 
> On 7/20/05, Greg Snow <greg.snow at ihc.com> wrote:
> > See:
> > 
> > http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> > and
> > http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> > 
> > Greg Snow, Ph.D.
> > Statistical Data Center, LDS Hospital
> > Intermountain Health Care
> > greg.snow at ihc.com
> > (801) 408-8111
> > 
> > >>> Wensui Liu <liuwensui at gmail.com> 07/19/05 03:22PM >>>
> > I remember in one slide of Prof. Ripley's presentation overhead, he
> > said the most popular data analysis software is excel.
> > 
> > So is there any resource or tutorial on this topic?
> > 
> > Thank you so much!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> -- 
> WenSui Liu, MS MA
> Senior Decision Support Analyst
> Division of Health Policy and Clinical Effectiveness
> Cincinnati Children Hospital Medical Center
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From buser at stat.math.ethz.ch  Thu Jul 21 15:18:11 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 21 Jul 2005 15:18:11 +0200
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507211112480.17314-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0507211112480.17314-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <17119.41107.864470.790867@stat.math.ethz.ch>

Dear Dan

I can only help you with your third problem, expression and
paste. You can use:

plot(1:5,1:5, type = "n")
text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)

I do not have an elegant solution for the alignment.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Dan Bolser writes:
 > 
 > I would like to annotate my plot with a little box containing the slope,
 > intercept and R^2 of a lm on the data.
 > 
 > I would like it to look like...
 > 
 >  +----------------------------+
 >  | Slope     :   3.45 +- 0.34 |
 >  | Intercept : -10.43 +- 1.42 |
 >  | R^2       :   0.78         |
 >  +----------------------------+
 > 
 > However I can't make anything this neat, and I can't find out how to
 > combine this with symbols for R^2 / +- (plus minus).
 > 
 > Below is my best attempt (which is franky quite pour). Can anyone
 > improve on the below?
 > 
 > Specifically, 
 > 
 > aligned text and numbers, 
 > aligned decimal places, 
 > symbol for R^2 in the text (expression(R^2) seems to fail with
 > 'paste') and +- 
 > 
 > 
 > 
 > Cheers,
 > Dan.
 > 
 > 
 > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
 > 
 > abline(coef(dat.lm),lty=2,lwd=1.5)
 > 
 > 
 > dat.lm.sum <- summary(dat.lm)
 > dat.lm.sum
 > 
 > attributes(dat.lm.sum)
 > 
 > my.text.1 <-
 >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
 >         "+/-",          round(dat.lm.sum$coefficients[4],2))
 > 
 > my.text.2 <-
 >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
 >         "+/-",          round(dat.lm.sum$coefficients[3],2))
 > 
 > my.text.3 <-
 >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
 > 
 > my.text.1
 > my.text.2
 > my.text.3
 > 
 > 
 > ## Add legend
 > text(x=3,
 >      y=300,
 >      paste(my.text.1,
 >            my.text.2,
 >            my.text.3,
 >            sep="\n"),
 >      adj=c(0,0),
 >      cex=1)
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Arne.Muller at sanofi-aventis.com  Thu Jul 21 15:20:50 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Thu, 21 Jul 2005 15:20:50 +0200
Subject: [R] RandomForest question
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3D4@CRBSMXSUSR04>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050721/1a7cd9df/attachment.pl

From jjmichael at comcast.net  Thu Jul 21 15:25:43 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Thu, 21 Jul 2005 07:25:43 -0600
Subject: [R] heatmap color distribution
Message-ID: <BD4DDF3C-21A3-4CE8-8585-7D37AC3566DC@comcast.net>

Hi all,

I've got a set of gene expression data, and I'm plotting several  
heatmaps for subsets of the whole set.  I'd like the heatmaps to have  
the same color distribution, so that comparisons may be made  
(roughly) across heatmaps; this would require that the color  
distribution and distance functions be based on the entire dataset,  
rather than on individual subsets.  Does anyone know how to do this?

Thanks in advance,

Jake



From blindglobe at gmail.com  Thu Jul 21 15:35:47 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 21 Jul 2005 15:35:47 +0200
Subject: [R] Chemoinformatic people
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D743B4@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D743B4@euromail.euroscreen.be>
Message-ID: <1abe3fa905072106353f136158@mail.gmail.com>

Just with R, or via another tool integrating R, such as Pipeline Pilot?

best,
-tony

On 7/20/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> Dear colleague,
> Just an e-mail to know if they are people working in the field of chemoinformatic that are using R in their work. If yes I was wondering if we couldn't exchange tips and tricks about the use of R in this area ?
> Best regards
> Fred Ooms
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From j.van_den_hoff at fz-rossendorf.de  Thu Jul 21 15:36:35 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Thu, 21 Jul 2005 15:36:35 +0200
Subject: [R] R graphics
In-Reply-To: <42DFA00E.2090506@rdg.ac.uk>
References: <42DFA00E.2090506@rdg.ac.uk>
Message-ID: <42DFA4E3.5000509@fz-rossendorf.de>

Sam Baxter wrote:
> Hi
> 
> I am trying to set up 16 graphs on one graphics page in R. I have used 
> the mfrow=c(4,4) command. However I get a lot of white space between 
> each graph. Does anyone know how I can reduce this?
> 
> Thanks
> 
> Sam
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


?layout
as an alternative to par(mfrow) might be helpful anyway


too large margins:

?par

reduce value of "mar", for instance



From ggrothendieck at gmail.com  Thu Jul 21 15:43:29 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2005 09:43:29 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DF9BC9.9050508@statistik.uni-dortmund.de>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
Message-ID: <971536df0507210643438a80ef@mail.gmail.com>

I think you have been using R too long.  Something like
this is very much needed.  There are two problems:

1. the process itself is too complex (need to get rid of perl,
    integrate package development tools with package installation 
   procedure [it should be as easy as downloading a package], 
   remove necessity to set or modify any environment variables
    including the path variables).

2. there is too much material to absorb just to create a package.
    The manuals are insufficient.

A step-by-step simplification is very much needed.  Its no
coincidence that there are a number of such descriptions on
the net (google for 'making creating R package') since I would 
guess that just about everyone has significant problems in creating 
their first package on Windows.


On 7/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Ivy_Li wrote:
> 
> > Dear All,
> >
> > With the warm support of every R expert, I have built my R library successfully.
> > Especially thanks: Duncan Murdoch
> >                   Gabor Grothendieck
> >                   Henrik Bengtsson
> >                   Uwe Ligges
> 
> You are welcome.
> 
> 
> The following is intended for the records in the archive in order to
> protect readers.
> 
> 
> > Without your help, I will lower efficiency.
> > I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.
> >
> > 1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> 
> Do you mean http://www.murdoch-sutherland.com/Rtools/ ?
> 
> > 2. Download the "rw2011.exe"; Install the newest version of R
> > 3. Download the "tools.zip"; Unpack it into c:\cygwin
> 
> Not required to call it "cygwin" - also a bit misleading...
> 
> > 4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl
> 
> Why in C:\Perl ?
> 
> > 5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin
> 
> Why in c:\mingwin ?
> 
> 
> > 6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin"
> >       The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";".
> >       Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.
> 
> 
> OK, this (1-6) is all described in the R Administration and Installation
> manual, hence I do not see why we have to repeat it here.
> 
> 
> > 7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
> > I type in R:
> >       >f <- function(x,y) x+y
> >       >g <- function(x,y) x-y
> >       >d <- data.frame(a=1, b=2)
> >       >e <- rnorm(1000)
> >       >package.skeleton(list=c("f","g","d","e"), name="example")
> > Then modify the 'DESCRIPTION':
> >       Package: example
> >       Version: 1.0-1
> >       Date: 2005-07-09
> >       Title: My first function
> >       Author: Ivy <Ivy_Li at smics.com>
> >       Maintainer: Ivy <Ivy_Li at smics.com>
> >       Description: simple sum and subtract
> >       License: GPL version 2 or later
> >       Depends: R (>= 1.9), stats, graphics, utils
> > You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference.
> 
> 
> This is described in Writing R Extension and is not related to the setup
> of you system in 1-6.
> 
> 
> >
> > 8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
> >     I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
> >     However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file
> 
> 
> This is described in the R Administration and Installation manual
> and I do not see why we should put the html compiler to the other tools.
> 
> 
> > 9. In the DOS environment. Into the "D:\>"  Type the following code:
> 
> There is no DOS environment in Windows NT based operating systems.
> 
> 
> >       cd \Program Files\R\rw2010
> >       bin\R CMD INSTALL "/Program Files/R/rw2011/example"
> 
> I do not see why anybody would like to contaminate the binary
> installation of R with some development source packages.
> I'd rather use a separate directory.
> 
> I think reading the two mentioned manuals shoul be sufficient. You have
> not added relevant information. By adding irrelevant information and
> omitting some relevant information, I guess we got something that is
> misleading if the reader does NOT read the manuals as well.
> 
> Best,
> Uwe Ligges
> 
> 
> > Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.
> >
> > 10.Finally, this package is successfully built up.
> >
> >         ---------- Making package example ------------
> >         adding build stamp to DESCRIPTION
> >         installing R files
> >         installing data files
> >         installing man source files
> >         installing indices
> >         not zipping data
> >         installing help
> >        >>> Building/Updating help pages for package 'example'
> >            Formats: text html latex example chm
> >         d                                 text    html    latex   example
> >         e                                 text    html    latex   example
> >         f                                 text    html    latex   example
> >         g                                 text    html    latex   example
> >         adding MD5 sums
> >
> >       * DONE (example)
> >
> > I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.
> >
> >
> > Best Regards!
> > Ivy Li
> > YMS in Production & Testing
> > Semiconductor Manufactory International(ShangHai) Corporation
> > #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> > Tel: 021-5080-2000 *11754
> > Email: Ivy_Li at smics.com
> >
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From sundar.dorai-raj at pdf.com  Thu Jul 21 15:46:21 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 21 Jul 2005 08:46:21 -0500
Subject: [R] R graphics
In-Reply-To: <42DFA00E.2090506@rdg.ac.uk>
References: <42DFA00E.2090506@rdg.ac.uk>
Message-ID: <42DFA72D.3000304@pdf.com>



Sam Baxter wrote:
> Hi
> 
> I am trying to set up 16 graphs on one graphics page in R. I have used 
> the mfrow=c(4,4) command. However I get a lot of white space between 
> each graph. Does anyone know how I can reduce this?
> 
> Thanks
> 
> Sam
> 

Two options:

1. play around with the `mar' parameter in ?par.

2. (Preferred) Use the lattice package. See, for example:

library(lattice)
trellis.device(theme = col.whitebg())
z <- expand.grid(x = 1:10, y = 1:10, g = LETTERS[1:16])
xyplot(y ~ x | g, z)


HTH,

--sundar



From fooms at euroscreen.com  Thu Jul 21 15:50:30 2005
From: fooms at euroscreen.com (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Thu, 21 Jul 2005 15:50:30 +0200
Subject: [R] Chemoinformatic people
Message-ID: <5198ADA420721246BC35BFA666E24F16D743BA@euromail.euroscreen.be>

I am looking for both.
Fred

-----Original Message-----
From: A.J. Rossini [mailto:blindglobe at gmail.com] 
Sent: Thursday, July 21, 2005 3:36 PM
To: Fr??d??ric Ooms
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Chemoinformatic people


Just with R, or via another tool integrating R, such as Pipeline Pilot?

best,
-tony

On 7/20/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> Dear colleague,
> Just an e-mail to know if they are people working in the field of 
> chemoinformatic that are using R in their work. If yes I was wondering 
> if we couldn't exchange tips and tricks about the use of R in this 
> area ? Best regards Fred Ooms
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From danbebber at yahoo.co.uk  Thu Jul 21 15:59:16 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Thu, 21 Jul 2005 14:59:16 +0100 (BST)
Subject: [R] bubble.plot() - standardize size of unit circle
In-Reply-To: <42E02E1F.6080607@ozemail.com.au>
Message-ID: <20050721135916.77325.qmail@web26310.mail.ukl.yahoo.com>

Thanks- 'sizeplot' didn't come up in any of my
searches.

Dan


--- Jim Lemon <bitwrit at ozemail.com.au> wrote:

> Dan Bebber wrote:
> > Hello,
> > 
> > I wrote a wrapper for symbols() that produces a
> > bivariate bubble plot, for use when plot(x,y)
> hides
> > multiple occurrences of the same x,y combination
> (e.g.
> > if x,y are integers).
> > Circle area ~ counts per bin, and circle size is
> > controlled by 'scale'.
> > Question: how can I automatically make the
> smallest
> > circle the same size as a standard plot character,
> > rather than having to approximate it using
> 'scale'?
> > 
> Ben Bolker's "sizeplot" in the plotrix package does
> this using the 
> standard plotting symbol 1.
> 
> Jim
> 



		
___________________________________________________________ 
How much free photo storage do you get? Store your holiday



From ripley at stats.ox.ac.uk  Thu Jul 21 15:59:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Jul 2005 14:59:19 +0100 (BST)
Subject: [R] Problem with read.table()
In-Reply-To: <42DF9F08.3080407@ucsd.edu>
References: <42DF9F08.3080407@ucsd.edu>
Message-ID: <Pine.LNX.4.61.0507211443580.28398@gannet.stats>

On Thu, 21 Jul 2005, Kristian Skrede Gleditsch wrote:

> Dear all,
>
> I have encountered a strange problem with read.table().

Most `strange problems' are user error, so please try not to blame your 
tools.

> When I try to
> read a tab delimited file I get an error message for line 260 not being
> equal to 14 (see below).

Yes, but not line 260 in that file, but line 260 as read by scan().

Think about quotes ... it works for me with quote="", and the quote on ca 
line 150 is causing you to get some very large fields with embedded new 
lines and tabs.

BTW, there is a 'R Data Import/Export' manual which goes through 
step-by-step the assumptions you make when using read.table with various 
options.  Do read it now.


> Using count.fields() suggests that a number of lines have length not
> equal to 14, but not 260.
>
> Looking at the actual file, however, I cannot see anything wrong with
> any lines. They all seem to have length 14, there are no double tabs
> etc., and the file reads correctly in other programs. Does anyone have
> any suggestions as to what this might stem from?
>
> I have placed a copy of the file at
> http://dss.ucsd.edu/~kgledits/archigos_v.1.9.asc
>
> regards,
> Kristian Skrede Gleditsch
>
>
> > archigos1.9 <- read.table("c:/work/work12/archigos/archigos_v.1.9.asc",
> +     sep="\t",header=T,as.is=T,row.names=NULL)
> Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
> dec,  :
>         line 260 did not have 14 elements
> > a <- count.fields("c:/work/work12/archigos/archigos_v.1.9.asc",sep="\t")
> > a <- data.frame(c(1:length(a)),a)
> > a[a[,2]!=14,]
>      c.1.length.a..  a
> 150             150 10
> 313             313 10
> 424             424 10
> 1189           1189  5
> 1510           1510 10
> 1514           1514 10
> 1590           1590  5
> 1600           1600 10
> 1612           1612 10
> 1618           1618 10
> 1619           1619 10
> 1709           1709 10
> 1722           1722 10
> 1981           1981 10
> 1985           1985 10
> 2112           2112 10
> 2178           2178 10
> 2208           2208 10
> 2224           2224 10
> 2530           2530  5
> 2536           2536  5
> 2573           2573  5
> 2928           2928  5
> -- 
> Kristian Skrede Gleditsch
> Department of Political Science, UCSD
> (On leave, University of Essex, 2005-6)
> Tel: +44 1206 872499, Fax: +44 1206 873234
> Email: kgleditsch at ucsd.edu or ksg at essex.ac.uk
> http://weber.ucsd.edu/~kgledits/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Jul 21 16:07:01 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 21 Jul 2005 07:07:01 -0700 (PDT)
Subject: [R] Clustered standard errors in a panel
In-Reply-To: <42DF407D.3050203@pdf.com>
References: <D82F4D50-13B7-4B73-8658-1BBA5A53EF9C@haas.berkeley.edu>
	<42DF407D.3050203@pdf.com>
Message-ID: <Pine.A41.4.61b.0507210659160.91454@homer11.u.washington.edu>


No, he wants to fit a glm() and get the right standard errors.  For linear 
models the best way to do this is to model some random effects, but doing 
so in glm changes the meanings of the parameters.  To estimate the same 
parameters you want to use the sandwich standard errors variously 
attributed to Huber and White.

The Design package has robcov() to do this, and there is also code at
http://faculty.washington.edu/tlumley/data2001/sandwich.R

 	-thomas


On Wed, 20 Jul 2005, Spencer Graves wrote:

> 	  Have you considered "lmer" in library(lme4)?  If you are interested
> in this, you may want to check the article by Doug Bates in the latest R
> news, www.r-project.org -> Documentation:  Newsletter.
>
> 	  spencer graves
>
> Thomas Davidoff wrote:
>
>> I want to do the following:
>>
>> glm(y ~ x1 + x2 +...)
>> within a panel.  Hence y, x1, and x2 all vary at the individual
>> level.  However, there is likely correlation of these variables
>> within an individual, so standard errors need adjustment.
>> I do not want to estimate fixed effects, but do want to cluster
>> standard errors at the individual level.
>> Is there an automated way to do this?  Nothing in the cluster
>> documentation makes it clear that there is.
>>
>> (An alternative is to do this by hand.  In that case, I would need to
>> be able to calculate weighted sums of x1 and x2... at the individual
>> level.  I can do this at the variable level [with lapply,split and
>> unsplit], but would love to be able to do so over the matrix of x's.
>> Of course, doing by hand is less easy than an automated solution if
>> it exists.)
>>
>>
>> Thomas Davidoff
>> Assistant Professor
>> Haas School of Business
>> UC Berkeley
>> Berkeley, CA 94720
>> phone:     (510) 643-1425
>> fax:        (510) 643-7357
>> davidoff at haas.berkeley.edu
>> http://faculty.haas.berkeley.edu/davidoff
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ozaksoy at lamar.colostate.edu  Thu Jul 21 16:07:29 2005
From: ozaksoy at lamar.colostate.edu (ozaksoy)
Date: Thu, 21 Jul 2005 08:07:29 -0600
Subject: [R] cutomized link function in R
Message-ID: <42E3FDBC@webmail.colostate.edu>

Hello!
I am trying to run my S+ code in R (version 2.1.0). I've created a customized 
link function, namely my.binomial where parameter theta has to be
given. I'm considering theta to be .05. Unfortunately, R is giving an
error (I had used MASS in adjusting the S+ code to R). I would very
much appreciate it if you could help me in finding the correction
needed in the code. My S+ code I'm trying to adjust to R is:
g<-glm(y~diffwhale+tdelta:diffwhale+tdelta2:diffwhale,
data=ind, family=my.binomial(theta=.05))
Thanks,
Isin



From helprhelp at gmail.com  Thu Jul 21 16:10:10 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 21 Jul 2005 09:10:10 -0500
Subject: [R] Chemoinformatic people
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D743BA@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D743BA@euromail.euroscreen.be>
Message-ID: <cdf8178305072107101411221f@mail.gmail.com>

I am just curious why I always want to have a position like that but
never find one. Am I lazy or unlucky for job huntering?(^%$$%*^(

weiwei

On 7/21/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> I am looking for both.
> Fred
> 
> -----Original Message-----
> From: A.J. Rossini [mailto:blindglobe at gmail.com]
> Sent: Thursday, July 21, 2005 3:36 PM
> To: Fr??d??ric Ooms
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Chemoinformatic people
> 
> 
> Just with R, or via another tool integrating R, such as Pipeline Pilot?
> 
> best,
> -tony
> 
> On 7/20/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> > Dear colleague,
> > Just an e-mail to know if they are people working in the field of
> > chemoinformatic that are using R in their work. If yes I was wondering
> > if we couldn't exchange tips and tricks about the use of R in this
> > area ? Best regards Fred Ooms
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> --
> best,
> -tony
> 
> "Commit early,commit often, and commit in a repository from which we can easily roll-back your mistakes" (AJR, 4Jan05).
> 
> A.J. Rossini
> blindglobe at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From matthew_wiener at merck.com  Thu Jul 21 16:09:32 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 21 Jul 2005 10:09:32 -0400
Subject: [R] heatmap color distribution
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049948EC@uswsmx03.merck.com>

You can use the "breaks" argument in image to do this.  (You don't specify a
function you're using, but other heatmap functions probably have a similar
parameter.)  Look across all your data, figure out the ranges you want to
have different colors, and specify the appropriate break points in each call
to image.  Then you're using the same color set in each one.  You run the
risk, of course, that some of your images will have a very narrow color
range, which might obscure interesting features.  But nothing stops you from
making more than one plot.

Hope this helps.

Regards,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jacob Michaelson
Sent: Thursday, July 21, 2005 9:26 AM
To: r-help at stat.math.ethz.ch
Subject: [R] heatmap color distribution


Hi all,

I've got a set of gene expression data, and I'm plotting several  
heatmaps for subsets of the whole set.  I'd like the heatmaps to have  
the same color distribution, so that comparisons may be made  
(roughly) across heatmaps; this would require that the color  
distribution and distance functions be based on the entire dataset,  
rather than on individual subsets.  Does anyone know how to do this?

Thanks in advance,

Jake

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lutz.thieme at amd.com  Thu Jul 21 16:10:39 2005
From: lutz.thieme at amd.com (Thieme, Lutz)
Date: Thu, 21 Jul 2005 16:10:39 +0200
Subject: [R] Rprof fails in combination with RMySQL
Message-ID: <7C60FEAE2EE12D4BA1B232662554CDE1012DF8AD@SF30EXMB1.amd.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050721/2c973648/attachment.pl

From rolf at math.unb.ca  Thu Jul 21 16:10:48 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 21 Jul 2005 11:10:48 -0300 (ADT)
Subject: [R] Problem with read.table()
Message-ID: <200507211410.j6LEAm6V023766@erdos.math.unb.ca>


I don't really understand it, but the problem seems to come down to
the presence of apostrophes (single right quotes "'") in the text
strings.

The first of these occurs in line 149 (not counting the header
line).  If one tries to scan just that line, one gets a vector of
length 10.  Fields 10 to 14 are read as a single field. Upon deleting
the apostrophe, I got a a vector of length 14 (OMMMMMMMMMMM!)

The help on scan() talks about a quote argument and indicates that if
sep is not the newline character, then quote defaults to "'\"".  It
remarks that you can include quotes inside strings by doubling them.
I did a global substitution, changing "'" to "''" throughout, and the
read.table() worked (i.e. didn't complain and yielded up a data frame
of dimension 2935 x 14).  But no apostrophes appeared in the fields
in the resulting data frame.

The help seems to indicate that you can get around the problem by
specifying quote = some character which doesn't appear in the file.
(This also saves having to do a global edit.)  I tried quote="#" and
it seemed to work in this instance.  And the apostrophes ***did***
appear in the strings in the data frame.

I don't grok why the complaint shows up at line 260 rather than
immediately at line 149 ....  but it's a start.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

Original message:

>  From r-help-bounces at stat.math.ethz.ch Thu Jul 21 10:12:09 2005
>  Date: Thu, 21 Jul 2005 14:11:36 +0100
>  From: Kristian Skrede Gleditsch <kgleditsch at ucsd.edu>
>  User-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)
>  X-Accept-Language: en-us, en
>  MIME-Version: 1.0
>  To: r-help at stat.math.ethz.ch
>  X-Essex-ClamAV: No malware found
>  X-Essex-MailScanner: Found to be clean
>  X-Essex-MailScanner-SpamCheck: not spam, SpamAssassin (score=-2.82,
>  	required 5, autolearn=disabled, ALL_TRUSTED -2.82)
>  X-MailScanner-From: kgleditsch at ucsd.edu
>  X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
>  Subject: [R] Problem with read.table()
>  X-BeenThere: r-help at stat.math.ethz.ch
>  X-Mailman-Version: 2.1.6
>  List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
>  List-Unsubscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
>  	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
>  List-Archive: <https://stat.ethz.ch/pipermail/r-help>
>  List-Post: <mailto:r-help at stat.math.ethz.ch>
>  List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
>  List-Subscribe: <https://stat.ethz.ch/mailman/listinfo/r-help>,
>  	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>  Content-Transfer-Encoding: 7bit
>  X-Spam-Checker-Version: SpamAssassin 3.0.4 (2005-06-05) on erdos.math.unb.ca
>  X-Spam-Math-Flag: NO
>  X-Spam-Math-Status: No, hits=0.0 required=5.0 tests=BAYES_50 autolearn=no 
>  	version=3.0.4
>  
>  Dear all,
>  
>  I have encountered a strange problem with read.table(). When I try to 
>  read a tab delimited file I get an error message for line 260 not being 
>  equal to 14 (see below).
>  
>  Using count.fields() suggests that a number of lines have length not 
>  equal to 14, but not 260.
>  
>  Looking at the actual file, however, I cannot see anything wrong with 
>  any lines. They all seem to have length 14, there are no double tabs 
>  etc., and the file reads correctly in other programs. Does anyone have 
>  any suggestions as to what this might stem from?
>  
>  I have placed a copy of the file at 
>  http://dss.ucsd.edu/~kgledits/archigos_v.1.9.asc
>  
>  regards,
>  Kristian Skrede Gleditsch
>  
>  
>   > archigos1.9 <- read.table("c:/work/work12/archigos/archigos_v.1.9.asc",
>  +     sep="\t",header=T,as.is=T,row.names=NULL)
>  Error in scan(file = file, what = what, sep = sep, quote = quote, dec = 
>  dec,  :
>           line 260 did not have 14 elements
>   > a <- count.fields("c:/work/work12/archigos/archigos_v.1.9.asc",sep="\t")
>   > a <- data.frame(c(1:length(a)),a)
>   > a[a[,2]!=14,]
>        c.1.length.a..  a
>  150             150 10
>  313             313 10
>  424             424 10
>  1189           1189  5
>  1510           1510 10
>  1514           1514 10
>  1590           1590  5
>  1600           1600 10
>  1612           1612 10
>  1618           1618 10
>  1619           1619 10
>  1709           1709 10
>  1722           1722 10
>  1981           1981 10
>  1985           1985 10
>  2112           2112 10
>  2178           2178 10
>  2208           2208 10
>  2224           2224 10
>  2530           2530  5
>  2536           2536  5
>  2573           2573  5
>  2928           2928  5
>  -- 
>  Kristian Skrede Gleditsch
>  Department of Political Science, UCSD
>  (On leave, University of Essex, 2005-6)
>  Tel: +44 1206 872499, Fax: +44 1206 873234
>  Email: kgleditsch at ucsd.edu or ksg at essex.ac.uk
>  http://weber.ucsd.edu/~kgledits/



From luke at stat.uiowa.edu  Thu Jul 21 16:13:13 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 21 Jul 2005 09:13:13 -0500 (CDT)
Subject: [R] Chemoinformatic people
In-Reply-To: <1abe3fa905072106353f136158@mail.gmail.com>
References: <5198ADA420721246BC35BFA666E24F16D743B4@euromail.euroscreen.be>
	<1abe3fa905072106353f136158@mail.gmail.com>
Message-ID: <Pine.LNX.4.63.0507210912200.14330@nokomis.stat.uiowa.edu>

I don't.  THere is an address an email at novartis in the ASA directory

  ID    	  068970
Name 	Anthony J. Rossini
Company 	Novartis Pharma AG
Address 	Biostatistics
   	WSJ-27.1.012
City State Zip 	CH-4002 Basel
Country 	Switzerland
Phone 	(206) 543-2005
Email 	anthony.rossini at pharma.novartis.com

luke

On Thu, 21 Jul 2005, A.J. Rossini wrote:

> Just with R, or via another tool integrating R, such as Pipeline Pilot?
>
> best,
> -tony
>
> On 7/20/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
>> Dear colleague,
>> Just an e-mail to know if they are people working in the field of chemoinformatic that are using R in their work. If yes I was wondering if we couldn't exchange tips and tricks about the use of R in this area ?
>> Best regards
>> Fred Ooms
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From blindglobe at gmail.com  Thu Jul 21 16:17:18 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 21 Jul 2005 16:17:18 +0200
Subject: [R] Chemoinformatic people
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D743BA@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D743BA@euromail.euroscreen.be>
Message-ID: <1abe3fa90507210717582e82ef@mail.gmail.com>

I know of a good number of companies who use R via pipeline pilot (and
have looked into it a bit recently), but not R by itself.

One of the big "I wish" items that I've got is "seemless handling of
large data".  Some of the  RDBMS will do it, but not quite seemlessly.
  SPLUS 7.0 does it for a limited class, but in a painful (very
non-seemless) manner.

This would be required to use R in this context, at least for what I've seen.

best,
-tony


On 7/21/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> I am looking for both.
> Fred
> 
> -----Original Message-----
> From: A.J. Rossini [mailto:blindglobe at gmail.com]
> Sent: Thursday, July 21, 2005 3:36 PM
> To: Fr??d??ric Ooms
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Chemoinformatic people
> 
> 
> Just with R, or via another tool integrating R, such as Pipeline Pilot?
> 
> best,
> -tony
> 
> On 7/20/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> > Dear colleague,
> > Just an e-mail to know if they are people working in the field of
> > chemoinformatic that are using R in their work. If yes I was wondering
> > if we couldn't exchange tips and tricks about the use of R in this
> > area ? Best regards Fred Ooms
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> --
> best,
> -tony
> 
> "Commit early,commit often, and commit in a repository from which we can easily roll-back your mistakes" (AJR, 4Jan05).
> 
> A.J. Rossini
> blindglobe at gmail.com
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From Allan at STATS.uct.ac.za  Thu Jul 21 16:18:02 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Thu, 21 Jul 2005 16:18:02 +0200
Subject: [R] R:plot and dots
Message-ID: <42DFAE9A.76B63ED4@STATS.uct.ac.za>

hi all

a very simple question.

i have plot(x,y)

but i would like to add in on the plot the observation number associated
with each point.

how can this be done?

/
allan

From kgleditsch at ucsd.edu  Thu Jul 21 16:18:27 2005
From: kgleditsch at ucsd.edu (Kristian Skrede Gleditsch)
Date: Thu, 21 Jul 2005 15:18:27 +0100
Subject: [R] Problem with read.table()
In-Reply-To: <42DF9F08.3080407@ucsd.edu>
References: <42DF9F08.3080407@ucsd.edu>
Message-ID: <42DFAEB3.7050805@ucsd.edu>

Thanks to all who responded to my earlier message. The problem lies in 
that apostrophes (i.e., ') in some of the text fields are read as quotes.

The file can be read without problems setting quotes="" in read.table.

Incidently, read.delim() also works, even without setting quotes="" 
explicitly.

best regards,

Kristian Skrede Gleditsch
Department of Political Science, UCSD
(On leave, University of Essex, 2005-6)
Tel: +44 1206 872499, Fax: +44 1206 873234
Email: kgleditsch at ucsd.edu or ksg at essex.ac.uk
http://weber.ucsd.edu/~kgledits/


Kristian Skrede Gleditsch wrote:
> Dear all,
> 
> I have encountered a strange problem with read.table(). When I try to 
> read a tab delimited file I get an error message for line 260 not being 
> equal to 14 (see below).
> 
> Using count.fields() suggests that a number of lines have length not 
> equal to 14, but not 260.
> 
> Looking at the actual file, however, I cannot see anything wrong with 
> any lines. They all seem to have length 14, there are no double tabs 
> etc., and the file reads correctly in other programs. Does anyone have 
> any suggestions as to what this might stem from?
> 
> I have placed a copy of the file at 
> http://dss.ucsd.edu/~kgledits/archigos_v.1.9.asc
> 
> regards,
> Kristian Skrede Gleditsch
> 
> 
>  > archigos1.9 <- read.table("c:/work/work12/archigos/archigos_v.1.9.asc",
> +     sep="\t",header=T,as.is=T,row.names=NULL)
> Error in scan(file = file, what = what, sep = sep, quote = quote, dec = 
> dec,  :
>         line 260 did not have 14 elements
>  > a <- count.fields("c:/work/work12/archigos/archigos_v.1.9.asc",sep="\t")
>  > a <- data.frame(c(1:length(a)),a)
>  > a[a[,2]!=14,]
>      c.1.length.a..  a
> 150             150 10
> 313             313 10
> 424             424 10
> 1189           1189  5
> 1510           1510 10
> 1514           1514 10
> 1590           1590  5
> 1600           1600 10
> 1612           1612 10
> 1618           1618 10
> 1619           1619 10
> 1709           1709 10
> 1722           1722 10
> 1981           1981 10
> 1985           1985 10
> 2112           2112 10
> 2178           2178 10
> 2208           2208 10
> 2224           2224 10
> 2530           2530  5
> 2536           2536  5
> 2573           2573  5
> 2928           2928  5



From blindglobe at gmail.com  Thu Jul 21 16:19:27 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 21 Jul 2005 16:19:27 +0200
Subject: [R] Chemoinformatic people
In-Reply-To: <Pine.LNX.4.63.0507210912200.14330@nokomis.stat.uiowa.edu>
References: <5198ADA420721246BC35BFA666E24F16D743B4@euromail.euroscreen.be>
	<1abe3fa905072106353f136158@mail.gmail.com>
	<Pine.LNX.4.63.0507210912200.14330@nokomis.stat.uiowa.edu>
Message-ID: <1abe3fa90507210719569ffac@mail.gmail.com>

Sure, but Luke, I DO NOT currently use R at work...

(now, that's not to say I won't be using it in a few months, but currently...).

best,
-tony


On 7/21/05, Luke Tierney <luke at stat.uiowa.edu> wrote:
> I don't.  THere is an address an email at novartis in the ASA directory
> 
>   ID              068970
> Name    Anthony J. Rossini
> Company         Novartis Pharma AG
> Address         Biostatistics
>         WSJ-27.1.012
> City State Zip  CH-4002 Basel
> Country         Switzerland
> Phone   (206) 543-2005
> Email   anthony.rossini at pharma.novartis.com
> 
> luke
> 
> On Thu, 21 Jul 2005, A.J. Rossini wrote:
> 
> > Just with R, or via another tool integrating R, such as Pipeline Pilot?
> >
> > best,
> > -tony
> >
> > On 7/20/05, Fr??d??ric Ooms <fooms at euroscreen.com> wrote:
> >> Dear colleague,
> >> Just an e-mail to know if they are people working in the field of chemoinformatic that are using R in their work. If yes I was wondering if we couldn't exchange tips and tricks about the use of R in this area ?
> >> Best regards
> >> Fred Ooms
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> >
> >
> 
> --
> Luke Tierney
> Chair, Statistics and Actuarial Science
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From andy_liaw at merck.com  Thu Jul 21 16:16:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Jul 2005 10:16:43 -0400
Subject: [R] RandomForest question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAE2@usctmx1106.Merck.com>

> From: Arne.Muller at sanofi-aventis.com
> 
> Hello,
> 
> I'm trying to find out the optimal number of splits (mtry 
> parameter) for a randomForest classification. The 
> classification is binary and there are 32 explanatory 
> variables (mostly factors with each up to 4 levels but also 
> some numeric variables) and 575 cases.
> 
> I've seen that although there are only 32 explanatory 
> variables the best classification performance is reached when 
> choosing mtry=80. How is it possible that more variables can 
> used than there are in columns the data frame?

It's not.  The code for randomForest.default() has:

    ## Make sure mtry is in reasonable range.
    mtry <- max(1, min(p, round(mtry)))

so it silently sets mtry to number of predictors if it's too large.
As an example:

> library(randomForest)
randomForest 4.5-12 
Type rfNews() to see new features/changes/bug fixes.
> iris.rf = randomForest(Species ~ ., iris, mtry=10)
> iris.rf$mtry
[1] 4

I should probably add a warning in such cases...

Andy

 
> 	thanks for your help
> 	+ kind regards,
> 
> 	Arne
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jul 21 16:29:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 16:29:28 +0200
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <971536df0507210643438a80ef@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>	
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
Message-ID: <42DFB148.6090907@statistik.uni-dortmund.de>

Gabor Grothendieck wrote:

> I think you have been using R too long.  Something like
> this is very much needed.  There are two problems:
> 
> 1. the process itself is too complex (need to get rid of perl,
>     integrate package development tools with package installation 
>    procedure [it should be as easy as downloading a package], 
>    remove necessity to set or modify any environment variables
>     including the path variables).
> 
> 2. there is too much material to absorb just to create a package.
>     The manuals are insufficient.
> 
> A step-by-step simplification is very much needed.  Its no
> coincidence that there are a number of such descriptions on
> the net (google for 'making creating R package') since I would 
> guess that just about everyone has significant problems in creating 
> their first package on Windows.

OK, if people really think this is required, I will sit down on a clean
Windows XP machine, do the setup, and write it down for the next R Help
Desk in R News -- something like "Creating my first R package under
Windows"?

If anybody else is willing to contribute and can write something up in a
manner that is *not* confusing or misleading (none of the other material
spread over the web satisfies this requirement, AFAICS), she/he is
invited to contribute, of course.

BTW, everybody else is invited to submit proposals for R Help Desk!!!

Uwe Ligges




> 
> On 7/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Ivy_Li wrote:
>>
>>
>>>Dear All,
>>>
>>>With the warm support of every R expert, I have built my R library successfully.
>>>Especially thanks: Duncan Murdoch
>>>                  Gabor Grothendieck
>>>                  Henrik Bengtsson
>>>                  Uwe Ligges
>>
>>You are welcome.
>>
>>
>>The following is intended for the records in the archive in order to
>>protect readers.
>>
>>
>>
>>>Without your help, I will lower efficiency.
>>>I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.
>>>
>>>1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
>>
>>Do you mean http://www.murdoch-sutherland.com/Rtools/ ?
>>
>>
>>>2. Download the "rw2011.exe"; Install the newest version of R
>>>3. Download the "tools.zip"; Unpack it into c:\cygwin
>>
>>Not required to call it "cygwin" - also a bit misleading...
>>
>>
>>>4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl
>>
>>Why in C:\Perl ?
>>
>>
>>>5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin
>>
>>Why in c:\mingwin ?
>>
>>
>>
>>>6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin"
>>>      The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";".
>>>      Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.
>>
>>
>>OK, this (1-6) is all described in the R Administration and Installation
>>manual, hence I do not see why we have to repeat it here.
>>
>>
>>
>>>7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
>>>I type in R:
>>>      >f <- function(x,y) x+y
>>>      >g <- function(x,y) x-y
>>>      >d <- data.frame(a=1, b=2)
>>>      >e <- rnorm(1000)
>>>      >package.skeleton(list=c("f","g","d","e"), name="example")
>>>Then modify the 'DESCRIPTION':
>>>      Package: example
>>>      Version: 1.0-1
>>>      Date: 2005-07-09
>>>      Title: My first function
>>>      Author: Ivy <Ivy_Li at smics.com>
>>>      Maintainer: Ivy <Ivy_Li at smics.com>
>>>      Description: simple sum and subtract
>>>      License: GPL version 2 or later
>>>      Depends: R (>= 1.9), stats, graphics, utils
>>>You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference.
>>
>>
>>This is described in Writing R Extension and is not related to the setup
>>of you system in 1-6.
>>
>>
>>
>>>8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
>>>    I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
>>>    However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file
>>
>>
>>This is described in the R Administration and Installation manual
>>and I do not see why we should put the html compiler to the other tools.
>>
>>
>>
>>>9. In the DOS environment. Into the "D:\>"  Type the following code:
>>
>>There is no DOS environment in Windows NT based operating systems.
>>
>>
>>
>>>      cd \Program Files\R\rw2010
>>>      bin\R CMD INSTALL "/Program Files/R/rw2011/example"
>>
>>I do not see why anybody would like to contaminate the binary
>>installation of R with some development source packages.
>>I'd rather use a separate directory.
>>
>>I think reading the two mentioned manuals shoul be sufficient. You have
>>not added relevant information. By adding irrelevant information and
>>omitting some relevant information, I guess we got something that is
>>misleading if the reader does NOT read the manuals as well.
>>
>>Best,
>>Uwe Ligges
>>
>>
>>
>>>Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.
>>>
>>>10.Finally, this package is successfully built up.
>>>
>>>        ---------- Making package example ------------
>>>        adding build stamp to DESCRIPTION
>>>        installing R files
>>>        installing data files
>>>        installing man source files
>>>        installing indices
>>>        not zipping data
>>>        installing help
>>>       >>> Building/Updating help pages for package 'example'
>>>           Formats: text html latex example chm
>>>        d                                 text    html    latex   example
>>>        e                                 text    html    latex   example
>>>        f                                 text    html    latex   example
>>>        g                                 text    html    latex   example
>>>        adding MD5 sums
>>>
>>>      * DONE (example)
>>>
>>>I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.
>>>
>>>
>>>Best Regards!
>>>Ivy Li
>>>YMS in Production & Testing
>>>Semiconductor Manufactory International(ShangHai) Corporation
>>>#18 ZhangJiang Road, PuDong New Area, Shanghai, China
>>>Tel: 021-5080-2000 *11754
>>>Email: Ivy_Li at smics.com
>>>
>>>
>>>
>>>------------------------------------------------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>



From herodote at oreka.com  Thu Jul 21 16:30:17 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Thu, 21 Jul 2005 15:30:17 +0100
Subject: [R] =?iso-8859-1?q?Concatenate_2_functions?=
Message-ID: <IJZEYH$CA8ACB676B65901FDA542BA4A3BFBF5E@oreka.com>

hi all

I need to concatenate 2 functions into one like
temp<-1:1000
for(i=0;i<1000;i++)
{
func<- func & function(beta) dweibull(temp[i],beta,eta)
}

Any idee on this?

thks
guillaume.

////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From ligges at statistik.uni-dortmund.de  Thu Jul 21 16:31:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 16:31:36 +0200
Subject: [R] RandomForest question
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3D4@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3D4@CRBSMXSUSR04>
Message-ID: <42DFB1C8.2070001@statistik.uni-dortmund.de>

Arne.Muller at sanofi-aventis.com wrote:

> Hello,
> 
> I'm trying to find out the optimal number of splits (mtry parameter)
> for a randomForest classification. The classification is binary and
> there are 32 explanatory variables (mostly factors with each up to 4
> levels but also some numeric variables) and 575 cases.
> 
> I've seen that although there are only 32 explanatory variables the
> best classification performance is reached when choosing mtry=80. How
> is it possible that more variables can used than there are in columns
> the data frame?

If some of the variables are factors, dummy variables are generated and 
you get a larger number of variables in the later process.

Uwe Ligges


> thanks for your help + kind regards,
> 
> Arne
> 
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From jjmichael at comcast.net  Thu Jul 21 16:44:48 2005
From: jjmichael at comcast.net (Jake Michaelson)
Date: Thu, 21 Jul 2005 08:44:48 -0600
Subject: [R] heatmap color distribution
In-Reply-To: <45AAE6FD142DCB43A38C00A11FF5DF3E049948EC@uswsmx03.merck.com>
References: <45AAE6FD142DCB43A38C00A11FF5DF3E049948EC@uswsmx03.merck.com>
Message-ID: <6fe61d22d6d78fb0f84aee77fbe00728@comcast.net>

Thanks for the reply.  As I understand it, "breaks" only controls the  
binning.  The problem I'm having is that each subset heatmap has  
slightly different min and max log2 intensities.  I'd like the colors  
to be based on the overall (complete set) max and min, not the subsets'  
max and min -- I could be wrong, but I don't think "breaks" will help  
me there.  And you're right - this might obscure some of the  
trends/features, but we'll also plot the "default" heatmaps.

Also (I should have specified) I'm using heatmap.2.

Thanks,

Jake

On Jul 21, 2005, at 8:09 AM, Wiener, Matthew wrote:

> You can use the "breaks" argument in image to do this.  (You don't  
> specify a
> function you're using, but other heatmap functions probably have a  
> similar
> parameter.)  Look across all your data, figure out the ranges you want  
> to
> have different colors, and specify the appropriate break points in  
> each call
> to image.  Then you're using the same color set in each one.  You run  
> the
> risk, of course, that some of your images will have a very narrow color
> range, which might obscure interesting features.  But nothing stops  
> you from
> making more than one plot.
>
> Hope this helps.
>
> Regards,
>
> Matt Wiener
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jacob Michaelson
> Sent: Thursday, July 21, 2005 9:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] heatmap color distribution
>
>
> Hi all,
>
> I've got a set of gene expression data, and I'm plotting several
> heatmaps for subsets of the whole set.  I'd like the heatmaps to have
> the same color distribution, so that comparisons may be made
> (roughly) across heatmaps; this would require that the color
> distribution and distance functions be based on the entire dataset,
> rather than on individual subsets.  Does anyone know how to do this?
>
> Thanks in advance,
>
> Jake
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
>
>
>
> ----------------------------------------------------------------------- 
> -------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From peter at fe.up.pt  Thu Jul 21 16:55:29 2005
From: peter at fe.up.pt (Peter Ho)
Date: Thu, 21 Jul 2005 15:55:29 +0100
Subject: [R] debian vcd package
Message-ID: <42DFB761.50605@fe.up.pt>

[Apologies if you  have already read this message sent  from another 
email address]

Hi R-Help,

I have been using R  in Linux (Debian) for the past month. The usual way 
I install packages is through apt. Recently, a new packages "vcd" became 
available on CRAN.  I tried installing it today and found that Debian 
does not seem to support this package. I also found that many other 
packages were unavailable.
Does anyone have any recommended sites where a full list is available? 
If none exist, what would be the best way to move ahead in installing 
say the vcd package.

I am still a novice in using Debian and so please forgive me if some of 
my questions may seem trivial for experienced users.



Peter
----------------------------------------------------------------

Peter Ho, PhD.
Escola Superior de Tecnologia e Gestao.
Instituto Politecnico de Viana do Castelo.
Avenida do Atlantico- Apartado 574.
4901-908 Viana do Castelo. Portugal.
Tel: +351-258-819700 Ext. 1252
Email: peter at estg.ipvc.pt



From matthew_wiener at merck.com  Thu Jul 21 17:01:28 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 21 Jul 2005 11:01:28 -0400
Subject: [R] heatmap color distribution
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049948EF@uswsmx03.merck.com>

Breaks affects the binning into colors.  Try this.  Assume that temp is one
of your data sets.  It's values are restricted to 0.25 - 0.75, and we'll
assume that the full data set goes from 0 to 1.

> temp <- matrix(runif(60, 0.25, 0.75), nc = 6)
> breaks <- seq(from = 0, to = 1, length = 11)
> image(temp2, col = heat.colors(10))                     # full range of
color
> image(temp2, col = heat.colors(10), breaks = breaks)    # muted colors

The second image is told about all the colors, and about the full range of
data through breaks, and only uses the colors in the middle.

Is that what you mean?

HTH, 

Matt

-----Original Message-----
From: Jake Michaelson [mailto:jjmichael at comcast.net] 
Sent: Thursday, July 21, 2005 10:45 AM
To: Wiener, Matthew
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] heatmap color distribution


Thanks for the reply.  As I understand it, "breaks" only controls the  
binning.  The problem I'm having is that each subset heatmap has  
slightly different min and max log2 intensities.  I'd like the colors  
to be based on the overall (complete set) max and min, not the subsets'  
max and min -- I could be wrong, but I don't think "breaks" will help  
me there.  And you're right - this might obscure some of the  
trends/features, but we'll also plot the "default" heatmaps.

Also (I should have specified) I'm using heatmap.2.

Thanks,

Jake

On Jul 21, 2005, at 8:09 AM, Wiener, Matthew wrote:

> You can use the "breaks" argument in image to do this.  (You don't  
> specify a
> function you're using, but other heatmap functions probably have a  
> similar
> parameter.)  Look across all your data, figure out the ranges you want  
> to
> have different colors, and specify the appropriate break points in  
> each call
> to image.  Then you're using the same color set in each one.  You run  
> the
> risk, of course, that some of your images will have a very narrow color
> range, which might obscure interesting features.  But nothing stops  
> you from
> making more than one plot.
>
> Hope this helps.
>
> Regards,
>
> Matt Wiener
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jacob Michaelson
> Sent: Thursday, July 21, 2005 9:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] heatmap color distribution
>
>
> Hi all,
>
> I've got a set of gene expression data, and I'm plotting several
> heatmaps for subsets of the whole set.  I'd like the heatmaps to have
> the same color distribution, so that comparisons may be made
> (roughly) across heatmaps; this would require that the color
> distribution and distance functions be based on the entire dataset,
> rather than on individual subsets.  Does anyone know how to do this?
>
> Thanks in advance,
>
> Jake
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
>
>
>
> ----------------------------------------------------------------------- 
> -------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From br44114 at gmail.com  Thu Jul 21 17:05:25 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 21 Jul 2005 11:05:25 -0400
Subject: [R] Rprof fails in combination with RMySQL
Message-ID: <8d5a3635050721080558d9ffb6@mail.gmail.com>

I think you're barking up the wrong tree. Optimize the MySQL code
separately from optimizing the R code. A very nice reference about the
former is http://highperformancemysql.com/. Also, if possible, do
everything in MySQL.
hth,
b.


> -----Original Message-----
> From: Thieme, Lutz [mailto:lutz.thieme at amd.com] 
> Sent: Thursday, July 21, 2005 10:11 AM
> To: Rhelp (E-mail)
> Subject: [R] Rprof fails in combination with RMySQL
> 
> 
> Dear R community,
> 
> I tried to optimized my R code by using Rprof. In my R code 
> I'm using MySQL
> database connections intensively. After a bunch of queries R 
> fails with the 
> following error message:
> Error in .Call("RS_MySQL_newConnection", drvId, con.params, 
> groups, PACKAGE = .MySQLPkgName) : 
>         RS-DBI driver: (could not connect mylogin at mydatabase 
> on dbname "myDB"
> 
> Without the R profiler this code runs very stable since weeks.
> 
> Do you have any ideas or suggestions?
> 
> I tried the following R versions:
> ___________________________
> platform i386-pc-solaris2.8
> arch     i386              
> os       solaris2.8        
> system   i386, solaris2.8  
> status                     
> major    1                 
> minor    9.1               
> year     2004              
> month    06                
> day      21                
> language R   
> ___________________________
> platform sparc-sun-solaris2.8
> arch     sparc               
> os       solaris2.8          
> system   sparc, solaris2.8   
> status                       
> major    2                   
> minor    1.1                 
> year     2005                
> month    06                  
> day      20                  
> language R   
> ___________________________
> platform sparc-sun-solaris2.8
> arch     sparc               
> os       solaris2.8          
> system   sparc, solaris2.8   
> status                       
> major    1                   
> minor    9.1                 
> year     2004                
> month    06                  
> day      21                  
> language R   
> 
> 
> Thank you in advance and kind regards,
> 
> Lutz Thieme
> AMD Saxony/ Product Engineering AMD Saxony Limited 
> Liability Company & Co. KG
> phone: + 49-351-277-4269 M/S E22-PE, 
> Wilschdorfer Landstr. 101
> fax: + 49-351-277-9-4269 D-01109 Dresden, Germany
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jul 21 17:06:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2005 11:06:57 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DFB148.6090907@statistik.uni-dortmund.de>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFB148.6090907@statistik.uni-dortmund.de>
Message-ID: <971536df0507210806666af307@mail.gmail.com>

An article like that would be really great. 

On 7/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Gabor Grothendieck wrote:
> 
> > I think you have been using R too long.  Something like
> > this is very much needed.  There are two problems:
> >
> > 1. the process itself is too complex (need to get rid of perl,
> >     integrate package development tools with package installation
> >    procedure [it should be as easy as downloading a package],
> >    remove necessity to set or modify any environment variables
> >     including the path variables).
> >
> > 2. there is too much material to absorb just to create a package.
> >     The manuals are insufficient.
> >
> > A step-by-step simplification is very much needed.  Its no
> > coincidence that there are a number of such descriptions on
> > the net (google for 'making creating R package') since I would
> > guess that just about everyone has significant problems in creating
> > their first package on Windows.
> 
> OK, if people really think this is required, I will sit down on a clean
> Windows XP machine, do the setup, and write it down for the next R Help
> Desk in R News -- something like "Creating my first R package under
> Windows"?
> 
> If anybody else is willing to contribute and can write something up in a
> manner that is *not* confusing or misleading (none of the other material
> spread over the web satisfies this requirement, AFAICS), she/he is
> invited to contribute, of course.
> 
> BTW, everybody else is invited to submit proposals for R Help Desk!!!
> 
> Uwe Ligges
> 
> 
> 
> 
> >
> > On 7/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >
> >>Ivy_Li wrote:
> >>
> >>
> >>>Dear All,
> >>>
> >>>With the warm support of every R expert, I have built my R library successfully.
> >>>Especially thanks: Duncan Murdoch
> >>>                  Gabor Grothendieck
> >>>                  Henrik Bengtsson
> >>>                  Uwe Ligges
> >>
> >>You are welcome.
> >>
> >>
> >>The following is intended for the records in the archive in order to
> >>protect readers.
> >>
> >>
> >>
> >>>Without your help, I will lower efficiency.
> >>>I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.
> >>>
> >>>1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>
> >>Do you mean http://www.murdoch-sutherland.com/Rtools/ ?
> >>
> >>
> >>>2. Download the "rw2011.exe"; Install the newest version of R
> >>>3. Download the "tools.zip"; Unpack it into c:\cygwin
> >>
> >>Not required to call it "cygwin" - also a bit misleading...
> >>
> >>
> >>>4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl
> >>
> >>Why in C:\Perl ?
> >>
> >>
> >>>5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin
> >>
> >>Why in c:\mingwin ?
> >>
> >>
> >>
> >>>6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin"
> >>>      The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";".
> >>>      Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.
> >>
> >>
> >>OK, this (1-6) is all described in the R Administration and Installation
> >>manual, hence I do not see why we have to repeat it here.
> >>
> >>
> >>
> >>>7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
> >>>I type in R:
> >>>      >f <- function(x,y) x+y
> >>>      >g <- function(x,y) x-y
> >>>      >d <- data.frame(a=1, b=2)
> >>>      >e <- rnorm(1000)
> >>>      >package.skeleton(list=c("f","g","d","e"), name="example")
> >>>Then modify the 'DESCRIPTION':
> >>>      Package: example
> >>>      Version: 1.0-1
> >>>      Date: 2005-07-09
> >>>      Title: My first function
> >>>      Author: Ivy <Ivy_Li at smics.com>
> >>>      Maintainer: Ivy <Ivy_Li at smics.com>
> >>>      Description: simple sum and subtract
> >>>      License: GPL version 2 or later
> >>>      Depends: R (>= 1.9), stats, graphics, utils
> >>>You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference.
> >>
> >>
> >>This is described in Writing R Extension and is not related to the setup
> >>of you system in 1-6.
> >>
> >>
> >>
> >>>8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
> >>>    I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
> >>>    However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file
> >>
> >>
> >>This is described in the R Administration and Installation manual
> >>and I do not see why we should put the html compiler to the other tools.
> >>
> >>
> >>
> >>>9. In the DOS environment. Into the "D:\>"  Type the following code:
> >>
> >>There is no DOS environment in Windows NT based operating systems.
> >>
> >>
> >>
> >>>      cd \Program Files\R\rw2010
> >>>      bin\R CMD INSTALL "/Program Files/R/rw2011/example"
> >>
> >>I do not see why anybody would like to contaminate the binary
> >>installation of R with some development source packages.
> >>I'd rather use a separate directory.
> >>
> >>I think reading the two mentioned manuals shoul be sufficient. You have
> >>not added relevant information. By adding irrelevant information and
> >>omitting some relevant information, I guess we got something that is
> >>misleading if the reader does NOT read the manuals as well.
> >>
> >>Best,
> >>Uwe Ligges
> >>
> >>
> >>
> >>>Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.
> >>>
> >>>10.Finally, this package is successfully built up.
> >>>
> >>>        ---------- Making package example ------------
> >>>        adding build stamp to DESCRIPTION
> >>>        installing R files
> >>>        installing data files
> >>>        installing man source files
> >>>        installing indices
> >>>        not zipping data
> >>>        installing help
> >>>       >>> Building/Updating help pages for package 'example'
> >>>           Formats: text html latex example chm
> >>>        d                                 text    html    latex   example
> >>>        e                                 text    html    latex   example
> >>>        f                                 text    html    latex   example
> >>>        g                                 text    html    latex   example
> >>>        adding MD5 sums
> >>>
> >>>      * DONE (example)
> >>>
> >>>I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.
> >>>
> >>>
> >>>Best Regards!
> >>>Ivy Li
> >>>YMS in Production & Testing
> >>>Semiconductor Manufactory International(ShangHai) Corporation
> >>>#18 ZhangJiang Road, PuDong New Area, Shanghai, China
> >>>Tel: 021-5080-2000 *11754
> >>>Email: Ivy_Li at smics.com
> >>>
> >>>
> >>>
> >>>------------------------------------------------------------------------
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> 
>



From Mugdha.Wagle at STJUDE.ORG  Thu Jul 21 17:09:50 2005
From: Mugdha.Wagle at STJUDE.ORG (Wagle, Mugdha)
Date: Thu, 21 Jul 2005 10:09:50 -0500
Subject: [R] principal component analysis in affy
Message-ID: <F2235647AC878D438F09255C39842FBC0D342C18@SJMEMXMB03.stjude.sjcrh.local>

Hi,
 
I have been using the prcomp function to perform PCA on my example microarray data, (stored in metric text files) which looks like this:
 
        1a 1b 1c 1d 1e 1f ...................................................4r 4s 4t
g1    1.2705 1.2766 ...........................................................2.0298
g2    0.1631 ........................................................................0.7067
g3    0.2212 ........................................................................1.0439
.
.
.
.
g99  1.3657..........................................................................2.3736
 
i.e. a matrix of 63 columns and 99 rows, where the columns represent chip and rows represent genes. Now, the biplot function
 
biplot(prcomp(pcadata, scale = TRUE), cex = c(0.75,0.75))
 
gives me a plot with one vector per gene. However, I actually need to get one vector per chip instead of one vector per gene.  I have been told that there is a function in the affy package that does what I am looking for i.e. gives one vector per chip. Can someone please tell me what the function is called, and how I can get hold of the code(since I believe affy only works on CEL files) ? I have downloaded the affy R code from Terry Speed's website already, but I don't know where (if at all) the code to perform PCA is.
 
Thank you everyone!
 
Sincerely,
Mugdha Wagle
Hartwell Center for Bioinformatics and Biotechnology,
St.Jude Children's Research Hospital, Memphis TN 38105



From helprhelp at gmail.com  Thu Jul 21 17:16:57 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 21 Jul 2005 10:16:57 -0500
Subject: [R] RandomForest question
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAE2@usctmx1106.Merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAE2@usctmx1106.Merck.com>
Message-ID: <cdf817830507210816417b2287@mail.gmail.com>

Hi,
I found the following lines from Leo's randomForest, and I am not sure
if it can be applied here but just tried to help:

mtry0 = the number of variables to split on at each node. Default is
the square root of mdim. ATTENTION! DO NOT USE THE DEFAULT VALUES OF
MTRY0 IF YOU WANT TO OPTIMIZE THE PERFORMANCE OF RANDOM FORESTS. TRY
DIFFERENT VALUES-GROW 20-30 TREES, AND SELECT THE VALUE OF MTRY THAT
GIVES THE SMALLEST OOB ERROR RATE.

mdim is the number of predicators.

HTH,

weiwei

On 7/21/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > From: Arne.Muller at sanofi-aventis.com
> >
> > Hello,
> >
> > I'm trying to find out the optimal number of splits (mtry
> > parameter) for a randomForest classification. The
> > classification is binary and there are 32 explanatory
> > variables (mostly factors with each up to 4 levels but also
> > some numeric variables) and 575 cases.
> >
> > I've seen that although there are only 32 explanatory
> > variables the best classification performance is reached when
> > choosing mtry=80. How is it possible that more variables can
> > used than there are in columns the data frame?
> 
> It's not.  The code for randomForest.default() has:
> 
>     ## Make sure mtry is in reasonable range.
>     mtry <- max(1, min(p, round(mtry)))
> 
> so it silently sets mtry to number of predictors if it's too large.
> As an example:
> 
> > library(randomForest)
> randomForest 4.5-12
> Type rfNews() to see new features/changes/bug fixes.
> > iris.rf = randomForest(Species ~ ., iris, mtry=10)
> > iris.rf$mtry
> [1] 4
> 
> I should probably add a warning in such cases...
> 
> Andy
> 
> 
> >       thanks for your help
> >       + kind regards,
> >
> >       Arne
> >
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From murdoch at stats.uwo.ca  Thu Jul 21 17:23:54 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 21 Jul 2005 11:23:54 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <971536df0507210643438a80ef@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
Message-ID: <42DFBE0A.3020803@stats.uwo.ca>

On 7/21/2005 9:43 AM, Gabor Grothendieck wrote:
> I think you have been using R too long.  Something like
> this is very much needed.  There are two problems:
> 
> 1. the process itself is too complex (need to get rid of perl,
>     integrate package development tools with package installation 
>    procedure [it should be as easy as downloading a package], 
>    remove necessity to set or modify any environment variables
>     including the path variables).

I agree with some of this, but I don't see much interest in fixing it.

For example, getting rid of Perl would be a lot of work.  When the Perl 
scripts were written, R was not capable of doing what they do.  I think 
it is capable now, but there's still a huge amount of translation work 
to do.  Who will do that?  Who will test that they did it right?  At the 
end, will it actually have been worth all the trouble?  Installing Perl 
is not all that hard.

> 2. there is too much material to absorb just to create a package.
>     The manuals are insufficient.

The first sentence here is basically a repetition of "the process is too 
complex".  I think the second sentence is incorrect.  Could you please 
point out what necessary steps are missing?
> 
> A step-by-step simplification is very much needed.  

Exactly this has been in the Installation and Administration manual 
since I put it there in February for the 2.1.0 release.  It's at the 
beginning of the appendix on the Windows toolset, with multiple 
references pointing people there.  It's followed by detailed 
descriptions of each of the steps.

If you think it could be further improved, please submit improvements.

 >Its no
> coincidence that there are a number of such descriptions on
> the net (google for 'making creating R package') since I would 
> guess that just about everyone has significant problems in creating 
> their first package on Windows.

As far as I can tell, those all predate the release of 2.1.0.  I think 
your complaints are out of date.

Duncan Murdoch

> 
> 
> On 7/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>> Ivy_Li wrote:
>> 
>> > Dear All,
>> >
>> > With the warm support of every R expert, I have built my R library successfully.
>> > Especially thanks: Duncan Murdoch
>> >                   Gabor Grothendieck
>> >                   Henrik Bengtsson
>> >                   Uwe Ligges
>> 
>> You are welcome.
>> 
>> 
>> The following is intended for the records in the archive in order to
>> protect readers.
>> 
>> 
>> > Without your help, I will lower efficiency.
>> > I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.
>> >
>> > 1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
>> 
>> Do you mean http://www.murdoch-sutherland.com/Rtools/ ?
>> 
>> > 2. Download the "rw2011.exe"; Install the newest version of R
>> > 3. Download the "tools.zip"; Unpack it into c:\cygwin
>> 
>> Not required to call it "cygwin" - also a bit misleading...
>> 
>> > 4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl
>> 
>> Why in C:\Perl ?
>> 
>> > 5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin
>> 
>> Why in c:\mingwin ?
>> 
>> 
>> > 6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin"
>> >       The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";".
>> >       Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.
>> 
>> 
>> OK, this (1-6) is all described in the R Administration and Installation
>> manual, hence I do not see why we have to repeat it here.
>> 
>> 
>> > 7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
>> > I type in R:
>> >       >f <- function(x,y) x+y
>> >       >g <- function(x,y) x-y
>> >       >d <- data.frame(a=1, b=2)
>> >       >e <- rnorm(1000)
>> >       >package.skeleton(list=c("f","g","d","e"), name="example")
>> > Then modify the 'DESCRIPTION':
>> >       Package: example
>> >       Version: 1.0-1
>> >       Date: 2005-07-09
>> >       Title: My first function
>> >       Author: Ivy <Ivy_Li at smics.com>
>> >       Maintainer: Ivy <Ivy_Li at smics.com>
>> >       Description: simple sum and subtract
>> >       License: GPL version 2 or later
>> >       Depends: R (>= 1.9), stats, graphics, utils
>> > You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference.
>> 
>> 
>> This is described in Writing R Extension and is not related to the setup
>> of you system in 1-6.
>> 
>> 
>> >
>> > 8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
>> >     I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
>> >     However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file
>> 
>> 
>> This is described in the R Administration and Installation manual
>> and I do not see why we should put the html compiler to the other tools.
>> 
>> 
>> > 9. In the DOS environment. Into the "D:\>"  Type the following code:
>> 
>> There is no DOS environment in Windows NT based operating systems.
>> 
>> 
>> >       cd \Program Files\R\rw2010
>> >       bin\R CMD INSTALL "/Program Files/R/rw2011/example"
>> 
>> I do not see why anybody would like to contaminate the binary
>> installation of R with some development source packages.
>> I'd rather use a separate directory.
>> 
>> I think reading the two mentioned manuals shoul be sufficient. You have
>> not added relevant information. By adding irrelevant information and
>> omitting some relevant information, I guess we got something that is
>> misleading if the reader does NOT read the manuals as well.
>> 
>> Best,
>> Uwe Ligges
>> 
>> 
>> > Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.
>> >
>> > 10.Finally, this package is successfully built up.
>> >
>> >         ---------- Making package example ------------
>> >         adding build stamp to DESCRIPTION
>> >         installing R files
>> >         installing data files
>> >         installing man source files
>> >         installing indices
>> >         not zipping data
>> >         installing help
>> >        >>> Building/Updating help pages for package 'example'
>> >            Formats: text html latex example chm
>> >         d                                 text    html    latex   example
>> >         e                                 text    html    latex   example
>> >         f                                 text    html    latex   example
>> >         g                                 text    html    latex   example
>> >         adding MD5 sums
>> >
>> >       * DONE (example)
>> >
>> > I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.
>> >
>> >
>> > Best Regards!
>> > Ivy Li
>> > YMS in Production & Testing
>> > Semiconductor Manufactory International(ShangHai) Corporation
>> > #18 ZhangJiang Road, PuDong New Area, Shanghai, China
>> > Tel: 021-5080-2000 *11754
>> > Email: Ivy_Li at smics.com
>> >
>> >
>> >
>> > ------------------------------------------------------------------------
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>> 
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>>
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Thu Jul 21 17:27:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 21 Jul 2005 11:27:42 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DFB148.6090907@statistik.uni-dortmund.de>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>		<42DF9BC9.9050508@statistik.uni-dortmund.de>	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFB148.6090907@statistik.uni-dortmund.de>
Message-ID: <42DFBEEE.6030000@stats.uwo.ca>

On 7/21/2005 10:29 AM, Uwe Ligges wrote:
> Gabor Grothendieck wrote:
> 
>> I think you have been using R too long.  Something like
>> this is very much needed.  There are two problems:
>> 
>> 1. the process itself is too complex (need to get rid of perl,
>>     integrate package development tools with package installation 
>>    procedure [it should be as easy as downloading a package], 
>>    remove necessity to set or modify any environment variables
>>     including the path variables).
>> 
>> 2. there is too much material to absorb just to create a package.
>>     The manuals are insufficient.
>> 
>> A step-by-step simplification is very much needed.  Its no
>> coincidence that there are a number of such descriptions on
>> the net (google for 'making creating R package') since I would 
>> guess that just about everyone has significant problems in creating 
>> their first package on Windows.
> 
> OK, if people really think this is required, I will sit down on a clean
> Windows XP machine, do the setup, and write it down for the next R Help
> Desk in R News -- something like "Creating my first R package under
> Windows"?
> 
> If anybody else is willing to contribute and can write something up in a
> manner that is *not* confusing or misleading (none of the other material
> spread over the web satisfies this requirement, AFAICS), she/he is
> invited to contribute, of course.
> 
> BTW, everybody else is invited to submit proposals for R Help Desk!!!

That sounds great.  Could you also take notes as you go about specific
problems in the writeup in the R-admin manual, so it can be improved for
the next release?

Another thing you could do which would be valuable:  get a student or
someone else who is reasonably computer literate, but unfamiliar with R
details, to do this while you sit watching and recording their mistakes.

Duncan Murdoch



From RRoa at fisheries.gov.fk  Thu Jul 21 15:28:55 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 21 Jul 2005 11:28:55 -0200
Subject: [R] heatmap color distribution
Message-ID: <03DCBBA079F2324786E8715BE538968A068EB2@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Jacob Michaelson
> Sent: 21 July 2005 12:26
> To: r-help at stat.math.ethz.ch
> Subject: [R] heatmap color distribution
> 
> 
> Hi all,
> 
> I've got a set of gene expression data, and I'm plotting several  
> heatmaps for subsets of the whole set.  I'd like the heatmaps 
> to have  the same color distribution, so that comparisons may be made  
> (roughly) across heatmaps; this would require that the color  
> distribution and distance functions be based on the entire dataset,  
> rather than on individual subsets.  Does anyone know how to do this?
> 
> Thanks in advance,

For each heatmap, in image() set the zlim argument to c(zmin,zmax) where 
zmin and zmax are the minimum and maximum observed across the entire data 
set. Also, for each heatmap set col=heat.colors(n) to the same n for all 
heatmaps. I do that with image.kriging in geoR. Hope it works for you.
Ruben



From ghwei at umich.edu  Thu Jul 21 17:28:40 2005
From: ghwei at umich.edu (ghwei@umich.edu)
Date: Thu, 21 Jul 2005 11:28:40 -0400
Subject: [R] output of variance estimate of random effect from a gamma
	frailty	model using Coxph in R
Message-ID: <20050721112840.37h0v0j11c0sgg4k@web.mail.umich.edu>

Hi,

I have a question about the output for variance of random effect from a gamma
frailty model using coxph in R. Is it the vairance of frailties themselves or
variance of log frailties? Thanks.

Guanghui



From mschwartz at mn.rr.com  Thu Jul 21 17:38:21 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 21 Jul 2005 10:38:21 -0500
Subject: [R] R:plot and dots
In-Reply-To: <42DFAE9A.76B63ED4@STATS.uct.ac.za>
References: <42DFAE9A.76B63ED4@STATS.uct.ac.za>
Message-ID: <1121960302.4611.18.camel@localhost.localdomain>

On Thu, 2005-07-21 at 16:18 +0200, Clark Allan wrote:
> hi all
> 
> a very simple question.
> 
> i have plot(x,y)
> 
> but i would like to add in on the plot the observation number associated
> with each point.
> 
> how can this be done?
> 
> /
> allan

If you mean the unique observation number associated with each x,y pair,
you can use:

  text(x, y, labels = ObsNumberVector, pos = 3)

after the plot(x, y) call:

 df <- data.frame(x = rnorm(10), y = rnorm(10), ID = 1:10)
 with(df, plot(x, y))
 with(df, text(x, y, labels = ID, pos = 3))

See ?text for more information.  Note that I used pos = 3 which places
the label above the data point. There are other positioning parameters
available, which are noted in the help file.

Note also that you might have to adjust the plot axis limits depending
upon where you place the text and your extreme points.

If you mean the frequency of each x,y pair (if there is more than one
observation per x,y pair), you might want to review Deepayan's recent
post here:

https://stat.ethz.ch/pipermail/r-help/2005-July/074042.html

HTH,

Marc Schwartz



From jjmichael at comcast.net  Thu Jul 21 17:43:25 2005
From: jjmichael at comcast.net (Jake Michaelson)
Date: Thu, 21 Jul 2005 09:43:25 -0600
Subject: [R] reorder bug in heatmap.2?
Message-ID: <e29fa669d18a35e7a1938538d11d2ac4@comcast.net>

I want to plot a heatmap without reordering the columns.  This works 
fine in "heatmap":

 > heatmap(meanX[selected,], col=cm.colors(256), Colv=NA)

But in "heatmap.2" I get:

 > heatmap.2(meanX[selected,], col=cm.colors(256), Colv=NA)
Error in if (!is.logical(Colv) || Colv) ddc <- reorder(ddc, Colv) :
	missing value where TRUE/FALSE needed

(Note that instructions for the use of "Colv" and "Rowv" are identical 
in both heatmap and heatmap.2 documentation)

Is there another way to not reorder columns in heatmap.2?

Thanks in advance,

Jake



From jtk at cmp.uea.ac.uk  Thu Jul 21 17:45:12 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 21 Jul 2005 16:45:12 +0100
Subject: [R] debian vcd package
In-Reply-To: <42DFB761.50605@fe.up.pt>
References: <42DFB761.50605@fe.up.pt>
Message-ID: <20050721154512.GE26243@jtkpc.cmp.uea.ac.uk>

On Thu, Jul 21, 2005 at 03:55:29PM +0100, Peter Ho wrote:
> [Apologies if you  have already read this message sent  from another 
> email address]
> 
> Hi R-Help,
> 
> I have been using R  in Linux (Debian) for the past month. The usual way 
> I install packages is through apt. Recently, a new packages "vcd" became 
> available on CRAN.  I tried installing it today and found that Debian 
> does not seem to support this package. I also found that many other 
> packages were unavailable.
> Does anyone have any recommended sites where a full list is available? 
> If none exist, what would be the best way to move ahead in installing 
> say the vcd package.
> 
> I am still a novice in using Debian and so please forgive me if some of 
> my questions may seem trivial for experienced users.

Unfortunately, the term "package" means different things in the context
of R and of Debian. A Debian package is what you install using tools like
apt etc. The traditional way of installing an R package on Linux is to

    * have R installed from source, or install the r-base-dev Debian
      package

    * download the package archive (e.g.
      http://www.stats.bris.ac.uk/R/src/contrib/vcd_0.9-0.tar.gz )

    * run the R CMD INSTALL command on it, e.g.

        R CMD INSTALL vcd_0.9-0.tar.gz

This requires having a number of development Debian packages installed,
such as gcc, g77 etc (installing r-base-dev will automatically resolve
such dependencies).

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From ligges at statistik.uni-dortmund.de  Thu Jul 21 17:49:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 17:49:44 +0200
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DFBEEE.6030000@stats.uwo.ca>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>		<42DF9BC9.9050508@statistik.uni-dortmund.de>	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFB148.6090907@statistik.uni-dortmund.de>
	<42DFBEEE.6030000@stats.uwo.ca>
Message-ID: <42DFC418.9040406@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> On 7/21/2005 10:29 AM, Uwe Ligges wrote:
> 
>>Gabor Grothendieck wrote:
>>
>>
>>>I think you have been using R too long.  Something like
>>>this is very much needed.  There are two problems:
>>>
>>>1. the process itself is too complex (need to get rid of perl,
>>>    integrate package development tools with package installation 
>>>   procedure [it should be as easy as downloading a package], 
>>>   remove necessity to set or modify any environment variables
>>>    including the path variables).
>>>
>>>2. there is too much material to absorb just to create a package.
>>>    The manuals are insufficient.
>>>
>>>A step-by-step simplification is very much needed.  Its no
>>>coincidence that there are a number of such descriptions on
>>>the net (google for 'making creating R package') since I would 
>>>guess that just about everyone has significant problems in creating 
>>>their first package on Windows.
>>
>>OK, if people really think this is required, I will sit down on a clean
>>Windows XP machine, do the setup, and write it down for the next R Help
>>Desk in R News -- something like "Creating my first R package under
>>Windows"?
>>
>>If anybody else is willing to contribute and can write something up in a
>>manner that is *not* confusing or misleading (none of the other material
>>spread over the web satisfies this requirement, AFAICS), she/he is
>>invited to contribute, of course.
>>
>>BTW, everybody else is invited to submit proposals for R Help Desk!!!
> 
> 
> That sounds great.  Could you also take notes as you go about specific
> problems in the writeup in the R-admin manual, so it can be improved for
> the next release?

Of course.


> Another thing you could do which would be valuable:  get a student or
> someone else who is reasonably computer literate, but unfamiliar with R
> details, to do this while you sit watching and recording their mistakes.

Good idea.

Uwe


> Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Thu Jul 21 17:54:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 17:54:06 +0200
Subject: [R] Concatenate 2 functions
In-Reply-To: <IJZEYH$CA8ACB676B65901FDA542BA4A3BFBF5E@oreka.com>
References: <IJZEYH$CA8ACB676B65901FDA542BA4A3BFBF5E@oreka.com>
Message-ID: <42DFC51E.7080306@statistik.uni-dortmund.de>

herodote at oreka.com wrote:

> hi all
> 
> I need to concatenate 2 functions into one like
> temp<-1:1000
> for(i=0;i<1000;i++)
 >
> {
> func<- func & function(beta) dweibull(temp[i],beta,eta)
> }

Please read An Introduction to R.
Please read the posting guide.
What do you expect to be in func? This is completely unclear to me.

Uwe Ligges



> Any idee on this?
> 
> thks
> guillaume.
> 
> ////////////////////////////////////////////////////////////
> // Webmail Oreka : http://www.oreka.com
> ////////////////////////////////////////////////////////////
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul 21 17:54:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 17:54:20 +0200
Subject: [R] R:plot and dots
In-Reply-To: <42DFAE9A.76B63ED4@STATS.uct.ac.za>
References: <42DFAE9A.76B63ED4@STATS.uct.ac.za>
Message-ID: <42DFC52C.5060903@statistik.uni-dortmund.de>

Clark Allan wrote:

> hi all
> 
> a very simple question.
> 
> i have plot(x,y)
> 
> but i would like to add in on the plot the observation number associated
> with each point.
> 
> how can this be done?

See ?text

Uwe Ligges


> /
> allan
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From christophe.pouzat at univ-paris5.fr  Thu Jul 21 18:01:36 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Thu, 21 Jul 2005 18:01:36 +0200
Subject: [R] About object of class mle returned by user defined functions
Message-ID: <42DFC6E0.8050901@univ-paris5.fr>

Hi,

There is something I don't get with object of class "mle" returned by a 
function I wrote. More precisely it's about the behaviour of method 
"confint" and "profile" applied to these object.

I've written a short function (see below) whose arguments are:
1) A univariate sample (arising from a gamma, log-normal or whatever).
2) A character string standing for one of the R densities, eg, "gamma", 
"lnorm", etc. That's the density the user wants to fit to the data.
3) A named list with initial values for the density parameters; that 
will be passed to optim via mle.
4) The method to be used by optim via mle. That can be change by the 
code if parameter boundaries are also supplied.
5) The lowest allowed values for the parameters.
6) The largest allowed values.

The "big" thing this short function does is writing on-fly the 
corresponding log-likelihood function before calling "mle". The object 
of class "mle" returned by the call to "mle" is itself returned by the 
function.

Here is the code:

newFit <- function(isi, ## The data set
                   isi.density = "gamma", ## The name of the density 
used as model
                   initial.para = list( shape = (mean(isi)/sd(isi))^2,
                     scale = sd(isi)^2 / mean(isi) ), ## Inital 
parameters passed to optim
                   optim.method = "BFGS", ## optim method
                   optim.lower = numeric(length(initial.para)) + 0.00001,
                   optim.upper = numeric(length(initial.para)) + Inf,
                   ...) {

  require(stats4)
 
  ## Create a string with the log likelihood definition
  minusLogLikelihood.txt <- paste("function( ",
                                  paste(names(initial.para), collapse = 
", "),
                                  " ) {",
                                  "isi <- eval(",
                                  deparse(substitute(isi)),
                                  ", envir = .GlobalEnv);",
                                  "-sum(",
                                  paste("d", isi.density, sep = ""),
                                  "(isi, ",
                                  paste(names(initial.para), collapse = 
", "),
                                  ", log = TRUE) ) }"
                                  )

  ## Define logLikelihood function
  minusLogLikelihood <- eval( parse(text = minusLogLikelihood.txt) )
  environment(minusLogLikelihood) <- .GlobalEnv

 
  if ( all( is.infinite( c(optim.lower,optim.upper) ) ) ) {
      getFit <- mle(minusLogLikelihood,
                    start = initial.para,
                    method = optim.method,
                    ...
                    )
  } else {
    getFit <- mle(minusLogLikelihood,
                  start = initial.para,
                  method = "L-BFGS-B",
                  lower = optim.lower,
                  upper = optim.upper,
                  ...
                  )
  }  ## End of conditional on all(is.infinite(c(optim.lower,optim.upper)))
 
  getFit
 
}


It seems to work fine on examples like:

 > isi1 <- rgamma(100, shape = 2, scale = 1)
 > fit1 <- newFit(isi1) ## fitting here with the "correct" density 
(initial parameters are obtained by the method of moments)
 > coef(fit1)
    shape     scale
1.8210477 0.9514774
 > vcov(fit1)
           shape      scale
shape 0.05650600 0.02952371
scale 0.02952371 0.02039714
 > logLik(fit1)
'log Lik.' -155.9232 (df=2)

If we compare with a "direct" call to "mle":

 > llgamma <- function(sh, sc) -sum(dgamma(isi1, shape = sh, scale = sc, 
log = TRUE))
 > fitA <- mle(llgamma, start = list( sh = (mean(isi1)/sd(isi1))^2, sc = 
sd(isi1)^2 / mean(isi1) ),lower = c(0.0001,0.0001), method = "L-BFGS-B")
 > coef(fitA)
      sh       sc
1.821042 1.051001
 > vcov(fitA)
            sh          sc
sh  0.05650526 -0.03261146
sc -0.03261146  0.02488714
 > logLik(fitA)
'log Lik.' -155.9232 (df=2)

I get almost the same estimated parameter values, same log-likelihood 
but not the same vcov matrix.

A call to "profile" or "confint" on fit1 does not work, eg:
 > confint(fit1)
Profiling...
Erreur dans approx(sp$y, sp$x, xout = cutoff) :
    need at least two non-NA values to interpolate
De plus : Message d'avis :
collapsing to unique 'x' values in: approx(sp$y, sp$x, xout = cutoff)

Although calling the log-likelihood function defined in fit1 
(fit1 at minuslogl) with argument values different from the MLE does return 
something sensible:

 > fit1 at minuslogl(coef(fit1)[1],coef(fit1)[2])
[1] 155.9232
 > fit1 at minuslogl(coef(fit1)[1]+0.01,coef(fit1)[2]+0.01)
[1] 155.9263

There is obviously something I'm missing here since I thought for a 
while that the problem was with the environment "attached" to the 
function "minusLogLikelihood" when calling "eval"; but the lines above 
make me think it is not the case...

Any help and/or ideas warmly welcomed.

Thanks,

Christophe.

-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From ggrothendieck at gmail.com  Thu Jul 21 18:03:46 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2005 12:03:46 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DFBE0A.3020803@stats.uwo.ca>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFBE0A.3020803@stats.uwo.ca>
Message-ID: <971536df05072109035a3948b8@mail.gmail.com>

On 7/21/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/21/2005 9:43 AM, Gabor Grothendieck wrote:
> > I think you have been using R too long.  Something like
> > this is very much needed.  There are two problems:
> >
> > 1. the process itself is too complex (need to get rid of perl,
> >     integrate package development tools with package installation
> >    procedure [it should be as easy as downloading a package],
> >    remove necessity to set or modify any environment variables
> >     including the path variables).
> 
> I agree with some of this, but I don't see much interest in fixing it.
> 
> For example, getting rid of Perl would be a lot of work.  When the Perl
> scripts were written, R was not capable of doing what they do.  I think
> it is capable now, but there's still a huge amount of translation work
> to do.  Who will do that?  Who will test that they did it right?  At the
> end, will it actually have been worth all the trouble?  Installing Perl
> is not all that hard.

Each step may not be hard but the totality of them all means its
pretty complex for most people.  I don't know who will do it or
whether anyone even will but a first step is identifying that it needs
to be done.  Since the key to expanding R is to expand the library
to me making it simple to create and install packages and R ought
to be of very high priority for the core group regardless of difficulty
in achieving this.  If no one is interested in doing it then it will remain
a limitation of R that commercial or other free systems can use to
gain advantage over R.

> 
> > 2. there is too much material to absorb just to create a package.
> >     The manuals are insufficient.
> 
> The first sentence here is basically a repetition of "the process is too
> complex".  I think the second sentence is incorrect.  Could you please
> point out what necessary steps are missing?

Its not that anything is missing that I am aware of.  Its that there is so much 
detail one is overwhelmed.  Its not completely the fault of the description 
since as point #1 mentions the process itself is a key part of the 
problem.

> >
> > A step-by-step simplification is very much needed.
> 
> Exactly this has been in the Installation and Administration manual
> since I put it there in February for the 2.1.0 release.  It's at the
> beginning of the appendix on the Windows toolset, with multiple
> references pointing people there.  It's followed by detailed
> descriptions of each of the steps.
> 
> If you think it could be further improved, please submit improvements.

That is easy to say but, in fact, if anyone does this they are not met
with a receptive atmosphere.  The excellent post describing the process
that started this out (even if there are some small errors) is just one 
example.  

> 
>  >Its no
> > coincidence that there are a number of such descriptions on
> > the net (google for 'making creating R package') since I would
> > guess that just about everyone has significant problems in creating
> > their first package on Windows.
> 
> As far as I can tell, those all predate the release of 2.1.0.  I think
> your complaints are out of date.

I am sure the situation is getting better but I did look at the manuals
again before posting and do think that a step by step article such
as that in Ivy Li's post, the various documents on the net findable by google
as I mentioned and the proposed article by Uwe are really needed
in addition to the manuals.  The manuals can then be used to get
additional detail.

> 
> Duncan Murdoch
> 
> >
> >
> > On 7/21/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >> Ivy_Li wrote:
> >>
> >> > Dear All,
> >> >
> >> > With the warm support of every R expert, I have built my R library successfully.
> >> > Especially thanks: Duncan Murdoch
> >> >                   Gabor Grothendieck
> >> >                   Henrik Bengtsson
> >> >                   Uwe Ligges
> >>
> >> You are welcome.
> >>
> >>
> >> The following is intended for the records in the archive in order to
> >> protect readers.
> >>
> >>
> >> > Without your help, I will lower efficiency.
> >> > I noticed that some other friends were puzzled by the method of building library. Now, I organize a document about it. Hoping it can help more friends.
> >> >
> >> > 1. Read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> >>
> >> Do you mean http://www.murdoch-sutherland.com/Rtools/ ?
> >>
> >> > 2. Download the "rw2011.exe"; Install the newest version of R
> >> > 3. Download the "tools.zip"; Unpack it into c:\cygwin
> >>
> >> Not required to call it "cygwin" - also a bit misleading...
> >>
> >> > 4. Download the "ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active Perl in c:\Perl
> >>
> >> Why in C:\Perl ?
> >>
> >> > 5. Download the "MinGW-3.1.0-1.exe"; Install the mingw32 port of gcc in c:\mingwin
> >>
> >> Why in c:\mingwin ?
> >>
> >>
> >> > 6. Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue"; add "c:\cygwin;c:\mingwin\bin"
> >> >       The PATH variable already contains a couple of paths, add the two given above in front of all others, separated by ";".
> >> >       Why we add them in the beginning of the path? Because we want the folder that contains the tools to be at the beginning so that you eliminate the possibility of finding a different program of the same name first in a folder that comes prior to the one where the tools are stored.
> >>
> >>
> >> OK, this (1-6) is all described in the R Administration and Installation
> >> manual, hence I do not see why we have to repeat it here.
> >>
> >>
> >> > 7. I use the package.skeleton() function to make a draft package. It will automate some of the setup for a new source package. It creates directories, saves functions and    data to appropriate places, and creates skeleton help files and 'README' files describing further steps in packaging.
> >> > I type in R:
> >> >       >f <- function(x,y) x+y
> >> >       >g <- function(x,y) x-y
> >> >       >d <- data.frame(a=1, b=2)
> >> >       >e <- rnorm(1000)
> >> >       >package.skeleton(list=c("f","g","d","e"), name="example")
> >> > Then modify the 'DESCRIPTION':
> >> >       Package: example
> >> >       Version: 1.0-1
> >> >       Date: 2005-07-09
> >> >       Title: My first function
> >> >       Author: Ivy <Ivy_Li at smics.com>
> >> >       Maintainer: Ivy <Ivy_Li at smics.com>
> >> >       Description: simple sum and subtract
> >> >       License: GPL version 2 or later
> >> >       Depends: R (>= 1.9), stats, graphics, utils
> >> > You can refer to the web page: http://cran.r-project.org/src/contrib/Descriptions/  There are larger source of examples. And you can read the part of 'Creating R Packages' in 'Writing R Extension'. It introduces some useful things for your reference.
> >>
> >>
> >> This is described in Writing R Extension and is not related to the setup
> >> of you system in 1-6.
> >>
> >>
> >> >
> >> > 8. Download hhc.exe Microsoft help compiler from somewhere. And save it somewhere in your path.
> >> >     I download a 'htmlhelp.exe' and setup. saved the hhc.exe into the 'C:\cygwin\bin' because this path has been writen in my PATH Variable Balue.
> >> >     However if you decided not to use the Help Compiler (hhc), then you need to modify the MkRules file in RHOME/src/gnuwin32 to tell it not to try to build that kind of help file
> >>
> >>
> >> This is described in the R Administration and Installation manual
> >> and I do not see why we should put the html compiler to the other tools.
> >>
> >>
> >> > 9. In the DOS environment. Into the "D:\>"  Type the following code:
> >>
> >> There is no DOS environment in Windows NT based operating systems.
> >>
> >>
> >> >       cd \Program Files\R\rw2010
> >> >       bin\R CMD INSTALL "/Program Files/R/rw2011/example"
> >>
> >> I do not see why anybody would like to contaminate the binary
> >> installation of R with some development source packages.
> >> I'd rather use a separate directory.
> >>
> >> I think reading the two mentioned manuals shoul be sufficient. You have
> >> not added relevant information. By adding irrelevant information and
> >> omitting some relevant information, I guess we got something that is
> >> misleading if the reader does NOT read the manuals as well.
> >>
> >> Best,
> >> Uwe Ligges
> >>
> >>
> >> > Firstly, because I install the new version R in the D:\Program Files\. So I should first into the D drive. Secondly, because I use the package.skeleton() function to build 'example' package in the path of D:\Program Files\R\rw2011\  So I must tell R the path where saved the 'example' package. So I write the code is like that. If your path is different from me, you should modify part of these code.
> >> >
> >> > 10.Finally, this package is successfully built up.
> >> >
> >> >         ---------- Making package example ------------
> >> >         adding build stamp to DESCRIPTION
> >> >         installing R files
> >> >         installing data files
> >> >         installing man source files
> >> >         installing indices
> >> >         not zipping data
> >> >         installing help
> >> >        >>> Building/Updating help pages for package 'example'
> >> >            Formats: text html latex example chm
> >> >         d                                 text    html    latex   example
> >> >         e                                 text    html    latex   example
> >> >         f                                 text    html    latex   example
> >> >         g                                 text    html    latex   example
> >> >         adding MD5 sums
> >> >
> >> >       * DONE (example)
> >> >
> >> > I was very happy to get the great results. I hope the document can help you. Thank you again for everyone's support.
> >> >
> >> >
> >> > Best Regards!
> >> > Ivy Li
> >> > YMS in Production & Testing
> >> > Semiconductor Manufactory International(ShangHai) Corporation
> >> > #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> >> > Tel: 021-5080-2000 *11754
> >> > Email: Ivy_Li at smics.com
> >> >
> >> >
> >> >
> >> > ------------------------------------------------------------------------
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Emili.Tortosa at eco.uji.es  Thu Jul 21 18:12:18 2005
From: Emili.Tortosa at eco.uji.es (Emili Tortosa-Ausina)
Date: Thu, 21 Jul 2005 18:12:18 +0200
Subject: [R] opening RDB files
Message-ID: <5.1.1.6.0.20050721175855.013feb78@mail.uji.es>

Hi all,

I've recently upgraded to R version 2.1.1 and when trying to inspect the 
contents of many packages in the library (for instance library\MASS\R) I've 
realized wordpad, or the notepad, won't open them since they have *.RDB and 
*.RDX extensions which these editors cannot recognize.

However, libraries in previous versions of R did not have these extensions 
and I could inspect the contents of each package without any trouble.

I've been searching for this thread but did not find it.

Thank you!

Emili



From ligges at statistik.uni-dortmund.de  Thu Jul 21 18:41:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2005 18:41:20 +0200
Subject: [R] opening RDB files
In-Reply-To: <5.1.1.6.0.20050721175855.013feb78@mail.uji.es>
References: <5.1.1.6.0.20050721175855.013feb78@mail.uji.es>
Message-ID: <42DFD030.50604@statistik.uni-dortmund.de>

Emili Tortosa-Ausina wrote:

> Hi all,
> 
> I've recently upgraded to R version 2.1.1 and when trying to inspect the 
> contents of many packages in the library (for instance library\MASS\R) I've 
> realized wordpad, or the notepad, won't open them since they have *.RDB and 
> *.RDX extensions which these editors cannot recognize.
> 
> However, libraries in previous versions of R did not have these extensions 
> and I could inspect the contents of each package without any trouble.
> 
> I've been searching for this thread but did not find it.


Well, these are the lazy loading databases which have been introduced in 
R-1.9.0, AFAIR. There is a corresponding article in R News.

Just download the source package in order to look at the code.

Uwe Ligges



> Thank you!
> 
> Emili
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From david.crabb at ntu.ac.uk  Thu Jul 21 18:47:24 2005
From: david.crabb at ntu.ac.uk (Crabb, David)
Date: Thu, 21 Jul 2005 17:47:24 +0100
Subject: [R] normal reference intervals
Message-ID: <958023F70A782142AFA14D3E82F794183CDCF7@cherry.ads.ntu.ac.uk>

I am interested in calculating Age-Specific normal reverence intervals,
using non-parametric methods - or ideally something called the LMS
method (which as I understand it uses cubic splines fitted to the data).
Any packages in R that you think might help me? Any other advice
gratefully received.

Many thanks. Best wishes, David.

------------------------------------------------------------------------
-----
Dr. David Crabb
School of Biomedical and Natural Sciences,
Nottingham Trent University, Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690
 



This email is intended solely for the addressee.  It may contain private and confidential information.  If you are not the intended addressee, please take no action based on it nor show a copy to anyone.  In this case, please reply to this email to highlight the error.  Opinions and information in this email that do not relate to the official business of Nottingham Trent University shall be understood as neither given nor endorsed by the University.
Nottingham Trent University has taken steps to ensure that this email and any attachments are virus-free, but we do advise that the recipient should check that the email and its attachments are actually virus free.  This is in keeping with good computing practice.



From hb at maths.lth.se  Thu Jul 21 18:52:14 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 21 Jul 2005 18:52:14 +0200
Subject: [R] opening RDB files
In-Reply-To: <5.1.1.6.0.20050721175855.013feb78@mail.uji.es>
References: <5.1.1.6.0.20050721175855.013feb78@mail.uji.es>
Message-ID: <42DFD2BE.8030006@maths.lth.se>

Emili Tortosa-Ausina wrote:
> Hi all,
> 
> I've recently upgraded to R version 2.1.1 and when trying to inspect the 
> contents of many packages in the library (for instance library\MASS\R) I've 
> realized wordpad, or the notepad, won't open them since they have *.RDB and 
> *.RDX extensions which these editors cannot recognize.
> 
> However, libraries in previous versions of R did not have these extensions 
> and I could inspect the contents of each package without any trouble.

The *.rdb etc are the new compact package file formats introduced around 
R v2.0.0; these are binary files that won't make much sense to look at. 
   It was only in version before this you could inspect the R code by 
looking at the file <pkg>/R/<pkg>.R in a text file viewer/editor.

To look at the code now, you have to either download the source of the 
package you're interested in (look for the *.tar.gz files), or you can 
always do it from within R, e.g. print(read.table). If the the function 
you want to look at gives "UseMethod" and so on, you're looking at a 
generic function, e.g. print(print):

function (x, ...)
UseMethod("print")
<environment: namespace:base>

then you want to track down the method for your specific object. To find 
all implementation of "print", use methods(), e.g. methods(print):

   [1] print.acf*                       print.anova
   [3] print.aov*                       print.aovlist*
   [5] print.ar*                        print.Arima*
   [7] print.arima0*                    print.AsIs
   [9] print.Bibtex*                    print.by
<snip></snip>
[123] print.vignette*                  print.xgettext*
[125] print.xngettext*                 print.xtabs*

Then do, say, print(print.by) and you'll see the code. All method with 
an asterisk are namespace protected methods. To get these you have to 
use getAnywhere(), e.g. print(getAnywhere("print.acf")).

Why the new file format?  It is used for package that utilized lazy 
loading, which more and more package now use (packages without lazy 
loading can still be inspected the "old" way). Thanks to lazy loading, 
packages now loads more or less instantainously. They are also more 
memory efficient, because all code is not loaded at once.

Cheers

Henrik Bengtsson

> I've been searching for this thread but did not find it.
> 
> Thank you!
> 
> Emili
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From gilbert.wu at sabrefund.com  Thu Jul 21 18:56:55 2005
From: gilbert.wu at sabrefund.com (Gilbert Wu)
Date: Thu, 21 Jul 2005 17:56:55 +0100
Subject: [R] colnames
Message-ID: <C7FF4EF92D5A794EA5820C75CFB938F963035B@MAILSERVER.sabrefund.com>

Hi Adai,

Your diagnosis is absolutely right; class(r1) returned data.frame and your suggested solution worked perfectly. Your assumption is also right; both x and y are positive.

If I want to compare the performance of the my old function with yours, are there some functions in R I could use to get the elapsed time etc?

Many Thanks indeed.

Regards,

Gilbert

-----Original Message-----
From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
Sent: 19 July 2005 23:38
To: Gilbert Wu
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] colnames


What does class(r1) give you ? If it is "data.frame", then try 
  exp( diff( log( as.matrix( df ) ) ) )

BTW, I made the assumption that both x and y are positive values only.

Regards, Adai



On Tue, 2005-07-19 at 16:30 +0100, Gilbert Wu wrote:
> Hi Adai,
> 
> When I tried the optimized routine, I got the following error message:
> 
> r1
>          899188 902232   901714 28176U 15322M
> 20050713  7.595  10.97 17.96999 5.1925  11.44
> 20050714  7.605  10.94 18.00999 5.2500  11.50
> 20050715  7.480  10.99 17.64999 5.2500  11.33
> 20050718  7.415  11.05 17.64000 5.2250  11.27
> > exp(diff(log(r1))) -1
> Error in r[i1] - r[-length(r):-(length(r) - lag + 1)] : 
>         non-numeric argument to binary operator
> >
> 
> Any idea?
> 
> Many Thanks.
> 
> Gilbert
> -----Original Message-----
> From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> Sent: 19 July 2005 12:20
> To: Gilbert Wu
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] colnames
> 
> 
> First, your problem could be boiled down to the following example. See
> how the colnames of the two outputs vary.
> 
> df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
> df/df
>   X100 X200
> 1    1    1
> 2    1    1
> 
> m  <- as.matrix( df )   # coerce to matrix class
> m/m
>   100 200
> 1   1   1
> 2   1   1
> 
> It appears that whenever R has to create a new dataframe automatically,
> it tries to get nice colnames. See help(data.frame). I am not exactly
> sure why this behaviour is different when creating a matrix. But I do
> not think this is a major problem for most people. If you coerce your
> input to matrix, the problem goes away.
> 
> 
> Next, note the following points :
>  a) "mat[ 1:3, 1:ncol(mat) ]" is equivalent to simply "mat[ 1:3,  ]". 
>  b) "mat[ 2:nrow(mat), ]" is equivalent to simply "mat[ -1,  ]"
> See help(subset) for more information.
> 
> Using the points above, we can simplify your function as 
> 
>  p.RIs2Returns <- function (mat){
> 
>    mat <- as.matrix(mat)
>    x <- mat[ -nrow(mat), ]
>    y <- mat[ -1, ]
>   
>    return( y/x -1 )
>  }
> 
> If your data contains only numerical data, it is probably good idea to
> work with matrices as matrix operations are faster.
> 
> 
> Finally, we can shorten your function. You can use the diff (which works
> column-wise if input is a matrix) and apply function if you know that 
> 
> 	y/x  =  exp(log(y/x))  =  exp( log(y) - log(x) )
> 
> which could be coded in R as
> 
> 	exp( diff( log(r1) ) )
> 
> and then subtract 1 from above to get your returns.
> 
> Regards, Adai
> 
> 
> 
> On Tue, 2005-07-19 at 09:17 +0100, Gilbert Wu wrote:
> > Hi Adai,
> > 
> > Many Thanks for the examples.
> > 
> > I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.
> > 
> > The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.
> > 
> > I would welcome any suggestion to improve the script.
> > 
> > 
> > Regards,
> > 
> > Gilbert
> > 
> > > "p.RIs2Returns" <-
> > + function (RIm)
> > + {
> > + x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
> > + y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
> > + RReturns <- (y/x -1)
> > + RReturns
> > + }
> > > 
> > > 
> > > channel<-odbcConnect("ourSQLDB")
> > > result<-sqlQuery(channel,paste("select * from equityRIs;"))
> > > odbcClose(channel)
> > > result
> >    stockid    sdate  dbPrice
> > 1   899188 20050713  7.59500
> > 2   899188 20050714  7.60500
> > 3   899188 20050715  7.48000
> > 4   899188 20050718  7.41500
> > 5   902232 20050713 10.97000
> > 6   902232 20050714 10.94000
> > 7   902232 20050715 10.99000
> > 8   902232 20050718 11.05000
> > 9   901714 20050713 17.96999
> > 10  901714 20050714 18.00999
> > 11  901714 20050715 17.64999
> > 12  901714 20050718 17.64000
> > 13  28176U 20050713  5.19250
> > 14  28176U 20050714  5.25000
> > 15  28176U 20050715  5.25000
> > 16  28176U 20050718  5.22500
> > 17  15322M 20050713 11.44000
> > 18  15322M 20050714 11.50000
> > 19  15322M 20050715 11.33000
> > 20  15322M 20050718 11.27000
> > > r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> > > r1
> >      sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> > 1 20050713          7.595          10.97       17.96999         5.1925          11.44
> > 2 20050714          7.605          10.94       18.00999         5.2500          11.50
> > 3 20050715          7.480          10.99       17.64999         5.2500          11.33
> > 4 20050718          7.415          11.05       17.64000         5.2250          11.27
> > > #Set sdate as the rownames
> > > rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> > > #Get rid of the first column
> > > r1 <- r1[1:nrow(r1),2:ncol(r1)]
> > > r1
> >          dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> > 20050713          7.595          10.97       17.96999         5.1925          11.44
> > 20050714          7.605          10.94       18.00999         5.2500          11.50
> > 20050715          7.480          10.99       17.64999         5.2500          11.33
> > 20050718          7.415          11.05       17.64000         5.2250          11.27
> > > colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> > > r1
> >          899188 902232   901714 28176U 15322M
> > 20050713  7.595  10.97 17.96999 5.1925  11.44
> > 20050714  7.605  10.94 18.00999 5.2500  11.50
> > 20050715  7.480  10.99 17.64999 5.2500  11.33
> > 20050718  7.415  11.05 17.64000 5.2250  11.27
> > > RRs<-p.RIs2Returns(r1)
> > > RRs
> >               X899188      X902232      X901714      X28176U      X15322M
> > 20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
> > 20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
> > 20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
> > > 
> > 
> 
>



From andy_liaw at merck.com  Thu Jul 21 18:59:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Jul 2005 12:59:33 -0400
Subject: [R] RandomForest question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAE6@usctmx1106.Merck.com>

See the tuneRF() function in the package for an implementation of 
the strategy recommended by Breiman & Cutler.

BTW, "randomForest" is only for the R package.  See Breiman's 
web page for notice on trademarks.

Andy

> From: Weiwei Shi 
> 
> Hi,
> I found the following lines from Leo's randomForest, and I am not sure
> if it can be applied here but just tried to help:
> 
> mtry0 = the number of variables to split on at each node. Default is
> the square root of mdim. ATTENTION! DO NOT USE THE DEFAULT VALUES OF
> MTRY0 IF YOU WANT TO OPTIMIZE THE PERFORMANCE OF RANDOM FORESTS. TRY
> DIFFERENT VALUES-GROW 20-30 TREES, AND SELECT THE VALUE OF MTRY THAT
> GIVES THE SMALLEST OOB ERROR RATE.
> 
> mdim is the number of predicators.
> 
> HTH,
> 
> weiwei
> 
> On 7/21/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > From: Arne.Muller at sanofi-aventis.com
> > >
> > > Hello,
> > >
> > > I'm trying to find out the optimal number of splits (mtry
> > > parameter) for a randomForest classification. The
> > > classification is binary and there are 32 explanatory
> > > variables (mostly factors with each up to 4 levels but also
> > > some numeric variables) and 575 cases.
> > >
> > > I've seen that although there are only 32 explanatory
> > > variables the best classification performance is reached when
> > > choosing mtry=80. How is it possible that more variables can
> > > used than there are in columns the data frame?
> > 
> > It's not.  The code for randomForest.default() has:
> > 
> >     ## Make sure mtry is in reasonable range.
> >     mtry <- max(1, min(p, round(mtry)))
> > 
> > so it silently sets mtry to number of predictors if it's too large.
> > As an example:
> > 
> > > library(randomForest)
> > randomForest 4.5-12
> > Type rfNews() to see new features/changes/bug fixes.
> > > iris.rf = randomForest(Species ~ ., iris, mtry=10)
> > > iris.rf$mtry
> > [1] 4
> > 
> > I should probably add a warning in such cases...
> > 
> > Andy
> > 
> > 
> > >       thanks for your help
> > >       + kind regards,
> > >
> > >       Arne
> > >
> > >
> > >
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> 
>



From efg at stowers-institute.org  Thu Jul 21 19:11:29 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 21 Jul 2005 12:11:29 -0500
Subject: [R] R graphics
References: <42DFA00E.2090506@rdg.ac.uk>
Message-ID: <dbol02$cda$1@sea.gmane.org>

"Sam Baxter" <s.j.baxter at reading.ac.uk> wrote in message
news:42DFA00E.2090506 at rdg.ac.uk...
> I am trying to set up 16 graphs on one graphics page in R. I have used
> the mfrow=c(4,4) command. However I get a lot of white space between
> each graph. Does anyone know how I can reduce this?

The default par()$mar is c(5,4,4,2) + 0.1 and can be reduced.

For example:
par(mfrow=c(4,4), mar=c(3,3,0,0))

for (i in 1:16)
{
  plot(0:10)
}

efg
--
Earl F. Glynn
Bioinformatics Department
Stowers Institute for Medical Research



From ramasamy at cancer.org.uk  Thu Jul 21 19:15:46 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 21 Jul 2005 18:15:46 +0100
Subject: [R] colnames
In-Reply-To: <C7FF4EF92D5A794EA5820C75CFB938F963035B@MAILSERVER.sabrefund.com>
References: <C7FF4EF92D5A794EA5820C75CFB938F963035B@MAILSERVER.sabrefund.com>
Message-ID: <1121966146.6531.0.camel@ramasamy.stats>

See help(system.time).


On Thu, 2005-07-21 at 17:56 +0100, Gilbert Wu wrote:
> Hi Adai,
> 
> Your diagnosis is absolutely right; class(r1) returned data.frame and your suggested solution worked perfectly. Your assumption is also right; both x and y are positive.
> 
> If I want to compare the performance of the my old function with yours, are there some functions in R I could use to get the elapsed time etc?
> 
> Many Thanks indeed.
> 
> Regards,
> 
> Gilbert
> 
> -----Original Message-----
> From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> Sent: 19 July 2005 23:38
> To: Gilbert Wu
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] colnames
> 
> 
> What does class(r1) give you ? If it is "data.frame", then try 
>   exp( diff( log( as.matrix( df ) ) ) )
> 
> BTW, I made the assumption that both x and y are positive values only.
> 
> Regards, Adai
> 
> 
> 
> On Tue, 2005-07-19 at 16:30 +0100, Gilbert Wu wrote:
> > Hi Adai,
> > 
> > When I tried the optimized routine, I got the following error message:
> > 
> > r1
> >          899188 902232   901714 28176U 15322M
> > 20050713  7.595  10.97 17.96999 5.1925  11.44
> > 20050714  7.605  10.94 18.00999 5.2500  11.50
> > 20050715  7.480  10.99 17.64999 5.2500  11.33
> > 20050718  7.415  11.05 17.64000 5.2250  11.27
> > > exp(diff(log(r1))) -1
> > Error in r[i1] - r[-length(r):-(length(r) - lag + 1)] : 
> >         non-numeric argument to binary operator
> > >
> > 
> > Any idea?
> > 
> > Many Thanks.
> > 
> > Gilbert
> > -----Original Message-----
> > From: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> > Sent: 19 July 2005 12:20
> > To: Gilbert Wu
> > Cc: r-help at stat.math.ethz.ch
> > Subject: RE: [R] colnames
> > 
> > 
> > First, your problem could be boiled down to the following example. See
> > how the colnames of the two outputs vary.
> > 
> > df <- cbind.data.frame( "100"=1:2, "200"=3:4 )
> > df/df
> >   X100 X200
> > 1    1    1
> > 2    1    1
> > 
> > m  <- as.matrix( df )   # coerce to matrix class
> > m/m
> >   100 200
> > 1   1   1
> > 2   1   1
> > 
> > It appears that whenever R has to create a new dataframe automatically,
> > it tries to get nice colnames. See help(data.frame). I am not exactly
> > sure why this behaviour is different when creating a matrix. But I do
> > not think this is a major problem for most people. If you coerce your
> > input to matrix, the problem goes away.
> > 
> > 
> > Next, note the following points :
> >  a) "mat[ 1:3, 1:ncol(mat) ]" is equivalent to simply "mat[ 1:3,  ]". 
> >  b) "mat[ 2:nrow(mat), ]" is equivalent to simply "mat[ -1,  ]"
> > See help(subset) for more information.
> > 
> > Using the points above, we can simplify your function as 
> > 
> >  p.RIs2Returns <- function (mat){
> > 
> >    mat <- as.matrix(mat)
> >    x <- mat[ -nrow(mat), ]
> >    y <- mat[ -1, ]
> >   
> >    return( y/x -1 )
> >  }
> > 
> > If your data contains only numerical data, it is probably good idea to
> > work with matrices as matrix operations are faster.
> > 
> > 
> > Finally, we can shorten your function. You can use the diff (which works
> > column-wise if input is a matrix) and apply function if you know that 
> > 
> > 	y/x  =  exp(log(y/x))  =  exp( log(y) - log(x) )
> > 
> > which could be coded in R as
> > 
> > 	exp( diff( log(r1) ) )
> > 
> > and then subtract 1 from above to get your returns.
> > 
> > Regards, Adai
> > 
> > 
> > 
> > On Tue, 2005-07-19 at 09:17 +0100, Gilbert Wu wrote:
> > > Hi Adai,
> > > 
> > > Many Thanks for the examples.
> > > 
> > > I work for a financial institution. We are exploring R as a tool to implement our portfolio optimization strategies. Hence, R is still a new language to us.
> > > 
> > > The script I wrote tried to make a returns matrix from the daily return indices extracted from a SQL database. Please find below the output that produces the 'X' prefix in the colnames. The reason to preserve the column names is that they are stock identifiers which are to be used by other sub systems rather than R.
> > > 
> > > I would welcome any suggestion to improve the script.
> > > 
> > > 
> > > Regards,
> > > 
> > > Gilbert
> > > 
> > > > "p.RIs2Returns" <-
> > > + function (RIm)
> > > + {
> > > + x<-RIm[1:(nrow(RIm)-1), 1:ncol(RIm)]
> > > + y<-RIm[2:nrow(RIm), 1:ncol(RIm)]
> > > + RReturns <- (y/x -1)
> > > + RReturns
> > > + }
> > > > 
> > > > 
> > > > channel<-odbcConnect("ourSQLDB")
> > > > result<-sqlQuery(channel,paste("select * from equityRIs;"))
> > > > odbcClose(channel)
> > > > result
> > >    stockid    sdate  dbPrice
> > > 1   899188 20050713  7.59500
> > > 2   899188 20050714  7.60500
> > > 3   899188 20050715  7.48000
> > > 4   899188 20050718  7.41500
> > > 5   902232 20050713 10.97000
> > > 6   902232 20050714 10.94000
> > > 7   902232 20050715 10.99000
> > > 8   902232 20050718 11.05000
> > > 9   901714 20050713 17.96999
> > > 10  901714 20050714 18.00999
> > > 11  901714 20050715 17.64999
> > > 12  901714 20050718 17.64000
> > > 13  28176U 20050713  5.19250
> > > 14  28176U 20050714  5.25000
> > > 15  28176U 20050715  5.25000
> > > 16  28176U 20050718  5.22500
> > > 17  15322M 20050713 11.44000
> > > 18  15322M 20050714 11.50000
> > > 19  15322M 20050715 11.33000
> > > 20  15322M 20050718 11.27000
> > > > r1<-reshape(result, timevar="stockid", idvar="sdate", direction="wide")
> > > > r1
> > >      sdate dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> > > 1 20050713          7.595          10.97       17.96999         5.1925          11.44
> > > 2 20050714          7.605          10.94       18.00999         5.2500          11.50
> > > 3 20050715          7.480          10.99       17.64999         5.2500          11.33
> > > 4 20050718          7.415          11.05       17.64000         5.2250          11.27
> > > > #Set sdate as the rownames
> > > > rownames(r1) <-as.character(r1[1:nrow(r1),1:1])
> > > > #Get rid of the first column
> > > > r1 <- r1[1:nrow(r1),2:ncol(r1)]
> > > > r1
> > >          dbPrice.899188 dbPrice.902232 dbPrice.901714 dbPrice.28176U dbPrice.15322M
> > > 20050713          7.595          10.97       17.96999         5.1925          11.44
> > > 20050714          7.605          10.94       18.00999         5.2500          11.50
> > > 20050715          7.480          10.99       17.64999         5.2500          11.33
> > > 20050718          7.415          11.05       17.64000         5.2250          11.27
> > > > colnames(r1) <- as.character(sub("[[:alnum:]]*\\.","", colnames(r1)))
> > > > r1
> > >          899188 902232   901714 28176U 15322M
> > > 20050713  7.595  10.97 17.96999 5.1925  11.44
> > > 20050714  7.605  10.94 18.00999 5.2500  11.50
> > > 20050715  7.480  10.99 17.64999 5.2500  11.33
> > > 20050718  7.415  11.05 17.64000 5.2250  11.27
> > > > RRs<-p.RIs2Returns(r1)
> > > > RRs
> > >               X899188      X902232      X901714      X28176U      X15322M
> > > 20050714  0.001316656 -0.002734731  0.002225933  0.011073664  0.005244755
> > > 20050715 -0.016436555  0.004570384 -0.019988906  0.000000000 -0.014782609
> > > 20050718 -0.008689840  0.005459509 -0.000566006 -0.004761905 -0.005295675
> > > > 
> > > 
> > 
> > 
> 
>



From deepayan.sarkar at gmail.com  Thu Jul 21 19:36:07 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 21 Jul 2005 12:36:07 -0500
Subject: [R] help with a xyplot legend
In-Reply-To: <200507201744.55343.chrysopa@insecta.ufv.br>
References: <200507201744.55343.chrysopa@insecta.ufv.br>
Message-ID: <eb555e66050721103613bc2502@mail.gmail.com>

On 7/20/05, Ronaldo Reis-Jr. <chrysopa at insecta.ufv.br> wrote:
> Hi,
> 
> I try to put a legend in a xyplot graphic.
> 
> xyplot(y~x|g,ylim=c(0,80),xlim=c(10,40),as.table=T,layout=c(2,3), ylab="N??mero
> de machos capturados",xlab=expression(paste("Temperatura (",degree,"C)")),
> key=list(corner=c(0,0),x=0, y=0, text=list(legenda),lines=list(col=cor, lwd =
> espessura, lty=linha),columns=7,between=0.5,betwen.columns=0.5,cex=0.8))
> 
> The problem is that legend is very close do xlab. I try change
> corner=c(0,0),x=0, y=0, to corner=c(0,0),x=0, y=1, but in this way the legend
> dont appear.
> 
> How to make the vertical bottom area of the plot bigger to put the legend a
> bit separated of the xlabel?

Where exactly do you want the legend? If it's outside and below the
plot, you should try
space="bottom" instead of x,y,corner, etc. Otherwise, with
space="inside", no attempt will be made to save space for the legend.

Deepayan



From murdoch at stats.uwo.ca  Thu Jul 21 19:38:15 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 21 Jul 2005 13:38:15 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <971536df05072109035a3948b8@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>	
	<42DF9BC9.9050508@statistik.uni-dortmund.de>	
	<971536df0507210643438a80ef@mail.gmail.com>	
	<42DFBE0A.3020803@stats.uwo.ca>
	<971536df05072109035a3948b8@mail.gmail.com>
Message-ID: <42DFDD87.8070409@stats.uwo.ca>

On 7/21/2005 12:03 PM, Gabor Grothendieck wrote:

>> 
>> > 2. there is too much material to absorb just to create a package.
>> >     The manuals are insufficient.
>> 
>> The first sentence here is basically a repetition of "the process is too
>> complex".  I think the second sentence is incorrect.  Could you please
>> point out what necessary steps are missing?
> 
> Its not that anything is missing that I am aware of.  Its that there is so much 
> detail one is overwhelmed.  Its not completely the fault of the description 
> since as point #1 mentions the process itself is a key part of the 
> problem.

And what solution do you propose to this problem?  Saying this ought to
be a high priority for the core group is not a solution.  Tell us where
the resources will come from to do this.

>> > A step-by-step simplification is very much needed.
>> 
>> Exactly this has been in the Installation and Administration manual
>> since I put it there in February for the 2.1.0 release.  It's at the
>> beginning of the appendix on the Windows toolset, with multiple
>> references pointing people there.  It's followed by detailed
>> descriptions of each of the steps.
>> 
>> If you think it could be further improved, please submit improvements.
> 
> That is easy to say but, in fact, if anyone does this they are not met
> with a receptive atmosphere.  The excellent post describing the process
> that started this out (even if there are some small errors) is just one 
> example.  

I don't think that post was written with the intention of putting it
into the manual.  It would still take a fair bit of work to do that:

 1. deciding where it fits and what to replace,
 2. correcting the errors,
 3. writing it in texinfo format.

I'd be happy to talk with someone who volunteers to do that.  (I'd
suggest the volunteer should do number 1 first, so as not to waste a lot
of time on versions that don't fit.)

Duncan Murdoch



From rachel at notalker.com  Thu Jul 21 19:41:16 2005
From: rachel at notalker.com (Rachel Lomasky)
Date: Thu, 21 Jul 2005 13:41:16 -0400
Subject: [R] problem running R with perl Statistics::R
Message-ID: <42DFDE3C.60504@notalker.com>



From rachel at notalker.com  Thu Jul 21 19:41:51 2005
From: rachel at notalker.com (Rachel Lomasky)
Date: Thu, 21 Jul 2005 13:41:51 -0400
Subject: [R] Using Perl Statistics::R module, not running the
Message-ID: <42DFDE5F.4020803@notalker.com>



From dmb at mrc-dunn.cam.ac.uk  Thu Jul 21 20:55:16 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 21 Jul 2005 19:55:16 +0100 (BST)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <17119.41107.864470.790867@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.21.0507211926400.23083-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 21 Jul 2005, Christoph Buser wrote:

>Dear Dan
>
>I can only help you with your third problem, expression and
>paste. You can use:
>
>plot(1:5,1:5, type = "n")
>text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
>text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
>text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)
>

Cheers for this.

I was trying to get it to work, but the problem is that I need to replace
the values above with variables, from the following code...


dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
dat.lm.sum <- summary(dat.lm)

my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
my.slope.2 <- round(dat.lm.sum$coefficients[4],2)

my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
my.inter.2 <- round(dat.lm.sum$coefficients[3],2)

my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)


Anything I try results in either the words 'paste("Slope:", my.slope.1,
%+-%my.slope.2,sep="")' being written to the plot, or just
'my.slope.1+-my.slope2' (where the +- is correctly written).

I want to script it up and write all three lines to the plot with
'sep="\n"', rather than deciding three different heights.


>I do not have an elegant solution for the alignment.

Thanks very much for what you gave, its a good start for me to figure out 
how I am supposed to be telling R what to do!

Any way to just get fixed width fonts with text? (for the alignment
problem)


Cheers,
Dan.

>
>Regards,
>
>Christoph Buser
>
>--------------------------------------------------------------
>Christoph Buser <buser at stat.math.ethz.ch>
>Seminar fuer Statistik, LEO C13
>ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
>phone: x-41-44-632-4673		fax: 632-1228
>http://stat.ethz.ch/~buser/
>--------------------------------------------------------------
>
>
>Dan Bolser writes:
> > 
> > I would like to annotate my plot with a little box containing the slope,
> > intercept and R^2 of a lm on the data.
> > 
> > I would like it to look like...
> > 
> >  +----------------------------+
> >  | Slope     :   3.45 +- 0.34 |
> >  | Intercept : -10.43 +- 1.42 |
> >  | R^2       :   0.78         |
> >  +----------------------------+
> > 
> > However I can't make anything this neat, and I can't find out how to
> > combine this with symbols for R^2 / +- (plus minus).
> > 
> > Below is my best attempt (which is franky quite pour). Can anyone
> > improve on the below?
> > 
> > Specifically, 
> > 
> > aligned text and numbers, 
> > aligned decimal places, 
> > symbol for R^2 in the text (expression(R^2) seems to fail with
> > 'paste') and +- 
> > 
> > 
> > 
> > Cheers,
> > Dan.
> > 
> > 
> > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> > 
> > abline(coef(dat.lm),lty=2,lwd=1.5)
> > 
> > 
> > dat.lm.sum <- summary(dat.lm)
> > dat.lm.sum
> > 
> > attributes(dat.lm.sum)
> > 
> > my.text.1 <-
> >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
> >         "+/-",          round(dat.lm.sum$coefficients[4],2))
> > 
> > my.text.2 <-
> >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
> >         "+/-",          round(dat.lm.sum$coefficients[3],2))
> > 
> > my.text.3 <-
> >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
> > 
> > my.text.1
> > my.text.2
> > my.text.3
> > 
> > 
> > ## Add legend
> > text(x=3,
> >      y=300,
> >      paste(my.text.1,
> >            my.text.2,
> >            my.text.3,
> >            sep="\n"),
> >      adj=c(0,0),
> >      cex=1)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sean.oriordain at gmail.com  Thu Jul 21 21:24:28 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Thu, 21 Jul 2005 20:24:28 +0100
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DFC418.9040406@statistik.uni-dortmund.de>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFB148.6090907@statistik.uni-dortmund.de>
	<42DFBEEE.6030000@stats.uwo.ca>
	<42DFC418.9040406@statistik.uni-dortmund.de>
Message-ID: <8ed68eed050721122458ebfaf8@mail.gmail.com>

Just a few thoughts... Good documentation helps everybody - the
beginners and the experts (less beginner questions if there is
thorough and accessible documentation.  I fully appreciate that this
is a volunteer effort - I'm just trying to pin down some places where
we have documentation issues.

Docs can be in a number of different forms - reference, examples,
carefully and throughly explained.  I personally find it difficult to
understand reference type material until I have seen a worked example,
and some of the reference material is a little light on the examples
for me, and others like me who thrive on examples.  The Linux-HOWTO
collection are a good example of step-by-step documentation... If
you're an expert, then you can read it fast and skip... otherwise you
read every line.

Since R comes as a computer "package" and a statistics "thingy"... the
questions on this list come in three forms - those that are very
'package', e.g. "how do I reduce the space between two graphs" - to
the statistics questions "how reliable is the coefficient of
determination in the presence of outliers" (which shouldn't really be
asked here), and then the how do I do 'statistic X' in R - "how do I
calculate a confidence interval around the coefficent of determination
in R?".

The standard documentation got me so far in learning about R - I got a
copy of MASS, S-Programming, "Introductory Statistics with R", and
Michael Crawley's new book "Statistics : An Introduction using R" -
along with every online book I could find on CRAN and elsewhere. 
Unfortunately for me, my experience leaves me in between the beginner
books and the more advanced texts like MASS, David, Schervish etc...
The learning curve is steep - but then like many people, I'd like to
be able to do sophisticated modelling with deep understanding and no
effort :-)

I feel there is a bit of a hole in the middle of the documentation
which could be attacked from both sides - the introduction element is
starting to be covered - it's the next step up from that.

And yes before you ask I would like to help - but my statistics
knowledge is very poor!  Should this conversation go to r-devel?

Thanks for listening,
Sean

On 21/07/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Duncan Murdoch wrote:
> 
> > On 7/21/2005 10:29 AM, Uwe Ligges wrote:
> >
> >>Gabor Grothendieck wrote:
> >>
> >>
> >>>I think you have been using R too long.  Something like
> >>>this is very much needed.  There are two problems:
> >>>
> >>>1. the process itself is too complex (need to get rid of perl,
> >>>    integrate package development tools with package installation
> >>>   procedure [it should be as easy as downloading a package],
> >>>   remove necessity to set or modify any environment variables
> >>>    including the path variables).
> >>>
> >>>2. there is too much material to absorb just to create a package.
> >>>    The manuals are insufficient.
> >>>
> >>>A step-by-step simplification is very much needed.  Its no
> >>>coincidence that there are a number of such descriptions on
> >>>the net (google for 'making creating R package') since I would
> >>>guess that just about everyone has significant problems in creating
> >>>their first package on Windows.
> >>
> >>OK, if people really think this is required, I will sit down on a clean
> >>Windows XP machine, do the setup, and write it down for the next R Help
> >>Desk in R News -- something like "Creating my first R package under
> >>Windows"?
> >>
> >>If anybody else is willing to contribute and can write something up in a
> >>manner that is *not* confusing or misleading (none of the other material
> >>spread over the web satisfies this requirement, AFAICS), she/he is
> >>invited to contribute, of course.
> >>
> >>BTW, everybody else is invited to submit proposals for R Help Desk!!!
> >
> >
> > That sounds great.  Could you also take notes as you go about specific
> > problems in the writeup in the R-admin manual, so it can be improved for
> > the next release?
> 
> Of course.
> 
> 
> > Another thing you could do which would be valuable:  get a student or
> > someone else who is reasonably computer literate, but unfamiliar with R
> > details, to do this while you sit watching and recording their mistakes.
> 
> Good idea.
> 
> Uwe
> 
> 
> > Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lobrien at icoria.com  Thu Jul 21 22:04:45 2005
From: lobrien at icoria.com (O'Brien, Laura)
Date: Thu, 21 Jul 2005 16:04:45 -0400
Subject: [R] unable to call R t-test from Java
Message-ID: <9F19C13E789D6E45A144CF22EDA28E3E046EFF@rtp-exchng02.paradigm.paragen.com>

In my last post, I left off version info:

Java: 	jdk1.5.0_03
R: 		2.1.1
SJava:	0.68
OS:		SunOs 5.8

By trying either the eval or the call method to execute a t-test results in a core dump.


e.g.  eval method

	 /* produces a core */
        System.err.println("eval a t.test");
        Object value = e.eval("t.test (c(1,2,3), c(4,5,6))");
        if (value != null)
            interp.show(value);

e.g   call method

        Object[] funArgs = new Object[2];
        double[] d0 = { 1.1, 2.2, 3.3};
        double[] d1 = { 9.9, 8.8, 7.7};

        funArgs[0] = d0;
        funArgs[1] = d1;
        
         System.err.println("\r\n Calling t.test and passing a java array");
   
         Object value =  e.call("t.test", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }

Thanks,
Laura

-----Original Message-----
From: O'Brien, Laura 
Sent: Wednesday, July 20, 2005 10:42 AM
To: 'r-help at stat.math.ethz.ch'
Subject: unable to call R t-test from Java


Hello,

My colleague and I would like to write Java code that invokes R to do a simple TTest.  I've included my sample java code below.  I tried various alternatives and am unable to pass a vector to the TTest method.  In my investigation, I tried to call other R methods that take vectors and also ran into various degrees of failure.   Any insight you can provide or other Web references you can point me to would be appreciated.

Thank you,
Laura O'Brien
Application Architect



---------------------------  code   ------------------------------


package org.omegahat.R.Java.Examples;

import org.omegahat.R.Java.ROmegahatInterpreter;
import org.omegahat.R.Java.REvaluator;


public class JavaRCall2
{

    /**
     * want to see if I can eval a t.test command like what I would run in the
     * R command line
     */

    static public void runTTestByEval_cores(REvaluator e, ROmegahatInterpreter interp)
    {
        /* produces a core */
        System.err.println("eval a t.test");
        Object value = e.eval("t.test (c(1,2,3), c(4,5,6))");
        if (value != null)
            interp.show(value);
    }

    
    /**
     * want to see if I can eval anything that takes a vector, e.g. mean, 
     * like what I would run in the R command line
     */

    static public void runMeanByEval_works(REvaluator e, ROmegahatInterpreter interp)
    {
        System.err.println("\r\n  evaluation string mean command");
        Object value = e.eval("mean(c(1,2,3))");
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }    
    }

/**
 *  if I pass mean a org.omegahat.Environment.DataStructures.numeric what do I get?  NaN
 */
 
    static public void runMeanByNumericList_nan(REvaluator e, ROmegahatInterpreter interp)
    {    
         Object[] funArgs = new Object[1];
         // given argument is not numeric or logical
         
         org.omegahat.Environment.DataStructures.numeric rList1 = new org.omegahat.Environment.DataStructures.numeric(3);       
    
         double[] dList = new double[3];
         dList[0] = (double) 1.1;
         dList[1] = (double) 2.2; 
         dList[2] = (double) 3.3;
         rList1.setData(dList, true);
         System.err.println(rList1.toString());

         funArgs[0] = rList1 ;
         
         System.err.println("\r\n Calling mean and passing an omegahat vector");

    
         Object value =  e.call("mean", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }

    }
    
    /**
     * let's run some tests on the vector passed in and see what R thinks I'm handing it
     * 
     * it returns 
     * isnumeric:     false
     * mode:          list
     * length:        2
     */

    public static void runTestsOnOmegahatNumeric(REvaluator e, ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[1];
        // given argument is not numeric or logical
         
        org.omegahat.Environment.DataStructures.numeric rList1 = new org.omegahat.Environment.DataStructures.numeric(3);       
    
        double[] dList = new double[3];
        dList[0] = (double) 1.1;
        dList[1] = (double) 2.2; 
        dList[2] = (double) 3.3;
        rList1.setData(dList, true);
        System.err.println(rList1.toString());

        funArgs[0] = rList1 ;
         
        System.err.println("\r\n Calling isnumeric and passing an omegahat vector");
   
        Object value =  e.call("is.numeric", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

        // mode is list

        System.err.println("\r\n Calling mode and passing an omegahat vector");
   
        value =  e.call("mode", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

        System.err.println("\r\n Calling length and passing an omegahat vector");
        System.err.println("\r\n");
        System.err.println("INTERESTING:  thinks the length is 2!");

   
        value =  e.call("length", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }

    }

    /**
     * run the mean on a java array.  Does it also return NAN?  
     * It returns a different value than the mean -- 2.19999... instead of 2.2
     */

    static public void runMeanOnJavaArray(REvaluator e, ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[1];
        double[] d = { 1.1, 2.2, 3.3};
        funArgs[0] = d;
        System.err.println("\r\n Calling mean of (1.1, 2.2, 3.3) and passing a java array\r\n");
        System.err.println("INTERSTING:  thinks the mean is 2.19999... it should be 2.2 \r\n");
   
        Object value =  e.call("mean", funArgs); 
        if(value != null) 
        {
            interp.show(value ); 
            System.err.println("\r\n");
        }
    }

    /**
     *    This is what I really want! 
     */

    static public void runTTestOnJavaArray_cores(REvaluator e, ROmegahatInterpreter interp)
    {
        Object[] funArgs = new Object[2];
        double[] d0 = { 1.1, 2.2, 3.3};
        double[] d1 = { 9.9, 8.8, 7.7};

        funArgs[0] = d0;
        funArgs[1] = d1;
        
         System.err.println("\r\n Calling t.test and passing a java array");
   
         Object value =  e.call("t.test", funArgs); 
         if(value != null) 
         {
         interp.show(value ); 
         System.err.println("\r\n");
         }
        
    }

    /**
     * 
     */
    static public void main(String[] args) 
    {
        ROmegahatInterpreter interp = new ROmegahatInterpreter(ROmegahatInterpreter.fixArgs(args), false);
        REvaluator e = new REvaluator();

        Object[] funArgs;
        String[] objects;
        Object value;
        int i;

        /** GOAL:  pass a vector of numbers into a t-test
        */
         
            // unfortunately it core dumps
            runTTestOnJavaArray_cores(e, interp);   


        /*         since I've been unable to get that work, 
        *         I try various evaluation commands to see if I 
        *         can get any sort of R commands that take vectors to work
        *         in any sort of fashion
        */
          

        //
        //   SAMPLE eval based calls
        //
          
        //  can I successfully invoke a t.test command similar to what I can
        //  do via the command line, e.g. t.test (c(1,2,3), c(4,5,6))
        //  NO -- this core dumps
        // runTTestByEval_cores(e, interp);
            
        // can I eval anything that takes a vector -- let's try the mean method
        // yes this works!
            //runMeanByEval_works(e, interp);
            
          
        //
        //    SAMPLE org.omegahat.Environment.DataStructures.numeric 
        //

        // this returns Nan
          
        // runMeanByNumericList_nan(e, interp);

        // given that the above returns NaN, what does R think I'm passing it for a vector?
        // let's run some tests
        runTestsOnOmegahatNumeric(e, interp);
          
        // now let's try some tests on a java array instead of an omegahat numeric
        runMeanOnJavaArray(e, interp);
    }
}



From tlumley at u.washington.edu  Thu Jul 21 22:06:30 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 21 Jul 2005 13:06:30 -0700 (PDT)
Subject: [R] output of variance estimate of random effect from a gamma
 frailty model using Coxph in R
In-Reply-To: <20050721112840.37h0v0j11c0sgg4k@web.mail.umich.edu>
References: <20050721112840.37h0v0j11c0sgg4k@web.mail.umich.edu>
Message-ID: <Pine.A41.4.61b.0507211305030.304580@homer04.u.washington.edu>

On Thu, 21 Jul 2005 ghwei at umich.edu wrote:

> Hi,
>
> I have a question about the output for variance of random effect from a gamma
> frailty model using coxph in R. Is it the vairance of frailties themselves or
> variance of log frailties? Thanks.
>

For a Gamma frailty model it is the variance of the Gamma distribution, so 
the variance of the frailties.  For Gaussian frailty it will be the log 
frailties, though.


 	-thomas



From jeaneid at chass.utoronto.ca  Thu Jul 21 22:41:10 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 21 Jul 2005 16:41:10 -0400
Subject: [R] debian vcd package
In-Reply-To: <42DFB761.50605@fe.up.pt>
Message-ID: <Pine.SGI.4.40.0507211639290.9471562-100000@origin.chass.utoronto.ca>

I just installed it on a Debian 2.6.8.1 and the following R

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R


By the way are you using apt-get install vcd. and if so why? just use
install.packages("vcd") from within R

HTH

Jean

On Thu, 21 Jul 2005, Peter Ho wrote:

> [Apologies if you  have already read this message sent  from another
> email address]
>
> Hi R-Help,
>
> I have been using R  in Linux (Debian) for the past month. The usual way
> I install packages is through apt. Recently, a new packages "vcd" became
> available on CRAN.  I tried installing it today and found that Debian
> does not seem to support this package. I also found that many other
> packages were unavailable.
> Does anyone have any recommended sites where a full list is available?
> If none exist, what would be the best way to move ahead in installing
> say the vcd package.
>
> I am still a novice in using Debian and so please forgive me if some of
> my questions may seem trivial for experienced users.
>
>
>
> Peter
> ----------------------------------------------------------------
>
> Peter Ho, PhD.
> Escola Superior de Tecnologia e Gestao.
> Instituto Politecnico de Viana do Castelo.
> Avenida do Atlantico- Apartado 574.
> 4901-908 Viana do Castelo. Portugal.
> Tel: +351-258-819700 Ext. 1252
> Email: peter at estg.ipvc.pt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rodrigoz at rpm.fmrp.usp.br  Thu Jul 21 22:34:50 2005
From: rodrigoz at rpm.fmrp.usp.br (Carlos Rodrigo Zarate Blades)
Date: Thu, 21 Jul 2005 17:34:50 -0300
Subject: [R] tav files
Message-ID: <20050721203356.M67935@rpm.fmrp.usp.br>


Dear colleagues:

I am using the spotfinder 2.2.3 and the .tav files generated there have 20 
columns. When I exclude the 3 last columns, the .tav file can not be 
recognized by aroma package in R platform.
What I have to do to generate .tav files with 17 columns only?

Thank you in advance.

Carlos 

Dept. of Immunology
University of S??o Paulo

--
Open WebMail Project (http://openwebmail.org)



From ny2005 at ipsiconferences.org  Thu Jul 21 22:59:35 2005
From: ny2005 at ipsiconferences.org (IPSI Conferences)
Date: Thu, 21 Jul 2005 22:59:35 +0200
Subject: [R] Invitation to New York, Spain, and Italy; c/ba
Message-ID: <E1Dvi8l-00077R-K1@ipsiconferences.org>

Dear potential Speaker:

On behalf of the organizing committee, I would like to extend a cordial
invitation for you to submit a paper to the IPSI Transactions journal, or
to attend one of the upcoming IPSI BgD multidisciplinary,
interdisciplinary, and transdisciplinary conferences.

------------------------------------------------------------------------

The first one will take place in New York City, NY, USA:

IPS-USA-2006 NEW YORK
Hotel Beacon (arrival: 5 January 06 / departure: 8 January 06)
New Deadlines: 1 August 05 (abstract) & 1 October 05 (full paper)
------------------------------------------------------------------------

The second one will take place in Marbella, Spain:

IPSI-2006 SPAIN
Hotel Puente Romano (arrival: 10 February 06 / departure: 13 February 06)
Deadlines: 1 September 05 (abstract) & 1 November 05 (full paper)
------------------------------------------------------------------------

The third one will take place in Amalfi, Italy:

IPSI-2006 ITALY
Hotel Santa Caterina (arrival: 23 March 06 / departure: 26 March 06)
Deadlines: 1 October 05 (abstract) & 1 December 05 (full paper)
------------------------------------------------------------------------

All IPSI BgD conferences are non-profit. They bring together the elite of
the world science; so far, we have had seven Nobel Laureates speaking at
the opening ceremonies. The conferences always take place in some of the
most attractive places of the world. All those who come to IPSI
conferences once, always love to come back (because of the unique
professional quality and the extremely creative atmosphere); lists of past
participants are on the web, as well as details of future conferences.

These conferences are in line with the newest recommendations of the US
National Science Foundation and of the EU research sponsoring agencies, to
stress multidisciplinary, interdisciplinary, and transdisciplinary
research (M+I+T++ research). The speakers and activities at the
conferences truly support this type of scientific interaction.

Among the main topics of these conferencs are: "E-education and E-business
with Special Emphasis on Semantic Web and Web Datamining"

Other topics of interest include, but are not limited to:

* Internet
* Computer Science and Engineering
* Mobile Communications/Computing for Science and Business
* Management and Business Administration
* Education
* e-Medicine
* e-Oriented Bio Engineering/Science and Molecular Engineering/Science
* Environmental Protection
* e-Economy
* e-Law
* Technology Based Art and Art to Inspire Technology Developments
* Internet Psychology

If you would like more information on either conference, please reply to
this e-mail message.

If you plan to submit an abstract and paper, please let us know
immediately for planning purposes. Remember that you can submit your paper
also to the IPSI Transactions journal.

Sincerely Yours,

Prof. V. Milutinovic, Chairman,
IPSI BgD Conferences


* * * CONTROLLING OUR E-MAILS TO YOU * * *

If you would like to continue to be informed about future IPSI BgD
conferences, please reply to this e-mail message with a subject line of
SUBSCRIBE.

If you would like to be removed from our mailing list, please reply to
this e-mail message with a subject line of REMOVE.



From ggrothendieck at gmail.com  Thu Jul 21 23:15:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2005 17:15:30 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507211926400.23083-100000@mail.mrc-dunn.cam.ac.uk>
References: <17119.41107.864470.790867@stat.math.ethz.ch>
	<Pine.LNX.4.21.0507211926400.23083-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <971536df05072114151cccaa8e@mail.gmail.com>

Use bquote instead of expression, e.g.

trees.lm <- lm(Volume ~ Girth, trees)
trees.sm <- summary(trees.lm)
trees.co <- round(trees.sm$coefficients,2)
trees.rsq <- round(trees.sm$r.squared,2)

plot(Volume ~ Girth, trees)

text(10,60, bquote(Intercept : .(trees.co[1,1])%+-%.(trees.co[1,2])), pos = 4)
text(10,57, bquote(Slope : .(trees.co[2,1])%+-%.(trees.co[2,2])), pos = 4)
text(10,54,bquote(R^2 : .(trees.rsq)), pos = 4)


On 7/21/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> On Thu, 21 Jul 2005, Christoph Buser wrote:
> 
> >Dear Dan
> >
> >I can only help you with your third problem, expression and
> >paste. You can use:
> >
> >plot(1:5,1:5, type = "n")
> >text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
> >text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
> >text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)
> >
> 
> Cheers for this.
> 
> I was trying to get it to work, but the problem is that I need to replace
> the values above with variables, from the following code...
> 
> 
> dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> dat.lm.sum <- summary(dat.lm)
> 
> my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
> my.slope.2 <- round(dat.lm.sum$coefficients[4],2)
> 
> my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
> my.inter.2 <- round(dat.lm.sum$coefficients[3],2)
> 
> my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)
> 
> 
> Anything I try results in either the words 'paste("Slope:", my.slope.1,
> %+-%my.slope.2,sep="")' being written to the plot, or just
> 'my.slope.1+-my.slope2' (where the +- is correctly written).
> 
> I want to script it up and write all three lines to the plot with
> 'sep="\n"', rather than deciding three different heights.
> 
> 
> >I do not have an elegant solution for the alignment.
> 
> Thanks very much for what you gave, its a good start for me to figure out
> how I am supposed to be telling R what to do!
> 
> Any way to just get fixed width fonts with text? (for the alignment
> problem)
> 
> 
> Cheers,
> Dan.
> 
> >
> >Regards,
> >
> >Christoph Buser
> >
> >--------------------------------------------------------------
> >Christoph Buser <buser at stat.math.ethz.ch>
> >Seminar fuer Statistik, LEO C13
> >ETH (Federal Inst. Technology) 8092 Zurich      SWITZERLAND
> >phone: x-41-44-632-4673                fax: 632-1228
> >http://stat.ethz.ch/~buser/
> >--------------------------------------------------------------
> >
> >
> >Dan Bolser writes:
> > >
> > > I would like to annotate my plot with a little box containing the slope,
> > > intercept and R^2 of a lm on the data.
> > >
> > > I would like it to look like...
> > >
> > >  +----------------------------+
> > >  | Slope     :   3.45 +- 0.34 |
> > >  | Intercept : -10.43 +- 1.42 |
> > >  | R^2       :   0.78         |
> > >  +----------------------------+
> > >
> > > However I can't make anything this neat, and I can't find out how to
> > > combine this with symbols for R^2 / +- (plus minus).
> > >
> > > Below is my best attempt (which is franky quite pour). Can anyone
> > > improve on the below?
> > >
> > > Specifically,
> > >
> > > aligned text and numbers,
> > > aligned decimal places,
> > > symbol for R^2 in the text (expression(R^2) seems to fail with
> > > 'paste') and +-
> > >
> > >
> > >
> > > Cheers,
> > > Dan.
> > >
> > >
> > > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> > >
> > > abline(coef(dat.lm),lty=2,lwd=1.5)
> > >
> > >
> > > dat.lm.sum <- summary(dat.lm)
> > > dat.lm.sum
> > >
> > > attributes(dat.lm.sum)
> > >
> > > my.text.1 <-
> > >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
> > >         "+/-",          round(dat.lm.sum$coefficients[4],2))
> > >
> > > my.text.2 <-
> > >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
> > >         "+/-",          round(dat.lm.sum$coefficients[3],2))
> > >
> > > my.text.3 <-
> > >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
> > >
> > > my.text.1
> > > my.text.2
> > > my.text.3
> > >
> > >
> > > ## Add legend
> > > text(x=3,
> > >      y=300,
> > >      paste(my.text.1,
> > >            my.text.2,
> > >            my.text.3,
> > >            sep="\n"),
> > >      adj=c(0,0),
> > >      cex=1)
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mschwartz at mn.rr.com  Thu Jul 21 23:26:35 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 21 Jul 2005 16:26:35 -0500
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507211926400.23083-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0507211926400.23083-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1121981195.4611.54.camel@localhost.localdomain>

[Note: the initial posts have been re-arranged to attempt to maintain
the flow from top to bottom]

> >Dan Bolser writes:
> > > 
> > > I would like to annotate my plot with a little box containing the slope,
> > > intercept and R^2 of a lm on the data.
> > > 
> > > I would like it to look like...
> > > 
> > >  +----------------------------+
> > >  | Slope     :   3.45 +- 0.34 |
> > >  | Intercept : -10.43 +- 1.42 |
> > >  | R^2       :   0.78         |
> > >  +----------------------------+
> > > 
> > > However I can't make anything this neat, and I can't find out how to
> > > combine this with symbols for R^2 / +- (plus minus).
> > > 
> > > Below is my best attempt (which is franky quite pour). Can anyone
> > > improve on the below?
> > > 
> > > Specifically, 
> > > 
> > > aligned text and numbers, 
> > > aligned decimal places, 
> > > symbol for R^2 in the text (expression(R^2) seems to fail with
> > > 'paste') and +- 
> > > 
> > > 
> > > 
> > > Cheers,
> > > Dan.
> > > 
> > > 
> > > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> > > 
> > > abline(coef(dat.lm),lty=2,lwd=1.5)
> > > 
> > > 
> > > dat.lm.sum <- summary(dat.lm)
> > > dat.lm.sum
> > > 
> > > attributes(dat.lm.sum)
> > > 
> > > my.text.1 <-
> > >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
> > >         "+/-",          round(dat.lm.sum$coefficients[4],2))
> > > 
> > > my.text.2 <-
> > >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
> > >         "+/-",          round(dat.lm.sum$coefficients[3],2))
> > > 
> > > my.text.3 <-
> > >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
> > > 
> > > my.text.1
> > > my.text.2
> > > my.text.3
> > > 
> > > 
> > > ## Add legend
> > > text(x=3,
> > >      y=300,
> > >      paste(my.text.1,
> > >            my.text.2,
> > >            my.text.3,
> > >            sep="\n"),
> > >      adj=c(0,0),
> > >      cex=1


> On Thu, 21 Jul 2005, Christoph Buser wrote:
> 
> >Dear Dan
> >
> >I can only help you with your third problem, expression and
> >paste. You can use:
> >
> >plot(1:5,1:5, type = "n")
> >text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
> >text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
> >text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)

> >I do not have an elegant solution for the alignment.


On Thu, 2005-07-21 at 19:55 +0100, Dan Bolser wrote:
> Cheers for this.
> 
> I was trying to get it to work, but the problem is that I need to replace
> the values above with variables, from the following code...
> 
> 
> dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> dat.lm.sum <- summary(dat.lm)
> 
> my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
> my.slope.2 <- round(dat.lm.sum$coefficients[4],2)
> 
> my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
> my.inter.2 <- round(dat.lm.sum$coefficients[3],2)
> 
> my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)
> 
> 
> Anything I try results in either the words 'paste("Slope:", my.slope.1,
> %+-%my.slope.2,sep="")' being written to the plot, or just
> 'my.slope.1+-my.slope2' (where the +- is correctly written).
> 
> I want to script it up and write all three lines to the plot with
> 'sep="\n"', rather than deciding three different heights.


> Thanks very much for what you gave, its a good start for me to figure out 
> how I am supposed to be telling R what to do!
> 
> Any way to just get fixed width fonts with text? (for the alignment
> problem)


Dan,

Here is one approach. It may not be the best, but it gets the job done.
You can certainly take this and encapsulate it in a function to automate
the text/box placement and to pass values as arguments.

A couple of quick concepts:

1. As far as I know, plotmath cannot do multiple lines, so each line in
your box needs to be done separately.

2. The horizontal alignment is a bit problematic when using expression()
or bquote() since I don't believe that multiple spaces are honored as
such after parsing. Thus I break up each component (label, ":" and
values) into separate text() calls. The labels are left justified.

3. The alignment for the numeric values are done with right
justification. So, as long as you use a consistent number of decimals in
the value outputs (2 here), you should be OK. This means you might need
to use formatC() or sprintf() to control the numeric output values on
either side of the +/- sign.

4. In the variable replacement, note the use of substitute() and the
list of x and y arguments as replacement values in the expressions.



# Set your values
my.slope.1 <- 3.45
my.slope.2 <- 0.34

my.inter.1 <- -10.43
my.inter.2 <- 1.42

my.Rsqua <- 0.78


# Create the initial plot as per Christoph's post
plot(1:5, 1:5, type = "n")


#-------------------------------------
# Do the Slope
#-------------------------------------

text(1, 4.5,  "Slope", pos = 4)
text(2, 4.5, ":")
text(3, 4.5, substitute(x %+-% y, 
                        list(x = my.slope.1, 
                             y = my.slope.2)),
     pos = 2)


#-------------------------------------
# Do the Intercept
#-------------------------------------

text(1, 4.25, "Intercept", pos = 4)
text(2, 4.25, ":")
text(3, 4.25, substitute(x %+-% y, 
                         list(x = my.inter.1, 
                              y = my.inter.2)),
     pos = 2)


#-------------------------------------
# Do R^2
#-------------------------------------

text(1, 4.0, expression(R^2), pos = 4)
text(2, 4.0, ":")
text(3, 4.0,  substitute(x, list(x = my.Rsqua)),
     pos = 2)


#-------------------------------------
# Do the Box
#-------------------------------------

rect(1, 3.75, 3, 4.75)


You can adjust the x,y coordinates for the various text elements as you
may require and can also calculate them based upon the xlim, ylim of
your actual plot. You can also modify the 'cex' argument to text() for
adjusting the sizes of the fonts in use.

BTW, to use a monospaced font, you can set par(family = "mono").
See ?par for more information.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Thu Jul 21 22:06:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Jul 2005 16:06:03 -0400
Subject: [R] RandomForest question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EAE9@usctmx1106.Merck.com>

> From: Uwe Ligges
> 
> Arne.Muller at sanofi-aventis.com wrote:
> 
> > Hello,
> > 
> > I'm trying to find out the optimal number of splits (mtry parameter)
> > for a randomForest classification. The classification is binary and
> > there are 32 explanatory variables (mostly factors with each up to 4
> > levels but also some numeric variables) and 575 cases.
> > 
> > I've seen that although there are only 32 explanatory variables the
> > best classification performance is reached when choosing 
> mtry=80. How
> > is it possible that more variables can used than there are 
> in columns
> > the data frame?
> 
> If some of the variables are factors, dummy variables are 
> generated and 
> you get a larger number of variables in the later process.

No, unless the OP is using the formula interface with a version of the 
package from two years or so ago.  We got the first formula interface
by copying and modifying the one for svm() in e1071, and forgot the
fact that SVM needs that for dealing with factors, but not trees 
(especially not how the underlying RF code handles them).  This has
been correctly long ago.

Cheers,
Andy


 
> Uwe Ligges
> 
> 
> > thanks for your help + kind regards,
> > 
> > Arne
> > 
> > 
> > 
> > 
> > [[alternative HTML version deleted]]
> > 
> > ______________________________________________ 
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> > posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From pantd at unlv.nevada.edu  Thu Jul 21 23:32:34 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Thu, 21 Jul 2005 14:32:34 -0700
Subject: [R] A Question About Inverse Gamma
Message-ID: <1121981554.42e01472cafef@webmail.scsv.nevada.edu>

Hi R users,


I am having a little problem finding the the solution to this problem in R:

1. I need to generate normal distribution of sample size 30, mean = 50, sd = 5.
2. From the statistics obtained in step 1, I need to generate the Inverse Gamma
distribution.

Your views and help will be appreciated.



From p.dalgaard at biostat.ku.dk  Fri Jul 22 00:01:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Jul 2005 00:01:18 +0200
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42DFBE0A.3020803@stats.uwo.ca>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFBE0A.3020803@stats.uwo.ca>
Message-ID: <x2mzoflsy9.fsf@turmalin.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> On 7/21/2005 9:43 AM, Gabor Grothendieck wrote:
> > I think you have been using R too long.  Something like
> > this is very much needed.  There are two problems:
> > 
> > 1. the process itself is too complex (need to get rid of perl,
> >     integrate package development tools with package installation 
> >    procedure [it should be as easy as downloading a package], 
> >    remove necessity to set or modify any environment variables
> >     including the path variables).
> 
> I agree with some of this, but I don't see much interest in fixing it.
> 
> For example, getting rid of Perl would be a lot of work.  When the Perl 
> scripts were written, R was not capable of doing what they do.  I think 
> it is capable now, but there's still a huge amount of translation work 
> to do.  Who will do that?  Who will test that they did it right?  At the 
> end, will it actually have been worth all the trouble?  Installing Perl 
> is not all that hard.

Another point of view is that the issue is that we cannot ship a full
set of build tools with R on Windows, the main obstacle being that
Active Perl has redistribution restrictions. 

If we could be sure that the build tools were present, then we could
start thinking about smoothing out the package creation process using
GUI tools or whatnot.

In principle, it is ridiculous if every application ships with its own
development environment, compilers and all, but hey! we're not the
ones who invented Windows...

So how hard would it be to build Perl from sources? We do that with
Tcl already. (Actually, there is an unencumbered Perl called SiePerl,
which was used in the GnuWin project. Wouldn't know if it's any good,
and GnuWin itself seems to be headless these days.)

Of course someone needs to maintain it all, but that might not  really
be much harder than what we already do.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From sundar.dorai-raj at pdf.com  Fri Jul 22 00:15:54 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 21 Jul 2005 17:15:54 -0500
Subject: [R] A Question About Inverse Gamma
In-Reply-To: <1121981554.42e01472cafef@webmail.scsv.nevada.edu>
References: <1121981554.42e01472cafef@webmail.scsv.nevada.edu>
Message-ID: <42E01E9A.1070500@pdf.com>



pantd at unlv.nevada.edu wrote:
> Hi R users,
> 
> 
> I am having a little problem finding the the solution to this problem in R:
> 
> 1. I need to generate normal distribution of sample size 30, mean = 50, sd = 5.
> 2. From the statistics obtained in step 1, I need to generate the Inverse Gamma
> distribution.
> 
> Your views and help will be appreciated.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

I found rinvgamma in the MCMCpack package. Perhaps that's what you need.

Did you read the posing guide? A help.search("normal") would have helped 
you with item 1.

--sundar



From ramasamy at cancer.org.uk  Fri Jul 22 01:03:17 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 22 Jul 2005 00:03:17 +0100
Subject: [R] R:plot and dots
In-Reply-To: <42DFAE9A.76B63ED4@STATS.uct.ac.za>
References: <42DFAE9A.76B63ED4@STATS.uct.ac.za>
Message-ID: <1121986998.5880.5.camel@dhcppc3>

Here is an example :

   x <- rnorm(20)
   y <- rnorm(20)
   plot(x, y, type="n")
   text(x, y, labels=as.character(1:20))

Also look into help(identify) if you want to point out specific points.

Regards, Adai



On Thu, 2005-07-21 at 16:18 +0200, Clark Allan wrote:
> hi all
> 
> a very simple question.
> 
> i have plot(x,y)
> 
> but i would like to add in on the plot the observation number associated
> with each point.
> 
> how can this be done?
> 
> /
> allan
> ______________________________________________ R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Jul 22 01:09:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2005 19:09:38 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <x2mzoflsy9.fsf@turmalin.kubism.ku.dk>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF38@ex119.smic-sh.com>
	<42DF9BC9.9050508@statistik.uni-dortmund.de>
	<971536df0507210643438a80ef@mail.gmail.com>
	<42DFBE0A.3020803@stats.uwo.ca> <x2mzoflsy9.fsf@turmalin.kubism.ku.dk>
Message-ID: <971536df050721160964c09174@mail.gmail.com>

On 22 Jul 2005 00:01:18 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Duncan Murdoch <murdoch at stats.uwo.ca> writes:
> 
> > On 7/21/2005 9:43 AM, Gabor Grothendieck wrote:
> > > I think you have been using R too long.  Something like
> > > this is very much needed.  There are two problems:
> > >
> > > 1. the process itself is too complex (need to get rid of perl,
> > >     integrate package development tools with package installation
> > >    procedure [it should be as easy as downloading a package],
> > >    remove necessity to set or modify any environment variables
> > >     including the path variables).
> >
> > I agree with some of this, but I don't see much interest in fixing it.
> >
> > For example, getting rid of Perl would be a lot of work.  When the Perl
> > scripts were written, R was not capable of doing what they do.  I think
> > it is capable now, but there's still a huge amount of translation work
> > to do.  Who will do that?  Who will test that they did it right?  At the
> > end, will it actually have been worth all the trouble?  Installing Perl
> > is not all that hard.
> 
> Another point of view is that the issue is that we cannot ship a full
> set of build tools with R on Windows, the main obstacle being that
> Active Perl has redistribution restrictions.
> 

Although the ultimate answer is to get rid of perl entirely,
in the same vein as your discussion, perhaps R could simply 
provide standalone executables for each perl program currently
used by R  (using perlcc to produce them).

I believe there is a free alternative to Microsoft's Help Compiler 
too but I just googled for it and was unable to locate it.

By placing all these items (and the UNIXish tools) in the \R\rw...\bin
directory and using the registry as in Rcmd.bat and Rgui.bat
found in batchfiles:
  http://cran.r-project.org/contrib/extra/batchfiles/
modifying the path and environment variables by the user might 
be eliminated.



From ramasamy at cancer.org.uk  Fri Jul 22 01:15:59 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 22 Jul 2005 00:15:59 +0100
Subject: [R] principal component analysis in affy
In-Reply-To: <F2235647AC878D438F09255C39842FBC0D342C18@SJMEMXMB03.stjude.sjcrh.local>
References: <F2235647AC878D438F09255C39842FBC0D342C18@SJMEMXMB03.stjude.sjcrh.local>
Message-ID: <1121987760.5880.14.camel@dhcppc3>

1) Please learn to wrap your emails to 72 characters per line. See 
http://expita.com/nomime.html

2) You might have better luck with the BioConductor folks. Their mailing
list is https://stat.ethz.ch/mailman/listinfo/bioconductor

3) The affy package has many functions including some algorithms for
preprocessing CEL files.

4) I do not understand why you need the affy package if you do not have
CEL files ? I presume that you only have a subset of the final data.

5) I do not understand your question but you might want to simply
transpose the matrix before doing a prcomp() on it. See help("t").

Regards, Adai



On Thu, 2005-07-21 at 10:09 -0500, Wagle, Mugdha wrote:
> Hi,
>  
> I have been using the prcomp function to perform PCA on my example microarray data, (stored in metric text files) which looks like this:
>  
>         1a 1b 1c 1d 1e 1f ...................................................4r 4s 4t
> g1    1.2705 1.2766 ...........................................................2.0298
> g2    0.1631 ........................................................................0.7067
> g3    0.2212 ........................................................................1.0439
> .
> .
> .
> .
> g99  1.3657..........................................................................2.3736
>  
> i.e. a matrix of 63 columns and 99 rows, where the columns represent chip and rows represent genes. Now, the biplot function
>  
> biplot(prcomp(pcadata, scale = TRUE), cex = c(0.75,0.75))
>  
> gives me a plot with one vector per gene. However, I actually need to get one vector per chip instead of one vector per gene.  I have been told that there is a function in the affy package that does what I am looking for i.e. gives one vector per chip. Can someone please tell me what the function is called, and how I can get hold of the code(since I believe affy only works on CEL files) ? I have downloaded the affy R code from Terry Speed's website already, but I don't know where (if at all) the code to perform PCA is.
>  
> Thank you everyone!
>  
> Sincerely,
> Mugdha Wagle
> Hartwell Center for Bioinformatics and Biotechnology,
> St.Jude Children's Research Hospital, Memphis TN 38105
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From f.calboli at imperial.ac.uk  Fri Jul 22 01:44:08 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 22 Jul 2005 00:44:08 +0100
Subject: [R] vectorising ifelse()
Message-ID: <1121989448.28528.81.camel@localhost.localdomain>

Hi All,

is there any chance of vectorising the two ifelse() statements in the
following code:

for(i in gp){
   new[i,1] = ifelse(srow[i]>0, new[srow[i],zippo[i]], sample(1:100, 1,
prob =Y1, rep = T))
   new[i,2] = ifelse(drow[i]>0, new[drow[i]>0,zappo[i]], sample(1:100,
1, prob =Y1, rep = T))
 }

Where I am forced to check if the value of drow and srow are >0 for each
line... in practical terms, I am attributing haplotypes to a pedigree,
so I have to give the haplotypes to the parents before I give them to
the offspring. The vectors *zippo* and *zappo* are the chances of
getting one or the other hap from the sire and dam respectively. *gp* is
the vectors of non-ancestral animals. *new* is a two col matrix where
the haps are stored.

Cheers,

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From cliff at ms.washington.edu  Fri Jul 22 02:18:50 2005
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Thu, 21 Jul 2005 17:18:50 -0700
Subject: [R] Re  Randomization test for interaction effect
Message-ID: <046201c58e52$f01d4640$6401a8c0@C56909A>

Dear Pedro,

How to test for an interaction--or, even, how to pose the question of an
interaction--in randomization-based inference is not at all obvious.
And, in the permutation test context reliance has been placed on the
exchangeability of (estimated) residuals under an additive,
homoscedastic model. Where estimated, the residuals are not exactly
exchangeable.

A reference you might find useful is Pesarin, F (2001) "Multivariate
permutations tests." Wiley: Chichester, UK. His method of synchronized
permutations "may" be applied to test for interactions under some
limited circumstances.


Pedro de Barros writes, in part:

Dear All,

I am trying to build a randomization test for interaction

The problem is as follows: I have a set of stations where the occurrence
and
biomass of each species being investigated was recorded.

<snip>

I would really appreciate any pointer to a solution of this problem. I
believe it is not complicated (and probably quite obvious) but the
solution
keeps out of reach, even though I have been searching for over a week.

Thanks,
Pedro



**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From r.shengzhe at gmail.com  Fri Jul 22 02:27:28 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Fri, 22 Jul 2005 02:27:28 +0200
Subject: [R] help: pls package
Message-ID: <ea57975b05072117274c2d8235@mail.gmail.com>

Hello,

I have a data set with 15 variables (first one is the response) and
1200 observations. Now I use pls package to do the plsr with cross
validation as below.

trainSet = as.data.frame(scale(trainSet, center = T, scale = T))
trainSet.plsr = mvr(formula, ncomp = 14, data = trainSet, method = "kernelpls", 
                           CV = TRUE, validation = "LOO", model =
TRUE, x = TRUE,
                            y = TRUE)

after that I wish to obtain the value of "se", estimated standard
errors of the estimates(cross validation), mentioned in the function
of MSEP, but not implemented yet, so I made the program by myself to
calculate it. The results I got seem not right, and I wonder which
step below is wrong.

y = trainSet.plsr$y
p = as.data.frame(trainSet.plsr$validation$pred)

i = 1; msep_element = c()
while(i <= length(p)){
    msep_element[,i] = (p[i]-y)^2 
    i = i + 1
}

msep = colMeans(msep_element)
msep_sd = sd(msep_element)

Then I compare "msep" with "trainSet.plsr$validation$MSEP", which are
the same, but the values of "msep_sd" seem much larger than I
expected, is it the same as "se"?

Thank you,
Shengzhe



From r.shengzhe at gmail.com  Fri Jul 22 02:35:17 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Fri, 22 Jul 2005 02:35:17 +0200
Subject: [R] help: pls package
In-Reply-To: <ea57975b05072117274c2d8235@mail.gmail.com>
References: <ea57975b05072117274c2d8235@mail.gmail.com>
Message-ID: <ea57975b050721173575681f74@mail.gmail.com>

Hello,

I have a data set with 15 variables (first one is the response) and
1200 observations. Now I use pls package to do the plsr with cross
validation as below.

trainSet = as.data.frame(scale(trainSet, center = T, scale = T))
trainSet.plsr = mvr(formula, ncomp = 14, data = trainSet, method = "kernelpls",
                          CV = TRUE, validation = "LOO", model = TRUE, x = TRUE,
                           y = TRUE)

after that I wish to obtain the value of "se", estimated standard
errors of the estimates(cross validation), mentioned in the function
of MSEP, but not implemented yet, so I made the program by myself to
calculate it. The results I got seem not right, and I wonder which
step below is wrong.

y = trainSet.plsr$y
p = as.data.frame(trainSet.plsr$validation$pred)

i = 1; msep_element = c()
while(i <= length(p)){
   msep_element[,i] = (p[i]-y)^2
   i = i + 1
}

msep = colMeans(msep_element)
msep_sd = sd(msep_element)

Then I compare "msep" with "trainSet.plsr$validation$MSEP", which are
the same, but the values of "msep_sd" seem much larger than I
expected, is it the same as "se"? If not, how to calculate "se" of
cross validation?

Thank you,
Shengzhe



From bartzk at yahoo-inc.com  Fri Jul 22 03:14:03 2005
From: bartzk at yahoo-inc.com (Kevin Bartz)
Date: Thu, 21 Jul 2005 18:14:03 -0700
Subject: [R] vectorising ifelse()
Message-ID: <E4EBBAD66D826C41BDED4CCCDCC6D0F18F0C09@EXCHG01-BUR>

The code as you provided it is a bit unusual. In the second assignment,
you're using "drow[i]>0" as an index into "new," but ifelse has already
found that condition to be true, which means what you wrote is just the
same as saying new[1,zappo[i]].

Also, if zappo and zappo are vectors of probabilities, why are they
being used as indices into new? Indices are supposed to be integers or
T/Fs. 

It would be nice if you could provide some example data. I'm sure
there's a way to vectorize it, but I'm struggling to get my head around
the Zippos, zappos and haplos.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico Calboli
Sent: Thursday, July 21, 2005 4:44 PM
To: r-help
Subject: [R] vectorising ifelse()

Hi All,

is there any chance of vectorising the two ifelse() statements in the
following code:

for(i in gp){
   new[i,1] = ifelse(srow[i]>0, new[srow[i],zippo[i]], sample(1:100, 1,
prob =Y1, rep = T))
   new[i,2] = ifelse(drow[i]>0, new[drow[i]>0,zappo[i]], sample(1:100,
1, prob =Y1, rep = T))
 }

Where I am forced to check if the value of drow and srow are >0 for each
line... in practical terms, I am attributing haplotypes to a pedigree,
so I have to give the haplotypes to the parents before I give them to
the offspring. The vectors *zippo* and *zappo* are the chances of
getting one or the other hap from the sire and dam respectively. *gp* is
the vectors of non-ancestral animals. *new* is a two col matrix where
the haps are stored.

Cheers,

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From fernandolapa at mail.telepac.pt  Fri Jul 22 03:31:27 2005
From: fernandolapa at mail.telepac.pt (Fernando Lapa)
Date: Fri, 22 Jul 2005 02:31:27 +0100
Subject: [R] :)
Message-ID: <002f01c58e5d$14c69af0$895b0dd5@xasso>

ol??
----- Original Message -----
From: <r-help at lists.r-project.org>
To: <fernandolapa at mail.telepac.pt>
Sent: Thursday, July 21, 2005 3:04 PM
Subject: :)


> pq nao me liga??
>



From iidn01 at yahoo.com  Fri Jul 22 03:47:09 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Thu, 21 Jul 2005 18:47:09 -0700 (PDT)
Subject: [R] find confounder in covariates
Message-ID: <20050722014709.71749.qmail@web31115.mail.mud.yahoo.com>

Hi,

I was wondering if there is a way, or function in R to
find confounders. For istance, 

> a = sample( c(1:3), size=10,replace=T)
> X1 = factor( c('A','B','C')[a] )
> X2 = factor( c('Aa','Bb','Cc')[a] )
> Xmat = data.frame(X1,X2,rnorm(10),rnorm(10))
> dimnames(Xmat)[[2]] = c('z1','z2','z3','y')

Now, z2 is just an alias of z1. There can be a
collinearity like one is a linear combination of
others. If you run lm on it:
 
> f = lm(y~.,data=Xmat)
> summary(f)

Call:
lm(formula = y ~ ., data = Xmat)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.2853 -0.3708 -0.1224  0.4617  1.2821 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  0.82141    0.44583   1.842   0.1150  
z1B         -1.34167    0.65176  -2.059   0.0852 .
z1C          0.80891    1.07639   0.751   0.4808  
z2Bb              NA         NA      NA       NA  
z2Cc              NA         NA      NA       NA  
z3           0.04231    0.23397   0.181   0.8625  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'
0.1 ' ' 1 

Residual standard error: 0.971 on 6 degrees of freedom
Multiple R-Squared: 0.5086,     Adjusted R-squared:
0.2629 
F-statistic:  2.07 on 3 and 6 DF,  p-value: 0.2057 

In this case, I can look at data and figure out which
variable is confounded with which. But, if we have
many categorial covariates ( not necessarily same
number of levels ), it is almost impossible to check
it out.  

Any help would be greatly appreicated. Thanks.

Young.



From d.scott at auckland.ac.nz  Fri Jul 22 04:36:45 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 22 Jul 2005 14:36:45 +1200 (NZST)
Subject: [R] A Question About Inverse Gamma
In-Reply-To: <42E01E9A.1070500@pdf.com>
References: <1121981554.42e01472cafef@webmail.scsv.nevada.edu>
	<42E01E9A.1070500@pdf.com>
Message-ID: <Pine.LNX.4.61.0507221347140.4161@stat12.stat.auckland.ac.nz>

On Thu, 21 Jul 2005, Sundar Dorai-Raj wrote:

>
>
> pantd at unlv.nevada.edu wrote:
>> Hi R users,
>>
>>
>> I am having a little problem finding the the solution to this problem in R:
>>
>> 1. I need to generate normal distribution of sample size 30, mean = 50, sd = 5.
>> 2. From the statistics obtained in step 1, I need to generate the Inverse Gamma
>> distribution.
>>
>> Your views and help will be appreciated.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> I found rinvgamma in the MCMCpack package. Perhaps that's what you need.
>


I think there is a problem with rinvgamma:

> rinvgamma
function (n, shape, scale = 1)
{
     return(1/rgamma(n, shape, scale))
}
<environment: namespace:MCMCpack>

I know it is not necessarily authoritative but look at wikipedia

http://en.wikipedia.org/wiki/Inverse-gamma_distribution

It seems the one line of the function should be:

return(1/rgamma(n, shape, 1/scale)


Or you could of course throw caution to the winds and write your own 
rinvgamma using rgamma :-)

David Scott




_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From lutz.thieme at amd.com  Fri Jul 22 08:04:21 2005
From: lutz.thieme at amd.com (Thieme, Lutz)
Date: Fri, 22 Jul 2005 08:04:21 +0200
Subject: [R] Rprof fails in combination with RMySQL
Message-ID: <7C60FEAE2EE12D4BA1B232662554CDE1012DF8B0@SF30EXMB1.amd.com>

Hello Bogdan, 

thanks for you reply. My MySQL is always optimized oustide 
from R (but many thanks for the interesting link!). 
I'm very sure that I have to optimize the R code which uses 
the data from my queries for calculations. To get more in-
formation which R function is the main speed limiter I tried 
Rprof. 
Because I'm always opening and closing the connection for every 
query I have never opened more than one connection. 
And again: The same R code runs without Rprof stable since weeks
multiple times a day. I can exclude by 99% that the error comes 
from the database. Maybe it comes from large number of opening
closing cycles?...

Regards,

Lutz



> -----Original Message-----
> From: bogdan romocea [mailto:br44114 at gmail.com]
> Sent: Thursday, July 21, 2005 5:05 PM
> To: Thieme, Lutz
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] Rprof fails in combination with RMySQL
> 
> 
> I think you're barking up the wrong tree. Optimize the MySQL code
> separately from optimizing the R code. A very nice reference about the
> former is http://highperformancemysql.com/. Also, if possible, do
> everything in MySQL.
> hth,
> b.
> 
> 
> > -----Original Message-----
> > From: Thieme, Lutz [mailto:lutz.thieme at amd.com] 
> > Sent: Thursday, July 21, 2005 10:11 AM
> > To: Rhelp (E-mail)
> > Subject: [R] Rprof fails in combination with RMySQL
> > 
> > 
> > Dear R community,
> > 
> > I tried to optimized my R code by using Rprof. In my R code 
> > I'm using MySQL
> > database connections intensively. After a bunch of queries R 
> > fails with the 
> > following error message:
> > Error in .Call("RS_MySQL_newConnection", drvId, con.params, 
> > groups, PACKAGE = .MySQLPkgName) : 
> >         RS-DBI driver: (could not connect mylogin at mydatabase 
> > on dbname "myDB"
> > 
> > Without the R profiler this code runs very stable since weeks.
> > 
> > Do you have any ideas or suggestions?
> > 
> > I tried the following R versions:
> > ___________________________
> > platform i386-pc-solaris2.8
> > arch     i386              
> > os       solaris2.8        
> > system   i386, solaris2.8  
> > status                     
> > major    1                 
> > minor    9.1               
> > year     2004              
> > month    06                
> > day      21                
> > language R   
> > ___________________________
> > platform sparc-sun-solaris2.8
> > arch     sparc               
> > os       solaris2.8          
> > system   sparc, solaris2.8   
> > status                       
> > major    2                   
> > minor    1.1                 
> > year     2005                
> > month    06                  
> > day      20                  
> > language R   
> > ___________________________
> > platform sparc-sun-solaris2.8
> > arch     sparc               
> > os       solaris2.8          
> > system   sparc, solaris2.8   
> > status                       
> > major    1                   
> > minor    9.1                 
> > year     2004                
> > month    06                  
> > day      21                  
> > language R   
> > 
> > 
> > Thank you in advance and kind regards,
> > 
> > Lutz Thieme
> > AMD Saxony/ Product Engineering AMD Saxony Limited 
> > Liability Company & Co. KG
> > phone: + 49-351-277-4269 M/S E22-PE, 
> > Wilschdorfer Landstr. 101
> > fax: + 49-351-277-9-4269 D-01109 Dresden, Germany
> > 
> > 
> > [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
>



From wolfram at fischer-zim.ch  Fri Jul 22 08:47:38 2005
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 22 Jul 2005 08:47:38 +0200
Subject: [R] automated response
Message-ID: <10507220847.AA00216@fischer-zim.ch>

Zu Ihrer Information:

Ihre Post kann im Moment nicht sofort bearbeitet werden,
denn das Buero bleibt geschlossen bis am 7.8.2005.


Buecherbestellungen koennen Sie direkt senden an:
- in der Schweiz: mailto:info at editionpunktuell.ch
- in Deutschland und Oesterreich: mailto:auslieferung at schaltungsdienst.de

Wolfram Fischer - ZIM



From hb at maths.lth.se  Fri Jul 22 09:32:12 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 22 Jul 2005 09:32:12 +0200
Subject: [R] sapply(NULL, ...) returns a list?!?
Message-ID: <42E0A0FC.7000909@maths.lth.se>

Hi,

I bet this one has be asked before, but doing

  sapply(x, FUN=as.character)

where 'x' is a vector, then the result "should [] be simplified to a
vector" according to ?sapply, correct?  However,

  > x <- 1:10
  > sapply(x, FUN=as.character)
  [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10"
  > sapply(x[1], FUN=as.character)
  [1] "1"

But,

  > sapply(x[c()], FUN=as.character)
  list()

or equivalent,

  > sapply(NULL, FUN=as.character)
  list()


Please enlight me if I missed the reason for this.


Looking at the code for sapply(),

> sapply
function (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)
{
     FUN <- match.fun(FUN)
     answer <- lapply(as.list(X), FUN, ...)
     if (USE.NAMES && is.character(X) && is.null(names(answer)))
         names(answer) <- X
     if (simplify && length(answer) && length(common.len <-
                  unique(unlist(lapply(answer, length)))) == 1) {
         if (common.len == 1)
             unlist(answer, recursive = FALSE)
         else if (common.len > 1)
             array(unlist(answer, recursive = FALSE),
                   dim = c(common.len, length(X)),
                   dimnames = if (!(is.null(n1 <- names(answer[[1]]))
                                  & is.null(n2 <- names(answer))))
                              list(n1, n2))
         else answer
     }
     else answer
}

I see that the above behavior is coded for (because of the "&&
length(answer) &&" statement), but is this wanted?  I would like to get
a vector of length zero, in line with

   if (simplify && length(X) == 0)
     answer <- FUN(X, ...)
   else
     answer <- lapply(as.list(X), FUN, ...)

Cheers

Henrik



From ripley at stats.ox.ac.uk  Fri Jul 22 10:29:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2005 09:29:54 +0100 (BST)
Subject: [R] sapply(NULL, ...) returns a list?!?
In-Reply-To: <42E0A0FC.7000909@maths.lth.se>
References: <42E0A0FC.7000909@maths.lth.se>
Message-ID: <Pine.LNX.4.61.0507220920580.27746@gannet.stats>

On Fri, 22 Jul 2005, Henrik Bengtsson wrote:

> Hi,
>
> I bet this one has be asked before, but doing
>
>  sapply(x, FUN=as.character)
>
> where 'x' is a vector, then the result "should [] be simplified to a
> vector" according to ?sapply, correct?  However,
>
>  > x <- 1:10
>  > sapply(x, FUN=as.character)
>  [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10"
>  > sapply(x[1], FUN=as.character)
>  [1] "1"
>
> But,
>
>  > sapply(x[c()], FUN=as.character)
>  list()
>
> or equivalent,
>
>  > sapply(NULL, FUN=as.character)
>  list()

A list is a vector of length 0, BTW.

> Please enlight me if I missed the reason for this.

Well, FUN is not required to work on a 0-length input, and if you have 
nothing to call it on you have no result to know what the result type 
should be.

If FUN is vectorized (as as.character is) you would not be using sapply 
but calling FUN directly.  So it is quite reasonable to assume that FUN 
needs a length-one input.

A secondary consideration is that this is the behaviour of S, and quite a 
lot of code relies on it.

> Looking at the code for sapply(),
>
>> sapply
> function (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)
> {
>     FUN <- match.fun(FUN)
>     answer <- lapply(as.list(X), FUN, ...)
>     if (USE.NAMES && is.character(X) && is.null(names(answer)))
>         names(answer) <- X
>     if (simplify && length(answer) && length(common.len <-
>                  unique(unlist(lapply(answer, length)))) == 1) {
>         if (common.len == 1)
>             unlist(answer, recursive = FALSE)
>         else if (common.len > 1)
>             array(unlist(answer, recursive = FALSE),
>                   dim = c(common.len, length(X)),
>                   dimnames = if (!(is.null(n1 <- names(answer[[1]]))
>                                  & is.null(n2 <- names(answer))))
>                              list(n1, n2))
>         else answer
>     }
>     else answer
> }
>
> I see that the above behavior is coded for (because of the "&&
> length(answer) &&" statement), but is this wanted?  I would like to get
> a vector of length zero, in line with
>
>   if (simplify && length(X) == 0)
>     answer <- FUN(X, ...)
>   else
>     answer <- lapply(as.list(X), FUN, ...)

That's not a valid call in general.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.hankin at noc.soton.ac.uk  Fri Jul 22 10:36:00 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 22 Jul 2005 09:36:00 +0100
Subject: [R] sequence()
Message-ID: <7CBD9E72-0812-47A5-8803-34FFDAB37274@soc.soton.ac.uk>

Hi

Function sequence() repeatedly concatenates
its output, and this is slow.

It is possible to improve on the performance of sequence by
defining

  myseq <- function(x){unlist(sapply(x,function(i){1:i}))}

The following session compares the  performance of
myseq(), and sequence(), at least on my G5:


 > identical(sequence(1:50),myseq(1:50))
[1] TRUE
 > system.time(ignore <- sequence(1:800))
[1] 1.16 0.88 2.07 0.00 0.00
 > system.time(ignore <- sequence(1:800))
[1] 1.14 0.84 1.99 0.00 0.00
 > system.time(ignore <- myseq(1:800))
[1] 0.02 0.02 0.04 0.00 0.00
 > system.time(ignore <- myseq(1:800))
[1] 0.03 0.00 0.03 0.00 0.00
 >

(the time differential is even more marked for longer arguments).

Is there any reason why we couldn't use  this definition instead?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Emili.Tortosa at eco.uji.es  Fri Jul 22 11:14:00 2005
From: Emili.Tortosa at eco.uji.es (Emili Tortosa-Ausina)
Date: Fri, 22 Jul 2005 11:14:00 +0200
Subject: [R] dpill in KernSmooth package
In-Reply-To: <000b01c58ddc$000164a0$e05a2880@economics.ucl.ac.uk>
Message-ID: <5.1.1.6.0.20050722111133.013f2888@mail.uji.es>

Hi Giagomo,

I think it computes the bandwith. Anyway, have you thought about using 
lokerns insteado of KernSmooth? It computes local plug-in bandwiths 
(bandwith matrices) which perform better in many instances.

Hope that helps,

Ciao

Emili

At 11:07 21/7/2005 +0100, Giacomo De Giorgi wrote:
>Hi,
>
>just a quick question does dpill computes the bandwidth or
>half-bandwidth? The help says bandwidth, but in the literature there is
>often confusion between the bandwidth and half-bandwidth.
>
>thanks,
>Giacomo
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From herodote at oreka.com  Fri Jul 22 12:12:29 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Fri, 22 Jul 2005 11:12:29 +0100
Subject: [R] =?iso-8859-1?q?Generate_a_function?=
Message-ID: <IK0XOT$1F6E17DF38C287BCD6F3898D754AFD63@oreka.com>

hi all,

I need to generate a function inside a loop:

tmp is an array

for (i in 1:10)
{
func<- func * function(beta1) dweibull(tmp[i],beta1,eta)
}

because then i need to integrate this function on beta.

I could have written this :

func<-function(beta1) prod(dweibull(tmp,beta1,eta)) (with eta and beta1 set)

but it is unplottable and no integrable... i could make it a bit different but if i do that ( prod(tmp)=~Inf ) i'm stuck.

I've looked in R-poetry and i didn't find anything usefull.

I think i have a problem with how i tell R my function is, R seems to think it is a function like programmers do but not a f(x) function.

Thks 
guillaume

////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From ramasamy at cancer.org.uk  Fri Jul 22 12:20:02 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 22 Jul 2005 11:20:02 +0100
Subject: [R] vectorising ifelse()
In-Reply-To: <1121989448.28528.81.camel@localhost.localdomain>
References: <1121989448.28528.81.camel@localhost.localdomain>
Message-ID: <1122027603.6005.23.camel@ipc143004.lif.icnet.uk>

Does either 'zippo' or 'zappo' contain the values 1 or 2 ?


If so, then you cannot vectorize this code because you are changing the
values in 'new' at every iteration and potentially sampling a value from
new[ ,1] or new[ ,2] .


If not, then it might be possible to vectorize. Something along the
following untested lines

 pos   <- which( srow > 0 )
 neg   <- which( srow <= 0 )

 new[pos ,1] <- new[ cbind(srow[pos] , zippo[pos]) ]
 new[neg, 1] <- sample( 1:100, length(neg), prob=Y1, replace=TRUE )

and then repeat for filling in new[ ,2].


Am I correct in guessing that your srow and zippo are of the equal
length here and thus new is a square matrix.

Regards, Adai



On Fri, 2005-07-22 at 00:44 +0100, Federico Calboli wrote:
> Hi All,
> 
> is there any chance of vectorising the two ifelse() statements in the
> following code:
> 
> for(i in gp){
>    new[i,1] = ifelse(srow[i]>0, new[srow[i],zippo[i]], sample(1:100, 1,
> prob =Y1, rep = T))
>    new[i,2] = ifelse(drow[i]>0, new[drow[i]>0,zappo[i]], sample(1:100,
> 1, prob =Y1, rep = T))
>  }
> 
> Where I am forced to check if the value of drow and srow are >0 for each
> line... in practical terms, I am attributing haplotypes to a pedigree,
> so I have to give the haplotypes to the parents before I give them to
> the offspring. The vectors *zippo* and *zappo* are the chances of
> getting one or the other hap from the sire and dam respectively. *gp* is
> the vectors of non-ancestral animals. *new* is a two col matrix where
> the haps are stored.
> 
> Cheers,
> 
> Federico
>



From fisher at plessthan.com  Fri Jul 22 12:45:15 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Fri, 22 Jul 2005 03:45:15 -0700
Subject: [R] Question regarding subsetting
In-Reply-To: <mailman.9.1122026401.1328.r-help@stat.math.ethz.ch>
References: <mailman.9.1122026401.1328.r-help@stat.math.ethz.ch>
Message-ID: <1C504145-F77F-4C92-9D4B-E2FAF5C5035E@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050722/655f78fb/attachment.pl

From bhx2 at mevik.net  Fri Jul 22 12:50:22 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 22 Jul 2005 12:50:22 +0200
Subject: [R] help: pls package
In-Reply-To: <ea57975b050721173575681f74@mail.gmail.com> (wu sz's message of
	"Fri, 22 Jul 2005 02:35:17 +0200")
References: <ea57975b05072117274c2d8235@mail.gmail.com>
	<ea57975b050721173575681f74@mail.gmail.com>
Message-ID: <m01x5rm7wx.fsf@bar.nemo-project.org>

wu sz writes:

> trainSet = as.data.frame(scale(trainSet, center = T, scale = T))
> trainSet.plsr = mvr(formula, ncomp = 14, data = trainSet, method = "kernelpls",
>                     CV = TRUE, validation = "LOO", model = TRUE, x = TRUE,
>                     y = TRUE)

[Two side notes here:
 1) scaling of the data (with its sd) should be performed inside the
 cross-validation.  In the current version of 'pls', one can use
 cvplsr <- crossval(plsr(y ~ scale(X), ncomp = 14, data = mydata),
                    length.seg = 1)
 (However, 'crossval' is slower than the built-in cross-validation on
 'mvr'/'plsr'.  In the development version of the package, scaling
 within the cross-validation has been implemented in the built-in
 cross-validation.  This will hopefully be published shortly.)

 2) The 'CV' argument is from the earlier 'pls.pcr' package, and is no
 longer used.  It is silently ignored.]

> i = 1; msep_element = c()
> while(i <= length(p)){
>    msep_element[,i] = (p[i]-y)^2
>    i = i + 1
> }

Hmm...  I don't see how you got that code to run.  This should work, though:

msep_element <- (p - y)^2

> msep = colMeans(msep_element)
> msep_sd = sd(msep_element)

You will get much closer to the true value with

sd(msep_element) / sqrt(length(y))

However, this will not produce an unbiased estimate of the sd of the
estimated MSEP, because it ignores the depencies between the
residuals.  E.g., the residual when sample 1 is predicted is not
independent of the residual when sample 2 is predicted.  In general, I
think, it will produce underestimated sds.  The effect should be
largest for small data sets.

This is the reason the pls package currently doesn't estimate se of
cross-validated MSEPs.  There is also the question of what the
estimated should be conditioned on: for leave-one-out
cross-validation, sd(MSEP | trainData) = 0.

[If someone knows how to calculate unbiased estimates of
cross-validated MSEPs, please let me know. :-)]

-- 
Bj??rn-Helge Mevik



From ligges at statistik.uni-dortmund.de  Fri Jul 22 12:51:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Jul 2005 12:51:47 +0200
Subject: [R] Generate a function
In-Reply-To: <IK0XOT$1F6E17DF38C287BCD6F3898D754AFD63@oreka.com>
References: <IK0XOT$1F6E17DF38C287BCD6F3898D754AFD63@oreka.com>
Message-ID: <42E0CFC3.70206@statistik.uni-dortmund.de>

herodote at oreka.com wrote:

> hi all,
> 
> I need to generate a function inside a loop:
> 
> tmp is an array

Well, a 1-d array, or better say a vector of length 10, given the code 
below is correct.


> for (i in 1:10)
> {
> func<- func * function(beta1) dweibull(tmp[i],beta1,eta)
> }
> 
> because then i need to integrate this function on beta.
> 
> I could have written this :
> 
> func<-function(beta1) prod(dweibull(tmp,beta1,eta)) (with eta and beta1 set)
> 
> but it is unplottable and no integrable... i could make it a bit different but if i do that ( prod(tmp)=~Inf ) i'm stuck.
>
 >
> I've looked in R-poetry and i didn't find anything usefull.
> 
> I think i have a problem with how i tell R my function is, R seems to think it is a function like programmers do but not a f(x) function.

Please specify a reproducible example (as the posting guide asks to do), 
that means including the values for tmp, beta1 and eta.
Then we might be able to explain where you have your problems.

Uwe Ligges

> Thks 
> guillaume
> 
> ////////////////////////////////////////////////////////////
> // Webmail Oreka : http://www.oreka.com
> ////////////////////////////////////////////////////////////
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jul 22 12:52:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Jul 2005 12:52:21 +0200
Subject: [R] Question regarding subsetting
In-Reply-To: <1C504145-F77F-4C92-9D4B-E2FAF5C5035E@plessthan.com>
References: <mailman.9.1122026401.1328.r-help@stat.math.ethz.ch>
	<1C504145-F77F-4C92-9D4B-E2FAF5C5035E@plessthan.com>
Message-ID: <42E0CFE5.2000503@statistik.uni-dortmund.de>

Dennis Fisher wrote:

> I run R 2.1.1 in a Linux environment (RedHat 9) although my question  
> is not platform-specific.
> 
> Consider the following:
>  > A <- c("Prefix-aaa", "Prefix-bbb", "Prefix-ccc")
>  > B <- strsplit(A, "-")
>  > B
> [[1]]
> [1] "Prefix" "aaa"
> 
> [[2]]
> [1] "Prefix" "bbb"
> 
> [[3]]
> [1] "Prefix" "ccc"
> 
> How do I extract the elements "aaa", "bbb", "ccc" from B?

For example:

sapply(B, "[", 2)

Uwe Ligges


> Thanks in advance.
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-415-564-2220
> www.PLessThan.com
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ghislainv at gmail.com  Fri Jul 22 12:35:01 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Fri, 22 Jul 2005 12:35:01 +0200
Subject: [R] Significant difference of coefficients in glm with factors?
Message-ID: <ff51f0220507220335334d7516@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050722/7e560bc2/attachment.pl

From ramasamy at cancer.org.uk  Fri Jul 22 13:00:36 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 22 Jul 2005 12:00:36 +0100
Subject: [R] Question regarding subsetting
In-Reply-To: <1C504145-F77F-4C92-9D4B-E2FAF5C5035E@plessthan.com>
References: <mailman.9.1122026401.1328.r-help@stat.math.ethz.ch>
	<1C504145-F77F-4C92-9D4B-E2FAF5C5035E@plessthan.com>
Message-ID: <1122030036.6005.46.camel@ipc143004.lif.icnet.uk>

A <- c("Prefix-aaa", "Prefix-bbb", "Prefix-ccc")

sapply( strsplit(A, split="-"), function(x) x[2] )
[1] "aaa" "bbb" "ccc"

Regards, Adai


On Fri, 2005-07-22 at 03:45 -0700, Dennis Fisher wrote:
> I run R 2.1.1 in a Linux environment (RedHat 9) although my question  
> is not platform-specific.
> 
> Consider the following:
>  > A <- c("Prefix-aaa", "Prefix-bbb", "Prefix-ccc")
>  > B <- strsplit(A, "-")
>  > B
> [[1]]
> [1] "Prefix" "aaa"
> 
> [[2]]
> [1] "Prefix" "bbb"
> 
> [[3]]
> [1] "Prefix" "ccc"
> 
> How do I extract the elements "aaa", "bbb", "ccc" from B?
> 
> Thanks in advance.
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-415-564-2220
> www.PLessThan.com
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmb at mrc-dunn.cam.ac.uk  Fri Jul 22 13:42:07 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 22 Jul 2005 12:42:07 +0100 (BST)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <1121981195.4611.54.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.21.0507221236390.28640-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 21 Jul 2005, Marc Schwartz (via MN) wrote:

>[Note: the initial posts have been re-arranged to attempt to maintain
>the flow from top to bottom]
>
>> >Dan Bolser writes:
>> > > 
>> > > I would like to annotate my plot with a little box containing the slope,
>> > > intercept and R^2 of a lm on the data.
>> > > 
>> > > I would like it to look like...
>> > > 
>> > >  +----------------------------+
>> > >  | Slope     :   3.45 +- 0.34 |
>> > >  | Intercept : -10.43 +- 1.42 |
>> > >  | R^2       :   0.78         |
>> > >  +----------------------------+
>> > > 
>> > > However I can't make anything this neat, and I can't find out how to
>> > > combine this with symbols for R^2 / +- (plus minus).
>> > > 
>> > > Below is my best attempt (which is franky quite pour). Can anyone
>> > > improve on the below?
>> > > 
>> > > Specifically, 
>> > > 
>> > > aligned text and numbers, 
>> > > aligned decimal places, 
>> > > symbol for R^2 in the text (expression(R^2) seems to fail with
>> > > 'paste') and +- 
>> > > 
>> > > 
>> > > 
>> > > Cheers,
>> > > Dan.
>> > > 
>> > > 
>> > > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
>> > > 
>> > > abline(coef(dat.lm),lty=2,lwd=1.5)
>> > > 
>> > > 
>> > > dat.lm.sum <- summary(dat.lm)
>> > > dat.lm.sum
>> > > 
>> > > attributes(dat.lm.sum)
>> > > 
>> > > my.text.1 <-
>> > >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
>> > >         "+/-",          round(dat.lm.sum$coefficients[4],2))
>> > > 
>> > > my.text.2 <-
>> > >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
>> > >         "+/-",          round(dat.lm.sum$coefficients[3],2))
>> > > 
>> > > my.text.3 <-
>> > >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
>> > > 
>> > > my.text.1
>> > > my.text.2
>> > > my.text.3
>> > > 
>> > > 
>> > > ## Add legend
>> > > text(x=3,
>> > >      y=300,
>> > >      paste(my.text.1,
>> > >            my.text.2,
>> > >            my.text.3,
>> > >            sep="\n"),
>> > >      adj=c(0,0),
>> > >      cex=1
>
>
>> On Thu, 21 Jul 2005, Christoph Buser wrote:
>> 
>> >Dear Dan
>> >
>> >I can only help you with your third problem, expression and
>> >paste. You can use:
>> >
>> >plot(1:5,1:5, type = "n")
>> >text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
>> >text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
>> >text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)
>
>> >I do not have an elegant solution for the alignment.
>
>
>On Thu, 2005-07-21 at 19:55 +0100, Dan Bolser wrote:
>> Cheers for this.
>> 
>> I was trying to get it to work, but the problem is that I need to replace
>> the values above with variables, from the following code...
>> 
>> 
>> dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
>> dat.lm.sum <- summary(dat.lm)
>> 
>> my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
>> my.slope.2 <- round(dat.lm.sum$coefficients[4],2)
>> 
>> my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
>> my.inter.2 <- round(dat.lm.sum$coefficients[3],2)
>> 
>> my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)
>> 
>> 
>> Anything I try results in either the words 'paste("Slope:", my.slope.1,
>> %+-%my.slope.2,sep="")' being written to the plot, or just
>> 'my.slope.1+-my.slope2' (where the +- is correctly written).
>> 
>> I want to script it up and write all three lines to the plot with
>> 'sep="\n"', rather than deciding three different heights.
>
>
>> Thanks very much for what you gave, its a good start for me to figure out 
>> how I am supposed to be telling R what to do!
>> 
>> Any way to just get fixed width fonts with text? (for the alignment
>> problem)
>
>
>Dan,
>
>Here is one approach. It may not be the best, but it gets the job done.
>You can certainly take this and encapsulate it in a function to automate
>the text/box placement and to pass values as arguments.
>
>A couple of quick concepts:
>
>1. As far as I know, plotmath cannot do multiple lines, so each line in
>your box needs to be done separately.
>
>2. The horizontal alignment is a bit problematic when using expression()
>or bquote() since I don't believe that multiple spaces are honored as
>such after parsing. Thus I break up each component (label, ":" and
>values) into separate text() calls. The labels are left justified.
>
>3. The alignment for the numeric values are done with right
>justification. So, as long as you use a consistent number of decimals in
>the value outputs (2 here), you should be OK. This means you might need
>to use formatC() or sprintf() to control the numeric output values on
>either side of the +/- sign.
>
>4. In the variable replacement, note the use of substitute() and the
>list of x and y arguments as replacement values in the expressions.
>
>
>
># Set your values
>my.slope.1 <- 3.45
>my.slope.2 <- 0.34
>
>my.inter.1 <- -10.43
>my.inter.2 <- 1.42
>
>my.Rsqua <- 0.78
>
>
># Create the initial plot as per Christoph's post
>plot(1:5, 1:5, type = "n")
>
>
>#-------------------------------------
># Do the Slope
>#-------------------------------------
>
>text(1, 4.5,  "Slope", pos = 4)
>text(2, 4.5, ":")
>text(3, 4.5, substitute(x %+-% y, 
>                        list(x = my.slope.1, 
>                             y = my.slope.2)),
>     pos = 2)
>
>
>#-------------------------------------
># Do the Intercept
>#-------------------------------------
>
>text(1, 4.25, "Intercept", pos = 4)
>text(2, 4.25, ":")
>text(3, 4.25, substitute(x %+-% y, 
>                         list(x = my.inter.1, 
>                              y = my.inter.2)),
>     pos = 2)
>
>
>#-------------------------------------
># Do R^2
>#-------------------------------------
>
>text(1, 4.0, expression(R^2), pos = 4)
>text(2, 4.0, ":")
>text(3, 4.0,  substitute(x, list(x = my.Rsqua)),
>     pos = 2)
>
>
>#-------------------------------------
># Do the Box
>#-------------------------------------
>
>rect(1, 3.75, 3, 4.75)
>
>
>You can adjust the x,y coordinates for the various text elements as you
>may require and can also calculate them based upon the xlim, ylim of
>your actual plot. You can also modify the 'cex' argument to text() for
>adjusting the sizes of the fonts in use.


I have been trying many different combinations of the suggestions so
far...

bquote, substitute, expression, paste, etc.

Also I was trying to use the 'expression' part of the 'legend' function to
make thing easier (make new lines, make columns, make rectangles)...

So far all in vain. 

I am sure a fairly neat solution is possible using 'legend' and
'expression', but I cant get a legend which contains a variable
substituted value *and* a special symbol.

Can anyone show me an example of this simple step, and then I can try to
build on that.


Thanks all for suggestions, its one of those 'it seems so easy' problems
(for me).

P.S. Whats with the documentation for bquote?



>BTW, to use a monospaced font, you can set par(family = "mono").
>See ?par for more information.
>
>HTH,
>
>Marc Schwartz
>
>



From chabotd at globetrotter.net  Fri Jul 22 14:30:04 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Fri, 22 Jul 2005 08:30:04 -0400
Subject: [R] PBSmapping and shapefiles
In-Reply-To: <B612B1AC-D971-46BD-8B65-144245576D2B@globetrotter.net>
References: <mailman.8.1121508001.926.r-help@stat.math.ethz.ch>
	<B612B1AC-D971-46BD-8B65-144245576D2B@globetrotter.net>
Message-ID: <DA978538-B003-43DC-9207-660D9A2E1A9A@globetrotter.net>

Hi,

I got no reply to this:
Le 16-Jul-05 ?? 2:42 PM, Denis Chabot a ??crit :

> Hi,
>
> Is there a way, preferably with R, to read shapefiles and transform  
> them in a format that I could then use with package PBSmapping?
>
> I have been able to read such files into R with maptools'  
> read.shape and plot it with plot.Map, but I'd like to bring the  
> data to PBSmapping and plot from there. I also looked at the  
> package shapefile, but it does not seem to do what I want either.
>
> Sincerely,
>
> Denis Chabot
>

but I managed to progress somewhat on my own. Although it does not  
allow one to use shapefiles in PBSmapping, "maptools" at least makes  
it possible to read such files. In some cases I can extract the  
information I want from not-too-complex shapefiles. For instance, to  
extract all the lines corresponding to 60-m isobath in a shapefile, I  
was able to do:

library(maptools)
test <- read.shape("bathy.shp")
test2 <- Map2lines(test)
bathy60 <- subset(test2, test$att.data$Z == 60)

I do not quite understand the structure of bathy60 (list of lists I  
think)
but at this point I resorted to printing bathy60 on the console and  
imported that text into Excel for further cleaning, which is easy  
enough. I'd like to complete the process within R to save time and to  
circumvent Excel's limit of around 64000 lines. But I have a hard  
time figuring out loops in R, coming from a background of  
"observation based" programs such as SAS.

The output of bathy60 looks like this:

[[1]]
            [,1]     [,2]
[1,] -55.99805 51.68817
[2,] -56.00222 51.68911
[3,] -56.01694 51.68911
[4,] -56.03781 51.68606
[5,] -56.04639 51.68759
[6,] -56.04637 51.69445
[7,] -56.03777 51.70207
[8,] -56.02301 51.70892
[9,] -56.01317 51.71578
[10,] -56.00330 51.73481
[11,] -55.99805 51.73840
attr(,"pstart")
attr(,"pstart")$from
[1] 1

attr(,"pstart")$to
[1] 11

attr(,"nParts")
[1] 1
attr(,"shpID")
[1] NA

[[2]]
           [,1]     [,2]
[1,] -57.76294 50.88770
[2,] -57.76292 50.88693
[3,] -57.76033 50.88163
[4,] -57.75668 50.88091
[5,] -57.75551 50.88169
[6,] -57.75562 50.88550
[7,] -57.75932 50.88775
[8,] -57.76294 50.88770
attr(,"pstart")
attr(,"pstart")$from
[1] 1

attr(,"pstart")$to
[1] 8

attr(,"nParts")
[1] 1
attr(,"shpID")
[1] NA

What I need to produce for PBSmapping is a file where each block of  
coordinates shares one ID number, called PID, and a variable POS  
indicating the number of each coordinate within a "shape". All other  
lines must disappear. So the above would become:

PID X Y
1 1 -55.99805 51.68817
1 2 -56.00222 51.68911
1 3 -56.01694 51.68911
1 4 -56.03781 51.68606
1 5 -56.04639 51.68759
1 6 -56.04637 51.69445
1 7 -56.03777 51.70207
1 8 -56.02301 51.70892
1 9 -56.01317 51.71578
1 10 -56.00330 51.73481
1 11 -55.99805 51.73840
2 1 -57.76294 50.88770
2 2 -57.76292 50.88693
2 3 -57.76033 50.88163
2 4 -57.75668 50.88091
2 5 -57.75551 50.88169
2 6 -57.75562 50.88550
2 7 -57.75932 50.88775
2 8 -57.76294 50.88770

I don't know how to do this in R. My algorithm would involve looking  
at the structure of a line, discarding it if not including  
coordinates, and then creating PID and POS for lines with  
coordinates, depending on the content of lines i and i-1. In R?

Thanks in advance,

Denis Chabot



From ggrothendieck at gmail.com  Fri Jul 22 15:05:36 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 09:05:36 -0400
Subject: [R] Question regarding subsetting
In-Reply-To: <1122030036.6005.46.camel@ipc143004.lif.icnet.uk>
References: <mailman.9.1122026401.1328.r-help@stat.math.ethz.ch>
	<1C504145-F77F-4C92-9D4B-E2FAF5C5035E@plessthan.com>
	<1122030036.6005.46.camel@ipc143004.lif.icnet.uk>
Message-ID: <971536df050722060548bd8b84@mail.gmail.com>

There have already been some good solutions but here are three
others just to see a range of approaches:

sub("^[^-]*-", "", A) # remove everything up to first minus

sub("Prefix-", "", A) # simplified version if prefix known

substring(A, 8) # simplification if prefix always 7 chars including minus


> On Fri, 2005-07-22 at 03:45 -0700, Dennis Fisher wrote:
> > I run R 2.1.1 in a Linux environment (RedHat 9) although my question
> > is not platform-specific.
> >
> > Consider the following:
> >  > A <- c("Prefix-aaa", "Prefix-bbb", "Prefix-ccc")
> >  > B <- strsplit(A, "-")
> >  > B
> > [[1]]
> > [1] "Prefix" "aaa"
> >
> > [[2]]
> > [1] "Prefix" "bbb"
> >
> > [[3]]
> > [1] "Prefix" "ccc"
> >
> > How do I extract the elements "aaa", "bbb", "ccc" from B?



From christophe.pouzat at univ-paris5.fr  Fri Jul 22 15:09:30 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 22 Jul 2005 15:09:30 +0200
Subject: [R] About object of class mle returned by user defined functions
In-Reply-To: <42DFC6E0.8050901@univ-paris5.fr>
References: <42DFC6E0.8050901@univ-paris5.fr>
Message-ID: <42E0F00A.7020404@univ-paris5.fr>

Guys,

I apologize for being slightly misleading in my previous e-mail.

First, I generated some confusion between the scale and rate parameters 
in the gamma distribution. My direct call to mle use a minuslogl 
function "working" with a scale parameter while my call to mle from my 
function used a minuslogl function "working" with a rate parameter!... 
To add to the confusion I had simulated data with a scale ( = 1/rate) 
value of 1... I really hope that none of you lost time with that.

Second, some "^2" in my original function definition got converted into 
exponents on the e-mail, meaning that if some of you tried to copy and 
paste it you must have gotten some insults from R while sourcing it. In 
order to avoid that I attach an ".R" file. In principle if you source it 
and then type the following commands you should get (exactly):


 > coef(fitA)
    shape     scale
2.2230421 0.8312374
 > coef(fit1)
    shape     scale
2.2230421 0.8312374
 > vcov(fitA)
            shape       scale
shape  0.08635158 -0.03228829
scale -0.03228829  0.01518126
 > vcov(fit1)
            shape       scale
shape  0.08635158 -0.03228829
scale -0.03228829  0.01518126
 > logLik(fitA)
'log Lik.' -146.6104 (df=2)
 > logLik(fit1)
'log Lik.' -146.6104 (df=2)
 > confint(fitA)
Profiling...
          2.5 %   97.5 %
shape 1.6985621 2.853007
scale 0.6307824 1.129889
 > confint(fit1)
Profiling...
Erreur dans approx(sp$y, sp$x, xout = cutoff) :
    need at least two non-NA values to interpolate
De plus : Message d'avis :
collapsing to unique 'x' values in: approx(sp$y, sp$x, xout = cutoff)

Here fitA is obtained by a direct call to mle (I mean from the command 
line) while fit1 is obtained by the same call but within a function: newFit.

The fundamental problem remains, I don't understand why confint does 
work with fitA and not with fit1.

Christophe.

PS: my version info

platform i686-pc-linux-gnu
arch     i686            
os       linux-gnu       
system   i686, linux-gnu 
status                   
major    2               
minor    1.1             
year     2005            
month    06              
day      20              
language R



Christophe Pouzat wrote:

>Hi,
>
>There is something I don't get with object of class "mle" returned by a 
>function I wrote. More precisely it's about the behaviour of method 
>"confint" and "profile" applied to these object.
>
>I've written a short function (see below) whose arguments are:
>1) A univariate sample (arising from a gamma, log-normal or whatever).
>2) A character string standing for one of the R densities, eg, "gamma", 
>"lnorm", etc. That's the density the user wants to fit to the data.
>3) A named list with initial values for the density parameters; that 
>will be passed to optim via mle.
>4) The method to be used by optim via mle. That can be change by the 
>code if parameter boundaries are also supplied.
>5) The lowest allowed values for the parameters.
>6) The largest allowed values.
>
>The "big" thing this short function does is writing on-fly the 
>corresponding log-likelihood function before calling "mle". The object 
>of class "mle" returned by the call to "mle" is itself returned by the 
>function.
>
>Here is the code:
>
>newFit <- function(isi, ## The data set
>                   isi.density = "gamma", ## The name of the density 
>used as model
>                   initial.para = list( shape = (mean(isi)/sd(isi))^2,
>                     scale = sd(isi)^2 / mean(isi) ), ## Inital 
>parameters passed to optim
>                   optim.method = "BFGS", ## optim method
>                   optim.lower = numeric(length(initial.para)) + 0.00001,
>                   optim.upper = numeric(length(initial.para)) + Inf,
>                   ...) {
>
>  require(stats4)
> 
>  ## Create a string with the log likelihood definition
>  minusLogLikelihood.txt <- paste("function( ",
>                                  paste(names(initial.para), collapse = 
>", "),
>                                  " ) {",
>                                  "isi <- eval(",
>                                  deparse(substitute(isi)),
>                                  ", envir = .GlobalEnv);",
>                                  "-sum(",
>                                  paste("d", isi.density, sep = ""),
>                                  "(isi, ",
>                                  paste(names(initial.para), collapse = 
>", "),
>                                  ", log = TRUE) ) }"
>                                  )
>
>  ## Define logLikelihood function
>  minusLogLikelihood <- eval( parse(text = minusLogLikelihood.txt) )
>  environment(minusLogLikelihood) <- .GlobalEnv
>
> 
>  if ( all( is.infinite( c(optim.lower,optim.upper) ) ) ) {
>      getFit <- mle(minusLogLikelihood,
>                    start = initial.para,
>                    method = optim.method,
>                    ...
>                    )
>  } else {
>    getFit <- mle(minusLogLikelihood,
>                  start = initial.para,
>                  method = "L-BFGS-B",
>                  lower = optim.lower,
>                  upper = optim.upper,
>                  ...
>                  )
>  }  ## End of conditional on all(is.infinite(c(optim.lower,optim.upper)))
> 
>  getFit
> 
>}
>
>
>It seems to work fine on examples like:
>
> > isi1 <- rgamma(100, shape = 2, scale = 1)
> > fit1 <- newFit(isi1) ## fitting here with the "correct" density 
>(initial parameters are obtained by the method of moments)
> > coef(fit1)
>    shape     scale
>1.8210477 0.9514774
> > vcov(fit1)
>           shape      scale
>shape 0.05650600 0.02952371
>scale 0.02952371 0.02039714
> > logLik(fit1)
>'log Lik.' -155.9232 (df=2)
>
>If we compare with a "direct" call to "mle":
>
> > llgamma <- function(sh, sc) -sum(dgamma(isi1, shape = sh, scale = sc, 
>log = TRUE))
> > fitA <- mle(llgamma, start = list( sh = (mean(isi1)/sd(isi1))^2, sc = 
>sd(isi1)^2 / mean(isi1) ),lower = c(0.0001,0.0001), method = "L-BFGS-B")
> > coef(fitA)
>      sh       sc
>1.821042 1.051001
> > vcov(fitA)
>            sh          sc
>sh  0.05650526 -0.03261146
>sc -0.03261146  0.02488714
> > logLik(fitA)
>'log Lik.' -155.9232 (df=2)
>
>I get almost the same estimated parameter values, same log-likelihood 
>but not the same vcov matrix.
>
>A call to "profile" or "confint" on fit1 does not work, eg:
> > confint(fit1)
>Profiling...
>Erreur dans approx(sp$y, sp$x, xout = cutoff) :
>    need at least two non-NA values to interpolate
>De plus : Message d'avis :
>collapsing to unique 'x' values in: approx(sp$y, sp$x, xout = cutoff)
>
>Although calling the log-likelihood function defined in fit1 
>(fit1 at minuslogl) with argument values different from the MLE does return 
>something sensible:
>
> > fit1 at minuslogl(coef(fit1)[1],coef(fit1)[2])
>[1] 155.9232
> > fit1 at minuslogl(coef(fit1)[1]+0.01,coef(fit1)[2]+0.01)
>[1] 155.9263
>
>There is obviously something I'm missing here since I thought for a 
>while that the problem was with the environment "attached" to the 
>function "minusLogLikelihood" when calling "eval"; but the lines above 
>make me think it is not the case...
>
>Any help and/or ideas warmly welcomed.
>
>Thanks,
>
>Christophe.
>
>  
>


-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: testScript.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050722/50371b4c/testScript.pl

From ggrothendieck at gmail.com  Fri Jul 22 15:15:29 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 09:15:29 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507221236390.28640-100000@mail.mrc-dunn.cam.ac.uk>
References: <1121981195.4611.54.camel@localhost.localdomain>
	<Pine.LNX.4.21.0507221236390.28640-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <971536df0507220615342c0541@mail.gmail.com>

Try as.expression(bquote(...whatever...))


On 7/22/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> On Thu, 21 Jul 2005, Marc Schwartz (via MN) wrote:
> 
> >[Note: the initial posts have been re-arranged to attempt to maintain
> >the flow from top to bottom]
> >
> >> >Dan Bolser writes:
> >> > >
> >> > > I would like to annotate my plot with a little box containing the slope,
> >> > > intercept and R^2 of a lm on the data.
> >> > >
> >> > > I would like it to look like...
> >> > >
> >> > >  +----------------------------+
> >> > >  | Slope     :   3.45 +- 0.34 |
> >> > >  | Intercept : -10.43 +- 1.42 |
> >> > >  | R^2       :   0.78         |
> >> > >  +----------------------------+
> >> > >
> >> > > However I can't make anything this neat, and I can't find out how to
> >> > > combine this with symbols for R^2 / +- (plus minus).
> >> > >
> >> > > Below is my best attempt (which is franky quite pour). Can anyone
> >> > > improve on the below?
> >> > >
> >> > > Specifically,
> >> > >
> >> > > aligned text and numbers,
> >> > > aligned decimal places,
> >> > > symbol for R^2 in the text (expression(R^2) seems to fail with
> >> > > 'paste') and +-
> >> > >
> >> > >
> >> > >
> >> > > Cheers,
> >> > > Dan.
> >> > >
> >> > >
> >> > > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> >> > >
> >> > > abline(coef(dat.lm),lty=2,lwd=1.5)
> >> > >
> >> > >
> >> > > dat.lm.sum <- summary(dat.lm)
> >> > > dat.lm.sum
> >> > >
> >> > > attributes(dat.lm.sum)
> >> > >
> >> > > my.text.1 <-
> >> > >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
> >> > >         "+/-",          round(dat.lm.sum$coefficients[4],2))
> >> > >
> >> > > my.text.2 <-
> >> > >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
> >> > >         "+/-",          round(dat.lm.sum$coefficients[3],2))
> >> > >
> >> > > my.text.3 <-
> >> > >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
> >> > >
> >> > > my.text.1
> >> > > my.text.2
> >> > > my.text.3
> >> > >
> >> > >
> >> > > ## Add legend
> >> > > text(x=3,
> >> > >      y=300,
> >> > >      paste(my.text.1,
> >> > >            my.text.2,
> >> > >            my.text.3,
> >> > >            sep="\n"),
> >> > >      adj=c(0,0),
> >> > >      cex=1
> >
> >
> >> On Thu, 21 Jul 2005, Christoph Buser wrote:
> >>
> >> >Dear Dan
> >> >
> >> >I can only help you with your third problem, expression and
> >> >paste. You can use:
> >> >
> >> >plot(1:5,1:5, type = "n")
> >> >text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
> >> >text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
> >> >text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)
> >
> >> >I do not have an elegant solution for the alignment.
> >
> >
> >On Thu, 2005-07-21 at 19:55 +0100, Dan Bolser wrote:
> >> Cheers for this.
> >>
> >> I was trying to get it to work, but the problem is that I need to replace
> >> the values above with variables, from the following code...
> >>
> >>
> >> dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> >> dat.lm.sum <- summary(dat.lm)
> >>
> >> my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
> >> my.slope.2 <- round(dat.lm.sum$coefficients[4],2)
> >>
> >> my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
> >> my.inter.2 <- round(dat.lm.sum$coefficients[3],2)
> >>
> >> my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)
> >>
> >>
> >> Anything I try results in either the words 'paste("Slope:", my.slope.1,
> >> %+-%my.slope.2,sep="")' being written to the plot, or just
> >> 'my.slope.1+-my.slope2' (where the +- is correctly written).
> >>
> >> I want to script it up and write all three lines to the plot with
> >> 'sep="\n"', rather than deciding three different heights.
> >
> >
> >> Thanks very much for what you gave, its a good start for me to figure out
> >> how I am supposed to be telling R what to do!
> >>
> >> Any way to just get fixed width fonts with text? (for the alignment
> >> problem)
> >
> >
> >Dan,
> >
> >Here is one approach. It may not be the best, but it gets the job done.
> >You can certainly take this and encapsulate it in a function to automate
> >the text/box placement and to pass values as arguments.
> >
> >A couple of quick concepts:
> >
> >1. As far as I know, plotmath cannot do multiple lines, so each line in
> >your box needs to be done separately.
> >
> >2. The horizontal alignment is a bit problematic when using expression()
> >or bquote() since I don't believe that multiple spaces are honored as
> >such after parsing. Thus I break up each component (label, ":" and
> >values) into separate text() calls. The labels are left justified.
> >
> >3. The alignment for the numeric values are done with right
> >justification. So, as long as you use a consistent number of decimals in
> >the value outputs (2 here), you should be OK. This means you might need
> >to use formatC() or sprintf() to control the numeric output values on
> >either side of the +/- sign.
> >
> >4. In the variable replacement, note the use of substitute() and the
> >list of x and y arguments as replacement values in the expressions.
> >
> >
> >
> ># Set your values
> >my.slope.1 <- 3.45
> >my.slope.2 <- 0.34
> >
> >my.inter.1 <- -10.43
> >my.inter.2 <- 1.42
> >
> >my.Rsqua <- 0.78
> >
> >
> ># Create the initial plot as per Christoph's post
> >plot(1:5, 1:5, type = "n")
> >
> >
> >#-------------------------------------
> ># Do the Slope
> >#-------------------------------------
> >
> >text(1, 4.5,  "Slope", pos = 4)
> >text(2, 4.5, ":")
> >text(3, 4.5, substitute(x %+-% y,
> >                        list(x = my.slope.1,
> >                             y = my.slope.2)),
> >     pos = 2)
> >
> >
> >#-------------------------------------
> ># Do the Intercept
> >#-------------------------------------
> >
> >text(1, 4.25, "Intercept", pos = 4)
> >text(2, 4.25, ":")
> >text(3, 4.25, substitute(x %+-% y,
> >                         list(x = my.inter.1,
> >                              y = my.inter.2)),
> >     pos = 2)
> >
> >
> >#-------------------------------------
> ># Do R^2
> >#-------------------------------------
> >
> >text(1, 4.0, expression(R^2), pos = 4)
> >text(2, 4.0, ":")
> >text(3, 4.0,  substitute(x, list(x = my.Rsqua)),
> >     pos = 2)
> >
> >
> >#-------------------------------------
> ># Do the Box
> >#-------------------------------------
> >
> >rect(1, 3.75, 3, 4.75)
> >
> >
> >You can adjust the x,y coordinates for the various text elements as you
> >may require and can also calculate them based upon the xlim, ylim of
> >your actual plot. You can also modify the 'cex' argument to text() for
> >adjusting the sizes of the fonts in use.
> 
> 
> I have been trying many different combinations of the suggestions so
> far...
> 
> bquote, substitute, expression, paste, etc.
> 
> Also I was trying to use the 'expression' part of the 'legend' function to
> make thing easier (make new lines, make columns, make rectangles)...
> 
> So far all in vain.
> 
> I am sure a fairly neat solution is possible using 'legend' and
> 'expression', but I cant get a legend which contains a variable
> substituted value *and* a special symbol.
> 
> Can anyone show me an example of this simple step, and then I can try to
> build on that.
> 
> 
> Thanks all for suggestions, its one of those 'it seems so easy' problems
> (for me).
> 
> P.S. Whats with the documentation for bquote?
> 
> 
> 
> >BTW, to use a monospaced font, you can set par(family = "mono").
> >See ?par for more information.
> >
> >HTH,
> >
> >Marc Schwartz
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From stagiaire2.urc at nck.ap-hop-paris.fr  Fri Jul 22 15:35:31 2005
From: stagiaire2.urc at nck.ap-hop-paris.fr (Claude Messiaen - Urc Necker)
Date: Fri, 22 Jul 2005 15:35:31 +0200
Subject: [R]  memory cleaning
Message-ID: <003901c58ec2$3b35fa40$39c9900a@nck.aphopparis.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050722/9a51b0e5/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Fri Jul 22 15:40:56 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 22 Jul 2005 14:40:56 +0100 (BST)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <971536df0507220615342c0541@mail.gmail.com>
Message-ID: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 22 Jul 2005, Gabor Grothendieck wrote:

>Try as.expression(bquote(...whatever...))

Sob, wimper, etc.


my.slope.1 <-   3.22
my.slope.2 <-   0.13
my.inter.1 <- -10.66
my.inter.2 <-   1.96
my.Rsqua.1 <-   0.97



text(2,5,
  paste("Slope:  ",
        as.expression(bquote(.(my.slope.1)%+-%.(my.slope.2)))))


That puts the text and variable together, but not the symbol, which is
printed as '%+-%'

The following is nearly right, but wrong enough it makes my first attempt
look reasonable...


# Create the initial plot as per Christoph's post
plot(1:5, 1:5, type = "n")

legend(x=1,
       y=4.5,
       bg='white',
       ncol=3,
       c("Slope","Intercept",expression(R^2),
         ":",":",":",
         bquote( .(my.slope.1)%+-%.(my.slope.2)),
         bquote( .(my.inter.1)%+-%.(my.inter.2)),
         my.Rsqua.1
         )
       )

Anyone got any ideas on how to fix the above?

Namely 1) right align numbers, 2) remove excessive white space.

I like the above because it dosn't require me to calculate exactly where
to put each piece of text.


I just want to annotate a plot :(



>
>
>On 7/22/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
>> On Thu, 21 Jul 2005, Marc Schwartz (via MN) wrote:
>> 
>> >[Note: the initial posts have been re-arranged to attempt to maintain
>> >the flow from top to bottom]
>> >
>> >> >Dan Bolser writes:
>> >> > >
>> >> > > I would like to annotate my plot with a little box containing the slope,
>> >> > > intercept and R^2 of a lm on the data.
>> >> > >
>> >> > > I would like it to look like...
>> >> > >
>> >> > >  +----------------------------+
>> >> > >  | Slope     :   3.45 +- 0.34 |
>> >> > >  | Intercept : -10.43 +- 1.42 |
>> >> > >  | R^2       :   0.78         |
>> >> > >  +----------------------------+
>> >> > >
>> >> > > However I can't make anything this neat, and I can't find out how to
>> >> > > combine this with symbols for R^2 / +- (plus minus).
>> >> > >
>> >> > > Below is my best attempt (which is franky quite pour). Can anyone
>> >> > > improve on the below?
>> >> > >
>> >> > > Specifically,
>> >> > >
>> >> > > aligned text and numbers,
>> >> > > aligned decimal places,
>> >> > > symbol for R^2 in the text (expression(R^2) seems to fail with
>> >> > > 'paste') and +-
>> >> > >
>> >> > >
>> >> > >
>> >> > > Cheers,
>> >> > > Dan.
>> >> > >
>> >> > >
>> >> > > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
>> >> > >
>> >> > > abline(coef(dat.lm),lty=2,lwd=1.5)
>> >> > >
>> >> > >
>> >> > > dat.lm.sum <- summary(dat.lm)
>> >> > > dat.lm.sum
>> >> > >
>> >> > > attributes(dat.lm.sum)
>> >> > >
>> >> > > my.text.1 <-
>> >> > >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
>> >> > >         "+/-",          round(dat.lm.sum$coefficients[4],2))
>> >> > >
>> >> > > my.text.2 <-
>> >> > >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
>> >> > >         "+/-",          round(dat.lm.sum$coefficients[3],2))
>> >> > >
>> >> > > my.text.3 <-
>> >> > >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
>> >> > >
>> >> > > my.text.1
>> >> > > my.text.2
>> >> > > my.text.3
>> >> > >
>> >> > >
>> >> > > ## Add legend
>> >> > > text(x=3,
>> >> > >      y=300,
>> >> > >      paste(my.text.1,
>> >> > >            my.text.2,
>> >> > >            my.text.3,
>> >> > >            sep="\n"),
>> >> > >      adj=c(0,0),
>> >> > >      cex=1
>> >
>> >
>> >> On Thu, 21 Jul 2005, Christoph Buser wrote:
>> >>
>> >> >Dear Dan
>> >> >
>> >> >I can only help you with your third problem, expression and
>> >> >paste. You can use:
>> >> >
>> >> >plot(1:5,1:5, type = "n")
>> >> >text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
>> >> >text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
>> >> >text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)
>> >
>> >> >I do not have an elegant solution for the alignment.
>> >
>> >
>> >On Thu, 2005-07-21 at 19:55 +0100, Dan Bolser wrote:
>> >> Cheers for this.
>> >>
>> >> I was trying to get it to work, but the problem is that I need to replace
>> >> the values above with variables, from the following code...
>> >>
>> >>
>> >> dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
>> >> dat.lm.sum <- summary(dat.lm)
>> >>
>> >> my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
>> >> my.slope.2 <- round(dat.lm.sum$coefficients[4],2)
>> >>
>> >> my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
>> >> my.inter.2 <- round(dat.lm.sum$coefficients[3],2)
>> >>
>> >> my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)
>> >>
>> >>
>> >> Anything I try results in either the words 'paste("Slope:", my.slope.1,
>> >> %+-%my.slope.2,sep="")' being written to the plot, or just
>> >> 'my.slope.1+-my.slope2' (where the +- is correctly written).
>> >>
>> >> I want to script it up and write all three lines to the plot with
>> >> 'sep="\n"', rather than deciding three different heights.
>> >
>> >
>> >> Thanks very much for what you gave, its a good start for me to figure out
>> >> how I am supposed to be telling R what to do!
>> >>
>> >> Any way to just get fixed width fonts with text? (for the alignment
>> >> problem)
>> >
>> >
>> >Dan,
>> >
>> >Here is one approach. It may not be the best, but it gets the job done.
>> >You can certainly take this and encapsulate it in a function to automate
>> >the text/box placement and to pass values as arguments.
>> >
>> >A couple of quick concepts:
>> >
>> >1. As far as I know, plotmath cannot do multiple lines, so each line in
>> >your box needs to be done separately.
>> >
>> >2. The horizontal alignment is a bit problematic when using expression()
>> >or bquote() since I don't believe that multiple spaces are honored as
>> >such after parsing. Thus I break up each component (label, ":" and
>> >values) into separate text() calls. The labels are left justified.
>> >
>> >3. The alignment for the numeric values are done with right
>> >justification. So, as long as you use a consistent number of decimals in
>> >the value outputs (2 here), you should be OK. This means you might need
>> >to use formatC() or sprintf() to control the numeric output values on
>> >either side of the +/- sign.
>> >
>> >4. In the variable replacement, note the use of substitute() and the
>> >list of x and y arguments as replacement values in the expressions.
>> >
>> >
>> >
>> ># Set your values
>> >my.slope.1 <- 3.45
>> >my.slope.2 <- 0.34
>> >
>> >my.inter.1 <- -10.43
>> >my.inter.2 <- 1.42
>> >
>> >my.Rsqua <- 0.78
>> >
>> >
>> ># Create the initial plot as per Christoph's post
>> >plot(1:5, 1:5, type = "n")
>> >
>> >
>> >#-------------------------------------
>> ># Do the Slope
>> >#-------------------------------------
>> >
>> >text(1, 4.5,  "Slope", pos = 4)
>> >text(2, 4.5, ":")
>> >text(3, 4.5, substitute(x %+-% y,
>> >                        list(x = my.slope.1,
>> >                             y = my.slope.2)),
>> >     pos = 2)
>> >
>> >
>> >#-------------------------------------
>> ># Do the Intercept
>> >#-------------------------------------
>> >
>> >text(1, 4.25, "Intercept", pos = 4)
>> >text(2, 4.25, ":")
>> >text(3, 4.25, substitute(x %+-% y,
>> >                         list(x = my.inter.1,
>> >                              y = my.inter.2)),
>> >     pos = 2)
>> >
>> >
>> >#-------------------------------------
>> ># Do R^2
>> >#-------------------------------------
>> >
>> >text(1, 4.0, expression(R^2), pos = 4)
>> >text(2, 4.0, ":")
>> >text(3, 4.0,  substitute(x, list(x = my.Rsqua)),
>> >     pos = 2)
>> >
>> >
>> >#-------------------------------------
>> ># Do the Box
>> >#-------------------------------------
>> >
>> >rect(1, 3.75, 3, 4.75)
>> >
>> >
>> >You can adjust the x,y coordinates for the various text elements as you
>> >may require and can also calculate them based upon the xlim, ylim of
>> >your actual plot. You can also modify the 'cex' argument to text() for
>> >adjusting the sizes of the fonts in use.
>> 
>> 
>> I have been trying many different combinations of the suggestions so
>> far...
>> 
>> bquote, substitute, expression, paste, etc.
>> 
>> Also I was trying to use the 'expression' part of the 'legend' function to
>> make thing easier (make new lines, make columns, make rectangles)...
>> 
>> So far all in vain.
>> 
>> I am sure a fairly neat solution is possible using 'legend' and
>> 'expression', but I cant get a legend which contains a variable
>> substituted value *and* a special symbol.
>> 
>> Can anyone show me an example of this simple step, and then I can try to
>> build on that.
>> 
>> 
>> Thanks all for suggestions, its one of those 'it seems so easy' problems
>> (for me).
>> 
>> P.S. Whats with the documentation for bquote?
>> 
>> 
>> 
>> >BTW, to use a monospaced font, you can set par(family = "mono").
>> >See ?par for more information.
>> >
>> >HTH,
>> >
>> >Marc Schwartz
>> >
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>



From stagiaire2.urc at nck.ap-hop-paris.fr  Fri Jul 22 15:40:04 2005
From: stagiaire2.urc at nck.ap-hop-paris.fr (Claude Messiaen - Urc Necker)
Date: Fri, 22 Jul 2005 15:40:04 +0200
Subject: [R]  memory cleaning
Message-ID: <004901c58ec2$dda6e870$39c9900a@nck.aphopparis.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050722/4e867802/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jul 22 15:47:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2005 14:47:39 +0100 (BST)
Subject: [R] About object of class mle returned by user defined functions
In-Reply-To: <42E0F00A.7020404@univ-paris5.fr>
References: <42DFC6E0.8050901@univ-paris5.fr> <42E0F00A.7020404@univ-paris5.fr>
Message-ID: <Pine.LNX.4.61.0507221436300.31141@gannet.stats>

confint has to be able to re-fit the function to form the profile 
likelihood.  The fit you return refers to values inside the function you 
used, and those are not available in the environment you call confint 
from.  You need to ensure that those values are substituted and not 
referred to.

Compare

> fitA at call
mle(minuslogl = llgamma, start = list(shape = (mean(isi1)/sd(isi1))^2,
     scale = sd(isi1)^2/mean(isi1)), method = "L-BFGS-B", lower = c(1e-05,
     1e-05))
> fit1 at call
mle(minuslogl = minusLogLikelihood, start = initial.para, method = 
"L-BFGS-B",
     lower = optim.lower, upper = optim.upper)

and the difference should be clear.

You can fix this up by constructing a call containing the values and not 
the names (one way is to use substitute()) and then eval() it.  Another 
way is something like

Call <- quote(mle())
Call$minuslogl <- minusLogLikelihood
Call$start <- initial.para
...

On Fri, 22 Jul 2005, Christophe Pouzat wrote:

> Guys,
>
> I apologize for being slightly misleading in my previous e-mail.
>
> First, I generated some confusion between the scale and rate parameters in 
> the gamma distribution. My direct call to mle use a minuslogl function 
> "working" with a scale parameter while my call to mle from my function used a 
> minuslogl function "working" with a rate parameter!... To add to the 
> confusion I had simulated data with a scale ( = 1/rate) value of 1... I 
> really hope that none of you lost time with that.
>
> Second, some "^2" in my original function definition got converted into 
> exponents on the e-mail, meaning that if some of you tried to copy and paste 
> it you must have gotten some insults from R while sourcing it. In order to 
> avoid that I attach an ".R" file. In principle if you source it and then type 
> the following commands you should get (exactly):
>
>
>> coef(fitA)
>   shape     scale
> 2.2230421 0.8312374
>> coef(fit1)
>   shape     scale
> 2.2230421 0.8312374
>> vcov(fitA)
>           shape       scale
> shape  0.08635158 -0.03228829
> scale -0.03228829  0.01518126
>> vcov(fit1)
>           shape       scale
> shape  0.08635158 -0.03228829
> scale -0.03228829  0.01518126
>> logLik(fitA)
> 'log Lik.' -146.6104 (df=2)
>> logLik(fit1)
> 'log Lik.' -146.6104 (df=2)
>> confint(fitA)
> Profiling...
>         2.5 %   97.5 %
> shape 1.6985621 2.853007
> scale 0.6307824 1.129889
>> confint(fit1)
> Profiling...
> Erreur dans approx(sp$y, sp$x, xout = cutoff) :
>   need at least two non-NA values to interpolate
> De plus : Message d'avis :
> collapsing to unique 'x' values in: approx(sp$y, sp$x, xout = cutoff)
>
> Here fitA is obtained by a direct call to mle (I mean from the command line) 
> while fit1 is obtained by the same call but within a function: newFit.
>
> The fundamental problem remains, I don't understand why confint does work 
> with fitA and not with fit1.
>
> Christophe.
>
> PS: my version info
>
> platform i686-pc-linux-gnu
> arch     i686            os       linux-gnu       system   i686, linux-gnu 
> status                   major    2               minor    1.1 
> year     2005            month    06              day      20 
> language R
>
>
>
> Christophe Pouzat wrote:
>
>> Hi,
>> 
>> There is something I don't get with object of class "mle" returned by a 
>> function I wrote. More precisely it's about the behaviour of method 
>> "confint" and "profile" applied to these object.
>> 
>> I've written a short function (see below) whose arguments are:
>> 1) A univariate sample (arising from a gamma, log-normal or whatever).
>> 2) A character string standing for one of the R densities, eg, "gamma", 
>> "lnorm", etc. That's the density the user wants to fit to the data.
>> 3) A named list with initial values for the density parameters; that will 
>> be passed to optim via mle.
>> 4) The method to be used by optim via mle. That can be change by the code 
>> if parameter boundaries are also supplied.
>> 5) The lowest allowed values for the parameters.
>> 6) The largest allowed values.
>> 
>> The "big" thing this short function does is writing on-fly the 
>> corresponding log-likelihood function before calling "mle". The object of 
>> class "mle" returned by the call to "mle" is itself returned by the 
>> function.
>> 
>> Here is the code:
>> 
>> newFit <- function(isi, ## The data set
>>                   isi.density = "gamma", ## The name of the density used as 
>> model
>>                   initial.para = list( shape = (mean(isi)/sd(isi))^2,
>>                     scale = sd(isi)^2 / mean(isi) ), ## Inital parameters 
>> passed to optim
>>                   optim.method = "BFGS", ## optim method
>>                   optim.lower = numeric(length(initial.para)) + 0.00001,
>>                   optim.upper = numeric(length(initial.para)) + Inf,
>>                   ...) {
>> 
>>  require(stats4)
>> 
>>  ## Create a string with the log likelihood definition
>>  minusLogLikelihood.txt <- paste("function( ",
>>                                  paste(names(initial.para), collapse = ", 
>> "),
>>                                  " ) {",
>>                                  "isi <- eval(",
>>                                  deparse(substitute(isi)),
>>                                  ", envir = .GlobalEnv);",
>>                                  "-sum(",
>>                                  paste("d", isi.density, sep = ""),
>>                                  "(isi, ",
>>                                  paste(names(initial.para), collapse = ", 
>> "),
>>                                  ", log = TRUE) ) }"
>>                                  )
>> 
>>  ## Define logLikelihood function
>>  minusLogLikelihood <- eval( parse(text = minusLogLikelihood.txt) )
>>  environment(minusLogLikelihood) <- .GlobalEnv
>> 
>> 
>>  if ( all( is.infinite( c(optim.lower,optim.upper) ) ) ) {
>>      getFit <- mle(minusLogLikelihood,
>>                    start = initial.para,
>>                    method = optim.method,
>>                    ...
>>                    )
>>  } else {
>>    getFit <- mle(minusLogLikelihood,
>>                  start = initial.para,
>>                  method = "L-BFGS-B",
>>                  lower = optim.lower,
>>                  upper = optim.upper,
>>                  ...
>>                  )
>>  }  ## End of conditional on all(is.infinite(c(optim.lower,optim.upper)))
>> 
>>  getFit
>> 
>> }
>> 
>> 
>> It seems to work fine on examples like:
>> 
>> > isi1 <- rgamma(100, shape = 2, scale = 1)
>> > fit1 <- newFit(isi1) ## fitting here with the "correct" density (initial 
>> parameters are obtained by the method of moments)
>> > coef(fit1)
>>    shape     scale
>> 1.8210477 0.9514774
>> > vcov(fit1)
>>           shape      scale
>> shape 0.05650600 0.02952371
>> scale 0.02952371 0.02039714
>> > logLik(fit1)
>> 'log Lik.' -155.9232 (df=2)
>> 
>> If we compare with a "direct" call to "mle":
>> 
>> > llgamma <- function(sh, sc) -sum(dgamma(isi1, shape = sh, scale = sc, log 
>> = TRUE))
>> > fitA <- mle(llgamma, start = list( sh = (mean(isi1)/sd(isi1))^2, sc = 
>> sd(isi1)^2 / mean(isi1) ),lower = c(0.0001,0.0001), method = "L-BFGS-B")
>> > coef(fitA)
>>      sh       sc
>> 1.821042 1.051001
>> > vcov(fitA)
>>            sh          sc
>> sh  0.05650526 -0.03261146
>> sc -0.03261146  0.02488714
>> > logLik(fitA)
>> 'log Lik.' -155.9232 (df=2)
>> 
>> I get almost the same estimated parameter values, same log-likelihood but 
>> not the same vcov matrix.
>> 
>> A call to "profile" or "confint" on fit1 does not work, eg:
>> > confint(fit1)
>> Profiling...
>> Erreur dans approx(sp$y, sp$x, xout = cutoff) :
>>    need at least two non-NA values to interpolate
>> De plus : Message d'avis :
>> collapsing to unique 'x' values in: approx(sp$y, sp$x, xout = cutoff)
>> 
>> Although calling the log-likelihood function defined in fit1 
>> (fit1 at minuslogl) with argument values different from the MLE does return 
>> something sensible:
>> 
>> > fit1 at minuslogl(coef(fit1)[1],coef(fit1)[2])
>> [1] 155.9232
>> > fit1 at minuslogl(coef(fit1)[1]+0.01,coef(fit1)[2]+0.01)
>> [1] 155.9263
>> 
>> There is obviously something I'm missing here since I thought for a while 
>> that the problem was with the environment "attached" to the function 
>> "minusLogLikelihood" when calling "eval"; but the lines above make me think 
>> it is not the case...
>> 
>> Any help and/or ideas warmly welcomed.
>> 
>> Thanks,
>> 
>> Christophe.
>> 
>> 
>
>
> -- 
> A Master Carpenter has many tools and is expert with most of them.If you
> only know how to use a hammer, every problem starts to look like a nail.
> Stay away from that trap.
> Richard B Johnson.
> --
>
> Christophe Pouzat
> Laboratoire de Physiologie Cerebrale
> CNRS UMR 8118
> UFR biomedicale de l'Universite Paris V
> 45, rue des Saints Peres
> 75006 PARIS
> France
>
> tel: +33 (0)1 42 86 38 28
> fax: +33 (0)1 42 86 38 30
> web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jul 22 15:52:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2005 14:52:13 +0100 (BST)
Subject: [R] memory cleaning
In-Reply-To: <003901c58ec2$3b35fa40$39c9900a@nck.aphopparis.fr>
References: <003901c58ec2$3b35fa40$39c9900a@nck.aphopparis.fr>
Message-ID: <Pine.LNX.4.61.0507221448250.31141@gannet.stats>

See help("gc") and read up about garbage collection.  Memory measurements 
without a preceding gc() are meaningless.

You have not told us your version of R, but 2.1.0 and later do use much 
less memory in write.table.  Nevertheless, they do have to convert each 
column to character and that does take some memory.

On Fri, 22 Jul 2005, Claude  Messiaen - Urc Necker wrote:

> Hi R Users,
> After some research I haven't find what I want.
> I'm manipulating a dataframe with 70k rows and 30 variables, and I run out of memory when exporting this in a *.txt file
>
> after some computing I have used :
>
>> memory.size()/1048576.0
> [1] 103.7730
>
> and I make my export :
>
>>  write.table(cox,"d:/tablefinal2.txt",row.names=F,sep=';')
>>  memory.size()/1048576.0
> [1] 241.9730
>
> I'm surprised so I try removing some objects :
>>  rm (trait,tany,tnor,toth,suivauxdany,dnor,doth,mod1,
> mod2,mod3,lok1,lok2,lok3,aux,risque,risk)
> and check memory space :
>> memory.size()/1048576.0
> [1] 242.1095
>
> First, I don't understand why when removing objects the memory used increase ?
> Next, why the memory used double when I make an export ?
> I look forward to your reply
>
> Claude
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kevin.thorpe at utoronto.ca  Fri Jul 22 15:58:09 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 22 Jul 2005 09:58:09 -0400
Subject: [R] Cannot update some packages after upgrade to 2.1.1
In-Reply-To: <Pine.LNX.4.61.0507201500490.27355@gannet.stats>
References: <42D7FB1D.2040302@utoronto.ca>
	<Pine.LNX.4.61.0507160802370.538@gannet.stats>
	<42DE583D.3040802@utoronto.ca>
	<Pine.LNX.4.61.0507201500490.27355@gannet.stats>
Message-ID: <42E0FB71.8030806@utoronto.ca>

Prof Brian Ripley wrote:

> On Wed, 20 Jul 2005, Kevin E. Thorpe wrote:
>
>> Thank you for the information. I have contacted the RPM maintainer 
>> and am awaiting a response.
>>
>> It occurs to me that my "problem" could also be fixed by putting 
>> ATLAS on my system. Are there advantages to doing that or any reasons 
>> not to?
>
>
> It is a good idea unless you share R systems between different 
> architectures (even down to the vintage of P4 or Xeon chips). We do 
> and so tend to avoid ATLAS (which gets statically compiled in by 
> default).

After following the discussion between you and Uwe, and thinking a bit, 
I've decided to give it a try. My system is basically stand-alone so 
different architectures are not an issue.

Now my question (recognising this may be the wrong list). I successfully 
compiled ATLAS, but it isn't clear to me from the install instructions 
that came with it how to make the libraries "seen" by gcc and g77 and 
therefore, presumably by the configure script with the R source. How do 
I make the libraries available? I've looked at the ATLAS project page 
and done a couple RSiteSearch's but have not yet recognised the answer 
to my question.

>>
>> Prof Brian Ripley wrote:
>>
>>> -lf77blas is part of ATLAS, so I do suspect the RPM builder had 
>>> ATLAS installed.
>>>
>>> lme4 needs a compatible Matrix installed.
>>>
>>> I do think installing from the sources would solve this, but 
>>> probably you need to discuss this with the RPM maintainer as a 
>>> dependency appears to be missing.
>>>
>>> On Fri, 15 Jul 2005, Kevin E. Thorpe wrote:
>>>
>>>> I just upgraded to version 2.1.1 (from 2.0.1) today.
>>>>
>>>> > R.version
>>>> _
>>>> platform i686-pc-linux-gnu
>>>> arch i686
>>>> os linux-gnu
>>>> system i686, linux-gnu
>>>> status
>>>> major 2
>>>> minor 1.1
>>>> year 2005
>>>> month 06
>>>> day 20
>>>> language R
>>>>
>>>> I am using SuSE 9.2 and did the upgrade using rpm -U with the RPM
>>>> available on CRAN. After upgrading r-base, I ran update.packages().
>>>> Four previously installed packages failed to update:
>>>>
>>>> Matrix (0.95-5 to 0.97-4)
>>>> gam (0.93 to 0.94)
>>>> lme4 (0.95-3 to 0.96-1)
>>>> mgcv (1.3-1 to 1.3-4)
>>>>
>>>> In the case of Matrix, gam and mgcv I get the message:
>>>>
>>>> [long path]/ld: cannot find -lf77blas
>>>>
>>>> In the case of lme4 the messages are:
>>>>
>>>> ** preparing package for lazy loading
>>>> Error in setMethod("coef", signature(object = "lmList"), 
>>>> function(object, :
>>>> no existing definition for function 'coef'
>>>> Error: unable to load R code in package 'lme4'
>>>> Execution halted
>>>> ERROR: lazy loading failed for package 'lme4'
>>>>
>>>> I have searched the SuSE repository for any package that provides
>>>> f77blas but came up empty.
>>>>
>>>> I also could not identify any relevant messages in the mailing list
>>>> archives.
>>>>
>>>> Is R looking for that library because it was present on the machine
>>>> the RPM was built on? Would building R myself solve the missing 
>>>> library
>>>> problem or did I do something wrong?
>>>>

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From maechler at stat.math.ethz.ch  Fri Jul 22 16:09:00 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 22 Jul 2005 16:09:00 +0200
Subject: [R] bubble.plot() - standardize size of unit circle
In-Reply-To: <20050721110545.67741.qmail@web26301.mail.ukl.yahoo.com>
References: <20050721110545.67741.qmail@web26301.mail.ukl.yahoo.com>
Message-ID: <17120.65020.31271.410640@stat.math.ethz.ch>

Hi,

>>>>> "Dan" == Dan Bebber <danbebber at yahoo.co.uk>
>>>>>     on Thu, 21 Jul 2005 12:05:45 +0100 (BST) writes:

    Dan> Hello,
    Dan> I wrote a wrapper for symbols() that produces a
    Dan> bivariate bubble plot, for use when plot(x,y) hides
    Dan> multiple occurrences of the same x,y combination (e.g.
    Dan> if x,y are integers).
    Dan> Circle area ~ counts per bin, and circle size is
    Dan> controlled by 'scale'.

I'm not answering your question, but still, I need to ask/tell
this:

Why don't use  sunflowerplot() instead?

The excellent researchers who invented sunflower plots in the
late 70s early 1980s knew well about "bubble" alternatives and
much about drawbacks of such bubbles
{mainly the perception laws of areas vs lengths ..}

That's why they came up with the sunflowers as improvement ..
See 'References' in   help(sunflowerplot)

Regards,
Martin Maechler, ETH Zurich




    Dan> Question: how can I automatically make the smallest
    Dan> circle the same size as a standard plot character,
    Dan> rather than having to approximate it using 'scale'?

    Dan> #Function:
    Dan> bubble.plot<-function(x,y,scale=0.1,xlab=substitute(x),ylab=substitute(y),...){
    Dan> z<-table(x,y)
    Dan> xx<-rep(as.numeric(rownames(z)),ncol(z))
    Dan> yy<-sort(rep(as.numeric(colnames(z)),nrow(z)))
    Dan> id<-which(z!=0)
    Dan> symbols(xx[id],yy[id],inches=F,circles=sqrt(z[id])*scale,xlab=xlab,ylab=ylab,...)}

    Dan> #Example:
    Dan> x<-rpois(100,3)
    Dan> y<-x+rpois(100,2)
    Dan> bubble.plot(x,y)


		
    Dan> ___________________________________________________________ 
    Dan> How much free photo storage do you get? Store your holiday

    Dan> ______________________________________________
    Dan> R-help at stat.math.ethz.ch mailing list
    Dan> https://stat.ethz.ch/mailman/listinfo/r-help
    Dan> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


    Dan> !DSPAM:42df82ae77251002021314!



From herodote at oreka.com  Fri Jul 22 16:10:46 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Fri, 22 Jul 2005 15:10:46 +0100
Subject: [R] =?iso-8859-1?q?multiplicate_2_functions?=
Message-ID: <IK18PY$BAA38222266B9A06A451C1056F8B299A@oreka.com>

Thks for your answer,
here is an exemple of what i do with the errors in french...
> tmp
[1] 200 150 245 125 134 345 320 450 678
> beta18
Erreur : Objet "beta18" not found //NORMAL just to show it
> eta
[1] 500
> func1<-function(beta18) dweibull(tmp[1],beta18,eta)

> func1<-func1(beta18) * function(beta18)
dweibull(tmp[2],beta18,eta)

Erreur dans dweibull(tmp[1], beta18, eta) : Objet "beta18"
not found 
//I don't want to evaluate weibull immediatly just want to
initialize the function then integrate on beta18,i have to
make the multiplication of weibull at each time pointed by
tmp then integrate it to obtain a norm.

> func1<-func1() * function(beta18) dweibull(tmp[2],beta18,eta)

Erreur dans dweibull(tmp[1], beta18, eta) : l'argument
"beta18" est manquant, avec aucune valeur par d??faut

> func1<-func1 * function(beta18) dweibull(tmp[2],beta18,eta)

Erreur dans func1 * function(beta18) dweibull(tmp[2],
beta18, eta) :
argument non num??rique pour un op??rateur binaire

> func1<-func1 & function(beta18) dweibull(tmp[2],beta18,eta)

Erreur dans func1 & function(beta18) dweibull(tmp[2],
beta18, eta) :
ces op??rations ne sont possibles que pour des types
num??riques ou logiques

> func1<-func1(beta18) & function(beta18)
dweibull(tmp[2],beta18,eta)

Erreur dans dweibull(tmp[1], beta18, eta) : Objet "beta18"
non trouv??

I hope you understand what i want to do.

thks
guillaume
---------- Initial Header -----------

>From : Uwe Ligges <ligges at statistik.uni-dortmund.de>
To : "herodote at oreka.com" <herodote at oreka.com>
Cc : r-help <r-help at stat.math.ethz.ch>
Date : Fri, 22 Jul 2005 12:51:47 +0200
Subject : Re: [R] Generate a function

herodote at oreka.com wrote:

> hi all,
> 
> I need to generate a function inside a loop:
> 
> tmp is an array

Well, a 1-d array, or better say a vector of length 10,
given the code 
below is correct.


> for (i in 1:10)
> {
> func<- func * function(beta1) dweibull(tmp[i],beta1,eta)
> }
> 
> because then i need to integrate this function on beta.
> 
> I could have written this :
> 
> func<-function(beta1) prod(dweibull(tmp,beta1,eta)) (with
eta and beta1 set)
> 
> but it is unplottable and no integrable... i could make it
a bit different but if i do that ( prod(tmp)=~Inf ) i'm stuck.
>
>
> I've looked in R-poetry and i didn't find anything usefull.
> 
> I think i have a problem with how i tell R my function is,
R seems to think it is a function like programmers do but
not a f(x) function.

Please specify a reproducible example (as the posting guide
asks to do), 
that means including the values for tmp, beta1 and eta.
Then we might be able to explain where you have your problems.

Uwe Ligges

> Thks 
> guillaume
> 
> ////////////////////////////////////////////////////////////
> // Webmail Oreka : http://www.oreka.com
> ////////////////////////////////////////////////////////////
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////


////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From danbebber at yahoo.co.uk  Fri Jul 22 16:20:13 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Fri, 22 Jul 2005 15:20:13 +0100 (BST)
Subject: [R] bubble.plot() - standardize size of unit circle
In-Reply-To: <17120.65020.31271.410640@stat.math.ethz.ch>
Message-ID: <20050722142013.55059.qmail@web26303.mail.ukl.yahoo.com>

Thanks Martin- I didn't know about the sunflowerplot
function. Somehow I prefer the look of bubbles, but I
guess you're right about the visual perception. Will
have to read up on it.

Dan

--- Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> Hi,
> 
> >>>>> "Dan" == Dan Bebber <danbebber at yahoo.co.uk>
> >>>>>     on Thu, 21 Jul 2005 12:05:45 +0100 (BST)
> writes:
> 
>     Dan> Hello,
>     Dan> I wrote a wrapper for symbols() that
> produces a
>     Dan> bivariate bubble plot, for use when
> plot(x,y) hides
>     Dan> multiple occurrences of the same x,y
> combination (e.g.
>     Dan> if x,y are integers).
>     Dan> Circle area ~ counts per bin, and circle
> size is
>     Dan> controlled by 'scale'.
> 
> I'm not answering your question, but still, I need
> to ask/tell
> this:
> 
> Why don't use  sunflowerplot() instead?
> 
> The excellent researchers who invented sunflower
> plots in the
> late 70s early 1980s knew well about "bubble"
> alternatives and
> much about drawbacks of such bubbles
> {mainly the perception laws of areas vs lengths ..}
> 
> That's why they came up with the sunflowers as
> improvement ..
> See 'References' in   help(sunflowerplot)
> 
> Regards,
> Martin Maechler, ETH Zurich
> 
> 
> 
> 
>     Dan> Question: how can I automatically make the
> smallest
>     Dan> circle the same size as a standard plot
> character,
>     Dan> rather than having to approximate it using
> 'scale'?
> 
>     Dan> #Function:
>     Dan>
>
bubble.plot<-function(x,y,scale=0.1,xlab=substitute(x),ylab=substitute(y),...){
>     Dan> z<-table(x,y)
>     Dan> xx<-rep(as.numeric(rownames(z)),ncol(z))
>     Dan>
> yy<-sort(rep(as.numeric(colnames(z)),nrow(z)))
>     Dan> id<-which(z!=0)
>     Dan>
>
symbols(xx[id],yy[id],inches=F,circles=sqrt(z[id])*scale,xlab=xlab,ylab=ylab,...)}
> 
>     Dan> #Example:
>     Dan> x<-rpois(100,3)
>     Dan> y<-x+rpois(100,2)
>     Dan> bubble.plot(x,y)
> 
> 
> 		
>     Dan>
>
___________________________________________________________
> 

> Store your holiday
> 
>     Dan>
> ______________________________________________
>     Dan> R-help at stat.math.ethz.ch mailing list
>     Dan>
> https://stat.ethz.ch/mailman/listinfo/r-help
>     Dan> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
>     Dan> !DSPAM:42df82ae77251002021314!
>



From christian.bieli at unibas.ch  Fri Jul 22 16:26:10 2005
From: christian.bieli at unibas.ch (Christian Bieli)
Date: Fri, 22 Jul 2005 16:26:10 +0200
Subject: [R] problems with submitting an eps-file created in R
Message-ID: <42E10202.1090809@unibas.ch>

Dear all

I've got some problems submitting a manuscript, because I can't manage 
creating the favourable eps-file of a graph created in R. The journal's 
graphic requirements are as followed:
format: eps
width: max. 6 inches
resolution: min. 1000 dpi
supported fonts: Arial, Courier, Helvetica, Symbol, Times, Charcoal, 
Chicago, Geneva, Georgia, Monaco, Zapf, New York

Itried to ways of getting appropriate file:

1.Creating eps-file in R by drawing into a x11-device and then:
/dev.copy2eps(file = "file.eps", onefile = TRUE, paper = "a4", family = 
"Helvetica",  pointsize=1, print.it = FALSE,  fonts = "Helvetica")

/2. Generating a postscript-file in R with /
//postscript(file = "file.ps", onefile = TRUE, paper = "a4", family = 
"Helvetica", width = 6, height = 2.2, pointsize=1, print.it = FALSE, 
fonts = "Helvetica")/
an trying to convert it in ghostview.

Neither approach brought the favoured result. The error message I got 
from the quality checking program was :

Warning: Document is Missing Non-Standard Font 

One or more non-standard fonts used in this image is not embedded. Standard fonts are: Arial, Courier, Helvetica, Symbol, Times, Charcoal, Chicago, Geneva, Georgia, Monaco, Zapf, New York.

In order to repair this problem, save your document with fonts embedded.

Error: Missing Fonts 

Challenge 
One or more linked or used fonts cannot be found. This is caused by DigitalExpert not being able to locate fonts on your system. All fonts used in the document must be active or have a defined and valid path so that DigitalExpert can find them.
 
Solution 
 The only way to repair this problem is to make the fonts available to DigitalExpert, and then reprocess the files. In order to make fonts active, either activate them using a font management program, or move them into the system:fonts folder.



1. Obviously there's a font problem. Helvetica IS installed in my OS 
(windows 2000). Why "DigitalExpert" does not recognize the "Helvetica" 
font as a standard font? I thought the fonts option would embed the 
specified font in the file. Am I right or is there another way to embed 
fonts?
2. Is there a way to set up the resolution of the created ps/eps-file in 
R (until now I did the settings in ghostview)?
3. I did not find particulars about the pointsize option. What is the 
effect of changing the pointsize value?

With best regards
Christian

-- 
Christian Bieli, project assistant
Institute of Social and Preventive Medicine
University of Basel, Switzerland
Steinengraben 49
CH-4051 Basel
Tel.: +41 61 270 22 12
Fax:  +41 61 270 22 25
christian.bieli at unibas.ch
www.unibas.ch/ispmbs



From br44114 at gmail.com  Fri Jul 22 14:45:48 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 22 Jul 2005 08:45:48 -0400
Subject: [R] Rprof fails in combination with RMySQL
Message-ID: <8d5a3635050722054535ffffa8@mail.gmail.com>

I think the opposite is applicable too - optimize R outside of MySQL.
Exclude the MySQL queries completely and use instead the same data
frames (prepared beforehand) with Rprof. Then, if you really want to
run the full code with Rprof, wrap the queries in try():
data <- try(fetch(dbSendQuery(connection,query),n=-1))
if (class(data) == "try-error") for (i in 1:100) {
	data <- try(fetch(dbSendQuery(connection,query),n=-1))	
	if (class(data) != "try-error") break
	}
Also, why do you close the connection after each query? Open one
connection and use it for the whole R session. (I never close the
connection after a query.)
hth,
b.


> -----Original Message-----
> From: Thieme, Lutz [mailto:lutz.thieme at amd.com] 
> Sent: Friday, July 22, 2005 2:04 AM
> To: bogdan romocea
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Rprof fails in combination with RMySQL
> 
> 
> Hello Bogdan, 
> 
> thanks for you reply. My MySQL is always optimized oustide 
> from R (but many thanks for the interesting link!). 
> I'm very sure that I have to optimize the R code which uses 
> the data from my queries for calculations. To get more in-
> formation which R function is the main speed limiter I tried 
> Rprof. 
> Because I'm always opening and closing the connection for every 
> query I have never opened more than one connection. 
> And again: The same R code runs without Rprof stable since weeks
> multiple times a day. I can exclude by 99% that the error comes 
> from the database. Maybe it comes from large number of opening
> closing cycles?...
> 
> Regards,
> 
> Lutz
> 
> 
> 
> > -----Original Message-----
> > From: bogdan romocea [mailto:br44114 at gmail.com]
> > Sent: Thursday, July 21, 2005 5:05 PM
> > To: Thieme, Lutz
> > Cc: R-help at stat.math.ethz.ch
> > Subject: RE: [R] Rprof fails in combination with RMySQL
> > 
> > 
> > I think you're barking up the wrong tree. Optimize the MySQL code
> > separately from optimizing the R code. A very nice 
> reference about the
> > former is http://highperformancemysql.com/. Also, if possible, do
> > everything in MySQL.
> > hth,
> > b.
> > 
> > 
> > > -----Original Message-----
> > > From: Thieme, Lutz [mailto:lutz.thieme at amd.com] 
> > > Sent: Thursday, July 21, 2005 10:11 AM
> > > To: Rhelp (E-mail)
> > > Subject: [R] Rprof fails in combination with RMySQL
> > > 
> > > 
> > > Dear R community,
> > > 
> > > I tried to optimized my R code by using Rprof. In my R code 
> > > I'm using MySQL
> > > database connections intensively. After a bunch of queries R 
> > > fails with the 
> > > following error message:
> > > Error in .Call("RS_MySQL_newConnection", drvId, con.params, 
> > > groups, PACKAGE = .MySQLPkgName) : 
> > >         RS-DBI driver: (could not connect mylogin at mydatabase 
> > > on dbname "myDB"
> > > 
> > > Without the R profiler this code runs very stable since weeks.
> > > 
> > > Do you have any ideas or suggestions?
> > > 
> > > I tried the following R versions:
> > > ___________________________
> > > platform i386-pc-solaris2.8
> > > arch     i386              
> > > os       solaris2.8        
> > > system   i386, solaris2.8  
> > > status                     
> > > major    1                 
> > > minor    9.1               
> > > year     2004              
> > > month    06                
> > > day      21                
> > > language R   
> > > ___________________________
> > > platform sparc-sun-solaris2.8
> > > arch     sparc               
> > > os       solaris2.8          
> > > system   sparc, solaris2.8   
> > > status                       
> > > major    2                   
> > > minor    1.1                 
> > > year     2005                
> > > month    06                  
> > > day      20                  
> > > language R   
> > > ___________________________
> > > platform sparc-sun-solaris2.8
> > > arch     sparc               
> > > os       solaris2.8          
> > > system   sparc, solaris2.8   
> > > status                       
> > > major    1                   
> > > minor    9.1                 
> > > year     2004                
> > > month    06                  
> > > day      21                  
> > > language R   
> > > 
> > > 
> > > Thank you in advance and kind regards,
> > > 
> > > Lutz Thieme
> > > AMD Saxony/ Product Engineering AMD Saxony Limited 
> > > Liability Company & Co. KG
> > > phone: + 49-351-277-4269 M/S E22-PE, 
> > > Wilschdorfer Landstr. 101
> > > fax: + 49-351-277-9-4269 D-01109 Dresden, Germany
> > > 
> > > 
> > > [[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Fri Jul 22 16:34:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Jul 2005 07:34:32 -0700 (PDT)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>

On Fri, 22 Jul 2005, Dan Bolser wrote:

> On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
>
>> Try as.expression(bquote(...whatever...))
>
> Sob, wimper, etc.

   a<-7
   plot(1)
   legend("topleft",legend=do.call("expression",
                      list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))))

works for me.  The trick is getting the inner calls to bquote 
evaluated, since expression doesn't evaluate its argument.


 	-thomas



From murdoch at stats.uwo.ca  Fri Jul 22 16:37:30 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 22 Jul 2005 10:37:30 -0400
Subject: [R] problems with submitting an eps-file created in R
In-Reply-To: <42E10202.1090809@unibas.ch>
References: <42E10202.1090809@unibas.ch>
Message-ID: <42E104AA.6060701@stats.uwo.ca>

On 7/22/2005 10:26 AM, Christian Bieli wrote:
> Dear all
> 
> I've got some problems submitting a manuscript, because I can't manage 
> creating the favourable eps-file of a graph created in R. The journal's 
> graphic requirements are as followed:
> format: eps
> width: max. 6 inches
> resolution: min. 1000 dpi
> supported fonts: Arial, Courier, Helvetica, Symbol, Times, Charcoal, 
> Chicago, Geneva, Georgia, Monaco, Zapf, New York
> 
> Itried to ways of getting appropriate file:
> 
> 1.Creating eps-file in R by drawing into a x11-device and then:
> /dev.copy2eps(file = "file.eps", onefile = TRUE, paper = "a4", family = 
> "Helvetica",  pointsize=1, print.it = FALSE,  fonts = "Helvetica")
> 
> /2. Generating a postscript-file in R with /
> //postscript(file = "file.ps", onefile = TRUE, paper = "a4", family = 
> "Helvetica", width = 6, height = 2.2, pointsize=1, print.it = FALSE, 
> fonts = "Helvetica")/
> an trying to convert it in ghostview.

The ?postscript man page suggests using onefile = FALSE to get an EPSF 
header directly.  It also suggests paper="special", and horizontal=FALSE.

If that doesn't work, you'll have to ask whoever wrote the quality 
checking program what they're looking for.

Duncan Murdoch

> 
> Neither approach brought the favoured result. The error message I got 
> from the quality checking program was :
> 
> Warning: Document is Missing Non-Standard Font 
> 
> One or more non-standard fonts used in this image is not embedded. Standard fonts are: Arial, Courier, Helvetica, Symbol, Times, Charcoal, Chicago, Geneva, Georgia, Monaco, Zapf, New York.
> 
> In order to repair this problem, save your document with fonts embedded.
> 
> Error: Missing Fonts 
> 
> Challenge 
> One or more linked or used fonts cannot be found. This is caused by DigitalExpert not being able to locate fonts on your system. All fonts used in the document must be active or have a defined and valid path so that DigitalExpert can find them.
>  
> Solution 
>  The only way to repair this problem is to make the fonts available to DigitalExpert, and then reprocess the files. In order to make fonts active, either activate them using a font management program, or move them into the system:fonts folder.
> 
> 
> 
> 1. Obviously there's a font problem. Helvetica IS installed in my OS 
> (windows 2000). Why "DigitalExpert" does not recognize the "Helvetica" 
> font as a standard font? I thought the fonts option would embed the 
> specified font in the file. Am I right or is there another way to embed 
> fonts?
> 2. Is there a way to set up the resolution of the created ps/eps-file in 
> R (until now I did the settings in ghostview)?
> 3. I did not find particulars about the pointsize option. What is the 
> effect of changing the pointsize value?
> 
> With best regards
> Christian
>



From mschwartz at mn.rr.com  Fri Jul 22 16:50:28 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 22 Jul 2005 09:50:28 -0500
Subject: [R] problems with submitting an eps-file created in R
In-Reply-To: <42E10202.1090809@unibas.ch>
References: <42E10202.1090809@unibas.ch>
Message-ID: <1122043828.3330.11.camel@localhost.localdomain>

On Fri, 2005-07-22 at 16:26 +0200, Christian Bieli wrote:
> Dear all
> 
> I've got some problems submitting a manuscript, because I can't
> manage 
> creating the favourable eps-file of a graph created in R. The
> journal's 
> graphic requirements are as followed:
> format: eps
> width: max. 6 inches
> resolution: min. 1000 dpi
> supported fonts: Arial, Courier, Helvetica, Symbol, Times, Charcoal, 
> Chicago, Geneva, Georgia, Monaco, Zapf, New York
> 
> Itried to ways of getting appropriate file:
> 
> 1.Creating eps-file in R by drawing into a x11-device and then:
> /dev.copy2eps(file = "file.eps", onefile = TRUE, paper = "a4", family
> = 
> "Helvetica",  pointsize=1, print.it = FALSE,  fonts = "Helvetica")
> 
> /2. Generating a postscript-file in R with /
> //postscript(file = "file.ps", onefile = TRUE, paper = "a4", family = 
> "Helvetica", width = 6, height = 2.2, pointsize=1, print.it = FALSE, 
> fonts = "Helvetica")/
> an trying to convert it in ghostview.
> 
> Neither approach brought the favoured result. The error message I got 
> from the quality checking program was :
> 
> Warning: Document is Missing Non-Standard Font 
> 
> One or more non-standard fonts used in this image is not embedded.
> Standard fonts are: Arial, Courier, Helvetica, Symbol, Times,
> Charcoal, Chicago, Geneva, Georgia, Monaco, Zapf, New York.
> 
> In order to repair this problem, save your document with fonts
> embedded.
> 
> Error: Missing Fonts 
> 
> Challenge 
> One or more linked or used fonts cannot be found. This is caused by
> DigitalExpert not being able to locate fonts on your system. All fonts
> used in the document must be active or have a defined and valid path
> so that DigitalExpert can find them.
>  
> Solution 
>  The only way to repair this problem is to make the fonts available to
> DigitalExpert, and then reprocess the files. In order to make fonts
> active, either activate them using a font management program, or move
> them into the system:fonts folder.
> 
> 
> 
> 1. Obviously there's a font problem. Helvetica IS installed in my OS 
> (windows 2000). Why "DigitalExpert" does not recognize the
> "Helvetica" 
> font as a standard font? I thought the fonts option would embed the 
> specified font in the file. Am I right or is there another way to
> embed 
> fonts?
> 2. Is there a way to set up the resolution of the created ps/eps-file
> in 
> R (until now I did the settings in ghostview)?
> 3. I did not find particulars about the pointsize option. What is the 
> effect of changing the pointsize value?
> 
> With best regards
> Christian


You need to read the help for postscript(), which specifically tells you
in the Details section to use:

postscript(..., onefile = FALSE, horizontal = FALSE, paper = "special")

to generate EPS files.

There is no resolution setting for a postscript file per se, since PS is
a device independent vector based format and the resolution of the
output is dependent upon the target device/viewer. One exception to this
would be the embedding of a bitmapped object in a PS file, but that is
not applicable here.

HTH,

Marc Schwartz



From christophe.pouzat at univ-paris5.fr  Fri Jul 22 16:51:12 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Fri, 22 Jul 2005 16:51:12 +0200
Subject: [R] About object of class mle returned by user defined functions
In-Reply-To: <Pine.LNX.4.61.0507221436300.31141@gannet.stats>
References: <42DFC6E0.8050901@univ-paris5.fr> <42E0F00A.7020404@univ-paris5.fr>
	<Pine.LNX.4.61.0507221436300.31141@gannet.stats>
Message-ID: <42E107E0.70507@univ-paris5.fr>

Dear Prof Ripley,

Your solution:

Call <- quote(mle())
Call$minuslogl <- minusLogLikelihood
...

Works beautifully (and simply)!

Thanks a lot,

Christophe.

Prof Brian Ripley wrote:

> confint has to be able to re-fit the function to form the profile 
> likelihood.  The fit you return refers to values inside the function 
> you used, and those are not available in the environment you call 
> confint from.  You need to ensure that those values are substituted 
> and not referred to.
>
> Compare
>
>> fitA at call
>
> mle(minuslogl = llgamma, start = list(shape = (mean(isi1)/sd(isi1))^2,
>     scale = sd(isi1)^2/mean(isi1)), method = "L-BFGS-B", lower = c(1e-05,
>     1e-05))
>
>> fit1 at call
>
> mle(minuslogl = minusLogLikelihood, start = initial.para, method = 
> "L-BFGS-B",
>     lower = optim.lower, upper = optim.upper)
>
> and the difference should be clear.
>
> You can fix this up by constructing a call containing the values and 
> not the names (one way is to use substitute()) and then eval() it.  
> Another way is something like
>
> Call <- quote(mle())
> Call$minuslogl <- minusLogLikelihood
> Call$start <- initial.para
> ...



-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From ripley at stats.ox.ac.uk  Fri Jul 22 16:56:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2005 15:56:57 +0100 (BST)
Subject: [R] problems with submitting an eps-file created in R
In-Reply-To: <42E10202.1090809@unibas.ch>
References: <42E10202.1090809@unibas.ch>
Message-ID: <Pine.LNX.4.61.0507221550540.31861@gannet.stats>

On Fri, 22 Jul 2005, Christian Bieli wrote:

> Dear all
>
> I've got some problems submitting a manuscript, because I can't manage
> creating the favourable eps-file of a graph created in R. The journal's
> graphic requirements are as followed:
> format: eps
> width: max. 6 inches
> resolution: min. 1000 dpi

EPS files do not normally have a resolution.

> supported fonts: Arial, Courier, Helvetica, Symbol, Times, Charcoal,
> Chicago, Geneva, Georgia, Monaco, Zapf, New York

Most of those are Windows fonts, and not accurately defined.

My guess is that the checkers are not properly aware of Adobe's standards 
and want something other than an EPS file.

> Itried to ways of getting appropriate file:
>
> 1.Creating eps-file in R by drawing into a x11-device and then:
> /dev.copy2eps(file = "file.eps", onefile = TRUE, paper = "a4", family =
> "Helvetica",  pointsize=1, print.it = FALSE,  fonts = "Helvetica")
>
> /2. Generating a postscript-file in R with /

Should be onefile=FALSE.

> //postscript(file = "file.ps", onefile = TRUE, paper = "a4", family =
> "Helvetica", width = 6, height = 2.2, pointsize=1, print.it = FALSE,
> fonts = "Helvetica")/
> an trying to convert it in ghostview.
>
> Neither approach brought the favoured result. The error message I got
> from the quality checking program was :
>
> Warning: Document is Missing Non-Standard Font
>
> One or more non-standard fonts used in this image is not embedded. 
> Standard fonts are: Arial, Courier, Helvetica, Symbol, Times, Charcoal, 
> Chicago, Geneva, Georgia, Monaco, Zapf, New York.
>
> In order to repair this problem, save your document with fonts embedded.
>
> Error: Missing Fonts
>
> Challenge
> One or more linked or used fonts cannot be found. This is caused by DigitalExpert not being able to locate fonts on your system. All fonts used in the document must be active or have a defined and valid path so that DigitalExpert can find them.
>
> Solution
> The only way to repair this problem is to make the fonts available to DigitalExpert, and then reprocess the files. In order to make fonts active, either activate them using a font management program, or move them into the system:fonts folder.
>
>

> 1. Obviously there's a font problem. Helvetica IS installed in my OS
> (windows 2000). Why "DigitalExpert" does not recognize the "Helvetica"
> font as a standard font? I thought the fonts option would embed the
> specified font in the file. Am I right or is there another way to embed
> fonts?

No, the help file explicitly says that it is your responsibility.
EPS files do not normally have fonts embedded, but they do have DSC marks 
telling the enclosing application to do so.

> 2. Is there a way to set up the resolution of the created ps/eps-file in
> R (until now I did the settings in ghostview)?

EPS files do not have a resolution.

> 3. I did not find particulars about the pointsize option. What is the
> effect of changing the pointsize value?

It's on the help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From caobg at email.uc.edu  Fri Jul 22 17:09:29 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 22 Jul 2005 11:09:29 -0400
Subject: [R] about nnet package
Message-ID: <200507221509.CPA75631@mirapoint.uc.edu>

Dear All,

I'm learning to train a neural network with my training data by using nnet package, then evaluate it with a evaluation set. My problem here is that, I need the trained network to be used in future, so, what should I store? and How? Any other options other than nnet package? Any example will be highly appreciated!

Best, 
 Baoqiang Cao



From ligges at statistik.uni-dortmund.de  Fri Jul 22 17:17:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Jul 2005 17:17:37 +0200
Subject: [R] about nnet package
In-Reply-To: <200507221509.CPA75631@mirapoint.uc.edu>
References: <200507221509.CPA75631@mirapoint.uc.edu>
Message-ID: <42E10E11.5040804@statistik.uni-dortmund.de>

Baoqiang Cao wrote:

> Dear All,
> 
> I'm learning to train a neural network with my training data by using nnet package, then evaluate it with a evaluation set. My problem here is that, I need the trained network to be used in future, so, what should I store? and How? Any other options other than nnet package? Any example will be highly appreciated!
> 
> Best, 
>  Baoqiang Cao


See ?nnet which point you to its predict method.

You say

nnetObject <- nnet(.....)

Now store the nnetObject and use it later as in:

predict(nnetObject, newdata)



Uwe Ligges






> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jul 22 17:24:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Jul 2005 17:24:11 +0200
Subject: [R] multiplicate 2 functions
In-Reply-To: <IK18PY$BAA38222266B9A06A451C1056F8B299A@oreka.com>
References: <IK18PY$BAA38222266B9A06A451C1056F8B299A@oreka.com>
Message-ID: <42E10F9B.8050306@statistik.uni-dortmund.de>

No, does not work that way. And I still do not know what you are finally 
going to achieve. You want to integrate over beta18, but use a set of 10 
density values at the same time? I am really confused.

Anyway, if you do not know beta18, you can onl write a function that 
depends on such an argument, but you cannot call that function without 
specifying that argument - it simply cannot be evaluated.

I guess you are looking for software that does symbolic algebra. R 
cannot (well, almost), R is mainly intended for numerical computations.

Uwe Ligges




herodote at oreka.com wrote:

> Thks for your answer,
> here is an exemple of what i do with the errors in french...
> 
>>tmp
> 
> [1] 200 150 245 125 134 345 320 450 678
> 
>>beta18
> 
> Erreur : Objet "beta18" not found //NORMAL just to show it
> 
>>eta
> 
> [1] 500
> 
>>func1<-function(beta18) dweibull(tmp[1],beta18,eta)
> 
> 
>>func1<-func1(beta18) * function(beta18)
> 
> dweibull(tmp[2],beta18,eta)
> 
> Erreur dans dweibull(tmp[1], beta18, eta) : Objet "beta18"
> not found 
> //I don't want to evaluate weibull immediatly just want to
> initialize the function then integrate on beta18,i have to
> make the multiplication of weibull at each time pointed by
> tmp then integrate it to obtain a norm.
> 
> 
>>func1<-func1() * function(beta18) dweibull(tmp[2],beta18,eta)
> 
> 
> Erreur dans dweibull(tmp[1], beta18, eta) : l'argument
> "beta18" est manquant, avec aucune valeur par d??faut
> 
> 
>>func1<-func1 * function(beta18) dweibull(tmp[2],beta18,eta)
> 
> 
> Erreur dans func1 * function(beta18) dweibull(tmp[2],
> beta18, eta) :
> argument non num??rique pour un op??rateur binaire
> 
> 
>>func1<-func1 & function(beta18) dweibull(tmp[2],beta18,eta)
> 
> 
> Erreur dans func1 & function(beta18) dweibull(tmp[2],
> beta18, eta) :
> ces op??rations ne sont possibles que pour des types
> num??riques ou logiques
> 
> 
>>func1<-func1(beta18) & function(beta18)
> 
> dweibull(tmp[2],beta18,eta)
> 
> Erreur dans dweibull(tmp[1], beta18, eta) : Objet "beta18"
> non trouv??
> 
> I hope you understand what i want to do.
> 
> thks
> guillaume
> ---------- Initial Header -----------
> 
>>From : Uwe Ligges <ligges at statistik.uni-dortmund.de>
> To : "herodote at oreka.com" <herodote at oreka.com>
> Cc : r-help <r-help at stat.math.ethz.ch>
> Date : Fri, 22 Jul 2005 12:51:47 +0200
> Subject : Re: [R] Generate a function
> 
> herodote at oreka.com wrote:
> 
> 
>>hi all,
>>
>>I need to generate a function inside a loop:
>>
>>tmp is an array
> 
> 
> Well, a 1-d array, or better say a vector of length 10,
> given the code 
> below is correct.
> 
> 
> 
>>for (i in 1:10)
>>{
>>func<- func * function(beta1) dweibull(tmp[i],beta1,eta)
>>}
>>
>>because then i need to integrate this function on beta.
>>
>>I could have written this :
>>
>>func<-function(beta1) prod(dweibull(tmp,beta1,eta)) (with
> 
> eta and beta1 set)
> 
>>but it is unplottable and no integrable... i could make it
> 
> a bit different but if i do that ( prod(tmp)=~Inf ) i'm stuck.
> 
>>
>>I've looked in R-poetry and i didn't find anything usefull.
>>
>>I think i have a problem with how i tell R my function is,
> 
> R seems to think it is a function like programmers do but
> not a f(x) function.
> 
> Please specify a reproducible example (as the posting guide
> asks to do), 
> that means including the values for tmp, beta1 and eta.
> Then we might be able to explain where you have your problems.
> 
> Uwe Ligges
> 
> 
>>Thks 
>>guillaume
>>
>>////////////////////////////////////////////////////////////
>>// Webmail Oreka : http://www.oreka.com
>>////////////////////////////////////////////////////////////
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> ////////////////////////////////////////////////////////////
> // Webmail Oreka : http://www.oreka.com
> ////////////////////////////////////////////////////////////
> 
> 
> ////////////////////////////////////////////////////////////
> // Webmail Oreka : http://www.oreka.com
> ////////////////////////////////////////////////////////////
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From caobg at email.uc.edu  Fri Jul 22 17:34:32 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 22 Jul 2005 11:34:32 -0400
Subject: [R] about nnet package
Message-ID: <200507221534.CPA81997@mirapoint.uc.edu>

Hello Uwe Ligges,
    I might misdiscribe my question. What I thought is that, after nnetObject<-nnet(...), this nnetObject will be gone after I exit R. Since I need the trained nnetObject for next time without training it again, how can I save this nnetObject (to some files I guess)? Thanks.

Best regards, 
  Baoqiang Cao

======= At 2005-07-22, 11:17:37 you wrote: =======

>Baoqiang Cao wrote:
>
>> Dear All,
>> 
>> I'm learning to train a neural network with my training data by using nnet package, then evaluate it with a evaluation set. My problem here is that, I need the trained network to be used in future, so, what should I store? and How? Any other options other than nnet package? Any example will be highly appreciated!
>> 
>> Best, 
>>  Baoqiang Cao
>
>
>See ?nnet which point you to its predict method.
>
>You say
>
>nnetObject <- nnet(.....)
>
>Now store the nnetObject and use it later as in:
>
>predict(nnetObject, newdata)
>
>
>
>Uwe Ligges
>
>
>
>
>
>
>> 
>> 
>> ------------------------------------------------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>.

= = = = = = = = = = = = = = = = = = = =



From robut at iinet.net.au  Fri Jul 22 17:35:16 2005
From: robut at iinet.net.au (Robert Cunningham)
Date: Fri, 22 Jul 2005 23:35:16 +0800
Subject: [R] Windows metafiles (emf) under Linux
Message-ID: <200507222335.17249.robut@iinet.net.au>

G'day Folks,

The R-help list has had assorted threads over the years on the best way for 
Linux users to produce EMF files, mainly for exchange with Windows users. 
There have been various suggestions for how this may be possible or ways 
around the problem, via other formats, but none have seemed very 
satisfactory.

I've found what seems to be a quite simple way to produce EMFs from Linux that 
may interest some. I simply downloaded R (2.1.1 for Windows) then installed 
and ran R (Rgui.exe - Rterm.exe did not work) under Wine (www.winehq.com, 
version 20050211). R ran fine and I was able to produce the figures I 
required with the right-click saving of  EMFs working fine. A dummy 
screenshot is attached. I also tried to install a package (vegan) and all 
went well. Indeed everything I did seemed to work well. The only issue is 
that all the text in the in the R console and figures appeared to in 
uppercase (perhaps a Wine setting?) and there were a few other character 
issues. However, despite appearances the characters in the EMF were fine. 
Indeed my Windows colleague told me all the figures were fine.

A little more about my system:
I am using Mandriva LE 2005 with KDE
Windows 98 does exist on the HD but I understand from the Wine site that 
Windows is not required at all.
R 2.1.0 for Linux is installed


Well I hope this information proves useful and others can make use of the 
wineR approach.



-- 
Cheers,

RJ Cunningham
-------------- next part --------------
A non-text attachment was scrubbed...
Name: wineR.png
Type: image/png
Size: 67370 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050722/01cfc2fd/wineR.png

From ligges at statistik.uni-dortmund.de  Fri Jul 22 17:45:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Jul 2005 17:45:36 +0200
Subject: [R] about nnet package
In-Reply-To: <200507221534.CPA81997@mirapoint.uc.edu>
References: <200507221534.CPA81997@mirapoint.uc.edu>
Message-ID: <42E114A0.2010107@statistik.uni-dortmund.de>

Baoqiang Cao wrote:

> Hello Uwe Ligges,
>     I might misdiscribe my question. What I thought is that, after nnetObject<-nnet(...), this nnetObject will be gone after I exit R. Since I need the trained nnetObject for next time without training it again, how can I save this nnetObject (to some files I guess)? Thanks.


See ?save

Uwe Ligges

> Best regards, 
>   Baoqiang Cao
> 
> ======= At 2005-07-22, 11:17:37 you wrote: =======
> 
> 
>>Baoqiang Cao wrote:
>>
>>
>>>Dear All,
>>>
>>>I'm learning to train a neural network with my training data by using nnet package, then evaluate it with a evaluation set. My problem here is that, I need the trained network to be used in future, so, what should I store? and How? Any other options other than nnet package? Any example will be highly appreciated!
>>>
>>>Best, 
>>> Baoqiang Cao
>>
>>
>>See ?nnet which point you to its predict method.
>>
>>You say
>>
>>nnetObject <- nnet(.....)
>>
>>Now store the nnetObject and use it later as in:
>>
>>predict(nnetObject, newdata)
>>
>>
>>
>>Uwe Ligges
>>
>>
>>
>>
>>
>>
>>
>>>
>>>------------------------------------------------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>.
> 
> 
> = = = = = = = = = = = = = = = = = = = =
>



From guerinche at gmail.com  Fri Jul 22 18:14:56 2005
From: guerinche at gmail.com (alejandro munoz)
Date: Fri, 22 Jul 2005 11:14:56 -0500
Subject: [R] normal reference intervals
In-Reply-To: <958023F70A782142AFA14D3E82F794183CDCF7@cherry.ads.ntu.ac.uk>
References: <958023F70A782142AFA14D3E82F794183CDCF7@cherry.ads.ntu.ac.uk>
Message-ID: <98c62e110507220914382586e0@mail.gmail.com>

David,

At the following URI you will find an R package called lmsqreg that
implements LMS quantile regression:

http://www.biostat.harvard.edu/~carey/vcwww_4.html

HTH,

alejandro

On 7/21/05, Crabb, David <david.crabb at ntu.ac.uk> wrote:
> I am interested in calculating Age-Specific normal reverence intervals,
> using non-parametric methods - or ideally something called the LMS
> method (which as I understand it uses cubic splines fitted to the data).
> Any packages in R that you think might help me? Any other advice
> gratefully received.
> 
> Many thanks. Best wishes, David.
> 
> ------------------------------------------------------------------------
> -----
> Dr. David Crabb
> School of Biomedical and Natural Sciences,
> Nottingham Trent University, Clifton Campus, Nottingham. NG11 8NS
> Tel: 0115 848 3275   Fax: 0115 848 6690
> 
> 
> 
> 
> This email is intended solely for the addressee.  It may contain private and confidential information.  If you are not the intended addressee, please take no action based on it nor show a copy to anyone.  In this case, please reply to this email to highlight the error.  Opinions and information in this email that do not relate to the official business of Nottingham Trent University shall be understood as neither given nor endorsed by the University.
> Nottingham Trent University has taken steps to ensure that this email and any attachments are virus-free, but we do advise that the recipient should check that the email and its attachments are actually virus free.  This is in keeping with good computing practice.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From caobg at email.uc.edu  Fri Jul 22 18:18:02 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 22 Jul 2005 12:18:02 -0400
Subject: [R] setting weights for such a two-class problem in nnet and svm
Message-ID: <200507221618.CPA93948@mirapoint.uc.edu>

Dear All,

I have such a two-class problem, one class is very large(~98% of total), and the other is just 2%. According to manual of nnet, I need setup "weights", so I intend to set 1 for class one, 49 for class 2. How do I do that? Just weights=49? 
Meanwhile I'd like to try svm(e1071), again, how do I setup "class.weights"? Thanks.

BTW: Many thanks to Jake and Uwe for their answers about using "save" to store trained networks!


Best regards,     
 Baoqiang Cao



From ggrothendieck at gmail.com  Fri Jul 22 18:49:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 12:49:42 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
References: <971536df0507220615342c0541@mail.gmail.com>
	<Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <971536df05072209495570dff9@mail.gmail.com>

On 7/22/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
> 
> >Try as.expression(bquote(...whatever...))
> 
> Sob, wimper, etc.
> 
> 
> my.slope.1 <-   3.22
> my.slope.2 <-   0.13
> my.inter.1 <- -10.66
> my.inter.2 <-   1.96
> my.Rsqua.1 <-   0.97
> 
> 
> 
> text(2,5,
>  paste("Slope:  ",
>        as.expression(bquote(.(my.slope.1)%+-%.(my.slope.2)))))

Thomas Lumley has already answered with a solution but I 
thought I would address this specific construct.  The problem is
that the above converts it back to character.  You want to put
everything in the bquote so it stays an expression:

my.slope.1 <-   3.22
my.slope.2 <-   0.13

plot(1:5)
text(2,5, as.expression(bquote(Slope: .(my.slope.1)%+-%.(my.slope.2))))






> 
> 
> That puts the text and variable together, but not the symbol, which is
> printed as '%+-%'
> 
> The following is nearly right, but wrong enough it makes my first attempt
> look reasonable...
> 
> 
> # Create the initial plot as per Christoph's post
> plot(1:5, 1:5, type = "n")
> 
> legend(x=1,
>       y=4.5,
>       bg='white',
>       ncol=3,
>       c("Slope","Intercept",expression(R^2),
>         ":",":",":",
>         bquote( .(my.slope.1)%+-%.(my.slope.2)),
>         bquote( .(my.inter.1)%+-%.(my.inter.2)),
>         my.Rsqua.1
>         )
>       )
> 
> Anyone got any ideas on how to fix the above?
> 
> Namely 1) right align numbers, 2) remove excessive white space.
> 
> I like the above because it dosn't require me to calculate exactly where
> to put each piece of text.
> 
> 
> I just want to annotate a plot :(
> 
> 
> 
> >
> >
> >On 7/22/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> >> On Thu, 21 Jul 2005, Marc Schwartz (via MN) wrote:
> >>
> >> >[Note: the initial posts have been re-arranged to attempt to maintain
> >> >the flow from top to bottom]
> >> >
> >> >> >Dan Bolser writes:
> >> >> > >
> >> >> > > I would like to annotate my plot with a little box containing the slope,
> >> >> > > intercept and R^2 of a lm on the data.
> >> >> > >
> >> >> > > I would like it to look like...
> >> >> > >
> >> >> > >  +----------------------------+
> >> >> > >  | Slope     :   3.45 +- 0.34 |
> >> >> > >  | Intercept : -10.43 +- 1.42 |
> >> >> > >  | R^2       :   0.78         |
> >> >> > >  +----------------------------+
> >> >> > >
> >> >> > > However I can't make anything this neat, and I can't find out how to
> >> >> > > combine this with symbols for R^2 / +- (plus minus).
> >> >> > >
> >> >> > > Below is my best attempt (which is franky quite pour). Can anyone
> >> >> > > improve on the below?
> >> >> > >
> >> >> > > Specifically,
> >> >> > >
> >> >> > > aligned text and numbers,
> >> >> > > aligned decimal places,
> >> >> > > symbol for R^2 in the text (expression(R^2) seems to fail with
> >> >> > > 'paste') and +-
> >> >> > >
> >> >> > >
> >> >> > >
> >> >> > > Cheers,
> >> >> > > Dan.
> >> >> > >
> >> >> > >
> >> >> > > dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> >> >> > >
> >> >> > > abline(coef(dat.lm),lty=2,lwd=1.5)
> >> >> > >
> >> >> > >
> >> >> > > dat.lm.sum <- summary(dat.lm)
> >> >> > > dat.lm.sum
> >> >> > >
> >> >> > > attributes(dat.lm.sum)
> >> >> > >
> >> >> > > my.text.1 <-
> >> >> > >   paste("Slope : ",     round(dat.lm.sum$coefficients[2],2),
> >> >> > >         "+/-",          round(dat.lm.sum$coefficients[4],2))
> >> >> > >
> >> >> > > my.text.2 <-
> >> >> > >   paste("Intercept : ", round(dat.lm.sum$coefficients[1],2),
> >> >> > >         "+/-",          round(dat.lm.sum$coefficients[3],2))
> >> >> > >
> >> >> > > my.text.3 <-
> >> >> > >   paste("R^2 : ",       round(dat.lm.sum$r.squared,2))
> >> >> > >
> >> >> > > my.text.1
> >> >> > > my.text.2
> >> >> > > my.text.3
> >> >> > >
> >> >> > >
> >> >> > > ## Add legend
> >> >> > > text(x=3,
> >> >> > >      y=300,
> >> >> > >      paste(my.text.1,
> >> >> > >            my.text.2,
> >> >> > >            my.text.3,
> >> >> > >            sep="\n"),
> >> >> > >      adj=c(0,0),
> >> >> > >      cex=1
> >> >
> >> >
> >> >> On Thu, 21 Jul 2005, Christoph Buser wrote:
> >> >>
> >> >> >Dear Dan
> >> >> >
> >> >> >I can only help you with your third problem, expression and
> >> >> >paste. You can use:
> >> >> >
> >> >> >plot(1:5,1:5, type = "n")
> >> >> >text(2,4,expression(paste("Slope : ", 3.45%+-%0.34, sep = "")), pos = 4)
> >> >> >text(2,3.8,expression(paste("Intercept : ", -10.43%+-%1.42)), pos = 4)
> >> >> >text(2,3.6,expression(paste(R^2,": ", "0.78", sep = "")), pos = 4)
> >> >
> >> >> >I do not have an elegant solution for the alignment.
> >> >
> >> >
> >> >On Thu, 2005-07-21 at 19:55 +0100, Dan Bolser wrote:
> >> >> Cheers for this.
> >> >>
> >> >> I was trying to get it to work, but the problem is that I need to replace
> >> >> the values above with variables, from the following code...
> >> >>
> >> >>
> >> >> dat.lm <- lm(dat$AVG_CH_PAIRS ~ dat$CHAINS)
> >> >> dat.lm.sum <- summary(dat.lm)
> >> >>
> >> >> my.slope.1 <- round(dat.lm.sum$coefficients[2],2)
> >> >> my.slope.2 <- round(dat.lm.sum$coefficients[4],2)
> >> >>
> >> >> my.inter.1 <- round(dat.lm.sum$coefficients[1],2)
> >> >> my.inter.2 <- round(dat.lm.sum$coefficients[3],2)
> >> >>
> >> >> my.Rsqua.1 <- round(dat.lm.sum$r.squared,2)
> >> >>
> >> >>
> >> >> Anything I try results in either the words 'paste("Slope:", my.slope.1,
> >> >> %+-%my.slope.2,sep="")' being written to the plot, or just
> >> >> 'my.slope.1+-my.slope2' (where the +- is correctly written).
> >> >>
> >> >> I want to script it up and write all three lines to the plot with
> >> >> 'sep="\n"', rather than deciding three different heights.
> >> >
> >> >
> >> >> Thanks very much for what you gave, its a good start for me to figure out
> >> >> how I am supposed to be telling R what to do!
> >> >>
> >> >> Any way to just get fixed width fonts with text? (for the alignment
> >> >> problem)
> >> >
> >> >
> >> >Dan,
> >> >
> >> >Here is one approach. It may not be the best, but it gets the job done.
> >> >You can certainly take this and encapsulate it in a function to automate
> >> >the text/box placement and to pass values as arguments.
> >> >
> >> >A couple of quick concepts:
> >> >
> >> >1. As far as I know, plotmath cannot do multiple lines, so each line in
> >> >your box needs to be done separately.
> >> >
> >> >2. The horizontal alignment is a bit problematic when using expression()
> >> >or bquote() since I don't believe that multiple spaces are honored as
> >> >such after parsing. Thus I break up each component (label, ":" and
> >> >values) into separate text() calls. The labels are left justified.
> >> >
> >> >3. The alignment for the numeric values are done with right
> >> >justification. So, as long as you use a consistent number of decimals in
> >> >the value outputs (2 here), you should be OK. This means you might need
> >> >to use formatC() or sprintf() to control the numeric output values on
> >> >either side of the +/- sign.
> >> >
> >> >4. In the variable replacement, note the use of substitute() and the
> >> >list of x and y arguments as replacement values in the expressions.
> >> >
> >> >
> >> >
> >> ># Set your values
> >> >my.slope.1 <- 3.45
> >> >my.slope.2 <- 0.34
> >> >
> >> >my.inter.1 <- -10.43
> >> >my.inter.2 <- 1.42
> >> >
> >> >my.Rsqua <- 0.78
> >> >
> >> >
> >> ># Create the initial plot as per Christoph's post
> >> >plot(1:5, 1:5, type = "n")
> >> >
> >> >
> >> >#-------------------------------------
> >> ># Do the Slope
> >> >#-------------------------------------
> >> >
> >> >text(1, 4.5,  "Slope", pos = 4)
> >> >text(2, 4.5, ":")
> >> >text(3, 4.5, substitute(x %+-% y,
> >> >                        list(x = my.slope.1,
> >> >                             y = my.slope.2)),
> >> >     pos = 2)
> >> >
> >> >
> >> >#-------------------------------------
> >> ># Do the Intercept
> >> >#-------------------------------------
> >> >
> >> >text(1, 4.25, "Intercept", pos = 4)
> >> >text(2, 4.25, ":")
> >> >text(3, 4.25, substitute(x %+-% y,
> >> >                         list(x = my.inter.1,
> >> >                              y = my.inter.2)),
> >> >     pos = 2)
> >> >
> >> >
> >> >#-------------------------------------
> >> ># Do R^2
> >> >#-------------------------------------
> >> >
> >> >text(1, 4.0, expression(R^2), pos = 4)
> >> >text(2, 4.0, ":")
> >> >text(3, 4.0,  substitute(x, list(x = my.Rsqua)),
> >> >     pos = 2)
> >> >
> >> >
> >> >#-------------------------------------
> >> ># Do the Box
> >> >#-------------------------------------
> >> >
> >> >rect(1, 3.75, 3, 4.75)
> >> >
> >> >
> >> >You can adjust the x,y coordinates for the various text elements as you
> >> >may require and can also calculate them based upon the xlim, ylim of
> >> >your actual plot. You can also modify the 'cex' argument to text() for
> >> >adjusting the sizes of the fonts in use.
> >>
> >>
> >> I have been trying many different combinations of the suggestions so
> >> far...
> >>
> >> bquote, substitute, expression, paste, etc.
> >>
> >> Also I was trying to use the 'expression' part of the 'legend' function to
> >> make thing easier (make new lines, make columns, make rectangles)...
> >>
> >> So far all in vain.
> >>
> >> I am sure a fairly neat solution is possible using 'legend' and
> >> 'expression', but I cant get a legend which contains a variable
> >> substituted value *and* a special symbol.
> >>
> >> Can anyone show me an example of this simple step, and then I can try to
> >> build on that.
> >>
> >>
> >> Thanks all for suggestions, its one of those 'it seems so easy' problems
> >> (for me).
> >>
> >> P.S. Whats with the documentation for bquote?
> >>
> >>
> >>
> >> >BTW, to use a monospaced font, you can set par(family = "mono").
> >> >See ?par for more information.
> >> >
> >> >HTH,
> >> >
> >> >Marc Schwartz
> >> >
> >> >
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> 
>



From ripley at stats.ox.ac.uk  Fri Jul 22 18:56:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2005 17:56:12 +0100 (BST)
Subject: [R] setting weights for such a two-class problem in nnet and svm
In-Reply-To: <200507221618.CPA93948@mirapoint.uc.edu>
References: <200507221618.CPA93948@mirapoint.uc.edu>
Message-ID: <Pine.LNX.4.61.0507221752000.2055@gannet.stats>

On Fri, 22 Jul 2005, Baoqiang Cao wrote:

> Dear All,
>
> I have such a two-class problem, one class is very large(~98% of total), 
> and the other is just 2%. According to manual of nnet, I need setup 
> "weights", so I intend to set 1 for class one, 49 for class 2. How do I 
> do that? Just weights=49?

I do not know what you mean by `manual of nnet', but no, not that since 
the help page says

   weights: (case) weights for each example - if missing defaults to 1.

More likely you want ifelse(class==1, 1/49, 1).

However, there is more to it than that, and do please read the reference 
books on the help page.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Fri Jul 22 19:16:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 13:16:28 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
Message-ID: <971536df05072210165b5776e7@mail.gmail.com>

On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Fri, 22 Jul 2005, Dan Bolser wrote:
> 
> > On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
> >
> >> Try as.expression(bquote(...whatever...))
> >
> > Sob, wimper, etc.
> 
>   a<-7
>   plot(1)
>   legend("topleft",legend=do.call("expression",
>                      list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))))
> 
> works for me.  The trick is getting the inner calls to bquote
> evaluated, since expression doesn't evaluate its argument.

I think legend accepts a list argument directly so that could be
simplified to just:

  a<-7
  plot(1)
  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
  legend("topleft",legend=L)

The same comment seems to apply to my prior suggestion about
as.expression(bquote(...)), namely that one can just write the
following as text also supports a list argument:


my.slope.1 <-   3.22
my.slope.2 <-   0.13
my.inter.1 <- -10.66
my.inter.2 <-   1.96

plot(1:5)
L <- list(
  "Intercept:", bquote(.(my.inter.1)%+-%.(my.inter.2)),
  "Slope:", bquote(.(my.slope.1)%+-%.(my.slope.2))
)
text(2, c(4,4,4.25,4.25), L, pos = c(2,4,2,4))



From caobg at email.uc.edu  Fri Jul 22 19:18:01 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 22 Jul 2005 13:18:01 -0400
Subject: [R] setting weights for such a two-class problem in nnet andsvm
Message-ID: <200507221718.CPB10440@mirapoint.uc.edu>

======= At 2005-07-22, 12:56:12 you wrote: =======

>On Fri, 22 Jul 2005, Baoqiang Cao wrote:
>
>> Dear All,
>>
>> I have such a two-class problem, one class is very large(~98% of total), 
>> and the other is just 2%. According to manual of nnet, I need setup 
>> "weights", so I intend to set 1 for class one, 49 for class 2. How do I 
>> do that? Just weights=49?
>
>I do not know what you mean by `manual of nnet', but no, not that since 
>the help page says
>
>   weights: (case) weights for each example - if missing defaults to 1.
>
Thanks! I should have said "according to the help page"...
>More likely you want ifelse(class==1, 1/49, 1).
>
>However, there is more to it than that, and do please read the reference 
>books on the help page.
Unfortunately, the two refered books(all by you) were all borrowed by someone else, nothing left in the library. Beside waiting longer or buy one, would you breifly mention some of what you think should be aware of? I'm hoping to find a optimal feature space with better predictions. Thanks.

Best,
 Baoqiang Cao
  
>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>.



From ggrothendieck at gmail.com  Fri Jul 22 19:39:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 13:39:32 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <971536df05072210165b5776e7@mail.gmail.com>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
	<971536df05072210165b5776e7@mail.gmail.com>
Message-ID: <971536df0507221039575f685e@mail.gmail.com>

There was an error in the second example.  It seems one can use
list in the way wanted in legend but not in text so for text one would
have to use the do.call approach of Thomas or sapply as shown here:

my.slope.1 <-   3.22
my.slope.2 <-   0.13
my.inter.1 <- -10.66
my.inter.2 <-   1.96

plot(1:5)
L <- list(
  "Intercept:", bquote(.(my.inter.1)%+-%.(my.inter.2)),
  "Slope:", bquote(.(my.slope.1)%+-%.(my.slope.2))
)
text(2, c(4,4,4.25,4.25), sapply(L, as.expression), pos = c(2,4,2,4))


On 7/22/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > On Fri, 22 Jul 2005, Dan Bolser wrote:
> >
> > > On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
> > >
> > >> Try as.expression(bquote(...whatever...))
> > >
> > > Sob, wimper, etc.
> >
> >   a<-7
> >   plot(1)
> >   legend("topleft",legend=do.call("expression",
> >                      list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))))
> >
> > works for me.  The trick is getting the inner calls to bquote
> > evaluated, since expression doesn't evaluate its argument.
> 
> I think legend accepts a list argument directly so that could be
> simplified to just:
> 
>  a<-7
>  plot(1)
>  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
>  legend("topleft",legend=L)
> 
> The same comment seems to apply to my prior suggestion about
> as.expression(bquote(...)), namely that one can just write the
> following as text also supports a list argument:
> 
> 
> my.slope.1 <-   3.22
> my.slope.2 <-   0.13
> my.inter.1 <- -10.66
> my.inter.2 <-   1.96
> 
> plot(1:5)
> L <- list(
>  "Intercept:", bquote(.(my.inter.1)%+-%.(my.inter.2)),
>  "Slope:", bquote(.(my.slope.1)%+-%.(my.slope.2))
> )
> text(2, c(4,4,4.25,4.25), L, pos = c(2,4,2,4))
>



From wl at eimb.ru  Fri Jul 22 19:45:36 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Fri, 22 Jul 2005 09:45:36 -0800
Subject: [R] Lattice: how to make x axis to appear on only one non-bottom
	plot?
Message-ID: <1438101309.20050722094536@eimb.ru>

Dear r-help,

   Attached is the fragment of the lattice plot, produced with xyplot
   function.
   Data were drawn for all months of a year except December.
   xyplot was called with parameters
   layout=c(3,4)
   scales=list(x=list(alternating=1),y=list(alternating=1))

   Because of the hole in the bottom right, caused by the absence of
   the December, right column of plots has no x axis.

   Is it possible to produce it using xyplot?

   Thank you very much for attention and help
   
---
Best regards,
Wladimir                mailto:wl at eimb.ru
-------------- next part --------------
A non-text attachment was scrubbed...
Name: for_Rhelp_question.png
Type: image/png
Size: 26591 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050722/d6b5a7c1/for_Rhelp_question.png

From tlumley at u.washington.edu  Fri Jul 22 19:54:24 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Jul 2005 10:54:24 -0700 (PDT)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <971536df05072210165b5776e7@mail.gmail.com>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
	<971536df05072210165b5776e7@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0507221051440.26024@homer06.u.washington.edu>

On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
>
> I think legend accepts a list argument directly so that could be
> simplified to just:
>
>  a<-7
>  plot(1)
>  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
>  legend("topleft",legend=L)

Except that it wouldn't then work: the mathematical stuff comes out as 
text.

> The same comment seems to apply to my prior suggestion about
> as.expression(bquote(...)), namely that one can just write the
> following as text also supports a list argument:

And this doesn't work either: you end up with %+-% rather than the 
plus-or-minus symbol.

The reason I gave the do.call() version is that I had tried these simpler 
versions and they didn't work.

 	-thomas



From ggrothendieck at gmail.com  Fri Jul 22 20:01:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 14:01:38 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.A41.4.61b.0507221051440.26024@homer06.u.washington.edu>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
	<971536df05072210165b5776e7@mail.gmail.com>
	<Pine.A41.4.61b.0507221051440.26024@homer06.u.washington.edu>
Message-ID: <971536df0507221101420ab7d2@mail.gmail.com>

You are right.   One would have to use do.call as you did
or the sapply method of one of my previous posts:

a <- 7
plot(1)
L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
legend("topleft",legend=sapply(L, as.expression))


On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
> >
> > I think legend accepts a list argument directly so that could be
> > simplified to just:
> >
> >  a<-7
> >  plot(1)
> >  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
> >  legend("topleft",legend=L)
> 
> Except that it wouldn't then work: the mathematical stuff comes out as
> text.
> 
> > The same comment seems to apply to my prior suggestion about
> > as.expression(bquote(...)), namely that one can just write the
> > following as text also supports a list argument:
> 
> And this doesn't work either: you end up with %+-% rather than the
> plus-or-minus symbol.
> 
> The reason I gave the do.call() version is that I had tried these simpler
> versions and they didn't work.
> 
>        -thomas
> 
>



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Jul 22 11:59:37 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 22 Jul 2005 11:59:37 +0200 (CEST)
Subject: [R] useR! 2006
Message-ID: <Pine.LNX.4.51.0507221158070.4089@artemis.imbe.med.uni-erlangen.de>



We are happy to announce that the second R user conference

                        useR! 2006

is scheduled for June 15-17 2006 and will take place at the Vienna
University of Economics and Business Administration.

This second world-meeting of the R user community will focus on
  - R as the `lingua franca' of data analysis and statistical computing,
  - providing a platform for R users to discuss and exchange ideas how R can
    be used to do statistical computations, data analysis, visualization and
    exciting applications in various fields,
  - giving an overview of the new features of the rapidly evolving R
    project.

As for the predecessor conference, the program will consist of two parts:
keynote lectures and user-contributed sessions (abstract submission will be
available online starting from October 2005). Prior to the conference, there
will be tutorials on R (proposals for tutorials should be sent before
2005-09-30).


KEYNOTE LECTURES

R becomes the standard computing engine in more and more disciplines, both
in academia and the business world. How R is used in different areas will be
presented in keynote lectures addressing hot topics including
  - Marketing
  - Teaching
  - R on Different Platforms
  - Graphics
  - History of S and R

Speakers will include John M. Chambers, John Fox, Stefan Iacus, Uwe
Ligges, Paul Murrell, Peter Rossi, Simon Urbanek and Sanford Weisberg.


USER-CONTRIBUTED SESSIONS

The sessions will be a platform to bring together R users, contributers,
package maintainers and developers in the S spirit that `users are developers'.
People from different fields will show us how they solve problems
with R in fascinating applications. The sessions are organized by
members of the program committee, including Claudio Agostinelli, Roger
Bivand, Peter Buehlmann, Ram??n D??az-Uriarte, Sandrine Dudoit, Dirk
Eddelbuettel, Frank Harrell, Simon Jackman, Roger Koenker, Uwe Ligges, Andrew
Martin, Balasubramanian Narasimhan, Peter Rossi, Anthony Rossini, Gerhard
Tutz and Antony Unwin and will cover topics such as
  - Applied Statistics & Biostatistics
  - Bayesian Statistics
  - Bioinformatics
  - Econometrics & Finance
  - Machine Learning
  - Marketing
  - Robust Statistics
  - Spatial Statistics
  - Statistics in the Social and Political Sciences
  - Teaching
  - Visualization & Graphics
  - and many more.


PRE-CONFERENCE TUTORIALS

Before the start of the official program, half-day tutorials will be offered
on Wednesday, June 14th, including a lecture on R graphics by Paul Murrell.
We invite R users to submit proposals for three hour tutorials on special
topics on R. The proposals should give a brief description of the tutorial,
including goals, detailed outline, justification why the tutorial is
important, background knowledge required and potential attendees. The
proposals should be sent before 2005-09-30 to useR-2006 at R-project.org.



After the official presentations, Vienna's famous wine and beer pubs, cafes
and restaurants proofed to be a very stimulating environment for fruitful
discussions at previous meetings of the R community like last year's `useR!
2004'.

We invite all R users to submit abstracts on topics presenting innovations or
exciting applications of R. A web page offering more information on the `useR!'
conference, abstract submission, registration and Vienna is available at

  http://www.R-project.org/useR-2006/


IMPORTANT DATES

  - registration: starting October 2005
  - online abstract submission: starting October 2005
  - submission deadline of tutorial proposals: 2005-09-30
  - early registration deadline: 2006-01-31
  - submission deadline of abstracts: 2006-02-28
  - registration deadline: 2006-05-31


We hope to meet you in Vienna!


The organizing committee:

Torsten Hothorn, Achim Zeileis, David Meyer, Bettina Gruen,
Kurt Hornik and Friedrich Leisch

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From nikko at hailmail.net  Fri Jul 22 20:26:40 2005
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Fri, 22 Jul 2005 13:26:40 -0500
Subject: [R] Correct variance for prediction intervals from nlme and gnls
	objects
In-Reply-To: <40e66e0b050719094815ff1e01@mail.gmail.com>
References: <1121789752.13362.238783805@webmail.messagingengine.com>
	<40e66e0b050719094815ff1e01@mail.gmail.com>
Message-ID: <1122056800.2857.239040795@webmail.messagingengine.com>

Hi,
An approximate prediction interval for f(x,b) from Rupert and Carrol
(1988) on pg 53
define q^2=g^2((f(x0,b),x0,a) + 1/N f'b(x0,b)^T S^-1 f'b(x0,b)

where g is the variance function and f'b is the derivative in terms of b
evaluted at x0
and S is
      S=(1/N) Sum(1,N) f'b(xi,b)f'b(xi,b)^T/g^2((f(xi,b),a)


 and the interval is defined at f(x0,b)+/- t(N-p,alpha/2) sqrt(sigma^2
 q^2)

My question is if I were to fit the model
fit<-gnls(y~SSlogis(x,Asymp,xmid,scal),data=foo, weights=varPower)

is attr(fit$parAssign,"varBetaFact") the correct quantity to use for
S^-1? or is (fit$dim$N/fit$sigma^2)*fit$VarBeta.
I looked in Pinero and Bates chapter 7.5 but i could not figure out
exactly how
varBeta is estimated in gnls. A pointer to a reference or some guidance
would be very helpful.

Thanks
Nicholas

PS Sorry for the sloppy notation, email is not latex


On Tue, 19 Jul 2005 11:48:18 -0500, "Douglas Bates" <dmbates at gmail.com>
said:
> On 7/19/05, Nicholas Lewin-Koh <nikko at hailmail.net> wrote:
> > Hello,
> > I am writing a set of functions to do prediction and calibration
> > intervals
> > for at least the subset of selfstarting models if not some more general
> > ones.
> > 
> > I need to be able to extract the varFunction from a fit object
> > and evaluate it at a predicted point. Are there any examples around?
> > 
> > Also in a self start model, say SSlogis, how would I get the gradient
> > at a point?
> 
> I think that the output of 
> 
> example(SSlogis)
> 
> should answer that question for you.



From ghwei at umich.edu  Fri Jul 22 20:37:31 2005
From: ghwei at umich.edu (ghwei@umich.edu)
Date: Fri, 22 Jul 2005 14:37:31 -0400
Subject: [R] how to test the equalness of several coefficients in a
	gamma	frailty model using R
Message-ID: <20050722143731.jll5gie1pk4c8ggk@web.mail.umich.edu>

Hi,

I want to test the equalness of several coefficients of a gamma frailty model
using R. In SAS, a TEST statement can be used for a cox model.How to do it in
R?
Thanks a lot!

Guanghui



From sdavis2 at mail.nih.gov  Fri Jul 22 20:46:31 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 22 Jul 2005 14:46:31 -0400
Subject: [R] time series and regions of change
Message-ID: <e09b65b8d293dd9c304b5b905063da65@mail.nih.gov>

Preface:  this is a statistical question more than an R question.

I have a vector of numbers (assume a regular time series).  Within this 
time series, I have a set of regions of interest (all of different 
lengths) that I want to compare against a "baseline" (which is known).  
There is some autocorrelation involved.  I would like to determine the 
"significance" of the regions as judged by their length and relative 
height from the baseline.  I could do a simple t-test or something like 
that, but this seems to be too sensitive (probably due to dependency 
between adjacent observations).  I have thought about Hidden Markov 
Models, but I don't know the number of states.  Any other ideas?

Thanks,
Sean



From Achim.Zeileis at wu-wien.ac.at  Fri Jul 22 21:21:44 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 22 Jul 2005 21:21:44 +0200
Subject: [R] time series and regions of change
In-Reply-To: <e09b65b8d293dd9c304b5b905063da65@mail.nih.gov>
References: <e09b65b8d293dd9c304b5b905063da65@mail.nih.gov>
Message-ID: <20050722212144.0cb0feed.Achim.Zeileis@wu-wien.ac.at>

On Fri, 22 Jul 2005 14:46:31 -0400 Sean Davis wrote:

> Preface:  this is a statistical question more than an R question.
> 
> I have a vector of numbers (assume a regular time series).  Within
> this time series, I have a set of regions of interest (all of
> different lengths) that I want to compare against a "baseline" (which
> is known).  There is some autocorrelation involved.  I would like to
> determine the "significance" of the regions as judged by their length
> and relative height from the baseline.  I could do a simple t-test or
> something like that, but this seems to be too sensitive (probably due
> to dependency between adjacent observations).  I have thought about
> Hidden Markov Models, but I don't know the number of states.  Any
> other ideas?

If the regions of interest are known (i.e., not determined in some way
from the data), then you could use a simple ANOVA using a covariance
matrix estimate that is robust to serial correlation. The package
sandwich provides such estimators (e.g., vcovHAC and kernHAC) that can
be plugged into the function waldtest() in the package lmtest. This
corresponds to conduncting an anova() with a different covariance matrix
estimate. See the sandwich vignette for some examples.

If the regions of interest are unknown and have to be estimated, the
function breakpoints() in package strucchange might be useful. See
help(breakpoints) for some examples and further references.

hth,
Z

> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From liuwensui at gmail.com  Fri Jul 22 21:40:10 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 22 Jul 2005 15:40:10 -0400
Subject: [R] about nnet package
In-Reply-To: <200507221534.CPA81997@mirapoint.uc.edu>
References: <200507221534.CPA81997@mirapoint.uc.edu>
Message-ID: <1115a2b005072212404ec45b65@mail.gmail.com>

you might need quit("yes") when you exit R.

On 7/22/05, Baoqiang Cao <caobg at email.uc.edu> wrote:
> Hello Uwe Ligges,
>     I might misdiscribe my question. What I thought is that, after nnetObject<-nnet(...), this nnetObject will be gone after I exit R. Since I need the trained nnetObject for next time without training it again, how can I save this nnetObject (to some files I guess)? Thanks.
> 
> Best regards,
>   Baoqiang Cao
> 
> ======= At 2005-07-22, 11:17:37 you wrote: =======
> 
> >Baoqiang Cao wrote:
> >
> >> Dear All,
> >>
> >> I'm learning to train a neural network with my training data by using nnet package, then evaluate it with a evaluation set. My problem here is that, I need the trained network to be used in future, so, what should I store? and How? Any other options other than nnet package? Any example will be highly appreciated!
> >>
> >> Best,
> >>  Baoqiang Cao
> >
> >
> >See ?nnet which point you to its predict method.
> >
> >You say
> >
> >nnetObject <- nnet(.....)
> >
> >Now store the nnetObject and use it later as in:
> >
> >predict(nnetObject, newdata)
> >
> >
> >
> >Uwe Ligges
> >
> >
> >
> >
> >
> >
> >>
> >>
> >> ------------------------------------------------------------------------
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >.
> 
> = = = = = = = = = = = = = = = = = = = =
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From mschwartz at mn.rr.com  Fri Jul 22 22:28:58 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 22 Jul 2005 15:28:58 -0500
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <971536df0507221101420ab7d2@mail.gmail.com>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
	<971536df05072210165b5776e7@mail.gmail.com>
	<Pine.A41.4.61b.0507221051440.26024@homer06.u.washington.edu>
	<971536df0507221101420ab7d2@mail.gmail.com>
Message-ID: <1122064138.3330.75.camel@localhost.localdomain>

Ok guys,

So I played around with this a bit, going back to Dan's original
requirements and using Thomas' do.call() approach with legend(). Gabor's
approach using sapply() will also work here. I have the following:

# Note the leading spaces here for alignment in the table
# This could be automated with formatC() or sprintf()
my.slope.1 <-  "  3.22"
my.slope.2 <-  "0.13"
my.inter.1 <-  "-10.66"
my.inter.2 <-  "1.96"
my.Rsqua <-    "  0.97"

plot(1:5)

L <- list("Intercept:",
          "Slope:",
          bquote(paste(R^2, ":")),
          bquote(.(my.inter.1) %+-% .(my.inter.2)),
          bquote(.(my.slope.1) %+-% .(my.slope.2)),
          bquote(.(my.Rsqua)))

par(family = "mono")

legend("topleft", legend = do.call("expression", L), ncol = 2)



Note however, that while using the mono font helps with vertical
alignment of numbers, the +/- sign still comes out in the default font,
which is bold[er] than the text.

If one uses the default font, which is variable spaced, it is
problematic to get the proper alignment for the numbers. I even tried
using phantom(), but that didn't quite get it, since the spacing is
variable, as opposed to LaTeX's mono numeric spacing with default fonts.

Also, note that I am only using two columns, rather than three, since
trying to place the ":" as a middle column results in spacing that is
too wide, given that the text.width argument is a scalar and is set to
the maximum width of the character vectors.

Note also that even with mono spaced fonts, the exponent in R^2 is still
horizontally smaller than the other characters. Thus, spacing on that
line may also be affected depending upon what else one might attempt.

Not sure where else to go from here.

HTH,

Marc Schwartz

On Fri, 2005-07-22 at 14:01 -0400, Gabor Grothendieck wrote:
> You are right.   One would have to use do.call as you did
> or the sapply method of one of my previous posts:
> 
> a <- 7
> plot(1)
> L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
> legend("topleft",legend=sapply(L, as.expression))
> 
> 
> On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
> > >
> > > I think legend accepts a list argument directly so that could be
> > > simplified to just:
> > >
> > >  a<-7
> > >  plot(1)
> > >  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
> > >  legend("topleft",legend=L)
> > 
> > Except that it wouldn't then work: the mathematical stuff comes out as
> > text.
> > 
> > > The same comment seems to apply to my prior suggestion about
> > > as.expression(bquote(...)), namely that one can just write the
> > > following as text also supports a list argument:
> > 
> > And this doesn't work either: you end up with %+-% rather than the
> > plus-or-minus symbol.
> > 
> > The reason I gave the do.call() version is that I had tried these simpler
> > versions and they didn't work.
> > 
> >        -thomas
> >



From f.calboli at imperial.ac.uk  Fri Jul 22 23:57:10 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 22 Jul 2005 22:57:10 +0100
Subject: [R] vectorising ifelse()
Message-ID: <1354DF51-DE9C-40B0-B9FD-ED516789C8BB@imperial.ac.uk>

On 22 Jul 2005, at 11:20, Adaikalavan Ramasamy wrote:


> Does either 'zippo' or 'zappo' contain the values 1 or 2 ?
>
>
> If so, then you cannot vectorize this code because you are changing  
> the
> values in 'new' at every iteration and potentially sampling a value  
> from
> new[ ,1] or new[ ,2] .
>

That's exactly my situation, and is exactly what I want to do.

After taking out the typo (and bug) "drow[i]>0" the code seems to  
work fast enough... I'll tinker a bit with it, but it could be good  
enough as it is.

Cheers,

Federico Calboli


--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From drewbrewit at yahoo.com  Sat Jul 23 00:32:43 2005
From: drewbrewit at yahoo.com (Nick Drew)
Date: Fri, 22 Jul 2005 15:32:43 -0700 (PDT)
Subject: [R] The steps of building library in R 2.1.1
Message-ID: <20050722223244.6472.qmail@web50911.mail.yahoo.com>

Agreed, It takes a lot of work for one to create
his/her first package. And
there are many opportunities to get it wrong.

I just created my first package, with thanks to Gabor,
Peter Rossi (and his
excellent document at
http://gsbwww.uchicago.edu/fac/peter.rossi/research/bayes%20book/bayesm/Maki
ng%20R%20Packages%20Under%20Windows.pdf ), and the
guys who developed
http://www.maths.bris.ac.uk/~maman/computerstuff/Rhelp/Rpackages.html
. I
could not have done it without them!!!

And lastly thanks to all of those who have developed R
and worked on this
great program over the years!

~Nick





-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Gabor Grothendieck
Sent: Thursday, July 21, 2005 5:43 AM
To: Uwe Ligges
Cc: r-help at stat.math.ethz.ch; Ivy_Li
Subject: Re: [R] The steps of building library in R
2.1.1


I think you have been using R too long.  Something
like
this is very much needed.  There are two problems:

1. the process itself is too complex (need to get rid
of perl,
    integrate package development tools with package
installation
   procedure [it should be as easy as downloading a
package],
   remove necessity to set or modify any environment
variables
    including the path variables).

2. there is too much material to absorb just to create
a package.
    The manuals are insufficient.

A step-by-step simplification is very much needed. 
Its no
coincidence that there are a number of such
descriptions on
the net (google for 'making creating R package') since
I would
guess that just about everyone has significant
problems in creating
their first package on Windows.


On 7/21/05, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote:
> Ivy_Li wrote:
>
> > Dear All,
> >
> > With the warm support of every R expert, I have
built my R library
successfully.
> > Especially thanks: Duncan Murdoch
> >                   Gabor Grothendieck
> >                   Henrik Bengtsson
> >                   Uwe Ligges
>
> You are welcome.
>
>
> The following is intended for the records in the
archive in order to
> protect readers.
>
>
> > Without your help, I will lower efficiency.
> > I noticed that some other friends were puzzled by
the method of building
library. Now, I organize a document about it. Hoping
it can help more
friends.
> >
> > 1. Read the webpage
<http://www.stats.ox.ac.uk/pub/Rtools>
>
> Do you mean
http://www.murdoch-sutherland.com/Rtools/ ?
>
> > 2. Download the "rw2011.exe"; Install the newest
version of R
> > 3. Download the "tools.zip"; Unpack it into
c:\cygwin
>
> Not required to call it "cygwin" - also a bit
misleading...
>
> > 4. Download the
"ActivePerl-5.6.1.633-MSWin32-x86.msi"; Install Active
Perl in c:\Perl
>
> Why in C:\Perl ?
>
> > 5. Download the "MinGW-3.1.0-1.exe"; Install the
mingw32 port of gcc in
c:\mingwin
>
> Why in c:\mingwin ?
>
>
> > 6. Then go to "Control Panel -> System -> Advanced
-> Environment
Variables -> Path -> Variable Balue"; add
"c:\cygwin;c:\mingwin\bin"
> >       The PATH variable already contains a couple
of paths, add the two
given above in front of all others, separated by ";".
> >       Why we add them in the beginning of the
path? Because we want the
folder that contains the tools to be at the beginning
so that you eliminate
the possibility of finding a different program of the
same name first in a
folder that comes prior to the one where the tools are
stored.
>
>
> OK, this (1-6) is all described in the R
Administration and Installation
> manual, hence I do not see why we have to repeat it
here.
>
>
> > 7. I use the package.skeleton() function to make a
draft package. It
will automate some of the setup for a new source
package. It creates
directories, saves functions and    data to
appropriate places, and creates
skeleton help files and 'README' files describing
further steps in
packaging.
> > I type in R:
> >       >f <- function(x,y) x+y
> >       >g <- function(x,y) x-y
> >       >d <- data.frame(a=1, b=2)
> >       >e <- rnorm(1000)
> >       >package.skeleton(list=c("f","g","d","e"),
name="example")
> > Then modify the 'DESCRIPTION':
> >       Package: example
> >       Version: 1.0-1
> >       Date: 2005-07-09
> >       Title: My first function
> >       Author: Ivy <Ivy_Li at smics.com>
> >       Maintainer: Ivy <Ivy_Li at smics.com>
> >       Description: simple sum and subtract
> >       License: GPL version 2 or later
> >       Depends: R (>= 1.9), stats, graphics, utils
> > You can refer to the web page:
http://cran.r-project.org/src/contrib/Descriptions/ 
There are larger source
of examples. And you can read the part of 'Creating R
Packages' in 'Writing
R Extension'. It introduces some useful things for
your reference.
>
>
> This is described in Writing R Extension and is not
related to the setup
> of you system in 1-6.
>
>
> >
> > 8. Download hhc.exe Microsoft help compiler from
somewhere. And save it
somewhere in your path.
> >     I download a 'htmlhelp.exe' and setup. saved
the hhc.exe into the
'C:\cygwin\bin' because this path has been writen in
my PATH Variable Balue.
> >     However if you decided not to use the Help
Compiler (hhc), then you
need to modify the MkRules file in RHOME/src/gnuwin32
to tell it not to try
to build that kind of help file
>
>
> This is described in the R Administration and
Installation manual
> and I do not see why we should put the html compiler
to the other tools.
>
>
> > 9. In the DOS environment. Into the "D:\>"  Type
the following code:
>
> There is no DOS environment in Windows NT based
operating systems.
>
>
> >       cd \Program Files\R\rw2010
> >       bin\R CMD INSTALL "/Program
Files/R/rw2011/example"
>
> I do not see why anybody would like to contaminate
the binary
> installation of R with some development source
packages.
> I'd rather use a separate directory.
>
> I think reading the two mentioned manuals shoul be
sufficient. You have
> not added relevant information. By adding irrelevant
information and
> omitting some relevant information, I guess we got
something that is
> misleading if the reader does NOT read the manuals
as well.
>
> Best,
> Uwe Ligges
>
>
> > Firstly, because I install the new version R in
the D:\Program Files\.
So I should first into the D drive. Secondly, because
I use the
package.skeleton() function to build 'example' package
in the path of
D:\Program Files\R\rw2011\  So I must tell R the path
where saved the
'example' package. So I write the code is like that.
If your path is
different from me, you should modify part of these
code.
> >
> > 10.Finally, this package is successfully built up.
> >
> >         ---------- Making package example
------------
> >         adding build stamp to DESCRIPTION
> >         installing R files
> >         installing data files
> >         installing man source files
> >         installing indices
> >         not zipping data
> >         installing help
> >        >>> Building/Updating help pages for
package 'example'
> >            Formats: text html latex example chm
> >         d                                 text   
html    latex
example
> >         e                                 text   
html    latex
example
> >         f                                 text   
html    latex
example
> >         g                                 text   
html    latex
example
> >         adding MD5 sums
> >
> >       * DONE (example)
> >
> > I was very happy to get the great results. I hope
the document can help
you. Thank you again for everyone's support.
> >
> >
> > Best Regards!
> > Ivy Li????????????????
> > YMS in Production & Testing
> > Semiconductor Manufactory International(ShangHai)
Corporation
> > #18 ZhangJiang Road, PuDong New Area, Shanghai,
China
> > Tel: 021-5080-2000 *11754
> > Email: Ivy_Li at smics.com
> >
> >
> >
> >
------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>





		
__________________________________ 

Stay connected, organized, and protected. Take the tour:



From gerifalte28 at hotmail.com  Sat Jul 23 01:19:30 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 22 Jul 2005 23:19:30 +0000
Subject: [R] poisson fit for histogram
In-Reply-To: <6DE2C8B4-3C9E-47F9-BB3D-87D6E0967898@plantpath.wisc.edu>
Message-ID: <BAY103-F30B6740CC8AD7D5BFD21AEA6C90@phx.gbl>

I would first reccomend you to update your version of R.  Then download the 
libraries rmutil and gnlm from Jim Lindsey at 
http://www.luc.ac.be/~jlindsey/rcode.html decompress and store the files in 
your "library" folder.   Sorry but you will have to donwload a package 
unless you seriously want to re-invent the wheel.
Finally try
library(gnlm)
?fit.dist()

Cheers

Francisco


>From: Thomas Isenbarger <isen at plantpath.wisc.edu>
>To: r-help at stat.math.ethz.ch
>Subject: [R] poisson fit for histogram
>Date: Wed, 20 Jul 2005 10:40:51 -0500
>
>I haven't been an R lister for a bit, but I hope to enlist someone's
>help here.  I think this is a simple question, so I hope the answer
>is not much trouble.  Can you please respond directly to this email
>address in addition to the list (if responding to the list is
>warranted)?
>
>I have a histogram and I want to see if the data fit a Poisson
>distribution.  How do I do this?  It is preferable if it could be
>done without having to install any or many packages.
>
>I use R Version 1.12 (1622) on OS X
>
>Thank-you very much,
>Tom Isenbarger
>
>
>--
>Tom Isenbarger PhD
>isen at plantpath.wisc.edu
>608.265.0850
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From r.shengzhe at gmail.com  Sat Jul 23 01:30:27 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Sat, 23 Jul 2005 01:30:27 +0200
Subject: [R] Help: PLSR
Message-ID: <ea57975b05072216304e4c3bab@mail.gmail.com>

Hello,

I have a data set with 15 variables (first one is the response) and
1200 observations. Now I use pls package to do the plsr with cross
validation as below.

trainSet = as.data.frame(scale(trainSet, center = T, scale = T))
trainSet.plsr = mvr(formula, ncomp = 14, data = trainSet, method = "kernelpls",
                         CV = TRUE, validation = "LOO", model = TRUE, x = TRUE,
                          y = TRUE)

after that I wish to obtain the value of "se", estimated standard
errors of the estimates(cross validation), mentioned in the function
of MSEP, but not implemented yet, so I made the program by myself to
calculate it. The results I got seem not right, and I wonder which
step below is wrong.

y = trainSet.plsr$y
p = as.data.frame(trainSet.plsr$validation$pred)

i = 1; msep_element = matrix(nrow = 1200, ncol = 14)
while(i <= length(p)){
  msep_element[,i] = (p[i]-y)^2
  i = i + 1
}

msep = colMeans(msep_element)
msep_sd = sd(msep_element)

Then I compare "msep" with "trainSet.plsr$validation$MSEP", which are
the same, but the values of "msep_sd" seem much larger than I
expected, is it the same as "se"? If not, how to calculate "se" of
cross validation?

Thank you,
Shengzhe



From spencer.graves at pdf.com  Sat Jul 23 02:26:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 22 Jul 2005 17:26:34 -0700
Subject: [R] find confounder in covariates
In-Reply-To: <20050722014709.71749.qmail@web31115.mail.mud.yahoo.com>
References: <20050722014709.71749.qmail@web31115.mail.mud.yahoo.com>
Message-ID: <42E18EBA.4080201@pdf.com>

	  It was easier than I thought.  To your example, just add one line:

 > alias(f)
Model :
y ~ z1 + z2 + z3

Complete :
      (Intercept) z1B z1C z3
z2Bb              1
z2Cc                  1

	  spencer graves

Young Cho wrote:

> Hi,
> 
> I was wondering if there is a way, or function in R to
> find confounders. For istance, 
> 
> 
>>a = sample( c(1:3), size=10,replace=T)
>>X1 = factor( c('A','B','C')[a] )
>>X2 = factor( c('Aa','Bb','Cc')[a] )
>>Xmat = data.frame(X1,X2,rnorm(10),rnorm(10))
>>dimnames(Xmat)[[2]] = c('z1','z2','z3','y')
> 
> 
> Now, z2 is just an alias of z1. There can be a
> collinearity like one is a linear combination of
> others. If you run lm on it:
>  
> 
>>f = lm(y~.,data=Xmat)
>>summary(f)
> 
> 
> Call:
> lm(formula = y ~ ., data = Xmat)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -1.2853 -0.3708 -0.1224  0.4617  1.2821 
> 
> Coefficients: (2 not defined because of singularities)
>             Estimate Std. Error t value Pr(>|t|)  
> (Intercept)  0.82141    0.44583   1.842   0.1150  
> z1B         -1.34167    0.65176  -2.059   0.0852 .
> z1C          0.80891    1.07639   0.751   0.4808  
> z2Bb              NA         NA      NA       NA  
> z2Cc              NA         NA      NA       NA  
> z3           0.04231    0.23397   0.181   0.8625  
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.'
> 0.1 ' ' 1 
> 
> Residual standard error: 0.971 on 6 degrees of freedom
> Multiple R-Squared: 0.5086,     Adjusted R-squared:
> 0.2629 
> F-statistic:  2.07 on 3 and 6 DF,  p-value: 0.2057 
> 
> In this case, I can look at data and figure out which
> variable is confounded with which. But, if we have
> many categorial covariates ( not necessarily same
> number of levels ), it is almost impossible to check
> it out.  
> 
> Any help would be greatly appreicated. Thanks.
> 
> Young.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sat Jul 23 04:55:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Jul 2005 22:55:04 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <1122064138.3330.75.camel@localhost.localdomain>
References: <Pine.LNX.4.21.0507221434290.30006-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.A41.4.61b.0507220732190.148728@homer11.u.washington.edu>
	<971536df05072210165b5776e7@mail.gmail.com>
	<Pine.A41.4.61b.0507221051440.26024@homer06.u.washington.edu>
	<971536df0507221101420ab7d2@mail.gmail.com>
	<1122064138.3330.75.camel@localhost.localdomain>
Message-ID: <971536df0507221955a86ff1d@mail.gmail.com>

If one modifies legend by adding a vfont=c("serif", "plain") argument 
to the call to text (which is within the function text2 that is defined 
within legend) then one can do this:

my.slope.1 <-  "  3.22"
my.slope.2 <-  "0.13"
my.inter.1 <-  "-10.66"
my.inter.2 <-  "1.96"
my.Rsqua <-    "0.97"

plot(1:5)

tt <- c("Intercept:", "Slope:", "R\\S2:", 
	paste(my.inter.1, "\\+-", my.inter.2),
	paste(my.slope.2, "\\+-", my.slope.2), my.Rsqua)

# modified legend to accept vfont
my.legend("topleft", legend = tt, ncol = 2, adj = 0.1, vfont =
c("serif", "plain"))

which only requires character manipulation -- no plotmath,
expression or bquote; however, unfortunately it still does
not line up.  It undoubtedly would be possible to fix up
legend so that when used with Hershey fonts everything
lines up as expected.



From samferguson at ihug.com.au  Sat Jul 23 06:22:02 2005
From: samferguson at ihug.com.au (Sam Ferguson)
Date: Sat, 23 Jul 2005 14:22:02 +1000
Subject: [R] Lattice: reversing order of panel placement in conditional
	histograms
Message-ID: <op.sucme02dc0cqda@samferguson-g5.arch.usyd.edu.au>

Hi R-people,

I have a question about lattice in general, and histogram specfically. How do you control the ordering of factors that controls the placement of the conditional panels. I have a dataset with factors that go 'Q1','Q2',"Q3','Q5' and of course I want the plot to place Question Q1 at the top and Question Q5 at the bottom of the graphical output. histogram() does the opposite as 5 is larger than 1. Similarly my 'AlertFormat' factor is a textual category, and I need the data to read from left to right (representing old to new) , with 'New A & V' on the right, and 'Pre-existing A & V' on the left, which is the opposite to how histogram plots.

The current lattice output from what's below is here:- http://www.arch.usyd.edu.au/~sfer9710/latticeoutput.pdf

All I really want to do is reverse the placement entirely from what I have received above.

I've checked the help to no avail, and have looked through the r-help archive. I love lattice, but always have problems of this nature. I hope someone can help me solve them.

Thanks heaps
-- Sam


> version
          _
platform powerpc-apple-darwin7.9.0
arch     powerpc
os       darwin7.9.0
system   powerpc, darwin7.9.0
status   Patched
major    2
minor    1.0
year     2005
month    05
day      12
language R

> aaRatings
    Question       X.Alert.Format. Response
1        Q1             New A & V        3
2        Q1             New A & V        3
3        Q1             New A & V        3
4        Q1             New A & V        3
5        Q1 New A, Pre-existing V        3
6        Q1 New A, Pre-existing V        3
7        Q1 New A, Pre-existing V        2
8        Q1 New A, Pre-existing V        2
9        Q1 New V, Pre-existing A        3
10       Q1 New V, Pre-existing A        2
11       Q1 New V, Pre-existing A        2
12       Q1 New V, Pre-existing A        2
13       Q1    Pre-existing A & V        1
14       Q1    Pre-existing A & V       -2
15       Q1    Pre-existing A & V       -2
16       Q1    Pre-existing A & V       -2
17       Q2             New A & V        3
18       Q2             New A & V        3
19       Q2             New A & V        3
20       Q2             New A & V        3
21       Q2 New A, Pre-existing V        3
22       Q2 New A, Pre-existing V        2
23       Q2 New A, Pre-existing V        2
24       Q2 New A, Pre-existing V        1
25       Q2 New V, Pre-existing A        3
26       Q2 New V, Pre-existing A        2
27       Q2 New V, Pre-existing A        2
28       Q2 New V, Pre-existing A        2
29       Q2    Pre-existing A & V        1
30       Q2    Pre-existing A & V       -2
31       Q2    Pre-existing A & V       -2
32       Q2    Pre-existing A & V       -2
33       Q3             New A & V        3
34       Q3             New A & V        3
35       Q3             New A & V        3
36       Q3             New A & V        3
37       Q3 New A, Pre-existing V        3
38       Q3 New A, Pre-existing V        3
39       Q3 New A, Pre-existing V        3
40       Q3 New A, Pre-existing V        2
41       Q3 New V, Pre-existing A        3
42       Q3 New V, Pre-existing A        3
43       Q3 New V, Pre-existing A        3
44       Q3 New V, Pre-existing A        2
45       Q3    Pre-existing A & V        2
46       Q3    Pre-existing A & V        1
47       Q3    Pre-existing A & V       -2
48       Q3    Pre-existing A & V       -3
49       Q5             New A & V        3
50       Q5             New A & V        3
51       Q5             New A & V        3
52       Q5             New A & V        3
53       Q5 New A, Pre-existing V        3
54       Q5 New A, Pre-existing V        3
55       Q5 New A, Pre-existing V        1
56       Q5 New A, Pre-existing V        1
57       Q5 New V, Pre-existing A        3
58       Q5 New V, Pre-existing A        2
59       Q5 New V, Pre-existing A        2
60       Q5 New V, Pre-existing A        2
61       Q5    Pre-existing A & V        1
62       Q5    Pre-existing A & V       -1
63       Q5    Pre-existing A & V       -1
64       Q5    Pre-existing A & V       -3
> attach(aaRatings)
> histogram(Response|X.Alert.Format.+Question,type="count",nint=8)



From ripley at stats.ox.ac.uk  Sat Jul 23 09:29:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Jul 2005 08:29:28 +0100 (BST)
Subject: [R] poisson fit for histogram
In-Reply-To: <BAY103-F30B6740CC8AD7D5BFD21AEA6C90@phx.gbl>
References: <BAY103-F30B6740CC8AD7D5BFD21AEA6C90@phx.gbl>
Message-ID: <Pine.LNX.4.61.0507230807430.2230@gannet.stats>

What does fit.dist do that fitdistr (MASS) does not in this context?  (It 
plots, but that is very easy to do in base R.  However, to see if a 
Poisson fits you need a test of goodness-of-fit.)

BTW, `decompress and store the files in your "library" folder' is on no OS 
(you did not mention one but Thomas did) the way to install a package. 
Even on Windows (where it might just work) there are simpler and better 
ways to do it, like using a menu.  Note that Lindsey does not provide 
pre-compiled packages for MacOS X, the platform Thomas is using (and as 
they use Fortran, people have reported that they are tricky to install on 
MacOS X), and your recipe is `seriously' misleading there.

On Fri, 22 Jul 2005, Francisco J. Zagmutt wrote:

> I would first reccomend you to update your version of R.  Then download the
> libraries rmutil and gnlm from Jim Lindsey at
> http://www.luc.ac.be/~jlindsey/rcode.html decompress and store the files in
> your "library" folder.   Sorry but you will have to donwload a package
> unless you seriously want to re-invent the wheel.
> Finally try
> library(gnlm)
> ?fit.dist()
>
> Cheers
>
> Francisco
>
>
>> From: Thomas Isenbarger <isen at plantpath.wisc.edu>
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] poisson fit for histogram
>> Date: Wed, 20 Jul 2005 10:40:51 -0500
>>
>> I haven't been an R lister for a bit, but I hope to enlist someone's
>> help here.  I think this is a simple question, so I hope the answer
>> is not much trouble.  Can you please respond directly to this email
>> address in addition to the list (if responding to the list is
>> warranted)?
>>
>> I have a histogram and I want to see if the data fit a Poisson
>> distribution.  How do I do this?  It is preferable if it could be
>> done without having to install any or many packages.
>>
>> I use R Version 1.12 (1622) on OS X
>>
>> Thank-you very much,
>> Tom Isenbarger


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Jul 23 09:38:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Jul 2005 08:38:03 +0100 (BST)
Subject: [R] Lattice: reversing order of panel placement in conditional
 histograms
In-Reply-To: <op.sucme02dc0cqda@samferguson-g5.arch.usyd.edu.au>
References: <op.sucme02dc0cqda@samferguson-g5.arch.usyd.edu.au>
Message-ID: <Pine.LNX.4.61.0507230830310.2230@gannet.stats>

>From what I understand, you want to set up a factor with the levels 
reversed.  It is not that `5 is larger than 1', but that you created a 
factor with the levels in alphabetic order.  Lattice plots in the order of 
the levels.

Something like  f <- factor(f, levels=rev(levels(f))) will do this.

For the horizontal factor, give the levels in the order you want them to 
appear (which might not be the reverse of alphabetic).


On Sat, 23 Jul 2005, Sam Ferguson wrote:

> I have a question about lattice in general, and histogram specfically. 
> How do you control the ordering of factors that controls the placement 
> of the conditional panels. I have a dataset with factors that go 
> 'Q1','Q2',"Q3','Q5' and of course I want the plot to place Question Q1 
> at the top and Question Q5 at the bottom of the graphical output. 
> histogram() does the opposite as 5 is larger than 1. Similarly my 
> 'AlertFormat' factor is a textual category, and I need the data to read 
> from left to right (representing old to new) , with 'New A & V' on the 
> right, and 'Pre-existing A & V' on the left, which is the opposite to 
> how histogram plots.
>
> The current lattice output from what's below is here:- 
> http://www.arch.usyd.edu.au/~sfer9710/latticeoutput.pdf
>
> All I really want to do is reverse the placement entirely from what I 
> have received above.
>
> I've checked the help to no avail, and have looked through the r-help 
> archive. I love lattice, but always have problems of this nature. I hope 
> someone can help me solve them.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Jul 23 11:45:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Jul 2005 11:45:56 +0200
Subject: [R] about nnet package
In-Reply-To: <1115a2b005072212404ec45b65@mail.gmail.com>
References: <200507221534.CPA81997@mirapoint.uc.edu>
	<1115a2b005072212404ec45b65@mail.gmail.com>
Message-ID: <42E211D4.5070909@statistik.uni-dortmund.de>

Wensui Liu wrote:

> you might need quit("yes") when you exit R.

I think nobody want to save the whole workspace if just one object is of 
interest, hence you save(), as already mentioned in a former post.

Uwe Ligges

> On 7/22/05, Baoqiang Cao <caobg at email.uc.edu> wrote:
> 
>>Hello Uwe Ligges,
>>    I might misdiscribe my question. What I thought is that, after nnetObject<-nnet(...), this nnetObject will be gone after I exit R. Since I need the trained nnetObject for next time without training it again, how can I save this nnetObject (to some files I guess)? Thanks.
>>
>>Best regards,
>>  Baoqiang Cao
>>
>>======= At 2005-07-22, 11:17:37 you wrote: =======
>>
>>
>>>Baoqiang Cao wrote:
>>>
>>>
>>>>Dear All,
>>>>
>>>>I'm learning to train a neural network with my training data by using nnet package, then evaluate it with a evaluation set. My problem here is that, I need the trained network to be used in future, so, what should I store? and How? Any other options other than nnet package? Any example will be highly appreciated!
>>>>
>>>>Best,
>>>> Baoqiang Cao
>>>
>>>
>>>See ?nnet which point you to its predict method.
>>>
>>>You say
>>>
>>>nnetObject <- nnet(.....)
>>>
>>>Now store the nnetObject and use it later as in:
>>>
>>>predict(nnetObject, newdata)
>>>
>>>
>>>
>>>Uwe Ligges
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>>
>>>>------------------------------------------------------------------------
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>.
>>
>>= = = = = = = = = = = = = = = = = = = =
>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
>



From turwa720 at student.otago.ac.nz  Sat Jul 23 12:33:55 2005
From: turwa720 at student.otago.ac.nz (Worik Turei stanton)
Date: Sat, 23 Jul 2005 22:33:55 +1200
Subject: [R] cor(X) with P-Value
Message-ID: <1122114835.42e21d13407a2@www.studentmail.otago.ac.nz>

Friends

I am new to R (and statistics) so am struggling a bit.

Briefly...

I am interested in getting the P-Value from cor(X) where X is a matrix.

I have found cor.test.

Verbosely...

I have 4 vectors and can generate the corellation matrix...


> cor(cbind(X1, X2, X3, X4))
            X1          X2           X3           X4
X1  1.00000000 -0.06190365 -0.156972795  0.182547517
X2 -0.06190365  1.00000000  0.264352860  0.146750844
X3 -0.15697279  0.26435286  1.000000000 -0.006380052
X4  0.18254752  0.14675084 -0.006380052  1.000000000

But I want the P-Values (gives me the significance I belive).

> cor.test(X2, X3)

	Pearson's product-moment correlation

data:  X2 and X3
t = 3.3346, df = 148, p-value = 0.001080
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.1086963 0.4073565
sample estimates:
      cor
0.2643529

is very cool.  Just what I want.  But there is too much information for
too little data.

What I would like to do is some thing like...

cor_with_p_test(cbind(X1, X2, X3, X4))
     X1           X2              X3
X1    1          -0.06190365      -0.1569728...
P                0.4517           0.05507   ...

X2  -0.06190365  1                0.2643529 ...
P   0.4517                        0.001080  ...
:
:

I think I could write a function for it if such a function does not exist
if I could do...

> CT23 <- cor.test(X2, X3)
> CT23$P
0.001080
> CT23$V
0.2643529

But I do not know how.

cheers
Worik



From ccleland at optonline.net  Sat Jul 23 12:45:27 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 23 Jul 2005 06:45:27 -0400
Subject: [R] cor(X) with P-Value
In-Reply-To: <1122114835.42e21d13407a2@www.studentmail.otago.ac.nz>
References: <1122114835.42e21d13407a2@www.studentmail.otago.ac.nz>
Message-ID: <42E21FC7.2030905@optonline.net>

   These functions are based on posts to either R-Help or S-News by 
Gabor Grothendieck and Bill Venables.

# pairwise sample size
# Gabor G - 11/23/2004 R-help List
pn <- function(X){crossprod(!is.na(X))}

cor.prob <- function(X){
# Correlations Below Main Diagonal
# Significance Tests with Pairwise Deletion
# Above Main Diagonal
# Believe part of this came from Bill Venables
pair.SampSize <- pn(X)
above1 <- row(pair.SampSize) < col(pair.SampSize)
pair.df <- pair.SampSize[above1] - 2
R <- cor(X, use="pair")
above2 <- row(R) < col(R)
r2 <- R[above2]^2
Fstat <- (r2 * pair.df)/(1 - r2)
R[above2] <- 1 - pf(Fstat, 1, pair.df)
R
}

mydata <- matrix(rnorm(1000), ncol=10)

cor.prob(mydata)

Worik Turei stanton wrote:
> Friends
> 
> I am new to R (and statistics) so am struggling a bit.
> 
> Briefly...
> 
> I am interested in getting the P-Value from cor(X) where X is a matrix.
> 
> I have found cor.test.
> 
> Verbosely...
> 
> I have 4 vectors and can generate the corellation matrix...
> 
> 
> 
>>cor(cbind(X1, X2, X3, X4))
> 
>             X1          X2           X3           X4
> X1  1.00000000 -0.06190365 -0.156972795  0.182547517
> X2 -0.06190365  1.00000000  0.264352860  0.146750844
> X3 -0.15697279  0.26435286  1.000000000 -0.006380052
> X4  0.18254752  0.14675084 -0.006380052  1.000000000
> 
> But I want the P-Values (gives me the significance I belive).
> 
> 
>>cor.test(X2, X3)
> 
> 
> 	Pearson's product-moment correlation
> 
> data:  X2 and X3
> t = 3.3346, df = 148, p-value = 0.001080
> alternative hypothesis: true correlation is not equal to 0
> 95 percent confidence interval:
>  0.1086963 0.4073565
> sample estimates:
>       cor
> 0.2643529
> 
> is very cool.  Just what I want.  But there is too much information for
> too little data.
> 
> What I would like to do is some thing like...
> 
> cor_with_p_test(cbind(X1, X2, X3, X4))
>      X1           X2              X3
> X1    1          -0.06190365      -0.1569728...
> P                0.4517           0.05507   ...
> 
> X2  -0.06190365  1                0.2643529 ...
> P   0.4517                        0.001080  ...
> :
> :
> 
> I think I could write a function for it if such a function does not exist
> if I could do...
> 
> 
>>CT23 <- cor.test(X2, X3)
>>CT23$P
> 
> 0.001080
> 
>>CT23$V
> 
> 0.2643529
> 
> But I do not know how.
> 
> cheers
> Worik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From murdoch at stats.uwo.ca  Sat Jul 23 14:54:24 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 23 Jul 2005 08:54:24 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <20050722223244.6472.qmail@web50911.mail.yahoo.com>
References: <20050722223244.6472.qmail@web50911.mail.yahoo.com>
Message-ID: <42E23E00.5010105@stats.uwo.ca>

Nick Drew wrote:
> Agreed, It takes a lot of work for one to create
> his/her first package. And
> there are many opportunities to get it wrong.
> 
> I just created my first package, with thanks to Gabor,
> Peter Rossi (and his
> excellent document at
> http://gsbwww.uchicago.edu/fac/peter.rossi/research/bayes%20book/bayesm/Maki
> ng%20R%20Packages%20Under%20Windows.pdf ), and the
> guys who developed
> http://www.maths.bris.ac.uk/~maman/computerstuff/Rhelp/Rpackages.html
> . I
> could not have done it without them!!!

Could you point out the specific bits that are missing from the R-Admin 
manual (and perhaps supply them)?  It won't get better unless someone 
improves it.

Duncan Murdoch



From f.harrell at vanderbilt.edu  Sat Jul 23 15:09:13 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 23 Jul 2005 09:09:13 -0400
Subject: [R] cor(X) with P-Value
In-Reply-To: <1122114835.42e21d13407a2@www.studentmail.otago.ac.nz>
References: <1122114835.42e21d13407a2@www.studentmail.otago.ac.nz>
Message-ID: <42E24179.2040606@vanderbilt.edu>

Worik Turei stanton wrote:
> Friends
> 
> I am new to R (and statistics) so am struggling a bit.
> 
> Briefly...
> 
> I am interested in getting the P-Value from cor(X) where X is a matrix.
> 
> I have found cor.test.
> 
> Verbosely...
> 
> I have 4 vectors and can generate the corellation matrix...
> 
> 
> 
>>cor(cbind(X1, X2, X3, X4))
> 
>             X1          X2           X3           X4
> X1  1.00000000 -0.06190365 -0.156972795  0.182547517
> X2 -0.06190365  1.00000000  0.264352860  0.146750844
> X3 -0.15697279  0.26435286  1.000000000 -0.006380052
> X4  0.18254752  0.14675084 -0.006380052  1.000000000
> 
> But I want the P-Values (gives me the significance I belive).
> 
> 
>>cor.test(X2, X3)
> 
> 
> 	Pearson's product-moment correlation
> 
> data:  X2 and X3
> t = 3.3346, df = 148, p-value = 0.001080
> alternative hypothesis: true correlation is not equal to 0
> 95 percent confidence interval:
>  0.1086963 0.4073565
> sample estimates:
>       cor
> 0.2643529
> 
> is very cool.  Just what I want.  But there is too much information for
> too little data.
> 
> What I would like to do is some thing like...
> 
> cor_with_p_test(cbind(X1, X2, X3, X4))
>      X1           X2              X3
> X1    1          -0.06190365      -0.1569728...
> P                0.4517           0.05507   ...
> 
> X2  -0.06190365  1                0.2643529 ...
> P   0.4517                        0.001080  ...
> :
> :
> 
> I think I could write a function for it if such a function does not exist
> if I could do...
> 
> 
>>CT23 <- cor.test(X2, X3)
>>CT23$P
> 
> 0.001080
> 
>>CT23$V
> 
> 0.2643529
> 
> But I do not know how.
> 
> cheers
> Worik

library(Hmisc)
rcorr(cbind(. . . .))

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jsorkin at grecc.umaryland.edu  Sat Jul 23 16:18:46 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 23 Jul 2005 10:18:46 -0400
Subject: [R] Package libblas.so.3 not found installing R2.1.1 on Linux FC4
Message-ID: <s2e21993.073@grecc.umaryland.edu>

I am trying to install 
R.2.1.1 under Linux FC4 using RPM. During the installation I get the
following message:
The following package could not be found on your system.  Installation
cannot continue until it is installed.

The package in question of libblas.so.3.

Does anyone know where I can get the package? Can I use RPM to intall
the package?
Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From p.dalgaard at biostat.ku.dk  Sat Jul 23 16:27:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jul 2005 16:27:00 +0200
Subject: [R] Package libblas.so.3 not found installing R2.1.1 on Linux
	FC4
In-Reply-To: <s2e21993.073@grecc.umaryland.edu>
References: <s2e21993.073@grecc.umaryland.edu>
Message-ID: <x2mzodd2dn.fsf@turmalin.kubism.ku.dk>

"John Sorkin" <jsorkin at grecc.umaryland.edu> writes:

> I am trying to install 
> R.2.1.1 under Linux FC4 using RPM. During the installation I get the
> following message:
> The following package could not be found on your system.  Installation
> cannot continue until it is installed.
> 
> The package in question of libblas.so.3.
> 
> Does anyone know where I can get the package? Can I use RPM to intall
> the package?
> Thanks,
> John


[pd at titmouse standalone]$ locate libblas.so.3
/usr/lib/libblas.so.3
/usr/lib/libblas.so.3.0.3
/usr/lib/libblas.so.3.0
[pd at titmouse standalone]$ rpm -qf /usr/lib/libblas.so.3
blas-3.0-29

Guess... ;-)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dmb at mrc-dunn.cam.ac.uk  Sat Jul 23 17:11:49 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 23 Jul 2005 16:11:49 +0100 (BST)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <1122064138.3330.75.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.21.0507231606040.4770-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 22 Jul 2005, Marc Schwartz (via MN) wrote:

>Ok guys,
>
>So I played around with this a bit, going back to Dan's original
>requirements and using Thomas' do.call() approach with legend(). Gabor's
>approach using sapply() will also work here. I have the following:
>
># Note the leading spaces here for alignment in the table
># This could be automated with formatC() or sprintf()
>my.slope.1 <-  "  3.22"
>my.slope.2 <-  "0.13"
>my.inter.1 <-  "-10.66"
>my.inter.2 <-  "1.96"
>my.Rsqua <-    "  0.97"
>
>plot(1:5)
>
>L <- list("Intercept:",
>          "Slope:",
>          bquote(paste(R^2, ":")),
>          bquote(.(my.inter.1) %+-% .(my.inter.2)),
>          bquote(.(my.slope.1) %+-% .(my.slope.2)),
>          bquote(.(my.Rsqua)))
>
>par(family = "mono")
>
>legend("topleft", legend = do.call("expression", L), ncol = 2)
>
>
>
>Note however, that while using the mono font helps with vertical
>alignment of numbers, the +/- sign still comes out in the default font,
>which is bold[er] than the text.
>
>If one uses the default font, which is variable spaced, it is
>problematic to get the proper alignment for the numbers. I even tried
>using phantom(), but that didn't quite get it, since the spacing is
>variable, as opposed to LaTeX's mono numeric spacing with default fonts.
>
>Also, note that I am only using two columns, rather than three, since
>trying to place the ":" as a middle column results in spacing that is
>too wide, given that the text.width argument is a scalar and is set to
>the maximum width of the character vectors.
>
>Note also that even with mono spaced fonts, the exponent in R^2 is still
>horizontally smaller than the other characters. Thus, spacing on that
>line may also be affected depending upon what else one might attempt.
>
>Not sure where else to go from here.

Ahhhhhh... So lovely! Thank you all so much!

I made a couple of tweeks to improve the overall appearance, using
"x.intersp = 0.1" tightens up the overall appearance, and using
"pch=c('','','',':',':',':')" adds the (aligned!) colons.

Here is the beauty...



my.slope.1 <-  "   3.22"
my.slope.2 <-  "0.13"
my.inter.1 <-  " -10.66"
my.inter.2 <-  "1.96"
my.Rsqua <-    "   0.97"

plot(1:5)

L <- list("Intercept",
          "Slope    ",
          bquote(paste(R^2)),
          bquote(.(my.inter.1) %+-% .(my.inter.2)),
          bquote(.(my.slope.1) %+-% .(my.slope.2)),
          bquote(.(my.Rsqua)))

par(family = "mono")

legend("topleft", #inset=-1,
       legend = do.call("expression", L),
       bg='white',
       ncol = 2,
       pch=c('','','',':',':',':'),
       x.intersp = 0.1,
       title="Yay! Thank You!"
       )


However (the final gripe ;) it seems 'inset=' dosn't work. Setting this to
anything (including the default) seems to surpress the legend without
error. But hey!

Thanks again,


 

 


>
>HTH,
>
>Marc Schwartz
>
>On Fri, 2005-07-22 at 14:01 -0400, Gabor Grothendieck wrote:
>> You are right.   One would have to use do.call as you did
>> or the sapply method of one of my previous posts:
>> 
>> a <- 7
>> plot(1)
>> L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
>> legend("topleft",legend=sapply(L, as.expression))
>> 
>> 
>> On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
>> > On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
>> > >
>> > > I think legend accepts a list argument directly so that could be
>> > > simplified to just:
>> > >
>> > >  a<-7
>> > >  plot(1)
>> > >  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
>> > >  legend("topleft",legend=L)
>> > 
>> > Except that it wouldn't then work: the mathematical stuff comes out as
>> > text.
>> > 
>> > > The same comment seems to apply to my prior suggestion about
>> > > as.expression(bquote(...)), namely that one can just write the
>> > > following as text also supports a list argument:
>> > 
>> > And this doesn't work either: you end up with %+-% rather than the
>> > plus-or-minus symbol.
>> > 
>> > The reason I gave the do.call() version is that I had tried these simpler
>> > versions and they didn't work.
>> > 
>> >        -thomas
>> > 
>



From dmb at mrc-dunn.cam.ac.uk  Sat Jul 23 17:21:41 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 23 Jul 2005 16:21:41 +0100 (BST)
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507231606040.4770-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0507231620290.4969-100000@mail.mrc-dunn.cam.ac.uk>

On Sat, 23 Jul 2005, Dan Bolser wrote:

>On Fri, 22 Jul 2005, Marc Schwartz (via MN) wrote:
>
>>Ok guys,
>>
>>So I played around with this a bit, going back to Dan's original
>>requirements and using Thomas' do.call() approach with legend(). Gabor's
>>approach using sapply() will also work here. I have the following:
>>
>># Note the leading spaces here for alignment in the table
>># This could be automated with formatC() or sprintf()
>>my.slope.1 <-  "  3.22"
>>my.slope.2 <-  "0.13"
>>my.inter.1 <-  "-10.66"
>>my.inter.2 <-  "1.96"
>>my.Rsqua <-    "  0.97"
>>
>>plot(1:5)
>>
>>L <- list("Intercept:",
>>          "Slope:",
>>          bquote(paste(R^2, ":")),
>>          bquote(.(my.inter.1) %+-% .(my.inter.2)),
>>          bquote(.(my.slope.1) %+-% .(my.slope.2)),
>>          bquote(.(my.Rsqua)))
>>
>>par(family = "mono")
>>
>>legend("topleft", legend = do.call("expression", L), ncol = 2)
>>
>>
>>
>>Note however, that while using the mono font helps with vertical
>>alignment of numbers, the +/- sign still comes out in the default font,
>>which is bold[er] than the text.
>>
>>If one uses the default font, which is variable spaced, it is
>>problematic to get the proper alignment for the numbers. I even tried
>>using phantom(), but that didn't quite get it, since the spacing is
>>variable, as opposed to LaTeX's mono numeric spacing with default fonts.
>>
>>Also, note that I am only using two columns, rather than three, since
>>trying to place the ":" as a middle column results in spacing that is
>>too wide, given that the text.width argument is a scalar and is set to
>>the maximum width of the character vectors.
>>
>>Note also that even with mono spaced fonts, the exponent in R^2 is still
>>horizontally smaller than the other characters. Thus, spacing on that
>>line may also be affected depending upon what else one might attempt.
>>
>>Not sure where else to go from here.
>
>Ahhhhhh... So lovely! Thank you all so much!
>
>I made a couple of tweeks to improve the overall appearance, using
>"x.intersp = 0.1" tightens up the overall appearance, and using
>"pch=c('','','',':',':',':')" adds the (aligned!) colons.
>
>Here is the beauty...
>
>
>
>my.slope.1 <-  "   3.22"
>my.slope.2 <-  "0.13"
>my.inter.1 <-  " -10.66"
>my.inter.2 <-  "1.96"
>my.Rsqua <-    "   0.97"
>
>plot(1:5)
>
>L <- list("Intercept",
>          "Slope    ",
>          bquote(paste(R^2)),
>          bquote(.(my.inter.1) %+-% .(my.inter.2)),
>          bquote(.(my.slope.1) %+-% .(my.slope.2)),
>          bquote(.(my.Rsqua)))
>
>par(family = "mono")
>
>legend("topleft", #inset=-1,
>       legend = do.call("expression", L),
>       bg='white',
>       ncol = 2,
>       pch=c('','','',':',':',':'),
>       x.intersp = 0.1,
>       title="Yay! Thank You!"
>       )
>
>
>However (the final gripe ;) it seems 'inset=' dosn't work. Setting this to
>anything (including the default) seems to surpress the legend without
>error. But hey!


Oh....

Error in strwidth(legend, units = "user", cex = cex) :
        family mono not included in PostScript device
Execution halted








>
>Thanks again,
>
>
> 
>
> 
>
>
>>
>>HTH,
>>
>>Marc Schwartz
>>
>>On Fri, 2005-07-22 at 14:01 -0400, Gabor Grothendieck wrote:
>>> You are right.   One would have to use do.call as you did
>>> or the sapply method of one of my previous posts:
>>> 
>>> a <- 7
>>> plot(1)
>>> L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
>>> legend("topleft",legend=sapply(L, as.expression))
>>> 
>>> 
>>> On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
>>> > On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
>>> > >
>>> > > I think legend accepts a list argument directly so that could be
>>> > > simplified to just:
>>> > >
>>> > >  a<-7
>>> > >  plot(1)
>>> > >  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
>>> > >  legend("topleft",legend=L)
>>> > 
>>> > Except that it wouldn't then work: the mathematical stuff comes out as
>>> > text.
>>> > 
>>> > > The same comment seems to apply to my prior suggestion about
>>> > > as.expression(bquote(...)), namely that one can just write the
>>> > > following as text also supports a list argument:
>>> > 
>>> > And this doesn't work either: you end up with %+-% rather than the
>>> > plus-or-minus symbol.
>>> > 
>>> > The reason I gave the do.call() version is that I had tried these simpler
>>> > versions and they didn't work.
>>> > 
>>> >        -thomas
>>> > 
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Sat Jul 23 18:00:01 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 23 Jul 2005 18:00:01 +0200 (CEST)
Subject: [R] PBSmapping and shapefiles
In-Reply-To: <DA978538-B003-43DC-9207-660D9A2E1A9A@globetrotter.net>
Message-ID: <Pine.LNX.4.44.0507231757100.15558-100000@reclus.nhh.no>

On Fri, 22 Jul 2005, Denis Chabot wrote:

> Hi,
> 
> I got no reply to this:
> Le 16-Jul-05 ?? 2:42 PM, Denis Chabot a ??crit :
> 
> > Hi,
> >
> > Is there a way, preferably with R, to read shapefiles and transform  
> > them in a format that I could then use with package PBSmapping?
> >
> > I have been able to read such files into R with maptools'  
> > read.shape and plot it with plot.Map, but I'd like to bring the  
> > data to PBSmapping and plot from there. I also looked at the  
> > package shapefile, but it does not seem to do what I want either.
> >
> > Sincerely,
> >
> > Denis Chabot
> >
> 
> but I managed to progress somewhat on my own. Although it does not  
> allow one to use shapefiles in PBSmapping, "maptools" at least makes  
> it possible to read such files. In some cases I can extract the  
> information I want from not-too-complex shapefiles. For instance, to  
> extract all the lines corresponding to 60-m isobath in a shapefile, I  
> was able to do:
> 
> library(maptools)
> test <- read.shape("bathy.shp")
> test2 <- Map2lines(test)
> bathy60 <- subset(test2, test$att.data$Z == 60)
> 
> I do not quite understand the structure of bathy60 (list of lists I  
> think)
> but at this point I resorted to printing bathy60 on the console and  
> imported that text into Excel for further cleaning, which is easy  
> enough. I'd like to complete the process within R to save time and to  
> circumvent Excel's limit of around 64000 lines. But I have a hard  
> time figuring out loops in R, coming from a background of  
> "observation based" programs such as SAS.
> 
> The output of bathy60 looks like this:
> 
> [[1]]
>             [,1]     [,2]
> [1,] -55.99805 51.68817
> [2,] -56.00222 51.68911
> [3,] -56.01694 51.68911
> [4,] -56.03781 51.68606
> [5,] -56.04639 51.68759
> [6,] -56.04637 51.69445
> [7,] -56.03777 51.70207
> [8,] -56.02301 51.70892
> [9,] -56.01317 51.71578
> [10,] -56.00330 51.73481
> [11,] -55.99805 51.73840
> attr(,"pstart")
> attr(,"pstart")$from
> [1] 1
> 
> attr(,"pstart")$to
> [1] 11
> 
> attr(,"nParts")
> [1] 1
> attr(,"shpID")
> [1] NA
> 
> [[2]]
>            [,1]     [,2]
> [1,] -57.76294 50.88770
> [2,] -57.76292 50.88693
> [3,] -57.76033 50.88163
> [4,] -57.75668 50.88091
> [5,] -57.75551 50.88169
> [6,] -57.75562 50.88550
> [7,] -57.75932 50.88775
> [8,] -57.76294 50.88770
> attr(,"pstart")
> attr(,"pstart")$from
> [1] 1
> 
> attr(,"pstart")$to
> [1] 8
> 
> attr(,"nParts")
> [1] 1
> attr(,"shpID")
> [1] NA
> 
> What I need to produce for PBSmapping is a file where each block of  
> coordinates shares one ID number, called PID, and a variable POS  
> indicating the number of each coordinate within a "shape". All other  
> lines must disappear. So the above would become:
> 
> PID X Y
> 1 1 -55.99805 51.68817
> 1 2 -56.00222 51.68911
> 1 3 -56.01694 51.68911
> 1 4 -56.03781 51.68606
> 1 5 -56.04639 51.68759
> 1 6 -56.04637 51.69445
> 1 7 -56.03777 51.70207
> 1 8 -56.02301 51.70892
> 1 9 -56.01317 51.71578
> 1 10 -56.00330 51.73481
> 1 11 -55.99805 51.73840
> 2 1 -57.76294 50.88770
> 2 2 -57.76292 50.88693
> 2 3 -57.76033 50.88163
> 2 4 -57.75668 50.88091
> 2 5 -57.75551 50.88169
> 2 6 -57.75562 50.88550
> 2 7 -57.75932 50.88775
> 2 8 -57.76294 50.88770
> 
> I don't know how to do this in R. My algorithm would involve looking  
> at the structure of a line, discarding it if not including  
> coordinates, and then creating PID and POS for lines with  
> coordinates, depending on the content of lines i and i-1. In R?
> 

The way to do it would be to manipulate the Map$Shapes object directly, 
making sure that the Map$att.data records stay associated with them. If 
you could send me (off-list) a small sample shapefile, I'll see if there 
are obvious solutions - they'll probably be through the "sp" package.

Roger Bivand

> Thanks in advance,
> 
> Denis Chabot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ggrothendieck at gmail.com  Sat Jul 23 18:09:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 23 Jul 2005 12:09:49 -0400
Subject: [R] Question about 'text' (add lm summary to a plot)
In-Reply-To: <Pine.LNX.4.21.0507231606040.4770-100000@mail.mrc-dunn.cam.ac.uk>
References: <1122064138.3330.75.camel@localhost.localdomain>
	<Pine.LNX.4.21.0507231606040.4770-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <971536df050723090949fabf8a@mail.gmail.com>

Here are a few tweaks.  We form a matrix and use round 
and format to automatically get the right number of 
decimals and widths right.  We have added extra digits
to show that they do not affect the result. Also the paste in the 
R^2 line was eliminated.  

nn <- matrix(c(-10.661, 1.961,                    # intercept
                 3.221, 0.131,                    # slope
                 0.971, NA), 3, 2, byrow = TRUE) # Rsquared
nnf <- apply(round(nn,2), 2, format)

plot(1:5)

L <- list("Intercept",
         "Slope",
         bquote(R^2),
         bquote(.(nnf[1,1]) %+-% .(nnf[1,2])),
         bquote(.(nnf[2,1]) %+-% .(nnf[2,2])),
         bquote(.(nnf[3,1])))

par(family = "mono")

legend("topleft", #inset=-1,
      legend = do.call("expression", L),
      bg='white',
      ncol = 2,
      pch=c('','','',':',':',':'),
      x.intersp = 0.4,
      title="Yay! Thank You!"
      )




On 7/23/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> On Fri, 22 Jul 2005, Marc Schwartz (via MN) wrote:
> 
> >Ok guys,
> >
> >So I played around with this a bit, going back to Dan's original
> >requirements and using Thomas' do.call() approach with legend(). Gabor's
> >approach using sapply() will also work here. I have the following:
> >
> ># Note the leading spaces here for alignment in the table
> ># This could be automated with formatC() or sprintf()
> >my.slope.1 <-  "  3.22"
> >my.slope.2 <-  "0.13"
> >my.inter.1 <-  "-10.66"
> >my.inter.2 <-  "1.96"
> >my.Rsqua <-    "  0.97"
> >
> >plot(1:5)
> >
> >L <- list("Intercept:",
> >          "Slope:",
> >          bquote(paste(R^2, ":")),
> >          bquote(.(my.inter.1) %+-% .(my.inter.2)),
> >          bquote(.(my.slope.1) %+-% .(my.slope.2)),
> >          bquote(.(my.Rsqua)))
> >
> >par(family = "mono")
> >
> >legend("topleft", legend = do.call("expression", L), ncol = 2)
> >
> >
> >
> >Note however, that while using the mono font helps with vertical
> >alignment of numbers, the +/- sign still comes out in the default font,
> >which is bold[er] than the text.
> >
> >If one uses the default font, which is variable spaced, it is
> >problematic to get the proper alignment for the numbers. I even tried
> >using phantom(), but that didn't quite get it, since the spacing is
> >variable, as opposed to LaTeX's mono numeric spacing with default fonts.
> >
> >Also, note that I am only using two columns, rather than three, since
> >trying to place the ":" as a middle column results in spacing that is
> >too wide, given that the text.width argument is a scalar and is set to
> >the maximum width of the character vectors.
> >
> >Note also that even with mono spaced fonts, the exponent in R^2 is still
> >horizontally smaller than the other characters. Thus, spacing on that
> >line may also be affected depending upon what else one might attempt.
> >
> >Not sure where else to go from here.
> 
> Ahhhhhh... So lovely! Thank you all so much!
> 
> I made a couple of tweeks to improve the overall appearance, using
> "x.intersp = 0.1" tightens up the overall appearance, and using
> "pch=c('','','',':',':',':')" adds the (aligned!) colons.
> 
> Here is the beauty...
> 
> 
> 
> my.slope.1 <-  "   3.22"
> my.slope.2 <-  "0.13"
> my.inter.1 <-  " -10.66"
> my.inter.2 <-  "1.96"
> my.Rsqua <-    "   0.97"
> 
> plot(1:5)
> 
> L <- list("Intercept",
>          "Slope    ",
>          bquote(paste(R^2)),
>          bquote(.(my.inter.1) %+-% .(my.inter.2)),
>          bquote(.(my.slope.1) %+-% .(my.slope.2)),
>          bquote(.(my.Rsqua)))
> 
> par(family = "mono")
> 
> legend("topleft", #inset=-1,
>       legend = do.call("expression", L),
>       bg='white',
>       ncol = 2,
>       pch=c('','','',':',':',':'),
>       x.intersp = 0.1,
>       title="Yay! Thank You!"
>       )
> 
> 
> However (the final gripe ;) it seems 'inset=' dosn't work. Setting this to
> anything (including the default) seems to surpress the legend without
> error. But hey!
> 
> Thanks again,
> 
> 
> 
> 
> 
> 
> 
> >
> >HTH,
> >
> >Marc Schwartz
> >
> >On Fri, 2005-07-22 at 14:01 -0400, Gabor Grothendieck wrote:
> >> You are right.   One would have to use do.call as you did
> >> or the sapply method of one of my previous posts:
> >>
> >> a <- 7
> >> plot(1)
> >> L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
> >> legend("topleft",legend=sapply(L, as.expression))
> >>
> >>
> >> On 7/22/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> >> > On Fri, 22 Jul 2005, Gabor Grothendieck wrote:
> >> > >
> >> > > I think legend accepts a list argument directly so that could be
> >> > > simplified to just:
> >> > >
> >> > >  a<-7
> >> > >  plot(1)
> >> > >  L <- list(bquote(alpha==.(a)),bquote(alpha^2+1==.(a^2+1)))
> >> > >  legend("topleft",legend=L)
> >> >
> >> > Except that it wouldn't then work: the mathematical stuff comes out as
> >> > text.
> >> >
> >> > > The same comment seems to apply to my prior suggestion about
> >> > > as.expression(bquote(...)), namely that one can just write the
> >> > > following as text also supports a list argument:
> >> >
> >> > And this doesn't work either: you end up with %+-% rather than the
> >> > plus-or-minus symbol.
> >> >
> >> > The reason I gave the do.call() version is that I had tried these simpler
> >> > versions and they didn't work.
> >> >
> >> >        -thomas
> >> >
> >
> 
>



From fisher at plessthan.com  Sat Jul 23 18:29:49 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Sat, 23 Jul 2005 09:29:49 -0700
Subject: [R] "%03d" in the pdf command
In-Reply-To: <mailman.9.1122112801.27994.r-help@stat.math.ethz.ch>
References: <mailman.9.1122112801.27994.r-help@stat.math.ethz.ch>
Message-ID: <83B96550-B951-4416-B6C8-B67EAE48B75C@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050723/a2650b5f/attachment.pl

From antonio.fabio at gmail.com  Sat Jul 23 17:01:48 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Sat, 23 Jul 2005 17:01:48 +0200
Subject: [R] [R-pkgs] tseriesChaos ver. 0.1
Message-ID: <1122130908.8703.6.camel@localhost.localdomain>

Dear all,

I have uploaded to CRAN a new package: tseriesChaos.
This is an early version (0.1) with basic tools for the explorative
analysis of nonlinear time series motivated by chaos theory. Until now,
the package is largely inspired by the TISEAN project (by Rainer Hegger,
Holger Kantz and Thomas Schreiber:
http://www.mpipks-dresden.mpg.de/~tisean/ ).

This version includes:
- Method of false nearest neighbours for the choice of the embedding
dimension.
- Tools for the estimation of the correlation dimension.
- Kantz algorithm for the estimation of the largest Lyapunov exponent.
- (naif) mutual information index estimation.
- Space-time separation plot.
- Recurrence plot.
- Simulation of noise-free continuous dynamic systems.

At the present time, algorithms are not too much optimized for speed, we
are working on optimizing the codes.
Future versions will (hopefully) include also code inspired from the
book of Chan and Tong (2001) (Chaos: A Statistical perspective) by
Springer as well as other contributions.
Checking was mainly done by comparing results with those available in
literature.
Any kind of feedback/help would be greatly appreciated.

Antonio, Fabio Di Narzo.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Sat Jul 23 19:26:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Jul 2005 18:26:36 +0100 (BST)
Subject: [R] "%03d" in the pdf command
In-Reply-To: <83B96550-B951-4416-B6C8-B67EAE48B75C@plessthan.com>
References: <mailman.9.1122112801.27994.r-help@stat.math.ethz.ch>
	<83B96550-B951-4416-B6C8-B67EAE48B75C@plessthan.com>
Message-ID: <Pine.LNX.4.61.0507231823200.8256@gannet.stats>

On Sat, 23 Jul 2005, Dennis Fisher wrote:

> The pdf man page contains the following text:
>
>      pdf(file = ifelse(onefile, "Rplots.pdf", "Rplot%03d.pdf"),
>          width = 6, height = 6, onefile = TRUE, family = "Helvetica",
>          title = "R Graphics Output", fonts = NULL, version = "1.1",
>          paper, encoding, bg, fg, pointsize)
>
> I am creating multi-page graphics in which each page is saved as a
> separate file.  If I invoke onefile=T and I include "%03d" in the
> filename, each page is saved as a separate file.  However, I don't
> understand the "%03d".  Can someone explain?  Am I calling a register
> that contains the page number?

See ?sprintf, or man printf on your system.
It formats the page number in that C format.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul 23 23:23:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 23 Jul 2005 22:23:08 +0100 (BST)
Subject: [R] "%03d" in the pdf command
In-Reply-To: <Pine.LNX.4.61.0507231823200.8256@gannet.stats>
Message-ID: <XFMail.050723222308.Ted.Harding@nessie.mcc.ac.uk>

On 23-Jul-05 Prof Brian Ripley wrote:
> On Sat, 23 Jul 2005, Dennis Fisher wrote:
> 
>> The pdf man page contains the following text:
>>
>>      pdf(file = ifelse(onefile, "Rplots.pdf", "Rplot%03d.pdf"),
>>          width = 6, height = 6, onefile = TRUE, family = "Helvetica",
>>          title = "R Graphics Output", fonts = NULL, version = "1.1",
>>          paper, encoding, bg, fg, pointsize)
>>
>> I am creating multi-page graphics in which each page is saved as a
>> separate file.  If I invoke onefile=T and I include "%03d" in the
>> filename, each page is saved as a separate file.  However, I don't
>> understand the "%03d".  Can someone explain?  Am I calling a register
>> that contains the page number?
> 
> See ?sprintf, or man printf on your system.
> It formats the page number in that C format.

If it's still obscure, the effect of the format string "Rplot%03d.pdf"
is that a string is generated consisting of the characters

  Rplot

followed by

  an integer of width 3 digits (padded to the left with zeros
  if required to make the width equal to 3),

followed by the characters

  d.pdf

where the value to be inserted in place of "%3d" will be derived
from somewhere -- not specified in the command itself but deducible
from "?pdf": If onefile=TRUE then you get all figures in a single
file whose name is "Rplots.pdf", otherwise you get them succesively
in separate files with names

  Rplot001.pdf
  Rplot002.pdf
  Rplot003.pdf
  Rplot004.pdf
  ...
  Rplot998.pdf
  Rplot999.pdf

after which you run out of road.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Jul-05                                       Time: 22:22:15
------------------------------ XFMail ------------------------------



From b83503104 at yahoo.com  Sun Jul 24 04:51:49 2005
From: b83503104 at yahoo.com (Bahoo)
Date: Sat, 23 Jul 2005 19:51:49 -0700 (PDT)
Subject: [R] calling R from C or C++
Message-ID: <20050724025149.63770.qmail@web31810.mail.mud.yahoo.com>

Hi,

I have C/C++ code from which I wish I could call R to
do something useful.

I saw a 2003 message by Thomas saying that "You can
compile R as a shared library, which allows you to
construct and evaluate R expressions from C."  Any
more information would be helpful.  I am looking for
some documentation that has a more step by step like
instructions.  Anyone who has experience, please point
me to some resources.

ps. There is a document on CRAN named "Writing R
extensions" which seems relevant, but it was too
difficult for me to understand.

Thanks in advance!



From b83503104 at yahoo.com  Sun Jul 24 05:18:19 2005
From: b83503104 at yahoo.com (Bahoo)
Date: Sat, 23 Jul 2005 20:18:19 -0700 (PDT)
Subject: [R] calling R from C or C++
In-Reply-To: <20050724025149.63770.qmail@web31810.mail.mud.yahoo.com>
Message-ID: <20050724031819.67623.qmail@web31810.mail.mud.yahoo.com>

> Hi,
> 
> I have C/C++ code from which I wish I could call R
> to
> do something useful.

By calling I mean linking with the R shared library,
instead of R BATCH.

In particular, I want to use regression functions such
as ridge and locfit.  

> 
> I saw a 2003 message by Thomas saying that "You can
> compile R as a shared library, which allows you to
> construct and evaluate R expressions from C."  Any
> more information would be helpful.  I am looking for
> some documentation that has a more step by step like
> instructions.  Anyone who has experience, please
> point
> me to some resources.
> 
> ps. There is a document on CRAN named "Writing R
> extensions" which seems relevant, but it was too
> difficult for me to understand.
>



From helprhelp at gmail.com  Sun Jul 24 07:46:08 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sun, 24 Jul 2005 00:46:08 -0500
Subject: [R] imbalanced data set
Message-ID: <cdf8178305072322469276384@mail.gmail.com>

Hi, 
I have a question of classification on imbalanced dataset. I am
wondering if there is a package which can solve this problem via
sampling approach, like one-sided selection.

A follow-up question is, how to select those 'representative' samples
and remove noise/borderlines and redundancy in order to increase
classification accuracy. Is there any work which has been implemented
in R or some GNU softwares?

Thanks,

weiwei



-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From wilks at dial.pipex.com  Sun Jul 24 17:46:14 2005
From: wilks at dial.pipex.com (John Wilkinson)
Date: Sun, 24 Jul 2005 16:46:14 +0100
Subject: [R] Question about 'text'
Message-ID: <AKEHKFBLKIAFEJCIOGOCIEHJCAAA.wilks@dial.pipex.com>

Dan,

Another tweak  !

If you want the 'legend' to look pretty you can resize it by adding,say,

'cex=0.6' into the legend code; try---

legend("topleft", #inset=-1,
      legend = do.call("expression", L),
      bg='white',
      ncol = 2,
      pch=c('','','',':',':',':'),
      x.intersp = 0.4,
      title="Yay! Thank You!",
	cex=0.6
      )


John



From edd at debian.org  Sun Jul 24 17:27:54 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Jul 2005 10:27:54 -0500
Subject: [R] calling R from C or C++
In-Reply-To: <20050724031819.67623.qmail@web31810.mail.mud.yahoo.com>
References: <20050724025149.63770.qmail@web31810.mail.mud.yahoo.com>
	<20050724031819.67623.qmail@web31810.mail.mud.yahoo.com>
Message-ID: <17123.45946.818908.81290@basebud.nulle.part>


On 23 July 2005 at 20:18, Bahoo wrote:
| > Hi,
| > 
| > I have C/C++ code from which I wish I could call R
| > to
| > do something useful.
| 
| By calling I mean linking with the R shared library,
| instead of R BATCH.
| 
| In particular, I want to use regression functions such
| as ridge and locfit.  
| 
| > 
| > I saw a 2003 message by Thomas saying that "You can
| > compile R as a shared library, which allows you to
| > construct and evaluate R expressions from C."  Any
| > more information would be helpful.  I am looking for
| > some documentation that has a more step by step like
| > instructions.  Anyone who has experience, please
| > point
| > me to some resources.
| > 
| > ps. There is a document on CRAN named "Writing R
| > extensions" which seems relevant, but it was too
| > difficult for me to understand.

Just get the Magic Wand from the Harry Potter books; that way you get the
functionality for free without having to read those pesky and difficult
documents we provide to explain how to do the other way -- when your Magic
Wand is broken.

Dirk, who still wants chocolate to grow on trees, preferably in his backyard

| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  (hint !)


-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From RRoa at fisheries.gov.fk  Sun Jul 24 16:27:38 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Sun, 24 Jul 2005 12:27:38 -0200
Subject: [R] Multiple series plot with different 'type' argument
Message-ID: <03DCBBA079F2324786E8715BE538968A068EC1@FIGMAIL-CLUS01.FIG.FK>

Hi:
I need to plot two time series in the same plot and
they cover the same time range and have the same 
frequency. With 
RSiteSearch("multiple series plot") 
i found this post by Gabor Grothendieck:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42281.html
Exactly what i need except for one detail. I want one series
to be made of points and the other by a line. When I simply
try:
xts <- ts(x$b,start=x$a[1]) 
yts <- ts(y$b,start=y$a[1]) 
ts.plot(xts,yts,type=c("p","l"))
or if i use the 'type' argument inside gpars=list()
i get an error of "invalid plot type".
Then if i try
ts.plot(xts,yts,type1="p",type2="l")
i get warnings about NAs introduced by coercion and the
plot still shows two lines.
Is there any other way i can get these two time series
in the same plot one with points and the other with lines?
Any help much appreciated.
Ruben



From edd at debian.org  Sun Jul 24 19:03:26 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Jul 2005 12:03:26 -0500
Subject: [R] Multiple series plot with different 'type' argument
In-Reply-To: <03DCBBA079F2324786E8715BE538968A068EC1@FIGMAIL-CLUS01.FIG.FK>
References: <03DCBBA079F2324786E8715BE538968A068EC1@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <17123.51678.779897.876542@basebud.nulle.part>


On 24 July 2005 at 12:27, Ruben Roa wrote:
| Hi:
| I need to plot two time series in the same plot and
| they cover the same time range and have the same 
| frequency. With 
| RSiteSearch("multiple series plot") 
| i found this post by Gabor Grothendieck:
| http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42281.html
| Exactly what i need except for one detail. I want one series
| to be made of points and the other by a line. When I simply
| try:
| xts <- ts(x$b,start=x$a[1]) 
| yts <- ts(y$b,start=y$a[1]) 
| ts.plot(xts,yts,type=c("p","l"))
| or if i use the 'type' argument inside gpars=list()
| i get an error of "invalid plot type".
| Then if i try
| ts.plot(xts,yts,type1="p",type2="l")
| i get warnings about NAs introduced by coercion and the
| plot still shows two lines.
| Is there any other way i can get these two time series
| in the same plot one with points and the other with lines?
| Any help much appreciated.

Yes, you can, but it requires a little bit of tinkering. Here is a complete
example with two random series. I prefer zoo as a container over ts(). You
can easily change the plotting styles to get points instead of lines etc pp.


library(zoo)

## make up an date index
index <- Sys.Date() + seq(-99,0,by=1)

## init random number generato
set.seed(42)
## and create two random series, with the date index
X1 <- zoo(cumsum(rnorm(100)), index)
X2 <- zoo(cumsum(rnorm(100)), index)

## plot series, suppress axes, set limit to range of both series
oldpar <- par(mar=c(4,4,2,4))           # extra space on the right
plot(X1, col='blue', ylab="X1", type='l', xaxs="i", ylim=range(X, Y))
axis(2, col.axis='blue')                # y-axis for X1, overplotting
grid()                                  # prettier 

par(new=TRUE)                           # add to the plot
plot(X2, col='green', type='l', ylab="", axes=FALSE, xaxs="i", ylim=range(X, Y))
axis(4, col.axis='green')
## need mtext() to annotate 2nd y-axis as title() doesn't do it
mtext("X2", side=4, line=2) 


Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From ggrothendieck at gmail.com  Sun Jul 24 19:18:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 24 Jul 2005 13:18:12 -0400
Subject: [R] Multiple series plot with different 'type' argument
In-Reply-To: <03DCBBA079F2324786E8715BE538968A068EC1@FIGMAIL-CLUS01.FIG.FK>
References: <03DCBBA079F2324786E8715BE538968A068EC1@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <971536df05072410185153bcd8@mail.gmail.com>

plot.zoo can do that.

library(zoo)
set.seed(1)
xts <- ts(rnorm(20)); xz <- as.zoo(xts)
yts <- ts(rnorm(20)); yz <- as.zoo(yts)
plot(cbind(xz,yz), type = c("p","l"), plot.type = "single")


On 7/24/05, Ruben Roa <RRoa at fisheries.gov.fk> wrote:
> Hi:
> I need to plot two time series in the same plot and
> they cover the same time range and have the same
> frequency. With
> RSiteSearch("multiple series plot")
> i found this post by Gabor Grothendieck:
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42281.html
> Exactly what i need except for one detail. I want one series
> to be made of points and the other by a line. When I simply
> try:
> xts <- ts(x$b,start=x$a[1])
> yts <- ts(y$b,start=y$a[1])
> ts.plot(xts,yts,type=c("p","l"))
> or if i use the 'type' argument inside gpars=list()
> i get an error of "invalid plot type".
> Then if i try
> ts.plot(xts,yts,type1="p",type2="l")
> i get warnings about NAs introduced by coercion and the
> plot still shows two lines.
> Is there any other way i can get these two time series
> in the same plot one with points and the other with lines?
> Any help much appreciated.
> Ruben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmb at mrc-dunn.cam.ac.uk  Sun Jul 24 19:19:38 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 24 Jul 2005 18:19:38 +0100 (BST)
Subject: [R] Question about 'text'
In-Reply-To: <AKEHKFBLKIAFEJCIOGOCIEHJCAAA.wilks@dial.pipex.com>
Message-ID: <Pine.LNX.4.21.0507241817350.13924-100000@mail.mrc-dunn.cam.ac.uk>

On Sun, 24 Jul 2005, John Wilkinson wrote:

>Dan,
>
>Another tweak  !
>
>If you want the 'legend' to look pretty you can resize it by adding,say,
>
>'cex=0.6' into the legend code; try---
>
>legend("topleft", #inset=-1,
>      legend = do.call("expression", L),
>      bg='white',
>      ncol = 2,
>      pch=c('','','',':',':',':'),
>      x.intersp = 0.4,
>      title="Yay! Thank You!",
>	cex=0.6
>      )

Thats cool, but my biggest problem is using monospaced fonts in a
postscript output. It dosn't seem to work!

Also, anybody else have the inset= problem?

Cheers for the tweek! 

Dan.


>
>
>John
>



From edd at debian.org  Sun Jul 24 19:20:05 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Jul 2005 12:20:05 -0500
Subject: [R] Multiple series plot with different 'type' argument
In-Reply-To: <17123.51678.779897.876542@basebud.nulle.part>
References: <03DCBBA079F2324786E8715BE538968A068EC1@FIGMAIL-CLUS01.FIG.FK>
	<17123.51678.779897.876542@basebud.nulle.part>
Message-ID: <17123.52677.413071.644962@basebud.nulle.part>


On 24 July 2005 at 12:03, Dirk Eddelbuettel wrote:
| library(zoo)
| 
| ## make up an date index
| index <- Sys.Date() + seq(-99,0,by=1)
| 
| ## init random number generato
| set.seed(42)
| ## and create two random series, with the date index
| X1 <- zoo(cumsum(rnorm(100)), index)
| X2 <- zoo(cumsum(rnorm(100)), index)
| 
| ## plot series, suppress axes, set limit to range of both series
| oldpar <- par(mar=c(4,4,2,4))           # extra space on the right
| plot(X1, col='blue', ylab="X1", type='l', xaxs="i", ylim=range(X, Y))
| axis(2, col.axis='blue')                # y-axis for X1, overplotting
| grid()                                  # prettier 
| 
| par(new=TRUE)                           # add to the plot
| plot(X2, col='green', type='l', ylab="", axes=FALSE, xaxs="i", ylim=range(X, Y))
| axis(4, col.axis='green')
| ## need mtext() to annotate 2nd y-axis as title() doesn't do it
| mtext("X2", side=4, line=2) 

Oops, the range() calls needs to be range(X1,X2). Sorry!

Dirk
 
| Hope this helps, Dirk
| 
| -- 
| Statistics: The (futile) attempt to offer certainty about uncertainty.
|          -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From jordi_molins at hotmail.com  Sun Jul 24 19:39:09 2005
From: jordi_molins at hotmail.com (Jordi)
Date: Sun, 24 Jul 2005 19:39:09 +0200
Subject: [R] problem building R packages in windows xp
Message-ID: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050724/cda99401/attachment.pl

From ggrothendieck at gmail.com  Sun Jul 24 19:44:31 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 24 Jul 2005 13:44:31 -0400
Subject: [R] problem building R packages in windows xp
In-Reply-To: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
Message-ID: <971536df05072410447d61152a@mail.gmail.com>

hhc.exe is the Microsoft help compiler.  You have to download that
and then put hhc.exe somewhere in your path.  The Windows
console command

path

will give you the pathnames in your current path, any of which 
you could put it in.

On 7/24/05, Jordi <jordi_molins at hotmail.com> wrote:
> Dear R users,
> 
> 
> 
> I am having problems building R packages in Windows xp. I have followed the
> instructions from Peter E. Rossi in Documentation -> Other, except for the
> TeX version (fpTeX), since when I go to the recommended webpage, it is said
> that fpTeX has been discontinued. I have MikTeX in my computer, and I have
> followed the recommendations in
> http://www.murdoch-sutherland.com/Rtools/miktex.html. I have followed the so
> called Workaround 4.
> 
> 
> 
> After running R CMD Rd2txt and R CMD Rdconv without problems, I have a
> problem when I run R CMD build <package>. It is always in the .chm file:
> 
> 
> 
> >>> Building/Updating help pages for package 'test'
> 
> Formats: text html latex example chm
> 
> riChisq                         text html latex example chm
> 
> hhc: not found
> 
> cp: cannot stat 'c:/temp/RPackageExample-19Jul05/test/chm/test.chm': No such
> file or directory
> 
> 
> 
> I do not know if this error is due to the TeX version, or if I am doing any
> other error. Does anybody have some idea?
> 
> 
> 
> Thank you in advance
> 
> 
> 
> Jordi
> 
> 
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From david.whiting at ncl.ac.uk  Sun Jul 24 19:45:09 2005
From: david.whiting at ncl.ac.uk (David Whiting)
Date: Sun, 24 Jul 2005 18:45:09 +0100
Subject: [R] chocolate [was Re:  calling R from C or C++]
In-Reply-To: <17123.45946.818908.81290@basebud.nulle.part>
References: <20050724025149.63770.qmail@web31810.mail.mud.yahoo.com>	<20050724031819.67623.qmail@web31810.mail.mud.yahoo.com>
	<17123.45946.818908.81290@basebud.nulle.part>
Message-ID: <42E3D3A5.7020809@ncl.ac.uk>

Dirk Eddelbuettel wrote:
> On 23 July 2005 at 20:18, Bahoo wrote:

[...]

> 
> Dirk, who still wants chocolate to grow on trees, preferably in his backyard

Well, it does, sort of. To have it in your backyard you are going to
have to move nearer the equator though :)

You can get the source, Theobroma Cacao, but you would have to compile
it yourself to make it edible. Check the Chocolate Ingestion and
Administration Manual. You will need a number of tools before you can do
so. Under debian this is easy with:

apt-get install chocolate-factory

Windows users seem to have more trouble getting the tools together and
some seem to believe that a clearer explanation of how to create
packaged chocolate in windows is required.

I think I should stop now and take my medication.

David

-- 
David Whiting
School of Clinical Medical Sciences, The Medical School
University of Newcastle upon Tyne, NE2 4HH, UK.



From ligges at statistik.uni-dortmund.de  Sun Jul 24 19:45:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 24 Jul 2005 19:45:43 +0200
Subject: [R] problem building R packages in windows xp
In-Reply-To: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
Message-ID: <42E3D3C7.4020109@statistik.uni-dortmund.de>

Jordi wrote:

> Dear R users,
> 
>  
> 
> I am having problems building R packages in Windows xp. I have followed the
> instructions from Peter E. Rossi in Documentation -> Other, except for the
> TeX version (fpTeX), since when I go to the recommended webpage, it is said
> that fpTeX has been discontinued. I have MikTeX in my computer, and I have
> followed the recommendations in
> http://www.murdoch-sutherland.com/Rtools/miktex.html. I have followed the so
> called Workaround 4.
> 
>  
> 
> After running R CMD Rd2txt and R CMD Rdconv without problems, I have a
> problem when I run R CMD build <package>. It is always in the .chm file: 
> 
>  
> 
> 
>>>>Building/Updating help pages for package 'test'
> 
> 
> Formats: text html latex example chm
> 
> riChisq                         text html latex example chm
> 
> hhc: not found


hhc is the Mircosoft html help compiler.
Do you have it installed?
Do you have it in your path?
Do you have modified MkRules accordingly?

Uwe Ligges


> cp: cannot stat 'c:/temp/RPackageExample-19Jul05/test/chm/test.chm': No such
> file or directory
> 
>  
> 
> I do not know if this error is due to the TeX version, or if I am doing any
> other error. Does anybody have some idea?
> 
>  
> 
> Thank you in advance
> 
>  
> 
> Jordi
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From david.meyer at wu-wien.ac.at  Sun Jul 24 19:57:37 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Sun, 24 Jul 2005 19:57:37 +0200
Subject: [R] setting weights for such a two-class problem in nnet and svm
Message-ID: <20050724195737.6a0c67e8.david.meyer@wu-wien.ac.at>


Dear Baoqiang,

there is an example on the svm Help page on the use of 'class.weights'. 

HTH
David


----------------

I have such a two-class problem, one class is very large(~98% of total),
and the other is just 2%. According to manual of nnet, I need setup
"weights", so I intend to set 1 for class one, 49 for class 2. How do I
do that? Just weights=49? 
Meanwhile I'd like to try svm(e1071), again, how do I setup
"class.weights"? Thanks.

-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From edd at debian.org  Sun Jul 24 20:00:44 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Jul 2005 13:00:44 -0500
Subject: [R] problem building R packages in windows xp
In-Reply-To: <971536df05072410447d61152a@mail.gmail.com>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
	<971536df05072410447d61152a@mail.gmail.com>
Message-ID: <17123.55116.567324.364365@basebud.nulle.part>


On 24 July 2005 at 13:44, Gabor Grothendieck wrote:
| hhc.exe is the Microsoft help compiler.  You have to download that
| and then put hhc.exe somewhere in your path.  The Windows
| console command
| 
| path
| 
| will give you the pathnames in your current path, any of which 
| you could put it in.

A cheap alternative is to create an empty batch file hhc.bat. I used that for
a while when I couldn't be bothered to download hhc.exe. However, if you
actually use compiled html (I don't, from ESS/XEmacs), you may want to get
hhc.exe.

If there ever was a vote, mine would be for making hhc.exe optional rather
than required.

Regards, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From jordi_molins at hotmail.com  Sun Jul 24 20:11:57 2005
From: jordi_molins at hotmail.com (Jordi)
Date: Sun, 24 Jul 2005 20:11:57 +0200
Subject: [R] problem building R packages in windows xp
In-Reply-To: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
Message-ID: <BAY104-DAV68C6E9600D6A2FC3C1A01F0CB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050724/49415bc3/attachment.pl

From ligges at statistik.uni-dortmund.de  Sun Jul 24 20:14:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 24 Jul 2005 20:14:33 +0200
Subject: [R] problem building R packages in windows xp
In-Reply-To: <17123.55116.567324.364365@basebud.nulle.part>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>	<971536df05072410447d61152a@mail.gmail.com>
	<17123.55116.567324.364365@basebud.nulle.part>
Message-ID: <42E3DA89.6030907@statistik.uni-dortmund.de>

Dirk Eddelbuettel wrote:

> On 24 July 2005 at 13:44, Gabor Grothendieck wrote:
> | hhc.exe is the Microsoft help compiler.  You have to download that
> | and then put hhc.exe somewhere in your path.  The Windows
> | console command
> | 
> | path
> | 
> | will give you the pathnames in your current path, any of which 
> | you could put it in.
> 
> A cheap alternative is to create an empty batch file hhc.bat. I used that for
> a while when I couldn't be bothered to download hhc.exe. However, if you
> actually use compiled html (I don't, from ESS/XEmacs), you may want to get
> hhc.exe.
> 
> If there ever was a vote, mine would be for making hhc.exe optional rather
> than required.

You can simply avoid building chm files by saying:

    R CMD INSTALL --docs="normal" MyPackage

Best,
Uwe

> Regards, Dirk
>



From MSchwartz at mn.rr.com  Sun Jul 24 20:21:43 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 24 Jul 2005 13:21:43 -0500
Subject: [R] Question about 'text'
In-Reply-To: <Pine.LNX.4.21.0507241817350.13924-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0507241817350.13924-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1122229303.3268.25.camel@localhost.localdomain>

On Sun, 2005-07-24 at 18:19 +0100, Dan Bolser wrote:
> On Sun, 24 Jul 2005, John Wilkinson wrote:
> 
> >Dan,
> >
> >Another tweak  !
> >
> >If you want the 'legend' to look pretty you can resize it by adding,say,
> >
> >'cex=0.6' into the legend code; try---
> >
> >legend("topleft", #inset=-1,
> >      legend = do.call("expression", L),
> >      bg='white',
> >      ncol = 2,
> >      pch=c('','','',':',':',':'),
> >      x.intersp = 0.4,
> >      title="Yay! Thank You!",
> >	cex=0.6
> >      )
> 
> Thats cool, but my biggest problem is using monospaced fonts in a
> postscript output. It dosn't seem to work!
> 
> Also, anybody else have the inset= problem?
> 
> Cheers for the tweek! 
> 
> Dan.

Dan,

Try this:

my.slope.1 <-  "   3.22"
my.slope.2 <-  "0.13"
my.inter.1 <-  " -10.66"
my.inter.2 <-  "1.96"
my.Rsqua <-    "   0.97"

# Include Courier font in the PS device
postscript(file = "LM.ps", font = "Courier")

plot(1:5)

L <- list("Intercept",
          "Slope    ",
          bquote(paste(R^2)),
          bquote(.(my.inter.1) %+-% .(my.inter.2)),
          bquote(.(my.slope.1) %+-% .(my.slope.2)),
          bquote(.(my.Rsqua)))


# Set font to Courier for mono spacing
par(family = "Courier")

legend("topleft", inset = .10,
      legend = do.call("expression", L),
      bg = 'white',
      ncol = 2,
      pch = rep(c('', ':'), each = 3),
      x.intersp = 0.1,
      title = "Yay! Thank You!")

dev.off()


Note that the 'inset' argument is to be a fraction of the plot region in
the range of 0 - 1. The above setting moves the legend to the right and
down from the upper left hand corner by 10% of the plot region
width/height.

One more tweak in the legend() call. Use rep() for the pch characters.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Sun Jul 24 20:33:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jul 2005 20:33:43 +0200
Subject: [R] problem building R packages in windows xp
In-Reply-To: <17123.55116.567324.364365@basebud.nulle.part>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
	<971536df05072410447d61152a@mail.gmail.com>
	<17123.55116.567324.364365@basebud.nulle.part>
Message-ID: <x2fyu4hx4o.fsf@turmalin.kubism.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> If there ever was a vote, mine would be for making hhc.exe optional rather
> than required.

You might have a point there. It's a proprietary format, requiring
proprietary build tools, and, as I remember it, mostly motivated by
deficiencies of a proprietary file system (the minimum file size on
VFAT combined with a bazillion little HTML files). However, those who
actually understand the Windows build probably knows better...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sun Jul 24 20:38:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 24 Jul 2005 19:38:35 +0100 (BST)
Subject: [R] problem building R packages in windows xp
In-Reply-To: <42E3DA89.6030907@statistik.uni-dortmund.de>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
	<971536df05072410447d61152a@mail.gmail.com>
	<17123.55116.567324.364365@basebud.nulle.part>
	<42E3DA89.6030907@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0507241935290.29718@gannet.stats>

On Sun, 24 Jul 2005, Uwe Ligges wrote:

> Dirk Eddelbuettel wrote:
>
>> On 24 July 2005 at 13:44, Gabor Grothendieck wrote:
>> | hhc.exe is the Microsoft help compiler.  You have to download that
>> | and then put hhc.exe somewhere in your path.  The Windows
>> | console command
>> |
>> | path
>> |
>> | will give you the pathnames in your current path, any of which
>> | you could put it in.
>>
>> A cheap alternative is to create an empty batch file hhc.bat. I used that for
>> a while when I couldn't be bothered to download hhc.exe. However, if you
>> actually use compiled html (I don't, from ESS/XEmacs), you may want to get
>> hhc.exe.
>>
>> If there ever was a vote, mine would be for making hhc.exe optional rather
>> than required.

It is already optional, and documented as such as Uwe points out:

> You can simply avoid building chm files by saying:
>
>    R CMD INSTALL --docs="normal" MyPackage

The reason it is on by default is that is the best option for building 
packages for distribution.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Jul 24 20:34:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 24 Jul 2005 19:34:50 +0100 (BST)
Subject: [R] problem building R packages in windows xp
In-Reply-To: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0507241931490.29718@gannet.stats>

On Sun, 24 Jul 2005, Jordi wrote:

>
> I am having problems building R packages in Windows xp. I have followed the
> instructions from Peter E. Rossi in Documentation -> Other, except for the

Better to follow the accurate official documentation.

> TeX version (fpTeX), since when I go to the recommended webpage, it is said
> that fpTeX has been discontinued.

Only development has been discontinued.  fptex is still available at e.g.

http://www.ctan.org/tex-archive/systems/win32/fptex/

and other CTAN nodes.

> 	[[alternative HTML version deleted]]
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do as it asks, including not sending HTML mail.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From blindglobe at gmail.com  Sun Jul 24 20:48:31 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Sun, 24 Jul 2005 20:48:31 +0200
Subject: [R] chocolate [was Re: calling R from C or C++]
In-Reply-To: <42E3D3A5.7020809@ncl.ac.uk>
References: <20050724025149.63770.qmail@web31810.mail.mud.yahoo.com>
	<20050724031819.67623.qmail@web31810.mail.mud.yahoo.com>
	<17123.45946.818908.81290@basebud.nulle.part>
	<42E3D3A5.7020809@ncl.ac.uk>
Message-ID: <1abe3fa9050724114826a5177b@mail.gmail.com>

On 7/24/05, David Whiting <david.whiting at ncl.ac.uk> wrote:
> Dirk Eddelbuettel wrote:
> > On 23 July 2005 at 20:18, Bahoo wrote:
> 
> [...]
> 
> >
> > Dirk, who still wants chocolate to grow on trees, preferably in his backyard
> 
> Well, it does, sort of. To have it in your backyard you are going to
> have to move nearer the equator though :)

I'll keep it in the grocery store.  Nothing like a whole aisle or 2 of
quality chocolate, instead of having to make it yourself.

best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From edd at debian.org  Sun Jul 24 20:49:08 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Jul 2005 13:49:08 -0500
Subject: [R] problem building R packages in windows xp
In-Reply-To: <Pine.LNX.4.61.0507241935290.29718@gannet.stats>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
	<971536df05072410447d61152a@mail.gmail.com>
	<17123.55116.567324.364365@basebud.nulle.part>
	<42E3DA89.6030907@statistik.uni-dortmund.de>
	<Pine.LNX.4.61.0507241935290.29718@gannet.stats>
Message-ID: <17123.58020.127814.524029@basebud.nulle.part>


On 24 July 2005 at 19:38, Prof Brian Ripley wrote:
| It is already optional, and documented as such as Uwe points out:

Yup, that one I evidently didn't know ... maybe the help on --docs needs to
be more explicit. Dunno. Or the section in the 'R Extensions' manual could
mention it too as it doesn't seem to:

edd at chibud:~/src/debian/R/R-2.1.1> grep 'docs="normal"' doc/manual/*
edd at chibud:~/src/debian/R/R-2.1.1>

| > You can simply avoid building chm files by saying:
| >
| >    R CMD INSTALL --docs="normal" MyPackage
| 
| The reason it is on by default is that is the best option for building 
| packages for distribution.

Hm. I find that a tad backwards as building packages for distribution is (or
should) be done by scripts. And those are perfectly capable of picking a
non-default value. Somehow I doubt Uwe builds the almost 600 Windows binaries
by manual invocations ...

Regards, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From ggrothendieck at gmail.com  Sun Jul 24 21:12:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 24 Jul 2005 15:12:40 -0400
Subject: [R] problem building R packages in windows xp
In-Reply-To: <Pine.LNX.4.61.0507241931490.29718@gannet.stats>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>
	<Pine.LNX.4.61.0507241931490.29718@gannet.stats>
Message-ID: <971536df050724121214abab6e@mail.gmail.com>

On 7/24/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sun, 24 Jul 2005, Jordi wrote:
> 
> >
> > I am having problems building R packages in Windows xp. I have followed the
> > instructions from Peter E. Rossi in Documentation -> Other, except for the
> 
> Better to follow the accurate official documentation.
> 
> > TeX version (fpTeX), since when I go to the recommended webpage, it is said
> > that fpTeX has been discontinued.
> 
> Only development has been discontinued.  fptex is still available at e.g.
> 
> http://www.ctan.org/tex-archive/systems/win32/fptex/
> 
> and other CTAN nodes.

You can't build vignettes with fptex (maybe there
are workarounds) so I think MiKTeX should be the recommended 
option in any case.  I have tried both and deleted fptex off my
drive some time ago once I realized that its problems were even
worse than those of MiKTeX.

> 
> >       [[alternative HTML version deleted]]
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> PLEASE do as it asks, including not sending HTML mail.
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Sun Jul 24 22:09:43 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 24 Jul 2005 16:09:43 -0400
Subject: [R] problem building R packages in windows xp
In-Reply-To: <17123.58020.127814.524029@basebud.nulle.part>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>	<971536df05072410447d61152a@mail.gmail.com>	<17123.55116.567324.364365@basebud.nulle.part>	<42E3DA89.6030907@statistik.uni-dortmund.de>	<Pine.LNX.4.61.0507241935290.29718@gannet.stats>
	<17123.58020.127814.524029@basebud.nulle.part>
Message-ID: <42E3F587.4070102@stats.uwo.ca>

Dirk Eddelbuettel wrote:
> On 24 July 2005 at 19:38, Prof Brian Ripley wrote:
> | It is already optional, and documented as such as Uwe points out:
> 
> Yup, that one I evidently didn't know ... maybe the help on --docs needs to
> be more explicit. Dunno. Or the section in the 'R Extensions' manual could
> mention it too as it doesn't seem to:

You were looking in the wrong place.  This is a problem in setting up on 
Windows, so you should be looking in the R-admin manual.  Quoting it:

"To skip building compiled html help, set WINHELP=NO in ?MkRules?. In 
this case the Help Workshop will not be needed."

You can also do it on a case-by-case basis, as Uwe says, but I think 
editing MkRules makes more sense if you don't want to install HHC. 
Uwe's option is documented in ?INSTALL.

Duncan Murdoch

> edd at chibud:~/src/debian/R/R-2.1.1> grep 'docs="normal"' doc/manual/*
> edd at chibud:~/src/debian/R/R-2.1.1>
> 
> | > You can simply avoid building chm files by saying:
> | >
> | >    R CMD INSTALL --docs="normal" MyPackage
> | 
> | The reason it is on by default is that is the best option for building 
> | packages for distribution.
> 
> Hm. I find that a tad backwards as building packages for distribution is (or
> should) be done by scripts. And those are perfectly capable of picking a
> non-default value. Somehow I doubt Uwe builds the almost 600 Windows binaries
> by manual invocations ...
> 
> Regards, Dirk
>



From f.calboli at imperial.ac.uk  Sun Jul 24 22:29:34 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Sun, 24 Jul 2005 21:29:34 +0100
Subject: [R] indexing a vector starting from 0
Message-ID: <56CE6028-6A64-4467-8873-5D3447B6EE2C@imperial.ac.uk>

Hi All,

I would like to ask if it possible to start indexing a vector from 0:

x = 1:10

y = c(0,0,3,4,5,6,0,8,9,10)

I need to use y as an index to extract the values of x, BUT I cannot  
cull/transform the 0s. What I would like is to start counting the  
elements of x 0:9 rather than 1:10. Would this be at all possible?

Regards,

Federico Calboli
--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From sdavis2 at mail.nih.gov  Sun Jul 24 22:38:20 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sun, 24 Jul 2005 16:38:20 -0400
Subject: [R] indexing a vector starting from 0
In-Reply-To: <56CE6028-6A64-4467-8873-5D3447B6EE2C@imperial.ac.uk>
References: <56CE6028-6A64-4467-8873-5D3447B6EE2C@imperial.ac.uk>
Message-ID: <cdb9922dde8e5779b0f215d94f9ede2e@mail.nih.gov>


On Jul 24, 2005, at 4:29 PM, Federico Calboli wrote:

> Hi All,
>
> I would like to ask if it possible to start indexing a vector from 0:
>
> x = 1:10
>
> y = c(0,0,3,4,5,6,0,8,9,10)
>
> I need to use y as an index to extract the values of x, BUT I cannot
> cull/transform the 0s. What I would like is to start counting the
> elements of x 0:9 rather than 1:10. Would this be at all possible?

Is there a reason why you can't add 1 to the x indices, like:

x[y+1]

Otherwise, you could use a list.

Sean



From p.dalgaard at biostat.ku.dk  Sun Jul 24 22:50:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jul 2005 22:50:22 +0200
Subject: [R] indexing a vector starting from 0
In-Reply-To: <56CE6028-6A64-4467-8873-5D3447B6EE2C@imperial.ac.uk>
References: <56CE6028-6A64-4467-8873-5D3447B6EE2C@imperial.ac.uk>
Message-ID: <x27jfghqsx.fsf@turmalin.kubism.ku.dk>

Federico Calboli <f.calboli at imperial.ac.uk> writes:

> Hi All,
> 
> I would like to ask if it possible to start indexing a vector from 0:
> 
> x = 1:10
> 
> y = c(0,0,3,4,5,6,0,8,9,10)
> 
> I need to use y as an index to extract the values of x, BUT I cannot  
> cull/transform the 0s. What I would like is to start counting the  
> elements of x 0:9 rather than 1:10. Would this be at all possible?

There's a 0array package on CRAN, but why not just add 1 to the index?

> x[y+1]
 [1]  1  1  4  5  6  7  1  9 10 NA

In general, 0-based indexing is possible, but messes with other things
in R, most obviously negative indexes, but also constructions that use
match(...., nomatch=0)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jh910 at juno.com  Sun Jul 24 23:02:32 2005
From: jh910 at juno.com (J. Hosking)
Date: Sun, 24 Jul 2005 17:02:32 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42E23E00.5010105@stats.uwo.ca>
References: <20050722223244.6472.qmail@web50911.mail.yahoo.com>
	<42E23E00.5010105@stats.uwo.ca>
Message-ID: <dc0vl8$48i$1@sea.gmane.org>

Duncan Murdoch wrote:

> Could you point out the specific bits that are missing from the R-Admin 
> manual (and perhaps supply them)?  It won't get better unless someone 
> improves it.

R-admin is fine.  The problem is in "Writing R extensions", which
would benefit from containing an explicit recipe for constructing an R
package, and in particular for constructing an R package under Windows
in both source and binary versions.  Several such recipes have been
posted to the internet or R-help.  The one that I have found to be the
most useful was posted to R-help by Gabor Grothendieck on 2 March 2005.
I am appending it below, with some trivial modifications of my own.
I think it would be very useful if this information were included in
the R-exts manual, perhaps at the end of the "Creating R packages"
section.

J. R. M. Hosking



Making a package under Windows
------------------------------

Make sure that:

- you have read:
    "Writing R Extensions" manual
    http://www.murdoch-sutherland.com/Rtools/

- you have downloaded and installed the tools from
   http://www.murdoch-sutherland.com/Rtools/tools.zip.

- you have installed LaTeX (fptex or MiKTeX), perl, the Microsoft help
   compiler, and (if the package contains C or Fortran source code) the
   MinGW compilers, as described at 
http://www.murdoch-sutherland.com/Rtools/.
     (MiKTeX requires some additional setup, described at
   http://www.murdoch-sutherland.com/Rtools/miktex.html).

- your path contains the tools, htmlhelp, and the bin directories for R,
   LaTeX, Perl, and (if the package contains C or Fortran source code
   to be compiled with MinGW) MinGW.  The tools directory should be the
   first item in the path.

Assuming that the R installation is in \Program Files\R\rw....

1. Assuming your source package tree is in \Rpkgs\mypackage
    then at a Windows command prompt:

         cd \Rpkgs
         Rcmd install mypackage

    which will install it to \Program Files\R\rw....\library\mypackage.
    Or if you want to install it to a separate library:

         cd \Rpkgs
         md library
         Rcmd install -l library mypackage

2. Now in R:

         library(mypackage)
         ... test it out ...

    or if you installed it to a separate library:

         library(mypackage, lib.loc = "/Rpkgs/library")

3. Once it seems reasonably OK, see whether it passes Rcmd check:

         cd \Rpkgs
         Rcmd check mypackage

    and fix it up until it does.

4. Now create versions for Unix and Windows that you can distribute:

         cd \Rpkgs
         Rcmd build mypackage
         Rcmd build mypackage --binary



From petr.mandys at matfyz.cz  Mon Jul 25 00:17:08 2005
From: petr.mandys at matfyz.cz (Petr Mandys)
Date: Mon, 25 Jul 2005 00:17:08 +0200 (CEST)
Subject: [R] Mean and variance of the right-censored data
In-Reply-To: <mailman.0.1122242790.18770.r-help@stat.math.ethz.ch>
References: <mailman.0.1122242790.18770.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.62.0507250011200.13317@artax.karlin.mff.cuni.cz>

Hi,

I need to get mean and variance of right censored data. How can I do that?

I have a vector of values (called a) and vector of booleans (whether value 
is censored) (called b). What to do with this? Sorry, I'm R beginner.

Thank you!

Pete



From p.murrell at auckland.ac.nz  Mon Jul 25 00:39:04 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 25 Jul 2005 10:39:04 +1200
Subject: [R] Lattice: how to make x axis to appear on only one
 non-bottom plot?
References: <1438101309.20050722094536@eimb.ru>
Message-ID: <42E41888.2030306@stat.auckland.ac.nz>

Hi


Wladimir Eremeev wrote:
> Dear r-help,
> 
>    Attached is the fragment of the lattice plot, produced with xyplot
>    function.
>    Data were drawn for all months of a year except December.
>    xyplot was called with parameters
>    layout=c(3,4)
>    scales=list(x=list(alternating=1),y=list(alternating=1))
> 
>    Because of the hole in the bottom right, caused by the absence of
>    the December, right column of plots has no x axis.
> 
>    Is it possible to produce it using xyplot?


Does the following do what you want?

xyplot(<whatever>)
trellis.focus("panel", 4, 2, clip.off=TRUE, highlight=FALSE)
panel.axis("bottom", check.overlap=TRUE, outside=TRUE)

(it would be easier to help if you could provide code that others can run)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From murdoch at stats.uwo.ca  Mon Jul 25 01:15:08 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 24 Jul 2005 19:15:08 -0400
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <dc0vl8$48i$1@sea.gmane.org>
References: <20050722223244.6472.qmail@web50911.mail.yahoo.com>	<42E23E00.5010105@stats.uwo.ca>
	<dc0vl8$48i$1@sea.gmane.org>
Message-ID: <42E420FC.2040804@stats.uwo.ca>

J. Hosking wrote:
> Duncan Murdoch wrote:
> 
> 
>>Could you point out the specific bits that are missing from the R-Admin 
>>manual (and perhaps supply them)?  It won't get better unless someone 
>>improves it.
> 
> 
> R-admin is fine.  The problem is in "Writing R extensions", which
> would benefit from containing an explicit recipe for constructing an R
> package, and in particular for constructing an R package under Windows
> in both source and binary versions. 

Thanks.  I'll see about putting something like this into R-ext.  (I'll 
probably not put the details about installing the tools there; it's bad 
to have installation instructions in more than one place.  But the idea 
of giving a sample install seems good.)  Not sure if this will happen 
before 2.2.0; I've got a number of higher priority things to get through 
first.  But if someone wants to volunteer to write it up in texinfo 
format, I'll be appreciative.

Duncan


  Several such recipes have been
> posted to the internet or R-help.  The one that I have found to be the
> most useful was posted to R-help by Gabor Grothendieck on 2 March 2005.
> I am appending it below, with some trivial modifications of my own.
> I think it would be very useful if this information were included in
> the R-exts manual, perhaps at the end of the "Creating R packages"
> section.
> 
> J. R. M. Hosking
> 
> 
> 
> Making a package under Windows
> ------------------------------
> 
> Make sure that:
> 
> - you have read:
>     "Writing R Extensions" manual
>     http://www.murdoch-sutherland.com/Rtools/
> 
> - you have downloaded and installed the tools from
>    http://www.murdoch-sutherland.com/Rtools/tools.zip.
> 
> - you have installed LaTeX (fptex or MiKTeX), perl, the Microsoft help
>    compiler, and (if the package contains C or Fortran source code) the
>    MinGW compilers, as described at 
> http://www.murdoch-sutherland.com/Rtools/.
>      (MiKTeX requires some additional setup, described at
>    http://www.murdoch-sutherland.com/Rtools/miktex.html).
> 
> - your path contains the tools, htmlhelp, and the bin directories for R,
>    LaTeX, Perl, and (if the package contains C or Fortran source code
>    to be compiled with MinGW) MinGW.  The tools directory should be the
>    first item in the path.
> 
> Assuming that the R installation is in \Program Files\R\rw....
> 
> 1. Assuming your source package tree is in \Rpkgs\mypackage
>     then at a Windows command prompt:
> 
>          cd \Rpkgs
>          Rcmd install mypackage
> 
>     which will install it to \Program Files\R\rw....\library\mypackage.
>     Or if you want to install it to a separate library:
> 
>          cd \Rpkgs
>          md library
>          Rcmd install -l library mypackage
> 
> 2. Now in R:
> 
>          library(mypackage)
>          ... test it out ...
> 
>     or if you installed it to a separate library:
> 
>          library(mypackage, lib.loc = "/Rpkgs/library")
> 
> 3. Once it seems reasonably OK, see whether it passes Rcmd check:
> 
>          cd \Rpkgs
>          Rcmd check mypackage
> 
>     and fix it up until it does.
> 
> 4. Now create versions for Unix and Windows that you can distribute:
> 
>          cd \Rpkgs
>          Rcmd build mypackage
>          Rcmd build mypackage --binary
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From nhawkins_seattle at yahoo.com  Mon Jul 25 02:35:40 2005
From: nhawkins_seattle at yahoo.com (Natalie Hawkins)
Date: Sun, 24 Jul 2005 17:35:40 -0700 (PDT)
Subject: [R] Fortran function name not in load table
Message-ID: <20050725003540.94299.qmail@web30003.mail.mud.yahoo.com>


Using R 2.0.1 on Windows XP, I am getting an error
msg:

Error in .Fortran("conic", nxy = nxy, npt = npt, CP =
cp, EP1 = ep1, EP2 = ep2,  : 
        
Fortran function name not in load table

I am wondering if there is a way to see what function
names are in the load table?  Maybe the function name
has been altered?

The first thing I do in my analysis script is to load
a DLL, conic.dll, which contains the fcn, conic:
dyn.load("c:\\natalie\\josh\\R\\conic.dll")

No error msgs are reported after this call. 

Calls to is.loaded() return FALSE:

> is.loaded(symbol.For("CONIC"))
[1] FALSE
> is.loaded(symbol.For("conic"))
[1] FALSE
 
I used a DLL Viewer (PE Explorer) to 'verify' that
"conic" is in conic.dll.  

Any suggestions?  

I did successfully use this code using R on Red Hat
Linux 4 yrs ago, in which case I created a .so.  

I would prefer to use Windows this time.

thanks,
Natalie



From turwa720 at student.otago.ac.nz  Mon Jul 25 03:03:06 2005
From: turwa720 at student.otago.ac.nz (Worik Turei stanton)
Date: Mon, 25 Jul 2005 13:03:06 +1200
Subject: [R] lda:  scaling to 'disctiminant function'
Message-ID: <1122253386.42e43a4a7e412@www.studentmail.otago.ac.nz>

Friends

Briefly...

In the documentation for lda in MASS it describes the value 'scaling' as
'a matrix which transforms observations into discriminint functions...'.

How?

Verbosely...

I have a matrix of data.  9 independent variables and describing
3-classes.   About 100 observations in total.  A 10x100 matrix of data.

I am trying to generate two discriminant functions and i think lda is the
R-function I want.  I have done this in SPSS using the code (is it 'S'
code?)...

"DISCRIMINANT  /GROUPS=group(1 3)  /VARIABLES=agr min man ps con ser fin
sps tc  /ANALYSIS ALL  /PRIORS  EQUAL  /STATISTICS=MEAN STDDEV UNIVF COEFF
RAW COV TABLE  /PLOT=COMBINED MAP  /CLASSIFY=NONMISSING POOLED "

In R I express this as...
library("MASS") # For lda
library("foreign")
EW <- read.spss("eurowork.sav")
Ind <- cbind(EW$AGR, EW$MIN, EW$MAN, EW$PS, EW$CON, EW$SER, EW$FIN,
EW$SPS, EW$TC)
Dep <- EW$GROUP
LDA <- lda(Dep ~ Ind, prior=c(1,1,1)/3)

So far so good.

I have....

> LDA$scaling
           LD1      LD2
Ind1 1.3415788 3.689959
Ind2 1.5118460 5.215123
Ind3 1.4480197 3.653298
Ind4 2.1898063 3.875867
Ind5 1.2055849 3.913750
Ind6 0.8257858 3.718032
Ind7 1.3481728 3.699259
Ind8 1.2364320 3.871438
Ind9 2.0652630 2.891655

What do I do with this to convert it to the coefficients for the
discrimination functions?

cheers
Worik



From ivo_welch-rstat8303 at mailblocks.com  Mon Jul 25 03:14:05 2005
From: ivo_welch-rstat8303 at mailblocks.com (ivo_welch-rstat8303@mailblocks.com)
Date: Sun, 24 Jul 2005 18:14:05 -0700
Subject: [R] sparse data frame (crsp?)
Message-ID: <200507250114.j6P1E7PO014769@hypatia.math.ethz.ch>


dear R wizards:  does R have the facilities to handle sparse data 
frames?  I am thinking of reading a data base like the daily CRSP data 
into R, observations being firms, columns being days, data being stock 
returns.  but this will only fit into my memory if I can convince R to 
not have to store missing observations, and to return NA upon read 
access to missing observations.  pointers appreciated.  sincerely,  /iaw



From murdoch at stats.uwo.ca  Mon Jul 25 04:40:09 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 24 Jul 2005 22:40:09 -0400
Subject: [R] Fortran function name not in load table
In-Reply-To: <20050725003540.94299.qmail@web30003.mail.mud.yahoo.com>
References: <20050725003540.94299.qmail@web30003.mail.mud.yahoo.com>
Message-ID: <42E45109.2090705@stats.uwo.ca>

Natalie Hawkins wrote:
> Using R 2.0.1 on Windows XP, I am getting an error
> msg:
> 
> Error in .Fortran("conic", nxy = nxy, npt = npt, CP =
> cp, EP1 = ep1, EP2 = ep2,  : 
>         
> Fortran function name not in load table
> 
> I am wondering if there is a way to see what function
> names are in the load table?  Maybe the function name
> has been altered?

You need to look at the DLL to see what name it is exporting. I believe 
R would be looking for "conic_".  If your Fortran compiler doesn't 
append underscores, you'll get this error.

You might want to look at this page

http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/index.html#badname

or this one

http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/fortran.html

for more help.

Duncan Murdoch



From holtlaura at gmail.com  Mon Jul 25 06:23:42 2005
From: holtlaura at gmail.com (Laura Holt)
Date: Sun, 24 Jul 2005 23:23:42 -0500
Subject: [R]  apply and arrays
Message-ID: <16b132510507242123c4c47@mail.gmail.com>

Hi R!

I have a 3 dimensional array, which is 21 x 3 x 3

I want to use apply to sum on each 21x3 matrix, which is fine.

Is there a way that I can do this in 1 step instead of a loop (3), please?

thanks,
Laura Holt
mailto: holtlaura at gmail.com



From 0034058 at fudan.edu.cn  Mon Jul 25 08:07:50 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 25 Jul 2005 14:07:50 +0800
Subject: [R] how to get the group mean deviation data ?
Message-ID: <0IK6003NE5X5TB@mail.fudan.edu.cn>

> n=10;t=3
> d<-cbind(id=rep(1:n,each=t),y=rnorm(n*t),x=rnorm(n*t),z=rnorm(n*t))
> head(d)
     id          y           x          z
[1,]  1 -2.1725379  0.07629954 -0.3985258
[2,]  1 -1.2383038 -2.49667038  0.6966127
[3,]  1 -1.2642401 -0.50613307  0.4895856
[4,]  2  0.2171246  0.86711864 -0.6660036
[5,]  2  2.2765760 -0.48547142 -1.4496664
[6,]  2  0.5985345 -1.06427035  2.1761071

first,i want to get the group mean of each variable,which i can use
> d<-data.frame(d)
> aggregate(d,list(d$id),mean)[,-1]
   id           y          x           z
1   1 -1.55836060 -0.9755013  0.26255754
2   2  1.03074502 -0.2275410  0.02014565
3   3  0.20700121 -0.7159450  1.35890176
4   4  0.17839650  1.2575891  0.04135165
5   5 -0.20012508  0.4310221  0.55458899
6   6 -0.13084185 -0.2953392  0.28229068
7   7  0.20737288 -0.8863761 -0.50793880
8   8  0.07512612 -0.6591304 -0.21656533
9   9  0.94727796 -0.6108891  0.13529884
10 10 -0.04434875  0.1332086 -0.88229808

then i want the  group mean deviation data,like
> head(sapply(d[,2:4],function(x) x-ave(x,d$id)))
              y          x          z
[1,] -0.6141773  1.0518008 -0.6610833
[2,]  0.3200568 -1.5211691  0.4340552
[3,]  0.2941205  0.4693682  0.2270281
[4,] -0.8136205  1.0946597 -0.6861493
[5,]  1.2458310 -0.2579304 -1.4698121
[6,] -0.4322105 -0.8367293  2.1559614

both above are what i want.though i can do it use the function  to do it.but if n id quite large,say n=1000 and t=3, it require too much time.so i want to know any more efficient way to do it?

myfun<-function(x,id)
 {
 x<-as.matrix(x)
 id<-as.factor(id)
 xm<- apply(x,2,function(y,z) tapply(y,z, mean), z=id)
 xdm<- x[] <- x-xm[id,]  
 re<-list(xm=xm, xdm=xdm)
 re
 }



From herodote at oreka.com  Mon Jul 25 08:32:55 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Mon, 25 Jul 2005 07:32:55 +0100
Subject: [R] =?iso-8859-1?q?calling_R_from_C_or_C++?=
Message-ID: <IK67IV$105E218E096BFD8069A6E04B848FE327@oreka.com>

hi all

You should look into the "tests" directory (download the source package extract it and you'll have the tests directory)

there are some exemples on how to embedded R into C.

I've got the same trouble, here nobody want to do that, and i don't know why...
---------- Initial Header -----------

>From      : r-help-bounces at stat.math.ethz.ch
To          : Bahoo <b83503104 at yahoo.com>
Cc          : r-help at stat.math.ethz.ch
Date      : Sun, 24 Jul 2005 10:27:54 -0500
Subject : Re: [R] calling R from C or C++


On 23 July 2005 at 20:18, Bahoo wrote:
| > Hi,
| > 
| > I have C/C++ code from which I wish I could call R
| > to
| > do something useful.
| 
| By calling I mean linking with the R shared library,
| instead of R BATCH.
| 
| In particular, I want to use regression functions such
| as ridge and locfit.  
| 
| > 
| > I saw a 2003 message by Thomas saying that "You can
| > compile R as a shared library, which allows you to
| > construct and evaluate R expressions from C."  Any
| > more information would be helpful.  I am looking for
| > some documentation that has a more step by step like
| > instructions.  Anyone who has experience, please
| > point
| > me to some resources.
| > 
| > ps. There is a document on CRAN named "Writing R
| > extensions" which seems relevant, but it was too
| > difficult for me to understand.

Just get the Magic Wand from the Harry Potter books; that way you get the
functionality for free without having to read those pesky and difficult
documents we provide to explain how to do the other way -- when your Magic
Wand is broken.

Dirk, who still wants chocolate to grow on trees, preferably in his backyard

| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  (hint !)


-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From ligges at statistik.uni-dortmund.de  Mon Jul 25 08:41:41 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Jul 2005 08:41:41 +0200
Subject: [R] problem building R packages in windows xp
In-Reply-To: <17123.58020.127814.524029@basebud.nulle.part>
References: <BAY104-DAV37DECD202C5A5C94D3565F0CB0@phx.gbl>	<971536df05072410447d61152a@mail.gmail.com>	<17123.55116.567324.364365@basebud.nulle.part>	<42E3DA89.6030907@statistik.uni-dortmund.de>	<Pine.LNX.4.61.0507241935290.29718@gannet.stats>
	<17123.58020.127814.524029@basebud.nulle.part>
Message-ID: <42E489A5.3080909@statistik.uni-dortmund.de>

Dirk Eddelbuettel wrote:
> On 24 July 2005 at 19:38, Prof Brian Ripley wrote:
> | It is already optional, and documented as such as Uwe points out:
> 
> Yup, that one I evidently didn't know ... maybe the help on --docs needs to
> be more explicit. Dunno. Or the section in the 'R Extensions' manual could
> mention it too as it doesn't seem to:
> 
> edd at chibud:~/src/debian/R/R-2.1.1> grep 'docs="normal"' doc/manual/*
> edd at chibud:~/src/debian/R/R-2.1.1>
> 
> | > You can simply avoid building chm files by saying:
> | >
> | >    R CMD INSTALL --docs="normal" MyPackage
> | 
> | The reason it is on by default is that is the best option for building 
> | packages for distribution.
> 
> Hm. I find that a tad backwards as building packages for distribution is (or
> should) be done by scripts. And those are perfectly capable of picking a
> non-default value. Somehow I doubt Uwe builds the almost 600 Windows binaries
> by manual invocations ...

Manually, hence no time to sleep left in a 32 hours day. ;-)

Seriously speaking, I think the compiler is small and easy to download, 
and at least I am using chm help as the default help system on all "my" 
department's machines. For example, I like the contents and index views 
on the left of the window.
Hence a good idea to stay with the default from my point of view. And it 
is easy to change it (Duncan pointed out an even more convinient way to 
disable it generally on your machine).

Best,
Uwe




> Regards, Dirk
>



From ligges at statistik.uni-dortmund.de  Mon Jul 25 08:47:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Jul 2005 08:47:23 +0200
Subject: [R] The steps of building library in R 2.1.1
In-Reply-To: <42E420FC.2040804@stats.uwo.ca>
References: <20050722223244.6472.qmail@web50911.mail.yahoo.com>	<42E23E00.5010105@stats.uwo.ca>	<dc0vl8$48i$1@sea.gmane.org>
	<42E420FC.2040804@stats.uwo.ca>
Message-ID: <42E48AFB.4090109@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> J. Hosking wrote:
> 
>>Duncan Murdoch wrote:
>>
>>
>>
>>>Could you point out the specific bits that are missing from the R-Admin 
>>>manual (and perhaps supply them)?  It won't get better unless someone 
>>>improves it.
>>
>>
>>R-admin is fine.  The problem is in "Writing R extensions", which
>>would benefit from containing an explicit recipe for constructing an R
>>package, and in particular for constructing an R package under Windows
>>in both source and binary versions. 
> 
> 
> Thanks.  I'll see about putting something like this into R-ext.  (I'll 
> probably not put the details about installing the tools there; it's bad 
> to have installation instructions in more than one place.  But the idea 
> of giving a sample install seems good.)  Not sure if this will happen 

How to install packages has been written down in an R Help Desk column 
with some examples on handling different libraries etc.

Uwe Ligges


> before 2.2.0; I've got a number of higher priority things to get through 
> first.  But if someone wants to volunteer to write it up in texinfo 
> format, I'll be appreciative.
> 
> Duncan
> 
> 
>   Several such recipes have been
> 
>>posted to the internet or R-help.  The one that I have found to be the
>>most useful was posted to R-help by Gabor Grothendieck on 2 March 2005.
>>I am appending it below, with some trivial modifications of my own.
>>I think it would be very useful if this information were included in
>>the R-exts manual, perhaps at the end of the "Creating R packages"
>>section.
>>
>>J. R. M. Hosking
>>
>>
>>
>>Making a package under Windows
>>------------------------------
>>
>>Make sure that:
>>
>>- you have read:
>>    "Writing R Extensions" manual
>>    http://www.murdoch-sutherland.com/Rtools/
>>
>>- you have downloaded and installed the tools from
>>   http://www.murdoch-sutherland.com/Rtools/tools.zip.
>>
>>- you have installed LaTeX (fptex or MiKTeX), perl, the Microsoft help
>>   compiler, and (if the package contains C or Fortran source code) the
>>   MinGW compilers, as described at 
>>http://www.murdoch-sutherland.com/Rtools/.
>>     (MiKTeX requires some additional setup, described at
>>   http://www.murdoch-sutherland.com/Rtools/miktex.html).
>>
>>- your path contains the tools, htmlhelp, and the bin directories for R,
>>   LaTeX, Perl, and (if the package contains C or Fortran source code
>>   to be compiled with MinGW) MinGW.  The tools directory should be the
>>   first item in the path.
>>
>>Assuming that the R installation is in \Program Files\R\rw....
>>
>>1. Assuming your source package tree is in \Rpkgs\mypackage
>>    then at a Windows command prompt:
>>
>>         cd \Rpkgs
>>         Rcmd install mypackage
>>
>>    which will install it to \Program Files\R\rw....\library\mypackage.
>>    Or if you want to install it to a separate library:
>>
>>         cd \Rpkgs
>>         md library
>>         Rcmd install -l library mypackage
>>
>>2. Now in R:
>>
>>         library(mypackage)
>>         ... test it out ...
>>
>>    or if you installed it to a separate library:
>>
>>         library(mypackage, lib.loc = "/Rpkgs/library")
>>
>>3. Once it seems reasonably OK, see whether it passes Rcmd check:
>>
>>         cd \Rpkgs
>>         Rcmd check mypackage
>>
>>    and fix it up until it does.
>>
>>4. Now create versions for Unix and Windows that you can distribute:
>>
>>         cd \Rpkgs
>>         Rcmd build mypackage
>>         Rcmd build mypackage --binary
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Jul 25 08:52:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Jul 2005 08:52:39 +0200
Subject: [R] apply and arrays
In-Reply-To: <16b132510507242123c4c47@mail.gmail.com>
References: <16b132510507242123c4c47@mail.gmail.com>
Message-ID: <42E48C37.7070304@statistik.uni-dortmund.de>

Laura Holt wrote:

> Hi R!
> 
> I have a 3 dimensional array, which is 21 x 3 x 3
> 
> I want to use apply to sum on each 21x3 matrix, which is fine.
> 
> Is there a way that I can do this in 1 step instead of a loop (3), please?

Don't know which direction you mean, I guess one of the following:

apply(x, c(1,2), sum)
apply(x, 3, sum)

Uwe Ligges


> thanks,
> Laura Holt
> mailto: holtlaura at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jul 25 08:57:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Jul 2005 07:57:47 +0100 (BST)
Subject: [R] how to get the group mean deviation data ?
In-Reply-To: <0IK6003NE5X5TB@mail.fudan.edu.cn>
References: <0IK6003NE5X5TB@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0507250734280.6821@gannet.stats>

> if n id quite large,say n=1000 and t=3, it require too much time.so i 
> want to know any more efficient way to do it?

Why is about 0.4 second (which is what it takes on my system) too long?

Given that you want to operate on 3000 cells, a second does not look 
unreasonable.

This is a toy problem, and it is unclear what the real problem is (if 
any).  Since you have the same number of replications for each cell 
(group-variable combination), I would use this as a n x 3 x t array (a 
simple call to dim and aperem).  Then rowMeans will find the group means, 
and you can just subtract those to get the deviations from the means, 
making use of recycling.

E.g.

D <- d[,-1]
dim(D) <- c(t,n,3)
D <- aperm(D, c(2,3,1))
gmeans <- rowMeans(D, dims=2)
d[,-1] - rep(gmeans, each=3)

That takes under 10ms for n=1000


On Mon, 25 Jul 2005, ronggui wrote:

>> n=10;t=3
>> d<-cbind(id=rep(1:n,each=t),y=rnorm(n*t),x=rnorm(n*t),z=rnorm(n*t))
>> head(d)
>     id          y           x          z
> [1,]  1 -2.1725379  0.07629954 -0.3985258
> [2,]  1 -1.2383038 -2.49667038  0.6966127
> [3,]  1 -1.2642401 -0.50613307  0.4895856
> [4,]  2  0.2171246  0.86711864 -0.6660036
> [5,]  2  2.2765760 -0.48547142 -1.4496664
> [6,]  2  0.5985345 -1.06427035  2.1761071
>
> first,i want to get the group mean of each variable,which i can use
>> d<-data.frame(d)
>> aggregate(d,list(d$id),mean)[,-1]
>   id           y          x           z
> 1   1 -1.55836060 -0.9755013  0.26255754
> 2   2  1.03074502 -0.2275410  0.02014565
> 3   3  0.20700121 -0.7159450  1.35890176
> 4   4  0.17839650  1.2575891  0.04135165
> 5   5 -0.20012508  0.4310221  0.55458899
> 6   6 -0.13084185 -0.2953392  0.28229068
> 7   7  0.20737288 -0.8863761 -0.50793880
> 8   8  0.07512612 -0.6591304 -0.21656533
> 9   9  0.94727796 -0.6108891  0.13529884
> 10 10 -0.04434875  0.1332086 -0.88229808
>
> then i want the  group mean deviation data,like
>> head(sapply(d[,2:4],function(x) x-ave(x,d$id)))
>              y          x          z
> [1,] -0.6141773  1.0518008 -0.6610833
> [2,]  0.3200568 -1.5211691  0.4340552
> [3,]  0.2941205  0.4693682  0.2270281
> [4,] -0.8136205  1.0946597 -0.6861493
> [5,]  1.2458310 -0.2579304 -1.4698121
> [6,] -0.4322105 -0.8367293  2.1559614
>
> both above are what i want.though i can do it use the function  to do it.but if n id quite large,say n=1000 and t=3, it require too much time.so i want to know any more efficient way to do it?
>
> myfun<-function(x,id)
> {
> x<-as.matrix(x)
> id<-as.factor(id)
> xm<- apply(x,2,function(y,z) tapply(y,z, mean), z=id)
> xdm<- x[] <- x-xm[id,]
> re<-list(xm=xm, xdm=xdm)
> re
> }
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 25 08:58:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Jul 2005 07:58:53 +0100 (BST)
Subject: [R] apply and arrays
In-Reply-To: <42E48C37.7070304@statistik.uni-dortmund.de>
References: <16b132510507242123c4c47@mail.gmail.com>
	<42E48C37.7070304@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0507250758020.6821@gannet.stats>

On Mon, 25 Jul 2005, Uwe Ligges wrote:

> Laura Holt wrote:
>
>> Hi R!
>>
>> I have a 3 dimensional array, which is 21 x 3 x 3
>>
>> I want to use apply to sum on each 21x3 matrix, which is fine.
>>
>> Is there a way that I can do this in 1 step instead of a loop (3), please?
>
> Don't know which direction you mean, I guess one of the following:
>
> apply(x, c(1,2), sum)
> apply(x, 3, sum)

Or use rowSums or colSums for clarity (and speed, irrelevant here).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.mandys at matfyz.cz  Mon Jul 25 09:36:44 2005
From: petr.mandys at matfyz.cz (Petr Mandys)
Date: Mon, 25 Jul 2005 09:36:44 +0200 (CEST)
Subject: [R] Mean and variance of the right-censored data
In-Reply-To: <644e1f32050724180546f08133@mail.gmail.com>
References: <mailman.0.1122242790.18770.r-help@stat.math.ethz.ch> 
	<Pine.LNX.4.62.0507250011200.13317@artax.karlin.mff.cuni.cz>
	<644e1f32050724180546f08133@mail.gmail.com>
Message-ID: <Pine.LNX.4.62.0507250923360.3291@artax.karlin.mff.cuni.cz>

No, it doesn't.

Right-censored data are data with (so-called) censored values. E.g. there 
is a newsvendor and he sells newspapers for a week.

Mon: 77
Tue: 56
Wed: 60
Thu: 80
Fri: 85
Sat: 59
Sun: 48

This values are numbers of sold pieces from Monday to Sunday. And he sold 
out newspapers on Wednesday and on Thursday. This is an example of 
right-censored data. He could sold more pieces in that two days.

And I need the variance and mean of data like this.

Thank you!

On Sun, 24 Jul 2005, jim holtman wrote:

> does
>
> mean(a[b])
> var(a[b])
>
> do what you want?  This selects just those values of 'a' that are TRUE in 'b'.
>
> On 7/24/05, Petr Mandys <petr.mandys at matfyz.cz> wrote:
>> Hi,
>>
>> I need to get mean and variance of right censored data. How can I do that?
>>
>> I have a vector of values (called a) and vector of booleans (whether value
>> is censored) (called b). What to do with this? Sorry, I'm R beginner.
>>
>> Thank you!
>>
>> Pete
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
> -- 
> Jim Holtman
>
> What the problem you are trying to solve?
>



From 0034058 at fudan.edu.cn  Mon Jul 25 10:32:20 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 25 Jul 2005 16:32:20 +0800
Subject: [R] how to get the group mean deviation data ?
Message-ID: <0IK6003J8CLXTN@mail.fudan.edu.cn>

Yeah,I meant n=10000,but i just missed a zero.

If n=10000,t=3,it take about 3 seconds.
If n=2000,t=7,it takes about 10 seconds.
I want to write a function to fit a model,and the data maybe quite large (maybe n<=20000,t<=10).When n and t become larger and larger,the time will be much longer. It is of course reasonable.But I think there should be much pretty code to do this job,so I post here.

What I really want to konw is how to optimize the code for this purpose.Of course, I can still fit my model even I use this code.and I still like R much as it's free ,flexibile and powerfull.

>> if n id quite large,say n=1000 and t=3, it require too much time.so i 
>> want to know any more efficient way to do it?
>
>Why is about 0.4 second (which is what it takes on my system) too long?
>
>Given that you want to operate on 3000 cells, a second does not look 
>unreasonable.
>
>This is a toy problem, and it is unclear what the real problem is (if 
>any).  Since you have the same number of replications for each cell 
>(group-variable combination)

I want to deal with the case with different number of replications for each cell too.

> I would use this as a n x 3 x t array (a 
>simple call to dim and aperem).  Then rowMeans will find the group means, 
>and you can just subtract those to get the deviations from the means, 
>making use of recycling.
>
>E.g.
>
>D <- d[,-1]
>dim(D) <- c(t,n,3)
>D <- aperm(D, c(2,3,1))
>gmeans <- rowMeans(D, dims=2)
>d[,-1] - rep(gmeans, each=3)
>
>That takes under 10ms for n=1000
>
>
>On Mon, 25 Jul 2005, ronggui wrote:
>
>>> n=10;t=3
>>> d<-cbind(id=rep(1:n,each=t),y=rnorm(n*t),x=rnorm(n*t),z=rnorm(n*t))
>>> head(d)
>>     id          y           x          z
>> [1,]  1 -2.1725379  0.07629954 -0.3985258
>> [2,]  1 -1.2383038 -2.49667038  0.6966127
>> [3,]  1 -1.2642401 -0.50613307  0.4895856
>> [4,]  2  0.2171246  0.86711864 -0.6660036
>> [5,]  2  2.2765760 -0.48547142 -1.4496664
>> [6,]  2  0.5985345 -1.06427035  2.1761071
>>
>> first,i want to get the group mean of each variable,which i can use
>>> d<-data.frame(d)
>>> aggregate(d,list(d$id),mean)[,-1]
>>   id           y          x           z
>> 1   1 -1.55836060 -0.9755013  0.26255754
>> 2   2  1.03074502 -0.2275410  0.02014565
>> 3   3  0.20700121 -0.7159450  1.35890176
>> 4   4  0.17839650  1.2575891  0.04135165
>> 5   5 -0.20012508  0.4310221  0.55458899
>> 6   6 -0.13084185 -0.2953392  0.28229068
>> 7   7  0.20737288 -0.8863761 -0.50793880
>> 8   8  0.07512612 -0.6591304 -0.21656533
>> 9   9  0.94727796 -0.6108891  0.13529884
>> 10 10 -0.04434875  0.1332086 -0.88229808
>>
>> then i want the  group mean deviation data,like
>>> head(sapply(d[,2:4],function(x) x-ave(x,d$id)))
>>              y          x          z
>> [1,] -0.6141773  1.0518008 -0.6610833
>> [2,]  0.3200568 -1.5211691  0.4340552
>> [3,]  0.2941205  0.4693682  0.2270281
>> [4,] -0.8136205  1.0946597 -0.6861493
>> [5,]  1.2458310 -0.2579304 -1.4698121
>> [6,] -0.4322105 -0.8367293  2.1559614
>>
>> both above are what i want.though i can do it use the function  to do it.but if n id quite large,say n=1000 and t=3, it require too much time.so i want to know any more efficient way to do it?
>>
>> myfun<-function(x,id)
>> {
>> x<-as.matrix(x)
>> id<-as.factor(id)
>> xm<- apply(x,2,function(y,z) tapply(y,z, mean), z=id)
>> xdm<- x[] <- x-xm[id,]
>> re<-list(xm=xm, xdm=xdm)
>> re
>> }
>>
>>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roger at ysidro.econ.uiuc.edu  Mon Jul 25 10:58:09 2005
From: roger at ysidro.econ.uiuc.edu (roger@ysidro.econ.uiuc.edu)
Date: Mon, 25 Jul 2005 03:58:09 -0500 (CDT)
Subject: [R] sparse data frame (crsp?)
In-Reply-To: <200507250114.j6P1E7PO014769@hypatia.math.ethz.ch>
References: <200507250114.j6P1E7PO014769@hypatia.math.ethz.ch>
Message-ID: <1281.217.245.71.241.1122281889.squirrel@217.245.71.241>

You could try reading it in a chunk at a time and making a sparse
matrix with SparseM, unless you really need some functionality
of dataframes.

R Koenker

>
> dear R wizards:  does R have the facilities to handle sparse data
> frames?  I am thinking of reading a data base like the daily CRSP data
> into R, observations being firms, columns being days, data being stock
> returns.  but this will only fit into my memory if I can convince R to
> not have to store missing observations, and to return NA upon read
> access to missing observations.  pointers appreciated.  sincerely,  /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From paolo.bulla at unibocconi.it  Mon Jul 25 11:31:48 2005
From: paolo.bulla at unibocconi.it (Paolo Bulla)
Date: Mon, 25 Jul 2005 11:31:48 +0200
Subject: [R] Autoregressive Distributed Lag Models
Message-ID: <1122283908.42e4b184bdae3@webmail.uni-bocconi.it>


Hallo to everyone,

is there anyone that could kindly indicate me a package or a function to deal 
with Autoregressive Distributed Lag Models, e.g.

y_t = a + b_0 * x_t + ... + b_k * x_(t-k) + c_1 * y_(t-1) + ... + c_h * y_(t-h) 
+ e_t

Thank you,
 Paolo

-- 
Paolo Bulla

Istituto di Metodi Quantitativi
Universit?? "L. Bocconi"
Tel. +39 02.5836.5651
viale Isonzo 25
20136 Milano
paolo.bulla at unibocconi.it



From stagiaire2.urc at nck.ap-hop-paris.fr  Mon Jul 25 12:09:44 2005
From: stagiaire2.urc at nck.ap-hop-paris.fr (Claude Messiaen - Urc Necker)
Date: Mon, 25 Jul 2005 12:09:44 +0200
Subject: [R] trend estimation for cohort study
Message-ID: <001401c59100$fb4afbb0$39c9900a@nck.aphopparis.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050725/01d5c5dd/attachment.pl

From jonathenwu at hotmail.com  Mon Jul 25 12:15:58 2005
From: jonathenwu at hotmail.com (=?gb2312?B?zuIg6rs=?=)
Date: Mon, 25 Jul 2005 10:15:58 +0000
Subject: [R] assignment of matrix
Message-ID: <BAY105-F208DC3A31689D5559CFDFAD0CA0@phx.gbl>

Hi,
I have created a matrix and initially all elements in the matrix was 
assigned to NA.Then I want to assign values to some elements of this 
matrix. Can I use formation like matrix[i][j] <- 4?if not, how can i do 
this? now I have three vectors
 y1  y2   y3
 3   4     3.1
 5   2     4.2
if I want to assign y3's value to matrix, where the value's row and column 
should correspond the y1's and y2's value, such like
matrix[y1[1]][y2[1]] <- y3[1]
how can this be realized?
thanks
Hao Wu



From ggrothendieck at gmail.com  Mon Jul 25 12:26:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Jul 2005 06:26:26 -0400
Subject: [R] Autoregressive Distributed Lag Models
In-Reply-To: <1122283908.42e4b184bdae3@webmail.uni-bocconi.it>
References: <1122283908.42e4b184bdae3@webmail.uni-bocconi.it>
Message-ID: <971536df050725032629c92946@mail.gmail.com>

Check out:

?arima

Also:

http://cran.r-project.org/doc/contrib/Ricci-refcard-ts.pdf
http://cran.r-project.org/src/contrib/Views/

Depending on what you need the dyn package, not mentioned in
the above, may also be useful.

On 7/25/05, Paolo Bulla <paolo.bulla at unibocconi.it> wrote:
> 
> Hallo to everyone,
> 
> is there anyone that could kindly indicate me a package or a function to deal
> with Autoregressive Distributed Lag Models, e.g.
> 
> y_t = a + b_0 * x_t + ... + b_k * x_(t-k) + c_1 * y_(t-1) + ... + c_h * y_(t-h)
> + e_t
> 
> Thank you,
>  Paolo
> 
> --
> Paolo Bulla
> 
> Istituto di Metodi Quantitativi
> Universit?? "L. Bocconi"
> Tel. +39 02.5836.5651
> viale Isonzo 25
> 20136 Milano
> paolo.bulla at unibocconi.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Mon Jul 25 13:25:50 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 25 Jul 2005 12:25:50 +0100
Subject: [R] passing formula arguments cv.glm
Message-ID: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk>

I am trying to write a wrapper for the last example in help(cv.glm) that
deals with leave-one-out-cross-validation (LOOCV) for a logistic model.
This wrapper will be used as part of a bigger program. 

Here is my wrapper funtion :

   logistic.LOOCV.err <- function( formu=NULL, data=NULL ){

     cost.fn <- function(cl, pred) mean( abs(cl-pred) > 0.5 )

     glmfit <- glm( formula=formu, data=data, family="binomial" )
     print("glmfit is OK")
  
     err    <- cv.glm( data=data, glmfit=glmfit, 
                       cost=cost.fn, K=nrow(data) )$delta[2]
     print("cv.glm OK")
   }


When I run the above function line by line with the arguments from
below, it works fine. But when I call it as function, I get this :

   rm( glmfit, formu, cv.err ) # cleanup if required
   logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )

 logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
[1] "glmfit is OK"
Error in model.frame(formula = formu, data = data[j.in, , drop =
FALSE],  : 
	Object "formu" not found


I think this has something to do with formula and environments but I do
not know enough to solve it myself. I searched the archive without much
help (perhaps I was using the wrong keywords).

Any help would be very much appreciated. Thank you.

Regards, 
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Wolfson College Annexe                  Tel : 01865 284 408
Linton Road, Oxford OX2 6UD             Fax : 01865 284 424



From ligges at statistik.uni-dortmund.de  Mon Jul 25 13:44:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Jul 2005 13:44:49 +0200
Subject: [R] assignment of matrix
In-Reply-To: <BAY105-F208DC3A31689D5559CFDFAD0CA0@phx.gbl>
References: <BAY105-F208DC3A31689D5559CFDFAD0CA0@phx.gbl>
Message-ID: <42E4D0B1.8040107@statistik.uni-dortmund.de>

  wrote:

> Hi,
> I have created a matrix and initially all elements in the matrix was 
> assigned to NA.Then I want to assign values to some elements of this 
> matrix. Can I use formation like matrix[i][j] <- 4?if not, how can i do 
> this? now I have three vectors
> y1  y2   y3
> 3   4     3.1
> 5   2     4.2
> if I want to assign y3's value to matrix, where the value's row and 
> column should correspond the y1's and y2's value, such like
> matrix[y1[1]][y2[1]] <- y3[1]

For a matrix X:

 X[cbind(y1, y2)] <- y3

Uwe Ligges



> how can this be realized?
> thanks
> Hao Wu
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From eph1v3t8-rhls6783 at mailblocks.com  Sat Jul 23 16:23:50 2005
From: eph1v3t8-rhls6783 at mailblocks.com (eph1v3t8-rhls6783@mailblocks.com)
Date: Sat, 23 Jul 2005 07:23:50 -0700
Subject: [R] Non-linear "linear" models?
Message-ID: <200507231423.j6NENpEr016061@hypatia.math.ethz.ch>

Hi,
    I'm new to R (though I have spent hours trying to learn how to use 
it) and also not very knowledgeable
about statistics, so I hope you will excuse what may seem like a very 
basic question. I'm trying to use
R to do an ANOVA analysis for some data with an unbalanced design, and 
while I was trying to figure that out, I got confused about the purpose 
of the "lm". All definitions I can find of "linear model" are of the 
form:
y = a + b * x + e

  In other words, y is only linear in the dependent variable(s) x. 
However, the lm model seems to support higher order polynomials, e.g.:

> lm(dist ~ speed + I(speed^2)+I(speed^3), cars)

  Is there some sense in which that model is "linear", or is R's lm() 
providing extra functionality?

Thanks,
--Paul


----------------------------------------------
Mailblocks - A Better Way to Do Email
http://about.mailblocks.com/info



From HDoran at air.org  Mon Jul 25 15:21:10 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 25 Jul 2005 09:21:10 -0400
Subject: [R] Non-linear "linear" models?
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409994449@dc1ex2.air.org>

Paul:

Even when the model includes polynomial terms as you have below, it is
still a linear model because it is linear in the parameters. It is your
coefficients that are not linear. There are other functions in R for
non-linear models.

help.search('non linear') 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
eph1v3t8-rhls6783 at mailblocks.com
Sent: Saturday, July 23, 2005 10:24 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Non-linear "linear" models?

Hi,
    I'm new to R (though I have spent hours trying to learn how to use
it) and also not very knowledgeable
about statistics, so I hope you will excuse what may seem like a very
basic question. I'm trying to use R to do an ANOVA analysis for some
data with an unbalanced design, and while I was trying to figure that
out, I got confused about the purpose of the "lm". All definitions I can
find of "linear model" are of the
form:
y = a + b * x + e

  In other words, y is only linear in the dependent variable(s) x. 
However, the lm model seems to support higher order polynomials, e.g.:

> lm(dist ~ speed + I(speed^2)+I(speed^3), cars)

  Is there some sense in which that model is "linear", or is R's lm()
providing extra functionality?

Thanks,
--Paul


----------------------------------------------
Mailblocks - A Better Way to Do Email
http://about.mailblocks.com/info

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Jul 25 15:59:10 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 25 Jul 2005 15:59:10 +0200
Subject: [R] FW: LyX and Sweave
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31937@pollux.bfro.uni-lj.si>

Hello R-users!

I have tried to use Sweave within LyX* and found two ways to accomplish
this. I have attached LyX source file for both ways.

*<http://www.lyx.org>

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
----------------------------------------------------------------------







From macq at llnl.gov  Mon Jul 25 16:03:09 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 25 Jul 2005 07:03:09 -0700
Subject: [R] Mean and variance of the right-censored data
In-Reply-To: <Pine.LNX.4.62.0507250011200.13317@artax.karlin.mff.cuni.cz>
References: <mailman.0.1122242790.18770.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.62.0507250011200.13317@artax.karlin.mff.cuni.cz>
Message-ID: <p06210200bf0aa112e32b@[128.115.153.6]>

The survival package has functions for fitting models to right censored data.

require(survival)
?survfit
?Surv

-Don

At 12:17 AM +0200 7/25/05, Petr Mandys wrote:
>Hi,
>
>I need to get mean and variance of right censored data. How can I do that?
>
>I have a vector of values (called a) and vector of booleans (whether value
>is censored) (called b). What to do with this? Sorry, I'm R beginner.
>
>Thank you!
>
>Pete
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From petr.mandys at matfyz.cz  Mon Jul 25 16:21:05 2005
From: petr.mandys at matfyz.cz (Petr Mandys)
Date: Mon, 25 Jul 2005 16:21:05 +0200 (CEST)
Subject: [R] Mean and variance of the right-censored data
In-Reply-To: <p06210200bf0aa112e32b@[128.115.153.6]>
References: <mailman.0.1122242790.18770.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.62.0507250011200.13317@artax.karlin.mff.cuni.cz>
	<p06210200bf0aa112e32b@[128.115.153.6]>
Message-ID: <Pine.LNX.4.62.0507251615490.14641@artax.karlin.mff.cuni.cz>

Thank you!

I'm using this:

require(NADA)

var_sf=survfit(Surv(a, !b))
var_result=new("cenfit", survfit=var_sf)

Class cenfit has methods to get mean a variance from survfit result. Is 
this correct?

Pete

On Mon, 25 Jul 2005, Don MacQueen wrote:

> The survival package has functions for fitting models to right censored 
> data.
>
> require(survival)
> ?survfit
> ?Surv
>
> -Don
>
> At 12:17 AM +0200 7/25/05, Petr Mandys wrote:
>> Hi,
>> 
>> I need to get mean and variance of right censored data. How can I do that?
>> 
>> I have a vector of values (called a) and vector of booleans (whether value
>> is censored) (called b). What to do with this? Sorry, I'm R beginner.
>> 
>> Thank you!
>> 
>> Pete
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> --------------------------------------
>



From h.wickham at gmail.com  Mon Jul 25 16:22:29 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 25 Jul 2005 09:22:29 -0500
Subject: [R] Help vectorising a function
Message-ID: <f8e6ff050507250722b213fdc@mail.gmail.com>

M <- function(m, s, init = 0) {
	A <- nrow(m); T <- ncol(m)
	M <- matrix(init, nrow = A, ncol = T)

	for(a in 1:(A-1)) {
		M[a+1, 2:T] <- (s[a] * (M[a, ] + m[a, ]))[1:(T-1)]
	}
	M
}

This is from a mark-recapture study where M is an estimate of the
number of marked fish in each age class (A) over each year (T).  s is
a vector of age dependent survival probabilities and m is a matrix
containing the number of fish marked for the first time (in each age
class and year).

I'm pretty sure there's a much better way of doing this - but I can't
see it.  I'm not looking for the exact code to solve the problem -
just a better way of attacking it.  Any hints would be much
appreciated!

Hadley



From christian_mora at arauco.cl  Mon Jul 25 16:24:46 2005
From: christian_mora at arauco.cl (Christian Mora)
Date: Mon, 25 Jul 2005 10:24:46 -0400
Subject: [R] error in gnls
Message-ID: <OFE2194F77.4FA51BA4-ON04257049.004E9DB7@arauco.cl>





Dear R users;

I'm trying to fit nonlinear model (asymptotic regression model) with gnls
from library nlme in R 2.1.0 with no big issues so far. However after
installed the version R 2.1.1, when I tried to update the initial model
including a var-cov model I've got the error: "Error: Object "convIter" not
found". This error occurs only with R 2.1.1. Any ideas?

Thanks

Christian Mora



From tlumley at u.washington.edu  Mon Jul 25 16:43:36 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 25 Jul 2005 07:43:36 -0700 (PDT)
Subject: [R] Mean and variance of the right-censored data
In-Reply-To: <Pine.LNX.4.62.0507251615490.14641@artax.karlin.mff.cuni.cz>
References: <mailman.0.1122242790.18770.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.62.0507250011200.13317@artax.karlin.mff.cuni.cz>
	<p06210200bf0aa112e32b@[128.115.153.6]>
	<Pine.LNX.4.62.0507251615490.14641@artax.karlin.mff.cuni.cz>
Message-ID: <Pine.A41.4.61b.0507250734550.286078@homer04.u.washington.edu>

On Mon, 25 Jul 2005, Petr Mandys wrote:

> Thank you!
>
> I'm using this:
>
> require(NADA)
>
> var_sf=survfit(Surv(a, !b))
> var_result=new("cenfit", survfit=var_sf)
>
> Class cenfit has methods to get mean a variance from survfit result. Is
> this correct?
>

NADA is designed for left censoring. I don't know if it will work for 
right censoring.

There is at least one problem that hasn't been mentioned yet.  Unless 
your largest observations are uncensored you can't compute the mean or 
variance without a parametric model.

survfit() in the survival package used to produce something it called the 
mean, but it wasn't.

 	-thomas



From Sebastian.Leuzinger at unibas.ch  Mon Jul 25 17:08:01 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Mon, 25 Jul 2005 17:08:01 +0200
Subject: [R] computation time gls()
Message-ID: <200507251708.01527.Sebastian.Leuzinger@unibas.ch>

dear R users

i try to fit a gls model to a rather large dataset with an AR(1) error 
structure:

attach(sf.a1filt)
m1.a.gls <- gls(fluxt~co2+light+vpd+wind,
   correlation = corAR1(0.8))
summary(m1.a.gls)
detach(sf.a1filt)

there are approx. 5000 observations, and the computation seems to take several 
hours, i actually killed the process because i became too impatient. is there 
any way to be more efficient with R? (because really the model will be more 
complex, i.e. more predictors and higher autoregressive order). 

os: linux suse, R version: latest, machine: ibm thinkpad 42T, 1GHz RAM


------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From dmbates at gmail.com  Mon Jul 25 17:14:38 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 25 Jul 2005 10:14:38 -0500
Subject: [R] error in gnls
In-Reply-To: <OFE2194F77.4FA51BA4-ON04257049.004E9DB7@arauco.cl>
References: <OFE2194F77.4FA51BA4-ON04257049.004E9DB7@arauco.cl>
Message-ID: <40e66e0b0507250814ea2a77@mail.gmail.com>

On 7/25/05, Christian Mora <christian_mora at arauco.cl> wrote:

> 
> Dear R users;
> 
> I'm trying to fit nonlinear model (asymptotic regression model) with gnls
> from library nlme in R 2.1.0 with no big issues so far. However after
> installed the version R 2.1.1, when I tried to update the initial model
> including a var-cov model I've got the error: "Error: Object "convIter" not
> found". This error occurs only with R 2.1.1. Any ideas?

There is an error in the current version of gnls.  I will upload a new
version of the nlme package fixing this  to CRAN today.



From lma5 at ncsu.edu  Mon Jul 25 17:22:47 2005
From: lma5 at ncsu.edu (lma5@ncsu.edu)
Date: Mon, 25 Jul 2005 11:22:47 -0400 (EDT)
Subject: [R] how to write a periodic function in R?
Message-ID: <1430.152.14.74.126.1122304967.squirrel@webmail.ncsu.edu>

My question is how to write a periodic function in R.
for example if there is a function f(x) that
 f(x)=f(x+4), for -2<x<2, f(x)=x
how to write it in R?
thanks a lot!



-------------------------------
liyun (Lauren) Ma



From murdoch at stats.uwo.ca  Mon Jul 25 17:43:07 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 25 Jul 2005 11:43:07 -0400
Subject: [R] how to write a periodic function in R?
In-Reply-To: <1430.152.14.74.126.1122304967.squirrel@webmail.ncsu.edu>
References: <1430.152.14.74.126.1122304967.squirrel@webmail.ncsu.edu>
Message-ID: <42E5088B.6000602@stats.uwo.ca>

On 7/25/2005 11:22 AM, lma5 at ncsu.edu wrote:
> My question is how to write a periodic function in R.
> for example if there is a function f(x) that
>  f(x)=f(x+4), for -2<x<2, f(x)=x
> how to write it in R?
> thanks a lot!

Just write a function that does that, e.g.

f <- function(x) (x + 2) %% 4 - 2

The %% is the mod operator, so (x + 2) %% 4 adds 2 then maps all values 
into the range 0 to 4.  You'll need a different formula for a different 
periodic function.

Use plot(f, from=-7, to=7) to see that this works.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Mon Jul 25 17:48:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Jul 2005 16:48:46 +0100 (BST)
Subject: [R] computation time gls()
In-Reply-To: <200507251708.01527.Sebastian.Leuzinger@unibas.ch>
References: <200507251708.01527.Sebastian.Leuzinger@unibas.ch>
Message-ID: <Pine.LNX.4.61.0507251647480.5377@gannet.stats>

I find arima() fits such models very much faster.

On Mon, 25 Jul 2005, Sebastian Leuzinger wrote:

> dear R users
>
> i try to fit a gls model to a rather large dataset with an AR(1) error
> structure:
>
> attach(sf.a1filt)
> m1.a.gls <- gls(fluxt~co2+light+vpd+wind,
>   correlation = corAR1(0.8))
> summary(m1.a.gls)
> detach(sf.a1filt)
>
> there are approx. 5000 observations, and the computation seems to take several
> hours, i actually killed the process because i became too impatient. is there
> any way to be more efficient with R? (because really the model will be more
> complex, i.e. more predictors and higher autoregressive order).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 25 17:51:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Jul 2005 16:51:23 +0100 (BST)
Subject: [R] how to write a periodic function in R?
In-Reply-To: <1430.152.14.74.126.1122304967.squirrel@webmail.ncsu.edu>
References: <1430.152.14.74.126.1122304967.squirrel@webmail.ncsu.edu>
Message-ID: <Pine.LNX.4.61.0507251649440.5377@gannet.stats>

On Mon, 25 Jul 2005 lma5 at ncsu.edu wrote:

> My question is how to write a periodic function in R.
> for example if there is a function f(x) that
> f(x)=f(x+4), for -2<x<2, f(x)=x
> how to write it in R?

f((x+2)%%4 - 2)  is one way.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rrendas at cox.net  Mon Jul 25 18:02:00 2005
From: rrendas at cox.net (rrendas@cox.net)
Date: Mon, 25 Jul 2005 12:02:00 -0400
Subject: [R] order of panels  in xyplot
Message-ID: <20050725160157.XQIZ8757.centrmmtao01.cox.net@smtp.east.cox.net>

I'm having trouble with the order of the panels using xyplot.  I had used this bit of code before and received the desired plot (the code was not identical, but very similar; perhaps more importantly I was working with an older version of R) .  Now the panels appear right to left instead of left to right, as it says in the help files, which is what I would like. Does anyone know if this has been changed?  OR has anybody encountered this problem and a way of fixing it?

Below is the code I'm using (ctry_pfi and v291 are factors, with 16 and 2 levels, respectively)):

superpose.line <- trellis.par.get("superpose.line") 
superpose.line$lwd <- 3 
superpose.line$lty <- c(1,2)
trellis.par.set("superpose.line", superpose.line) 

trellis.strip.blank() 
trellis.par.set(list(background = list(col = "white"))) 

xyplot(pfi~age|ctry_pfi,groups= v291,data=pfi,
	panel=panel.superpose, type=c("n", "smooth"),
	span=.75, as.table=F, col=c(4,2), lwd=2, lty=c(1,2),
	xlab = 'Age', ylab='Progressive Feminism Index (PFI)',
        xlim=c(10, 100), ylim=c(10,25),
        key = list(lines = Rows(trellis.par.get("superpose.line"), c(14, 5)),
		lwd=3, lty=c(2,1), text = list(lab = c('Women', 'Men')),
                space="bottom", border=T, columns=2))



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Jul 25 18:15:56 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 25 Jul 2005 18:15:56 +0200
Subject: [R] FW: LyX and Sweave
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31938@pollux.bfro.uni-lj.si>

>On 25/07/05, Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si> wrote:
>> Hello R-users!
>> 
>> I have tried to use Sweave within LyX* and found two ways to accomplish
>> this. I have attached LyX source file for both ways.
>> 
>> *<http://www.lyx.org>
>> 

> Hi Gregor,
> I think the mailing list strips off attachments.
> cheers,
> Sean

Well ascii attachments should go through* but they are not stored in R-help
list, at least I don't know how to get them. I'll cc to Friedrich Leisch and
he might consider to add them to his Sweave repository at

<http://www.ci.tuwien.ac.at/~leisch/Sweave/>

In case he doesn't like to do that I will put them on my homepage and mention
that on R-help list.

*I can send them once more to private e-mails if there is a need for that!



From sundar.dorai-raj at pdf.com  Mon Jul 25 18:17:00 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 25 Jul 2005 11:17:00 -0500
Subject: [R] order of panels  in xyplot
In-Reply-To: <20050725160157.XQIZ8757.centrmmtao01.cox.net@smtp.east.cox.net>
References: <20050725160157.XQIZ8757.centrmmtao01.cox.net@smtp.east.cox.net>
Message-ID: <42E5107C.6000706@pdf.com>



rrendas at cox.net wrote:
> I'm having trouble with the order of the panels using xyplot.  I had used this bit of code before and received the desired plot (the code was not identical, but very similar; perhaps more importantly I was working with an older version of R) .  Now the panels appear right to left instead of left to right, as it says in the help files, which is what I would like. Does anyone know if this has been changed?  OR has anybody encountered this problem and a way of fixing it?
> 
> Below is the code I'm using (ctry_pfi and v291 are factors, with 16 and 2 levels, respectively)):
> 
> superpose.line <- trellis.par.get("superpose.line") 
> superpose.line$lwd <- 3 
> superpose.line$lty <- c(1,2)
> trellis.par.set("superpose.line", superpose.line) 
> 
> trellis.strip.blank() 
> trellis.par.set(list(background = list(col = "white"))) 
> 
> xyplot(pfi~age|ctry_pfi,groups= v291,data=pfi,
> 	panel=panel.superpose, type=c("n", "smooth"),
> 	span=.75, as.table=F, col=c(4,2), lwd=2, lty=c(1,2),
> 	xlab = 'Age', ylab='Progressive Feminism Index (PFI)',
>         xlim=c(10, 100), ylim=c(10,25),
>         key = list(lines = Rows(trellis.par.get("superpose.line"), c(14, 5)),
> 		lwd=3, lty=c(2,1), text = list(lab = c('Women', 'Men')),
>                 space="bottom", border=T, columns=2))
> 


I haven't noticed if anything's been changed. However, you can always 
explictly define the panel order by setting the levels of your grouping 
variable (i.e. ctry_pfi in your case). See ?factor.

HTH,

--sundar



From gerifalte28 at hotmail.com  Mon Jul 25 18:49:26 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 25 Jul 2005 16:49:26 +0000
Subject: [R] poisson fit for histogram
In-Reply-To: <Pine.LNX.4.61.0507230807430.2230@gannet.stats>
Message-ID: <BAY103-F22FDDFA440E58DA1D114E4A6CA0@phx.gbl>

Ups! Mr. Ripley is right.  I ignored the OS in the posting.  My appologies 
to Tom Isenbarger for the misleading answer.

Regards

Francisco


>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>To: "Francisco J. Zagmutt" <gerifalte28 at hotmail.com>
>CC: isen at plantpath.wisc.edu, r-help at stat.math.ethz.ch
>Subject: Re: [R] poisson fit for histogram
>Date: Sat, 23 Jul 2005 08:29:28 +0100 (BST)
>
>What does fit.dist do that fitdistr (MASS) does not in this context?  (It 
>plots, but that is very easy to do in base R.  However, to see if a Poisson 
>fits you need a test of goodness-of-fit.)
>
>BTW, `decompress and store the files in your "library" folder' is on no OS 
>(you did not mention one but Thomas did) the way to install a package. Even 
>on Windows (where it might just work) there are simpler and better ways to 
>do it, like using a menu.  Note that Lindsey does not provide pre-compiled 
>packages for MacOS X, the platform Thomas is using (and as they use 
>Fortran, people have reported that they are tricky to install on MacOS X), 
>and your recipe is `seriously' misleading there.
>
>On Fri, 22 Jul 2005, Francisco J. Zagmutt wrote:
>
>>I would first reccomend you to update your version of R.  Then download 
>>the
>>libraries rmutil and gnlm from Jim Lindsey at
>>http://www.luc.ac.be/~jlindsey/rcode.html decompress and store the files 
>>in
>>your "library" folder.   Sorry but you will have to donwload a package
>>unless you seriously want to re-invent the wheel.
>>Finally try
>>library(gnlm)
>>?fit.dist()
>>
>>Cheers
>>
>>Francisco
>>
>>
>>>From: Thomas Isenbarger <isen at plantpath.wisc.edu>
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] poisson fit for histogram
>>>Date: Wed, 20 Jul 2005 10:40:51 -0500
>>>
>>>I haven't been an R lister for a bit, but I hope to enlist someone's
>>>help here.  I think this is a simple question, so I hope the answer
>>>is not much trouble.  Can you please respond directly to this email
>>>address in addition to the list (if responding to the list is
>>>warranted)?
>>>
>>>I have a histogram and I want to see if the data fit a Poisson
>>>distribution.  How do I do this?  It is preferable if it could be
>>>done without having to install any or many packages.
>>>
>>>I use R Version 1.12 (1622) on OS X
>>>
>>>Thank-you very much,
>>>Tom Isenbarger
>
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wl at eimb.ru  Mon Jul 25 19:16:15 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Mon, 25 Jul 2005 09:16:15 -0800
Subject: [R] Lattice: how to make x axis to appear on only one
	non-bottom plot?
In-Reply-To: <42E41888.2030306@stat.auckland.ac.nz>
References: <1438101309.20050722094536@eimb.ru>
	<42E41888.2030306@stat.auckland.ac.nz>
Message-ID: <798299827.20050725091615@eimb.ru>

Hi,

PM> Does the following do what you want?

PM> xyplot(<whatever>)
PM> trellis.focus("panel", 4, 2, clip.off=TRUE, highlight=FALSE)
PM> panel.axis("bottom", check.overlap=TRUE, outside=TRUE)

Yes, it does. Thank you very much!
With the exception, that first call must be
trellis.focus("panel", 3, 3, clip.off=TRUE, highlight=FALSE)

PM> (it would be easier to help if you could provide code that others can run)
I am sorry.
Here it is if any one still interested.
Any other comments concerned to the R code below will be appreciated.

====8<====

# The draw.key behaves slightly incorrect, and I have redefined it.
# (this was discussed in R-help on May 19-21, 2005)
source('../common_r_things/draw.key.r')

# Lattice tunings, like colors, cex-s, etc.
source('../common_r_things/wl_theme.r')

# I need also two additional curves on each plot panel,
# and a slope and its significance.
# The function below plots everything.

# Colors for additional curves, mentioned above.
# These variables are global, as the curves will appear in the plot key,
# and I would like to have a single place to define the plot and key parameters.
i1.col<-"tomato3"
i2.col<-"gray50"

plot.panel<-function(x,y,panel.counter,...){
# Plot additional curves. They are constant.
  i2<-c(-0.7,-1.7,0.2,0.8,-0.4,-1.7,-0.8,-0.4,0.9,0.7,0.5,0.7,1.1,1.5,-1.7)
  i2<-(i2-min(i2))*(max(y)-min(y))/(max(i2)-min(i2))+min(y)

  i1<-c(7.39230,-0.67956,2.65596, 4.44415, 0.73161, 1.66836,-2.52039, 2.52273,
           -2.51820,-0.89912, 1.89464,-3.26774,3.58680, 0.58910,-2.89624)
  i1<-(i1-min(i1))*(max(y)-min(y))/(max(i1)-min(i1))+min(y)

  panel.lines(x,y=i2,lwd=2,col=i2.col)
  panel.lines(x,y=i1,lwd=2,col=i1.col)

# Plot actual data.
  panel.xyplot(x,y,...);

# Calculate and print the slope and its significance.
  m<-lm(y~x);
  v<-pf(summary(m)$fstatistic[1],summary(m)$fstatistic[2],
        summary(m)$fstatistic[3]);
  if(v=="NaN") v<-0;
  if(v>0.99) {
    sign.exp<-expression(bold(S>0.99))
  }else{
    sign.exp<-substitute(expression(bold(S==V)),list(V=round(v,2)))
  }
  sl<-formatC(summary(m)$coefficients[2,1]*10,digits=2,format="f");
  err<-formatC(summary(m)$coefficients[2,2]*10,digits=2,format="f");

  grid.text(label=c(eval(substitute(expression(bold(paste("Slope: ",Sl%+-%Err~~dec^-1))),
                                    list(Sl=sl,Err=err))),
                    eval(sign.exp)),
             x=0.1,

     # The place for the text is chosen so, that it appears on the empty space.
     # In my case, first 6 panels have empty top and the rest 5 ones have empty bottom.
             y=if(panel.counter<7) c(1,0.85) else c(0.25,0.1),
             
             default.units="npc",just= c("left", "top")
            )
}

trellis.device("png",filename="general_plot2.png",theme=wl.theme,width=1000,height=1200)

# Data frame flow contains monthly mean data and some their aggregations
# That's why month variable varies from 1 to 29.
# Year varies from 1990 through 2004.
# However, now I need only a subset of the data.

print(
xyplot(A3.index~year|factor(month),
       strip=strip.custom(factor.levels=c("January",
             "Integral (Jan-Feb)","Integral (Jan-Mar)","Integral (Jan-Apr)","Integral (Jan-May)","Integral (Jan-Jun)",
             "Integral (Jan-Jul)","Integral (Jan-Aug)","Integral (Jan-Sep)","Integral (Jan-Oct)","Integral (Jan-Nov)")),
       subset=month %in% c(1,19:28),
       data=flow,as.table=TRUE,type=c("o","g","r"),layout=c(3,4),pch=1,col="black",
       ylab="flow index",xlab=NULL,
       scales=list(x=list(alternating=1),y=list(alternating=1)),
       between=list(x=0.2,y=0.2),
       panel=plot.panel,
       key=list(size=2,between=2,col="black",lty=1,border=TRUE,divide=1,
                x=0.95,y=0.07,corner=c(1,0),columns=1,
                lines=list(pch=c(32,1,32,32,32,32,32),c(0,1,0,1,0,1,0),lwd=c(0,1,0,2,0,2,0),
                           col=c(0,"black",0,i1.col,0,i2.col,0),type="b"),
                text=list(c("","Flow index","","i1 index(scaled)","",
                               "i2 index (scaled)",""),cex=1.1)
               )

      )
)
trellis.focus("panel", 3, 3, clip.off=TRUE, highlight=FALSE)
panel.axis("bottom", check.overlap=TRUE, outside=TRUE)

dev.off()

====8<====

---
Best regards,
Wladimir                mailto:wl at eimb.ru



From edhuang00 at yahoo.com  Mon Jul 25 19:24:57 2005
From: edhuang00 at yahoo.com (Haibo Huang)
Date: Mon, 25 Jul 2005 10:24:57 -0700 (PDT)
Subject: [R] Convert quarterly data to monthly data
Message-ID: <20050725172458.82480.qmail@web31003.mail.mud.yahoo.com>

Hi,

I am new to use R, but can anyone tell me how to
tranform quarterly data to monthly data? I know SAS
has this procedure, so may I assume it is also
available in R?

Thanks a lot!

Ed



From RVARADHAN at JHMI.EDU  Mon Jul 25 19:39:24 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Mon, 25 Jul 2005 13:39:24 -0400
Subject: [R] A one-liner to create a 3-dim array
Message-ID: <0IK7005IA2DPCV@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050725/076e89ce/attachment.pl

From deepayan.sarkar at gmail.com  Mon Jul 25 20:19:47 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 25 Jul 2005 13:19:47 -0500
Subject: [R] Lattice: reversing order of panel placement in conditional
	histograms
In-Reply-To: <Pine.LNX.4.61.0507230830310.2230@gannet.stats>
References: <op.sucme02dc0cqda@samferguson-g5.arch.usyd.edu.au>
	<Pine.LNX.4.61.0507230830310.2230@gannet.stats>
Message-ID: <eb555e6605072511192fed5213@mail.gmail.com>

On 7/23/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> >From what I understand, you want to set up a factor with the levels
> reversed.  It is not that `5 is larger than 1', but that you created a
> factor with the levels in alphabetic order.  Lattice plots in the order of
> the levels.
> 
> Something like  f <- factor(f, levels=rev(levels(f))) will do this.
> 
> For the horizontal factor, give the levels in the order you want them to
> appear (which might not be the reverse of alphabetic).
> 
> 
> On Sat, 23 Jul 2005, Sam Ferguson wrote:
> 
> > I have a question about lattice in general, and histogram specfically.
> > How do you control the ordering of factors that controls the placement
> > of the conditional panels. I have a dataset with factors that go
> > 'Q1','Q2',"Q3','Q5' and of course I want the plot to place Question Q1
> > at the top and Question Q5 at the bottom of the graphical output.

One additional comment: if you think top to bottom is the natural
order (which lattice doesn't), try using

as.table = TRUE

Deepayan

> > histogram() does the opposite as 5 is larger than 1. Similarly my
> > 'AlertFormat' factor is a textual category, and I need the data to read
> > from left to right (representing old to new) , with 'New A & V' on the
> > right, and 'Pre-existing A & V' on the left, which is the opposite to
> > how histogram plots.

[...]



From rolf at math.unb.ca  Mon Jul 25 20:24:01 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 25 Jul 2005 15:24:01 -0300 (ADT)
Subject: [R] A one-liner to create a 3-dim array
Message-ID: <200507251824.j6PIO1UC000644@erdos.math.unb.ca>

	B <- array(apply(A,1,function(x){x%o%x}),dim=c(nrow(A),dim(A)))

				cheers,

					Rolf Turner
					rolf at math.unb.ca

Ravi Varadhan wrote:

>  I would like to write a one-line R code to create a 3-dim array, B, of
>  dimension (n,n,m) from a matrix, A, of dimension (m,n) such that the i-th
>  element of the 3-dim array, B[, , i]  is the outer product of the i-th row
>  of A.



From deepayan.sarkar at gmail.com  Mon Jul 25 20:37:44 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 25 Jul 2005 13:37:44 -0500
Subject: [R] Lattice: how to get default ylim?
In-Reply-To: <200505190948.18843.deepayan@stat.wisc.edu>
References: <244663517.20050519141653@eimb.ru>
	<200505190911.38996.deepayan@stat.wisc.edu>
	<20050519141128.GA12813@jessie.research.bell-labs.com>
	<200505190948.18843.deepayan@stat.wisc.edu>
Message-ID: <eb555e66050725113733057f10@mail.gmail.com>

On 5/19/05, Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> On Thursday 19 May 2005 9:11 am, David James wrote:
> > Deepayan Sarkar wrote:
> > > >      v <- current.viewport()        ## requires R 2.1.0 (I believe)
> > >
> > > No, I think it's been there for a while. However, AFAIR the fact that
> > > viewports have components xscale and yscale that can be accessed like
> > > this is undocumented and may change if the implementation changes (which
> > > is a real possibility).
> > >
> > > Ideally, there should be exported interfaces to access this information,
> > > either in grid or lattice. One of the reasons there isn't is that you
> > > rarely
> >
> > Yes, I agree that such an interface is quite desirable.
> 
> OK, I'll put something in the next version of lattice.

For the record, lattice now has (had for a while, actually) the function

current.panel.limits

which returns the native scales of the current viewport (as a list
with components xlim and ylim).

Deepayan



From reid_huntsinger at merck.com  Mon Jul 25 20:58:09 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 25 Jul 2005 14:58:09 -0400
Subject: [R] A one-liner to create a 3-dim array
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9538@uswpmx00.merck.com>

I think this works:

B <- aperm(rep(t(A),n)*rep(A,rep(n,n*m)), perm=c(1,3,2))

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
Sent: Monday, July 25, 2005 1:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] A one-liner to create a 3-dim array


Hi,

 

I would like to write a one-line R code to create a 3-dim array, B, of
dimension (n,n,m) from a matrix, A, of dimension (m,n) such that the i-th
element of the 3-dim array, B[, , i]  is the outer product of the i-th row
of A.  

 

Thanks for any help,

Ravi.


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From caitlin1 at u.washington.edu  Mon Jul 25 21:37:37 2005
From: caitlin1 at u.washington.edu (Caitlin Burgess)
Date: Mon, 25 Jul 2005 12:37:37 -0700
Subject: [R] alaska map?
Message-ID: <1122320257.29093.0.camel@localhost.localdomain>

Hello,

I've installed the Becker and Wilks maps, mapdata, and mapproj packages
so I can begin to try these out for some work I need to do on a map of
Alaska but I don't know where to find a map of Alaska. Has anyone solved
this already and could help?

Thanks very much in advance,

Caitlin



From eph1v3t8-rhls6783 at mailblocks.com  Mon Jul 25 22:04:52 2005
From: eph1v3t8-rhls6783 at mailblocks.com (eph1v3t8-rhls6783@mailblocks.com)
Date: Mon, 25 Jul 2005 13:04:52 -0700
Subject: [R] ANOVA/aov question
Message-ID: <200507252004.j6PK4rNO012361@hypatia.math.ethz.ch>

I'm a bit confused about the anova/aov functions.  Both seem to rely on 
data models, where the relationship between the dependent variables and 
the independent variables can be expressed as a formula.  In what I am 
trying to do, all of my independent variables are qualitative, not 
quantitative.  For example, for each of two options, "option A" and 
"option B" I have collected a set of measurements of the same quantity, 
and I wish to do an ANOVA test to see if that factor (actually I have 
three factors, with 2 levels each) has a significant influence on the 
measured quantity.

Can R be used for this kind of ANOVA, or must the independent variables 
be quantitative in nature?  If the answer is that it can be done, and 
that it can be done with anova/aov, how do I go about expressing the 
"data model" for qualitative factors?

Thanks much,
    --Paul

----------------------------------------------
Mailblocks - A Better Way to Do Email
http://about.mailblocks.com/info



From wl at eimb.ru  Mon Jul 25 22:11:24 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Mon, 25 Jul 2005 12:11:24 -0800
Subject: [R] Don't understand plotmath behaviour (bug?)
Message-ID: <1443722582.20050725121124@eimb.ru>

Hello, all

 Please, consider the following pieces of code.

 1.
 v<-0.5
 text(x=2,y=2,eval(substitute(expression(bold(S==V)),list(V=formatC(v,format="f",digits=2)))))

 This plots "S=0.5" in bold. Both "S" and "0.5" are bold.

 2.
 v<-0.5
 text(x=2,y=2,eval(substitute(expression(bold(S==V)),list(V=round(v,2)))))

 Here, only "S" is bold, 0.5 is usual, non-bold.

 Why?

 Thank you very much for attention and explanations.
 
---
Best regards,
Wladimir                mailto:wl at eimb.ru



From Roger.Bivand at nhh.no  Mon Jul 25 22:28:48 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 25 Jul 2005 22:28:48 +0200 (CEST)
Subject: [R] alaska map?
In-Reply-To: <1122320257.29093.0.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0507252226040.4996-100000@reclus.nhh.no>

On Mon, 25 Jul 2005, Caitlin Burgess wrote:

> Hello,
> 
> I've installed the Becker and Wilks maps, mapdata, and mapproj packages
> so I can begin to try these out for some work I need to do on a map of
> Alaska but I don't know where to find a map of Alaska. Has anyone solved
> this already and could help?

map("world2", "USA:alaska")

gives the low resolution, using the "world" database is split on the date 
line. The mapdata package has better resolution.

> 
> Thanks very much in advance,
> 
> Caitlin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From edhuang00 at yahoo.com  Mon Jul 25 22:34:07 2005
From: edhuang00 at yahoo.com (Haibo Huang)
Date: Mon, 25 Jul 2005 13:34:07 -0700 (PDT)
Subject: [R] cubic spline curves
Message-ID: <20050725203408.89027.qmail@web31013.mail.mud.yahoo.com>

I have a question about how to generate monthly data
from quarterly data:

What I want to do is basically fits cubic spline
curves to the quarterly data to form continuous-time
approximations of the input series, and then generate
output series (monthly data) from the spline
approximations. 

SAS can do it using proc expand:
http://support.sas.com/rnd/app/examples/ets/expa/

Anyone know how to do it in R? Any input is
appreciated.

Best,
Ed



From reid_huntsinger at merck.com  Mon Jul 25 22:58:47 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 25 Jul 2005 16:58:47 -0400
Subject: [R] cubic spline curves
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A953A@uswpmx00.merck.com>

Have a look at help(spline) for starters.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Haibo Huang
Sent: Monday, July 25, 2005 4:34 PM
To: r-help at stat.math.ethz.ch
Subject: [R] cubic spline curves


I have a question about how to generate monthly data
from quarterly data:

What I want to do is basically fits cubic spline
curves to the quarterly data to form continuous-time
approximations of the input series, and then generate
output series (monthly data) from the spline
approximations. 

SAS can do it using proc expand:
http://support.sas.com/rnd/app/examples/ets/expa/

Anyone know how to do it in R? Any input is
appreciated.

Best,
Ed

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jul 25 23:35:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Jul 2005 22:35:09 +0100 (BST)
Subject: [R] ANOVA/aov question
In-Reply-To: <200507252004.j6PK4rNO012361@hypatia.math.ethz.ch>
References: <200507252004.j6PK4rNO012361@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0507252230220.8741@gannet.stats>

On Mon, 25 Jul 2005 eph1v3t8-rhls6783 at mailblocks.com wrote:

> I'm a bit confused about the anova/aov functions.  Both seem to rely on
> data models, where the relationship between the dependent variables and
> the independent variables can be expressed as a formula.  In what I am
> trying to do, all of my independent variables are qualitative, not
> quantitative.  For example, for each of two options, "option A" and
> "option B" I have collected a set of measurements of the same quantity,
> and I wish to do an ANOVA test to see if that factor (actually I have
> three factors, with 2 levels each) has a significant influence on the
> measured quantity.
>
> Can R be used for this kind of ANOVA, or must the independent variables
> be quantitative in nature?  If the answer is that it can be done, and
> that it can be done with anova/aov, how do I go about expressing the
> "data model" for qualitative factors?

aov() is the model fitting function for ANOVA models.  anova() is a helper 
function to produce ANOVA tables (or analogues like analysis of deviance 
tables) from one or more fitted models.

The first example in ?aov has three binary explanatory variables (and one 
6-way one).  So it certainly can be done with factors (which in the R 
sense are necessarily qualitative), and the help page is a good start.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Jul 25 23:40:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 25 Jul 2005 14:40:45 -0700 (PDT)
Subject: [R] Don't understand plotmath behaviour (bug?)
In-Reply-To: <1443722582.20050725121124@eimb.ru>
References: <1443722582.20050725121124@eimb.ru>
Message-ID: <Pine.A41.4.61b.0507251436580.217516@homer04.u.washington.edu>

On Mon, 25 Jul 2005, Wladimir Eremeev wrote:

> Hello, all
>
> Please, consider the following pieces of code.
>
> 1.
> v<-0.5
> text(x=2,y=2,eval(substitute(expression(bold(S==V)),list(V=formatC(v,format="f",digits=2)))))
>
> This plots "S=0.5" in bold. Both "S" and "0.5" are bold.
>
> 2.
> v<-0.5
> text(x=2,y=2,eval(substitute(expression(bold(S==V)),list(V=round(v,2)))))
>
> Here, only "S" is bold, 0.5 is usual, non-bold.
>

In case 1 you have the string "0.5", in case 2 you have the number 0.5.
   text(x=2,y=2, quote(bold("0.5"==0.5)))
shows what is happening.

 	-thomas



From helprhelp at gmail.com  Mon Jul 25 23:45:12 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 25 Jul 2005 16:45:12 -0500
Subject: [R] cluster
Message-ID: <cdf817830507251445309dea83@mail.gmail.com>

Dear listers:

Here I have a question on clustering methods available in R. I am
trying to down-sampling the majority class in a classification problem
on an imbalanced dataset. Since I don't want to lose information in
the original dataset, I don't want to use naive down-sampling: I think
using clustering on the majority class' side to select
"representative" samples might help. So, my question is, which
clustering method should be tested to get the best result. I think the
key thing might be the selection of "distance" considering the next
step in which I would like to use  decision trees.

Please share your experience in using clustering (Any available
implementation outside R is also welcome)

weiwei
-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From wl at eimb.ru  Tue Jul 26 00:13:38 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Mon, 25 Jul 2005 14:13:38 -0800
Subject: [R] Don't understand plotmath behaviour (bug?)
In-Reply-To: <Pine.A41.4.61b.0507251436580.217516@homer04.u.washington.edu>
References: <1443722582.20050725121124@eimb.ru>
	<Pine.A41.4.61b.0507251436580.217516@homer04.u.washington.edu>
Message-ID: <164116838.20050725141338@eimb.ru>

Dear Thomas,

TL> In case 1 you have the string "0.5", in case 2 you have the number 0.5.
TL>    text(x=2,y=2, quote(bold("0.5"==0.5)))
TL> shows what is happening.

I know about the different types in the cases.
That is, 'bold' affects only on text strings.
Am I right?

---
Best regards,
Wladimir                mailto:wl at eimb.ru



From Quin_Wills at msn.com  Tue Jul 26 01:21:17 2005
From: Quin_Wills at msn.com (Quin Wills)
Date: Tue, 26 Jul 2005 00:21:17 +0100
Subject: [R] Installing SJava
Message-ID: <BAY103-DAV138C20E25713691FE4C1A8F4CA0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/2150cb10/attachment.pl

From ivo_welch-rstat8303 at mailblocks.com  Tue Jul 26 03:49:11 2005
From: ivo_welch-rstat8303 at mailblocks.com (ivo_welch-rstat8303@mailblocks.com)
Date: Mon, 25 Jul 2005 18:49:11 -0700
Subject: [R] sparse data frame (crsp?)
In-Reply-To: <1281.217.245.71.241.1122281889.squirrel@217.245.71.241>
References: <200507250114.j6P1E7PO014769@hypatia.math.ethz.ch>
	<1281.217.245.71.241.1122281889.squirrel@217.245.71.241>
Message-ID: <200507260149.j6Q1nDDm031077@hypatia.math.ethz.ch>


thank you for the hints.  a matrix is and it isn't the right tool for 
the job.  it is really a data frame, not a matrix.  the columns have 
meanings.  actually, a more general data frame would make a nice 
feature for the future---as if the folks doing the R development did 
not have enough to do ;-).  one particular kind of sparse data set 
implementation would make sense, in which either rows or columns are 
assumed to be contiguous, and an internal R parameter determines the 
start and end.  I bet there are many data sets that take the form of 
data that is available for a a consecutive set of columns only.  the 
data structure implementation of such a data frame may not be too 
difficult, either.

regards,

/iaw
---
ivo welch



-----Original Message-----
From: roger at ysidro.econ.uiuc.edu
To: ivo_welch-rstat8303 at mailblocks.com
Cc: r-help at stat.math.ethz.ch
Sent: Mon, 25 Jul 2005 03:58:09 -0500 (CDT)
Subject: Re: [R] sparse data frame (crsp?)

You could try reading it in a chunk at a time and making a sparse
matrix with SparseM, unless you really need some functionality
of dataframes.

R Koenker



From chabotd at globetrotter.net  Tue Jul 26 04:46:13 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Mon, 25 Jul 2005 22:46:13 -0400
Subject: [R] grep help needed
Message-ID: <26B889CA-4A31-4161-B9D6-180471997C80@globetrotter.net>

Hi,

In another thread ("PBSmapping and shapefiles") I asked for an easy  
way to read "shapefiles" and transform them in data that PBSmapping  
could use. One person is exploring some ways of doing this, but it is  
possible I'll have to do this "manually".

With package "maptools" I am able to extract the information I need  
from a shapefile but it is formatted like this:

[[1]]
            [,1]     [,2]
[1,] -55.99805 51.68817
[2,] -56.00222 51.68911
[3,] -56.01694 51.68911
[4,] -56.03781 51.68606
[5,] -56.04639 51.68759
[6,] -56.04637 51.69445
[7,] -56.03777 51.70207
[8,] -56.02301 51.70892
[9,] -56.01317 51.71578
[10,] -56.00330 51.73481
[11,] -55.99805 51.73840
attr(,"pstart")
attr(,"pstart")$from
[1] 1

attr(,"pstart")$to
[1] 11

attr(,"nParts")
[1] 1
attr(,"shpID")
[1] NA

[[2]]
           [,1]     [,2]
[1,] -57.76294 50.88770
[2,] -57.76292 50.88693
[3,] -57.76033 50.88163
[4,] -57.75668 50.88091
[5,] -57.75551 50.88169
[6,] -57.75562 50.88550
[7,] -57.75932 50.88775
[8,] -57.76294 50.88770
attr(,"pstart")
attr(,"pstart")$from
[1] 1

attr(,"pstart")$to
[1] 8

attr(,"nParts")
[1] 1
attr(,"shpID")
[1] NA

I do not quite understand the structure of this data object (list of  
lists I think)
but at this point I resorted to printing it on the console and  
imported that text into Excel for further cleaning, which is easy  
enough. I'd like to complete the process within R to save time and to  
circumvent Excel's limit of around 64000 lines. But I have a hard  
time figuring out how to clean up this text in R.

What I need to produce for PBSmapping is a file where each block of  
coordinates shares one ID number, called PID, and a variable POS  
indicates the position of each coordinate within a "shape". All other  
lines must disappear. So the above would become:

PID POS X Y
1 1 -55.99805 51.68817
1 2 -56.00222 51.68911
1 3 -56.01694 51.68911
1 4 -56.03781 51.68606
1 5 -56.04639 51.68759
1 6 -56.04637 51.69445
1 7 -56.03777 51.70207
1 8 -56.02301 51.70892
1 9 -56.01317 51.71578
1 10 -56.00330 51.73481
1 11 -55.99805 51.73840
2 1 -57.76294 50.88770
2 2 -57.76292 50.88693
2 3 -57.76033 50.88163
2 4 -57.75668 50.88091
2 5 -57.75551 50.88169
2 6 -57.75562 50.88550
2 7 -57.75932 50.88775
2 8 -57.76294 50.88770

First I imported this text file into R:
test <- read.csv2("test file.txt",header=F, sep=";", colClasses =  
"character")

I used sep=";" to insure there would be only one variable in this  
file, as it contains no ";"

To remove lines that do not contain coordinates, I used the fact that  
longitudes are expressed as negative numbers, so with my very limited  
knowledge of grep searches, I thought of this, which is probably not  
the best way to go:

a <- rep("-", length(test$V1))
b <- grep(a, test$V1)

this gives me a warning ("Warning message:
the condition has length > 1 and only the first element will be used  
in: if (is.na(pattern)) {"
but seems to do what I need anyway

c <- seq(1, length(test$V1))
d <- c %in% b

e <- test$V1[d]

Partial victory, now I only have lines that look like
[1,] -57.76294 50.88770

But I don't know how to go further: the number in square brackets can  
be used for variable POS, after removing the square brackets and the  
comma, but this requires a better knowledge of grep than I have.  
Furthermore, I don't know how to add a PID (polygon ID) variable,  
i.e. all lines of a polygon must have the same ID, as in the example  
above (i.e. each time POS == 1, a new polygon starts and PID needs to  
be incremented by 1, and PID is kept constant for lines where POS ! 1).

Any help will be much appreciated.

Sincerely,

Denis Chabot



From paolo.radaelli at unimib.it  Tue Jul 26 10:06:40 2005
From: paolo.radaelli at unimib.it (Paolo Radaelli)
Date: Tue, 26 Jul 2005 10:06:40 +0200
Subject: [R] Hierarchical clustering with centroid method
Message-ID: <034201c591b8$f4609ef0$90788495@dimequant.unimib.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/07f94ab4/attachment.pl

From h.andersson at nioo.knaw.nl  Tue Jul 26 10:16:12 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Tue, 26 Jul 2005 10:16:12 +0200
Subject: [R] Plot zooming i.e. changing ylim according to xlim
Message-ID: <dc4rip$50i$1@sea.gmane.org>

Dear R-gurus,

I would like to zoom in a plot, e.g. I select a region on the x-axis and 
then I would like the ranges on the y-axis to change accordingly.

Is it possible to do this with existing functions, or do I have to 
invent some data selection before plotting?

See below a short example, where I select ylim with trial and error, 
which I want to avoid.

Cheers, Henrik Andersson
--------------------------------------------------------------------
## Example -- in reality more numbers, no function
x <- seq(0,20)
y <- exp(-x)

plot(y~x,type='l')

## Zoom in the end, to see what's happenning

plot(y~x,type='l',xlim=c(19,20))

## Try other ylim

plot(y~x,type='l',xlim=c(19,20),ylim=c(0,1))

## Not enough

plot(y~x,type='l',xlim=c(19,20),ylim=c(0,1E-8))

## Better
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From Tom.Mulholland at dpi.wa.gov.au  Tue Jul 26 11:07:07 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 26 Jul 2005 17:07:07 +0800
Subject: [R] Plot zooming i.e. changing ylim according to xlim
Message-ID: <4702645135092E4497088F71D9C8F51A128BD7@afhex01.dpi.wa.gov.au>

Search the archives for zoom and you will find plenty of answers on this question. 

RSiteSearch("zoom")

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Henrik Andersson
> Sent: Tuesday, 26 July 2005 4:16 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plot zooming i.e. changing ylim according to xlim
> 
> 
> Dear R-gurus,
> 
> I would like to zoom in a plot, e.g. I select a region on the 
> x-axis and 
> then I would like the ranges on the y-axis to change accordingly.
> 
> Is it possible to do this with existing functions, or do I have to 
> invent some data selection before plotting?
> 
> See below a short example, where I select ylim with trial and error, 
> which I want to avoid.
> 
> Cheers, Henrik Andersson
> --------------------------------------------------------------------
> ## Example -- in reality more numbers, no function
> x <- seq(0,20)
> y <- exp(-x)
> 
> plot(y~x,type='l')
> 
> ## Zoom in the end, to see what's happenning
> 
> plot(y~x,type='l',xlim=c(19,20))
> 
> ## Try other ylim
> 
> plot(y~x,type='l',xlim=c(19,20),ylim=c(0,1))
> 
> ## Not enough
> 
> plot(y~x,type='l',xlim=c(19,20),ylim=c(0,1E-8))
> 
> ## Better
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From h.andersson at nioo.knaw.nl  Tue Jul 26 11:13:06 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Tue, 26 Jul 2005 11:13:06 +0200
Subject: [R] Plot zooming i.e. changing ylim according to xlim
In-Reply-To: <4702645135092E4497088F71D9C8F51A128BD7@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128BD7@afhex01.dpi.wa.gov.au>
Message-ID: <dc4ute$dr5$1@sea.gmane.org>

>>Dear R-gurus,
>>
>>I would like to zoom in a plot, e.g. I select a region on the 
>>x-axis and 
>>then I would like the ranges on the y-axis to change accordingly.
>>
>>Is it possible to do this with existing functions, or do I have to 
>>invent some data selection before plotting?
>>
>>See below a short example, where I select ylim with trial and error, 
>>which I want to avoid.
Mulholland, Tom wrote:
 > Search the archives for zoom and you will find plenty of answers on 
this question.
 >
 > RSiteSearch("zoom")
 >
 > Tom

I looked there already, but I could only find interactive zooming, which 
  is also nice, but I want a non-interactive function that automagically 
changes the y-axis to fit only the data specified in the part of the 
x-axis, or the other way around. I guess that was not very clear from my 
first post.

- Henrik



From petr.pikal at precheza.cz  Tue Jul 26 11:16:41 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 26 Jul 2005 11:16:41 +0200
Subject: [R] Plot zooming i.e. changing ylim according to xlim
In-Reply-To: <dc4rip$50i$1@sea.gmane.org>
Message-ID: <42E61B99.18841.B961F2@localhost>

Hi

Not avoiding trial and error but you can do it interactively by point 
clicking on a plot.

> replot
function (x, y, type = "l") 
{
    body <- locator(2)
    plot(x, y, xlim = range(body$x), ylim = range(body$y), type = 
type)
}
>

HTH

Best regards
Petr Pikal



On 26 Jul 2005 at 10:16, Henrik Andersson wrote:

> Dear R-gurus,
> 
> I would like to zoom in a plot, e.g. I select a region on the x-axis
> and then I would like the ranges on the y-axis to change accordingly.
> 
> Is it possible to do this with existing functions, or do I have to
> invent some data selection before plotting?
> 
> See below a short example, where I select ylim with trial and error,
> which I want to avoid.
> 
> Cheers, Henrik Andersson
> --------------------------------------------------------------------
> ## Example -- in reality more numbers, no function x <- seq(0,20) y <-
> exp(-x)
> 
> plot(y~x,type='l')
> 
> ## Zoom in the end, to see what's happenning
> 
> plot(y~x,type='l',xlim=c(19,20))
> 
> ## Try other ylim
> 
> plot(y~x,type='l',xlim=c(19,20),ylim=c(0,1))
> 
> ## Not enough
> 
> plot(y~x,type='l',xlim=c(19,20),ylim=c(0,1E-8))
> 
> ## Better
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Jul 26 11:57:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Jul 2005 10:57:38 +0100 (BST)
Subject: [R] passing formula arguments cv.glm
In-Reply-To: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk>
References: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk>
Message-ID: <Pine.LNX.4.61.0507261049560.24483@gannet.stats>

Adai,

using traceback() helps, as does giving a reproducible example when 
reporting a problem.

However, the problem is I think the line

         d.glm <- update(glmfit, data = data[j.in, , drop = FALSE])

in cv.glm.  I think that should be

         d.glm <- eval.parent(update(glmfit,
                     data = data[j.in, , drop = FALSE], evaluate = FALSE))

as in add.default and many other places.

Could you please confirm that is the cause?

Brian

On Mon, 25 Jul 2005, Adaikalavan Ramasamy wrote:

> I am trying to write a wrapper for the last example in help(cv.glm) that
> deals with leave-one-out-cross-validation (LOOCV) for a logistic model.
> This wrapper will be used as part of a bigger program.
>
> Here is my wrapper funtion :
>
>   logistic.LOOCV.err <- function( formu=NULL, data=NULL ){
>
>     cost.fn <- function(cl, pred) mean( abs(cl-pred) > 0.5 )
>
>     glmfit <- glm( formula=formu, data=data, family="binomial" )
>     print("glmfit is OK")
>
>     err    <- cv.glm( data=data, glmfit=glmfit,
>                       cost=cost.fn, K=nrow(data) )$delta[2]
>     print("cv.glm OK")
>   }
>
>
> When I run the above function line by line with the arguments from
> below, it works fine. But when I call it as function, I get this :
>
>   rm( glmfit, formu, cv.err ) # cleanup if required
>   logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
>
> logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
> [1] "glmfit is OK"
> Error in model.frame(formula = formu, data = data[j.in, , drop =
> FALSE],  :
> 	Object "formu" not found
>
>
> I think this has something to do with formula and environments but I do
> not know enough to solve it myself. I searched the archive without much
> help (perhaps I was using the wrong keywords).
>
> Any help would be very much appreciated. Thank you.
>
> Regards,
> -- 
> Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
> Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
> Wolfson College Annexe                  Tel : 01865 284 408
> Linton Road, Oxford OX2 6UD             Fax : 01865 284 424
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Tue Jul 26 12:19:36 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 26 Jul 2005 12:19:36 +0200
Subject: [R] Plot zooming i.e. changing ylim according to xlim
In-Reply-To: <dc4ute$dr5$1@sea.gmane.org>
References: <4702645135092E4497088F71D9C8F51A128BD7@afhex01.dpi.wa.gov.au>
Message-ID: <42E62A58.31999.F2FCF4@localhost>

Well
 here is the better shot

x <- seq(0,20)
y <- exp(-x)
plot(x,y, type="l")
intervalx<-c(19,20)
intervaly<-y[x%in%intervalx]
plot(x,y, xlim=range(intervalx), ylim=range(intervaly), type="l")
 
HTH

Petr


On 26 Jul 2005 at 11:13, Henrik Andersson wrote:

> >>Dear R-gurus,
> >>
> >>I would like to zoom in a plot, e.g. I select a region on the 
> >>x-axis and 
> >>then I would like the ranges on the y-axis to change accordingly.
> >>
> >>Is it possible to do this with existing functions, or do I have to
> >>invent some data selection before plotting?
> >>
> >>See below a short example, where I select ylim with trial and error,
> >> which I want to avoid.
> Mulholland, Tom wrote:
>  > Search the archives for zoom and you will find plenty of answers on
>  
> this question.
>  >
>  > RSiteSearch("zoom")
>  >
>  > Tom
> 
> I looked there already, but I could only find interactive zooming,
> which 
>   is also nice, but I want a non-interactive function that
>   automagically 
> changes the y-axis to fit only the data specified in the part of the
> x-axis, or the other way around. I guess that was not very clear from
> my first post.
> 
> - Henrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From herodote at oreka.com  Tue Jul 26 12:22:35 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Tue, 26 Jul 2005 11:22:35 +0100
Subject: [R] =?iso-8859-1?q?sort_a_table?=
Message-ID: <IK8CTN$DCC3B64D25FE1F0EEF54B46DF58CA236@oreka.com>

hi all,

I need to sort a table like this one:

n tmp s
1 215 0
2 654 1
3 213 0
4 569 1
5 954 1
6 562 1
7 252 0
8 555 0
9 988 1

I want to organize it with tmp increasing to produce the same tab but ordered by tmp

I think it is simple but just show me how good you are in R ...

thks
guillaume.

////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From h.andersson at nioo.knaw.nl  Tue Jul 26 12:37:54 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Tue, 26 Jul 2005 12:37:54 +0200
Subject: [R] sort a table
In-Reply-To: <IK8CTN$DCC3B64D25FE1F0EEF54B46DF58CA236@oreka.com>
References: <IK8CTN$DCC3B64D25FE1F0EEF54B46DF58CA236@oreka.com>
Message-ID: <dc53se$r4p$1@sea.gmane.org>

herodote at oreka.com wrote:
> hi all,
> 
> I need to sort a table like this one:
> 
> n tmp s
> 1 215 0
> 2 654 1
> 3 213 0
> 4 569 1
> 5 954 1
> 6 562 1
> 7 252 0
> 8 555 0
> 9 988 1
> 
test <- read.table("clipboard",header=T)
test[order(test$tmp),]
   n tmp s
3 3 213 0
1 1 215 0
7 7 252 0
8 8 555 0
6 6 562 1
4 4 569 1
2 2 654 1
5 5 954 1
9 9 988 1


Happy?

Cheers, Henrik Andersson



From ray at mcs.vuw.ac.nz  Tue Jul 26 12:51:20 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 26 Jul 2005 22:51:20 +1200 (NZST)
Subject: [R] problem with Hershey fonts
Message-ID: <200507261051.j6QApK9M028246@tahi.mcs.vuw.ac.nz>

This was reported to me by a colleague in China, so I may not be
reproducing exactly what they are seeing (which I suspect is rw2011), but
this is what I see:
> version
	 _
platform i386--netbsdelf
arch     i386
os       netbsdelf
system   i386, netbsdelf
status
major    2
minor    1.1
year     2005
month    06
day      20
language R
> help(Hershey)
	:
	:
     If the 'vfont' argument to one of the text-drawing functions
     ('text', 'mtext', 'title', 'axis', and 'contour') is a character
     vector of length 2, Hershey vector fonts are used to render the
     text.
	:
          The other useful escape sequences all begin with '\\'.  These
          are described below. Remember that backslashes have to be
          doubled in R character strings, so they need to be entered
          with _four_ backslashes.
	:
	:
> plot(runif(100))	# to get something on the screen
> text(0, 1, '\\Re', vfont=c('serif', 'plain'))	# works
> title(main = '\\Re', vfont=c('serif','plain'))	# doesn't work, and...
Warning message:
parameter "vfont" could not be set in high-level plot() function
> mtext('\\Re', 2, vfont=c('serif','plain'))	# doesn't work, but...
Warning message:
Hershey fonts not yet implemented for mtext() in: mtext(text, side, line, outer, at, adj, padj, cex, col, font,
> axis(3, at=50, tick=F, labels='\\Re', vfont=c('serif','plain'))	# doesn't work 

Now at least mtext() 'redeems' itself with its Warning, but title()
contradicts the help(Hershey), and axis() just gets it wrong by printing
the characters "\Re" (or "\\Re" when the string is specified as
'\\\\Re' as alluded to in the Hershey documentation - but somewhat
contradicted by the success of the text() call above).

Now perhaps the problem is just with the Hershey documentation (or my
reading of it), but I don't know what is supposed to work.  Or is there
some combination that I haven't tried?

Ray Brownrigg



From ramasamy at cancer.org.uk  Tue Jul 26 12:57:19 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 26 Jul 2005 11:57:19 +0100
Subject: [R] passing formula arguments cv.glm
In-Reply-To: <Pine.LNX.4.61.0507261049560.24483@gannet.stats>
References: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk>
	<Pine.LNX.4.61.0507261049560.24483@gannet.stats>
Message-ID: <1122375440.6005.32.camel@ipc143004.lif.icnet.uk>

Dear Prof. Ripley,

Thank you for your response. See below for my comments.


On Tue, 2005-07-26 at 10:57 +0100, Prof Brian Ripley wrote:
> Adai,

> using traceback() helps, as does giving a reproducible example when 
> reporting a problem.

You are right. The traceback below indicates that your guess is correct.

 10: model.frame(formula = formu, data = data[j.in, , drop = FALSE],  
     drop.unused.levels = TRUE)


The example is actually reproducible but you need the load the boot
package first (which I failed to mention).

> However, the problem is I think the line
> 
>          d.glm <- update(glmfit, data = data[j.in, , drop = FALSE])
> 
> in cv.glm.  I think that should be
> 
>          d.glm <- eval.parent(update(glmfit,
>                      data = data[j.in, , drop = FALSE], evaluate = FALSE))
> 
> as in add.default and many other places.
> 
> Could you please confirm that is the cause?

I believe this is the cause but the fix may need bit more tweaking. I
get the following error message with your fix.

 Error in "[.data.frame"(data, j.in, , drop = FALSE) : 
  	Object "j.in" not found

The traceback gives the final same output as original one but this is
numbered at 12 instead. Could it be because "j.in" is created within the
for() loop environment ? I do not know how to fix this and would
appreciate any help. 

For your convenience, you can find the script with your current fix at
the following URL www.cbrg.ox.ac.uk/~ramasamy/cv.glm2.R

Thank you very much. Much appreciated.

Regards, Adai


> Brian
> 
> On Mon, 25 Jul 2005, Adaikalavan Ramasamy wrote:
> 
> > I am trying to write a wrapper for the last example in help(cv.glm) that
> > deals with leave-one-out-cross-validation (LOOCV) for a logistic model.
> > This wrapper will be used as part of a bigger program.
> >
> > Here is my wrapper funtion :
> >
> >   logistic.LOOCV.err <- function( formu=NULL, data=NULL ){
> >
> >     cost.fn <- function(cl, pred) mean( abs(cl-pred) > 0.5 )
> >
> >     glmfit <- glm( formula=formu, data=data, family="binomial" )
> >     print("glmfit is OK")
> >
> >     err    <- cv.glm( data=data, glmfit=glmfit,
> >                       cost=cost.fn, K=nrow(data) )$delta[2]
> >     print("cv.glm OK")
> >   }
> >
> >
> > When I run the above function line by line with the arguments from
> > below, it works fine. But when I call it as function, I get this :
> >
> >   rm( glmfit, formu, cv.err ) # cleanup if required
> >   logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
> >
> > logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
> > [1] "glmfit is OK"
> > Error in model.frame(formula = formu, data = data[j.in, , drop =
> > FALSE],  :
> > 	Object "formu" not found
> >
> >
> > I think this has something to do with formula and environments but I do
> > not know enough to solve it myself. I searched the archive without much
> > help (perhaps I was using the wrong keywords).
> >
> > Any help would be very much appreciated. Thank you.
> >
> > Regards,
> > -- 
> > Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
> > Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
> > Wolfson College Annexe                  Tel : 01865 284 408
> > Linton Road, Oxford OX2 6UD             Fax : 01865 284 424
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From 0034058 at fudan.edu.cn  Tue Jul 26 13:25:19 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 26 Jul 2005 19:25:19 +0800
Subject: [R] any package to fit such model?
Message-ID: <0IK800FJNFA44R@mail.fudan.edu.cn>

I have search the internet but none are found.The lme function is like the proc mixed the SAS.But I know no package to fit the model described in
http://gsbwww.uchicago.edu/computing/research/SASManual/ets/chap20/sect17.htm

,which  fit with the proc tscs in SAS.

Thank you.



From butchar.2 at osu.edu  Tue Jul 26 13:37:38 2005
From: butchar.2 at osu.edu (jon butchar)
Date: Tue, 26 Jul 2005 07:37:38 -0400
Subject: [R] OpenBSD and large R vectors
Message-ID: <20050726073738.7a46fc44.butchar.2@osu.edu>

Is anyone running R on OpenBSD?  I've got R-2.1.1 installed on OpenBSD -current and have some microarray datasets that took ~1.4 GB RAM on FreeBSD (computer has 4 GB total, Pentium 4 system).  With FreeBSD, setting maxdsiz in /boot/loader.conf worked very well.

With OpenBSD, even after setting maxdsiz to ~1.8 GB for a new kernel and changing /etc/login.conf, R runs into memory problems and gives the "could not allocate vector of..." error.  Then R locks up and starts taking progressively more CPU time until it gets killed.  This is a stock R installation and a generic OpenBSD except for the altered MAXDSIZ in vmparam.h.  I'm guessing that these problems stem at least in part from OpenBSD's memory allocation safety mechanisms.  Still, are there any R or OpenBSD compile options / tweaks I can do to make things work?

Thanks,

jon butchar



From ripley at stats.ox.ac.uk  Tue Jul 26 14:06:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Jul 2005 13:06:33 +0100 (BST)
Subject: [R] passing formula arguments cv.glm
In-Reply-To: <1122375440.6005.32.camel@ipc143004.lif.icnet.uk>
References: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk> 
	<Pine.LNX.4.61.0507261049560.24483@gannet.stats>
	<1122375440.6005.32.camel@ipc143004.lif.icnet.uk>
Message-ID: <Pine.LNX.4.61.0507261257000.1884@gannet.stats>

On Tue, 26 Jul 2005, Adaikalavan Ramasamy wrote:

> Dear Prof. Ripley,
>
> Thank you for your response. See below for my comments.
>
>
> On Tue, 2005-07-26 at 10:57 +0100, Prof Brian Ripley wrote:
>> Adai,
>
>> using traceback() helps, as does giving a reproducible example when
>> reporting a problem.
>
> You are right. The traceback below indicates that your guess is correct.
>
> 10: model.frame(formula = formu, data = data[j.in, , drop = FALSE],
>     drop.unused.levels = TRUE)
>
>
> The example is actually reproducible but you need the load the boot
> package first (which I failed to mention).
>
>> However, the problem is I think the line
>>
>>          d.glm <- update(glmfit, data = data[j.in, , drop = FALSE])
>>
>> in cv.glm.  I think that should be
>>
>>          d.glm <- eval.parent(update(glmfit,
>>                      data = data[j.in, , drop = FALSE], evaluate = FALSE))
>>
>> as in add.default and many other places.
>>
>> Could you please confirm that is the cause?
>
> I believe this is the cause but the fix may need bit more tweaking. I
> get the following error message with your fix.
>
> Error in "[.data.frame"(data, j.in, , drop = FALSE) :
>  	Object "j.in" not found
>
> The traceback gives the final same output as original one but this is
> numbered at 12 instead. Could it be because "j.in" is created within the
> for() loop environment ? I do not know how to fix this and would
> appreciate any help.

Ah, that needs a different fix, namely

         Call <- glmfit$call
         Call$data <- data[j.in, , drop=FALSE]
         d.glm <- eval.parent(Call)

which works for me when I modify your script accordingly.  (You can take 
the first line outside the loop, for tidiness.)


> For your convenience, you can find the script with your current fix at
> the following URL www.cbrg.ox.ac.uk/~ramasamy/cv.glm2.R
>
> Thank you very much. Much appreciated.
>
> Regards, Adai
>
>
>> Brian
>>
>> On Mon, 25 Jul 2005, Adaikalavan Ramasamy wrote:
>>
>>> I am trying to write a wrapper for the last example in help(cv.glm) that
>>> deals with leave-one-out-cross-validation (LOOCV) for a logistic model.
>>> This wrapper will be used as part of a bigger program.
>>>
>>> Here is my wrapper funtion :
>>>
>>>   logistic.LOOCV.err <- function( formu=NULL, data=NULL ){
>>>
>>>     cost.fn <- function(cl, pred) mean( abs(cl-pred) > 0.5 )
>>>
>>>     glmfit <- glm( formula=formu, data=data, family="binomial" )
>>>     print("glmfit is OK")
>>>
>>>     err    <- cv.glm( data=data, glmfit=glmfit,
>>>                       cost=cost.fn, K=nrow(data) )$delta[2]
>>>     print("cv.glm OK")
>>>   }
>>>
>>>
>>> When I run the above function line by line with the arguments from
>>> below, it works fine. But when I call it as function, I get this :
>>>
>>>   rm( glmfit, formu, cv.err ) # cleanup if required
>>>   logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
>>>
>>> logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
>>> [1] "glmfit is OK"
>>> Error in model.frame(formula = formu, data = data[j.in, , drop =
>>> FALSE],  :
>>> 	Object "formu" not found
>>>
>>>
>>> I think this has something to do with formula and environments but I do
>>> not know enough to solve it myself. I searched the archive without much
>>> help (perhaps I was using the wrong keywords).
>>>
>>> Any help would be very much appreciated. Thank you.
>>>
>>> Regards,
>>> --
>>> Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
>>> Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
>>> Wolfson College Annexe                  Tel : 01865 284 408
>>> Linton Road, Oxford OX2 6UD             Fax : 01865 284 424
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Tue Jul 26 14:48:35 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 26 Jul 2005 08:48:35 -0400
Subject: [R] grep help needed
In-Reply-To: <26B889CA-4A31-4161-B9D6-180471997C80@globetrotter.net>
Message-ID: <20050726124835.OLXI2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Denis,

I don't believe that anyone fielded your question -- my apologies if I
missed a response.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Denis Chabot
> Sent: Monday, July 25, 2005 9:46 PM
> To: R list
> Subject: [R] grep help needed
> 
> Hi,
> 
> In another thread ("PBSmapping and shapefiles") I asked for 
> an easy way to read "shapefiles" and transform them in data 
> that PBSmapping could use. One person is exploring some ways 
> of doing this, but it is possible I'll have to do this "manually".
> 
> With package "maptools" I am able to extract the information 
> I need from a shapefile but it is formatted like this:
> 
> [[1]]
>             [,1]     [,2]
> [1,] -55.99805 51.68817
> [2,] -56.00222 51.68911
> [3,] -56.01694 51.68911
> [4,] -56.03781 51.68606
> [5,] -56.04639 51.68759
> [6,] -56.04637 51.69445
> [7,] -56.03777 51.70207
> [8,] -56.02301 51.70892
> [9,] -56.01317 51.71578
> [10,] -56.00330 51.73481
> [11,] -55.99805 51.73840
> attr(,"pstart")
> attr(,"pstart")$from
> [1] 1
> 
> attr(,"pstart")$to
> [1] 11
> 
> attr(,"nParts")
> [1] 1
> attr(,"shpID")
> [1] NA
> 
> [[2]]
>            [,1]     [,2]
> [1,] -57.76294 50.88770
> [2,] -57.76292 50.88693
> [3,] -57.76033 50.88163
> [4,] -57.75668 50.88091
> [5,] -57.75551 50.88169
> [6,] -57.75562 50.88550
> [7,] -57.75932 50.88775
> [8,] -57.76294 50.88770
> attr(,"pstart")
> attr(,"pstart")$from
> [1] 1
> 
> attr(,"pstart")$to
> [1] 8
> 
> attr(,"nParts")
> [1] 1
> attr(,"shpID")
> [1] NA
> 
> I do not quite understand the structure of this data object 
> (list of lists I think)

Actually, it looks like a list of matrices, each with some attributes
(which, I gather, aren't of interest to you).

> but at this point I resorted to 
> printing it on the console and imported that text into Excel 
> for further cleaning, which is easy enough. I'd like to 
> complete the process within R to save time and to circumvent 
> Excel's limit of around 64000 lines. But I have a hard time 
> figuring out how to clean up this text in R.
> 

If I understand correctly what you want, this seems a very awkward way to
proceed. Why not just extract the matrices from the list, stick on the
additional columns that you want, stick the matrices together, name the
columns, and then output the data to a file?

M1 <- Data[[1]]  # assuming that the original list is named Data
M2 <- Data[[2]]
M1 <- cbind(1, 1:nrow(M1), M1)
M2 <- cbind(2, 1:nrow(M2), M2)
M <- rbind(M1, M2)
colnames(M) <- c("PID", "POS", "X", "Y")
write.table(M, "Data.txt", row.names=FALSE, quote=FALSE)

It wouldn't be hard to generalize this to any number of matrices and to
automate the process.

I hope that this helps,
 John

> What I need to produce for PBSmapping is a file where each 
> block of coordinates shares one ID number, called PID, and a 
> variable POS indicates the position of each coordinate within 
> a "shape". All other lines must disappear. So the above would become:
> 
> PID POS X Y
> 1 1 -55.99805 51.68817
> 1 2 -56.00222 51.68911
> 1 3 -56.01694 51.68911
> 1 4 -56.03781 51.68606
> 1 5 -56.04639 51.68759
> 1 6 -56.04637 51.69445
> 1 7 -56.03777 51.70207
> 1 8 -56.02301 51.70892
> 1 9 -56.01317 51.71578
> 1 10 -56.00330 51.73481
> 1 11 -55.99805 51.73840
> 2 1 -57.76294 50.88770
> 2 2 -57.76292 50.88693
> 2 3 -57.76033 50.88163
> 2 4 -57.75668 50.88091
> 2 5 -57.75551 50.88169
> 2 6 -57.75562 50.88550
> 2 7 -57.75932 50.88775
> 2 8 -57.76294 50.88770
> 
> First I imported this text file into R:
> test <- read.csv2("test file.txt",header=F, sep=";", colClasses =
> "character")
> 
> I used sep=";" to insure there would be only one variable in 
> this file, as it contains no ";"
> 
> To remove lines that do not contain coordinates, I used the 
> fact that longitudes are expressed as negative numbers, so 
> with my very limited knowledge of grep searches, I thought of 
> this, which is probably not the best way to go:
> 
> a <- rep("-", length(test$V1))
> b <- grep(a, test$V1)
> 
> this gives me a warning ("Warning message:
> the condition has length > 1 and only the first element will be used
> in: if (is.na(pattern)) {"
> but seems to do what I need anyway
> 
> c <- seq(1, length(test$V1))
> d <- c %in% b
> 
> e <- test$V1[d]
> 
> Partial victory, now I only have lines that look like [1,] 
> -57.76294 50.88770
> 
> But I don't know how to go further: the number in square 
> brackets can be used for variable POS, after removing the 
> square brackets and the comma, but this requires a better 
> knowledge of grep than I have.  
> Furthermore, I don't know how to add a PID (polygon ID) 
> variable, i.e. all lines of a polygon must have the same ID, 
> as in the example above (i.e. each time POS == 1, a new 
> polygon starts and PID needs to be incremented by 1, and PID 
> is kept constant for lines where POS ! 1).
> 
> Any help will be much appreciated.
> 
> Sincerely,
> 
> Denis Chabot



From navarre_sabine at yahoo.fr  Tue Jul 26 15:59:04 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Tue, 26 Jul 2005 15:59:04 +0200 (CEST)
Subject: [R] text on some lines
Message-ID: <20050726135904.74578.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/0b91495b/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jul 26 16:06:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Jul 2005 16:06:24 +0200
Subject: [R] text on some lines
In-Reply-To: <20050726135904.74578.qmail@web26604.mail.ukl.yahoo.com>
References: <20050726135904.74578.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42E64360.3070101@statistik.uni-dortmund.de>

Navarre Sabine wrote:

> Hi,
>  
> I would like to write text on 2 lines for example.
> For example, if you have a long sentence and you want to cut it at the 45 caracter and put the continuation underneath!
> Is it possible?

Do you mean for output to console / in plots?
Simply insert a "\n" (newline), for example.

Uwe Ligges


> Thanks
>  
> Sabine
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From 0034058 at fudan.edu.cn  Tue Jul 26 16:08:39 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 26 Jul 2005 22:08:39 +0800
Subject: [R] text on some lines
Message-ID: <0IK800FUDMUA4U@mail.fudan.edu.cn>

> plot(1:10,type="n")
> title(main="cut in the 45 char \n new line")
pay attention to the \n,which means new line.
is it what you want?
	

======= 2005-07-26 21:59:04 =======

>Hi,
> 
>I would like to write text on 2 lines for example.
>For example, if you have a long sentence and you want to cut it at the 45 caracter and put the continuation underneath!
>Is it possible?
> 
>Thanks
> 
>Sabine
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-07-26

------
Deparment of Sociology
Fudan University

Blog:http://sociology.yculblog.com



From sophie.tricaud at ccecqa.asso.fr  Tue Jul 26 16:18:48 2005
From: sophie.tricaud at ccecqa.asso.fr (Sophie Tricaud-Vialle)
Date: Tue, 26 Jul 2005 16:18:48 +0200
Subject: [R] Association rules
References: <20050726135904.74578.qmail@web26604.mail.ukl.yahoo.com>
	<42E64360.3070101@statistik.uni-dortmund.de>
Message-ID: <007901c591ec$f120b890$6ce438c1@chubordeaux.fr>

Hello !

I just start in using R, and I have already questions...

I want to use the arules package : I have installed the package, and have
the arules package 's reference manual, but I haven't yet understood how to
use it.

I have my data (from Excel, .txt), that I have read on R : I think (hope !)
it's ok
Then, what must I do first to analyse my data (to identify the association
rules) ? I suppose I have to "transform" my file, but how ? Which commands
must I use to ?

Thanks for all !

Sophie !



From ripley at stats.ox.ac.uk  Tue Jul 26 16:24:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Jul 2005 15:24:18 +0100 (BST)
Subject: [R] text on some lines
In-Reply-To: <42E64360.3070101@statistik.uni-dortmund.de>
References: <20050726135904.74578.qmail@web26604.mail.ukl.yahoo.com>
	<42E64360.3070101@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0507261518290.4240@gannet.stats>

On Tue, 26 Jul 2005, Uwe Ligges wrote:

> Navarre Sabine wrote:

>> I would like to write text on 2 lines for example.
>> For example, if you have a long sentence and you want to cut it
> at the 45 caracter and put the continuation underneath!
>> Is it possible?
>
> Do you mean for output to console / in plots?
> Simply insert a "\n" (newline), for example.

And strwrap() will help you insert it in the right place, e.g.

> x <- "For example, if you have a long sentence and you want to cut it at the 45 caracter and put the continuation underneath!"
> strwrap(x, 45)
[1] "For example, if you have a long sentence"
[2] "and you want to cut it at the 45 caracter"
[3] "and put the continuation underneath!"
> paste(strwrap(x, 45), collapse="\n")
[1] "For example, if you have a long sentence\nand you want to cut it at the 45 caracter\nand put the continuation underneath!"

the latter for use when plotting.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Tue Jul 26 16:36:37 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 26 Jul 2005 22:36:37 +0800
Subject: [R] Association rules
Message-ID: <0IK800F59O4W4H@mail.fudan.edu.cn>

>library(foreign)
in the foreign package,you can use read.csv command to read the csv file.you can use excel to open the file and save as cvs file.you should read the manuals first,which tells in details how to import your data into R.

before you use the command frome the arules package,you should first type the command
>library(arules)
	

======= 2005-07-26 22:18:48 =======

>Hello !
>
>I just start in using R, and I have already questions...
>
>I want to use the arules package : I have installed the package, and have
>the arules package 's reference manual, but I haven't yet understood how to
>use it.
>
>I have my data (from Excel, .txt), that I have read on R : I think (hope !)
>it's ok
>Then, what must I do first to analyse my data (to identify the association
>rules) ? I suppose I have to "transform" my file, but how ? Which commands
>must I use to ?
>
>Thanks for all !
>
>Sophie !
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-07-26

------
Deparment of Sociology
Fudan University

Blog:http://sociology.yculblog.com



From ramasamy at cancer.org.uk  Tue Jul 26 17:01:35 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 26 Jul 2005 16:01:35 +0100
Subject: [R] passing formula arguments cv.glm
In-Reply-To: <Pine.LNX.4.61.0507261257000.1884@gannet.stats>
References: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk>
	<Pine.LNX.4.61.0507261049560.24483@gannet.stats>
	<1122375440.6005.32.camel@ipc143004.lif.icnet.uk>
	<Pine.LNX.4.61.0507261257000.1884@gannet.stats>
Message-ID: <1122390095.6005.42.camel@ipc143004.lif.icnet.uk>

It works ! Thank you very much. 

Can I request this fix in the next version of boot package please if it
is likely not to break compatibility with other functions. The modified
cv.glm function can be found at www.cbrg.ox.ac.uk/~ramasamy/cv.glm2.R

Thank you again.

Regards, Adai



On Tue, 2005-07-26 at 13:06 +0100, Prof Brian Ripley wrote:
> On Tue, 26 Jul 2005, Adaikalavan Ramasamy wrote:
> 
> > Dear Prof. Ripley,
> >
> > Thank you for your response. See below for my comments.
> >
> >
> > On Tue, 2005-07-26 at 10:57 +0100, Prof Brian Ripley wrote:
> >> Adai,
> >
> >> using traceback() helps, as does giving a reproducible example when
> >> reporting a problem.
> >
> > You are right. The traceback below indicates that your guess is correct.
> >
> > 10: model.frame(formula = formu, data = data[j.in, , drop = FALSE],
> >     drop.unused.levels = TRUE)
> >
> >
> > The example is actually reproducible but you need the load the boot
> > package first (which I failed to mention).
> >
> >> However, the problem is I think the line
> >>
> >>          d.glm <- update(glmfit, data = data[j.in, , drop = FALSE])
> >>
> >> in cv.glm.  I think that should be
> >>
> >>          d.glm <- eval.parent(update(glmfit,
> >>                      data = data[j.in, , drop = FALSE], evaluate = FALSE))
> >>
> >> as in add.default and many other places.
> >>
> >> Could you please confirm that is the cause?
> >
> > I believe this is the cause but the fix may need bit more tweaking. I
> > get the following error message with your fix.
> >
> > Error in "[.data.frame"(data, j.in, , drop = FALSE) :
> >  	Object "j.in" not found
> >
> > The traceback gives the final same output as original one but this is
> > numbered at 12 instead. Could it be because "j.in" is created within the
> > for() loop environment ? I do not know how to fix this and would
> > appreciate any help.
> 
> Ah, that needs a different fix, namely
> 
>          Call <- glmfit$call
>          Call$data <- data[j.in, , drop=FALSE]
>          d.glm <- eval.parent(Call)
> 
> which works for me when I modify your script accordingly.  (You can take 
> the first line outside the loop, for tidiness.)
> 
> 
> > For your convenience, you can find the script with your current fix at
> > the following URL www.cbrg.ox.ac.uk/~ramasamy/cv.glm2.R
> >
> > Thank you very much. Much appreciated.
> >
> > Regards, Adai
> >
> >
> >> Brian
> >>
> >> On Mon, 25 Jul 2005, Adaikalavan Ramasamy wrote:
> >>
> >>> I am trying to write a wrapper for the last example in help(cv.glm) that
> >>> deals with leave-one-out-cross-validation (LOOCV) for a logistic model.
> >>> This wrapper will be used as part of a bigger program.
> >>>
> >>> Here is my wrapper funtion :
> >>>
> >>>   logistic.LOOCV.err <- function( formu=NULL, data=NULL ){
> >>>
> >>>     cost.fn <- function(cl, pred) mean( abs(cl-pred) > 0.5 )
> >>>
> >>>     glmfit <- glm( formula=formu, data=data, family="binomial" )
> >>>     print("glmfit is OK")
> >>>
> >>>     err    <- cv.glm( data=data, glmfit=glmfit,
> >>>                       cost=cost.fn, K=nrow(data) )$delta[2]
> >>>     print("cv.glm OK")
> >>>   }
> >>>
> >>>
> >>> When I run the above function line by line with the arguments from
> >>> below, it works fine. But when I call it as function, I get this :
> >>>
> >>>   rm( glmfit, formu, cv.err ) # cleanup if required
> >>>   logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
> >>>
> >>> logistic.LOOCV.err( formu=as.formula(r~stage+xray+acid), data=nodal )
> >>> [1] "glmfit is OK"
> >>> Error in model.frame(formula = formu, data = data[j.in, , drop =
> >>> FALSE],  :
> >>> 	Object "formu" not found
> >>>
> >>>
> >>> I think this has something to do with formula and environments but I do
> >>> not know enough to solve it myself. I searched the archive without much
> >>> help (perhaps I was using the wrong keywords).
> >>>
> >>> Any help would be very much appreciated. Thank you.
> >>>
> >>> Regards,
> >>> --
> >>> Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
> >>> Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
> >>> Wolfson College Annexe                  Tel : 01865 284 408
> >>> Linton Road, Oxford OX2 6UD             Fax : 01865 284 424
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Tue Jul 26 17:13:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Jul 2005 16:13:39 +0100 (BST)
Subject: [R] passing formula arguments cv.glm
In-Reply-To: <1122390095.6005.42.camel@ipc143004.lif.icnet.uk>
References: <1122290750.6005.82.camel@ipc143004.lif.icnet.uk> 
	<Pine.LNX.4.61.0507261049560.24483@gannet.stats> 
	<1122375440.6005.32.camel@ipc143004.lif.icnet.uk> 
	<Pine.LNX.4.61.0507261257000.1884@gannet.stats>
	<1122390095.6005.42.camel@ipc143004.lif.icnet.uk>
Message-ID: <Pine.LNX.4.61.0507261613040.4776@gannet.stats>

On Tue, 26 Jul 2005, Adaikalavan Ramasamy wrote:

> It works ! Thank you very much.
>
> Can I request this fix in the next version of boot package please if it
> is likely not to break compatibility with other functions. The modified
> cv.glm function can be found at www.cbrg.ox.ac.uk/~ramasamy/cv.glm2.R

Yes, it will be there if it survives enough tests.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ekhous at po-box.mcgill.ca  Tue Jul 26 17:34:53 2005
From: ekhous at po-box.mcgill.ca (ekhous@po-box.mcgill.ca)
Date: Tue, 26 Jul 2005 11:34:53 -0400
Subject: [R] SETAR Estimation
Message-ID: <1122392093.42e6581d9c506@webmail.mcgill.ca>

Dear R-helpers,

I was wondering if anyone has or knows someone who might have an implementation
of algorithm for estimating SETAR models including the lag-order. For some
reason my code gives me a bit wrong results. I am fighting with it for a week
and cannot bring it down.
Thanks a million in advance,
Sincerely,

Evgueni

McGill University
Department of Economics



From csae1552 at uibk.ac.at  Tue Jul 26 17:40:45 2005
From: csae1552 at uibk.ac.at (Hansi Weissensteiner)
Date: Tue, 26 Jul 2005 17:40:45 +0200
Subject: [R] farimaSim
In-Reply-To: <mailman.13.1122372001.29817.r-help@stat.math.ethz.ch>
References: <mailman.13.1122372001.29817.r-help@stat.math.ethz.ch>
Message-ID: <1122392445.42e6597dcfeed@web-mail1.uibk.ac.at>

Hello!

I installed the fSeries package to get some farima time-series which i tried
with farimaSim, but unfortunately i got always an error. I tried it this way:

> farimaSim(n = 1000, model = list(ar = 0.5,  d = 0.3, ma = 0.1), method="freq")

Error in farimaSim(n = 1000, model = list(ar = 0.5, d = 0.3, ma = 0.1),  :
 ... used in an incorrect context

Some ideas?

Regards,

    ___
 _ /_|_|   Hansi Weissensteiner
/o\__/O\=  csae1552 at uibk.ac.at



From nestor.fernandez at ufz.de  Tue Jul 26 17:48:51 2005
From: nestor.fernandez at ufz.de (Nestor Fernandez)
Date: Tue, 26 Jul 2005 17:48:51 +0200
Subject: [R] Assign new observations to Clara clusters
Message-ID: <1122392931.42e65b63eb7ef@webmail.ufz.de>

Dear all,

I need to assign new observations to cluster groups previously identified for a
different dataset. The original clustering was performed using Clara. I gess
one way is to assign each new observation to the nearest medioid of the
original cluster. Is there a way of doing this in R? Is there a better way of
classifying new observations into clusters?

Thanks a lot,

Nestor



From ripley at stats.ox.ac.uk  Tue Jul 26 18:11:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Jul 2005 17:11:48 +0100 (BST)
Subject: [R] Assign new observations to Clara clusters
In-Reply-To: <1122392931.42e65b63eb7ef@webmail.ufz.de>
References: <1122392931.42e65b63eb7ef@webmail.ufz.de>
Message-ID: <Pine.LNX.4.61.0507261707470.5499@gannet.stats>

On Tue, 26 Jul 2005, Nestor Fernandez wrote:

> Dear all,
>
> I need to assign new observations to cluster groups previously identified for a
> different dataset. The original clustering was performed using Clara. I gess
> one way is to assign each new observation to the nearest medioid of the
> original cluster. Is there a way of doing this in R? Is there a better way of
> classifying new observations into clusters?

If you have the medoids (and these are in the fit object), use knn1 (in 
package class) to assign.

Cluster analysis is not really designed to classify new observations. 
With Euclidean distance the above is possible, but I would probably take 
the clusters formed and used them to do a supervised classification: your 
default option is edited NN-1 classification and you may well be able to 
do better, depending on the size of the problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aliscla at yahoo.com  Tue Jul 26 18:41:20 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Tue, 26 Jul 2005 09:41:20 -0700 (PDT)
Subject: [R] Compute dissimilarity matrix for ordinal data
In-Reply-To: <1122392445.42e6597dcfeed@web-mail1.uibk.ac.at>
Message-ID: <20050726164120.95798.qmail@web61225.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/ebbb1243/attachment.pl

From maechler at stat.math.ethz.ch  Tue Jul 26 18:51:11 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Jul 2005 18:51:11 +0200
Subject: [R] farimaSim
In-Reply-To: <1122392445.42e6597dcfeed@web-mail1.uibk.ac.at>
References: <mailman.13.1122372001.29817.r-help@stat.math.ethz.ch>
	<1122392445.42e6597dcfeed@web-mail1.uibk.ac.at>
Message-ID: <17126.27135.562583.304828@stat.math.ethz.ch>

>>>>> "Hansi" == Hansi Weissensteiner <csae1552 at uibk.ac.at>
>>>>>     on Tue, 26 Jul 2005 17:40:45 +0200 writes:

    Hansi> Hello!  I installed the fSeries package to get some
    Hansi> farima time-series which i tried with farimaSim, but
    Hansi> unfortunately i got always an error. I tried it this
    Hansi> way:

    >> farimaSim(n = 1000, model = list(ar = 0.5, d = 0.3, ma =
    >> 0.1), method="freq")

    Hansi> Error in farimaSim(n = 1000, model = list(ar = 0.5, d
    Hansi> = 0.3, ma = 0.1), : ... used in an incorrect context

    Hansi> Some ideas?

Yes, the function farimaSim() is bogous, pretty obviously if you look
at it. 

I've wondered for a while if we (R developers) shouldn't
improve "R CMD check" so as to require that every exported
function in a package must have at least one running example.
For the farimaSim(), there is no call possible without an error
message.

Package 'fracdiff', though much older,  has a working
fracdiff.sim() function.

Martin Maechler, ETH Zurich



From dkf at specere.com  Tue Jul 26 03:58:34 2005
From: dkf at specere.com (dkf@specere.com)
Date: Tue, 26 Jul 2005 08:58:34 +0700 (ICT)
Subject: [R] plotting horizontally
Message-ID: <1608.203.118.106.215.1122343114.squirrel@mail.specere.com>


Hello,

Is there any way to use plot() horizontally similar to
boxplot(....., horiz=TRUE)?  I want to use to illustrate
the distribution of y-values on an adjacent plot using
layout().

Thanks in advance for any help.

Regards,

--Dan



From spencer.graves at pdf.com  Tue Jul 26 19:12:11 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 26 Jul 2005 10:12:11 -0700
Subject: [R] any package to fit such model?
In-Reply-To: <0IK800FJNFA44R@mail.fudan.edu.cn>
References: <0IK800FJNFA44R@mail.fudan.edu.cn>
Message-ID: <42E66EEB.3020308@pdf.com>

	  Have you looked at "?lm", including the examples?  The link in your
email described a standard one-way, fixed effects ANOVA, and the "lm"
help page includes a worked example for that.

	  spencer graves

ronggui wrote:

> I have search the internet but none are found.The lme function is like the proc mixed the SAS.But I know no package to fit the model described in
> http://gsbwww.uchicago.edu/computing/research/SASManual/ets/chap20/sect17.htm
> 
> ,which  fit with the proc tscs in SAS.
> 
> Thank you.
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From s.ingrassia at unical.it  Tue Jul 26 19:49:12 2005
From: s.ingrassia at unical.it (Salvatore Ingrassia)
Date: Tue, 26 Jul 2005 19:49:12 +0200
Subject: [R] draw ellipse of equal concentration
Message-ID: <00c201c5920a$588533e0$2201a8c0@INGRAXPRESARIO>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/4dcfb497/attachment.pl

From murdoch at stats.uwo.ca  Tue Jul 26 19:56:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 26 Jul 2005 13:56:36 -0400
Subject: [R] draw ellipse of equal concentration
In-Reply-To: <00c201c5920a$588533e0$2201a8c0@INGRAXPRESARIO>
References: <00c201c5920a$588533e0$2201a8c0@INGRAXPRESARIO>
Message-ID: <42E67954.1050400@stats.uwo.ca>

On 7/26/2005 1:49 PM, Salvatore Ingrassia wrote:
> Dear all,
> 
> do you know some routines to draw ellipses and its axes given the algebraic
> equation?  The application is to draw ellipses of equal concentration for
> bivariate normal distribution given the vector mean and the covariance matrix.

The ellipse package can draw ellipses.

Duncan Murdoch



From mark.salsburg at gmail.com  Tue Jul 26 20:07:26 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 26 Jul 2005 14:07:26 -0400
Subject: [R] tapply t.test
Message-ID: <dd48e20f0507261107125d69c1@mail.gmail.com>

I cannot find in the literature a way to conduct the following t.test
on 2 objects, A and B

A                                       B
col1 col2 col3                  col1 col2  col3

Where col(i)'s name is identical in both A and B (they are names of tissues).

How do I test (t.test) if each tissue across the object is
signifanctly different?? (i'm pretty sure I have to use tapply())


Also is there a way to multi plot all 89 tissues showing the A values
and the B values..

thank you



From peteoutside at yahoo.com  Tue Jul 26 20:28:36 2005
From: peteoutside at yahoo.com (Pete Cap)
Date: Tue, 26 Jul 2005 11:28:36 -0700 (PDT)
Subject: [R] FFT post-processing
In-Reply-To: <dd48e20f0507261107125d69c1@mail.gmail.com>
Message-ID: <20050726182836.56623.qmail@web52408.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/c30733ed/attachment.pl

From chrish at stats.ucl.ac.uk  Tue Jul 26 20:39:04 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Tue, 26 Jul 2005 19:39:04 +0100 (BST)
Subject: [R] cluster
In-Reply-To: <cdf817830507251445309dea83@mail.gmail.com>
References: <cdf817830507251445309dea83@mail.gmail.com>
Message-ID: <Pine.LNX.4.58.0507261924200.20587@egon.stats.ucl.ac.uk>

Dear Weiwei,

your question sounds a bit too general and complicated for the R-list.
Perhaps you should look for personal statistical advice.
The quality of methods (and especially distance choice) for down-sampling
ceratinly depends on the structure of the data set. I do not see at the moment why
you need any down-sampling at all, and you should find out first if and
why it's a good thing to do (by whatever method).

An obvious candidate for a clustering algorithm would be pam/clara in
package cluster, because this approach chooses points already in the data
set as cluster centroids (and produces therefore a proper subsample),
which does not apply to most other clustering methods.

However, in
 C. Hennig and L. J. Latecki:  The choice of vantage objects for image
retrieval.  Pattern Recognition 36 (2003), 2187-2196.
the clustering approach has been clearly outperformed by some stepwise
selection approaches for down-sampling - admittedly in a different kind of
problem, but I think that the reasons for this may apply also to your
situation,

You can compare different clusterings (or choices of a subset) by
cross-validation or
bootstrap applied to the resulting decision tree in the classification
problem.

Best,
Christian


On Mon, 25 Jul 2005, Weiwei Shi wrote:

> Dear listers:
>
> Here I have a question on clustering methods available in R. I am
> trying to down-sampling the majority class in a classification problem
> on an imbalanced dataset. Since I don't want to lose information in
> the original dataset, I don't want to use naive down-sampling: I think
> using clustering on the majority class' side to select
> "representative" samples might help. So, my question is, which
> clustering method should be tested to get the best result. I think the
> key thing might be the selection of "distance" considering the next
> step in which I would like to use  decision trees.
>
> Please share your experience in using clustering (Any available
> implementation outside R is also welcome)
>
> weiwei
> --
> Weiwei Shi, Ph.D
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** NEW ADDRESS! ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From greg.snow at ihc.com  Tue Jul 26 20:48:10 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 26 Jul 2005 12:48:10 -0600
Subject: [R] alaska map?
Message-ID: <s2e63111.068@lp-msg1.co.ihc.com>

Look at:

http://www.census.gov/geo/www/cob/bdy_files.html

There are shapefiles of the 50 states there, outlines, counties, and
others. 



Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Caitlin Burgess <caitlin1 at u.washington.edu> 07/25/05 01:37PM >>>
Hello,

I've installed the Becker and Wilks maps, mapdata, and mapproj
packages
so I can begin to try these out for some work I need to do on a map of
Alaska but I don't know where to find a map of Alaska. Has anyone
solved
this already and could help?

Thanks very much in advance,

Caitlin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gattuso2 at obs-vlfr.fr  Tue Jul 26 20:52:23 2005
From: gattuso2 at obs-vlfr.fr (Jean-Pierre Gattuso)
Date: Tue, 26 Jul 2005 12:52:23 -0600
Subject: [R] error with scan
Message-ID: <4C886C38-8710-479F-91AE-C1FF655CBE87@obs-vlfr.fr>

Hi:

I am trying to read a large (500000+ lines) with scan() as read.table  
is unable to read it.

I get a strange error (below) which says that 'a real' was expected  
and '5' was read. Can someone help?

Thanks,
jp

 > type=list(a=0,b=0,c=0,d=0,e=0,f="",g=0,h=0,i=0)
 > tmp2 <- scan(file="tmp2.txt", what=type, sep=",", quote="\"",  
dec=".", skip=1, nmax=541502)
Erreur dans scan(file = "tmp2.txt", what = type, sep = ",", quote =  
"\"",  :
     scan() attendait 'a real' et a re??u '5'



From mark.salsburg at gmail.com  Tue Jul 26 20:53:42 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 26 Jul 2005 14:53:42 -0400
Subject: [R] T test across tissues
Message-ID: <dd48e20f0507261153fedb3ed@mail.gmail.com>

I cannot find in the literature a way to conduct the following t.test
on 2 objects, A and B

A                                       B
col1 col2 col3                  col1 col2  col3

Where col(i)'s name is identical in both A and B (they are names of tissues).

How do I test (t.test) if each tissue across the object is
signifanctly different?? (i'm pretty sure I have to use tapply())


Also is there a way to multi plot all 89 tissues showing the A values
and the B values..

thank you



From mark.salsburg at gmail.com  Tue Jul 26 20:53:42 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 26 Jul 2005 14:53:42 -0400
Subject: [R] T test across tissues
Message-ID: <dd48e20f0507261153fedb3ed@mail.gmail.com>

I cannot find in the literature a way to conduct the following t.test
on 2 objects, A and B

A                                       B
col1 col2 col3                  col1 col2  col3

Where col(i)'s name is identical in both A and B (they are names of tissues).

How do I test (t.test) if each tissue across the object is
signifanctly different?? (i'm pretty sure I have to use tapply())


Also is there a way to multi plot all 89 tissues showing the A values
and the B values..

thank you



From deepayan.sarkar at gmail.com  Tue Jul 26 21:02:38 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 26 Jul 2005 14:02:38 -0500
Subject: [R] Plot zooming i.e. changing ylim according to xlim
In-Reply-To: <dc4rip$50i$1@sea.gmane.org>
References: <dc4rip$50i$1@sea.gmane.org>
Message-ID: <eb555e66050726120266c67b07@mail.gmail.com>

On 7/26/05, Henrik Andersson <h.andersson at nioo.knaw.nl> wrote:
> Dear R-gurus,
> 
> I would like to zoom in a plot, e.g. I select a region on the x-axis and
> then I would like the ranges on the y-axis to change accordingly.
> 
> Is it possible to do this with existing functions, or do I have to
> invent some data selection before plotting?
> 
> See below a short example, where I select ylim with trial and error,
> which I want to avoid.
> 
> Cheers, Henrik Andersson
> --------------------------------------------------------------------
> ## Example -- in reality more numbers, no function
> x <- seq(0,20)
> y <- exp(-x)
> 
> plot(y~x,type='l')
> 
> ## Zoom in the end, to see what's happenning
> 
> plot(y~x,type='l',xlim=c(19,20))

I tend to use constructs like 

plot(y~x,type='l', subset = x > 15)

(18 is not very interesting).

Deepayan



From lma5 at ncsu.edu  Tue Jul 26 21:18:06 2005
From: lma5 at ncsu.edu (lma5@ncsu.edu)
Date: Tue, 26 Jul 2005 15:18:06 -0400 (EDT)
Subject: [R] a question about fft ( fast fourier transform)
Message-ID: <2401.152.14.74.126.1122405486.squirrel@webmail.ncsu.edu>


Dear listers

 In R, if I have a sequence x(t), t=1,...N, fft(x) is actually giving us
sum(x(t)exp(-i*omega*t)) at fourier frequency omega= 2*pi*i/N,
i=0,1,...(N-1).

The question is if I want to calculate sum(x(t)exp(-i*2*omega*t)), how can
I do it?
thanks a lot!


-------------------------------
liyun (Lauren) Ma
Dept of Statistics
North Carolina State University



From mark.salsburg at gmail.com  Tue Jul 26 21:28:24 2005
From: mark.salsburg at gmail.com (mark salsburg)
Date: Tue, 26 Jul 2005 15:28:24 -0400
Subject: [R] Help on T test
Message-ID: <dd48e20f050726122840e92c1c@mail.gmail.com>

ok I created a matrix C with 

A   B  C    A1   B1    C1
..................................
.................................

the columns contain the gene expression values..

I ran the following t.test:

apply(C, 1, function(x) t.test( x[1:3], x[4,6] )$p.value )

which outputs out 16063 pvalues (the number of rows)

I just want to output 3 pvalues showing if A's column is different from A1 etc..

any help would be great.. thank you..



From chabotd at globetrotter.net  Tue Jul 26 21:35:51 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Tue, 26 Jul 2005 15:35:51 -0400
Subject: [R] grep help needed
In-Reply-To: <4702645135092E4497088F71D9C8F51A128BD4@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128BD4@afhex01.dpi.wa.gov.au>
Message-ID: <27912AB0-BE1E-40DB-A6EE-18E6CDFADF24@globetrotter.net>

Thanks for your help, the proposed solutions were much more elegant  
than what I was attempting. I adopted a slight modification of Tom  
Mulholland's solution with a piece from John Fox's solution, but many  
of you had very similar solutions.

require(maptools)
nc <- read.shape(system.file("shapes/sids.shp", package = "maptools") 
[1])
mappolys <- Map2poly(nc, as.character(nc$att.data$FIPSNO))
selected.shapes <- which(nc$att.data$SID74 > 20)
# just to make it a smaller example
submap <- subset(mappolys, nc$att.data$SID74 > 20)

final.data <- NULL
for (j in 1:length(selected.shapes)){
     temp.verts <- matrix(as.vector(submap[[j]]),ncol = 2)
     n <- length(temp.verts[,1])
     temp.order <- 1:n
     temp.data <- cbind(rep(j,n),temp.order,temp.verts)
     final.data <- rbind(final.data,temp.data)
     }
colnames(final.data) <- c("PID", "POS", "X", "Y")
final.data
my.data <- as.data.frame(final.data)
class(my.data) <- c("PolySet", "data.frame")
attr(my.data, "projection") <- "LL"

meta <- nc[2]$att.data[selected.shapes,]
PID <- seq(1,length(submap))
meta.data <- cbind(PID, meta)
class(meta.data) <- c("PolyData", "data.frame")
attr(meta.data, "projection") <- "LL"

It would be nice if a variant of this was incorporated into  
PBSmapping to make it easier to import data from shapefiles!

Thanks again for your help,

Denis Chabot
Le 05-07-26 ?? 00:48, Mulholland, Tom a ??crit :

>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Denis Chabot
>> Sent: Tuesday, 26 July 2005 10:46 AM
>> To: R list
>> Subject: [R] grep help needed
>>
>>
>> Hi,
>>
>> In another thread ("PBSmapping and shapefiles") I asked for an easy
>> way to read "shapefiles" and transform them in data that PBSmapping
>> could use. One person is exploring some ways of doing this,
>> but it is
>> possible I'll have to do this "manually".
>>
>> With package "maptools" I am able to extract the information I need
>> from a shapefile but it is formatted like this:
>>
>> [[1]]
>>             [,1]     [,2]
>> [1,] -55.99805 51.68817
>> [2,] -56.00222 51.68911
>> [3,] -56.01694 51.68911
>> [4,] -56.03781 51.68606
>> [5,] -56.04639 51.68759
>> [6,] -56.04637 51.69445
>> [7,] -56.03777 51.70207
>> [8,] -56.02301 51.70892
>> [9,] -56.01317 51.71578
>> [10,] -56.00330 51.73481
>> [11,] -55.99805 51.73840
>> attr(,"pstart")
>> attr(,"pstart")$from
>> [1] 1
>>
>> attr(,"pstart")$to
>> [1] 11
>>
>> attr(,"nParts")
>> [1] 1
>> attr(,"shpID")
>> [1] NA
>>
>> [[2]]
>>            [,1]     [,2]
>> [1,] -57.76294 50.88770
>> [2,] -57.76292 50.88693
>> [3,] -57.76033 50.88163
>> [4,] -57.75668 50.88091
>> [5,] -57.75551 50.88169
>> [6,] -57.75562 50.88550
>> [7,] -57.75932 50.88775
>> [8,] -57.76294 50.88770
>> attr(,"pstart")
>> attr(,"pstart")$from
>> [1] 1
>>
>> attr(,"pstart")$to
>> [1] 8
>>
>> attr(,"nParts")
>> [1] 1
>> attr(,"shpID")
>> [1] NA
>>
>> I do not quite understand the structure of this data object (list of
>> lists I think)
>> but at this point I resorted to printing it on the console and
>> imported that text into Excel for further cleaning, which is easy
>> enough. I'd like to complete the process within R to save
>> time and to
>> circumvent Excel's limit of around 64000 lines. But I have a hard
>> time figuring out how to clean up this text in R.
>>
>> What I need to produce for PBSmapping is a file where each block of
>> coordinates shares one ID number, called PID, and a variable POS
>> indicates the position of each coordinate within a "shape".
>> All other
>> lines must disappear. So the above would become:
>>
>> PID POS X Y
>> 1 1 -55.99805 51.68817
>> 1 2 -56.00222 51.68911
>> 1 3 -56.01694 51.68911
>> 1 4 -56.03781 51.68606
>> 1 5 -56.04639 51.68759
>> 1 6 -56.04637 51.69445
>> 1 7 -56.03777 51.70207
>> 1 8 -56.02301 51.70892
>> 1 9 -56.01317 51.71578
>> 1 10 -56.00330 51.73481
>> 1 11 -55.99805 51.73840
>> 2 1 -57.76294 50.88770
>> 2 2 -57.76292 50.88693
>> 2 3 -57.76033 50.88163
>> 2 4 -57.75668 50.88091
>> 2 5 -57.75551 50.88169
>> 2 6 -57.75562 50.88550
>> 2 7 -57.75932 50.88775
>> 2 8 -57.76294 50.88770
>>
>> First I imported this text file into R:
>> test <- read.csv2("test file.txt",header=F, sep=";", colClasses =
>> "character")
>>
>> I used sep=";" to insure there would be only one variable in this
>> file, as it contains no ";"
>>
>> To remove lines that do not contain coordinates, I used the
>> fact that
>> longitudes are expressed as negative numbers, so with my very
>> limited
>> knowledge of grep searches, I thought of this, which is probably not
>> the best way to go:
>>
>> a <- rep("-", length(test$V1))
>> b <- grep(a, test$V1)
>>
>> this gives me a warning ("Warning message:
>> the condition has length > 1 and only the first element will be used
>> in: if (is.na(pattern)) {"
>> but seems to do what I need anyway
>>
>> c <- seq(1, length(test$V1))
>> d <- c %in% b
>>
>> e <- test$V1[d]
>>
>> Partial victory, now I only have lines that look like
>> [1,] -57.76294 50.88770
>>
>> But I don't know how to go further: the number in square
>> brackets can
>> be used for variable POS, after removing the square brackets and the
>> comma, but this requires a better knowledge of grep than I have.
>> Furthermore, I don't know how to add a PID (polygon ID) variable,
>> i.e. all lines of a polygon must have the same ID, as in the example
>> above (i.e. each time POS == 1, a new polygon starts and PID
>> needs to
>> be incremented by 1, and PID is kept constant for lines where
>> POS ! 1).
>>
>> Any help will be much appreciated.
>>
>> Sincerely,
>>
>> Denis Chabot
>



From helprhelp at gmail.com  Tue Jul 26 21:38:07 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 26 Jul 2005 14:38:07 -0500
Subject: [R] cluster
In-Reply-To: <Pine.LNX.4.58.0507261924200.20587@egon.stats.ucl.ac.uk>
References: <cdf817830507251445309dea83@mail.gmail.com>
	<Pine.LNX.4.58.0507261924200.20587@egon.stats.ucl.ac.uk>
Message-ID: <cdf8178305072612382b065604@mail.gmail.com>

Dear Chris:

You are right and It IS too general. I think I should ask like "what
kind of cluster algorithms or functions are available in R" , which
might be easier. But for that, I probably can google or use help() in
R to find out. I want to know more about the performance of clustering
on this kind of problems and hope someone can share previous experince
if he/she had similar situation or problems before. And I will share
my experience later :)

As to the reason of using downsampling here, it is one fo the
straightforward ways to deal with imbalanced data classification
problem. In my understanding of classification problems, among others,
two things are important: feature construction/selection and sample
selection. I had an idea (which might be discovered by others) that
finding the best subset of features in clustering (to get highest
inter-cluster dissimilarities and the largest intra-cluster
similarity) might help the next classification process. I quickly read
through the abstract of your paper and I think your approach here is
applying feature selection (use p instead of n), while here, in my
proposal, I would like to try both.

thanks for further advice!

weiwei

On 7/26/05, Christian Hennig <chrish at stats.ucl.ac.uk> wrote:
> Dear Weiwei,
> 
> your question sounds a bit too general and complicated for the R-list.
> Perhaps you should look for personal statistical advice.
> The quality of methods (and especially distance choice) for down-sampling
> ceratinly depends on the structure of the data set. I do not see at the moment why
> you need any down-sampling at all, and you should find out first if and
> why it's a good thing to do (by whatever method).
> 
> An obvious candidate for a clustering algorithm would be pam/clara in
> package cluster, because this approach chooses points already in the data
> set as cluster centroids (and produces therefore a proper subsample),
> which does not apply to most other clustering methods.
> 
> However, in
>  C. Hennig and L. J. Latecki:  The choice of vantage objects for image
> retrieval.  Pattern Recognition 36 (2003), 2187-2196.
> the clustering approach has been clearly outperformed by some stepwise
> selection approaches for down-sampling - admittedly in a different kind of
> problem, but I think that the reasons for this may apply also to your
> situation,
> 
> You can compare different clusterings (or choices of a subset) by
> cross-validation or
> bootstrap applied to the resulting decision tree in the classification
> problem.
> 
> Best,
> Christian
> 
> 
> On Mon, 25 Jul 2005, Weiwei Shi wrote:
> 
> > Dear listers:
> >
> > Here I have a question on clustering methods available in R. I am
> > trying to down-sampling the majority class in a classification problem
> > on an imbalanced dataset. Since I don't want to lose information in
> > the original dataset, I don't want to use naive down-sampling: I think
> > using clustering on the majority class' side to select
> > "representative" samples might help. So, my question is, which
> > clustering method should be tested to get the best result. I think the
> > key thing might be the selection of "distance" considering the next
> > step in which I would like to use  decision trees.
> >
> > Please share your experience in using clustering (Any available
> > implementation outside R is also welcome)
> >
> > weiwei
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> *** NEW ADDRESS! ***
> Christian Hennig
> University College London, Department of Statistical Science
> Gower St., London WC1E 6BT, phone +44 207 679 1698
> chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From bartzk at yahoo-inc.com  Tue Jul 26 21:35:59 2005
From: bartzk at yahoo-inc.com (Kevin Bartz)
Date: Tue, 26 Jul 2005 12:35:59 -0700
Subject: [R] error with scan
Message-ID: <E4EBBAD66D826C41BDED4CCCDCC6D0F19BC6E3@EXCHG01-BUR>

Can you show us the first line of the file?

The error means that in one of the values you specified as numeric (first, second, third, fourth, fifth, seventh, eighth, ninth), it found the character value it displayed.

Otherwise, this looks like a good use of scan.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jean-Pierre Gattuso
Sent: Tuesday, July 26, 2005 11:52 AM
To: r-help at stat.math.ethz.ch
Cc: Jean-Pierre Gattuso
Subject: [R] error with scan

Hi:

I am trying to read a large (500000+ lines) with scan() as read.table  
is unable to read it.

I get a strange error (below) which says that 'a real' was expected  
and '5' was read. Can someone help?

Thanks,
jp

 > type=list(a=0,b=0,c=0,d=0,e=0,f="",g=0,h=0,i=0)
 > tmp2 <- scan(file="tmp2.txt", what=type, sep=",", quote="\"",  
dec=".", skip=1, nmax=541502)
Erreur dans scan(file = "tmp2.txt", what = type, sep = ",", quote =  
"\"",  :
     scan() attendait 'a real' et a re??u '5
'

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Tue Jul 26 21:47:43 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 26 Jul 2005 14:47:43 -0500
Subject: [R] plotting horizontally
In-Reply-To: <1608.203.118.106.215.1122343114.squirrel@mail.specere.com>
References: <1608.203.118.106.215.1122343114.squirrel@mail.specere.com>
Message-ID: <1122407264.4326.26.camel@localhost.localdomain>

On Tue, 2005-07-26 at 08:58 +0700, dkf at specere.com wrote:
> Hello,
> 
> Is there any way to use plot() horizontally similar to
> boxplot(....., horiz=TRUE)?  I want to use to illustrate
> the distribution of y-values on an adjacent plot using
> layout().
> 
> Thanks in advance for any help.
> 
> Regards,
> 
> --Dan

Have you looked at the last example in ?layout, which has a scatterplot
with marginal histograms for the x and y axes?

Alternatively, you can generally reverse the x and y values for most
plots. For example, compare:

 d <- density(rnorm(100))

 # Vertical density plot
 plot(d)

 # Horizontal density plot 
 plot(d$y, d$x, type = "l", xlab = "Density", 
      main = "density(y = rnorm(100))",
      ylab = paste("N =", d$n, "  Bandwidth =", formatC(d$bw)))


HTH,

Marc Schwartz



From jhainm at fas.harvard.edu  Tue Jul 26 21:48:31 2005
From: jhainm at fas.harvard.edu (jhainm@fas.harvard.edu)
Date: Tue, 26 Jul 2005 15:48:31 -0400
Subject: [R] elegant solution to transform vector into percentages?
Message-ID: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>

Hi,

I am looking for an elegant way to transform a vector into percentages of values
that meet certain criteria.

store<-c(1,1.4,3,1.1,0.3,0.6,4,5)

# now I want to get the precentages of values
# that fall into the categories <=M , >M & <=N , >N
# let
M <-.8
N <- 1.2
# In my real example I have many more of these cutoff-points

# What I did is:

out <- matrix(NA,1,3)

  out[1,1] <- ( (sum(store<=M                ))  /length(store) )*100
  out[1,2] <- ( (sum(store> M  & store<= N   ))  /length(store) )*100
  out[1,3] <- ( (sum(store> N                ))  /length(store) )*100

colnames(out)<-c("percent<=M","percent>M & <=N","percent>N")
out

But this gets very tedious if I have many cutoff-points. Does anybody know a
more elegant way to do this task?

Thanks so much.

Cheers,
Jens



From kerryrekky at yahoo.com  Tue Jul 26 21:54:24 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Tue, 26 Jul 2005 12:54:24 -0700 (PDT)
Subject: [R] choose between dates and times
Message-ID: <20050726195424.44925.qmail@web51801.mail.yahoo.com>

Dear R-helpers,
  I have the following data:

 y    happenat                 x
5185 (07/22/05 00:05:14)       14
5186 (07/22/05 00:15:14)       14
5187 (07/22/05 00:25:14)       14
5188 (07/22/05 00:35:14)       14
......

I want to choose between 07/25/05 15:30:00 and
07/26/05 12:30:00. Anybody had experience in handling
this kind of data? Is there a simple way to subset by
the variable 'happenat'? Thanks.



From reid_huntsinger at merck.com  Tue Jul 26 21:57:47 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 26 Jul 2005 15:57:47 -0400
Subject: [R] elegant solution to transform vector into percentages?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9545@uswpmx00.merck.com>

hist() or cut() followed by tabulate() would probably be the ingredients
you'd want.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
jhainm at fas.harvard.edu
Sent: Tuesday, July 26, 2005 3:49 PM
To: r-help at stat.math.ethz.ch
Subject: [R] elegant solution to transform vector into percentages?


Hi,

I am looking for an elegant way to transform a vector into percentages of
values
that meet certain criteria.

store<-c(1,1.4,3,1.1,0.3,0.6,4,5)

# now I want to get the precentages of values
# that fall into the categories <=M , >M & <=N , >N
# let
M <-.8
N <- 1.2
# In my real example I have many more of these cutoff-points

# What I did is:

out <- matrix(NA,1,3)

  out[1,1] <- ( (sum(store<=M                ))  /length(store) )*100
  out[1,2] <- ( (sum(store> M  & store<= N   ))  /length(store) )*100
  out[1,3] <- ( (sum(store> N                ))  /length(store) )*100

colnames(out)<-c("percent<=M","percent>M & <=N","percent>N")
out

But this gets very tedious if I have many cutoff-points. Does anybody know a
more elegant way to do this task?

Thanks so much.

Cheers,
Jens

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jholtman at gmail.com  Tue Jul 26 21:59:23 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 26 Jul 2005 15:59:23 -0400
Subject: [R] elegant solution to transform vector into percentages?
In-Reply-To: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>
References: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>
Message-ID: <644e1f32050726125965d610e5@mail.gmail.com>

use 'cut':

> store<-c(1,1.4,3,1.1,0.3,0.6,4,5)
> x.1 <- cut(store, breaks=c(-Inf,.8,1.2,Inf))
> table(x.1)/length(x.1)*100
x.1
(-Inf,0.8]  (0.8,1.2]  (1.2,Inf] 
        25         25         50 
> 

On 7/26/05, jhainm at fas.harvard.edu <jhainm at fas.harvard.edu> wrote:
> Hi,
> 
> I am looking for an elegant way to transform a vector into percentages of values
> that meet certain criteria.
> 
> store<-c(1,1.4,3,1.1,0.3,0.6,4,5)
> 
> # now I want to get the precentages of values
> # that fall into the categories <=M , >M & <=N , >N
> # let
> M <-.8
> N <- 1.2
> # In my real example I have many more of these cutoff-points
> 
> # What I did is:
> 
> out <- matrix(NA,1,3)
> 
>  out[1,1] <- ( (sum(store<=M                ))  /length(store) )*100
>  out[1,2] <- ( (sum(store> M  & store<= N   ))  /length(store) )*100
>  out[1,3] <- ( (sum(store> N                ))  /length(store) )*100
> 
> colnames(out)<-c("percent<=M","percent>M & <=N","percent>N")
> out
> 
> But this gets very tedious if I have many cutoff-points. Does anybody know a
> more elegant way to do this task?
> 
> Thanks so much.
> 
> Cheers,
> Jens
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Jim Holtman

What the problem you are trying to solve?



From W.E.Wolski at newcastle.ac.uk  Tue Jul 26 22:02:41 2005
From: W.E.Wolski at newcastle.ac.uk (nwew)
Date: Tue, 26 Jul 2005 21:02:41 +0100
Subject: [R] / Right division.
Message-ID: <42E0F1D7@webmail.ncl.ac.uk>

Dear R gurus.

Is there an R function equivalent to octaves / (Right division)
withouth forming the inverse of Y' using solve ?

[snip - from octave docu]

 Right division.  This is conceptually equivalent to the expression

          (inverse (y') * x')'

     but it is computed without forming the inverse of Y'.

     If the system is not square, or if the coefficient matrix is
     singular, a minimum norm solution is computed.

[snip]

Thanks
Eryk



From murdoch at stats.uwo.ca  Tue Jul 26 22:15:15 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 26 Jul 2005 16:15:15 -0400
Subject: [R] / Right division.
In-Reply-To: <42E0F1D7@webmail.ncl.ac.uk>
References: <42E0F1D7@webmail.ncl.ac.uk>
Message-ID: <42E699D3.8010906@stats.uwo.ca>

On 7/26/2005 4:02 PM, nwew wrote:
> Dear R gurus.
> 
> Is there an R function equivalent to octaves / (Right division)
> withouth forming the inverse of Y' using solve ?
> 
> [snip - from octave docu]
> 
>  Right division.  This is conceptually equivalent to the expression
> 
>           (inverse (y') * x')'
> 
>      but it is computed without forming the inverse of Y'.
> 
>      If the system is not square, or if the coefficient matrix is
>      singular, a minimum norm solution is computed.

If x is a vector, I think it's solve(t(Y), x).  If x is stored as a 1xn 
matrix, you'd use t(solve(t(Y), t(x)).

If you're trying to do multiple operations at once, you probably need to 
use the second form.

Duncan Murdoch



From br44114 at gmail.com  Tue Jul 26 22:16:05 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 26 Jul 2005 16:16:05 -0400
Subject: [R] choose between dates and times
Message-ID: <8d5a363505072613161027a554@mail.gmail.com>

If happenat is not a datetime value, convert it with strptime(). Then,
one solution is to transform it in the following way:
num.time <- as.numeric(format(happenat,"%Y%m%d%H%M%S"))
This way, 07/22/05 00:05:14 becomes 20050722000514, and you can subset
your data frame with
dfr[which(num.time >= 20050725153000 & num.time <= 20050726123000),]
hth,
b.


> -----Original Message-----
> From: Kerry Bush [mailto:kerryrekky at yahoo.com] 
> Sent: Tuesday, July 26, 2005 3:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] choose between dates and times
> 
> 
> Dear R-helpers,
>   I have the following data:
> 
>  y    happenat                 x
> 5185 (07/22/05 00:05:14)       14
> 5186 (07/22/05 00:15:14)       14
> 5187 (07/22/05 00:25:14)       14
> 5188 (07/22/05 00:35:14)       14
> ......
> 
> I want to choose between 07/25/05 15:30:00 and
> 07/26/05 12:30:00. Anybody had experience in handling
> this kind of data? Is there a simple way to subset by
> the variable 'happenat'? Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mschwartz at mn.rr.com  Tue Jul 26 22:18:20 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 26 Jul 2005 15:18:20 -0500
Subject: [R] elegant solution to transform vector into percentages?
In-Reply-To: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>
References: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>
Message-ID: <1122409101.4326.36.camel@localhost.localdomain>

On Tue, 2005-07-26 at 15:48 -0400, jhainm at fas.harvard.edu wrote:
> Hi,
> 
> I am looking for an elegant way to transform a vector into percentages of values
> that meet certain criteria.
> 
> store<-c(1,1.4,3,1.1,0.3,0.6,4,5)
> 
> # now I want to get the precentages of values
> # that fall into the categories <=M , >M & <=N , >N
> # let
> M <-.8
> N <- 1.2
> # In my real example I have many more of these cutoff-points
> 
> # What I did is:
> 
> out <- matrix(NA,1,3)
> 
>   out[1,1] <- ( (sum(store<=M                ))  /length(store) )*100
>   out[1,2] <- ( (sum(store> M  & store<= N   ))  /length(store) )*100
>   out[1,3] <- ( (sum(store> N                ))  /length(store) )*100
> 
> colnames(out)<-c("percent<=M","percent>M & <=N","percent>N")
> out
> 
> But this gets very tedious if I have many cutoff-points. Does anybody know a
> more elegant way to do this task?
> 
> Thanks so much.
> 
> Cheers,
> Jens

Something alone the lines of:

store <- c(1, 1.4, 3, 1.1, 0.3, 0.6, 4, 5)
M <- 0.8
N <- 1.2

x <- hist(store, br = c(min(store), M, N, max(store)), 
          plot = FALSE)$counts  

pct.x <- prop.table(x) * 100

names(pct.x) <- c("percent <= M","percent > M & <= N","percent > N")

> pct.x
      percent <= M percent > M & <= N        percent > N 
                25                 25                 50 


I think that should do it. See ?hist for more information and take note
of the 'include.lowest' and 'right' arguments relative to whether or not
values are or are not included in the specified intervals.

See ?prop.table as well.

Also be acutely aware of potential problems with exact equality
comparisons with floating point numbers and the break points...if you
have a float value equal to a breakpoint in your vector.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Tue Jul 26 22:19:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jul 2005 22:19:58 +0200
Subject: [R] choose between dates and times
In-Reply-To: <20050726195424.44925.qmail@web51801.mail.yahoo.com>
References: <20050726195424.44925.qmail@web51801.mail.yahoo.com>
Message-ID: <x2ack948wh.fsf@turmalin.kubism.ku.dk>

Kerry Bush <kerryrekky at yahoo.com> writes:

> Dear R-helpers,
>   I have the following data:
> 
>  y    happenat                 x
> 5185 (07/22/05 00:05:14)       14
> 5186 (07/22/05 00:15:14)       14
> 5187 (07/22/05 00:25:14)       14
> 5188 (07/22/05 00:35:14)       14
> ......
> 
> I want to choose between 07/25/05 15:30:00 and
> 07/26/05 12:30:00. Anybody had experience in handling
> this kind of data? Is there a simple way to subset by
> the variable 'happenat'? Thanks.

Simple, perhaps not. This stuff always gets a little heavy, but start
here: 

> strptime("(07/22/05 00:05:14)",format="(%m/%d/%y %H:%M:%S)") >
+     as.POSIXct("2005-07-22 00:06:00")
[1] FALSE
> strptime("(07/22/05 00:05:14)",format="(%m/%d/%y %H:%M:%S)") > 
+     as.POSIXct("2005-07-22 00:05:00")
[1] TRUE

Once you figure out how this works, the rest should follow.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From wang_m at nsabp.pitt.edu  Tue Jul 26 23:13:35 2005
From: wang_m at nsabp.pitt.edu (Wang, Meihua)
Date: Tue, 26 Jul 2005 17:13:35 -0400
Subject: [R] Wishart Density
Message-ID: <3D0B2434377E984E9C85CAA316F8B18301B35C34@nsabpmail>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/6d855b6c/attachment.pl

From ramasamy at cancer.org.uk  Tue Jul 26 23:50:05 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 26 Jul 2005 22:50:05 +0100
Subject: [R] Help on T test
In-Reply-To: <dd48e20f050726122840e92c1c@mail.gmail.com>
References: <dd48e20f050726122840e92c1c@mail.gmail.com>
Message-ID: <1122414605.6146.13.camel@dhcppc3>

Mark,

Please do not post the same question to both R-help and BioC-help
mailing lists because 1) there are many people who are on both lists and
2) people's replies will be archived in two different places making it
harder to others to search in future.

Please see the responses on BioC-help list.

Regards, Adai


On Tue, 2005-07-26 at 15:28 -0400, mark salsburg wrote:
> ok I created a matrix C with 
> 
> A   B  C    A1   B1    C1
> ..................................
> .................................
> 
> the columns contain the gene expression values..
> 
> I ran the following t.test:
> 
> apply(C, 1, function(x) t.test( x[1:3], x[4,6] )$p.value )
> 
> which outputs out 16063 pvalues (the number of rows)
> 
> I just want to output 3 pvalues showing if A's column is different from A1 etc..
> 
> any help would be great.. thank you..
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Wed Jul 27 00:21:15 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 26 Jul 2005 23:21:15 +0100
Subject: [R] elegant solution to transform vector into percentages?
In-Reply-To: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>
References: <1122407311.42e6938f8f1d7@webmail.fas.harvard.edu>
Message-ID: <1122416476.6146.20.camel@dhcppc3>

Why not write a function ? Here is one.

 mytable <- function(x, br){

   n  <- length(br)
  
   tb <- table( cut( x, breaks=c(-Inf, br, Inf) ) )
   tb <- 100 * tb / sum(tb)

   tb.n      <- paste( c("", br), c(br, ""), sep=" < x <= " )
   tb.n[1]   <- paste("x <= ", br[1], sep="")
   tb.n[n+1] <- paste("x > ",  br[n], sep="")

   names(tb) <- tb.n
   return(tb)
 }

 mytable( store, br=c(0.8, 1.2) )
       x <= 0.8 0.8 < x <= 1.2        x > 1.2
             25             25             50

You might want to do a bit more testing especially at the break points.
However I do not like the output of the above because the names of the
table overlaps. Here is another function that might have nicer output.

 mytable2 <- function(x, br){
   tb <- as.matrix( mytable( x=x, br=br ) )
   colnames(tb) <- "Percentage"
   return(tb)
 }

 mytable2( store, br=c(0.8, 1.2) )
                Percentage
 x <= 0.8               25
 0.8 < x <= 1.2         25
 x > 1.2                50

Hope this helps.

Regards, Adai



On Tue, 2005-07-26 at 15:48 -0400, jhainm at fas.harvard.edu wrote:
> Hi,
> 
> I am looking for an elegant way to transform a vector into percentages of values
> that meet certain criteria.
> 
> store<-c(1,1.4,3,1.1,0.3,0.6,4,5)
> 
> # now I want to get the precentages of values
> # that fall into the categories <=M , >M & <=N , >N
> # let
> M <-.8
> N <- 1.2
> # In my real example I have many more of these cutoff-points
> 
> # What I did is:
> 
> out <- matrix(NA,1,3)
> 
>   out[1,1] <- ( (sum(store<=M                ))  /length(store) )*100
>   out[1,2] <- ( (sum(store> M  & store<= N   ))  /length(store) )*100
>   out[1,3] <- ( (sum(store> N                ))  /length(store) )*100
> 
> colnames(out)<-c("percent<=M","percent>M & <=N","percent>N")
> out
> 
> But this gets very tedious if I have many cutoff-points. Does anybody know a
> more elegant way to do this task?
> 
> Thanks so much.
> 
> Cheers,
> Jens
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From peter at fe.up.pt  Wed Jul 27 01:27:51 2005
From: peter at fe.up.pt (Peter Ho)
Date: Wed, 27 Jul 2005 00:27:51 +0100
Subject: [R] Durbin test for Incomplete block
Message-ID: <42E6C6F7.3040002@fe.up.pt>

Hi R-users,

Does anyone know of a package that contains a function to conduct 
Durbin's test and it's extension for incomplete block designs, as 
described in Rayner and Best 2001 "A contingency Table Approach to 
Nonparametric Testing"?


Peter
------------------------------
ISR-Porto



From gfox at stat.rice.edu  Wed Jul 27 00:37:46 2005
From: gfox at stat.rice.edu (Garrett Fox)
Date: Tue, 26 Jul 2005 17:37:46 -0500
Subject: [R] Difficulty getting standard deviation of ALL odds ratios with
	glm function, logistic regression, need cov of parameters
Message-ID: <20050726223750.03D538DA0B@stat.rice.edu>

I am trying to do logistic regression with a categorical predictor variable
with the glm() function, family=binomial.  Using glm() I would like to be
able to calculate the confidence intervals of all three possible odds ratios
for a factor (the factor has three categories).  Three categories imply two
columns of 0's and 1's in the design matrix, and two parameter estimates
with their SE's.  Two confidence intervals for odds ratios can be easily
calculated, the third confidence interval can be calculated if I know the SE
of B1-B2, but this requires the covariance matrix of parameter estimates,
which is not given to my knowledge.  

My initial thought was that Cov(B1,B2)=0, but this cannot be true or the
variance estimates would depend on how you set up the design matrix (the
factor symbolized by 0,0 would always have a higher SD).

Please help me find the covariance of the parameter estimates.

Thank you in advance,

Garrett Fox



From Quin_Wills at msn.com  Wed Jul 27 00:39:04 2005
From: Quin_Wills at msn.com (Quin Wills)
Date: Tue, 26 Jul 2005 23:39:04 +0100
Subject: [R] Anybody have a binary version of SJava for rw2001 (Windows)?
Message-ID: <BAY103-DAV11C23E66B723B2A9555462F4CD0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/460d9870/attachment.pl

From tlumley at u.washington.edu  Wed Jul 27 00:53:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Jul 2005 15:53:45 -0700 (PDT)
Subject: [R] Difficulty getting standard deviation of ALL odds ratios
 with glm function, logistic regression, need cov of parameters
In-Reply-To: <20050726223750.03D538DA0B@stat.rice.edu>
References: <20050726223750.03D538DA0B@stat.rice.edu>
Message-ID: <Pine.A41.4.61b.0507261552490.324204@homer08.u.washington.edu>

On Tue, 26 Jul 2005, Garrett Fox wrote:

> I am trying to do logistic regression with a categorical predictor variable
> with the glm() function, family=binomial.  Using glm() I would like to be
> able to calculate the confidence intervals of all three possible odds ratios
> for a factor (the factor has three categories).  Three categories imply two
> columns of 0's and 1's in the design matrix, and two parameter estimates
> with their SE's.  Two confidence intervals for odds ratios can be easily
> calculated, the third confidence interval can be calculated if I know the SE
> of B1-B2, but this requires the covariance matrix of parameter estimates,
> which is not given to my knowledge.
>

vcov(your.model) returns the variance-covariance matrix.

 	-thomas



From p.murrell at auckland.ac.nz  Wed Jul 27 01:14:21 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 27 Jul 2005 11:14:21 +1200
Subject: [R] problem with Hershey fonts
References: <200507261051.j6QApK9M028246@tahi.mcs.vuw.ac.nz>
Message-ID: <42E6C3CD.5080705@stat.auckland.ac.nz>

Hi


Ray Brownrigg wrote:
> This was reported to me by a colleague in China, so I may not be
> reproducing exactly what they are seeing (which I suspect is rw2011), but
> this is what I see:
> 
>>version
> 
> 	 _
> platform i386--netbsdelf
> arch     i386
> os       netbsdelf
> system   i386, netbsdelf
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
> 
>>help(Hershey)
> 
> 	:
> 	:
>      If the 'vfont' argument to one of the text-drawing functions
>      ('text', 'mtext', 'title', 'axis', and 'contour') is a character
>      vector of length 2, Hershey vector fonts are used to render the
>      text.
> 	:
>           The other useful escape sequences all begin with '\\'.  These
>           are described below. Remember that backslashes have to be
>           doubled in R character strings, so they need to be entered
>           with _four_ backslashes.
> 	:
> 	:
> 
>>plot(runif(100))	# to get something on the screen
>>text(0, 1, '\\Re', vfont=c('serif', 'plain'))	# works
>>title(main = '\\Re', vfont=c('serif','plain'))	# doesn't work, and...
> 
> Warning message:
> parameter "vfont" could not be set in high-level plot() function
> 
>>mtext('\\Re', 2, vfont=c('serif','plain'))	# doesn't work, but...
> 
> Warning message:
> Hershey fonts not yet implemented for mtext() in: mtext(text, side, line, outer, at, adj, padj, cex, col, font,
> 
>>axis(3, at=50, tick=F, labels='\\Re', vfont=c('serif','plain'))	# doesn't work 
> 
> 
> Now at least mtext() 'redeems' itself with its Warning, but title()
> contradicts the help(Hershey), and axis() just gets it wrong by printing
> the characters "\Re" (or "\\Re" when the string is specified as
> '\\\\Re' as alluded to in the Hershey documentation - but somewhat
> contradicted by the success of the text() call above).
> 
> Now perhaps the problem is just with the Hershey documentation (or my
> reading of it), but I don't know what is supposed to work.  Or is there
> some combination that I haven't tried?


The problem with title() has been reported before (PR#7031), but thanks 
for pointing out the extension to axis() and mtext() as well.

The "good" news is that the 'vfont' argument has been superseded.  It 
was a horrible way of incorporating Hershey fonts (I should know, I did 
it) and a much better way now exists via par(family=).  The following 
modification of your example should work:

plot(runif(100))	# to get something on the screen
oldf <- par(family="HersheySerif")
text(0, 1, '\\Re')
title(main = '\\Re')
mtext('\\Re', 2)
axis(3, at=50, tick=F, labels='\\Re')
par(oldf)

The bad news is that, as you point out, the documentation is lacking.  I 
will make some updates so that this solution is easier to find in future.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From joel3000 at gmail.com  Wed Jul 27 02:04:42 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Tue, 26 Jul 2005 17:04:42 -0700
Subject: [R] spss.read factor reversal
Message-ID: <1253d67a05072617042eb9e9ec@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050726/1d20ee2c/attachment.pl

From jsorkin at grecc.umaryland.edu  Wed Jul 27 02:25:16 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 26 Jul 2005 20:25:16 -0400
Subject: [R] CART analysis
Message-ID: <s2e69c41.096@grecc.umaryland.edu>

Is there an R package that can be used for CART analysis?
Thank you,
John 

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From ramasamy at cancer.org.uk  Wed Jul 27 02:30:50 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 27 Jul 2005 01:30:50 +0100
Subject: [R] spss.read factor reversal
In-Reply-To: <1253d67a05072617042eb9e9ec@mail.gmail.com>
References: <1253d67a05072617042eb9e9ec@mail.gmail.com>
Message-ID: <1122424251.6146.36.camel@dhcppc3>

I think it is doing what is supposed to do but I never used read.spss,
so take this with a pinch of salt.

In R when you use as.integer on a factor, the one with the lowest level
gets a value of 1 and so on. The lowest level of the factor can
determined from levels() function.

   f <- factor( c("Green", "Green", "Red", "Blue"), 
                levels=c("Red", "Blue", "Green") )
   levels(f)
   [1] "Red"   "Blue"  "Green"

   as.integer(f)
   [1] 3 3 1 2

But the levels of a factor can be changed 

   as.integer( factor( f, levels=c("Green", "Blue", "Red" ) ) )
   [1] 1 1 3 2

You can also try setting use.value.labels=FALSE in read.spss function
and then creating a factor out of it.

Regards, Adai



On Tue, 2005-07-26 at 17:04 -0700, Joel Bremson wrote:
> Hi,
> 
> I'm having a problem with spss.read reversing my factor input.
> 
> Here is the input copied from the spss data editor:
> 
> color cost
> 1 2.30
> 2 2.40
> 3 3.00
> 1 2.10
> 1 1.00
> 1 2.00
> 2 4.00
> 2 3.20
> 2 2.33
> 3 2.44
> 3 2.55
> 
> For color, red=1, blue=2, and green = 3. It's type is 'String' and
> 
> >out=read.spss(file)
> >out
> 
> $COLOR
> [1] green blue red green green green blue blue blue red red 
> Levels: red blue green
> 
> $COST
> [1] 2.30 2.40 3.00 2.10 1.00 2.00 4.00 3.20 2.33 2.44 2.55
> 
> attr(,"label.table")
> attr(,"label.table")$COLOR
> green blue red 
> 3 2 1 
> 
> attr(,"label.table")$COST
> NULL
> 
> attr(,"variable.labels")
> COLOR COST 
> "color" "cost" 
> 
> =====EOF===================
> 
> Notice that the $COLOR factor data are inverted, looking at the integer 
> output
> we see:
> 
> > as.integer(out$COLOR)
> [1] 3 2 1 3 3 3 2 2 2 1 1
> 
> The spss original data looks like this:
> 1 2 3 1 1 1 2 2 2 3 3
> 
> I can easily invert the output mathematically with:
> q = sapply(m,function(x){ x + 2*(median(unique(m))-x)})
> 
> (m is composed of sequential integers starting at one)
> 
> ,but it seems as though something wrong is happening with read.spss.
> 
> Any ideas?
> 
> Joel Bremson
> Graduate Student
> UC Davis
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Wed Jul 27 02:43:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 27 Jul 2005 01:43:00 +0100
Subject: [R] CART analysis
In-Reply-To: <s2e69c41.096@grecc.umaryland.edu>
References: <s2e69c41.096@grecc.umaryland.edu>
Message-ID: <1122424981.6146.39.camel@dhcppc3>

RSiteSearch("CART") should bring up a few hits including 

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/25850.html

Regards, Adai


On Tue, 2005-07-26 at 20:25 -0400, John Sorkin wrote:
> Is there an R package that can be used for CART analysis?
> Thank you,
> John 
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From feldesmanm at pdx.edu  Wed Jul 27 03:03:20 2005
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Tue, 26 Jul 2005 18:03:20 -0700 (PDT)
Subject: [R] CART analysis
In-Reply-To: <1122424981.6146.39.camel@dhcppc3>
Message-ID: <20050727010321.45037.qmail@web32604.mail.mud.yahoo.com>



--- Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:

> RSiteSearch("CART") should bring up a few hits including 
> 
>   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/25850.html
> 

"CART" is a trademarked statistical procedure owned by Salford Systems
of San Diego, CA.  If you're looking for an R-package that implements
the procedures described in the Breiman, Friedman, Olshen, and Stone
book entitled "Classification and Regression Trees", the closest you
can come to the original algorithm is the R-package called "rpart", by
Therneau and Atkinson.  If you combine that with Andy Liaw's
"randomForest", you have a pretty potent set of tools.  If you really
want "CART", you need to contact Salford Systems for their
implementation and pay their very expensive licensing fees.

Dr. Marc R Feldesman
Professor & Chair Emeritus
Department of Anthropology
Portland State University
Portland, OR 97207

Please respond to all emails at:  feldesmanm at pdx.edu

"Some people live and die by actuarial tables"  Groundhog Day



From mcclatchie.sam at saugov.sa.gov.au  Wed Jul 27 04:07:37 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Wed, 27 Jul 2005 11:37:37 +0930
Subject: [R] trellis graphics/ trellis.par.set()
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73B96@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.0.0
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------
Colleagues

I want to increase the size of the axis markings and labels on some trellis
graphs, and I am having some trouble with trellis.par.set()

trellis.par.set(par.xlab.text.cex = list(cex=1.5))

is not quite right, and having read the documentation, I wonder if anyone
help me get the graphical parameter right?

print(trellis.par.get()) 
returns 
$par.xlab.text.cex

Best fishes

Sam
----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From srini_iyyer_bio at yahoo.com  Wed Jul 27 04:22:02 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Tue, 26 Jul 2005 19:22:02 -0700 (PDT)
Subject: [R] Error in FUN(newX[, i], ...) : `x' must be atomic
Message-ID: <20050727022202.63121.qmail@web53501.mail.yahoo.com>

Hello Group, 
 What is the meaning of the error.  is there any place
to look for this. I guess 'atomic' seems to be OOP
related concept.

thank you
srini



From ramasamy at cancer.org.uk  Wed Jul 27 04:37:02 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 27 Jul 2005 03:37:02 +0100
Subject: [R] Error in FUN(newX[, i], ...) : `x' must be atomic
In-Reply-To: <20050727022202.63121.qmail@web53501.mail.yahoo.com>
References: <20050727022202.63121.qmail@web53501.mail.yahoo.com>
Message-ID: <1122431822.6552.8.camel@dhcppc3>

Perhaps a reproducible example and short explanation of what you want to
do might help. I suspect that you fed a null value into a function that
expects a non-null value or something.


On Tue, 2005-07-26 at 19:22 -0700, Srinivas Iyyer wrote:
> Hello Group, 
>  What is the meaning of the error.  is there any place
> to look for this. I guess 'atomic' seems to be OOP
> related concept.
> 
> thank you
> srini
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From blomsp at ozemail.com.au  Wed Jul 27 04:39:03 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Wed, 27 Jul 2005 12:39:03 +1000
Subject: [R] Error in FUN(newX[, i], ...) : `x' must be atomic
In-Reply-To: <20050727022202.63121.qmail@web53501.mail.yahoo.com>
References: <20050727022202.63121.qmail@web53501.mail.yahoo.com>
Message-ID: <6.2.1.2.0.20050727123046.01d5f310@mail.ozemail.com.au>

Actually, atoms are originally a Lisp concept. Objects are either atoms or 
not. Atoms are data types that cannot be taken apart, such as numbers or 
symbols. Lists and vectors (of length > 1) are examples of  non-atomic data 
types. Did you pass a vector to FUN?

Cheers,

Simon.

At 12:22 PM 27/07/2005, Srinivas Iyyer wrote:
>Hello Group,
>  What is the meaning of the error.  is there any place
>to look for this. I guess 'atomic' seems to be OOP
>related concept.
>
>thank you
>srini
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From Narcyz.Ghinea at swsahs.nsw.gov.au  Wed Jul 27 05:01:26 2005
From: Narcyz.Ghinea at swsahs.nsw.gov.au (Narcyz Ghinea)
Date: Wed, 27 Jul 2005 13:01:26 +1000
Subject: [R] Problem specifying "function" for "mle" operation
Message-ID: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F79FD@isdex001.intra.swsahs.nsw.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/c20e505e/attachment.pl

From pantd at unlv.nevada.edu  Wed Jul 27 05:10:27 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Tue, 26 Jul 2005 20:10:27 -0700
Subject: [R] gamma distribution
Message-ID: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>

Hi R Users


This is a code I wrote and just want to confirm if the first 1000 values are raw
gamma (z) and the next 1000 values are transformed gamma (k) or not. As I get
2000 rows once I import into excel, the p - values beyond 1000 dont look that
good, they are very high.


--
sink("a1.txt");

for (i in 1:1000)
{
x<-rgamma(10, 2.5, scale = 10)
y<-rgamma(10, 2.5, scale = 10)
z<-wilcox.test(x, y, var.equal = FALSE)
print(z)
x1<-log(x)
y1<-log(y)
k<-wilcox.test(x1, y1, var.equal = FALSE)
print(k)
}

---
any suggestions are welcome

thanks

-devarshi



From harikiyer at hotmail.com  Wed Jul 27 05:52:44 2005
From: harikiyer at hotmail.com (hari iyer)
Date: Wed, 27 Jul 2005 03:52:44 +0000
Subject: [R] A question about par.plot in gamlss
Message-ID: <BAY107-F30876ADD1ABA8C6B48C01AB1CC0@phx.gbl>

Hello
I am using the following code to plot a data matrix into a form that seems 
suitable for
the use of par.plot.

library(gamlss)
a<-matrix(c(1,2,3,4,5,6,7,8,9,8,7,6),nrow=3)
rownames(a)<-c("trt1","trt2","trt3")
colnames(a)<-c("col1","col2","col3","col4")
hpar.plot<-function(ZZ){
ZZvar<-c(t(ZZ))
ZZtrt<-c(matrix(rep(1:ncol(ZZ),nrow(ZZ))))
ZZcov<-as.factor(c(t(matrix(rep(rownames(ZZ),ncol(ZZ)),nrow=nrow(ZZ)))))
par.plot(ZZvar ~ ZZtrt,sub=ZZcov)
}
hpar.plot(a)

I keep getting the following error message:

Error in eval(expr, envir, enclos) : Object "ZZvar" not found

I am unable to figure out my error. Can someone help please? Thanks in 
advance.

Hari



From buser at stat.math.ethz.ch  Wed Jul 27 09:01:50 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 27 Jul 2005 09:01:50 +0200
Subject: [R] gamma distribution
In-Reply-To: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
Message-ID: <17127.12638.216321.456715@stat.math.ethz.ch>

Hi

I am a little bit confused. You create two sample (from a gamma
distribution) and you do a wilcoxon test with this two samples.
Then you use the same monotone transformation (log) for both
samples and redo the wilcoxon test.
But since the transformations keeps the order of your samples
the second wilcoxon test is identical to the first one:

x<-rgamma(10, 2.5, scale = 10)
y<-rgamma(10, 2.5, scale = 10)
wilcox.test(x, y, var.equal = FALSE)
x1<-log(x)
y1<-log(y)
wilcox.test(x1, y1, var.equal = FALSE)

Maybe you can give some more details about the hypothesis you'd
like to test.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



pantd at unlv.nevada.edu writes:
 > Hi R Users
 > 
 > 
 > This is a code I wrote and just want to confirm if the first 1000 values are raw
 > gamma (z) and the next 1000 values are transformed gamma (k) or not. As I get
 > 2000 rows once I import into excel, the p - values beyond 1000 dont look that
 > good, they are very high.
 > 
 > 
 > --
 > sink("a1.txt");
 > 
 > for (i in 1:1000)
 > {
 > x<-rgamma(10, 2.5, scale = 10)
 > y<-rgamma(10, 2.5, scale = 10)
 > z<-wilcox.test(x, y, var.equal = FALSE)
 > print(z)
 > x1<-log(x)
 > y1<-log(y)
 > k<-wilcox.test(x1, y1, var.equal = FALSE)
 > print(k)
 > }
 > 
 > ---
 > any suggestions are welcome
 > 
 > thanks
 > 
 > -devarshi
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jul 27 09:18:26 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Jul 2005 09:18:26 +0200
Subject: [R] gamma distribution
In-Reply-To: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
Message-ID: <42E73542.6090505@statistik.uni-dortmund.de>

pantd at unlv.nevada.edu wrote:

> Hi R Users
> 
> 
> This is a code I wrote and just want to confirm if the first 1000 values are raw
> gamma (z) and the next 1000 values are transformed gamma (k) or not. As I get
> 2000 rows once I import into excel, the p - values beyond 1000 dont look that
> good, they are very high.

He?
- log() transforming the data does not change the Wilcoxon statistics 
(based on ranks!)!
- Why is this related to Excel?
- What are you going to show?

I get

  erg <- replicate(1000, {
      x<-rgamma(10, 2.5, scale = 10)
      y<-rgamma(10, 2.5, scale = 10)
      wilcox.test(x, y, var.equal = FALSE)$p.value
  })
  sum(erg < 0.05) # 45

which seems plausible to me.


Uwe Ligges



> 
> --
> sink("a1.txt");
> 
> for (i in 1:1000)
> {
> x<-rgamma(10, 2.5, scale = 10)
> y<-rgamma(10, 2.5, scale = 10)
> z<-wilcox.test(x, y, var.equal = FALSE)
> print(z)
> x1<-log(x)
> y1<-log(y)
> k<-wilcox.test(x1, y1, var.equal = FALSE)
> print(k)
> }
> 
> ---
> any suggestions are welcome
> 
> thanks
> 
> -devarshi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bioconductor.cn at gmail.com  Wed Jul 27 09:34:05 2005
From: bioconductor.cn at gmail.com (Xiao Shi)
Date: Wed, 27 Jul 2005 15:34:05 +0800
Subject: [R] How learn a probabilities matrix from a large fasta file in R?
Message-ID: <cedaa40b0507270034322c6c1a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/2716a790/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jul 27 09:38:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 08:38:47 +0100 (BST)
Subject: [R] A question about par.plot in gamlss
In-Reply-To: <BAY107-F30876ADD1ABA8C6B48C01AB1CC0@phx.gbl>
References: <BAY107-F30876ADD1ABA8C6B48C01AB1CC0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0507270836450.21618@gannet.stats>

As the posting guide says, please ask the package maintainer -- I believe 
he does not read R-help regularly.

Your layout is hard to read (please do use spaces and indentation in 
posted code), but probably there is a scoping problem with par.plot used 
inside a function.

On Wed, 27 Jul 2005, hari iyer wrote:

> Hello
> I am using the following code to plot a data matrix into a form that seems
> suitable for
> the use of par.plot.
>
> library(gamlss)
> a<-matrix(c(1,2,3,4,5,6,7,8,9,8,7,6),nrow=3)
> rownames(a)<-c("trt1","trt2","trt3")
> colnames(a)<-c("col1","col2","col3","col4")
> hpar.plot<-function(ZZ){
> ZZvar<-c(t(ZZ))
> ZZtrt<-c(matrix(rep(1:ncol(ZZ),nrow(ZZ))))
> ZZcov<-as.factor(c(t(matrix(rep(rownames(ZZ),ncol(ZZ)),nrow=nrow(ZZ)))))
> par.plot(ZZvar ~ ZZtrt,sub=ZZcov)
> }
> hpar.plot(a)
>
> I keep getting the following error message:
>
> Error in eval(expr, envir, enclos) : Object "ZZvar" not found
>
> I am unable to figure out my error. Can someone help please? Thanks in
> advance.
>
> Hari
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jul 27 09:42:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Jul 2005 09:42:27 +0200
Subject: [R] trellis graphics/ trellis.par.set()
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F01D73B96@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F01D73B96@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <42E73AE3.7040907@statistik.uni-dortmund.de>

McClatchie, Sam (PIRSA-SARDI) wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.0.0
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
> Colleagues
> 
> I want to increase the size of the axis markings and labels on some trellis
> graphs, and I am having some trouble with trellis.par.set()
> 
> trellis.par.set(par.xlab.text.cex = list(cex=1.5))

You mean:
  trellis.par.set(par.xlab.text = list(cex=1.5))

Uwe Ligges

> is not quite right, and having read the documentation, I wonder if anyone
> help me get the graphical parameter right?
> 
> print(trellis.par.get()) 
> returns 
> $par.xlab.text.cex
> 
> Best fishes
> 
> Sam
> ----
> Sam McClatchie,
> Biological oceanography 
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Telephone: (61-8) 8207 5448
> FAX: (61-8) 8207 5481
> Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
>   
>                    /\
>       ...>><xX(??> 
>                 //// \\\\
>                    <??)Xx><<
>               /////  \\\\\\
>                         ><(((??> 
>   >><(((??>   ...>><xX(??>O<??)Xx><<
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Jan.Wijffels at ucs.kuleuven.be  Wed Jul 27 09:53:52 2005
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Wed, 27 Jul 2005 09:53:52 +0200
Subject: [R] rpart.permutation, snow, rsprng binary files
Message-ID: <000601c59280$5505cd80$2c70210a@UCSPC32>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/5f771d42/attachment.pl

From Christian.Stratowa at vie.boehringer-ingelheim.com  Wed Jul 27 09:54:55 2005
From: Christian.Stratowa at vie.boehringer-ingelheim.com (Christian.Stratowa@vie.boehringer-ingelheim.com)
Date: Wed, 27 Jul 2005 09:54:55 +0200
Subject: [R] Asymmetric colors for heatmap
Message-ID: <7433775A4AB9D147BF1DF09DBEFA0B3F387DB7@vieex07.eu.boehringer.com>

> Dear expeRts,
> 
> Currently, my colors are as follows:
> mycol <-
> c("blue1","blue2","blue3","blue4","black","yellow4","yellow3","yellow2","y
> ellow1")
> heatmap(snp, Rowv=NA, Colv=NA, col=mycol)
> 
> However, I would like to have the following colors:
> bright blue -> dark blue: for intensity range from 0 to 2 in  steps of 0.5
> (i.e. 4 grades of blue)
> black:  for intensity 2
> dark yellow -> bright yellow: for intensity range from 2 to 8 in steps of
> 0.5 (i.e. 8 grades of yellow)
> 
> You may realize that I want to display copy number data from SNP-chips as
> heatmap. 
> Since copynumber = 2, is the default value, I want to display it in black,
> LOH in increasing blue, and amplifications in increasing yellow.
> Even though there may be higher amplification rates, a value of CN=8
> should already display the brightest yellow.
> 
> In Spotfire it is easy to achieve this, especially that CN=2 is always
> displayed as black, however, I do not know how to do it in R.
> Can you tell me how I have to create the colors to achieve this?
> 
> (P.S.: Of course, I could do: snp[snp>8] <- 8, but this will not solve my
> problem with asymmetric colors)
> 
> Thank you in advance
> Christian Stratowa
> 
> 
> 
> 
> 
>



From antonio.fabio at gmail.com  Wed Jul 27 10:00:00 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 27 Jul 2005 10:00:00 +0200
Subject: [R] Anybody have a binary version of SJava for rw2001	(Windows)?
In-Reply-To: <BAY103-DAV11C23E66B723B2A9555462F4CD0@phx.gbl>
References: <BAY103-DAV11C23E66B723B2A9555462F4CD0@phx.gbl>
Message-ID: <1122451201.6723.4.camel@localhost.localdomain>

On the omegahat site, you can read:
"There is currently no binary available for Windows. I hope to release
one wihtin a few weeks (i.e. near the end of September, 2004)."

You can ask to developers an "unofficial" binary version for you. Maybe
they have, or maybe they're esperiencing problems in building it, in
which case I fear you have to stand by a little more...

Antonio, Fabio Di Narzo.

Il giorno mar, 26/07/2005 alle 23.39 +0100, Quin Wills ha scritto:
> I am not a techie and have been struggling 2 days solid to try and install
> SJava (the source from http://www.omegahat.org/RSJava/). Does anybody have a
> binary file for me (I am Windows XP and rw2001)? I have tried installing
> Perl, mingwin and the cygwin tools but still no luck. When I try R CMD
> INSTALL c:\SJava_0.78-0.tar.gz I get the following (and havent a clue what
> it could mean):
> 
>  
> 
> ---------Making package SJava-------------- 
> Building JNI header files... 
> Extracting the classes from Environment.jar 
> /jdk1.3/bin/jar: not found 
>     RForeignReference 
> /jdk1.3/bin/javah: not found 
>     ROmegahat Interpreter 
> /jdkl.3/bin/javah: not found 
>     REvaluator 
> /jdkl.3/bin/Javah: not found 
>     RManualFunctionActionListener 
> /jdk1.3/bin/javah: not found 
> /jdkl.3/bin/javah: not found 
> adding build stamp to DESCRIPTION 
> running src/Makefile.win 
> (cd ..  ; ./configure.win c:/PROGRW1/R/rw200l) 
> /configure.win: not found 
> make[3]: *** [conf ig] Error 127 
> make[2]: *** [srcDynLib] Error 2 
> make[1]: *** [all] Error 2 
> make: *** [pkgSJava] Error 2 
> *** Installation of SJava failed *** 
> 
>  
> 
> 
> ---
> 
> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jul 27 10:31:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 09:31:32 +0100 (BST)
Subject: [R] rpart.permutation, snow, rsprng binary files
In-Reply-To: <000601c59280$5505cd80$2c70210a@UCSPC32>
References: <000601c59280$5505cd80$2c70210a@UCSPC32>
Message-ID: <Pine.LNX.4.61.0507270929400.8535@gannet.stats>

They rely on multiprocessing infrastructure not normally available on 
Windows.  If you have this (and I don't know if it can be compiled on 
Windows but dome at least of the options cannot) you should be able to 
compile the source packages against the versions you have.

On Wed, 27 Jul 2005, Jan Wijffels wrote:

> Does anyone of you have binary files for the packages rpart.permutation,
> snow and rsprng. I would like to use them in my classification tree. I
> know they are still at the 0.x development stage, though.
> Where can I get information on how to compile tar.gz files? I'm using
> windows XP with R 2.1.0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From u9370004 at cc.kmu.edu.tw  Wed Jul 27 10:51:37 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Wed, 27 Jul 2005 16:51:37 +0800
Subject: [R] how to overlook the zero in the denominator
Message-ID: <20050727075919.M66552@cc.kmu.edu.tw>

Dear R users:

I have two set of data, as follow:
x<-c(0,0,0.28,0.55,1.2,2,1.95,1.85,
     1.6,0.86,0.78,0.6,0.21,0.18)
y<-c(0,0,0,0.53,1.34,1.79,2.07,1.88,
    1.52,0.92,0.71,0.55,0.32,0.19)
i<-1:length(x)

I want to sum each (x[i]-y[i])^2/x[i] together, 
like:
>Sum <-sum((x[i]-y[i])^2/x[i])
>Sum
[1] NaN

Because the denominator shoud not be zero.
So I want to overlook those when x[i]=0,
and just to sum those x[i] not equal to 0.
What should I do?
Any suggestion.
Thanks in advance !!



From p.dalgaard at biostat.ku.dk  Wed Jul 27 11:01:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jul 2005 11:01:19 +0200
Subject: [R] spss.read factor reversal
In-Reply-To: <1122424251.6146.36.camel@dhcppc3>
References: <1253d67a05072617042eb9e9ec@mail.gmail.com>
	<1122424251.6146.36.camel@dhcppc3>
Message-ID: <x2zms8k4gw.fsf@turmalin.kubism.ku.dk>

Adaikalavan Ramasamy <ramasamy at cancer.org.uk> writes:

> I think it is doing what is supposed to do but I never used read.spss,
> so take this with a pinch of salt.
> 
> In R when you use as.integer on a factor, the one with the lowest level
> gets a value of 1 and so on. The lowest level of the factor can
> determined from levels() function.
> 
>    f <- factor( c("Green", "Green", "Red", "Blue"), 
>                 levels=c("Red", "Blue", "Green") )
>    levels(f)
>    [1] "Red"   "Blue"  "Green"
> 
>    as.integer(f)
>    [1] 3 3 1 2
> 
> But the levels of a factor can be changed 
> 
>    as.integer( factor( f, levels=c("Green", "Blue", "Red" ) ) )
>    [1] 1 1 3 2

Doesn't explain why  1 2 3 in the input file comes out as Green Blue
Red, does it?
 
> You can also try setting use.value.labels=FALSE in read.spss function
> and then creating a factor out of it.

Would be interesting to see this. I would suspect that the damage is
already done at that point though.

I notice that the value labels are in reverse order. Shouldn't matter
to read.spss which has

            rval[[nm]] <- factor(rval[[nm]], levels = vl[[v]],
                labels = trim(names(vl[[v]])))

i.e. levels and labels should be in the correct order. 

But something is odd, you'd expect the following effect:

> x <- 1:3
> factor(x,levels=3:1,labels=c("G","B","R"))
[1] R B G
Levels: G B R
> as.integer(factor(x,levels=3:1,labels=c("G","B","R")))
[1] 3 2 1

but Joel's output has the levels in the order R B G, which contradicts
the 

attr(,"label.table")$COLOR

BTW, this is R 2.1.1, I hope Joel isn't wasting our time by using an
older version...

        -p


> Regards, Adai
> 
> 
> 
> On Tue, 2005-07-26 at 17:04 -0700, Joel Bremson wrote:
> > Hi,
> > 
> > I'm having a problem with spss.read reversing my factor input.
> > 
> > Here is the input copied from the spss data editor:
> > 
> > color cost
> > 1 2.30
> > 2 2.40
> > 3 3.00
> > 1 2.10
> > 1 1.00
> > 1 2.00
> > 2 4.00
> > 2 3.20
> > 2 2.33
> > 3 2.44
> > 3 2.55
> > 
> > For color, red=1, blue=2, and green = 3. It's type is 'String' and
> > 
> > >out=read.spss(file)
> > >out
> > 
> > $COLOR
> > [1] green blue red green green green blue blue blue red red 
> > Levels: red blue green
> > 
> > $COST
> > [1] 2.30 2.40 3.00 2.10 1.00 2.00 4.00 3.20 2.33 2.44 2.55
> > 
> > attr(,"label.table")
> > attr(,"label.table")$COLOR
> > green blue red 
> > 3 2 1 
> > 
> > attr(,"label.table")$COST
> > NULL
> > 
> > attr(,"variable.labels")
> > COLOR COST 
> > "color" "cost" 
> > 
> > =====EOF===================
> > 
> > Notice that the $COLOR factor data are inverted, looking at the integer 
> > output
> > we see:
> > 
> > > as.integer(out$COLOR)
> > [1] 3 2 1 3 3 3 2 2 2 1 1
> > 
> > The spss original data looks like this:
> > 1 2 3 1 1 1 2 2 2 3 3
> > 
> > I can easily invert the output mathematically with:
> > q = sapply(m,function(x){ x + 2*(median(unique(m))-x)})
> > 
> > (m is composed of sequential integers starting at one)
> > 
> > ,but it seems as though something wrong is happening with read.spss.
> > 
> > Any ideas?
> > 
> > Joel Bremson
> > Graduate Student
> > UC Davis
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.be  Wed Jul 27 11:13:26 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 27 Jul 2005 11:13:26 +0200
Subject: [R] how to overlook the zero in the denominator
References: <20050727075919.M66552@cc.kmu.edu.tw>
Message-ID: <027901c5928b$725a5800$0540210a@www.domain>

look at ?sum() and use the `na.rm' argument, i.e.,

sum((x - y)^2 / x, na.rm = TRUE)

I hope it helps.

Best,
Dimitris

p.s., R is vectorized, you don't have to use `x[i]' in your example.

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Chun-Ying Lee" <u9370004 at cc.kmu.edu.tw>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, July 27, 2005 10:51 AM
Subject: [R] how to overlook the zero in the denominator


> Dear R users:
>
> I have two set of data, as follow:
> x<-c(0,0,0.28,0.55,1.2,2,1.95,1.85,
>     1.6,0.86,0.78,0.6,0.21,0.18)
> y<-c(0,0,0,0.53,1.34,1.79,2.07,1.88,
>    1.52,0.92,0.71,0.55,0.32,0.19)
> i<-1:length(x)
>
> I want to sum each (x[i]-y[i])^2/x[i] together,
> like:
>>Sum <-sum((x[i]-y[i])^2/x[i])
>>Sum
> [1] NaN
>
> Because the denominator shoud not be zero.
> So I want to overlook those when x[i]=0,
> and just to sum those x[i] not equal to 0.
> What should I do?
> Any suggestion.
> Thanks in advance !!
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Wed Jul 27 11:09:53 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 27 Jul 2005 11:09:53 +0200
Subject: [R] how to overlook the zero in the denominator
In-Reply-To: <20050727075919.M66552@cc.kmu.edu.tw>
References: <20050727075919.M66552@cc.kmu.edu.tw>
Message-ID: <17127.20321.437005.126921@stat.math.ethz.ch>

What about

x<-c(0,0,0.28,0.55,1.2,2,1.95,1.85,
     1.6,0.86,0.78,0.6,0.21,0.18)
y<-c(0,0,0,0.53,1.34,1.79,2.07,1.88,
    1.52,0.92,0.71,0.55,0.32,0.19)

sum(((x-y)^2/x)[x!=0])

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Chun-Ying Lee writes:
 > Dear R users:
 > 
 > I have two set of data, as follow:
 > x<-c(0,0,0.28,0.55,1.2,2,1.95,1.85,
 >      1.6,0.86,0.78,0.6,0.21,0.18)
 > y<-c(0,0,0,0.53,1.34,1.79,2.07,1.88,
 >     1.52,0.92,0.71,0.55,0.32,0.19)
 > i<-1:length(x)
 > 
 > I want to sum each (x[i]-y[i])^2/x[i] together, 
 > like:
 > >Sum <-sum((x[i]-y[i])^2/x[i])
 > >Sum
 > [1] NaN
 > 
 > Because the denominator shoud not be zero.
 > So I want to overlook those when x[i]=0,
 > and just to sum those x[i] not equal to 0.
 > What should I do?
 > Any suggestion.
 > Thanks in advance !!
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Wed Jul 27 11:19:00 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 27 Jul 2005 10:19:00 +0100
Subject: [R] how to overlook the zero in the denominator
In-Reply-To: <20050727075919.M66552@cc.kmu.edu.tw>
References: <20050727075919.M66552@cc.kmu.edu.tw>
Message-ID: <1122455941.7346.8.camel@dhcppc3>

The simplest way would be to ignore them in the sum

	sum( ( x - y )/x, na.rm=T )

and notice that I have utilised the vectorised operation. But if you
want to, you can explicitly remove them with

	good <- which( x != 0 )
	sum(  ( x[good] - y[good] )/ x[good]   )

Regards, Adai



On Wed, 2005-07-27 at 16:51 +0800, Chun-Ying Lee wrote:
> Dear R users:
> 
> I have two set of data, as follow:
> x<-c(0,0,0.28,0.55,1.2,2,1.95,1.85,
>      1.6,0.86,0.78,0.6,0.21,0.18)
> y<-c(0,0,0,0.53,1.34,1.79,2.07,1.88,
>     1.52,0.92,0.71,0.55,0.32,0.19)
> i<-1:length(x)
> 
> I want to sum each (x[i]-y[i])^2/x[i] together, 
> like:
> >Sum <-sum((x[i]-y[i])^2/x[i])
> >Sum
> [1] NaN
> 
> Because the denominator shoud not be zero.
> So I want to overlook those when x[i]=0,
> and just to sum those x[i] not equal to 0.
> What should I do?
> Any suggestion.
> Thanks in advance !!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Jul 27 11:26:40 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jul 2005 11:26:40 +0200
Subject: [R] Error in FUN(newX[, i], ...) : `x' must be atomic
In-Reply-To: <6.2.1.2.0.20050727123046.01d5f310@mail.ozemail.com.au>
References: <20050727022202.63121.qmail@web53501.mail.yahoo.com>
	<6.2.1.2.0.20050727123046.01d5f310@mail.ozemail.com.au>
Message-ID: <x2vf2wk3an.fsf@turmalin.kubism.ku.dk>

Simon Blomberg <blomsp at ozemail.com.au> writes:

> Actually, atoms are originally a Lisp concept. Objects are either atoms or 
> not. Atoms are data types that cannot be taken apart, such as numbers or 
> symbols. Lists and vectors (of length > 1) are examples of  non-atomic data 
> types. Did you pass a vector to FUN?

Actually, vectors ARE atomic in R, so your definition is somewhat off
target. 

>  is.atomic(rnorm(5))
[1] TRUE

In the extreme, the only true atom is the bit, everything else can be
taken apart - doubles into (sign,exponent,mantissa) etc. So languages
*define* their own atoms, as objects that are not composed of other
objects in the language. (And even that is a partial lie for R,
because atoms can have attributes. Atomicity is purely based on the
type of an object. The help page has a list of the atomic types.)

> Cheers,
> 
> Simon.
> 
> At 12:22 PM 27/07/2005, Srinivas Iyyer wrote:
> >Hello Group,
> >  What is the meaning of the error.  is there any place
> >to look for this. I guess 'atomic' seems to be OOP
> >related concept.
> >
> >thank you
> >srini
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
> Centre for Resource and Environmental Studies
> The Australian National University
> Canberra ACT 0200
> Australia
> T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
> F: +61 2 6125 0757
> CRICOS Provider # 00120C
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Jul 27 11:34:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jul 2005 11:34:24 +0200
Subject: [R] Problem specifying "function" for "mle" operation
In-Reply-To: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F79FD@isdex001.intra.swsahs.nsw.gov.au>
References: <588D8BDAAC0BEB4B82DA2CC5AEBA899C3F79FD@isdex001.intra.swsahs.nsw.gov.au>
Message-ID: <x2r7dkk2xr.fsf@turmalin.kubism.ku.dk>

"Narcyz Ghinea" <Narcyz.Ghinea at swsahs.nsw.gov.au> writes:

> MY THEORY:
> 
> I think it has something to do with the fact that the function
> argument is a vector in CASE 2. Hope this doesn't mean I have to
> re-write the function in a way that it doesn't require vector
> inputs.....

You do. The innards of mle has

    f <- function(p) {
        l <- as.list(p)
        names(l) <- nm
        l[n] <- fixed
        do.call("minuslogl", l)
    }

which converts the parameters to a list before calling the likelihood.
 
> Any suggestions on how to make CASE 2 work would be appreciated.

Well, either you conform or you patch mle to handle functions with
vector parameters....

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From daniel.pastor at uam.es  Wed Jul 27 12:07:21 2005
From: daniel.pastor at uam.es (daniel.pastor@uam.es)
Date: Wed, 27 Jul 2005 12:07:21 MET
Subject: [R] GAM weights
Message-ID: <200507271007.j6RA7KYK021093@asterix.ti.uam.es>

Dear all,
we are trying to model some data from rare plants so we always have less than 50
1x1 km presences, and the total area is about 550.000 square km. So we have a
real problem, when we perform a GAM, if we consider only the same amount of
absences than presences.
We have thought to use a greater number of absences but in this case we shoud
downweight them.
Does anybody know how to use the ?wheight? term?
thank you in advance
daniel

--------------------------------------------------------------------------
Mensaje enviado mediante una herramienta Webmail integrada en *El Rincon*:
------------->>>>>>>>     https://rincon.uam.es     <<<<<<<<--------------



From Jan.Verbesselt at biw.kuleuven.be  Wed Jul 27 12:11:06 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Wed, 27 Jul 2005 12:11:06 +0200
Subject: [R] HOW to Create Movies with R with repeated plot()?
Message-ID: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/aa7b87ae/attachment.pl

From luk111111 at yahoo.com  Wed Jul 27 13:10:15 2005
From: luk111111 at yahoo.com (luk)
Date: Wed, 27 Jul 2005 04:10:15 -0700 (PDT)
Subject: [R] default family object in glm
In-Reply-To: <200507271007.j6RA7KYK021093@asterix.ti.uam.es>
Message-ID: <20050727111015.45632.qmail@web30905.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/f802af48/attachment.pl

From sdavis2 at mail.nih.gov  Wed Jul 27 13:15:51 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 27 Jul 2005 07:15:51 -0400
Subject: [R] Asymmetric colors for heatmap
References: <7433775A4AB9D147BF1DF09DBEFA0B3F387DB7@vieex07.eu.boehringer.com>
Message-ID: <000401c5929d$4399a270$6401a8c0@WATSON>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/e805804e/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Wed Jul 27 13:29:41 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 27 Jul 2005 13:29:41 +0200
Subject: [R] default family object in glm
References: <20050727111015.45632.qmail@web30905.mail.mud.yahoo.com>
Message-ID: <000a01c5929e$7b11cf60$0540210a@www.domain>

>From ?glm() you get:

...

Usage

glm(formula, family = gaussian, data, weights, subset,
    na.action, start = NULL, etastart, mustart,
    offset, control = glm.control(...), model = TRUE,
    method = "glm.fit", x = FALSE, y = TRUE, contrasts = NULL, ...)

...


So the default family is the gaussian.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "luk" <luk111111 at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, July 27, 2005 1:10 PM
Subject: [R] default family object in glm


> hi
>
> what is the default family object in glm? I cannot find it from the 
> doc. It would be great if you could tell me where I should look 
> into.
>
> thanks,
>
> lu
>
>
> ---------------------------------
>
> Stay connected, organized, and protected. Take the tour
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Christian.Stratowa at vie.boehringer-ingelheim.com  Wed Jul 27 14:06:39 2005
From: Christian.Stratowa at vie.boehringer-ingelheim.com (Christian.Stratowa@vie.boehringer-ingelheim.com)
Date: Wed, 27 Jul 2005 14:06:39 +0200
Subject: [R] Asymmetric colors for heatmap
Message-ID: <7433775A4AB9D147BF1DF09DBEFA0B3F387DB8@vieex07.eu.boehringer.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/545e7c7b/attachment.pl

From xavier.fim at menta.net  Wed Jul 27 14:17:19 2005
From: xavier.fim at menta.net (Xavier =?utf-8?Q?Fern=C3=A1ndez_i_Mar=C3=ADn?=)
Date: Wed, 27 Jul 2005 13:17:19 +0100
Subject: [R] make.names() does not return what is expected
Message-ID: <20050727121719.GA9278@deu.xfim>

Hello,

I don't know if it's only me, but I can't get make.names() transform '_' in
the way it is expected when I add the "allow_" option. A simple example trying
to replicate the example provided in the help page gives:

-----8<---------------
> make.names(c("a and b", "a_and_b"), unique=TRUE, allow_=FALSE)
[1] "a and b." "a_and_b"
-----8<---------------



When the example says it should be:
-----8<---------------
make.names(c("a and b", "a_and_b"), unique=TRUE, allow_=FALSE)
# "a.and.b"  "a.and.b.1"
-----8<---------------


I'm using R-2.1.1 compiled by myself on a gentoo GNU/Linux laptop.

Anybody with the same problems? 

Thank you,


-- 

Xavier Fernndez i Marn
xavier.fim at menta.net
^^^^^^^^^^^^^^^^^^^^^^^^



From aliscla at yahoo.com  Wed Jul 27 14:12:45 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Wed, 27 Jul 2005 05:12:45 -0700 (PDT)
Subject: [R] a class matrix with class ordered data
In-Reply-To: <20050726164120.95798.qmail@web61225.mail.yahoo.com>
Message-ID: <20050727121245.49921.qmail@web61216.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/78040b89/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jul 27 14:21:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 13:21:53 +0100 (BST)
Subject: [R] default family object in glm
In-Reply-To: <20050727111015.45632.qmail@web30905.mail.mud.yahoo.com>
References: <20050727111015.45632.qmail@web30905.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0507271320270.16622@gannet.stats>

On Wed, 27 Jul 2005, luk wrote:

> what is the default family object in glm? I cannot find it from the doc. 
> It would be great if you could tell me where I should look into.

Try the help page:

      glm(formula, family = gaussian, data, weights, subset,
                            ^^^^^^^^

It is not clear what you don't understand, e.g. the format of R help 
pages?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Friedrich.Leisch at tuwien.ac.at  Wed Jul 27 14:24:00 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 27 Jul 2005 14:24:00 +0200
Subject: [R] LyX and Sweave
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31936@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31936@pollux.bfro.uni-lj.si>
Message-ID: <17127.31968.599285.291012@celebrian.ci.tuwien.ac.at>

>>>>> On Mon, 25 Jul 2005 14:12:41 +0200,
>>>>> Gorjanc Gregor (GG) wrote:

  > Hello R-users!
  > I have tried to use Sweave within LyX* and found two ways to accomplish
  > this. I have attached LyX source file for both ways as well as generated 
  > PDFs.

I have copied Gregor's files at

	http://www.ci.tuwien.ac.at/~leisch/Sweave/LyX

for those who didn't get the attachments. LyX looks actually much
better and stable then when I last had a look a couple of years ago.

To add to the discussion: One might even get better integration
between Sweave and LyX because LyX allows specification of file
converters:

	Edit->Preferences->Converters
	Edit->Preferences->File Formats

After registering .Stex as a file format for Sweave I can import .Stex
files using conversion command

	reLyX -f $$i && mv $$i.lyx $$o

and the move is necessary only because reLyX produces foo.Stex.lyx
rather then foo.lyx.

Using an Sweave shell script like Gregor's in combination with
"lyx -e latex" might do the trick to have direct conversion to PDF
from the LyX GUI.

Best,
Fritz



From ripley at stats.ox.ac.uk  Wed Jul 27 14:33:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 13:33:33 +0100 (BST)
Subject: [R] make.names() does not return what is expected
In-Reply-To: <20050727121719.GA9278@deu.xfim>
References: <20050727121719.GA9278@deu.xfim>
Message-ID: <Pine.LNX.4.61.0507271332110.16811@gannet.stats>

It's a bug, soon to be fixed in R-patched.

On Wed, 27 Jul 2005, Xavier [utf-8] Fern?ndez i Mar?n wrote:

> Hello,
> I don't know if it's only me, but I can't get make.names() transform '_' inthe way it is expected when I add the "allow_" option. A simple example tryingto replicate the example provided in the help page gives:
> -----8<---------------> make.names(c("a and b", "a_and_b"), unique=TRUE, allow_=FALSE)[1] "a and b." "a_and_b"-----8<---------------
>
>
> When the example says it should be:-----8<---------------make.names(c("a and b", "a_and_b"), unique=TRUE, allow_=FALSE)# "a.and.b"  "a.and.b.1"-----8<---------------
>
> I'm using R-2.1.1 compiled by myself on a gentoo GNU/Linux laptop.
> Anybody with the same problems?
> Thank you,

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jul 27 14:47:21 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 27 Jul 2005 14:47:21 +0200
Subject: [R] LyX and Sweave
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31946@pollux.bfro.uni-lj.si>

>  > Hello R-users!
<  > I have tried to use Sweave within LyX* and found two ways to accomplish
>  > this. I have attached LyX source file for both ways as well as generated 
>  > PDFs.

> I have copied Gregor's files at

>	http://www.ci.tuwien.ac.at/~leisch/Sweave/LyX

> for those who didn't get the attachments. LyX looks actually much
> better and stable then when I last had a look a couple of years ago.

Well, I am satisfied with it, actaully I wrote my graduation thesis in it and it 
was much, much more stable than let's say MS Word. Of course one does not need
to know much about LaTeX.

>> To add to the discussion: One might even get better integration
>> between Sweave and LyX because LyX allows specification of file
>> converters:

>	Edit->Preferences->Converters
>	Edit->Preferences->File Formats

> After registering .Stex as a file format for Sweave I can import .Stex
> files using conversion command

>	reLyX -f $$i && mv $$i.lyx $$o

> and the move is necessary only because reLyX produces foo.Stex.lyx
> rather then foo.lyx.

Nice!

> Using an Sweave shell script like Gregor's in combination with
> "lyx -e latex" might do the trick to have direct conversion to PDF
> from the LyX GUI.

Note that this is not the only possible way. As I showed, the other one i.e.
noweb mode in LyX (look in file SweaveNoweb.lyx and its PDF at URL above) 
can deal with Sweave in all ways i.e. export or view to any supported formats 
by LyX.



From xavier.fim at menta.net  Wed Jul 27 15:05:15 2005
From: xavier.fim at menta.net (Xavier =?utf-8?Q?Fern=C3=A1ndez_i_Mar=C3=ADn?=)
Date: Wed, 27 Jul 2005 14:05:15 +0100
Subject: [R] make.names() does not return what is expected
In-Reply-To: <Pine.LNX.4.61.0507271332110.16811@gannet.stats>
References: <20050727121719.GA9278@deu.xfim>
	<Pine.LNX.4.61.0507271332110.16811@gannet.stats>
Message-ID: <20050727130515.GA9544@deu.xfim>

Ok, thanks.

BTW, before writing the email I've been trying to search in the bugs
web interface at
http://r-bugs.biostat.ku.dk/cgi-bin/R

searching for make.names, but with no success. Is this the way to procede?

Prof Brian Ripley vas escriure el dia dc, 27 jul 2005:

> It's a bug, soon to be fixed in R-patched.
> 

-- 

Xavier Fernndez i Marn
xavier.fim at menta.net
^^^^^^^^^^^^^^^^^^^^^^^^



From Friedrich.Leisch at tuwien.ac.at  Wed Jul 27 15:13:10 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 27 Jul 2005 15:13:10 +0200
Subject: [R] LyX and Sweave
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31946@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31946@pollux.bfro.uni-lj.si>
Message-ID: <17127.34918.405207.234560@celebrian.ci.tuwien.ac.at>

>>>>> On Wed, 27 Jul 2005 14:47:21 +0200,
>>>>> Gorjanc Gregor (GG) wrote:

[...]

  >>> To add to the discussion: One might even get better integration
  >>> between Sweave and LyX because LyX allows specification of file
  >>> converters:

  Edit-> Preferences->Converters
  Edit-> Preferences->File Formats

  >> After registering .Stex as a file format for Sweave I can import .Stex
  >> files using conversion command

  >> reLyX -f $$i && mv $$i.lyx $$o

  >> and the move is necessary only because reLyX produces foo.Stex.lyx
  >> rather then foo.lyx.

  > Nice!

  >> Using an Sweave shell script like Gregor's in combination with
  >> "lyx -e latex" might do the trick to have direct conversion to PDF
  >> from the LyX GUI.

  > Note that this is not the only possible way. As I showed, the other one i.e.
  > noweb mode in LyX (look in file SweaveNoweb.lyx and its PDF at URL above) 
  > can deal with Sweave in all ways i.e. export or view to any supported formats 
  > by LyX.

The point was actually not so much about .Stex versus .Snw but that
you can integrate all into the LyX GUI: For my import above I get a
menu entry

	File->Import->Sweave

in the LyX GUI whhich probably is appealing to GUI-users. The same
could be done by registering .Snw and modifying the noweb import
filter accordingly.

The same should be done for output, such that one can do

	File->Export->Sweave+PDF

(or whatever you want to call it) directly from within LyX, rather
then saving & renaming as you suggest under "Compilation".

But I'm too much of an Emacs person to really spend time on such
things ... but I'll be happy to collect information and integrate it
into the Sweave manual.

Best,
Fritz



From plummer at iarc.fr  Wed Jul 27 15:23:54 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 27 Jul 2005 15:23:54 +0200
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <1122470634.3702.18.camel@seurat>

On Wed, 2005-07-27 at 12:11 +0200, Jan Verbesselt wrote:
> Dear R-helpers,
> 
>  
> 
> Is it possible to create a type of 'movie' in R based on the output of
> several figures (e.g., jpegs) via the plot() function.  I obtained dynamic
> results with the plotting function and would like to save these as a movie
> (e.g., avi or other formats)?

You can use ImageMagick tools to convert a set of bitmap files into an
animated GIF. For example, in R, this creates 101 separate png files:

x <- seq(-1,1,length=101)
y <- x^2
png()
for (i in 1:length(y)) {
  plot(x[1:i], y[1:i], xlim=c(-1,1), ylim=c(0,1), type="l")
}
dev.off()

Then, on the command line (on Linux) this joins them together

convert -delay 10 Rplot*.png Rplot.gif
animate Rplot.gif

You can also create MNG format, but this is less widely supported.

Martyn 


-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jul 27 15:28:54 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 27 Jul 2005 15:28:54 +0200
Subject: [R] LyX and Sweave
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31948@pollux.bfro.uni-lj.si>

[...]

>  >> Using an Sweave shell script like Gregor's in combination with
>  >> "lyx -e latex" might do the trick to have direct conversion to PDF
>  >> from the LyX GUI.

>  > Note that this is not the only possible way. As I showed, the other one i.e.
>  > noweb mode in LyX (look in file SweaveNoweb.lyx and its PDF at URL above) 
>  > can deal with Sweave in all ways i.e. export or view to any supported formats 
>  > by LyX.

> The point was actually not so much about .Stex versus .Snw but that
> you can integrate all into the LyX GUI: For my import above I get a
> menu entry

>	File->Import->Sweave

> in the LyX GUI whhich probably is appealing to GUI-users. The same
> could be done by registering .Snw and modifying the noweb import
> filter accordingly.

> The same should be done for output, such that one can do

> 	File->Export->Sweave+PDF

Nope. That's what I also thought, but I was really amaized when discovered that
there is no need for this. You only need to set up noweb stuff as mentioned at

<http://thread.gmane.org/gmane.editors.lyx.general/18847>

and use article(noweb) for document style. Then everything (export and view 
directly from LyX GUI) just works. As I said I was really surprised with it. 
Try with SweaveNoweb.lyx and you will see. This way, one can skip my silly
Sweave.sh ;)

> (or whatever you want to call it) directly from within LyX, rather
> then saving & renaming as you suggest under "Compilation".

> But I'm too much of an Emacs person to really spend time on such
> things ... but I'll be happy to collect information and integrate it
> into the Sweave manual.

I also prefer Emacs this days, and I really did not spend a lot on this LyX
feature, but since it is "so easy" I just couldn't retain to share.

Gregor



From Friedrich.Leisch at tuwien.ac.at  Wed Jul 27 15:49:19 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Wed, 27 Jul 2005 15:49:19 +0200
Subject: [R] LyX and Sweave
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31948@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31948@pollux.bfro.uni-lj.si>
Message-ID: <17127.37087.151213.51904@celebrian.ci.tuwien.ac.at>


>>>>> On Wed, 27 Jul 2005 15:28:54 +0200,
>>>>> Gorjanc Gregor (GG) wrote:

[...]

  > Nope. That's what I also thought, but I was really amaized when discovered that
  > there is no need for this. You only need to set up noweb stuff as mentioned at

  > <http://thread.gmane.org/gmane.editors.lyx.general/18847>

  > and use article(noweb) for document style. Then everything (export and view 
  > directly from LyX GUI) just works. As I said I was really surprised with it. 
  > Try with SweaveNoweb.lyx and you will see. This way, one can skip my silly
  > Sweave.sh ;)

Ahh, now I get it. Still it needs some polishing, because after
configuring LyX that way regular noweb will no longer work, but it
shouldn't take too much of an effort to duplicate the noweb configs in
LyX to recognise Sweave.

That certainly looks like the way to go, might give Sweave another
strong push because people do no longer have to learn all of LaTeX ...

If we make a clean solution then I think it shouldn't be too hard to
convince the LyX guys to include it in their distribution -> that
would make an out-of-the-box solution.

But it certainly looks very promising.

Best,
Fritz



From p.campbell at econ.bbk.ac.uk  Wed Jul 27 15:46:37 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Wed, 27 Jul 2005 14:46:37 +0100
Subject: [R] Off Topic Simulation Techniques.
Message-ID: <s2e79e72.037@markets.econ.bbk.ac.uk>

I wonder if anyone can help me find a text or reference to the
probabilistic under pinnings of simulation techniques.  I come from an
econometrics background where the approach to Monte Carlo is a bit cook
booky, most of the focus is on the implementation rather than the
theoretical justification.  

Given a standard setup; a probability triple, random variable and a
functional on this random variable, we need to find the measure on the
functional.  This measure can be approximated by the ECDF, which is the
counting measure.  Asymptotically the counting measure must coincide
with the measure on the functional.  Are there any authors who have
followed this approach?


Phineas Campbell



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jul 27 16:02:41 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 27 Jul 2005 16:02:41 +0200
Subject: [R] LyX and Sweave
Message-ID: <7FFEE688B57D7346BC6241C55900E730F3194A@pollux.bfro.uni-lj.si>

[...]

>  > Nope. That's what I also thought, but I was really amaized when discovered that
>  > there is no need for this. You only need to set up noweb stuff as mentioned at

>  > <http://thread.gmane.org/gmane.editors.lyx.general/18847>

>  > and use article(noweb) for document style. Then everything (export and view 
>  > directly from LyX GUI) just works. As I said I was really surprised with it. 
>  > Try with SweaveNoweb.lyx and you will see. This way, one can skip my silly
>  > Sweave.sh ;)

> Ahh, now I get it. Still it needs some polishing, because after
> configuring LyX that way regular noweb will no longer work, but it
> shouldn't take too much of an effort to duplicate the noweb configs in
> LyX to recognise Sweave.

Nice isn't it.

> That certainly looks like the way to go, might give Sweave another
> strong push because people do no longer have to learn all of LaTeX ...

> If we make a clean solution then I think it shouldn't be too hard to
> convince the LyX guys to include it in their distribution -> that
> would make an out-of-the-box solution.

Yes, LyX developers should copy "noweb configuration" to "Sweave configuration" and
add small (like yours at Sweave repository) Sweave.sh and additionaly also
import configuration you mentioned before. I strongly encourage you to 
send this to LyX developers.

Gregor



From ripley at stats.ox.ac.uk  Wed Jul 27 16:10:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 15:10:05 +0100 (BST)
Subject: [R] make.names() does not return what is expected
In-Reply-To: <20050727130515.GA9544@deu.xfim>
References: <20050727121719.GA9278@deu.xfim>
	<Pine.LNX.4.61.0507271332110.16811@gannet.stats>
	<20050727130515.GA9544@deu.xfim>
Message-ID: <Pine.LNX.4.61.0507271509350.17501@gannet.stats>

On Wed, 27 Jul 2005, Xavier [utf-8] Fern?ndez i Mar?n wrote:

> Ok, thanks.
> BTW, before writing the email I've been trying to search in the bugsweb interface athttp://r-bugs.biostat.ku.dk/cgi-bin/R
> searching for make.names, but with no success. Is this the way to procede?

See the posting guide about how to discover already fixed bugs (not 
including this one).

> Prof Brian Ripley vas escriure el dia dc, 27 jul 2005:
>> It's a bug, soon to be fixed in R-patched.>
> -- 
> Xavier Fern?ndez i Mar?nxavier.fim at menta.net^^^^^^^^^^^^^^^^^^^^^^^^
> ______________________________________________R-help at stat.math.ethz.ch mailing listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Christian.Stratowa at vie.boehringer-ingelheim.com  Wed Jul 27 16:13:41 2005
From: Christian.Stratowa at vie.boehringer-ingelheim.com (Christian.Stratowa@vie.boehringer-ingelheim.com)
Date: Wed, 27 Jul 2005 16:13:41 +0200
Subject: [R] Asymmetric colors for heatmap
Message-ID: <7433775A4AB9D147BF1DF09DBEFA0B3F387DB9@vieex07.eu.boehringer.com>

Sorry, my mistake, I did not realize that image() has breaks and that
heatmap() inherits from image().
However, I have the following problem, maybe I am doing something wrong.

I have defined:
   mycol <-
c("blue1","blue2","blue3","blue4","black","yellow4","yellow3","yellow2","yel
low1")
   breaks<-c(0,0.5,1,1.5,2,2.5,3,4,5,6)

When I call image() then the colors change slightly:
   image(t(tmp),col=mycol,axes=F)
   image(t(tmp),col=mycol,axes=F,breaks=breaks)

However, when I call heatmap():
   heatmap(tmp,Rowv=NA,Colv=NA,col=mycol)
   heatmap(tmp,Rowv=NA,Colv=NA,col=mycol,breaks=breaks)
then breaks results in half of the heatmap drawn in white!

Do you know what may be my mistake?

Best regards
Christian

-----Original Message-----
From: Stratowa,Dr.,Christian FEX BIG-AT-V
Sent: Wednesday, July 27, 2005 14:07
To: 'Sean Davis'; r-help at stat.math.ethz.ch
Subject: RE: [R] Asymmetric colors for heatmap


Dear Sean

Thank you, however the heatmap() function from the stats package does not
have this option.
Browsing around I see that you  mean heatmap.2() from package gplots, which
we have not
installed yet.

Is there also another possibility besides heatmap.2() since I would also
need this option for
e.g. function image().

Best regards
Christian

-----Original Message-----
From: Sean Davis [mailto:sdavis2 at mail.nih.gov]
Sent: Wednesday, July 27, 2005 13:16
To: Christian.Stratowa at vie.boehringer-ingelheim.com;
r-help at stat.math.ethz.ch
Subject: Re: [R] Asymmetric colors for heatmap


See the breaks argument to heatmap.

Sean



From Charles.Annis at StatisticalEngineering.com  Wed Jul 27 16:14:12 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 27 Jul 2005 10:14:12 -0400
Subject: [R] Off Topic Simulation Techniques.
In-Reply-To: <s2e79e72.037@markets.econ.bbk.ac.uk>
Message-ID: <200507271414.j6REE3xS016650@hypatia.math.ethz.ch>

_Monte Carlo Statistical Methods_
by Christian P. Robert, George Casella
Springer, 2nd ed 2005


This book (I have edition 1) is a dandy.  It will be rough sledding unless
you have a reasonable background in math stats but I think it is just what
you are looking for.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Campbell
Sent: Wednesday, July 27, 2005 9:47 AM
To: r-help
Cc: phineas
Subject: [R] Off Topic Simulation Techniques.

I wonder if anyone can help me find a text or reference to the
probabilistic under pinnings of simulation techniques.  I come from an
econometrics background where the approach to Monte Carlo is a bit cook
booky, most of the focus is on the implementation rather than the
theoretical justification.  

Given a standard setup; a probability triple, random variable and a
functional on this random variable, we need to find the measure on the
functional.  This measure can be approximated by the ECDF, which is the
counting measure.  Asymptotically the counting measure must coincide
with the measure on the functional.  Are there any authors who have
followed this approach?


Phineas Campbell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From herodote at oreka.com  Wed Jul 27 16:14:14 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Wed, 27 Jul 2005 15:14:14 +0100
Subject: [R] =?iso-8859-1?q?thks_all?=
Message-ID: <IKAI7Q$F7435DF403F30E1E58E3E1830A8C1E20@oreka.com>

hi all

I wish to thanks every body on the R mailing list for answering very fast, directly in my mail box ;).

I've finish my work with R and i can say that it is very difficult at the beginning, and when you succeed you are stopped by a stack overflow when you call your nice recursive function (which was working with a tab of 100 element) with a tab of 900 elements, but R just do what you tell him and these errors have push me to optimize , i succeed to do it with R ! and you've helped me that way :)

so THKS all R-users!
guillaume.


////////////////////////////////////////////////////////////
// Webmail Oreka : http://www.oreka.com
////////////////////////////////////////////////////////////



From pinard at iro.umontreal.ca  Wed Jul 27 16:25:45 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 27 Jul 2005 10:25:45 -0400
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <20050727142545.GA15862@alcyon.progiciels-bpi.ca>

[Jan Verbesselt]

> Is it possible to create a type of 'movie' in R based on the output
> of several figures (e.g., jpegs) via the plot() function.  I obtained
> dynamic results with the plotting function and would like to save
> these as a movie (e.g., avi or other formats)?

You may also peek at an actual example of using R for mini-movies:

   http://pinard.progiciels-bpi.ca/plaisirs/animations/index.html

I wrote this toy about the same week I started to learn R, and it was a
hell of a good exercise for the poor little me! :-)

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From ghislainv at gmail.com  Wed Jul 27 16:26:32 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Wed, 27 Jul 2005 16:26:32 +0200
Subject: [R] Question on glm for Poisson distribution.
Message-ID: <ff51f02205072707261837b51a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/87f2df41/attachment.pl

From Robert.McGehee at geodecapital.com  Wed Jul 27 16:30:58 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 27 Jul 2005 10:30:58 -0400
Subject: [R] Redirecting Messages and Errors to a file
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946641@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello all,
I'm trying to setup a simple construct that will email me all errors,
warnings, and output of a sample R script. My initial thought was to use
sink() and tryCatch around the script such that at the end of the script
any output would be mailed to me, i.e.:

sinkFile <- tempfile()
sink(sinkFile)
tryCatch({
	[R script]},
	finally = {
		sink()
		if (length(readLines(sinkFile)))
			mailOutput(readLines(sinkFile))
	})

However, the sink() does not seem to be capturing the error messages as
I would have hoped. That is, if the R-script is {print("abc");
stop("def")}, the print output is captured to the file, the stop error
message is not, and instead sent to the screen.

Any thoughts on how to do this such that everything is sent to the file
for emailing? Perhaps something different all together?

Thanks,
Robert


This e-mail, and any attachments hereto, are intended for us...{{dropped}}



From tlumley at u.washington.edu  Wed Jul 27 16:32:07 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 27 Jul 2005 07:32:07 -0700 (PDT)
Subject: [R] spss.read factor reversal
In-Reply-To: <x2zms8k4gw.fsf@turmalin.kubism.ku.dk>
References: <1253d67a05072617042eb9e9ec@mail.gmail.com>
	<1122424251.6146.36.camel@dhcppc3>
	<x2zms8k4gw.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.A41.4.61b.0507270729310.316858@homer03.u.washington.edu>

On Wed, 27 Jul 2005, Peter Dalgaard wrote:

> Adaikalavan Ramasamy <ramasamy at cancer.org.uk> writes:
>
>> I think it is doing what is supposed to do but I never used read.spss,
>> so take this with a pinch of salt.
>>
>> In R when you use as.integer on a factor, the one with the lowest level
>> gets a value of 1 and so on. The lowest level of the factor can
>> determined from levels() function.
>>
>>    f <- factor( c("Green", "Green", "Red", "Blue"),
>>                 levels=c("Red", "Blue", "Green") )
>>    levels(f)
>>    [1] "Red"   "Blue"  "Green"
>>
>>    as.integer(f)
>>    [1] 3 3 1 2
>>
>> But the levels of a factor can be changed
>>
>>    as.integer( factor( f, levels=c("Green", "Blue", "Red" ) ) )
>>    [1] 1 1 3 2
>
> Doesn't explain why  1 2 3 in the input file comes out as Green Blue
> Red, does it?
>
>> You can also try setting use.value.labels=FALSE in read.spss function
>> and then creating a factor out of it.
>
> Would be interesting to see this. I would suspect that the damage is
> already done at that point though.

It would also be interesting because the raw value label information is 
then available as the "value.labels" attribute of the variable.

> I notice that the value labels are in reverse order. Shouldn't matter
> to read.spss which has
>
>            rval[[nm]] <- factor(rval[[nm]], levels = vl[[v]],
>                labels = trim(names(vl[[v]])))
>
> i.e. levels and labels should be in the correct order.
>

yes, something wrong must be coming out of the .C call.

 	-thomas



From tlumley at u.washington.edu  Wed Jul 27 16:37:28 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 27 Jul 2005 07:37:28 -0700 (PDT)
Subject: [R] Redirecting Messages and Errors to a file
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C946641@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020C946641@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.A41.4.61b.0507270735090.316858@homer03.u.washington.edu>

On Wed, 27 Jul 2005, McGehee, Robert wrote:

>
> However, the sink() does not seem to be capturing the error messages as
> I would have hoped. That is, if the R-script is {print("abc");
> stop("def")}, the print output is captured to the file, the stop error
> message is not, and instead sent to the screen.
>
> Any thoughts on how to do this such that everything is sent to the file
> for emailing? Perhaps something different all together?

There is a separate output connection for errors.  The help page for 
sink() describes how to divert it, and warns of risks in doing so.

 	-thomas



From baron at psych.upenn.edu  Wed Jul 27 16:38:08 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 27 Jul 2005 10:38:08 -0400
Subject: [R] how to generate 00Index.html from Rd or html
Message-ID: <20050727143808.GA12347@psych>

I'd like to generate 00Index.html for a bunch of libraries for my 
R search page.  I have now almost all the html help files for
individual functions, even for packages that won't install on my
computer (e.g., because they are specific to Windows).  I made
these using Rdconv.  But that won't make the 00Index file, and
neither will Rdindex, which makes a text file.  I'm probably
missing something really simple, but I've looked for a while.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From efg at stowers-institute.org  Wed Jul 27 16:39:37 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 27 Jul 2005 09:39:37 -0500
Subject: [R] HOW to Create Movies with R with repeated plot()?
References: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <dc86bb$75e$1@sea.gmane.org>



"Jan Verbesselt" <Jan.Verbesselt at biw.kuleuven.be> wrote in message
news:000e01c59293$80ee5d00$1145210a at agr.ad10.intern.kuleuven.ac.be...
Dear R-helpers,

Is it possible to create a type of 'movie' in R based on the output of
several figures (e.g., jpegs) via the plot() function.  I obtained dynamic
results with the plotting function and would like to save these as a movie
(e.g., avi or other formats)?

1) You can create animated GIFs with the caTools package.

2) ImageMagick (clipped from R-Help)
It is quite easy to do with ImageMagick (www.imagemagick.org), which can be
installed on most OSes.  I tried this simple sequence and it worked

beautifully.



In R:



> for(i in 1:5) {

+ jpeg(paste("fig", i, ".jpg", sep = ""))

+ hist(rnorm(100))

+ dev.off()

+ }



Then from the command line (I tried it using Linux, though it should be the

same on any platform):



% convert -delay 50 -loop 50 fig*.jpg animated.gif



This created animated.gif, a nice animation of my sequence of files.  You

can control the timing of the animation by playing with -delay and -loop.



3) TCL/TK Animation (clipped from R-Help)



The tcltk package also has ways of doing this kind of stuff:



 library(tcltk)

 f <- function(){plot(rnorm(1000)); tkcmd("after", 1000,f)}

 f()



(To stop, set  f <- NULL)





efg



From helprhelp at gmail.com  Wed Jul 27 16:43:30 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 27 Jul 2005 09:43:30 -0500
Subject: [R] CART analysis
In-Reply-To: <20050727010321.45037.qmail@web32604.mail.mud.yahoo.com>
References: <1122424981.6146.39.camel@dhcppc3>
	<20050727010321.45037.qmail@web32604.mail.mud.yahoo.com>
Message-ID: <cdf8178305072707433a723dfb@mail.gmail.com>

Most of time, I prefer to use randomForest of R though it really
depends on your problem. And don't forget Leo's Random Forest if you
don't use it for commercial purpose.

weiwei

On 7/26/05, Marc R. Feldesman <feldesmanm at pdx.edu> wrote:
> 
> 
> --- Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> 
> > RSiteSearch("CART") should bring up a few hits including
> >
> >   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/25850.html
> >
> 
> "CART" is a trademarked statistical procedure owned by Salford Systems
> of San Diego, CA.  If you're looking for an R-package that implements
> the procedures described in the Breiman, Friedman, Olshen, and Stone
> book entitled "Classification and Regression Trees", the closest you
> can come to the original algorithm is the R-package called "rpart", by
> Therneau and Atkinson.  If you combine that with Andy Liaw's
> "randomForest", you have a pretty potent set of tools.  If you really
> want "CART", you need to contact Salford Systems for their
> implementation and pay their very expensive licensing fees.
> 
> Dr. Marc R Feldesman
> Professor & Chair Emeritus
> Department of Anthropology
> Portland State University
> Portland, OR 97207
> 
> Please respond to all emails at:  feldesmanm at pdx.edu
> 
> "Some people live and die by actuarial tables"  Groundhog Day
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From helprhelp at gmail.com  Wed Jul 27 16:54:31 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 27 Jul 2005 09:54:31 -0500
Subject: [R] thks all
In-Reply-To: <IKAI7Q$F7435DF403F30E1E58E3E1830A8C1E20@oreka.com>
References: <IKAI7Q$F7435DF403F30E1E58E3E1830A8C1E20@oreka.com>
Message-ID: <cdf817830507270754243dcb59@mail.gmail.com>

add me.

I really appreciate r-help mail list VERY MUCH! I learn a lot.  You 
guys are doing a great and very important job.

thanks.

On 7/27/05, herodote at oreka.com <herodote at oreka.com> wrote:
> hi all
> 
> I wish to thanks every body on the R mailing list for answering very fast, directly in my mail box ;).
> 
> I've finish my work with R and i can say that it is very difficult at the beginning, and when you succeed you are stopped by a stack overflow when you call your nice recursive function (which was working with a tab of 100 element) with a tab of 900 elements, but R just do what you tell him and these errors have push me to optimize , i succeed to do it with R ! and you've helped me that way :)
> 
> so THKS all R-users!
> guillaume.
> 
> 
> ////////////////////////////////////////////////////////////
> // Webmail Oreka : http://www.oreka.com
> ////////////////////////////////////////////////////////////
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From Robert.McGehee at geodecapital.com  Wed Jul 27 16:54:47 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 27 Jul 2005 10:54:47 -0400
Subject: [R] Redirecting Messages and Errors to a file
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946642@MSGBOSCLB2WIN.DMN1.FMR.COM>

Ahh, I misunderstood the help page. I took that since the default values
for type were c("output", "message"), that both the output _and_ the
messages were simultaneously being redirected (else why not just make
the default "output"). I see that this is not the case (as noted in the
details). 

After specifying type = "message", I was also able to successfully send
off the error messages to a file, but not at the same time as sending
the output. That is, do I need to have two sink files, one for errors,
one for output and then append them before mailing? I'd prefer a way of
sending all output/messages to the same file to preserve the order of
the messages/output, if this is possible.

Robert

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Wednesday, July 27, 2005 10:37 AM
To: McGehee, Robert
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Redirecting Messages and Errors to a file


On Wed, 27 Jul 2005, McGehee, Robert wrote:

>
> However, the sink() does not seem to be capturing the error messages
as
> I would have hoped. That is, if the R-script is {print("abc");
> stop("def")}, the print output is captured to the file, the stop error
> message is not, and instead sent to the screen.
>
> Any thoughts on how to do this such that everything is sent to the
file
> for emailing? Perhaps something different all together?

There is a separate output connection for errors.  The help page for 
sink() describes how to divert it, and warns of risks in doing so.

 	-thomas



From shellyzhang77 at gmail.com  Wed Jul 27 17:21:09 2005
From: shellyzhang77 at gmail.com (qi zhang)
Date: Wed, 27 Jul 2005 11:21:09 -0400
Subject: [R] error message running R2WinBUGS
Message-ID: <d9f1219405072708217fabdd1d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/9ad6a4ec/attachment.pl

From David.Ruau at rwth-aachen.de  Wed Jul 27 17:21:58 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Wed, 27 Jul 2005 17:21:58 +0200
Subject: [R] OPTICS clustering algorithm
Message-ID: <0ae211f0c2d955813d8bfabc2cf548eb@rwth-aachen.de>

Hi all,

Did anyone know where to find or have at hand an implementation of the 
OPTICS (Ordering Points To Identify the Clustering Structure) algorithm 
for R?

David



From Ted.Harding at nessie.mcc.ac.uk  Wed Jul 27 17:44:36 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 27 Jul 2005 16:44:36 +0100 (BST)
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <XFMail.050727140804.Ted.Harding@nessie.mcc.ac.uk>

On 27-Jul-05 Jan Verbesselt wrote:
> Dear R-helpers,
> 
> Is it possible to create a type of 'movie' in R based on the
> output of several figures (e.g., jpegs) via the plot() function.
> I obtained dynamic results with the plotting function and would
> like to save these as a movie (e.g., avi or other formats)?

A similar question was discussed on R-help some months ago: Start
at

  Cari G Kaufman <cgk at stat.cmu.edu>, 
  "[R] animation without intermediate files?"
  Thu, 6 Jan 2005 16:21:36 -0500 (EST)

However, this is not quite your question! It seems you would be
content with making a movie out of files already saved from R,
so it's not a "real-time" situation.

There are several approaches you could adopt.

One (if you are using Linux with ImageMagick installed -- or if
this is available for your platform if different) is to use the
program 'animate' in that suite of programs.

The format of the image files does not matter for 'animate'
(though I think all should have the same format), so they could
be .jpg, .gif, etc. Suppose you had a set of files

  graph001.jpg
  graph001.jpg
  ...
  graph100.jpg

then the command

  animate graph*.jpg

would present them as an animation in due order. There are several
options to the 'animate' command which you can use to control
features of how they are displayed. One you would probably want to
use is "-delay" -- e.g.

  animate -delay 7 graph*.jpg

would pause for 7/100 seconds between successive images.

Another option is to create an "animated GIF" -- a single file
which encapsulates several images and displays them in sequence
when the file is opened with suitable software (most Web browsers
do this well; likewise so does ImageMagick's 'display').

Programs which can create animated GIFs include the GIMP and
gifsicle in the Free Software world. There are also several programs
for non-Linux platforms.

Sorry I can't help with suggestions for creating .avi files.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 27-Jul-05                                       Time: 14:08:00
------------------------------ XFMail ------------------------------



From Jan.Verbesselt at biw.kuleuven.be  Wed Jul 27 17:52:10 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Wed, 27 Jul 2005 17:52:10 +0200
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <20050727142545.GA15862@alcyon.progiciels-bpi.ca>
References: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
	<20050727142545.GA15862@alcyon.progiciels-bpi.ca>
Message-ID: <1122479530.42e7adaad1fb9@webmail2.kuleuven.be>

Thanks!

I only cannot use the functions nr.movie() or nr.image() because I'm
plotting data points (scatterplots) through time and do not use
expressions which are needed for these functions.

I will probably use the dos program BMP2avi available at:
http://www.bialith.com/TPDownloads/BATPDownloads.htm and try to run it
via DOS code in R ...if that works.

(I'm working on a winXP workstation)

Regards,
Jan


Quoting Fran??ois Pinard <pinard at iro.umontreal.ca>:

> [Jan Verbesselt]
> 
> > Is it possible to create a type of 'movie' in R based on the
> output
> > of several figures (e.g., jpegs) via the plot() function.  I
> obtained
> > dynamic results with the plotting function and would like to save
> > these as a movie (e.g., avi or other formats)?
> 
> You may also peek at an actual example of using R for mini-movies:
> 
>    http://pinard.progiciels-bpi.ca/plaisirs/animations/index.html
> 
> I wrote this toy about the same week I started to learn R, and it was
> a
> hell of a good exercise for the poor little me! :-)
> 
> -- 
> Fran??ois Pinard   http://pinard.progiciels-bpi.ca
> 
> 


-- 
Ir. Jan Verbesselt
Research Associate
Lab of Geomatics, K.U.Leuven
Vital Decosterstraat 102, 3000 Leuven, Belgium
Tel:+32-16-329750   Fax:+32-16-329760
http://gloveg.kuleuven.ac.be/



From gunter.berton at gene.com  Wed Jul 27 18:00:32 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 27 Jul 2005 09:00:32 -0700
Subject: [R] Redirecting Messages and Errors to a file
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C946642@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <200507271600.j6RG0WaA023444@faraday.gene.com>

Robert:

I agree that the Help file does not clearly state that one sink file can be
used, but neither does it say that it can't. So why not **try it and see**.
Following the Help instructions I tried:

> con<-file('test',open='w')
> sink(con)
## Now produce some output
> rnorm(10)
> sink(con,type='message',append=TRUE)
## produce an error message
> rnorm('abc')
## more output
> rnorm(10)
## stop sinking for messages and output
> sink(NULL,type='message')
> sink(NULL)
## see what went into the file
> readLines('test')

So one sees that one file **can** be used. One also reads that it probably
should **not** be.

While one would prefer that Help documents were written so clearly and
completely that no possibility of misinterpretation or confusion could
occur, that is a standard way beyond any to which I would wish to be held. I
think the core code documentation is generally remarkably clear (especially
when supplemented by the several available manuals), but sometimes
ambiguities like this do occur.  I have found that whenever I have questions
of this sort, a little experimentation like the above almost always resolves
my confusion -- and very rarely even uncovers a legitimate bug or
documentation error. I know this takes a bit more time than a post to the
list (which gracious gurus often answer in minutes), but you tend to learn a
lot more by doing the experimentation first.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
> Sent: Wednesday, July 27, 2005 7:55 AM
> To: Thomas Lumley
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Redirecting Messages and Errors to a file
> 
> Ahh, I misunderstood the help page. I took that since the 
> default values
> for type were c("output", "message"), that both the output _and_ the
> messages were simultaneously being redirected (else why not just make
> the default "output"). I see that this is not the case (as 
> noted in the
> details). 
> 
> After specifying type = "message", I was also able to 
> successfully send
> off the error messages to a file, but not at the same time as sending
> the output. That is, do I need to have two sink files, one for errors,
> one for output and then append them before mailing? I'd 
> prefer a way of
> sending all output/messages to the same file to preserve the order of
> the messages/output, if this is possible.
> 
> Robert
> 
> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> Sent: Wednesday, July 27, 2005 10:37 AM
> To: McGehee, Robert
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Redirecting Messages and Errors to a file
> 
> 
> On Wed, 27 Jul 2005, McGehee, Robert wrote:
> 
> >
> > However, the sink() does not seem to be capturing the error messages
> as
> > I would have hoped. That is, if the R-script is {print("abc");
> > stop("def")}, the print output is captured to the file, the 
> stop error
> > message is not, and instead sent to the screen.
> >
> > Any thoughts on how to do this such that everything is sent to the
> file
> > for emailing? Perhaps something different all together?
> 
> There is a separate output connection for errors.  The help page for 
> sink() describes how to divert it, and warns of risks in doing so.
> 
>  	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jmartinez5 at verizon.net  Wed Jul 27 18:09:43 2005
From: jmartinez5 at verizon.net (Jason W. Martinez)
Date: Wed, 27 Jul 2005 09:09:43 -0700
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <XFMail.050727140804.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050727140804.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1122480583.4007.15.camel@wizard>

Dear R-list,

I'm surprised that no one on this list has mentioned anything about the
ffmpeg tools! Hands down these are the tools to use for generating
animations and movies.

Processing the images is much quicker than imagemagick and with a little
tweaking, you can add additional filter effects to the images as they
are being processed.

http://ffmpeg.sourceforge.net/index.php

Also see the smilutils and some examples at:

http://users.pandora.be/acp/ppmfilter/

Although I haven't used these tools in quite a while, I have been able 
to export the resulting mpeg files into VCD quality videos for playing
on DVDs.

I hope this helps.

Jason


On Wed, 2005-07-27 at 08:44, Ted.Harding at nessie.mcc.ac.uk wrote:
> On 27-Jul-05 Jan Verbesselt wrote:
> > Dear R-helpers,
> > 
> > Is it possible to create a type of 'movie' in R based on the
> > output of several figures (e.g., jpegs) via the plot() function.
> > I obtained dynamic results with the plotting function and would
> > like to save these as a movie (e.g., avi or other formats)?
> 
> A similar question was discussed on R-help some months ago: Start
> at
> 
>   Cari G Kaufman <cgk at stat.cmu.edu>, 
>   "[R] animation without intermediate files?"
>   Thu, 6 Jan 2005 16:21:36 -0500 (EST)
> 
> However, this is not quite your question! It seems you would be
> content with making a movie out of files already saved from R,
> so it's not a "real-time" situation.
> 
> There are several approaches you could adopt.
> 
> One (if you are using Linux with ImageMagick installed -- or if
> this is available for your platform if different) is to use the
> program 'animate' in that suite of programs.
> 
> The format of the image files does not matter for 'animate'
> (though I think all should have the same format), so they could
> be .jpg, .gif, etc. Suppose you had a set of files
> 
>   graph001.jpg
>   graph001.jpg
>   ...
>   graph100.jpg
> 
> then the command
> 
>   animate graph*.jpg
> 
> would present them as an animation in due order. There are several
> options to the 'animate' command which you can use to control
> features of how they are displayed. One you would probably want to
> use is "-delay" -- e.g.
> 
>   animate -delay 7 graph*.jpg
> 
> would pause for 7/100 seconds between successive images.
> 
> Another option is to create an "animated GIF" -- a single file
> which encapsulates several images and displays them in sequence
> when the file is opened with suitable software (most Web browsers
> do this well; likewise so does ImageMagick's 'display').
> 
> Programs which can create animated GIFs include the GIMP and
> gifsicle in the Free Software world. There are also several programs
> for non-Linux platforms.
> 
> Sorry I can't help with suggestions for creating .avi files.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 27-Jul-05                                       Time: 14:08:00
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 

Jason W. Martinez
E-mail: jmartinez5 at verizon.net



From edhuang00 at yahoo.com  Wed Jul 27 18:22:55 2005
From: edhuang00 at yahoo.com (Haibo Huang)
Date: Wed, 27 Jul 2005 09:22:55 -0700 (PDT)
Subject: [R] logistic regression: categorical value, and multinomial
Message-ID: <20050727162255.41159.qmail@web31015.mail.mud.yahoo.com>

I have two questions:

1. If I want to do a binomial logit, how to handle the
categorical response variable? Data for the response
variables are not numerical, but text.

2. What if I want to do a multinomial logit, still
with categorical response variable? The variable has 5
non-numerical response levels, I have to do it with a
multinomial logit.

Any input is highly appreciated! Thanks!

Ed



From lobrien at icoria.com  Wed Jul 27 18:32:47 2005
From: lobrien at icoria.com (O'Brien, Laura)
Date: Wed, 27 Jul 2005 12:32:47 -0400
Subject: [R] core dump when call t.test via the "RJava --example --gui-none"
	interface
Message-ID: <9F19C13E789D6E45A144CF22EDA28E3E06296E@rtp-exchng02.paradigm.paragen.com>

using ./RJava --example --gui-none to invoke t.test core dumps.  The line of R works if I go directly thru R and not RJava.

Version info and code are below.  Any help would be appreciated.

--Laura O'Brien
Applications Architect


Version info
------------
Java: 	jdk1.5.0_03
R: 		2.1.1
SJava:	0.68
OS:		SunOs 5.8

example code that works 
-----------------------
[omegahat->R] mean (c (1,2,3))   
2.0

example code that core dumps
-----------------------------
[omegahat->R] t.test (c (1,2,3), c(4,5,6))

Unexpected Signal : 11 occurred at PC=0xFE0C2B44
Function=[Unknown. Nearest: JVM_Close+0x6BFC4]
Library=/imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so

Current Java thread:
        at org.omegahat.R.Java.REvaluator.eval(Native Method)
        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:90)
        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:40)
        at org.omegahat.R.Java.Examples.JavaRPrompt.main(JavaRPrompt.java:26)

Dynamic libraries:
0x10000         /tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/bin/java
0xff360000      /usr/lib/libthread.so.1
0xff3a0000      /usr/lib/libdl.so.1
0xff200000      /usr/lib/libc.so.1
0xff330000      /usr/platform/SUNW,Ultra-60/lib/libc_psr.so.1
0xfe000000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so
0xff2d0000      /usr/lib/libCrun.so.1
0xff1e0000      /usr/lib/libsocket.so.1
0xff100000      /usr/lib/libnsl.so.1
0xff0d0000      /usr/lib/libm.so.1
0xff300000      /usr/lib/libw.so.1
0xff0b0000      /usr/lib/libmp.so.2
0xff080000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/native_threads/libhpi.so
0xff050000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libverify.so
0xfe7c0000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libjava.so
0xff020000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libzip.so
0xfdfc0000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/SJava.so
0xfdf90000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/libRSNativeJava.so
0xf1780000      /imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so
0xfc7c0000      /tools/app/gcc-3.4.3/lib/libg2c.so.0
0xfc4e0000      /tools/app/gcc-3.4.3/lib/libgcc_s.so.1
0xfc4b0000      /tools/app/R-2.1.1/lib/R/library/grDevices/libs/grDevices.so
0xfc450000      /tools/app/R-2.1.1/lib/R/library/stats/libs/stats.so
0xfc430000      /tools/app/R-2.1.1/lib/R/library/methods/libs/methods.so

Local Time = Wed Jul 27 12:28:07 2005
Elapsed Time = 28
#
# HotSpot Virtual Machine Error : 11
# Error ID : 4F530E43505002E6 01
# Please report this error at
# http://java.sun.com/cgi-bin/bugreport.cgi
#
# Java VM: Java HotSpot(TM) Client VM (1.4.1_02-b06 mixed mode)
#
# An error report file has been saved as /tmp/hs_err_pid22520.log.
# Please refer to the file for further information.
#
Abort



From HStevens at MUOhio.edu  Wed Jul 27 18:36:17 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Wed, 27 Jul 2005 12:36:17 -0400
Subject: [R] odesolve/lsoda differences on Windows and Mac
Message-ID: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>

Hi -
I am getting different results when I run the numerical integrator 
function lsoda (odesolve package) on a Mac and a PC. I am trying to 
simulating a system of 10 ODE's with two exogenous pulsed inputs to the 
system, and have had reasonably good success with many model parameter 
sets. Under some parameter sets, however, the simulations fail on the 
Mac (see error message below). The same parameter sets, however, appear 
to run fine for our computational technician on his PC, generating 
apparently very  reasonable data.

Our tech is successfully  running
Dell Latitude D810, Windows XP Pro (Service Pack 2), 1Gb
RAM.  RGUI 2.1.1

I am running:
  R Version 2.1.1  (2005-06-20) on a
Mac OS 10.3.9
   Machine Model:	Power Mac G5
   CPU Type:	PowerPC 970  (2.2)
   Number Of CPUs:	2
   CPU Speed:	2 GHz
   L2 Cache (per CPU):	512 KB
   Memory:	1.5 GB
   Bus Speed:	1 GHz
   Boot ROM Version:	5.0.7f0
   Serial Number:	XB3472Q1NVS

My Error Message
 > system.time(
+ outAc2 <- as.data.frame(lsoda(xstart,times, pondamph, parms, 
tcrit=170*730, hmin=.1))
+ )
[1] 0.02 0.01 0.04 0.00 0.00
Warning messages:
1: lsoda--  at t (=r1) and step size h (=r2), the
2:       corrector convergence failed repeatedly
3:       or with abs(h) = hmin
4: Returning early from lsoda.  Results are accurate, as far as they go

Thanks for any input.

Hank Stevens



Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From lobrien at icoria.com  Wed Jul 27 18:42:58 2005
From: lobrien at icoria.com (O'Brien, Laura)
Date: Wed, 27 Jul 2005 12:42:58 -0400
Subject: [R] unable to source a .R file using RJava
Message-ID: <9F19C13E789D6E45A144CF22EDA28E3E06296F@rtp-exchng02.paradigm.paragen.com>

I am unable to source a ".R" file using RJava.  I tried a couple of different tests:
     1)  using java and the evaluation method core dumps
     2)  using ./RJava --example --gui-none to invoke source core dumps. 
     3)  The line of R works if I go directly thru R and not RJava.

Version info and code are below. Any help would be appreciated.

--Laura O'Brien
Applications Architect



version info
------------
Java: 	j2sdk1.4.1_02
R: 		2.1.1
SJava:	0.68
OS:		SunOs 5.8


doNothing.R file I'd like to source
-----------------------------------

#
# $Author: lobrien $
#
# $Date: 2005/07/27 16:20:00 $
#
# $Header: /unixfiles/softengr/cvs/repository/hh/src/r/doNothing.R,v 1.2 2005/07/27 16:20:00 lobrien Exp $
#
#

#
# test file to see if I can source a .R file via SJava
#
# Works when
# ----------
#    I invoke R version 2.1.1 on SunOS jupiter 5.8
#  >  source ("/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R")
#  >  res <- getData("something")
#  >  res
#  [1] 2
#
#
# doesn't work when
# -----------------
# (A)   launch R thru java via -->  ./RJava --example --gui-none
#       [omegahat->R] source ("/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R")
#     
#       Unexpected Signal : 11 occurred at PC=0xFE0C2B44
#       Function=[Unknown.  Nearest:  JVM_Close+0x6BFC4]
#       <snip>
#
# (B)   call it via .java file.  See JavaRCall3



getData <- function(query,
                    db.name="csbdev",
                    user.name="noone",
                    passwd="pass"){
  R <- 2
  R
}

.java code to invoke the evaluate method
-----------------------------------------
package com.icoria.rwrapper;

import org.omegahat.R.Java.ROmegahatInterpreter;
import org.omegahat.R.Java.REvaluator;


public class JavaRCall3
{

    /**
     * want to see if I can eval a t.test command like what I would run in the
     * R command line
     */

    static public void run_sourceRFile(REvaluator e, ROmegahatInterpreter interp)
    {
        /* produces a core */
        System.err.println("running source doNothing.R");

        Object value = e.eval("source (\"/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R");
        if (value != null)
            interp.show(value);
    }


   
    
    /**
     * 
     */
    static public void main(String[] args) 
    {
        ROmegahatInterpreter interp = new ROmegahatInterpreter(ROmegahatInterpreter.fixArgs(args), false);
        REvaluator e = new REvaluator();

        Object[] funArgs;
        String[] objects;
        Object value;
        int i;

        run_sourceRFile(e, interp);   
    }
}

the corresponding core dump when using java
--------------------------------------------
[lobrien at fox] ./RJava
Loading RInterpreter library

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

running source doNothing.R
Problems: unrecognized user-level object of mode 0
Error: No default Java type for S class ???

An unexpected exception has been detected in native code outside the VM.
Unexpected Signal : 11 occurred at PC=0xF1835E50
Function=setup_Rmainloop+0x4F8
Library=/imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so

Current Java thread:
        at org.omegahat.R.Java.REvaluator.eval(Native Method)
        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:90)
        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:40)
        at com.icoria.rwrapper.JavaRCall3.run_sourceRFile(JavaRCall3.java:25)
        at com.icoria.rwrapper.JavaRCall3.main(JavaRCall3.java:46

Dynamic libraries:
0x10000         /tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/bin/java
0xff360000      /usr/lib/libthread.so.1
0xff3a0000      /usr/lib/libdl.so.1
0xff200000      /usr/lib/libc.so.1
0xff330000      /usr/platform/SUNW,Ultra-60/lib/libc_psr.so.1
0xfe000000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so
0xff2d0000      /usr/lib/libCrun.so.1
0xff1e0000      /usr/lib/libsocket.so.1
0xff100000      /usr/lib/libnsl.so.1
0xff0d0000      /usr/lib/libm.so.1
0xff300000      /usr/lib/libw.so.1
0xff0b0000      /usr/lib/libmp.so.2
0xff080000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/native_threads/libhpi.so
0xff050000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libverify.so
0xfe7c0000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libjava.so
0xff020000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libzip.so
0xfdfc0000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/SJava.so
0xfdf90000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/libRSNativeJava.so
0xf1780000      /imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so
0xfc7c0000      /tools/app/gcc-3.4.3/lib/libg2c.so.0
0xfc4e0000      /tools/app/gcc-3.4.3/lib/libgcc_s.so.1
0xfc4b0000      /tools/app/R-2.1.1/lib/R/library/grDevices/libs/grDevices.so
0xfc450000      /tools/app/R-2.1.1/lib/R/library/stats/libs/stats.so
0xfc430000      /tools/app/R-2.1.1/lib/R/library/methods/libs/methods.so

Local Time = Wed Jul 27 12:37:24 2005
Elapsed Time = 5
#
# The exception above was detected in native code outside the VM
#
# Java VM: Java HotSpot(TM) Client VM (1.4.1_02-b06 mixed mode)
#
# An error report file has been saved as /tmp/hs_err_pid22533.log.
# Please refer to the file for further information.
#
Abort

using ./RJava --example --gui-none to invoke source core dumps. 
---------------------------------------------------------------
[omegahat->R] source ("/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R")

Unexpected Signal : 11 occurred at PC=0xFE0C2B44
Function=[Unknown. Nearest: JVM_Close+0x6BFC4]
Library=/imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so

Current Java thread:
        at org.omegahat.R.Java.REvaluator.eval(Native Method)
        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:90)
        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:40)
        at org.omegahat.R.Java.Examples.JavaRPrompt.main(JavaRPrompt.java:26)

Dynamic libraries:
0x10000         /tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/bin/java
0xff360000      /usr/lib/libthread.so.1
0xff3a0000      /usr/lib/libdl.so.1
0xff200000      /usr/lib/libc.so.1
0xff330000      /usr/platform/SUNW,Ultra-60/lib/libc_psr.so.1
0xfe000000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so
0xff2d0000      /usr/lib/libCrun.so.1
0xff1e0000      /usr/lib/libsocket.so.1
0xff100000      /usr/lib/libnsl.so.1
0xff0d0000      /usr/lib/libm.so.1
0xff300000      /usr/lib/libw.so.1
0xff0b0000      /usr/lib/libmp.so.2
0xff080000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/native_threads/libhpi.so
0xff050000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libverify.so
0xfe7c0000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libjava.so
0xff020000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libzip.so
0xfdfc0000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/SJava.so
0xf1780000      /imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so
0xfc7c0000      /tools/app/gcc-3.4.3/lib/libg2c.so.0
0xfc4e0000      /tools/app/gcc-3.4.3/lib/libgcc_s.so.1
0xfc4b0000      /tools/app/R-2.1.1/lib/R/library/grDevices/libs/grDevices.so
0xfc450000      /tools/app/R-2.1.1/lib/R/library/stats/libs/stats.so
0xfc430000      /tools/app/R-2.1.1/lib/R/library/methods/libs/methods.so
0xfa3c0000      /tools/app/R-2.1.1/lib/R/modules/R_X11.so
0xfc410000      /usr/lib/libSM.so.6
0xfa390000      /usr/lib/libICE.so.6
0xf1680000      /usr/lib/libX11.so.4
0xfa2d0000      /usr/lib/libXext.so.0

Local Time = Wed Jul 27 12:39:30 2005
Elapsed Time = 37
#
# HotSpot Virtual Machine Error : 11
# Error ID : 4F530E43505002E6 01
# Please report this error at
# http://java.sun.com/cgi-bin/bugreport.cgi
#
# Java VM: Java HotSpot(TM) Client VM (1.4.1_02-b06 mixed mode)
#
# An error report file has been saved as /tmp/hs_err_pid22537.log.
# Please refer to the file for further information.
#
Abort



From michael_graber at gmx.de  Wed Jul 27 18:43:52 2005
From: michael_graber at gmx.de (Michael Graber)
Date: Wed, 27 Jul 2005 18:43:52 +0200
Subject: [R] How to delete rows
Message-ID: <42E7B9C8.3030804@gmx.de>

Dear R-users,

I am very new to R, so maybe my question is very easy to answer.
I have the following table:
TAB1<-data.frame(Name,Number), "Name" and "Number" are all character 
strings,
it looks like this:

Name  Number

ab      2

ab      2

NA     15

NA     15

NA     15

cd      3

ef      1

NA     15

NA     15

gh     15

gh     15

I want to delete all the rows which begin with "NA"
and all the rows where names are duplicates
(for example the second row).
I have tried this, but I only get numbers:

 for (i in 1:ZeileMax )  {if ( TAB1[[1]] [i] != "NA" ) 
{cat(TAB1[[1]][i],file = "Name.txt",fill= TRUE,append = TRUE ,sep = 
"");cat(TAB1[[2]][i], file="Number.txt", fill=TRUE,append=TRUE, sep="")}}
Name<-readLines("Name.txt")
Number<-readLines("Number.txt")
TAB<-data.frame(Name,Number)

 
Thanks in advance,

 

Michael Graber



From 0034058 at fudan.edu.cn  Wed Jul 27 18:42:22 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 28 Jul 2005 00:42:22 +0800
Subject: [R] logistic regression: categorical value, and multinomial
Message-ID: <0IKA003SION0HP@mail.fudan.edu.cn>

> d<-data.frame(y=sample(letters[1:2],100,T),x=rnorm(100))
> head(d,10)
   y           x
1  b  0.55915620
2  b  0.87575380
3  b -0.13093156
4  b  0.75925729
5  b  0.40233427
6  b  1.34685918
7  a  1.10487752
8  a -2.27456596
9  a  1.65919787
10 b  0.05095611

> glm(y~x,data=d,family=binomial)

Call:  glm(formula = y ~ x, family = binomial, data = d) 

Coefficients:
(Intercept)            x  
     0.2771       0.5348  

Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
Null Deviance:      136.1 
Residual Deviance: 129.5        AIC: 133.5 


======= 2005-07-28 00:22:55 =======

>I have two questions:
>
>1. If I want to do a binomial logit, how to handle the
>categorical response variable? Data for the response
>variables are not numerical, but text.
>
>2. What if I want to do a multinomial logit, still
>with categorical response variable? The variable has 5
>non-numerical response levels, I have to do it with a
>multinomial logit.
>
>Any input is highly appreciated! Thanks!
>
>Ed
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-07-28

------
Deparment of Sociology
Fudan University

Blog:http://sociology.yculblog.com



From petzoldt at rcs.urz.tu-dresden.de  Wed Jul 27 18:53:40 2005
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 27 Jul 2005 18:53:40 +0200 (MEST)
Subject: [R] odesolve/lsoda differences on Windows and Mac
In-Reply-To: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>
Message-ID: <Pine.OSF.4.44.0507271846510.45332-100000@rcs12.urz.tu-dresden.de>

Hello Hank,

thank you for your interest in simulating ODEs with R. Unfortunately, I am
out of office until end of August and I have no acces to a Mac with R.

Maybe, Woodrow Setzer has an idea, and as an alternative I would suggest
to post a minimal reproducible example to the R-help mailing list.

Sorry that I can't do more in the moment

Thomas Petzoldt


Thomas Petzoldt, Institute of Hydrobiology,
Dresden University of Technology
petzoldt at rcs.urz.tu-dresden.de
http://www.tu-dresden.de/fghhihb/



From ripley at stats.ox.ac.uk  Wed Jul 27 18:55:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 17:55:29 +0100 (BST)
Subject: [R] how to generate 00Index.html from Rd or html
In-Reply-To: <20050727143808.GA12347@psych>
References: <20050727143808.GA12347@psych>
Message-ID: <Pine.LNX.4.61.0507271749310.29585@gannet.stats>

On Wed, 27 Jul 2005, Jonathan Baron wrote:

> I'd like to generate 00Index.html for a bunch of libraries for my
> R search page.

I think you mean for a package, not a library, that is the file

/path/to/library/some_pky/html/00index.html

?

> I have now almost all the html help files for
> individual functions, even for packages that won't install on my
> computer (e.g., because they are specific to Windows).  I made
> these using Rdconv.  But that won't make the 00Index file, and
> neither will Rdindex, which makes a text file.  I'm probably
> missing something really simple, but I've looked for a while.

(Assuming Linux, which I am pretty sure is true for you.)

You need to use share/perl/build-help.pl --index.  Using --help should 
tell you enough to get started.  That will in fact generate all the 
per-package html files you need with other options.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jul 27 18:59:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 17:59:43 +0100 (BST)
Subject: [R] unable to source a .R file using RJava
In-Reply-To: <9F19C13E789D6E45A144CF22EDA28E3E06296F@rtp-exchng02.paradigm.paragen.com>
References: <9F19C13E789D6E45A144CF22EDA28E3E06296F@rtp-exchng02.paradigm.paragen.com>
Message-ID: <Pine.LNX.4.61.0507271755390.29585@gannet.stats>

Please, this is R-help, not the list for SJava questions.

SJava is part of the Omegahat project (not the R project) and that used to 
have its own mailing lists.  I believe they are currently non-operational, 
but please check.

The R posting guide suggests that you first ask the maintainer and then 
perhaps the R-devel list.


On Wed, 27 Jul 2005, O'Brien, Laura wrote:

> I am unable to source a ".R" file using RJava.  I tried a couple of different tests:
>     1)  using java and the evaluation method core dumps
>     2)  using ./RJava --example --gui-none to invoke source core dumps.
>     3)  The line of R works if I go directly thru R and not RJava.
>
> Version info and code are below. Any help would be appreciated.
>
> --Laura O'Brien
> Applications Architect
>
>
>
> version info
> ------------
> Java: 	j2sdk1.4.1_02
> R: 		2.1.1
> SJava:	0.68
> OS:		SunOs 5.8
>
>
> doNothing.R file I'd like to source
> -----------------------------------
>
> #
> # $Author: lobrien $
> #
> # $Date: 2005/07/27 16:20:00 $
> #
> # $Header: /unixfiles/softengr/cvs/repository/hh/src/r/doNothing.R,v 1.2 2005/07/27 16:20:00 lobrien Exp $
> #
> #
>
> #
> # test file to see if I can source a .R file via SJava
> #
> # Works when
> # ----------
> #    I invoke R version 2.1.1 on SunOS jupiter 5.8
> #  >  source ("/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R")
> #  >  res <- getData("something")
> #  >  res
> #  [1] 2
> #
> #
> # doesn't work when
> # -----------------
> # (A)   launch R thru java via -->  ./RJava --example --gui-none
> #       [omegahat->R] source ("/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R")
> #
> #       Unexpected Signal : 11 occurred at PC=0xFE0C2B44
> #       Function=[Unknown.  Nearest:  JVM_Close+0x6BFC4]
> #       <snip>
> #
> # (B)   call it via .java file.  See JavaRCall3
>
>
>
> getData <- function(query,
>                    db.name="csbdev",
>                    user.name="noone",
>                    passwd="pass"){
>  R <- 2
>  R
> }
>
> .java code to invoke the evaluate method
> -----------------------------------------
> package com.icoria.rwrapper;
>
> import org.omegahat.R.Java.ROmegahatInterpreter;
> import org.omegahat.R.Java.REvaluator;
>
>
> public class JavaRCall3
> {
>
>    /**
>     * want to see if I can eval a t.test command like what I would run in the
>     * R command line
>     */
>
>    static public void run_sourceRFile(REvaluator e, ROmegahatInterpreter interp)
>    {
>        /* produces a core */
>        System.err.println("running source doNothing.R");
>
>        Object value = e.eval("source (\"/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R");
>        if (value != null)
>            interp.show(value);
>    }
>
>
>
>
>    /**
>     *
>     */
>    static public void main(String[] args)
>    {
>        ROmegahatInterpreter interp = new ROmegahatInterpreter(ROmegahatInterpreter.fixArgs(args), false);
>        REvaluator e = new REvaluator();
>
>        Object[] funArgs;
>        String[] objects;
>        Object value;
>        int i;
>
>        run_sourceRFile(e, interp);
>    }
> }
>
> the corresponding core dump when using java
> --------------------------------------------
> [lobrien at fox] ./RJava
> Loading RInterpreter library
>
> R : Copyright 2005, The R Foundation for Statistical Computing
> Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
>
> running source doNothing.R
> Problems: unrecognized user-level object of mode 0
> Error: No default Java type for S class ???
>
> An unexpected exception has been detected in native code outside the VM.
> Unexpected Signal : 11 occurred at PC=0xF1835E50
> Function=setup_Rmainloop+0x4F8
> Library=/imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so
>
> Current Java thread:
>        at org.omegahat.R.Java.REvaluator.eval(Native Method)
>        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:90)
>        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:40)
>        at com.icoria.rwrapper.JavaRCall3.run_sourceRFile(JavaRCall3.java:25)
>        at com.icoria.rwrapper.JavaRCall3.main(JavaRCall3.java:46
>
> Dynamic libraries:
> 0x10000         /tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/bin/java
> 0xff360000      /usr/lib/libthread.so.1
> 0xff3a0000      /usr/lib/libdl.so.1
> 0xff200000      /usr/lib/libc.so.1
> 0xff330000      /usr/platform/SUNW,Ultra-60/lib/libc_psr.so.1
> 0xfe000000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so
> 0xff2d0000      /usr/lib/libCrun.so.1
> 0xff1e0000      /usr/lib/libsocket.so.1
> 0xff100000      /usr/lib/libnsl.so.1
> 0xff0d0000      /usr/lib/libm.so.1
> 0xff300000      /usr/lib/libw.so.1
> 0xff0b0000      /usr/lib/libmp.so.2
> 0xff080000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/native_threads/libhpi.so
> 0xff050000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libverify.so
> 0xfe7c0000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libjava.so
> 0xff020000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libzip.so
> 0xfdfc0000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/SJava.so
> 0xfdf90000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/libRSNativeJava.so
> 0xf1780000      /imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so
> 0xfc7c0000      /tools/app/gcc-3.4.3/lib/libg2c.so.0
> 0xfc4e0000      /tools/app/gcc-3.4.3/lib/libgcc_s.so.1
> 0xfc4b0000      /tools/app/R-2.1.1/lib/R/library/grDevices/libs/grDevices.so
> 0xfc450000      /tools/app/R-2.1.1/lib/R/library/stats/libs/stats.so
> 0xfc430000      /tools/app/R-2.1.1/lib/R/library/methods/libs/methods.so
>
> Local Time = Wed Jul 27 12:37:24 2005
> Elapsed Time = 5
> #
> # The exception above was detected in native code outside the VM
> #
> # Java VM: Java HotSpot(TM) Client VM (1.4.1_02-b06 mixed mode)
> #
> # An error report file has been saved as /tmp/hs_err_pid22533.log.
> # Please refer to the file for further information.
> #
> Abort
>
> using ./RJava --example --gui-none to invoke source core dumps.
> ---------------------------------------------------------------
> [omegahat->R] source ("/imports/nas1/people/lobrien/dist2.1_rc1/hh/src/r/doNothing.R")
>
> Unexpected Signal : 11 occurred at PC=0xFE0C2B44
> Function=[Unknown. Nearest: JVM_Close+0x6BFC4]
> Library=/imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so
>
> Current Java thread:
>        at org.omegahat.R.Java.REvaluator.eval(Native Method)
>        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:90)
>        at org.omegahat.R.Java.REvaluator.eval(REvaluator.java:40)
>        at org.omegahat.R.Java.Examples.JavaRPrompt.main(JavaRPrompt.java:26)
>
> Dynamic libraries:
> 0x10000         /tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/bin/java
> 0xff360000      /usr/lib/libthread.so.1
> 0xff3a0000      /usr/lib/libdl.so.1
> 0xff200000      /usr/lib/libc.so.1
> 0xff330000      /usr/platform/SUNW,Ultra-60/lib/libc_psr.so.1
> 0xfe000000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/client/libjvm.so
> 0xff2d0000      /usr/lib/libCrun.so.1
> 0xff1e0000      /usr/lib/libsocket.so.1
> 0xff100000      /usr/lib/libnsl.so.1
> 0xff0d0000      /usr/lib/libm.so.1
> 0xff300000      /usr/lib/libw.so.1
> 0xff0b0000      /usr/lib/libmp.so.2
> 0xff080000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/native_threads/libhpi.so
> 0xff050000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libverify.so
> 0xfe7c0000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libjava.so
> 0xff020000      /imports/filer04/tools/app/j2sdk1.4.1_02-20030305/j2sdk1.4.1_02/jre/lib/sparc/libzip.so
> 0xfdfc0000      /imports/filer04/tools/app/R-2.1.1/lib/R/library/SJava/libs/SJava.so
> 0xf1780000      /imports/filer04/tools/app/R-2.1.1/lib/R/lib/libR.so
> 0xfc7c0000      /tools/app/gcc-3.4.3/lib/libg2c.so.0
> 0xfc4e0000      /tools/app/gcc-3.4.3/lib/libgcc_s.so.1
> 0xfc4b0000      /tools/app/R-2.1.1/lib/R/library/grDevices/libs/grDevices.so
> 0xfc450000      /tools/app/R-2.1.1/lib/R/library/stats/libs/stats.so
> 0xfc430000      /tools/app/R-2.1.1/lib/R/library/methods/libs/methods.so
> 0xfa3c0000      /tools/app/R-2.1.1/lib/R/modules/R_X11.so
> 0xfc410000      /usr/lib/libSM.so.6
> 0xfa390000      /usr/lib/libICE.so.6
> 0xf1680000      /usr/lib/libX11.so.4
> 0xfa2d0000      /usr/lib/libXext.so.0
>
> Local Time = Wed Jul 27 12:39:30 2005
> Elapsed Time = 37
> #
> # HotSpot Virtual Machine Error : 11
> # Error ID : 4F530E43505002E6 01
> # Please report this error at
> # http://java.sun.com/cgi-bin/bugreport.cgi
> #
> # Java VM: Java HotSpot(TM) Client VM (1.4.1_02-b06 mixed mode)
> #
> # An error report file has been saved as /tmp/hs_err_pid22537.log.
> # Please refer to the file for further information.
> #
> Abort
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jul 27 19:02:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Jul 2005 18:02:43 +0100 (BST)
Subject: [R] How to delete rows
In-Reply-To: <42E7B9C8.3030804@gmx.de>
References: <42E7B9C8.3030804@gmx.de>
Message-ID: <Pine.LNX.4.61.0507271800330.29585@gannet.stats>

To delete duplicate rows, use unique(TAB1): see its help page.

It looks to me as if the names are missing values NA and *not* start with 
NA.  If so, you want to use

TAB1[!is.na(TAB1$Name), ]

Otherwise, perhaps TAB1[substr(TAB1$Name, 1, 2) == "NA", ].

On Wed, 27 Jul 2005, Michael Graber wrote:

> Dear R-users,
>
> I am very new to R, so maybe my question is very easy to answer.
> I have the following table:
> TAB1<-data.frame(Name,Number), "Name" and "Number" are all character
> strings,
> it looks like this:
>
> Name  Number
>
> ab      2
>
> ab      2
>
> NA     15
>
> NA     15
>
> NA     15
>
> cd      3
>
> ef      1
>
> NA     15
>
> NA     15
>
> gh     15
>
> gh     15
>
> I want to delete all the rows which begin with "NA"
> and all the rows where names are duplicates
> (for example the second row).
> I have tried this, but I only get numbers:
>
> for (i in 1:ZeileMax )  {if ( TAB1[[1]] [i] != "NA" )
> {cat(TAB1[[1]][i],file = "Name.txt",fill= TRUE,append = TRUE ,sep =
> "");cat(TAB1[[2]][i], file="Number.txt", fill=TRUE,append=TRUE, sep="")}}
> Name<-readLines("Name.txt")
> Number<-readLines("Number.txt")
> TAB<-data.frame(Name,Number)
>
>
> Thanks in advance,
>
>
>
> Michael Graber
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Wed Jul 27 19:05:30 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 27 Jul 2005 18:05:30 +0100
Subject: [R] How to delete rows
In-Reply-To: <42E7B9C8.3030804@gmx.de>
References: <42E7B9C8.3030804@gmx.de>
Message-ID: <42E7BEDA.7010800@lancaster.ac.uk>

Michael Graber wrote:
> Dear R-users,
> 
> I am very new to R, so maybe my question is very easy to answer.
> I have the following table:
> TAB1<-data.frame(Name,Number), "Name" and "Number" are all character 
> strings,
> it looks like this:
> 
> Name  Number
> 
> ab      2
> 

  [etc]

> gh     15
> 
> gh     15
> 

>  for (i in 1:ZeileMax )  {if ( TAB1[[1]] [i] != "NA" ) 
> {cat(TAB1[[1]][i],file = "Name.txt",fill= TRUE,append = TRUE ,sep = 
> "");cat(TAB1[[2]][i], file="Number.txt", fill=TRUE,append=TRUE, sep="")}}
> Name<-readLines("Name.txt")
> Number<-readLines("Number.txt")
> TAB<-data.frame(Name,Number)

  I'm not going to bother working out why that fails!

  The following assumes you want to keep one of any row that has a 
duplicated Name, in this case the first instance. I think your mail was 
a bit ambiguous as to whether you wanted to delete all rows with a 
duplicate Name...

  You can do it in two lines. First select the rows that dont have
Name=="NA", and then select the rows that dont have duplicated Name:

  > TAB <- TAB1[TAB1$Name!="NA",]
  > TAB <- TAB[!duplicated(TAB$Name),]

  > TAB
    Name Number
1    ab      2
6    cd      3
7    ef      1
10   gh     15

  Or you can do it in one line:

  > TAB=TAB1[!duplicated(TAB1$Name) & TAB1$Name!="NA",]
  > TAB

    Name Number
1    ab      2
6    cd      3
7    ef      1
10   gh     15

  Dont think of it as deleting rows, you are selecting the rows you want
and creating a new data frame.

  Any simple intro to R (see www.r-project.org for plenty) will have
examples on selecting rows and columns.

Baz



From drcarbon at gmail.com  Wed Jul 27 19:19:21 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Wed, 27 Jul 2005 13:19:21 -0400
Subject: [R] setting elements to NA across an array
Message-ID: <e89bb7ac050727101947bcc6c2@mail.gmail.com>

Please excuse what is obviously a trivial matter...

I have a large 3-d array. I wish to set the third dimension (z) to NA
if there are any NA values in the first two dimnesions (xy). So, given
array foo:

  foo <- array(data = NA, dim = c(5,5,3))
  foo[,,1] <- matrix(rnorm(25), 5, 5)
  foo[,,2] <- matrix(rnorm(25), 5, 5)
  foo[,,3] <- matrix(rnorm(25), 5, 5)

I'll set two elements to NA

  foo[2,2,1]<- NA
  foo[3,5,2]<- NA
  foo

Now I want to set foo[2,2,] <- NA and foo[3,5,] <- NA. How can I build
a logical statement to do this?



From gunter.berton at gene.com  Wed Jul 27 19:22:51 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 27 Jul 2005 10:22:51 -0700
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200507271722.j6RHMqCK027849@faraday.gene.com>

 
Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From jfox at mcmaster.ca  Wed Jul 27 19:33:30 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 27 Jul 2005 13:33:30 -0400
Subject: [R] logistic regression: categorical value, and multinomial
In-Reply-To: <20050727162255.41159.qmail@web31015.mail.mud.yahoo.com>
Message-ID: <20050727173330.GJQ21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Ed,

See ?glm for fitting binomial logit models, and ?multinom (in the nnet
package) for multinomial logit models. Neither function will handle a
character ("text") variable as the response, but you could easily convert
the variable to a factor.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Haibo Huang
> Sent: Wednesday, July 27, 2005 11:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] logistic regression: categorical value, and multinomial
> 
> I have two questions:
> 
> 1. If I want to do a binomial logit, how to handle the 
> categorical response variable? Data for the response 
> variables are not numerical, but text.
> 
> 2. What if I want to do a multinomial logit, still with 
> categorical response variable? The variable has 5 
> non-numerical response levels, I have to do it with a 
> multinomial logit.
> 
> Any input is highly appreciated! Thanks!
> 
> Ed



From jfox at mcmaster.ca  Wed Jul 27 19:37:25 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 27 Jul 2005 13:37:25 -0400
Subject: [R] setting elements to NA across an array
In-Reply-To: <e89bb7ac050727101947bcc6c2@mail.gmail.com>
Message-ID: <20050727173726.DNON26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Dr Carbon,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr Carbon
> Sent: Wednesday, July 27, 2005 12:19 PM
> To: r-help at r-project.org
> Subject: [R] setting elements to NA across an array
> 
> Please excuse what is obviously a trivial matter...
> 
> I have a large 3-d array. I wish to set the third dimension 
> (z) to NA if there are any NA values in the first two 
> dimnesions (xy). So, given array foo:
> 
>   foo <- array(data = NA, dim = c(5,5,3))
>   foo[,,1] <- matrix(rnorm(25), 5, 5)
>   foo[,,2] <- matrix(rnorm(25), 5, 5)
>   foo[,,3] <- matrix(rnorm(25), 5, 5)
> 
> I'll set two elements to NA
> 
>   foo[2,2,1]<- NA
>   foo[3,5,2]<- NA
>   foo
> 
> Now I want to set foo[2,2,] <- NA and foo[3,5,] <- NA. How 
> can I build a logical statement to do this?
> 

That should work just as you've specified it: That is, elements in all
"layers" in the second row, second column and in the the third row, fifth
column of the array should be NA. Since you posed the question, I suppose
that I'm missing something.

I hope this helps,
 John

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From drcarbon at gmail.com  Wed Jul 27 19:44:32 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Wed, 27 Jul 2005 13:44:32 -0400
Subject: [R] setting elements to NA across an array
In-Reply-To: <20050727173726.DNON26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <e89bb7ac050727101947bcc6c2@mail.gmail.com>
	<20050727173726.DNON26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <e89bb7ac050727104476ee7e4@mail.gmail.com>

Sorry for being obtuse. 

How can I build a logical statement that will set foo[2,2,] <- NA and
foo[3,5,] <- NA? Something like, if any row and column are NA, then
set NA to that row and column to NA for the entire array....



On 7/27/05, John Fox <jfox at mcmaster.ca> wrote:
> Dear Dr Carbon,
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr Carbon
> > Sent: Wednesday, July 27, 2005 12:19 PM
> > To: r-help at r-project.org
> > Subject: [R] setting elements to NA across an array
> >
> > Please excuse what is obviously a trivial matter...
> >
> > I have a large 3-d array. I wish to set the third dimension
> > (z) to NA if there are any NA values in the first two
> > dimnesions (xy). So, given array foo:
> >
> >   foo <- array(data = NA, dim = c(5,5,3))
> >   foo[,,1] <- matrix(rnorm(25), 5, 5)
> >   foo[,,2] <- matrix(rnorm(25), 5, 5)
> >   foo[,,3] <- matrix(rnorm(25), 5, 5)
> >
> > I'll set two elements to NA
> >
> >   foo[2,2,1]<- NA
> >   foo[3,5,2]<- NA
> >   foo
> >
> > Now I want to set foo[2,2,] <- NA and foo[3,5,] <- NA. How
> > can I build a logical statement to do this?
> >
> 
> That should work just as you've specified it: That is, elements in all
> "layers" in the second row, second column and in the the third row, fifth
> column of the array should be NA. Since you posed the question, I suppose
> that I'm missing something.
> 
> I hope this helps,
>  John
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
>



From r.ghezzo at staff.mcgill.ca  Wed Jul 27 19:46:00 2005
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Wed, 27 Jul 2005 13:46:00 -0400
Subject: [R] reformat data for time-varying Cox
Message-ID: <1122486360.42e7c858bac90@webmail.mcgill.ca>

Hello, using R2.1.1 in Win XP
I have data in the format
..
ID , DateOfEntry, Drug0, DateVisit1, Drug1, DateVisit2, Drug2, DateOfDeath
..
The DateOfDeath can be 999 if the patient is still alive at end of study.
The number of visits can be 2, 3 or 4 per patient
I want to do a time varying Cox analysis, so I need the data as:
..
ID   from  to   drug   censor
..
with a line for each visit.
I searched the RSiteSearch but I could not find anything but I remember that
somebody some time ago presented to the list a mini function that recoded the
data so it could be used in this manner.
If somebody in the list have a copy of that function I will appreciate if he/she
send me a copy. I tried to program it but got into a knot of spaghetti code that
is impossible to debug, at least by me.
Thanks for any help
Heberto Ghezzo Ph.D.
McGill University
Montreal - Canada



From gerifalte28 at hotmail.com  Wed Jul 27 19:47:56 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 27 Jul 2005 17:47:56 +0000
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <1122470634.3702.18.camel@seurat>
Message-ID: <BAY103-F163D8379E2D9A4738390AEA6CC0@phx.gbl>

What platform are you working on?  On windows you can also create an 
animated gif file using ImageMagik (http://www.imagemagick.org/) from the 
cmd or you can use the more user friendly UnFREEz downloadable at 
http://www.whitsoftdev.com/unfreez/ ot Microsoft GIF animator(search for it 
on google). All the software mentioned are free and I have used them in the 
past to animate R output.  Take a look at RSiteSearch("movie") for other 
interesting threads.

Cheers

Francisco


>From: Martyn Plummer <plummer at iarc.fr>
>To: Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] HOW to Create Movies with R with repeated plot()?
>Date: Wed, 27 Jul 2005 15:23:54 +0200
>
>On Wed, 2005-07-27 at 12:11 +0200, Jan Verbesselt wrote:
> > Dear R-helpers,
> >
> >
> >
> > Is it possible to create a type of 'movie' in R based on the output of
> > several figures (e.g., jpegs) via the plot() function.  I obtained 
>dynamic
> > results with the plotting function and would like to save these as a 
>movie
> > (e.g., avi or other formats)?
>
>You can use ImageMagick tools to convert a set of bitmap files into an
>animated GIF. For example, in R, this creates 101 separate png files:
>
>x <- seq(-1,1,length=101)
>y <- x^2
>png()
>for (i in 1:length(y)) {
>   plot(x[1:i], y[1:i], xlim=c(-1,1), ylim=c(0,1), type="l")
>}
>dev.off()
>
>Then, on the command line (on Linux) this joins them together
>
>convert -delay 10 Rplot*.png Rplot.gif
>animate Rplot.gif
>
>You can also create MNG format, but this is less widely supported.
>
>Martyn
>
>
>-----------------------------------------------------------------------
>This message and its attachments are strictly confidential. ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Jul 27 20:04:50 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 27 Jul 2005 14:04:50 -0400
Subject: [R] setting elements to NA across an array
In-Reply-To: <e89bb7ac050727104476ee7e4@mail.gmail.com>
Message-ID: <20050727180450.MDP21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Dr Carbon,

Actually, it appears that I'm the one who was being obtuse. To make sure
that I'm now interpreting what you want correctly, you'd like to set all
entries in a layer to NA if any entry in a layer is NA.  If that's correct,
then how about the following?

foo[array(apply(foo, c(1,2), function(x) any(is.na(x))), dim(foo))] <- NA

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Dr Carbon [mailto:drcarbon at gmail.com] 
> Sent: Wednesday, July 27, 2005 12:45 PM
> To: John Fox
> Cc: r-help at r-project.org
> Subject: Re: [R] setting elements to NA across an array
> 
> Sorry for being obtuse. 
> 
> How can I build a logical statement that will set foo[2,2,] 
> <- NA and foo[3,5,] <- NA? Something like, if any row and 
> column are NA, then set NA to that row and column to NA for 
> the entire array....
> 
> 
> 
> On 7/27/05, John Fox <jfox at mcmaster.ca> wrote:
> > Dear Dr Carbon,
> > 
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr Carbon
> > > Sent: Wednesday, July 27, 2005 12:19 PM
> > > To: r-help at r-project.org
> > > Subject: [R] setting elements to NA across an array
> > >
> > > Please excuse what is obviously a trivial matter...
> > >
> > > I have a large 3-d array. I wish to set the third dimension
> > > (z) to NA if there are any NA values in the first two dimnesions 
> > > (xy). So, given array foo:
> > >
> > >   foo <- array(data = NA, dim = c(5,5,3))
> > >   foo[,,1] <- matrix(rnorm(25), 5, 5)
> > >   foo[,,2] <- matrix(rnorm(25), 5, 5)
> > >   foo[,,3] <- matrix(rnorm(25), 5, 5)
> > >
> > > I'll set two elements to NA
> > >
> > >   foo[2,2,1]<- NA
> > >   foo[3,5,2]<- NA
> > >   foo
> > >
> > > Now I want to set foo[2,2,] <- NA and foo[3,5,] <- NA. How can I 
> > > build a logical statement to do this?
> > >
> > 
> > That should work just as you've specified it: That is, 
> elements in all 
> > "layers" in the second row, second column and in the the third row, 
> > fifth column of the array should be NA. Since you posed the 
> question, 
> > I suppose that I'm missing something.
> > 
> > I hope this helps,
> >  John
> > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > 
> >



From kunnavrv at slu.edu  Wed Jul 27 19:58:10 2005
From: kunnavrv at slu.edu (kunnavrv)
Date: Wed, 27 Jul 2005 12:58:10 -0500
Subject: [R] fitting extreme value distribution
Message-ID: <42e7cb32.128.4c1d.1120840127@slu.edu>

hi,
  rgev function gives me random deviates and I have a data
set which I am fitting to an EVD,IS there a way I can plot
both observed and ideal evd on the same plot
thankyou
Rangesh



From mengcheng81 at gmail.com  Wed Jul 27 20:33:57 2005
From: mengcheng81 at gmail.com (Meng Cheng)
Date: Wed, 27 Jul 2005 14:33:57 -0400
Subject: [R] get the content of object in ls()?
Message-ID: <6a43515205072711333d5ba84f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/54577899/attachment.pl

From vehbisinan at gmail.com  Wed Jul 27 20:45:07 2005
From: vehbisinan at gmail.com (Vehbi Sinan Tunalioglu)
Date: Wed, 27 Jul 2005 21:45:07 +0300
Subject: [R] HOW to Create Movies with R with repeated plot()?
In-Reply-To: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000e01c59293$80ee5d00$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <42E7D633.4060301@gmail.com>

Hi,

Here was a short tutorial about howto do that:
http://r.vsthost.com/wordpress/2005/07/01/creating-video-files-on-gnulinux-using-r/

PS: You can submit such short articles or tutorials, if you have time. I
 was thinking to add more stuff, but these days I don't have much time.

--Vehbi Sinan

Jan Verbesselt wrote:
> Dear R-helpers,
> 
>  
> 
> Is it possible to create a type of 'movie' in R based on the output of
> several figures (e.g., jpegs) via the plot() function.  I obtained dynamic
> results with the plotting function and would like to save these as a movie
> (e.g., avi or other formats)?
> 
>  
> 
> Regards,
> 
> Jan
> 
> _________________________________________________________________
> Ir. Jan Verbesselt
> Research Associate
> Group of Geomatics Engineering
> Department Biosystems ~ M??-BIORES
> Vital Decosterstraat 102, 3000 Leuven, Belgium
> Tel: +32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> _______________________________________________________________________
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Wed Jul 27 20:49:26 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 27 Jul 2005 14:49:26 -0400
Subject: [R] setting elements to NA across an array
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9550@uswpmx00.merck.com>

It's easiest to reshape foo first:

d <- dim(foo)
dim(foo) <- c(d[1]*d[2],d[3])

m <- rowSums(is.na(foo))

and then assign

foo[m,] <- NA

and reshape

dim(foo) <- d

(Reshaping just makes the indexing easier).

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr Carbon
Sent: Wednesday, July 27, 2005 1:45 PM
To: John Fox
Cc: r-help at r-project.org
Subject: Re: [R] setting elements to NA across an array


Sorry for being obtuse. 

How can I build a logical statement that will set foo[2,2,] <- NA and
foo[3,5,] <- NA? Something like, if any row and column are NA, then
set NA to that row and column to NA for the entire array....



On 7/27/05, John Fox <jfox at mcmaster.ca> wrote:
> Dear Dr Carbon,
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr Carbon
> > Sent: Wednesday, July 27, 2005 12:19 PM
> > To: r-help at r-project.org
> > Subject: [R] setting elements to NA across an array
> >
> > Please excuse what is obviously a trivial matter...
> >
> > I have a large 3-d array. I wish to set the third dimension
> > (z) to NA if there are any NA values in the first two
> > dimnesions (xy). So, given array foo:
> >
> >   foo <- array(data = NA, dim = c(5,5,3))
> >   foo[,,1] <- matrix(rnorm(25), 5, 5)
> >   foo[,,2] <- matrix(rnorm(25), 5, 5)
> >   foo[,,3] <- matrix(rnorm(25), 5, 5)
> >
> > I'll set two elements to NA
> >
> >   foo[2,2,1]<- NA
> >   foo[3,5,2]<- NA
> >   foo
> >
> > Now I want to set foo[2,2,] <- NA and foo[3,5,] <- NA. How
> > can I build a logical statement to do this?
> >
> 
> That should work just as you've specified it: That is, elements in all
> "layers" in the second row, second column and in the the third row, fifth
> column of the array should be NA. Since you posed the question, I suppose
> that I'm missing something.
> 
> I hope this helps,
>  John
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jul 27 20:58:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 27 Jul 2005 14:58:31 -0400
Subject: [R] get the content of object in ls()?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EB1F@usctmx1106.Merck.com>

See ?get.

Andy

> From: Meng Cheng
> 
> Hi,
> I am wirting a R script to process some data routinely and 
> have a problem 
> when try to get the content of object in ls()?
> Here is the script:
> > ls()
> [1] "a" "b"
> [3] "files.num" "fll92_1a_gpr"
> [5] "fll92_1a_gpr_norm" "fll92_1a.gpr.norm.scale"
> [7] "fname" "fullnames"
> [9] "gal" "galfile"
> [11] "i" "readGalfile2"
> [13] "sample1" "ttest1"
> > grep("_gpr$",ls())
> [1] 4
> > ls()[4]
> [1] "fll92_1a_gpr"
> > class(ls()[4])
> [1] "character"
> > class(fll92_1a_gpr)
> [1] "marrayRaw"
> attr(,"package")
> [1] "marray"
> 
> I use the regular expression to get the name of the object I 
> want, which is 
> fll92_1a_gpr, the fourth element in ls() list. 
> But i can't get the content of that object using ls()[4], 
> which gives me a 
> character object(having a double quote). 
> I can still get the data in fll92_1a_gpr by typing 
> fll92_1a_gpr. But this 
> can't be done if I want to implement it in a executable R script.
> Anyone has an idea how to solve this?
> 
> meng
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From pantd at unlv.nevada.edu  Wed Jul 27 21:24:43 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Wed, 27 Jul 2005 12:24:43 -0700
Subject: [R] gamma distribution
In-Reply-To: <17127.12638.216321.456715@stat.math.ethz.ch>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
	<17127.12638.216321.456715@stat.math.ethz.ch>
Message-ID: <1122492283.42e7df7b5b01b@webmail.scsv.nevada.edu>

Hi
You are right but here I am taking into account the p values I get from the
tests on the raw and the transformed samples. And then I calculate the power of
the tests based on the # of rejections of the p values.
DO you think its a good way to determine the power of a test?

thanks

-dev


Quoting Christoph Buser <buser at stat.math.ethz.ch>:

> Hi
>
> I am a little bit confused. You create two sample (from a gamma
> distribution) and you do a wilcoxon test with this two samples.
> Then you use the same monotone transformation (log) for both
> samples and redo the wilcoxon test.
> But since the transformations keeps the order of your samples
> the second wilcoxon test is identical to the first one:
>
> x<-rgamma(10, 2.5, scale = 10)
> y<-rgamma(10, 2.5, scale = 10)
> wilcox.test(x, y, var.equal = FALSE)
> x1<-log(x)
> y1<-log(y)
> wilcox.test(x1, y1, var.equal = FALSE)
>
> Maybe you can give some more details about the hypothesis you'd
> like to test.
>
> Regards,
>
> Christoph Buser
>
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
>
>
>
> pantd at unlv.nevada.edu writes:
>  > Hi R Users
>  >
>  >
>  > This is a code I wrote and just want to confirm if the first 1000 values
> are raw
>  > gamma (z) and the next 1000 values are transformed gamma (k) or not. As I
> get
>  > 2000 rows once I import into excel, the p - values beyond 1000 dont look
> that
>  > good, they are very high.
>  >
>  >
>  > --
>  > sink("a1.txt");
>  >
>  > for (i in 1:1000)
>  > {
>  > x<-rgamma(10, 2.5, scale = 10)
>  > y<-rgamma(10, 2.5, scale = 10)
>  > z<-wilcox.test(x, y, var.equal = FALSE)
>  > print(z)
>  > x1<-log(x)
>  > y1<-log(y)
>  > k<-wilcox.test(x1, y1, var.equal = FALSE)
>  > print(k)
>  > }
>  >
>  > ---
>  > any suggestions are welcome
>  >
>  > thanks
>  >
>  > -devarshi
>  >
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From caobg at email.uc.edu  Wed Jul 27 21:33:06 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Wed, 27 Jul 2005 15:33:06 -0400
Subject: [R] how to get actual value from predict in nnet?
Message-ID: <200507271933.CPO49890@mirapoint.uc.edu>

Dear All,

After followed the help of nnet, I could get the networks trained and, excitedly, get the prediction for other samples. It is a two classes data set, I used "N" and "P" to label the two. My question is, how do I get the predicted numerical value for each sample? Not just give me the label(either "N" or "P")?  Thanks!

FYI: The nnet example I followed from help document is,
 ird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
             species = c(rep("s",50), rep("c", 50), rep("v", 50)))
     ir.nn2 <- nnet(species ~ ., data = ird, subset = samp, size = 2, rang = 0.1,
                    decay = 5e-4, maxit = 200)
     table(ird$species[-samp], predict(ir.nn2, ird[-samp,], type = "class"))
 
Best regards,  
 Baoqiang Cao



From Quin_Wills at msn.com  Wed Jul 27 22:22:04 2005
From: Quin_Wills at msn.com (Quin Wills)
Date: Wed, 27 Jul 2005 21:22:04 +0100
Subject: [R] Installing SJava (I'm almost there,
	just a little more help please!....please!)
Message-ID: <BAY103-DAV1698CF0CA36CDDA6578C3F4CC0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/97e3d9ee/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Jul 27 22:51:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jul 2005 22:51:30 +0200
Subject: [R] odesolve/lsoda differences on Windows and Mac
In-Reply-To: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>
References: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>
Message-ID: <x2d5p4j7l9.fsf@turmalin.kubism.ku.dk>

"Martin Henry H. Stevens" <HStevens at muohio.edu> writes:

> Hi -
> I am getting different results when I run the numerical integrator 
> function lsoda (odesolve package) on a Mac and a PC. I am trying to 
> simulating a system of 10 ODE's with two exogenous pulsed inputs to the 
> system, and have had reasonably good success with many model parameter 
> sets. Under some parameter sets, however, the simulations fail on the 
> Mac (see error message below). The same parameter sets, however, appear 
> to run fine for our computational technician on his PC, generating 
> apparently very  reasonable data.

One thought: Integrating across input pulses is a known source of
"turbulence" in lsoda. You might have better luck integrating over
intervals in which the input function is continuous.

Tweaking the lsoda tolerances is another thing to try. 

I haven't seen lsoda fail like that, but it's not too surprising that
marginal cases show platform dependency (i.e. the integrator just
fails on Mac and barely succeeds on PC). 
 
> Our tech is successfully  running
> Dell Latitude D810, Windows XP Pro (Service Pack 2), 1Gb
> RAM.  RGUI 2.1.1
> 
> I am running:
>   R Version 2.1.1  (2005-06-20) on a
> Mac OS 10.3.9
>    Machine Model:	Power Mac G5
>    CPU Type:	PowerPC 970  (2.2)
>    Number Of CPUs:	2
>    CPU Speed:	2 GHz
>    L2 Cache (per CPU):	512 KB
>    Memory:	1.5 GB
>    Bus Speed:	1 GHz
>    Boot ROM Version:	5.0.7f0
>    Serial Number:	XB3472Q1NVS
> 
> My Error Message
>  > system.time(
> + outAc2 <- as.data.frame(lsoda(xstart,times, pondamph, parms, 
> tcrit=170*730, hmin=.1))
> + )
> [1] 0.02 0.01 0.04 0.00 0.00
> Warning messages:
> 1: lsoda--  at t (=r1) and step size h (=r2), the
> 2:       corrector convergence failed repeatedly
> 3:       or with abs(h) = hmin
> 4: Returning early from lsoda.  Results are accurate, as far as they go
> 
> Thanks for any input.
> 
> Hank Stevens

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Quin_Wills at msn.com  Wed Jul 27 23:06:19 2005
From: Quin_Wills at msn.com (Quin Wills)
Date: Wed, 27 Jul 2005 22:06:19 +0100
Subject: [R] Installing SJava (I'm almost there,
	just a little more help please!....please!)
In-Reply-To: <20050727202955.GD4295@wald.ucdavis.edu>
Message-ID: <BAY103-DAV1620150E598F0E4EF1FD38F4CC0@phx.gbl>

Hi Duncan

Thank you for responding... I apologise for being so ignorant. I presume
that is a UNIX command - so have just downloaded cygwin (read about that
today) and ran your suggested line. I get:

Chmod: cannot acess 'configure.win'. No such file or directory

I am assuming that isn't good. Is this the configure file I find in SJava?
Do I need to move it somewhere for this to work?

All of the best,
Quin

-----Original Message-----
From: Duncan Temple Lang [mailto:duncan at wald.ucdavis.edu] 
Sent: 27 July 2005 09:30 PM
To: Quin Wills
Subject: Re: [R] Installing SJava (I'm almost there, just a little more help
please!....please!)


Hi.
  It has been a long time since I looked at the Windows
side of things of SJava.

  Is configure.win present _AND_ executable.
Make certain that it is by using
  chmod +x configure.win

 D.

Quin Wills wrote:
> Hi. Day three and I?m still struggling with this. Any advice to overcome
the
> final hurdle will be enormously appreciated. I now have all the right Java
> applications etc. in their right places and have managed to get rid of
most
> errors but still get this:
> 
>  
> 
> ?    Making package SJava ?
> 
>  Building JNI header files... 
> adding build stamp to DESCRIPTION 
> running src/Makefile.win 
> (cd .. ; ./configure.win c:/PROGR~1/R/rw200l) 
> /configure.win: not found 
> make[3]: *** [config] Error 127 
> make[2]: *** [srcDynLib] Error 2 
> make[1]: *** [all] Error 2 
> Make: *** [pk9?SJava] Error 2 
> *** Installation of SJava failed *** 
> Removing ?c:/PROGR~1/R/rw200l/library/SJava? 
> 
> I am Windows XP with SJava (SJava_0.68-0.tar.gz) downloaded to my c drive
> (c:\SJava_0.68-0.tar.gz). R is rw2001 (c:\Program Files\R\rw2001). I am
> using the following ?R CMD INSTALL c:\SJava-0.68-0.tar.gz?.
> 
>  
> 
> Why is the configure.win file not being found? Where is it looking for it?
> My eternal gratitude to anybody willing to take me out of my pain.
> 
>  
> 
>  
> 
> 
> ---
> 
> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

-- 
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA




---
Incoming mail is certified Virus Free.


 
  

---



From MikeJones at westat.com  Wed Jul 27 23:44:47 2005
From: MikeJones at westat.com (Mike Jones)
Date: Wed, 27 Jul 2005 17:44:47 -0400
Subject: [R] Returning positions of a vector
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D01123022@MAILBE2.westat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/1cd65249/attachment.pl

From sdavis2 at mail.nih.gov  Thu Jul 28 00:01:03 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 27 Jul 2005 18:01:03 -0400
Subject: [R] Returning positions of a vector
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D01123022@MAILBE2.westat.com>
Message-ID: <BF0D7C5F.AA8B%sdavis2@mail.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050727/22f9c188/attachment.pl

From sundar.dorai-raj at pdf.com  Thu Jul 28 00:07:26 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 27 Jul 2005 17:07:26 -0500
Subject: [R] Returning positions of a vector
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D01123022@MAILBE2.westat.com>
References: <403593359CA56C4CAE1F8F4F00DCFE7D01123022@MAILBE2.westat.com>
Message-ID: <42E8059E.10201@pdf.com>



Mike Jones wrote:
> Hi,
> 
> I'd like to return the rows of sampled values of a vector.  Can't figure
> out how to do it.
> 
> For example:  I start with [3, 1, 7, 4, 10] that have position
> [1,2,3,4,5].  I sample [10,1,4] and I want to return [5,2,4].  
> 
> I'm sure there's a simple way to do that but haven't been able to figure
> it out or locate it.
> 
> Thanks in advance...mj

Try ?match.

x <- c(3, 1, 7, 4, 10)
match(c(10, 1, 4), x)

HTH,

--sundar



From pantd at unlv.nevada.edu  Thu Jul 28 00:42:51 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Wed, 27 Jul 2005 15:42:51 -0700
Subject: [R] gamma distribution
In-Reply-To: <42E73542.6090505@statistik.uni-dortmund.de>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
	<42E73542.6090505@statistik.uni-dortmund.de>
Message-ID: <1122504171.42e80debf07e9@webmail.scsv.nevada.edu>

Hi
I ran your code. I think it should give me the number of p values below 0.05
significance level  (thats what i could understand from your code), but after
running your code there is neither any error that shows up nor any value that
the console displays.


thanks in advance

-dev.

Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:
> pantd at unlv.nevada.edu wrote:
>
> > Hi R Users
> >
> >
> > This is a code I wrote and just want to confirm if the first 1000 values
> are raw
> > gamma (z) and the next 1000 values are transformed gamma (k) or not. As I
> get
> > 2000 rows once I import into excel, the p - values beyond 1000 dont look
> that
> > good, they are very high.
>
> He?
> - log() transforming the data does not change the Wilcoxon statistics
> (based on ranks!)!
> - Why is this related to Excel?
> - What are you going to show?
>
> I get
>
>   erg <- replicate(1000, {
>       x<-rgamma(10, 2.5, scale = 10)
>       y<-rgamma(10, 2.5, scale = 10)
>       wilcox.test(x, y, var.equal = FALSE)$p.value
>   })
>   sum(erg < 0.05) # 45
>
> which seems plausible to me.
>
>
> Uwe Ligges
>
>
>
> >
> > --
> > sink("a1.txt");
> >
> > for (i in 1:1000)
> > {
> > x<-rgamma(10, 2.5, scale = 10)
> > y<-rgamma(10, 2.5, scale = 10)
> > z<-wilcox.test(x, y, var.equal = FALSE)
> > print(z)
> > x1<-log(x)
> > y1<-log(y)
> > k<-wilcox.test(x1, y1, var.equal = FALSE)
> > print(k)
> > }
> >
> > ---
> > any suggestions are welcome
> >
> > thanks
> >
> > -devarshi
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From blomsp at ozemail.com.au  Thu Jul 28 02:17:48 2005
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 28 Jul 2005 10:17:48 +1000
Subject: [R] Error in FUN(newX[, i], ...) : `x' must be atomic
In-Reply-To: <x2vf2wk3an.fsf@turmalin.kubism.ku.dk>
References: <20050727022202.63121.qmail@web53501.mail.yahoo.com>
	<6.2.1.2.0.20050727123046.01d5f310@mail.ozemail.com.au>
	<x2vf2wk3an.fsf@turmalin.kubism.ku.dk>
Message-ID: <6.2.1.2.0.20050728101535.01d836e0@mail.ozemail.com.au>

Aah, OK. Thanks for the correction. I learn something new every day. That's 
why I like this list! Serves me right for relying on my Lisp knowledge.

Cheers,

Simon.


At 07:26 PM 27/07/2005, Peter Dalgaard wrote:
>Simon Blomberg <blomsp at ozemail.com.au> writes:
>
> > Actually, atoms are originally a Lisp concept. Objects are either atoms or
> > not. Atoms are data types that cannot be taken apart, such as numbers or
> > symbols. Lists and vectors (of length > 1) are examples of  non-atomic 
> data
> > types. Did you pass a vector to FUN?
>
>Actually, vectors ARE atomic in R, so your definition is somewhat off
>target.
>
> >  is.atomic(rnorm(5))
>[1] TRUE
>
>In the extreme, the only true atom is the bit, everything else can be
>taken apart - doubles into (sign,exponent,mantissa) etc. So languages
>*define* their own atoms, as objects that are not composed of other
>objects in the language. (And even that is a partial lie for R,
>because atoms can have attributes. Atomicity is purely based on the
>type of an object. The help page has a list of the atomic types.)
>
> > Cheers,
> >
> > Simon.
> >
> > At 12:22 PM 27/07/2005, Srinivas Iyyer wrote:
> > >Hello Group,
> > >  What is the meaning of the error.  is there any place
> > >to look for this. I guess 'atomic' seems to be OOP
> > >related concept.
> > >
> > >thank you
> > >srini
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> > Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
> > Centre for Resource and Environmental Studies
> > The Australian National University
> > Canberra ACT 0200
> > Australia
> > T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
> > F: +61 2 6125 0757
> > CRICOS Provider # 00120C
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
>
>--
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jsorkin at grecc.umaryland.edu  Thu Jul 28 03:28:54 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 27 Jul 2005 21:28:54 -0400
Subject: [R] CSV file and date. Dates are read as factors!
Message-ID: <s2e7fca5.036@grecc.umaryland.edu>

I am using read.csv to read a CSV file (produced by saving an Excel file
as a CSV file). The columns containing dates are being read as factors.
Because of this, I can not compute follow-up time, i.e.
Followup<-postDate-preDate. I would appreciate any suggestion that would
help me read the dates as dates and thus allow me to calculate follow-up
time.
Thanks
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From Tom.Mulholland at dpi.wa.gov.au  Thu Jul 28 04:12:32 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 28 Jul 2005 10:12:32 +0800
Subject: [R] fitting extreme value distribution
Message-ID: <4702645135092E4497088F71D9C8F51A128BD9@afhex01.dpi.wa.gov.au>

If this is a question about plotting 2 series on a plot then in general terms you can plot 2 series as per this example in the list http://finzi.psych.upenn.edu/R/Rhelp02a/archive/5463.html 

If it is about the use of rgev you should tell us the name of the package.

If you read the posting guide you will find ways to help imporve the way that you ask questions, so that it is easier for people to give you an answer that works for you.


Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of kunnavrv
> Sent: Thursday, 28 July 2005 1:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] fitting extreme value distribution
> 
> 
> hi,
>   rgev function gives me random deviates and I have a data
> set which I am fitting to an EVD,IS there a way I can plot
> both observed and ideal evd on the same plot
> thankyou
> Rangesh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jsorkin at grecc.umaryland.edu  Thu Jul 28 05:07:35 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 27 Jul 2005 23:07:35 -0400
Subject: [R] Dates are read as factors!
Message-ID: <s2e813c9.050@grecc.umaryland.edu>

I am using read.csv to read a CSV file (produced by saving an Excel file
as a CSV file). The columns containing dates are being read as factors.
Because of this, I can not compute follow-up time, i.e.
Followup<-postDate-preDate. I would appreciate any suggestion that would
help me read the dates as dates and thus allow me to calculate follow-up
time.
Thanks
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From mcclatchie.sam at saugov.sa.gov.au  Thu Jul 28 05:13:34 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Thu, 28 Jul 2005 12:43:34 +0930
Subject: [R] lattice/ grid.layout/ multiple graphs per page
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73B9D@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.0.0
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------
Colleagues

I have a set of lattice plots, and want to plot 4 of them on the page.
I am having trouble with the layout.

grid.newpage()
pushViewport(viewport(layout = grid.layout(2,2)))
pushviewport(viewport(layout.pos.col = 1, layout.pos.row = 1))
working trellis graph code here
pushviewport(viewport(layout.pos.col = 1, layout.pos.row = 2))
working trellis graph code here
pushviewport(viewport(layout.pos.col = 2, layout.pos.row = 1))
working trellis graph code here
pushviewport(viewport(layout.pos.col = 2, layout.pos.row = 2))

I'm obviously doing something wrong since my graphs still print one per
page?

Would you be able to provide advice?

Thanks

Sam
----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From MSchwartz at mn.rr.com  Thu Jul 28 05:25:24 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 27 Jul 2005 22:25:24 -0500
Subject: [R] Dates are read as factors!
In-Reply-To: <s2e813c9.050@grecc.umaryland.edu>
References: <s2e813c9.050@grecc.umaryland.edu>
Message-ID: <1122521124.4387.121.camel@localhost.localdomain>

On Wed, 2005-07-27 at 23:07 -0400, John Sorkin wrote:
> I am using read.csv to read a CSV file (produced by saving an Excel file
> as a CSV file). The columns containing dates are being read as factors.
> Because of this, I can not compute follow-up time, i.e.
> Followup<-postDate-preDate. I would appreciate any suggestion that would
> help me read the dates as dates and thus allow me to calculate follow-up
> time.
> Thanks
> John

John,

Depending upon how may columns you are reading, you could:

1. Use the 'colClasses' argument in read.csv() to specify the data types
for each incoming column, one of which is 'Date'

Or

2. Post process the data after importing by using the as.Date()
function. See ?as.Date for more information and pay particular attention
to the 'format' argument.

My own preference is number 2.

Once the requisite columns are converted to the Date class, you can
engage in date based arithmetic, etc.

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Thu Jul 28 05:43:21 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Jul 2005 20:43:21 -0700
Subject: [R] fitting extreme value distribution
In-Reply-To: <42e7cb32.128.4c1d.1120840127@slu.edu>
References: <42e7cb32.128.4c1d.1120840127@slu.edu>
Message-ID: <42E85459.1050606@pdf.com>

	  Which "rgev" are you using?  There are at least two in at least three 
different packages:  evd, evir, and fExtremes.  I found this by using 
'help.search("rgev")', which will however only identify related material 
I already have installed locally.  'RSiteSearch("rgev")' produced 22 
hits, the first of which was for the one in fExtremes.  The 
documentation for the one in fExtremes includes a reasonable plotting 
example.  Regarding fitting, have you considered fitdistr in 
library(MASS)?

	  Beyond this, may I suggest you PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html.  It can increase the 
likelihood you will get a useful reply relatively quickly.

	  spencer graves

kunnavrv wrote:

> hi,
>   rgev function gives me random deviates and I have a data
> set which I am fitting to an EVD,IS there a way I can plot
> both observed and ideal evd on the same plot
> thankyou
> Rangesh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From lists at compassroseenterprises.com  Thu Jul 28 05:44:38 2005
From: lists at compassroseenterprises.com (D. Dailey)
Date: Wed, 27 Jul 2005 20:44:38 -0700
Subject: [R] Unexpected behavior in recode{car}
Message-ID: <42E854A6.2000003@CompassRoseEnterprises.com>

Thanks to the R creators for such a great statistical system. Thanks to
the R help list, I have (finally) gotten far enough in R to have a
question I hope to be worth posting.

I'm using the recode function from John Fox's car package and have
encountered some unexpected behavior.

Consider the following example:

## Begin cut-and-paste example
require( car )
set.seed(12345)
nn <- sample( c( 2, 4 ), size=50, replace=TRUE )
rr <- recode( nn, "2='TWO';4='FOUR'" )
table( rr, exclude=NULL )
ss <- recode( nn, "2='Num2';4='Num4'" )  # Doesn't work as expected
table( ss, exclude=NULL )
tt <- recode( nn, "2='TWO'; 4='Num4'" )
table( tt, exclude=NULL )
uu <- recode( nn, "2='Num2'; 4='FOUR'" )
table( uu, exclude=NULL )
## End cut-and-paste example

All but the recoding to ss work as expected: I get a character vector
with 23 instances of either "FOUR" or "Num4" and 27 instances of "TWO"
or "Num2".

But for the ss line, wherein all the strings to be assigned contain a
digit, the resulting vector contains all NAs. Using str(), I note that
ss is a numeric vector.

Is there a tidy way (using recode) to recode numeric values into
character strings, all of which contain a digit? I have a workaround for
my current project, but it would be nice to be able to use mixed
alphanumeric strings in this context.

Thanks in advance for any insight you can give into this question.

Using R 2.1.1 (downloaded binary) on Windows XP Pro, car version 1.0-17
(installed from CRAN via Windows GUI). Complete version information
below:

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.1
year     2005
month    06
day      20
language R

 > t(t( installed.packages()['car',] ))
          [,1]
Package  "car"
LibPath  "C:/Programs/R/rw2011/library"
Version  "1.0-17"
Priority NA
Bundle   NA
Contains NA
Depends  "R (>= 1.9.0)"
Suggests "MASS, nnet, leaps"
Imports  NA
Built    "2.1.0"


I subscribe to the help list in digest form, so would appreciate being
copied directly in addition to seeing responses sent to the list.

David Dailey
Shoreline, Washington, USA
Lists at CompassRoseEnterprises.com



From spencer.graves at pdf.com  Thu Jul 28 06:01:42 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Jul 2005 21:01:42 -0700
Subject: [R] Wishart Density
In-Reply-To: <3D0B2434377E984E9C85CAA316F8B18301B35C34@nsabpmail>
References: <3D0B2434377E984E9C85CAA316F8B18301B35C34@nsabpmail>
Message-ID: <42E858A6.8060400@pdf.com>

	  'RSiteSearch("wishart")' just produced 45 hits for me, the first of 
which was for function 'Simnu.mix' in package 'SharedHT2'.  If this does 
not help you, PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html.  It can improve the chances 
that you will get a useful reply.

	  spencer graves

Wang, Meihua wrote:

> Dear R users,
>  
> I am doing MCMC using Metropolis-Hastings.  My model is
> bivariate-log-normal and the prior for variance-covariance is wishart
> distribution.   I am wondering if there are some simple codes about how
> to get the density of Wishart distribution in my case ?
> Thanks in advance.  
>  
> Meihua
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Jul 28 06:13:48 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Jul 2005 21:13:48 -0700
Subject: [R] SETAR Estimation
In-Reply-To: <1122392093.42e6581d9c506@webmail.mcgill.ca>
References: <1122392093.42e6581d9c506@webmail.mcgill.ca>
Message-ID: <42E85B7C.9040705@pdf.com>

	  I just got zero hits from 'RSiteSearch("SETAR")'.  When I Googled for 
"SETAR", I got 219,000 hits, but nothing on the first page looked like 
it might be your SETAR.  Then I Googled for "SETAR Model" and got 
something that suggested "threshold autoregressive ..."  Then and 
'RSiteSearch("threshold autoregressive") produced 6 hits, the first of 
which was for "wle.ar" in package "wle".

	  hope this helps.
	  spencer graves
p.s.  If this does not solve your problem, PLEASE do read the posting 
guide! http://www.R-project.org/posting-guide.html.  It can increase 
your chances of getting a useful reply from this list.

ekhous at po-box.mcgill.ca wrote:

> Dear R-helpers,
> 
> I was wondering if anyone has or knows someone who might have an implementation
> of algorithm for estimating SETAR models including the lag-order. For some
> reason my code gives me a bit wrong results. I am fighting with it for a week
> and cannot bring it down.
> Thanks a million in advance,
> Sincerely,
> 
> Evgueni
> 
> McGill University
> Department of Economics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Jul 28 06:24:16 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Jul 2005 21:24:16 -0700
Subject: [R] a class matrix with class ordered data
In-Reply-To: <20050727121245.49921.qmail@web61216.mail.yahoo.com>
References: <20050727121245.49921.qmail@web61216.mail.yahoo.com>
Message-ID: <42E85DF0.3030307@pdf.com>

	  If you still are interested in help with your problem, PLEASE do read 
the posting guide! http://www.R-project.org/posting-guide.html.  It can 
increase the likelihood of you getting a useful reply.

	  See other comments in line.

	  spencer graves

Werner Bier wrote:
> Deart R-help,
> 
> I could not transfer ordered data into a matrix, does anybody knows if there is something wrong in the code below please?
> 
> Thanks in advance, Tom
> 
> Y <- ordered( unlist( Q[,1:2] ) )

	  This creates Y as an ordered factor.
> 
> z <- matrix(0, nrow(Q), 2)
> 
> z <- Y
> 
	  This completely discards the matrix z and replaces it with Y. 
Nothing of the matrix remains (except in your memory).

> is.ordered(z)
> 
> [1] T
> 
> is.matrix(z)
> 
> [1] F
> 
> i.e. Is it possibile somehow to have is.matrix(z) equal TRUE?
> 
	  Rather than asking only for an explanation of some little thing that 
didn't solve your problem, may I suggest you tell us more about the 
problem you are actually trying to solve.  Why are you doing this?  What 
specifically do you want to accomplish -- but expressed in terms of a 
very simple example with a few lines of code that a reader can copy into 
R on their computer and get something, then try a couple of 
modifications that might be closer to what you want.
>  
> 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Jul 28 06:33:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Jul 2005 21:33:08 -0700
Subject: [R] Convert quarterly data to monthly data
In-Reply-To: <20050725172458.82480.qmail@web31003.mail.mud.yahoo.com>
References: <20050725172458.82480.qmail@web31003.mail.mud.yahoo.com>
Message-ID: <42E86004.4080207@pdf.com>

	  Have you considered aggregate in library(zoo)?  If you are not 
familiar with "zoo", I suggest you install it, then run 
'vignette("zoo")' and 'edit(vignette("zoo"))' [NOTE:  If you use Emacs, 
'edit(vignette(...))' won't work.  There is something that will, but I 
don't remember what it is.  If you need it, please PLEASE do read the 
posting guide! http://www.R-project.org/posting-guide.html and try this 
list again.]

	  spencer graves

Haibo Huang wrote:

> Hi,
> 
> I am new to use R, but can anyone tell me how to
> tranform quarterly data to monthly data? I know SAS
> has this procedure, so may I assume it is also
> available in R?
> 
> Thanks a lot!
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Jul 28 06:51:40 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Jul 2005 21:51:40 -0700
Subject: [R] Question on glm for Poisson distribution.
In-Reply-To: <ff51f02205072707261837b51a@mail.gmail.com>
References: <ff51f02205072707261837b51a@mail.gmail.com>
Message-ID: <42E8645C.2080900@pdf.com>

	  I can't help you much, because you did not include a simple example 
that is sufficiently complete for me to run it myself and play with the 
results.

	  I ran the example "glm.D93 <- glm(counts ~ outcome + treatment, 
family=poisson())" in the "glm" help file.  Then I looked at 
"str(glm.D93)";  "attributes(glm.D93)" might have been a better choice 
in this case, but "str" is more general and will expose things hidden 
from "attributes".  Then I looked at "glm.D93$contrasts", and found that 
the contrasts used for "outcome" and "treatment" were both 
"contr.treatment".  Then I tried "contr.treatment(outcome)" and got 
something useful.

	  If this is not adequate and you still could use help, PLEASE do read 
the posting guide! http://www.R-project.org/posting-guide.html and try 
this list again.

	  spencer graves

Ghislain Vieilledent wrote:

> Good afternoon,
> 
> I REALLY try to answer to my question as an autonomous student searching in 
> the huge pile of papers on my desk and on the Internet but I can't find out 
> the solution. 
> Would you mind giving me some help? Please.
> 
> #########################################
> 
> I'm trying to use glm with factors:
> 
> 
>>Pyr.1.glm<-glm(Pyrale~Trait,DataRav,family=poisson)
> 
> 
> If I have correctly payed attention to my cyber professor explanations I 
> have, for the variable Pyrale which I suppose Poisson-distributed, the 
> following mathematical expression:
> 
> P(Pyrale=k)=exp(-m).[(m^k)/k!]
> with log(m)=Intercept+Trait(i) (link function is log for Poisson 
> distribution)
> 
> Then I test the significativity of Trait:
> 
> 
>>anova(Pyr.1.glm,test="Chisq")
> 
> Analysis of Deviance Table
> 
> Model: poisson, link: log
> 
> Response: Pyrale
> 
> Terms added sequentially (first to last)
> 
> 
> Df Deviance Resid. Df Resid. Dev P(>|Chi|)
> NULL 19 49.813 
> Trait 3 31.281 16 18.532 7.419e-07
> 
> Which means that variable Trait is significant for determining the value of 
> P(Pyrale=k).
> 
> I tried to order the effects of the modalities of my variable Trait using:
> 
>  > summary(Pyr.1.glm)
> 
> Call:
> glm(formula = Pyrale ~ Trait, family = poisson, data = DataRav)
> 
> Deviance Residuals: 
> Min 1Q Median 3Q Max 
> -1.7117 -0.8944 -0.6237 0.6390 1.5224 
> 
> Coefficients:
> Estimate Std. Error z value Pr(>|z|) 
> (Intercept) 1.3350 0.2294 5.819 5.92e-09 ***
> TraitIns&Fong -2.9444 1.0259 -2.870 0.00410 ** 
> TraitInsecticide -2.2513 0.7434 -3.028 0.00246 ** 
> TraitTemoin -0.2364 0.3454 -0.684 0.49372 
> ---
> Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> (Dispersion parameter for poisson family taken to be 1)
> 
> Null deviance: 49.813 on 19 degrees of freedom
> Residual deviance: 18.532 on 16 degrees of freedom
> AIC: 61.85
> 
> Number of Fisher Scoring iterations: 5
> 
> ##############################################################
> 
> I have therefore two questions:
> 
> - Considering the values of estimated coefficients for Trait(i), does it 
> mean that the bigger is the coefficient, the lower is the probability 
> considering the mathematical expression (exp(-m))?
> 
> - How can I check that coefficients are significatively different one from 
> each other (as with function TukeyHSD for other models)?
> 
> 
> Thanks for you help.
> 
> Regards
> 
> Ghislain.
> 
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ripley at stats.ox.ac.uk  Thu Jul 28 07:41:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Jul 2005 06:41:57 +0100 (BST)
Subject: [R] how to get actual value from predict in nnet?
In-Reply-To: <200507271933.CPO49890@mirapoint.uc.edu>
References: <200507271933.CPO49890@mirapoint.uc.edu>
Message-ID: <Pine.LNX.4.61.0507280638590.9458@gannet.stats>

Ask predict.nnet for a type other than "class", as described on its help 
page.

Please do read the help pages for yourself (as the posting guide asks).

On Wed, 27 Jul 2005, Baoqiang Cao wrote:

> Dear All,
>
> After followed the help of nnet, I could get the networks trained and, 
> excitedly, get the prediction for other samples. It is a two classes 
> data set, I used "N" and "P" to label the two. My question is, how do I 
> get the predicted numerical value for each sample? Not just give me the 
> label(either "N" or "P")?  Thanks!
>
> FYI: The nnet example I followed from help document is,
> ird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
>             species = c(rep("s",50), rep("c", 50), rep("v", 50)))
>     ir.nn2 <- nnet(species ~ ., data = ird, subset = samp, size = 2, rang = 0.1,
>                    decay = 5e-4, maxit = 200)
>     table(ird$species[-samp], predict(ir.nn2, ird[-samp,], type = "class"))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vito_ricci at yahoo.com  Thu Jul 28 08:52:51 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 28 Jul 2005 08:52:51 +0200 (CEST)
Subject: [R] fitting extreme value distribution
Message-ID: <20050728065251.51411.qmail@web41210.mail.yahoo.com>

Hi,
for fitting have you seen ?fgev in evd package, or
?gevFit in fExtremes or ?gev in evir package?


Regards,
Vito


<kunnavrv <at> slu.edu> wrote:

hi,
  rgev function gives me random deviates and I have a
data
set which I am fitting to an EVD,IS there a way I can
plot
both observed and ideal evd on the same plot
thankyou
Rangesh


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From Tom.Mulholland at dpi.wa.gov.au  Thu Jul 28 09:01:05 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 28 Jul 2005 15:01:05 +0800
Subject: [R] Unexpected behavior in recode{car}
Message-ID: <4702645135092E4497088F71D9C8F51A128BDE@afhex01.dpi.wa.gov.au>

require( car )
set.seed(12345)
nn <- sample( c( 2, 4 ), size=50, replace=TRUE )
rr <- recode( nn, "2='TWO';4='FOUR'" )
table( rr, exclude=NULL )
ss <- recode( nn, "2='Num2';4='Num4'" )  # Doesn't work as expected
table( ss, exclude=NULL )
ss <- recode( nn, "2='Num2';4='Num4'", TRUE )  #?
table( ss, exclude=NULL )
tt <- recode( nn, "2='TWO'; 4='Num4'" )
table( tt, exclude=NULL )
uu <- recode( nn, "2='Num2'; 4='FOUR'" )
table( uu, exclude=NULL )

I looked at the code and found it too difficult to immediately decipher. So does making the result a factor cause any real problems?

I noticed that the same response happens with any letterset followed by a number
recode( nn, "2='Num2'; 4='abc5'" )

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of D. Dailey
> Sent: Thursday, 28 July 2005 11:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Unexpected behavior in recode{car}
> 
> 
> Thanks to the R creators for such a great statistical system. 
> Thanks to
> the R help list, I have (finally) gotten far enough in R to have a
> question I hope to be worth posting.
> 
> I'm using the recode function from John Fox's car package and have
> encountered some unexpected behavior.
> 
> Consider the following example:
> 
> ## Begin cut-and-paste example
> require( car )
> set.seed(12345)
> nn <- sample( c( 2, 4 ), size=50, replace=TRUE )
> rr <- recode( nn, "2='TWO';4='FOUR'" )
> table( rr, exclude=NULL )
> ss <- recode( nn, "2='Num2';4='Num4'" )  # Doesn't work as expected
> table( ss, exclude=NULL )
> tt <- recode( nn, "2='TWO'; 4='Num4'" )
> table( tt, exclude=NULL )
> uu <- recode( nn, "2='Num2'; 4='FOUR'" )
> table( uu, exclude=NULL )
> ## End cut-and-paste example
> 
> All but the recoding to ss work as expected: I get a character vector
> with 23 instances of either "FOUR" or "Num4" and 27 instances of "TWO"
> or "Num2".
> 
> But for the ss line, wherein all the strings to be assigned contain a
> digit, the resulting vector contains all NAs. Using str(), I note that
> ss is a numeric vector.
> 
> Is there a tidy way (using recode) to recode numeric values into
> character strings, all of which contain a digit? I have a 
> workaround for
> my current project, but it would be nice to be able to use mixed
> alphanumeric strings in this context.
> 
> Thanks in advance for any insight you can give into this question.
> 
> Using R 2.1.1 (downloaded binary) on Windows XP Pro, car 
> version 1.0-17
> (installed from CRAN via Windows GUI). Complete version information
> below:
> 
>  > version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
> 
>  > t(t( installed.packages()['car',] ))
>           [,1]
> Package  "car"
> LibPath  "C:/Programs/R/rw2011/library"
> Version  "1.0-17"
> Priority NA
> Bundle   NA
> Contains NA
> Depends  "R (>= 1.9.0)"
> Suggests "MASS, nnet, leaps"
> Imports  NA
> Built    "2.1.0"
> 
> 
> I subscribe to the help list in digest form, so would appreciate being
> copied directly in addition to seeing responses sent to the list.
> 
> David Dailey
> Shoreline, Washington, USA
> Lists at CompassRoseEnterprises.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul 28 09:28:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 28 Jul 2005 09:28:25 +0200
Subject: [R] gamma distribution
In-Reply-To: <1122504171.42e80debf07e9@webmail.scsv.nevada.edu>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>	<42E73542.6090505@statistik.uni-dortmund.de>
	<1122504171.42e80debf07e9@webmail.scsv.nevada.edu>
Message-ID: <42E88919.9020603@statistik.uni-dortmund.de>

Answering both messges here:


1. pantd at unlv.nevada.edu wrote:
 > Hi I appreciate your response. This is what I observed..taking
 > the log transform of the raw gamma does change the p value of
 > the test. That is what I am importing into excel (the p - values)

Well, so you made a mistake! And I still do not know why anybody realy 
want to import data to Excel, if the data is already in R.

For me, the results are identical (and there is no reason why not).


 > and then calculating the power of the test (both raw and
 > transformed).
 >
 > can you tell me what exactly your code is doing?

See below.


2. pantd at unlv.nevada.edu wrote:
> Hi
> I ran your code. I think it should give me the number of p values below 0.05
> significance level  (thats what i could understand from your code), but after
> running your code there is neither any error that shows up nor any value that
> the console displays.

You are right in the point what the code I sent does:

   erg <- replicate(1000, {
        x<-rgamma(10, 2.5, scale = 10)
        y<-rgamma(10, 2.5, scale = 10)
        wilcox.test(x, y, var.equal = FALSE)$p.value
    })
    sum(erg < 0.05) # 45


and it works for me. It results in a random number close to 50, hopefully.

Since both points above seem to be very strange on your machine: Which 
version of R are you using? We assume the most recent one which is R-2.1.1.

Uwe Ligges



From pantd at unlv.nevada.edu  Thu Jul 28 09:44:42 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Thu, 28 Jul 2005 00:44:42 -0700
Subject: [R] gamma distribution
In-Reply-To: <42E88919.9020603@statistik.uni-dortmund.de>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
	<42E73542.6090505@statistik.uni-dortmund.de>
	<1122504171.42e80debf07e9@webmail.scsv.nevada.edu>
	<42E88919.9020603@statistik.uni-dortmund.de>
Message-ID: <1122536682.42e88cea1592f@webmail.scsv.nevada.edu>

thanks for your response. btw i am calculating the power of the wilcoxon test. i
divide the total no. of rejections by the no. of simulations. so for 1000
simulations, at 0.05 level of significance if the no. of rejections are 50 then
the power will be 50/1000 = 0.05. thats y im importing in excel the p values.

is my approach correct??

thanks n regards
-dev


Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:

> Answering both messges here:
>
>
> 1. pantd at unlv.nevada.edu wrote:
>  > Hi I appreciate your response. This is what I observed..taking
>  > the log transform of the raw gamma does change the p value of
>  > the test. That is what I am importing into excel (the p - values)
>
> Well, so you made a mistake! And I still do not know why anybody realy
> want to import data to Excel, if the data is already in R.
>
> For me, the results are identical (and there is no reason why not).
>
>
>  > and then calculating the power of the test (both raw and
>  > transformed).
>  >
>  > can you tell me what exactly your code is doing?
>
> See below.
>
>
> 2. pantd at unlv.nevada.edu wrote:
> > Hi
> > I ran your code. I think it should give me the number of p values below
> 0.05
> > significance level  (thats what i could understand from your code), but
> after
> > running your code there is neither any error that shows up nor any value
> that
> > the console displays.
>
> You are right in the point what the code I sent does:
>
>    erg <- replicate(1000, {
>         x<-rgamma(10, 2.5, scale = 10)
>         y<-rgamma(10, 2.5, scale = 10)
>         wilcox.test(x, y, var.equal = FALSE)$p.value
>     })
>     sum(erg < 0.05) # 45
>
>
> and it works for me. It results in a random number close to 50, hopefully.
>
> Since both points above seem to be very strange on your machine: Which
> version of R are you using? We assume the most recent one which is R-2.1.1.
>
> Uwe Ligges
>
>



From ligges at statistik.uni-dortmund.de  Thu Jul 28 10:12:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 28 Jul 2005 10:12:47 +0200
Subject: [R] gamma distribution
In-Reply-To: <1122536682.42e88cea1592f@webmail.scsv.nevada.edu>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>	<42E73542.6090505@statistik.uni-dortmund.de>	<1122504171.42e80debf07e9@webmail.scsv.nevada.edu>	<42E88919.9020603@statistik.uni-dortmund.de>
	<1122536682.42e88cea1592f@webmail.scsv.nevada.edu>
Message-ID: <42E8937F.3000009@statistik.uni-dortmund.de>

pantd at unlv.nevada.edu wrote:

> thanks for your response. btw i am calculating the power of the wilcoxon test. i
> divide the total no. of rejections by the no. of simulations. so for 1000
> simulations, at 0.05 level of significance if the no. of rejections are 50 then
> the power will be 50/1000 = 0.05. thats y im importing in excel the p values.

No, since H1 is NOT true in your case (the power is undefined under H0).
In this case it is an estimator for the alpha error, but not the power. 
You might want to reread some basic textbook on testing theory.

BTW: Why do you think R cannot calculate 50/1000 and Excel does better?

> is my approach correct??

No.

Uwe Ligges



> thanks n regards
> -dev
> 
> 
> Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:
> 
> 
>>Answering both messges here:
>>
>>
>>1. pantd at unlv.nevada.edu wrote:
>> > Hi I appreciate your response. This is what I observed..taking
>> > the log transform of the raw gamma does change the p value of
>> > the test. That is what I am importing into excel (the p - values)
>>
>>Well, so you made a mistake! And I still do not know why anybody realy
>>want to import data to Excel, if the data is already in R.
>>
>>For me, the results are identical (and there is no reason why not).
>>
>>
>> > and then calculating the power of the test (both raw and
>> > transformed).
>> >
>> > can you tell me what exactly your code is doing?
>>
>>See below.
>>
>>
>>2. pantd at unlv.nevada.edu wrote:
>>
>>>Hi
>>>I ran your code. I think it should give me the number of p values below
>>
>>0.05
>>
>>>significance level  (thats what i could understand from your code), but
>>
>>after
>>
>>>running your code there is neither any error that shows up nor any value
>>
>>that
>>
>>>the console displays.
>>
>>You are right in the point what the code I sent does:
>>
>>   erg <- replicate(1000, {
>>        x<-rgamma(10, 2.5, scale = 10)
>>        y<-rgamma(10, 2.5, scale = 10)
>>        wilcox.test(x, y, var.equal = FALSE)$p.value
>>    })
>>    sum(erg < 0.05) # 45
>>
>>
>>and it works for me. It results in a random number close to 50, hopefully.
>>
>>Since both points above seem to be very strange on your machine: Which
>>version of R are you using? We assume the most recent one which is R-2.1.1.
>>
>>Uwe Ligges
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Thu Jul 28 10:27:01 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 28 Jul 2005 10:27:01 +0200
Subject: [R] gamma distribution
In-Reply-To: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
Message-ID: <17128.38613.68339.317568@stat.math.ethz.ch>

Hi 

Again to come back on the question why you don't get identical
p.values for the untransformed and the transformed data.

I ran your script below and I get always 2 identical test per
loop. In your text you are talking about the first 1000 values
for the untransformed and the next 1000 values for the
transformed. 

But did you consider that in each loop there is a test for the
untransformed and the transformed, so the tests are printed
alternating. 
This might be a reason why you did not get equal results.

Hope this helps,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


pantd at unlv.nevada.edu writes:
 > Hi R Users
 > 
 > 
 > This is a code I wrote and just want to confirm if the first 1000 values are raw
 > gamma (z) and the next 1000 values are transformed gamma (k) or not. As I get
 > 2000 rows once I import into excel, the p - values beyond 1000 dont look that
 > good, they are very high.
 > 
 > 
 > --
 > sink("a1.txt");
 > 
 > for (i in 1:1000)
 > {
 > x<-rgamma(10, 2.5, scale = 10)
 > y<-rgamma(10, 2.5, scale = 10)
 > z<-wilcox.test(x, y, var.equal = FALSE)
 > print(z)
 > x1<-log(x)
 > y1<-log(y)
 > k<-wilcox.test(x1, y1, var.equal = FALSE)
 > print(k)
 > }
 > 
 > ---
 > any suggestions are welcome
 > 
 > thanks
 > 
 > -devarshi
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jul 28 10:59:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 28 Jul 2005 10:59:06 +0200
Subject: [R] error message running R2WinBUGS
In-Reply-To: <d9f1219405072708217fabdd1d@mail.gmail.com>
References: <d9f1219405072708217fabdd1d@mail.gmail.com>
Message-ID: <42E89E5A.3090003@statistik.uni-dortmund.de>

This is a question related to WinBUGS, not to R nor to the R2WinBUGS 
"interface". Please ask on the appropriate lists/forums related to 
WinBUGS (with plain WinBUGS code, rather than R's interface code).


BTW: Let's answer your question before:

You have specified x[1,28] to be 127, but it is from a Binomial 
distribution with n=100.... (hence impossible!).
And that's what the WinBUGS (NOT R!) error message tells you.

Uwe Ligges


qi zhang wrote:

> *Dear R-user,
> *
> I try to run Winbugs from R using bugs function in R2WinBUGS.My model works 
> well in Winbugs except that I can't get DIC. Since I don't need DIC, when I 
> try to run Winbugs from R , I set "DIC=FALSE". My model is as following:
>  model {
> for (i in 1:N) {
> for(j in 1 : T ) {
> x[i, j] ~ dbin(p[i, j],n[i])
> #Hier.prior 
> p[i, j] ~ dbeta(alpha[i, j], beta[i, j])
> alpha[i, j] < - pi[ j]*(1-theta[i])/theta[i]
> beta[i, j] < -(1-pi[ j])*(1-theta[i])/theta[i]
> }
> } 
> 
> for(j in 1 : T ) {
> pi[ j ] ~dbeta(0.1,0.1)I(0.1,0.9)
> }
>  
> for (i in 1:N) {
> theta[i] ~dbeta(2,10)
> }
> }
> 
>  And my R code is as followings:
> 
> initials1<-list(theta=c(0.2,0.01), pi=rbeta(50, 0.1, 0.1)*0.8+0.1
> 
> , p=matrix(rbeta(100, 2, 10)*0.8+0.1,nr=2,nc=50,byrow=TRUE))
> 
> inits<-list(initials1 initials1)
> 
> data<-list(N=2,T=50 ,n=c(100,150),x = structure(.Data = c(
> 
> 3, 47, 8, 19, 87, 69,
> 
> 2, 4, 75, 24, 16, 81,
> 
> 10, 78, 87, 44, 17, 56,
> 
> 23, 75, 55, 85, 84, 69,
> 
> 6, 36, 8, 90, 47, 6,
> 
> 87, 61, 49, 57, 28, 56,
> 
> 31, 54, 75, 79, 67, 38,
> 
> 28, 13, 89, 63, 32, 68,
> 
> 70, 7,
> 
> 24, 95, 8, 14, 127, 134,
> 
> 7, 8, 133, 40, 76, 126,
> 
> 0, 132, 120, 137, 9, 91,
> 
> 3, 130, 18, 80, 134, 95,
> 
> 12, 34, 19, 111, 34, 25,
> 
> 127, 79, 132, 84, 72, 134,
> 
> 67, 44, 95, 69, 80, 51,
> 
> 57, 12, 138, 137, 64, 80,
> 
> 130, 58), .Dim = c(2, 50)))
> 
>  #run winbugs
> 
> library(R2WinBUGS)
> 
> structure<-bugs(data,inits,model.file="S:/Common/zhangqi/ebayes/for 
> R/model.txt",
> 
> parameters=c("p","pi","theta"),n.chains=2,n.iter=2000,n.burnin=500,
> 
> debug=TRUE,DIC=FALSE,bugs.directory="c:/Program Files/WinBUGS14/",
> 
> working.directory = NULL )
> 
>   Even if I changed initial values several times, I still keep geting the 
> following error message in Winbugs: 
>  
> display(log)
> 
> check(S:/Common/zhangqi/ebayes/for R/model.txt)
> 
> model is syntactically correct
> 
> data(C:/Program Files/R/rw2011/data.txt)
> 
> data loaded
> 
> compile(2)
> 
> model compiled
> 
> inits(1,C:/Program Files/R/rw2011/inits1.txt)
> 
> value of binomial x[1,28] must be between zero and order of x[1,28]
> 
>  I really have no clue about what I can do to fix it. 
> 
> Could any of your please take a look at my problem, I truely appreciate any 
> suggestions.
> 
>  Qi Zhang
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petzoldt at rcs.urz.tu-dresden.de  Thu Jul 28 11:05:58 2005
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 28 Jul 2005 11:05:58 +0200 (MEST)
Subject: [R] odesolve/lsoda differences on Windows and Mac
In-Reply-To: <x2d5p4j7l9.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.OSF.4.44.0507281054180.99789-100000@rcs12.urz.tu-dresden.de>

On 27 Jul 2005, Peter Dalgaard wrote:

> One thought: Integrating across input pulses is a known source of
> "turbulence" in lsoda. You might have better luck integrating over
> intervals in which the input function is continuous.
>
> Tweaking the lsoda tolerances is another thing to try.

Yes, that's also our experience. Where I am usually succesful
when playing with the tolerances or the interpolation rule of external
pulses, some of our students use the fixed step rk4 algorithm and some
others wrote their own integrators in R.

I have heared that several people had plans to provide alternative ODE
integrators for R but I currently do not know about the state of these
projects. It wold be nice if they might post this to the list in order to
avoid double work.

>
> I haven't seen lsoda fail like that, but it's not too surprising that
> marginal cases show platform dependency (i.e. the integrator just
> fails on Mac and barely succeeds on PC).
>

Aha, I see. It should be regarded carefully when publishing examples that
result in "marginal cases" as the common user would expect that R is
platform independent.

Thomas P.



From tshort at eprisolutions.com  Mon Jul 25 16:11:07 2005
From: tshort at eprisolutions.com (Short, Tom)
Date: Mon, 25 Jul 2005 10:11:07 -0400
Subject: [R] [R-pkgs] New version of Rpad
Message-ID: <FC4627EC695CD71180280003477108C50203CEEC@exchange.epripeac.com>

Announcing release 0.9.6 of Rpad. This version provides bug fixes and
some improved HTML handling. This is also the first widespread release
that supports Rpad as an installed package within R.

Rpad is an interactive, web-based analysis system. Rpad pages are
interactive workbook-type sheets based on R. Rpad is an analysis
package, a web-page designer, and a gui designer all wrapped in
one. Rpad makes it easy to develop powerful data analysis applications
that can be shared with others (most likely on an intranet).

Rpad is a new type of application like Google's gmail where the
browser page is dynamically updated with R calculations (Ajax is the 
latest buzzword for this).

Rpad can be installed in two ways: (1) local package and (2) server
style. The server version uses Apache (or other web server) to serve
up web pages and act as an R calculation engine. In the local version,
Rpad is installed as an R package, and it uses a builtin mini web
server to serve Rpad pages to your local browser.

You can try demos of Rpad at: http://www.rpad.org/Rpad

Here are two basic demos that show how it's used:

http://www.rpad.org/Rpad/Example1.Rpad
http://www.rpad.org/Rpad/InputExamples.Rpad

Here are two of the fancier demos that show off interactivity using
Rpad (in these examples, the R code is hidden by default):

http://www.rpad.org/Rpad/mapdemo.Rpad
http://www.rpad.org/Rpad/SearchRKeywords.Rpad

Rpad can be downloaded from CRAN or from www.rpad.org.

- Tom

Tom Short
EPRI Solutions, Inc.

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From b83503104 at yahoo.com  Mon Jul 25 19:45:45 2005
From: b83503104 at yahoo.com (Bahoo)
Date: Mon, 25 Jul 2005 10:45:45 -0700 (PDT)
Subject: [R] How to save the object of linear regression in file and load it
	later
Message-ID: <20050725174546.49604.qmail@web31803.mail.mud.yahoo.com>

Hi,

I am using locfit for regression.
After I do 
out = locfit(...),
I want to save "out" in a file, and load it later for
prediction.

How should I do it?  Thanks!



From buser at stat.math.ethz.ch  Thu Jul 28 11:19:06 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 28 Jul 2005 11:19:06 +0200
Subject: [R] gamma distribution
In-Reply-To: <1122536682.42e88cea1592f@webmail.scsv.nevada.edu>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
	<42E73542.6090505@statistik.uni-dortmund.de>
	<1122504171.42e80debf07e9@webmail.scsv.nevada.edu>
	<42E88919.9020603@statistik.uni-dortmund.de>
	<1122536682.42e88cea1592f@webmail.scsv.nevada.edu>
Message-ID: <17128.41738.29156.99223@stat.math.ethz.ch>

Hi 

As Uwe mentioned be careful about the difference the
significance level alpha and the power of a test.

To do power calculations you should specify and alternative
hypothesis H_A, e.g. if you have two populations you want to
compare and we assume that they are normal distributed (equal
unknown variance for simplicity). We are interested if there is
a difference in the mean and want to use the t.test.
Our Null hypothesis H_0: there is no difference in the means

To do a power calculation for our test, we first have to specify
and alternative H_A: the mean difference is 1 (unit)
Now for a fix number of observations we can calculate the power
of our test, which is in that case the probability that (if the
true unknown difference is 1, meaning that H_A is true) our test
is significant, meaning if I repeat the test many times (always
taking samples with mean difference of 1), the number of
significant test divided by the total number of tests is an
estimate for the power.


In you case the situation is a little bit more complicated. You
need to specify an alternative hypothesis.
In one of your first examples you draw samples from two gamma
distributions with different shape parameter and the same
scale. But by varying the shape parameter the two distributions
not only differ in their mean but also in their form.
 
I got an email from Prof. Ripley in which he explained in
details and very precise some examples of tests and what they
are testing. It was in addition to the first posts about t tests
and wilcoxon test. 
I attached the email below and recommend to read it carefully. It
might be helpful for you, too.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------
 
________________________________________________________________________

From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Christoph Buser <buser at stat.math.ethz.ch>
cc: "Liaw, Andy" <andy_liaw at merck.com>
Subject: Re: [R] Alternatives to t-tests (was Code Verification)
Date: Thu, 21 Jul 2005 10:33:28 +0100 (BST)

I believe there is a rather more to this than Christoph's account.  The 
Wilcoxon test is not testing the same null hypothesis as the t-test, and 
that may very well matter in practice and it does in the example given.

The (default in R) Welch t-test tests a difference in means between two 
samples, not necessarily of the same variance or shape.  A difference in 
means is simple to understand, and is unambiguously defined at least if 
the distributions have means, even for real-life long-tailed 
distributions.  Inference from the t-test is quite accurate even a long 
way from normality and from equality of the shapes of the two 
distributions, except in very small sample sizes.  (I point my beginning 
students at the simulation study in `The Statistical Sleuth' by Ramsey and 
Schafer, stressing that the unequal-variance t-test ought to be the 
default choice as it is in R.  So I get them to redo the simulations.)

The Wilcoxon test tests a shift in location between two samples from 
distributions of the same shape differing only by location.  Having the 
same shape is part of the null hypothesis, and so is an assumption that 
needs to be verified if you want to conclude there is a difference in 
location (e.g. in means).  Even if you assume symmetric distributions (so 
the location is unambiguously defined) the level of the test depends on 
the shapes, tending to reject equality of location in the presence of 
difference of shape.  So you really are testing equality of distribution, 
both location and shape, with power concentrated on location-shift 
alternatives.

Given samples from a gamma(shape=2) and gamma(shape=20) distributions, we 
know what the t-test is testing (equality of means).  What is the Wilcoxon 
test testing?  Something hard to describe and less interesting, I believe.

BTW, I don't see the value of the gamma simulation as this 
simultaneously changes mean and shape between the samples.  How about
checking holding the mean the same:

n <- 1000
z1 <- z2 <- numeric(n)
for (i in 1:n) {
   x <- rgamma(40, 2.5, 0.1)
   y <- rgamma(40, 10, 0.1*10/2.5)
   z1[i] <- t.test(x, y)$p.value
   z2[i] <- wilcox.test(x, y)$p.value
}
## Level
1 - sum(z1>0.05)/1000  ## 0.049
1 - sum(z2>0.05)/1000  ## 0.15

? -- the Wilcoxon test is shown to be a poor test of equality of means. 
Christoph's simulation shows that it is able to use difference in shape as 
well as location in the test of these two distributions, whereas the 
t-test is designed only to use the difference in means.  Why compare the 
power of two tests testing different null hypotheses?

I would say a very good reason to use a t-test is if you are actually 
interested in the hypothesis it tests ....





pantd at unlv.nevada.edu writes:
 > thanks for your response. btw i am calculating the power of the wilcoxon test. i
 > divide the total no. of rejections by the no. of simulations. so for 1000
 > simulations, at 0.05 level of significance if the no. of rejections are 50 then
 > the power will be 50/1000 = 0.05. thats y im importing in excel the p values.
 > 
 > is my approach correct??
 > 
 > thanks n regards
 > -dev
 > 
 >



From dragos.ilie at bth.se  Thu Jul 28 11:33:55 2005
From: dragos.ilie at bth.se (Dragos Ilie)
Date: Thu, 28 Jul 2005 11:33:55 +0200
Subject: [R] Minimum-chi-square estimation
Message-ID: <42E8A683.7050605@bth.se>

Is there any R package that does point estimation by using the
minimum-chi-square method?

Regards,
Dragos



From sean.oriordain at gmail.com  Thu Jul 28 11:37:51 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Thu, 28 Jul 2005 09:37:51 +0000
Subject: [R] How to save the object of linear regression in file and
	load it later
In-Reply-To: <20050725174546.49604.qmail@web31803.mail.mud.yahoo.com>
References: <20050725174546.49604.qmail@web31803.mail.mud.yahoo.com>
Message-ID: <8ed68eed05072802375d582601@mail.gmail.com>

Hi Bahoo!

I've found the R/Rpad Reference Card quite good at helping me find
this sort of information... i.e. towards the bottom of the first
column of the first page it says...

save(file,...) saves the specified ojects (...) in the XDR platform
independent binary format

if I then say ?save it tells me that ?load is what you're looking for
at another date...

cheers!
Sean




On 25/07/05, Bahoo <b83503104 at yahoo.com> wrote:
> Hi,
> 
> I am using locfit for regression.
> After I do
> out = locfit(...),
> I want to save "out" in a file, and load it later for
> prediction.
> 
> How should I do it?  Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From r.shengzhe at gmail.com  Thu Jul 28 13:26:59 2005
From: r.shengzhe at gmail.com (Shengzhe Wu)
Date: Thu, 28 Jul 2005 13:26:59 +0200
Subject: [R] Help: how to specify the current active window by mouse
Message-ID: <ea57975b05072804267d91afd6@mail.gmail.com>

Hello,

When I use "windows()" to open several windows, e.g. 4 windows, device
2, device 3, device 4 and device 5. Normally using "dev.set()" to
specify the current active window, if there is a way to specify the
active window by mouse click? When I click device 3, device 3 becomes
active (shown on device header), and when I click device 4, device 4
becomes active, so on....

Thank you,
Shengzhe



From f.harrell at vanderbilt.edu  Thu Jul 28 13:55:05 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Jul 2005 07:55:05 -0400
Subject: [R] CSV file and date. Dates are read as factors!
In-Reply-To: <s2e7fca5.036@grecc.umaryland.edu>
References: <s2e7fca5.036@grecc.umaryland.edu>
Message-ID: <42E8C799.6090109@vanderbilt.edu>

John Sorkin wrote:
> I am using read.csv to read a CSV file (produced by saving an Excel file
> as a CSV file). The columns containing dates are being read as factors.
> Because of this, I can not compute follow-up time, i.e.
> Followup<-postDate-preDate. I would appreciate any suggestion that would
> help me read the dates as dates and thus allow me to calculate follow-up
> time.
> Thanks
> John

library(Hmisc)
?csv.get   (see datevars argument)

Frank

> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Sebastian.Leuzinger at unibas.ch  Thu Jul 28 13:55:10 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Thu, 28 Jul 2005 13:55:10 +0200
Subject: [R] stl()
Message-ID: <200507281355.10175.Sebastian.Leuzinger@unibas.ch>

Hello, anyone got an idea on how to use stl() so that the remainder eventually 
becomes white noise? i used stl repeatedly but there is autocorrelation in 
the remainder that i can't get rid of. 
os: linux suse9.3
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From murdoch at stats.uwo.ca  Thu Jul 28 13:58:03 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 28 Jul 2005 07:58:03 -0400
Subject: [R] Help: how to specify the current active window by mouse
In-Reply-To: <ea57975b05072804267d91afd6@mail.gmail.com>
References: <ea57975b05072804267d91afd6@mail.gmail.com>
Message-ID: <42E8C84B.30009@stats.uwo.ca>

On 7/28/2005 7:26 AM, Shengzhe Wu wrote:
> Hello,
> 
> When I use "windows()" to open several windows, e.g. 4 windows, device
> 2, device 3, device 4 and device 5. Normally using "dev.set()" to
> specify the current active window, if there is a way to specify the
> active window by mouse click? When I click device 3, device 3 becomes
> active (shown on device header), and when I click device 4, device 4
> becomes active, so on....

No, there's nothing built in to do that, though you could probably write 
something using the tcltk package to do it.

The logic is that being active means being the target of plot output, 
and essentially all plot output comes from executed commands, not from 
mouse clicks.  Since you're executing commands already, why not put in 
one more?

Duncan Murdoch



From roger.bos at gmail.com  Thu Jul 28 14:06:47 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 28 Jul 2005 08:06:47 -0400
Subject: [R] CSV file and date. Dates are read as factors!
In-Reply-To: <s2e7fca5.036@grecc.umaryland.edu>
References: <s2e7fca5.036@grecc.umaryland.edu>
Message-ID: <1db72680050728050621746019@mail.gmail.com>

Working with dates is not easy (for me at least).  I always manage to
get it done, but the code is somewhat messy.  I have not tried using
the Hmisc package as Frank suggested, but I will show you my code as
an alternate way:

w <- unclass((as.Date(as.character(dataMat$fy1_period_end_date),
format="%m/%d/%Y") - as.Date(datec[i], format="%m/%d/%Y"))/365)

w is the time (in days) between two dates.  You can see that I had to
"unclasss" the first date vector.  I read my files in csv also, so I
am sure something similar can be made to work for you.

HTH,

Roger




On 7/27/05, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> I am using read.csv to read a CSV file (produced by saving an Excel file
> as a CSV file). The columns containing dates are being read as factors.
> Because of this, I can not compute follow-up time, i.e.
> Followup<-postDate-preDate. I would appreciate any suggestion that would
> help me read the dates as dates and thus allow me to calculate follow-up
> time.
> Thanks
> John
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From alxmilton at yahoo.it  Thu Jul 28 14:11:30 2005
From: alxmilton at yahoo.it (alessandro carletti)
Date: Thu, 28 Jul 2005 05:11:30 -0700 (PDT)
Subject: [R] conversion from SAS
Message-ID: <20050728121131.55390.qmail@web26610.mail.ukl.yahoo.com>

Hi, I wonder if anybody could help me in converting
this easy SAS program into R.
(I'm still trying to do that!)

PROC IMPORT OUT= WORK.CHLA_italian 
            DATAFILE= "C:\Documents and
Settings\carleal\My
Documents\REBECCA\stat\sas\All&nutrients.xls" 
            DBMS=EXCEL2000 REPLACE;
     GETNAMES=YES;
RUN;
data chla_italian;
   set chla_italian;
   year=year(datepart(date));
   month=month(datepart(date));
   run;

proc sort data=chla_italian; by station; run;
/* Check bloom for seasonal cycle outliers */
data sort_dataset;
   set chla_italian;
   chla=chl_a;
   dayno=date-mdy(1,1,year)+1;
   cos1=cos(2*3.14*dayno/365);
   sin1=sin(2*3.14*dayno/365);
   cos2=cos(4*3.14*dayno/365);
   sin2=sin(4*3.14*dayno/365);
   cos3=cos(6*3.14*dayno/365);
   sin3=sin(6*3.14*dayno/365);
   cos4=cos(8*3.14*dayno/365);
   sin4=sin(8*3.14*dayno/365);
   bloom=0;
   w_chla=1/chla/chla;
run;
ODS listing close;
%macro sort_event(cut_off,last=0);
/*proc glm data=sort_dataset;
   class year;
   model logchla=year cos1 sin1 cos2 sin2 cos3 sin3
cos4 sin4 /solution;
   by station;
   where bloom=0;
   output out=chla_res predicted=pred student=studres
cookd=cookd rstudent=rstudent u95=u95;
   lsmeans year / at (cos1 sin1 cos2 sin2 cos3 sin3
cos4 sin4)=(0 0 0 0 0 0 0 0);
   ODS output ParameterEstimates=parmest
LSmeans=lsmeans;
run;*/
proc glm data=sort_dataset;
   class year month;
   model chla=/solution;
   by station;
   weight w_chla;
   where bloom=0;
   output out=chla_res predicted=pred student=studres
cookd=cookd 
daynumber<-data$date-mdy(1,1,y)+1 
rstudent=rstudent ucl=ucl lcl=lcl / alpha=0.02;
*   lsmeans year / at (cos1 sin1)=(0 0);
*   ODS output ParameterEstimates=parmest
LSmeans=lsmeans;
run;
quit;
data sort_dataset;
   set chla_res;
   increase=ucl-pred;
   if chla>UCL then bloom=1;
   w_chla=1/(50+5*pred*pred);
   %if &last=0 %then %do; drop ucl lcl cookd rstudent
studres pred; %end;
run;
%mend sort_event;
ODS listing;
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0);
%sort_event(2.0,last=1);
/* Combine bloom information with all chlorophyll
values */
data chla_sep;
   merge sort_dataset(keep=station date bloom pred ucl
lcl) chla_italian (rename=(chl_a=chla));
   by station date;
*   lcl=exp(lcl);
*   ucl=exp(ucl);
*   pred=exp(pred);
   if bloom=. then bloom=1;
   if bloom=0 then chla1=chla; else chla1=.;
   if bloom=1 then chla2=chla; else chla2=.;
run;
 
symbol1 i=none value=plus color=red;
symbol2 i=none value=plus color=green;
symbol3 i=join value=none line=1 color=black;
axis1 logbase=10; axis1;
proc gplot data=chla_sep;
   plot chla2*date=1 chla1*date=2 (ucl pred
lcl)*date=3 /overlay vaxis=axis1;
   by station;
run;
quit;
proc glm data=chla_sep;
   class station year month;
   model salinity temperature Transparency__m_
Nitrate__mmol_l_1_ Phosphate__mmol_l_1_
Silicate__mmol_l_1_=bloom month/solution;
   by station;
run;
quit;

Thanks



From Setzer.Woodrow at epamail.epa.gov  Thu Jul 28 15:14:01 2005
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow@epamail.epa.gov)
Date: Thu, 28 Jul 2005 09:14:01 -0400
Subject: [R] odesolve/lsoda differences on Windows and Mac
In-Reply-To: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>
Message-ID: <OF9096CC0D.C6F6EFE1-ON8525704C.00484493-8525704C.0048B1DF@epamail.epa.gov>

I've been talking offline with Hank Stephens about this; note that in
the example he quotes, he set hmin  = 0.1, and the quoted error message
says that the stepsize had reached hmin with no convergence.  I believe
that he intended to set hmax (because of the pulsed input).  Then, Peter
Dalgaard's explanation would make sense -- the PPC platform just needs a
smaller stepsize than does the PC to achieve convergence.

R. Woodrow Setzer, Jr.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B305-03/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-4284


                                                                        
             "Martin Henry H.                                           
             Stevens"                                                   
             <HStevens at MUOhio                                        To 
             .edu>                    "'R-Help'"                        
                                      <r-help at stat.math.ethz.ch>        
             07/27/2005 12:36                                        cc 
             PM                       Thomas Petzoldt                   
                                      <petzoldt at rcs.urz.tu-dresden.de>, 
                                      Woodrow Setzer/RTP/USEPA/US at EPA   
                                                                Subject 
                                      odesolve/lsoda differences on     
                                      Windows and Mac                   
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        




Hi -
I am getting different results when I run the numerical integrator
function lsoda (odesolve package) on a Mac and a PC. I am trying to
simulating a system of 10 ODE's with two exogenous pulsed inputs to the
system, and have had reasonably good success with many model parameter
sets. Under some parameter sets, however, the simulations fail on the
Mac (see error message below). The same parameter sets, however, appear
to run fine for our computational technician on his PC, generating
apparently very  reasonable data.

Our tech is successfully  running
Dell Latitude D810, Windows XP Pro (Service Pack 2), 1Gb
RAM.  RGUI 2.1.1

I am running:
  R Version 2.1.1  (2005-06-20) on a
Mac OS 10.3.9
   Machine Model:        Power Mac G5
   CPU Type:             PowerPC 970  (2.2)
   Number Of CPUs:             2
   CPU Speed:            2 GHz
   L2 Cache (per CPU):         512 KB
   Memory:         1.5 GB
   Bus Speed:            1 GHz
   Boot ROM Version:           5.0.7f0
   Serial Number:        XB3472Q1NVS

My Error Message
 > system.time(
+ outAc2 <- as.data.frame(lsoda(xstart,times, pondamph, parms,
tcrit=170*730, hmin=.1))
+ )
[1] 0.02 0.01 0.04 0.00 0.00
Warning messages:
1: lsoda--  at t (=r1) and step size h (=r2), the
2:       corrector convergence failed repeatedly
3:       or with abs(h) = hmin
4: Returning early from lsoda.  Results are accurate, as far as they go

Thanks for any input.

Hank Stevens



Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From dmbates at gmail.com  Thu Jul 28 15:22:07 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 28 Jul 2005 08:22:07 -0500
Subject: [R] [R-pkgs] New versions of Matrix and lme4 packages
Message-ID: <40e66e0b05072806227ee2381d@mail.gmail.com>

Version 0.98-1 of the lme4 package and of the Matrix package are now
on CRAN.  This version provides the adaptive Gauss-Hermite quadrature
(AGQ) method for fitting generalized linear mixed models.

A new generic function mcmcsamp has been added with a method for
objects in the "lmer" (linear mixed model fit or generalized linear
mixed-effects model fit) class.  This function provides a Markov Chain
Monte Carlo sample from the posterior distribution of the model
parameters.  The returned object has (S3) class "mcmc" and the
diagnostic functions in the coda package can be used on it.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From jfox at mcmaster.ca  Thu Jul 28 15:26:29 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 28 Jul 2005 09:26:29 -0400
Subject: [R] Unexpected behavior in recode{car}
In-Reply-To: <4702645135092E4497088F71D9C8F51A128BDE@afhex01.dpi.wa.gov.au>
Message-ID: <20050728132630.ZGUN27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Tom and David,

The source of the problem isn't hard to see if you trace the execution of
recode() via debug(): The test for whether the result can be coerced to
numeric is faulty. I've fixed the bug and will upload a new version of car
to CRAN shortly. In the meantime, you can use 

ss <- recode(nn, "2='Num2'; 4='Num4'", as.factor=TRUE)

Thanks for bringing this bug to my attention.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mulholland, Tom
> Sent: Thursday, July 28, 2005 2:01 AM
> To: D. Dailey; r-help at stat.math.ethz.ch
> Subject: Re: [R] Unexpected behavior in recode{car}
> 
> require( car )
> set.seed(12345)
> nn <- sample( c( 2, 4 ), size=50, replace=TRUE ) rr <- 
> recode( nn, "2='TWO';4='FOUR'" ) table( rr, exclude=NULL ) ss 
> <- recode( nn, "2='Num2';4='Num4'" )  # Doesn't work as 
> expected table( ss, exclude=NULL ) ss <- recode( nn, 
> "2='Num2';4='Num4'", TRUE )  #?
> table( ss, exclude=NULL )
> tt <- recode( nn, "2='TWO'; 4='Num4'" )
> table( tt, exclude=NULL )
> uu <- recode( nn, "2='Num2'; 4='FOUR'" ) table( uu, exclude=NULL )
> 
> I looked at the code and found it too difficult to 
> immediately decipher. So does making the result a factor 
> cause any real problems?
> 
> I noticed that the same response happens with any letterset 
> followed by a number recode( nn, "2='Num2'; 4='abc5'" )
> 
> Tom
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of D. Dailey
> > Sent: Thursday, 28 July 2005 11:45 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Unexpected behavior in recode{car}
> > 
> > 
> > Thanks to the R creators for such a great statistical system. 
> > Thanks to
> > the R help list, I have (finally) gotten far enough in R to have a 
> > question I hope to be worth posting.
> > 
> > I'm using the recode function from John Fox's car package and have 
> > encountered some unexpected behavior.
> > 
> > Consider the following example:
> > 
> > ## Begin cut-and-paste example
> > require( car )
> > set.seed(12345)
> > nn <- sample( c( 2, 4 ), size=50, replace=TRUE ) rr <- recode( nn, 
> > "2='TWO';4='FOUR'" ) table( rr, exclude=NULL ) ss <- recode( nn, 
> > "2='Num2';4='Num4'" )  # Doesn't work as expected table( ss, 
> > exclude=NULL ) tt <- recode( nn, "2='TWO'; 4='Num4'" ) table( tt, 
> > exclude=NULL ) uu <- recode( nn, "2='Num2'; 4='FOUR'" ) table( uu, 
> > exclude=NULL ) ## End cut-and-paste example
> > 
> > All but the recoding to ss work as expected: I get a 
> character vector 
> > with 23 instances of either "FOUR" or "Num4" and 27 
> instances of "TWO"
> > or "Num2".
> > 
> > But for the ss line, wherein all the strings to be assigned 
> contain a 
> > digit, the resulting vector contains all NAs. Using str(), 
> I note that 
> > ss is a numeric vector.
> > 
> > Is there a tidy way (using recode) to recode numeric values into 
> > character strings, all of which contain a digit? I have a 
> workaround 
> > for my current project, but it would be nice to be able to 
> use mixed 
> > alphanumeric strings in this context.
> > 
> > Thanks in advance for any insight you can give into this question.
> > 
> > Using R 2.1.1 (downloaded binary) on Windows XP Pro, car version 
> > 1.0-17 (installed from CRAN via Windows GUI). Complete version 
> > information
> > below:
> > 
> >  > version
> >           _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    2
> > minor    1.1
> > year     2005
> > month    06
> > day      20
> > language R
> > 
> >  > t(t( installed.packages()['car',] ))
> >           [,1]
> > Package  "car"
> > LibPath  "C:/Programs/R/rw2011/library"
> > Version  "1.0-17"
> > Priority NA
> > Bundle   NA
> > Contains NA
> > Depends  "R (>= 1.9.0)"
> > Suggests "MASS, nnet, leaps"
> > Imports  NA
> > Built    "2.1.0"
> > 
> > 
> > I subscribe to the help list in digest form, so would 
> appreciate being 
> > copied directly in addition to seeing responses sent to the list.
> > 
> > David Dailey
> > Shoreline, Washington, USA
> > Lists at CompassRoseEnterprises.com
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Setzer.Woodrow at epamail.epa.gov  Thu Jul 28 15:30:43 2005
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow@epamail.epa.gov)
Date: Thu, 28 Jul 2005 09:30:43 -0400
Subject: [R] odesolve/lsoda differences on Windows and Mac
In-Reply-To: <Pine.OSF.4.44.0507281054180.99789-100000@rcs12.urz.tu-dresden.de>
Message-ID: <OF69D2121D.539B155A-ON8525704C.0048B5F5-8525704C.004A3971@epamail.epa.gov>

I will have added most of the solvers from the ode package odepack by
Alan Hindmarsh, LLNL, to odesolve this year.  These are all solvers in
the lsode family.
I would also like to add solver(s) for DAEs, like daskr (Brown,
Hindmarsh, Petzold), but that may take a bit longer.
If other folks are planning to contribute solvers, I would be happy to
discuss including them in odesolve, so there would be a single main
package for solving ODEs.

R. Woodrow Setzer, Jr.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B305-03/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-4284


                                                                        
             Thomas Petzoldt                                            
             <petzoldt at rcs.ur                                           
             z.tu-dresden.de>                                        To 
                                      Peter Dalgaard                    
             07/28/2005 05:05         <p.dalgaard at biostat.ku.dk>        
             AM                                                      cc 
                                      "Martin Henry H. Stevens"         
                                      <HStevens at muohio.edu>, "'R-Help'" 
                                      <r-help at stat.math.ethz.ch>,       
                                      Woodrow Setzer/RTP/USEPA/US at EPA,  
                                      Thomas Petzoldt                   
                                      <petzoldt at rcs.urz.tu-dresden.de>  
                                                                Subject 
                                      Re: [R] odesolve/lsoda            
                                      differences on Windows and Mac    
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        




On 27 Jul 2005, Peter Dalgaard wrote:

> One thought: Integrating across input pulses is a known source of
> "turbulence" in lsoda. You might have better luck integrating over
> intervals in which the input function is continuous.
>
> Tweaking the lsoda tolerances is another thing to try.

Yes, that's also our experience. Where I am usually succesful
when playing with the tolerances or the interpolation rule of external
pulses, some of our students use the fixed step rk4 algorithm and some
others wrote their own integrators in R.

I have heared that several people had plans to provide alternative ODE
integrators for R but I currently do not know about the state of these
projects. It wold be nice if they might post this to the list in order
to
avoid double work.

>
> I haven't seen lsoda fail like that, but it's not too surprising that
> marginal cases show platform dependency (i.e. the integrator just
> fails on Mac and barely succeeds on PC).
>

Aha, I see. It should be regarded carefully when publishing examples
that
result in "marginal cases" as the common user would expect that R is
platform independent.

Thomas P.



From r.ramyar at gmail.com  Thu Jul 28 15:36:52 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 14:36:52 +0100
Subject: [R] CUSUM SQUARED structural breaks approach?
In-Reply-To: <20050112144553.4faf3cc8.Achim.Zeileis@wu-wien.ac.at>
References: <5f4f8279050111051031974bc4@mail.gmail.com>
	<20050111151032.07d05ab4.Achim.Zeileis@wu-wien.ac.at>
	<5f4f82790501111133193a21fb@mail.gmail.com>
	<20050112144553.4faf3cc8.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <5f4f827905072806363e363c6d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/b288ec93/attachment.pl

From amelie2000 at gmx.de  Thu Jul 28 15:30:57 2005
From: amelie2000 at gmx.de (amelie2000@gmx.de)
Date: Thu, 28 Jul 2005 15:30:57 +0200 (MEST)
Subject: [R] =?iso-8859-1?q?Cochran-Armitage-trend-test?=
Message-ID: <17358.1122557457@www36.gmx.net>

Hi!

I am searching for the Cochran-Armitage-trend-test. Is it included in an
R-package? 

Thank you!

--



From fjarrad at pgrad.unimelb.edu.au  Fri Jul 29 07:09:18 2005
From: fjarrad at pgrad.unimelb.edu.au (Frith Jarrad)
Date: Thu, 28 Jul 2005 22:09:18 -0700
Subject: [R] Anova's in R
Message-ID: <6.1.1.1.2.20050728220728.01a8b6e8@mail.student.unimelb.edu.au>

Hello.

I am looking for some help using anova's in RGui.

My experiment ie data, has a fixed burning treatment (Factor A) 2 levels, 
unburnt/burnt.
Nested within each level of Factor A are 2 random sites (Factor B).
All sites are crossed with a fixed temperature treatment (Factor C) 2 
levels, 0 degreesC/2 degreesC, with many replicates of these temperature 
treatments randomly located at each site.

I am trying the following
aov(dependent 
variable~burning*temperature*site+Error(replicate),data=dataset) and 
variations on that, however can't get it quite right.... the F ratios are 
not correct. I imagine this is a fairly common experimental design in 
ecology and would ask that anyone who has some advice please reply to this 
email?

Thank-you,
Frith



From jhainm at fas.harvard.edu  Thu Jul 28 15:52:29 2005
From: jhainm at fas.harvard.edu (jhainm@fas.harvard.edu)
Date: Thu, 28 Jul 2005 09:52:29 -0400
Subject: [R] replace matrix values with names from a dataframe
In-Reply-To: <mailman.9.1122544801.6372.r-help@stat.math.ethz.ch>
References: <mailman.9.1122544801.6372.r-help@stat.math.ethz.ch>
Message-ID: <1122558749.42e8e31dd4eb5@webmail.fas.harvard.edu>

Hi,

I am looking for a way to replace matrix values with names from a dataframe.

Let me do this by example: I have a dataframe:

>data
  city.name
1    munich
2     paris
3     tokio
4    london
5    boston

each city name corresponds to only one index number (there is only one
observation for each city). After doing some matching I end up with a matrix
that looks something like this:

> X
       [,1] [,2]
  [1,]    2    4
  [2,]    5    1
  [3,]    5    3
  [4,]   12  217
  [5,]   16   13

Here the numbers in the matrix are the index numbers from my original dataset,
each row is a matched pair (so e.g. the first row tells me that obs. number 2
(i.e. Paris) was matched to obs number 4 (i.e. London)).

Now I am looking for a quick way to transform the index numbers back to city
names, so that at the end I have a matrix that looks something like this:

> X.transformed
         [,1]     [,2]
  [1,]  paris   london
  [2,] boston   munich
  [3,] boston    tokio
  [4,]     12      217
  [5,]     16       13

etc. So instead of the index number, the matrix should contain the names that
corresponds to it. In my real data, I have many many names and replacing each
value by hand would take too long. Any help is highly appreciated.

Thank you.

Regards,
Jens



From anders.bjorgesater at bio.uio.no  Thu Jul 28 15:52:27 2005
From: anders.bjorgesater at bio.uio.no (Anders =?iso-8859-1?Q?Bj=F8rges=E6ter?=)
Date: Thu, 28 Jul 2005 15:52:27 +0200
Subject: [R] catching errors in a loop
Message-ID: <6.2.0.14.2.20050728152314.01ee3750@pop.uio.no>

Hello

I can't figure out how to handle errors in R. I have a loop, e.g.

for (i in 2:n) {
.
fit <- nls(model), start=list

if any type of error occur write i to a text file
.
}

I putted ?try? around the nls-expression and this let me run through the 
loop without R stopping (which I want because each loop takes some time so 
I do not want it to stop), but I also want to capture the variable when an 
error occur.

Appreciate any help

/Anders
- - - - - - - - - - - - - - - - - - -
I tried to use:
**?options(error=write(variable.names(matrix[i]), 
file="..\\error.txt",append = TRUE))?, hoping this made R write to the text 
file every time an error occurred (but this made R write all i?s in the 
loop to the text file).
**tryCatch(<- nls(model), start=list
), finally =write(
) also writes to a 
text file but not necessary when there is an error.
**?if (Parameter>x errorM=9 else errorM =0? works but I want to capture any 
type of error.
- - - - - - - - - - - - - - - - -



From HStevens at MUOhio.edu  Thu Jul 28 15:51:30 2005
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Thu, 28 Jul 2005 09:51:30 -0400
Subject: [R] odesolve/lsoda differences on Windows and Mac UPDATE
In-Reply-To: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>
References: <4a7fd9de474d87edd96144a522ae2548@MUOhio.edu>
Message-ID: <5b7e31fcd2734780875377adc3d864f1@MUOhio.edu>

To all:
After talking to Woody Setzer offline, I ran my "problem" scripts 
again. I am embarrassed to say that it worked fine for all previously 
intransigent parameter sets. I compared the results to those of my 
Windows buddy, and they are essentially identical, with the average 
absolute difference at each time point is 1.3e-06. I am planning to go 
back and try to understand what went wrong before. I never used hmin 
(mistakenly of course, instead of hmax) UNTIL a run with default lsoda 
argument values failed. Now the default values work! Thus the mistaken 
use of hmin isn't the entire answer.
Thank you for the time and interest, and I apologize for troubling you. 
I will get back to the list if I can ever repeat the problem or if I 
can figure out what I did wrong.

Best Regards,
Hank

On Jul 27, 2005, at 12:36 PM, Martin Henry H. Stevens wrote:

> Hi -
> I am getting different results when I run the numerical integrator
> function lsoda (odesolve package) on a Mac and a PC. I am trying to
> simulating a system of 10 ODE's with two exogenous pulsed inputs to the
> system, and have had reasonably good success with many model parameter
> sets. Under some parameter sets, however, the simulations fail on the
> Mac (see error message below). The same parameter sets, however, appear
> to run fine for our computational technician on his PC, generating
> apparently very  reasonable data.
>
> Our tech is successfully  running
> Dell Latitude D810, Windows XP Pro (Service Pack 2), 1Gb
> RAM.  RGUI 2.1.1
>
> I am running:
>   R Version 2.1.1  (2005-06-20) on a
> Mac OS 10.3.9
>    Machine Model:	Power Mac G5
>    CPU Type:	PowerPC 970  (2.2)
>    Number Of CPUs:	2
>    CPU Speed:	2 GHz
>    L2 Cache (per CPU):	512 KB
>    Memory:	1.5 GB
>    Bus Speed:	1 GHz
>    Boot ROM Version:	5.0.7f0
>    Serial Number:	XB3472Q1NVS
>
> My Error Message
>> system.time(
> + outAc2 <- as.data.frame(lsoda(xstart,times, pondamph, parms,
> tcrit=170*730, hmin=.1))
> + )
> [1] 0.02 0.01 0.04 0.00 0.00
> Warning messages:
> 1: lsoda--  at t (=r1) and step size h (=r2), the
> 2:       corrector convergence failed repeatedly
> 3:       or with abs(h) = hmin
> 4: Returning early from lsoda.  Results are accurate, as far as they go
>
> Thanks for any input.
>
> Hank Stevens
>
>
>
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From jorge.delavegagongora at gmail.com  Thu Jul 28 15:57:36 2005
From: jorge.delavegagongora at gmail.com (Jorge de la Vega Gongora)
Date: Thu, 28 Jul 2005 08:57:36 -0500
Subject: [R] CSV file and date. Dates are read as factors!
In-Reply-To: <s2e7fca5.036@grecc.umaryland.edu>
References: <s2e7fca5.036@grecc.umaryland.edu>
Message-ID: <ab1f54ec05072806577cad1dd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/c5feea51/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Thu Jul 28 16:06:42 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 28 Jul 2005 16:06:42 +0200
Subject: [R] replace matrix values with names from a dataframe
References: <mailman.9.1122544801.6372.r-help@stat.math.ethz.ch>
	<1122558749.42e8e31dd4eb5@webmail.fas.harvard.edu>
Message-ID: <00ad01c5937d$94b71a30$0540210a@www.domain>

maybe something like this could be helpful

city.name <- c("munich", "paris", "tokio", "london", "boston")
X <- cbind(c(2, 5, 5), c(4, 1, 3))
####
matrix(city.name[X], ncol = 2)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: <jhainm at fas.harvard.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, July 28, 2005 3:52 PM
Subject: [R] replace matrix values with names from a dataframe


> Hi,
>
> I am looking for a way to replace matrix values with names from a 
> dataframe.
>
> Let me do this by example: I have a dataframe:
>
>>data
>  city.name
> 1    munich
> 2     paris
> 3     tokio
> 4    london
> 5    boston
>
> each city name corresponds to only one index number (there is only 
> one
> observation for each city). After doing some matching I end up with 
> a matrix
> that looks something like this:
>
>> X
>       [,1] [,2]
>  [1,]    2    4
>  [2,]    5    1
>  [3,]    5    3
>  [4,]   12  217
>  [5,]   16   13
>
> Here the numbers in the matrix are the index numbers from my 
> original dataset,
> each row is a matched pair (so e.g. the first row tells me that obs. 
> number 2
> (i.e. Paris) was matched to obs number 4 (i.e. London)).
>
> Now I am looking for a quick way to transform the index numbers back 
> to city
> names, so that at the end I have a matrix that looks something like 
> this:
>
>> X.transformed
>         [,1]     [,2]
>  [1,]  paris   london
>  [2,] boston   munich
>  [3,] boston    tokio
>  [4,]     12      217
>  [5,]     16       13
>
> etc. So instead of the index number, the matrix should contain the 
> names that
> corresponds to it. In my real data, I have many many names and 
> replacing each
> value by hand would take too long. Any help is highly appreciated.
>
> Thank you.
>
> Regards,
> Jens
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Thu Jul 28 16:13:00 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Jul 2005 10:13:00 -0400
Subject: [R] conversion from SAS
In-Reply-To: <20050728121131.55390.qmail@web26610.mail.ukl.yahoo.com>
References: <20050728121131.55390.qmail@web26610.mail.ukl.yahoo.com>
Message-ID: <42E8E7EC.3010805@vanderbilt.edu>

alessandro carletti wrote:
> Hi, I wonder if anybody could help me in converting
> this easy SAS program into R.
> (I'm still trying to do that!)

Converting that program into R will be very feasible and the solution 
will be far more elegant than SAS.  But I think you are expecting other 
people to do your work.

Frank

> 
> PROC IMPORT OUT= WORK.CHLA_italian 
>             DATAFILE= "C:\Documents and
> Settings\carleal\My
> Documents\REBECCA\stat\sas\All&nutrients.xls" 
>             DBMS=EXCEL2000 REPLACE;
>      GETNAMES=YES;
> RUN;
> data chla_italian;
>    set chla_italian;
>    year=year(datepart(date));
>    month=month(datepart(date));
>    run;
> 
> proc sort data=chla_italian; by station; run;
> /* Check bloom for seasonal cycle outliers */
> data sort_dataset;
>    set chla_italian;
>    chla=chl_a;
>    dayno=date-mdy(1,1,year)+1;
>    cos1=cos(2*3.14*dayno/365);
>    sin1=sin(2*3.14*dayno/365);
>    cos2=cos(4*3.14*dayno/365);
>    sin2=sin(4*3.14*dayno/365);
>    cos3=cos(6*3.14*dayno/365);
>    sin3=sin(6*3.14*dayno/365);
>    cos4=cos(8*3.14*dayno/365);
>    sin4=sin(8*3.14*dayno/365);
>    bloom=0;
>    w_chla=1/chla/chla;
> run;
> ODS listing close;
> %macro sort_event(cut_off,last=0);
> /*proc glm data=sort_dataset;
>    class year;
>    model logchla=year cos1 sin1 cos2 sin2 cos3 sin3
> cos4 sin4 /solution;
>    by station;
>    where bloom=0;
>    output out=chla_res predicted=pred student=studres
> cookd=cookd rstudent=rstudent u95=u95;
>    lsmeans year / at (cos1 sin1 cos2 sin2 cos3 sin3
> cos4 sin4)=(0 0 0 0 0 0 0 0);
>    ODS output ParameterEstimates=parmest
> LSmeans=lsmeans;
> run;*/
> proc glm data=sort_dataset;
>    class year month;
>    model chla=/solution;
>    by station;
>    weight w_chla;
>    where bloom=0;
>    output out=chla_res predicted=pred student=studres
> cookd=cookd 
> daynumber<-data$date-mdy(1,1,y)+1 
> rstudent=rstudent ucl=ucl lcl=lcl / alpha=0.02;
> *   lsmeans year / at (cos1 sin1)=(0 0);
> *   ODS output ParameterEstimates=parmest
> LSmeans=lsmeans;
> run;
> quit;
> data sort_dataset;
>    set chla_res;
>    increase=ucl-pred;
>    if chla>UCL then bloom=1;
>    w_chla=1/(50+5*pred*pred);
>    %if &last=0 %then %do; drop ucl lcl cookd rstudent
> studres pred; %end;
> run;
> %mend sort_event;
> ODS listing;
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0);
> %sort_event(2.0,last=1);
> /* Combine bloom information with all chlorophyll
> values */
> data chla_sep;
>    merge sort_dataset(keep=station date bloom pred ucl
> lcl) chla_italian (rename=(chl_a=chla));
>    by station date;
> *   lcl=exp(lcl);
> *   ucl=exp(ucl);
> *   pred=exp(pred);
>    if bloom=. then bloom=1;
>    if bloom=0 then chla1=chla; else chla1=.;
>    if bloom=1 then chla2=chla; else chla2=.;
> run;
>  
> symbol1 i=none value=plus color=red;
> symbol2 i=none value=plus color=green;
> symbol3 i=join value=none line=1 color=black;
> axis1 logbase=10; axis1;
> proc gplot data=chla_sep;
>    plot chla2*date=1 chla1*date=2 (ucl pred
> lcl)*date=3 /overlay vaxis=axis1;
>    by station;
> run;
> quit;
> proc glm data=chla_sep;
>    class station year month;
>    model salinity temperature Transparency__m_
> Nitrate__mmol_l_1_ Phosphate__mmol_l_1_
> Silicate__mmol_l_1_=bloom month/solution;
>    by station;
> run;
> quit;
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ligges at statistik.uni-dortmund.de  Thu Jul 28 16:16:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 28 Jul 2005 16:16:10 +0200
Subject: [R] catching errors in a loop
In-Reply-To: <6.2.0.14.2.20050728152314.01ee3750@pop.uio.no>
References: <6.2.0.14.2.20050728152314.01ee3750@pop.uio.no>
Message-ID: <42E8E8AA.7050508@statistik.uni-dortmund.de>

Anders Bjrgester wrote:
> Hello
> 
> I can't figure out how to handle errors in R. I have a loop, e.g.
> 
> for (i in 2:n) {
> .
> fit <- nls(model), start=list
> if any type of error occur write i to a text file
> .
> }
> 
> I putted try around the nls-expression and this let me run through the 
> loop without R stopping (which I want because each loop takes some time so 
> I do not want it to stop), but I also want to capture the variable when an 
> error occur.

Right idea:

   fit <-  try(nls(model, .....))
   if(inherits(fit, "try-error"))
	write(i, file="hello.txt")

Uwe Ligges





> 
> Appreciate any help
> 
> /Anders
> - - - - - - - - - - - - - - - - - - -
> I tried to use:
> **options(error=write(variable.names(matrix[i]), 
> file="..\\error.txt",append = TRUE)), hoping this made R write to the text 
> file every time an error occurred (but this made R write all is in the 
> loop to the text file).
> **tryCatch(<- nls(model), start=list), finally =write() also writes to a 
> text file but not necessary when there is an error.
> **if (Parameter>x errorM=9 else errorM =0 works but I want to capture any 
> type of error.
> - - - - - - - - - - - - - - - - -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Jul 28 16:24:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 28 Jul 2005 10:24:24 -0400
Subject: [R] Cochran-Armitage-trend-test
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EB28@usctmx1106.Merck.com>

Simply do RSiteSearch("Armitage") would have given you:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20396.html

Andy

> From: amelie2000 at gmx.de
> 
> Hi!
> 
> I am searching for the Cochran-Armitage-trend-test. Is it 
> included in an
> R-package? 
> 
> Thank you!
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From macq at llnl.gov  Thu Jul 28 16:25:29 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 28 Jul 2005 07:25:29 -0700
Subject: [R] CSV file and date. Dates are read as factors!
In-Reply-To: <s2e7fca5.036@grecc.umaryland.edu>
References: <s2e7fca5.036@grecc.umaryland.edu>
Message-ID: <p06210202bf0e97b5b515@[128.115.153.6]>

It's really pretty simple.

First, if you supply as.is=TRUE to read.csv() [or read.table()] then 
your dates will be read as character strings, not factors. That saves 
the step of converting them from factor to character.

Then, use as.Date() to convert the date columns to objects of class 
"Date". You will have to specify the format, if your dates are not in 
the default format.

>  tmp <- as.Date('2002-5-1')
>  as.Date(Sys.time())-tmp
Time difference of 1184 days

If your dates include times, then use as.POSIXct() instead of as.Date().

>  tmp <- as.POSIXct('2002-5-1 13:21')
>  Sys.time()-tmp
Time difference of 1183.746 days

If you don't want to use as.is, perhaps because you have other 
columns that you *want* to have as factors, then either supply 
colClasses to read.csv, or else just use format() to convert the 
factors to character.

as.Date(format(your_date_column))

As an aside, you might save yourself some time by using read.xls() 
from the gdata package.

And of course, there's always the ugly work-around. In your Excel, 
create new columns in which the dates are formatted as numbers, 
presumably as the number of days since whatever Excel uses for its 
origin. Then, in R, you can simply subtract the numbers. If you have 
date-time values in Excel, this might be a little trickier.

-Don

At 9:28 PM -0400 7/27/05, John Sorkin wrote:
>I am using read.csv to read a CSV file (produced by saving an Excel file
>as a CSV file). The columns containing dates are being read as factors.
>Because of this, I can not compute follow-up time, i.e.
>Followup<-postDate-preDate. I would appreciate any suggestion that would
>help me read the dates as dates and thus allow me to calculate follow-up
>time.
>Thanks
>John
>
>John Sorkin M.D., Ph.D.
>Chief, Biostatistics and Informatics
>Baltimore VA Medical Center GRECC and
>University of Maryland School of Medicine Claude Pepper OAIC
>
>University of Maryland School of Medicine
>Division of Gerontology
>Baltimore VA Medical Center
>10 North Greene Street
>GRECC (BT/18/GR)
>Baltimore, MD 21201-1524
>
>410-605-7119
>-- NOTE NEW EMAIL ADDRESS:
>jsorkin at grecc.umaryland.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From vito_ricci at yahoo.com  Thu Jul 28 16:29:56 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 28 Jul 2005 16:29:56 +0200 (CEST)
Subject: [R] Cochran-Armitage-trend-test
Message-ID: <20050728142956.12389.qmail@web41210.mail.yahoo.com>

Hi,
see:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20396.html

Regards,
Vito



amelie2000 at gmx.de wrote:

Hi!

I am searching for the Cochran-Armitage-trend-test. Is
it included in an
R-package? 

Thank you!



Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From vito_ricci at yahoo.com  Thu Jul 28 16:41:02 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 28 Jul 2005 16:41:02 +0200 (CEST)
Subject: [R] stl()
Message-ID: <20050728144103.53888.qmail@web41211.mail.yahoo.com>

Hi,

maybe residuals are autocorreleted, you could you use
ARIMA models. See arima() in R to fit an ARIMA model.
Regards,
Vito

Sebastian.Leuzinger <at> unibas.ch> wrote:

Hello, anyone got an idea on how to use stl() so that
the remainder eventually 
becomes white noise? i used stl repeatedly but there
is autocorrelation in 
the remainder that i can't get rid of. 
os: linux suse9.3
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger <at> unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger


Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From p.dalgaard at biostat.ku.dk  Thu Jul 28 17:15:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jul 2005 17:15:36 +0200
Subject: [R] CSV file and date. Dates are read as factors!
In-Reply-To: <p06210202bf0e97b5b515@[128.115.153.6]>
References: <s2e7fca5.036@grecc.umaryland.edu>
	<p06210202bf0e97b5b515@[128.115.153.6]>
Message-ID: <x2br4notbb.fsf@turmalin.kubism.ku.dk>

Don MacQueen <macq at llnl.gov> writes:

> It's really pretty simple.
> 
> First, if you supply as.is=TRUE to read.csv() [or read.table()] then 
> your dates will be read as character strings, not factors. That saves 
> the step of converting them from factor to character.
> 
> Then, use as.Date() to convert the date columns to objects of class 
> "Date". You will have to specify the format, if your dates are not in 
> the default format.
> 
> >  tmp <- as.Date('2002-5-1')
> >  as.Date(Sys.time())-tmp
> Time difference of 1184 days
> 
> If your dates include times, then use as.POSIXct() instead of as.Date().
> 
> >  tmp <- as.POSIXct('2002-5-1 13:21')
> >  Sys.time()-tmp
> Time difference of 1183.746 days
> 
> If you don't want to use as.is, perhaps because you have other 
> columns that you *want* to have as factors, then either supply 
> colClasses to read.csv, or else just use format() to convert the 
> factors to character.
> 
> as.Date(format(your_date_column))

 Actually, you can forget about the as.is stuff from 2.1.1 onwards
since as.Date works happily with factors:

> as.Date.factor
function (x, ...)
as.Date(as.character(x), ...)

(previous versions forgot to pass the ... arguments so it only worked
there if the standard format was used.) I suspect that as.character()
is preferable to format() - there could be issues with padding.

However, you can apply as.is selectively on columns: It can be a
logical vector or a vector of indices (numeric or character).  
 
> As an aside, you might save yourself some time by using read.xls() 
> from the gdata package.
> 
> And of course, there's always the ugly work-around. In your Excel, 
> create new columns in which the dates are formatted as numbers, 
> presumably as the number of days since whatever Excel uses for its 
> origin. Then, in R, you can simply subtract the numbers. If you have 
> date-time values in Excel, this might be a little trickier.
> 
> -Don
> 
> At 9:28 PM -0400 7/27/05, John Sorkin wrote:
> >I am using read.csv to read a CSV file (produced by saving an Excel file
> >as a CSV file). The columns containing dates are being read as factors.
> >Because of this, I can not compute follow-up time, i.e.
> >Followup<-postDate-preDate. I would appreciate any suggestion that would
> >help me read the dates as dates and thus allow me to calculate follow-up
> >time.
> >Thanks
> >John
> >
> >John Sorkin M.D., Ph.D.
> >Chief, Biostatistics and Informatics
> >Baltimore VA Medical Center GRECC and
> >University of Maryland School of Medicine Claude Pepper OAIC
> >
> >University of Maryland School of Medicine
> >Division of Gerontology
> >Baltimore VA Medical Center
> >10 North Greene Street
> >GRECC (BT/18/GR)
> >Baltimore, MD 21201-1524
> >
> >410-605-7119
> >--- NOTE NEW EMAIL ADDRESS:
> >jsorkin at grecc.umaryland.edu
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From lecoutre at stat.ucl.ac.be  Thu Jul 28 17:11:09 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 28 Jul 2005 17:11:09 +0200
Subject: [R] Cochran-Armitage-trend-test
In-Reply-To: <20050728142956.12389.qmail@web41210.mail.yahoo.com>
Message-ID: <038b01c59386$95d37090$6e8b6882@didacdom.stat.ucl.ac.be>

Hi there,

I often do receive some mails about this piece of code regarding
Cochran-Armitage or Mantel Chi square.
The archived mail does unfortunately lack some pieces of code (function
"scores").
I copy there all my raw code that I did implement to mimic SAS PROC FREQ
statistics regarding the analysis of contingency tables. Whoever is
interested to take it and rework it a little bit (for example redefining
outputs so that they suits a htest object) is welcome.

Best wishes,

Eric


-----



# R functions to provides statistics on contingency tables
# Mimics SAS PROC FREQ outputs
# Implementation is the one described in SAS PROC FREQ manual

# Eric Lecoutre <ericlecoutre at gmail.com

# Feel free to use / modify / document / add to a package



#------------------------------------ UTILITARY FUNCTIONS
------------------------------------#


print.ordtest=function(l,...)
{
 tmp=matrix(c(l$estimate,l$ASE),nrow=1)
 dimnames(tmp)=list(l$name,c("Estimate","ASE"))
 print(round(tmp,4),...)
}


compADPQ=function(x)
{
	nr=nrow(x)
	nc=ncol(x)
	Aij=matrix(0,nrow=nr,ncol=nc)
	Dij=matrix(0,nrow=nr,ncol=nc)
	for (i in 1:nr)	{
		for (j in 1:nc)	{
	
Aij[i,j]=sum(x[1:i,1:j])+sum(x[i:nr,j:nc])-sum(x[i,])-sum(x[,j])
	
Dij[i,j]=sum(x[i:nr,1:j])+sum(x[1:i,j:nc])-sum(x[i,])-sum(x[,j])
	}}
	P=sum(x*Aij)
	Q=sum(x*Dij)
	return(list(Aij=Aij,Dij=Dij,P=P,Q=Q))
}


scores=function(x,MARGIN=1,method="table",...)
{
	# MARGIN
	#	1 - row
	# 	2 - columns

	# Methods for ranks are
	#
	# x - default
	# rank
	# ridit
	# modridit
	
	if (method=="table")
	{
		if (is.null(dimnames(x))) return(1:(dim(x)[MARGIN]))
		else {
			options(warn=-1)
			if
(sum(is.na(as.numeric(dimnames(x)[[MARGIN]])))>0)
			{
				out=(1:(dim(x)[MARGIN]))
			}
			else
			{
			out=(as.numeric(dimnames(x)[[MARGIN]]))
			}
			options(warn=0)
		}
	}
	else	{
	### method is a rank one
	Ndim=dim(x)[MARGIN]
	OTHERMARGIN=3-MARGIN
	
ranks=c(0,(cumsum(apply(x,MARGIN,sum))))[1:Ndim]+(apply(x,MARGIN,sum)+1)
/2
	if (method=="ranks") out=ranks
	if (method=="ridit") out=ranks/(sum(x))
	if (method=="modridit") out=ranks/(sum(x)+1)
	}
	
	return(out)
}


#------------------------------------ FUNCTIONS
------------------------------------#

tablegamma=function(x)
{
# Statistic
	tmp=compADPQ(x)
	P=tmp$P
	Q=tmp$Q
	gamma=(P-Q)/(P+Q)
# ASE
	Aij=tmp$Aij
	Dij=tmp$Dij
	tmp1=4/(P+Q)^2
	tmp2=sqrt(sum((Q*Aij - P*Dij)^2 * x))
	gamma.ASE=tmp1*tmp2
# Test	
	var0=(4/(P+Q)^2) * (sum(x*(Aij-Dij)^2) - ((P-Q)^2)/sum(x))
	tb=gamma/sqrt(var0)
	p.value=2*(1-pnorm(tb))
# Output
	
out=list(estimate=gamma,ASE=gamma.ASE,statistic=tb,p.value=p.value,name=
"Gamma",bornes=c(-1,1))
	class(out)="ordtest"
	return(out)
}


tabletauc=function(x)
{
	tmp=compADPQ(x)
	P=tmp$P
	Q=tmp$Q
	m=min(dim(x))
	n=sum(x)
# statistic
	
	tauc=(m*(P-Q))/(n^2*(m-1))
# ASE	
	Aij=tmp$Aij
	Dij=tmp$Dij
	dij=Aij-Dij
	tmp1=2*m/((m-1)*n^2)
	tmp2= sum(x * dij^2) - (P-Q)^2/n
	ASE=tmp1*sqrt(tmp2)
	
# Test	
	tb=tauc/ASE
	p.value=2*(1-pnorm(tb))
# Output
	
out=list(estimate=tauc,ASE=ASE,statistic=tb,p.value=p.value,name="Kendal
l's tau-c",bornes=c(-1,1))
	class(out)="ordtest"
	return(out)
}

tabletaub=function(x)
{
# Statistic
	tmp=compADPQ(x)
	P=tmp$P
	Q=tmp$Q
	n=sum(x)
	wr=n^2 - sum(apply(x,1,sum)^2)
	wc=n^2 - sum(apply(x,2,sum)^2)
	taub=(P-Q)/sqrt(wr*wc)	
# ASE
	Aij=tmp$Aij
	Dij=tmp$Dij
	w=sqrt(wr*wc)
	dij=Aij-Dij
	nidot=apply(x,1,sum)
	ndotj=apply(x,2,sum)
	n=sum(x)
	vij=outer(nidot,ndotj, FUN=function(a,b) return(a*wc+b*wr))
	tmp1=1/(w^2)
	tmp2= sum(x*(2*w*dij + taub*vij)^2)
	tmp3=n^3*taub^2*(wr+wc)^2
	tmp4=sqrt(tmp2-tmp3)
	taub.ASE=tmp1*tmp4
# Test
	var0=4/(wr*wc) * (sum(x*(Aij-Dij)^2) - (P-Q)^2/n)
	tb=taub/sqrt(var0)
	p.value=2*(1-pnorm(tb))
# Output	
	
out=list(estimate=taub,ASE=taub.ASE,statistic=tb,p.value=p.value,name="K
endall's tau-b",bornes=c(-1,1))
	class(out="ordtest")
	return(out)
}

tablesomersD=function(x,dep=2)
{
	# dep: which dimension stands for the dependant variable
	# 1 - ROWS
	# 2 - COLS
# Statistic
	if (dep==1) x=t(x)
	tmp=compADPQ(x)
	P=tmp$P
	Q=tmp$Q
	n=sum(x)
	wr=n^2 - sum(apply(x,1,sum)^2)
	somers=(P-Q)/wr
# ASE
	Aij=tmp$Aij
	Dij=tmp$Dij
	dij=Aij-Dij
	tmp1=2/wr^2
	tmp2=sum(x*(wr*dij - (P-Q)*(n-apply(x,1,sum)))^2)
	ASE=tmp1*sqrt(tmp2)
# Test
	var0=4/(wr^2) * (sum(x*(Aij-Dij)^2) - (P-Q)^2/n)
	tb=somers/sqrt(var0)
	p.value=2*(1-pnorm(tb))
# Output
	if (dep==1) dir="R|C" else dir= "C|R"
	name=paste("Somer's D",dir)
	
out=list(estimate=somers,ASE=ASE,statistic=tb,p.value=p.value,name=name,
bornes=c(-1,1))
	class(out)="ordtest"
	return(out)

	
}

#out=table.somersD(data)

 

tablepearson=function(x,scores.type="table")
{

# Statistic
	sR=scores(x,1,scores.type)
	sC=scores(x,2,scores.type)
	n=sum(x)
	Rbar=sum(apply(x,1,sum)*sR)/n
	Cbar=sum(apply(x,2,sum)*sC)/n
	ssr=sum(x*(sR-Rbar)^2)
	ssc=sum(t(x)* (sC-Cbar)^2)
	tmpij=outer(sR,sC,FUN=function(a,b) return((a-Rbar)*(b-Cbar)))
	ssrc= sum(x*tmpij)
	v=ssrc
	w=sqrt(ssr*ssc)
	r=v/w
# ASE
	bij=outer(sR,sC, FUN=function(a,b)return((a-Rbar)^2*ssc +
(b-Cbar)^2*ssr))	
	tmp1=1/w^2
	tmp2=x*(w*tmpij - (bij*v)/(2*w))^2
	tmp3=sum(tmp2)
	ASE=tmp1*sqrt(tmp3)
# Test
	var0= (sum(x*tmpij) - (ssrc^2/n))/ (ssr*ssc)
	tb=r/sqrt(var0)
	p.value=2*(1-pnorm(tb))
# Output
	
out=list(estimate=r,ASE=ASE,statistic=tb,p.value=p.value,name="Pearson
Correlation",bornes=c(-1,1))
	class(out)="ordtest"
	return(out)
}

# table.pearson(data)


tablespearman=function(x)
{
	# Details algorithme manuel SAS PROC FREQ page 540
# Statistic
	n=sum(x)
	nr=nrow(x)
	nc=ncol(x)
	tmpd=cbind(expand.grid(1:nr,1:nc))
	ind=rep(1:(nr*nc),as.vector(x))
	tmp=tmpd[ind,]
	rhos=cor(apply(tmp,2,rank))[1,2]
# ASE
	Ri=scores(x,1,"ranks")- n/2
	Ci=scores(x,2,"ranks")- n/2
	sr=apply(x,1,sum)
	sc=apply(x,2,sum)
	F=n^3 - sum(sr^3)
	G=n^3 - sum(sc^3)
	w=(1/12)*sqrt(F*G)
	vij=data
	for (i in 1:nrow(x))
	{
		qi=0
		if (i<nrow(x))
		{
		for (k in i:nrow(x)) qi=qi+sum(x[k,]*Ci)
		}
	}
	for (j in 1:ncol(x))
		{
			qj=0
			if (j<ncol(x))
			{
			for (k in j:ncol(x)) qj=qj+sum(x[,k]*Ri)
			}
		vij[i,j]=n*(Ri[i]*Ci[j] +
0.5*sum(x[i,]*Ci)+0.5*sum(data[,j]*Ri) +qi+qj)
		}


	v=sum(data*outer(Ri,Ci))
	wij=-n/(96*w)*outer(sr,sc,FUN=function(a,b) return(a^2*G+b^2*F))
	zij=w*vij-v*wij
	zbar=sum(data*zij)/n
	vard=(1/(n^2*w^4))*sum(x*(zij-zbar)^2)
	ASE=sqrt(vard)		
# Test
	vbar=sum(x*vij)/n
	p1=sum(x*(vij-vbar)^2)
	p2=n^2*w^2
	var0=p1/p2
	stat=rhos/sqrt(var0)

# Output
	out=list(estimate=rhos,ASE=ASE,name="Spearman
correlation",bornes=c(-1,1))
	class(out)="ordtest"
	return(out)
}

#tablespearman(data)



 
tablelambdasym=function(x)
{
# Statistic 
	ri = apply(x,1,max)
	r=max(apply(x,2,sum))
	n=sum(x)
	cj=apply(x,2,max)
	c=max(apply(x,1,sum))
	sri=sum(ri)
	w=2*n - r -c
	v=2*n - sri - sum(cj)
	lambda=(w-v)/w
# ASE ...

	tmpSi=0
	l=min(which(apply(x,2,sum)==r))
	for (i in 1:length(ri))
	{
		li=min(which(x[i,]==ri[i]))
		if (li==l) tmpSi=tmpSi+x[i,li]
	}
	
	tmpSj=0
	k=min(which(apply(x,1,sum)==c))
	for (j in 1:length(cj))
	{
		kj=min(which(x[,j]==cj[j]))
		if (kj==k) tmpSj=tmpSj+x[kj,j]
	}

	rk=max(x[k,])
	cl=max(x[,l])
	tmpx=tmpSi+tmpSj+rk+cl
	y=8*n-w-v-2*tmpx
	
	nkl=x[k,l]
	tmpSij=0
	for (i in 1:nrow(x))
	{
		for (j in 1:ncol(x))
		{
		li=min(which(x[i,]==ri[i]))
		kj=min(which(x[,j]==cj[j]))
		tmpSij=tmpSij+x[kj,li]
		}
	}
	
	
	ASE=(1/(w^2))*sqrt(w*v*y-(2*(w^2)*(n-tmpSij))-2*(v^2)*(n-nkl))
# Output	
	
	out=list(estimate=lambda,ASE=ASE,name="Lambda
Symetric",bornes=c(0,1))
	class(out)="ordtest"
	return(out)

}
#tablelambdasym(data)

tablelambdaasym=function(x,transpose=FALSE)
{
# Statistic
	if (transpose==TRUE) x=t(x)
	ri = apply(x,1,max)
	r=max(apply(x,2,sum))
	sri=sum(ri)
	n=sum(x)
	lambda=(sum(ri)-r)/(n-r)
# ASE	
	l=min(which(apply(x,2,sum)==r))
	tmp=0
	for (i in 1:length(ri))
	{
		li=min(which(x[i,]==ri[i]))
		if (li==l) tmp=tmp+x[i,li]
	}
	ASE=sqrt(((n-sri)/(n-r)^3) *(sri+r-2*tmp))
# Output
	if (transpose) dir="R|C" else dir= "C|R"
	name=paste("Lambda asymetric",dir)
	out=list(estimate=lambda,ASE=ASE,name=name,bornes=c(0,1))
	class(out)="ordtest"
	return(out)
}




tableUCA=function(x,transpose=TRUE)
{
	if (transpose==TRUE) x=t(x)
	# Statistic
		n=sum(x)
		ni=apply(x,1,sum)
		nj=apply(x,2,sum)
		Hx=-sum((ni/n)*log(ni/n))
		Hy=-sum((nj/n)*log(nj/n))
		Hxy=-sum((x/n)*log(x/n))
		v=Hx+Hy- Hxy
		w=Hy
		U=v/w
	# ASE
		tmp1=1/((n)*(w^2))
		tmpij=0
		for (i in 1:nrow(x))
		{
			for (j in 1:ncol(x))
			{
			tmpij=tmpij+(  x[i,j]*  (
Hy*log(x[i,j]/ni[i])+(Hx-Hxy)*    log(nj[j]/n)               )^2     )
			}
		}
		ASE=tmp1*sqrt(tmpij)
	# Output
		if (transpose) dir="R|C" else dir= "C|R"
		name=paste("Uncertainty Coefficient",dir)
		out=list(estimate=U,ASE=ASE,name=name,bornes=c(0,1))
		class(out)="ordtest"
		return(out)
}

#tableUCA(data)

tableUCS=function(x)
{

	# Statistic
		n=sum(x)
		ni=apply(x,1,sum)
		nj=apply(x,2,sum)
		Hx=-sum((ni/n)*log(ni/n))
		Hy=-sum((nj/n)*log(nj/n))
		Hxy=-sum((x/n)*log(x/n))
		U=(2*(Hx+Hy-Hxy))/(Hx+Hy)
	# ASE
		tmpij=0
		for (i in 1:nrow(x))
		{
			for (j in 1:ncol(x))
			{
			tmpij=tmpij+(  x[i,j]*
(Hxy*log(ni[i]*nj[j]/n^2) - (Hx+Hy)*log(x[i,j]/n))^2   /(n^2*(Hx+Hy)^4)
)
			}
		}
		ASE=2*sqrt(tmpij)
	# Output
		name="Uncertainty Coefficient Symetric"
		out=list(estimate=U,ASE=ASE,name=name,bornes=c(0,1))
		class(out)="ordtest"
		return(out)
}





tablelinear=function(x,scores.type="table")
{
	r=tablepearson(x,scores.type)$estimate
	n=sum(x)
	ll=r^2*(n-1)
	out=list(estimate=ll)
	return(out)
}

tablephi=function(x)
{
	if (all.equal(dim(x),c(2,2))==TRUE)
	{
	rtot=apply(x,1,sum)
	ctot=apply(x,2,sum)
	phi= det(x)/sqrt(prod(rtot)*prod(ctot))
	}
	else {
	Qp=chisq.test(x)$statistic
	phi=sqrt(Qp/sum(x))
	}
	names(phi)="phi"
	return(phi=phi)
}


tableCramerV=function(x)
{
	if (all.equal(dim(x),c(2,2))==TRUE)
	{
	cramerV=tablephi(x)
	}
	else
	{
	Qp=tableChisq(x)$estimate
	cramerV=sqrt((Qp/n)/min(dim(x)-1))
	}
	names(cramerV)="Cramer's V"
	return(cramerV)
}


tableChisq=function(x)
{
	nidot=apply(x,1,sum)
	ndotj=apply(x,2,sum)
	n=sum(nidot)
	eij=outer(nidot,ndotj,"*")/n
	R=length(nidot)
	C=length(ndotj)
	dll=(R-1)*(C-1)
	Qp=sum((x-eij)^2/eij)
	p.value=1-pchisq(Qp,dll)
	
out=list(estimate=Qp,dll=dll,p.value=p.value,dim=c(R,C),name="Pearson's
Chi-square")
	return(out)
}


tableChisqLR=function(x)
{
# Likelihood ratio Chi-squared test
	nidot=apply(x,1,sum)
	ndotj=apply(x,2,sum)
	n=sum(nidot)
	eij=outer(nidot,ndotj,"*")/n
	R=length(nidot)
	C=length(ndotj)
	dll=(R-1)*(C-1)
	G2=2*sum(x*log(x/eij))
	p.value=1-pchisq(G2,dll)
	
out=list(estimate=G2,dll=dll,p.value=p.value,dim=c(R,C),name="Likelihood
ratio Chi-square")
	return(out)
}

tableChisqCA=function(x)
{
	if (all.equal(dim(x),c(2,2))==TRUE)
	{
		nidot=apply(x,1,sum)
		ndotj=apply(x,2,sum)
		n=sum(nidot)
		eij=outer(nidot,ndotj,"*")/n
		R=length(nidot)
		C=length(ndotj)
		dll=(R-1)*(C-1)
		tmp=as.vector(abs(x-eij))
		tmp=pmax(tmp-0.5,0)
		tmp=matrix(tmp,byrow=TRUE,ncol=C)
		Qc=sum(tmp^2/eij)
		p.value=1-pchisq(Qc,dll)
	
out=list(estimate=Qc,dll=dll,p.value=p.value,dim=c(R,C),name="Continuity
adjusted Chi-square")
		return(out)
	}
	else
	{ stop("Continuity-adjusted chi-square must be used with
(2,2)-tables",call.=FALSE) }
}

tableChisqMH=function(x)
{
	n=sum(x)
	G2=(n-1)*(tablepearson(x)$estimate^2)
	dll=1
	p.value=1-pchisq(G2,dll)
	
out=list(estimate=G2,dll=dll,p.value=p.value,dim=dim(x),name="Mantel-Hae
nszel Chi-square")
	return(out)

}

tableCC=function(x)
{
	Qp=tableChisq(x)$estimate
	n=sum(x)
	P=sqrt(Qp/(Qp+n))
	m=min(dim(x))
	
out=list(estimate=P,dim=dim(x),bornes=c(0,sqrt((m-1)/m)),name="Contingen
cy coefficient")
	return(out)
	
}

tabletrend=function(x,transpose=FALSE)
{
	if (any(dim(x)==2))
	{
	if (transpose==TRUE) {
	x=t(x)
	}
	
	if (dim(x)[2]!=2){stop("Cochran-Armitage test for trend must be
used with a (R,2) table. Use transpose argument",call.=FALSE) }
	
	nidot=apply(x,1,sum)
	n=sum(nidot)

	Ri=scores(x,1,"table")
	Rbar=sum(nidot*Ri)/n
	
	s2=sum(nidot*(Ri-Rbar)^2)
	pdot1=sum(x[,1])/n
	T=sum(x[,1]*(Ri-Rbar))/sqrt(pdot1*(1-pdot1)*s2)
	p.value.uni=1-pnorm(abs(T))
	p.value.bi=2*p.value.uni
	
out=list(estimate=T,dim=dim(x),p.value.uni=p.value.uni,p.value.bi=p.valu
e.bi,name="Cochran-Armitage test for trend")
	return(out)
	
	}
	else {stop("Cochran-Armitage test for trend must be used with a
(2,C) or a (R,2) table",call.=FALSE) }
}











Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
> Sent: jeudi 28 juillet 2005 16:30
> To: r-help at stat.math.ethz.ch
> Cc: amelie2000 at gmx.de
> Subject: Re: [R] Cochran-Armitage-trend-test
> 
> 
> Hi,
> see:
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20396.html
> 
> Regards,
> Vito
> 
> 
> 
> amelie2000 at gmx.de wrote:
> 
> Hi!
> 
> I am searching for the Cochran-Armitage-trend-test. Is
> it included in an
> R-package? 
> 
> Thank you!
> 
> 
> 
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> "Statistical thinking will one day be as necessary for 
> efficient citizenship as the ability to read and write" H. G. Wells
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese  
> http://www.modugno.it/archivio/palesesanto_spirito/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From r.shengzhe at gmail.com  Thu Jul 28 18:01:07 2005
From: r.shengzhe at gmail.com (Shengzhe Wu)
Date: Thu, 28 Jul 2005 18:01:07 +0200
Subject: [R] Help: how to specify the current active window by mouse
In-Reply-To: <42E8C84B.30009@stats.uwo.ca>
References: <ea57975b05072804267d91afd6@mail.gmail.com>
	<42E8C84B.30009@stats.uwo.ca>
Message-ID: <ea57975b050728090136e79194@mail.gmail.com>

Hi Duncan,

Thanks for your reply. I will try to use tcltk to do that.

Sincerely,
Shengzhe



From dataanalytics at earthlink.net  Thu Jul 28 18:08:00 2005
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Thu, 28 Jul 2005 16:08:00 +0000
Subject: [R] Running Internet Explorer from Withing R
Message-ID: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>

Good morning,

Is it possible to open an html file using IE but from within R?  I wrote a small function to generate tables in html but I'd like to write another function to call IE and open the html file.

Thanks,

Walt Paczkowski

________________________

Walter R. Paczkowski, Ph.D.
Data Analytics Corp.
44 Hamilton Lane
Plainsboro, NJ  08536
(V) 609-936-8999
(F) 609-936-3733



From p.dalgaard at biostat.ku.dk  Thu Jul 28 18:19:11 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jul 2005 18:19:11 +0200
Subject: [R] Running Internet Explorer from Withing R
In-Reply-To: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>
References: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>
Message-ID: <x27jfaq4xs.fsf@turmalin.kubism.ku.dk>

"Walter R. Paczkowski" <dataanalytics at earthlink.net> writes:

> Good morning,
> 
> Is it possible to open an html file using IE but from within R? I
> wrote a small function to generate tables in html but I'd like to
> write another function to call IE and open the html file.

browseURL() (if it exists on Windows)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tom at maladmin.com  Thu Jul 28 14:23:21 2005
From: tom at maladmin.com (tom wright)
Date: Thu, 28 Jul 2005 08:23:21 -0400
Subject: [R] problem with an IF statement?
Message-ID: <1122553401.4111.13.camel@localhost.localdomain>

Can somebody please take a look at this and tell me whats going wrong?
It seems to be parsing wronly around the 'if' statement and gives me a
directory listing.
Thanks in advance
Tom
N.B. datan is an invented dataset


xvals<-c(1,0.4,0.2)
datan<-data.frame(s1=c(3,4,5),s2=c(5,5,5),s3=c(21,55,34),s4=c(5,3,2))

datan$sint<-NA
datan$sgrad<-NA
for(icount in 1:dim(datan)[1]) {
	yvals<-c(datan[icount,4],datan[icount,3],datan[icount,2])
	if((is.na(yvals[1]) + is.na(yvals[2]) + is.na(yvals[3]))<2) {
		g<-lm(yvals~xvals)
		datan$sint[icount]<-g$coef[1]
		datan$sgrad[icount]<-g$coef[2]
	}
}



From 0034058 at fudan.edu.cn  Thu Jul 28 18:23:18 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 29 Jul 2005 00:23:18 +0800
Subject: [R] Running Internet Explorer from Withing R
Message-ID: <0IKC009SSIF5FY@mail.fudan.edu.cn>

see ?browseURL 

	

======= 2005-07-29 00:08:00 =======

>Good morning,
>
>Is it possible to open an html file using IE but from within R?  I wrote a small function to generate tables in html but I'd like to write another function to call IE and open the html file.
>
>Thanks,
>
>Walt Paczkowski
>
>________________________
>
>Walter R. Paczkowski, Ph.D.
>Data Analytics Corp.
>44 Hamilton Lane
>Plainsboro, NJ  08536
>(V) 609-936-8999
>(F) 609-936-3733
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-07-29

------
Deparment of Sociology
Fudan University

Blog:http://sociology.yculblog.com



From dr.mike at ntlworld.com  Thu Jul 28 18:29:21 2005
From: dr.mike at ntlworld.com (Mike Waters)
Date: Thu, 28 Jul 2005 17:29:21 +0100
Subject: [R] Running Internet Explorer from Withing R
In-Reply-To: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>
Message-ID: <20050728162930.LJQZ9998.aamta12-winn.ispmail.ntl.com@d600>

Walt,

As Peter said - browsURL(), which does work on Windows (well XP SP2 for
definite).

For example, to open a google search window:

browseURL("http://www.google.co.uk")

Regards,

Mike

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Walter 
> R. Paczkowski
> Sent: 28 July 2005 17:08
> To: r-help at stat.math.ethz.ch
> Subject: [R] Running Internet Explorer from Withing R
> 
> Good morning,
> 
> Is it possible to open an html file using IE but from within 
> R?  I wrote a small function to generate tables in html but 
> I'd like to write another function to call IE and open the html file.
> 
> Thanks,
> 
> Walt Paczkowski
> 
> ________________________
> 
> Walter R. Paczkowski, Ph.D.
> Data Analytics Corp.
> 44 Hamilton Lane
> Plainsboro, NJ  08536
> (V) 609-936-8999
> (F) 609-936-3733
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Thu Jul 28 18:30:22 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 28 Jul 2005 09:30:22 -0700 (PDT)
Subject: [R] problem with an IF statement?
In-Reply-To: <1122553401.4111.13.camel@localhost.localdomain>
References: <1122553401.4111.13.camel@localhost.localdomain>
Message-ID: <Pine.A41.4.61b.0507280925440.158358@homer04.u.washington.edu>

On Thu, 28 Jul 2005, tom wright wrote:

> Can somebody please take a look at this and tell me whats going wrong?
> It seems to be parsing wronly around the 'if' statement and gives me a
> directory listing.

I think the code that you posted is not quite the code you were 
using.

The problem is that your code had tab characters in it.  The readline 
support in R interprets a tab character as a request to complete a file 
name.  If it can't do that unambiguously it will show the directory 
listing in response to a second tab.

Your email program has turned the tabs into spaces, which fixes the 
problem.  Your text editor probably has a way to do this.

Incidentally, this happens only on certain platforms, which is why the 
posting guide suggests you should say what system you are using.

 	-thomas


> Thanks in advance
> Tom
> N.B. datan is an invented dataset
>
>
> xvals<-c(1,0.4,0.2)
> datan<-data.frame(s1=c(3,4,5),s2=c(5,5,5),s3=c(21,55,34),s4=c(5,3,2))
>
> datan$sint<-NA
> datan$sgrad<-NA
> for(icount in 1:dim(datan)[1]) {
> 	yvals<-c(datan[icount,4],datan[icount,3],datan[icount,2])
> 	if((is.na(yvals[1]) + is.na(yvals[2]) + is.na(yvals[3]))<2) {
> 		g<-lm(yvals~xvals)
> 		datan$sint[icount]<-g$coef[1]
> 		datan$sgrad[icount]<-g$coef[2]
> 	}
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From p.dalgaard at biostat.ku.dk  Thu Jul 28 18:31:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jul 2005 18:31:08 +0200
Subject: [R] problem with an IF statement?
In-Reply-To: <1122553401.4111.13.camel@localhost.localdomain>
References: <1122553401.4111.13.camel@localhost.localdomain>
Message-ID: <x23bpyq4dv.fsf@turmalin.kubism.ku.dk>

tom wright <tom at maladmin.com> writes:

> Can somebody please take a look at this and tell me whats going wrong?
> It seems to be parsing wronly around the 'if' statement and gives me a
> directory listing.
> Thanks in advance
> Tom
> N.B. datan is an invented dataset
> 
> 
> xvals<-c(1,0.4,0.2)
> datan<-data.frame(s1=c(3,4,5),s2=c(5,5,5),s3=c(21,55,34),s4=c(5,3,2))
> 
> datan$sint<-NA
> datan$sgrad<-NA
> for(icount in 1:dim(datan)[1]) {
> 	yvals<-c(datan[icount,4],datan[icount,3],datan[icount,2])
> 	if((is.na(yvals[1]) + is.na(yvals[2]) + is.na(yvals[3]))<2) {
> 		g<-lm(yvals~xvals)
> 		datan$sint[icount]<-g$coef[1]
> 		datan$sgrad[icount]<-g$coef[2]
> 	}
> }


Did you copy+paste it? The TABs will behave as if you pressed the TAB
key... 

        -p

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From andreas.cordes at stud.uni-goettingen.de  Thu Jul 28 18:44:37 2005
From: andreas.cordes at stud.uni-goettingen.de (Andreas Cordes)
Date: Thu, 28 Jul 2005 18:44:37 +0200
Subject: [R] NA handling with lm
Message-ID: <42E90B75.2080105@stud.uni-goettingen.de>

Hi,
I have a problem that is hopefully easily solvable, but I dont find the 
clue in the documentation. I am examining a linear model. One of the 
variables has NA values. Even though na.action=na.omit, i get NA as 
results for this variable. Can I use lm in such a case to get estimates? 
Or do I have to do some form of imputation before doing so?
Here is the call and the results, hope you can help.
Best regards,
Andreas
---------------------------------------------------------------------------------------------
lm(formula = ESSIK ~ ALTER + as.factor(S2) + as.factor(S15A) +
    as.factor(S8) + as.factor(LAND) + as.factor(S18B) + as.factor(BERUF) +
    as.factor(KIRCHE) + as.factor(H_EINKOM) + as.factor(PARTNERS),
    na.action = na.omit)

Residuals:
     Min       1Q   Median       3Q      Max
-17.0675  -2.0151   0.4267   2.7644   9.7333

Coefficients: (2 not defined because of singularities)
                      Estimate Std. Error t value Pr(>|t|)   
(Intercept)          23.755915   1.844110  12.882  < 2e-16 ***
[...]
as.factor(BERUF)7    -1.236836   0.701323  -1.764 0.077942 . 
as.factor(KIRCHE)1   -0.811751   0.237699  -3.415 0.000649 ***
as.factor(H_EINKOM)2        NA         NA      NA       NA   
as.factor(H_EINKOM)3        NA         NA      NA       NA   
as.factor(PARTNERS)1  2.057070   0.342546   6.005 2.23e-09 ***



From Hathaikan.Chootrakool at newcastle.ac.uk  Thu Jul 28 18:45:23 2005
From: Hathaikan.Chootrakool at newcastle.ac.uk (Hathaikan Chootrakool)
Date: Thu, 28 Jul 2005 17:45:23 +0100 (BST)
Subject: [R] matrix form
Message-ID: <4715.128.240.6.80.1122569123.squirrel@128.240.6.80>


I am a new user, i was wondering how to define a collection of data in
matrix form,
this is a part of my data,there are 26 studies, 3 Treatments

   Arm No  Study no.  Treatment  Num(r) Total(n)
1    1        1         1          2    43
2    2        1         2          9    42
3    3        1         3          13   41
4    4        2         1          12   68
5    5        2         2          13   73
6    6        2         3          13   72
7    7        3         1           4   20
8    8        3         3           4   16
9    9        4         1          20   116
10  10        4         3          30   111

I would like to use matrix [study No,Treatment] how can i define code for
using matrix?

has anyone can help me?,thank you very much.

Hathaikan



From p.dalgaard at biostat.ku.dk  Thu Jul 28 18:45:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jul 2005 18:45:39 +0200
Subject: [R] problem with an IF statement?
In-Reply-To: <Pine.A41.4.61b.0507280925440.158358@homer04.u.washington.edu>
References: <1122553401.4111.13.camel@localhost.localdomain>
	<Pine.A41.4.61b.0507280925440.158358@homer04.u.washington.edu>
Message-ID: <x2y87qop58.fsf@turmalin.kubism.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Thu, 28 Jul 2005, tom wright wrote:
> 
> > Can somebody please take a look at this and tell me whats going wrong?
> > It seems to be parsing wronly around the 'if' statement and gives me a
> > directory listing.
> 
> I think the code that you posted is not quite the code you were 
> using.
> 
> The problem is that your code had tab characters in it.  The readline 
> support in R interprets a tab character as a request to complete a file 
> name.  If it can't do that unambiguously it will show the directory 
> listing in response to a second tab.
> 
> Your email program has turned the tabs into spaces, which fixes the 
> problem.  Your text editor probably has a way to do this.

I did see the TABs and they're still present for me in the cited code
below, so I suppose it is Thomas' system which occasionally converts
them to spaces...
 
> Incidentally, this happens only on certain platforms, which is why the 
> posting guide suggests you should say what system you are using.
> 
>  	-thomas
> 
> 
> > Thanks in advance
> > Tom
> > N.B. datan is an invented dataset
> >
> >
> > xvals<-c(1,0.4,0.2)
> > datan<-data.frame(s1=c(3,4,5),s2=c(5,5,5),s3=c(21,55,34),s4=c(5,3,2))
> >
> > datan$sint<-NA
> > datan$sgrad<-NA
> > for(icount in 1:dim(datan)[1]) {
> > 	yvals<-c(datan[icount,4],datan[icount,3],datan[icount,2])
> > 	if((is.na(yvals[1]) + is.na(yvals[2]) + is.na(yvals[3]))<2) {
> > 		g<-lm(yvals~xvals)
> > 		datan$sint[icount]<-g$coef[1]
> > 		datan$sgrad[icount]<-g$coef[2]
> > 	}
> > }
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From petr.pikal at precheza.cz  Thu Jul 28 18:59:58 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 28 Jul 2005 18:59:58 +0200
Subject: [R] NA handling with lm
In-Reply-To: <42E90B75.2080105@stud.uni-goettingen.de>
Message-ID: <42E92B2E.25031.1D2BCBD@localhost>

Hallo

On 28 Jul 2005 at 18:44, Andreas Cordes wrote:

> Hi,
> I have a problem that is hopefully easily solvable, but I dont find
> the clue in the documentation. I am examining a linear model. One of
> the variables has NA values. Even though na.action=na.omit, i get NA
> as results for this variable. Can I use lm in such a case to get
> estimates? Or do I have to do some form of imputation before doing so?
> Here is the call and the results, hope you can help. Best regards,
> Andreas
> ----------------------------------------------------------------------
> ----------------------- lm(formula = ESSIK ~ ALTER + as.factor(S2) +
> as.factor(S15A) +
>     as.factor(S8) + as.factor(LAND) + as.factor(S18B) +
>     as.factor(BERUF) + as.factor(KIRCHE) + as.factor(H_EINKOM) +
>     as.factor(PARTNERS), na.action = na.omit)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -17.0675  -2.0151   0.4267   2.7644   9.7333
> 
> Coefficients: (2 not defined because of singularities)

Problem is not in NA handling but that some of your coeficients 
can be represented as linear combination of other coeficients. You 
have to omit them.

HTH
Petr


>                       Estimate Std. Error t value Pr(>|t|)   
> (Intercept)          23.755915   1.844110  12.882  < 2e-16 ***
> [...]
> as.factor(BERUF)7    -1.236836   0.701323  -1.764 0.077942 . 
> as.factor(KIRCHE)1   -0.811751   0.237699  -3.415 0.000649 ***
> as.factor(H_EINKOM)2        NA         NA      NA       NA   
> as.factor(H_EINKOM)3        NA         NA      NA       NA   
> as.factor(PARTNERS)1  2.057070   0.342546   6.005 2.23e-09 ***
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From r.ramyar at gmail.com  Thu Jul 28 19:03:41 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 18:03:41 +0100
Subject: [R] Forcing coefficents in lm(), recursive residuals, etc.
Message-ID: <5f4f82790507281003589a4b7c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/cfb5cba0/attachment.pl

From r.ramyar at gmail.com  Thu Jul 28 19:16:14 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 18:16:14 +0100
Subject: [R] R Reference Card (especially useful for Newbies)
In-Reply-To: <200507271722.j6RHMqCK027849@faraday.gene.com>
References: <200507271722.j6RHMqCK027849@faraday.gene.com>
Message-ID: <5f4f827905072810163f02dada@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/45f347ee/attachment.pl

From roger.bos at gmail.com  Thu Jul 28 20:01:21 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 28 Jul 2005 14:01:21 -0400
Subject: [R] Running Internet Explorer from Withing R
In-Reply-To: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>
References: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>
Message-ID: <1db72680050728110174405265@mail.gmail.com>

This might be slightly off topic, but Rpad() is a good library that
displays tables in HTML format in a brower very well.  You can also
use it to easily make a gui for your program/code.

HTH,

Roger

On 7/28/05, Walter R. Paczkowski <dataanalytics at earthlink.net> wrote:
> Good morning,
> 
> Is it possible to open an html file using IE but from within R?  I wrote a small function to generate tables in html but I'd like to write another function to call IE and open the html file.
> 
> Thanks,
> 
> Walt Paczkowski
> 
> ________________________
> 
> Walter R. Paczkowski, Ph.D.
> Data Analytics Corp.
> 44 Hamilton Lane
> Plainsboro, NJ  08536
> (V) 609-936-8999
> (F) 609-936-3733
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Thu Jul 28 20:14:11 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 28 Jul 2005 13:14:11 -0500
Subject: [R] lattice/ grid.layout/ multiple graphs per page
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F01D73B9D@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F01D73B9D@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <eb555e66050728111470a5db95@mail.gmail.com>

On 7/27/05, McClatchie, Sam (PIRSA-SARDI)
<mcclatchie.sam at saugov.sa.gov.au> wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.0.0
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
> Colleagues
> 
> I have a set of lattice plots, and want to plot 4 of them on the page.
> I am having trouble with the layout.
> 
> grid.newpage()
> pushViewport(viewport(layout = grid.layout(2,2)))
> pushviewport(viewport(layout.pos.col = 1, layout.pos.row = 1))
> working trellis graph code here
> pushviewport(viewport(layout.pos.col = 1, layout.pos.row = 2))
> working trellis graph code here
> pushviewport(viewport(layout.pos.col = 2, layout.pos.row = 1))
> working trellis graph code here
> pushviewport(viewport(layout.pos.col = 2, layout.pos.row = 2))
> 
> I'm obviously doing something wrong since my graphs still print one per
> page?

Depends on the details of your 'working trellis code'. Are you using
print() explicitly with 'newpage=FALSE'? See ?print.trellis for
details.

Deepayan

> Would you be able to provide advice?
> 
> Thanks
> 
> Sam
> ----
> Sam McClatchie,
> Biological oceanography
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Telephone: (61-8) 8207 5448
> FAX: (61-8) 8207 5481
> Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
> 
>                    /\
>       ...>><xX(??>
>                 //// \\\\
>                    <??)Xx><<
>               /////  \\\\\\
>                         ><(((??>
>   >><(((??>   ...>><xX(??>O<??)Xx><<
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From r.ramyar at gmail.com  Thu Jul 28 20:23:33 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 19:23:33 +0100
Subject: [R] Fwd: Forcing coefficents in lm(), recursive residuals, etc.
In-Reply-To: <5f4f82790507281003589a4b7c@mail.gmail.com>
References: <5f4f82790507281003589a4b7c@mail.gmail.com>
Message-ID: <5f4f8279050728112365e9aeb3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/061294d5/attachment.pl

From r.ramyar at gmail.com  Thu Jul 28 21:44:34 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 20:44:34 +0100
Subject: [R] CUSUM SQUARED structural breaks approach?
In-Reply-To: <5f4f827905072806363e363c6d@mail.gmail.com>
References: <5f4f8279050111051031974bc4@mail.gmail.com>
	<20050111151032.07d05ab4.Achim.Zeileis@wu-wien.ac.at>
	<5f4f82790501111133193a21fb@mail.gmail.com>
	<20050112144553.4faf3cc8.Achim.Zeileis@wu-wien.ac.at>
	<5f4f827905072806363e363c6d@mail.gmail.com>
Message-ID: <5f4f82790507281244516ad391@mail.gmail.com>

Sorry guys, resending this - none of my posts have gone through
because HTML emails where not being delivered... sending this
plaintext now!

On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
> Hi all,
>  
> I have not looked at this CUSUM SQUARED issue since the emails at the
> beginning of the year but am looking at it again.  For those who are
> interested the following paper gives critical values where n>60 in addition
> to the ones in Durbin 1969.  
>  
> Edgerton, David & Wells, Curt, 1994. "Critical Values for the Cusumsq
> Statistic in Medium and Large Sized Samples," Oxford Bulletin of Economics
> and Statistics, Blackwell Publishing, vol. 56(3), pages 355-65. 
>  
>  
> All the best,
>  
> R.
> 
> On 12/01/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote: 
> > On Tue, 11 Jan 2005 19:33:41 +0000 Rick Ram wrote:
> > 
> > > Groundwork for the choice of break method in my specific application 
> > > has already been done - otherwise I would need to rework the wheel
> > > (make a horribly detailed comparison of performance of break
> > > approaches in context of modelling post break)
> > >
> > > If it interests you, Pesaran & Timmerman 2002 compared CUSUM Squared, 
> > > BaiPerron and a time varying approach to detect singular previous
> > > breaks in reverse ordered financial time series so as to update a
> > > forecasting model.
> > 
> > Yes, I know that paper. And if I recall correctly they are mainly 
> > interested in modelling the time period after the last break. For this,
> > the reverse ordered recursive CUSUM approach works because they
> > essentially look back in time to see when their predictions break down.
> > And for their application looking for variance changes also makes sense.
> > The approach is surely valid and sound in this context...but it might be
> > possible to do something better (but I would have to look much closer at 
> > the particular application to have an idea what might be a way to go).
> > 
> > > This works "fine" i.e. the plot looks correct.  The problem is how to
> > > appropriately normalise these to rescale them to what the CUSUM 
> > > squared procedure expects (this looks to be a different and more
> > > complicated procedure than the normalisation used for the basic
> > > CUSUM).  I am from an IT background and am slightly illiterate in
> > > terms of math notation... guidance from anyone would be appreciated
> > 
> > I just had a brief glance at BDE75, page 154, Section 2.4. If I
> > haven't missed anything important on reading it very quickly, you just
> > need to do something like the following (a reproducible example, based
> > on data from strucchange, using a notation similar to BDE's):
> > 
> > ## load GermanM1 data and model
> > library(strucchange)
> > data(GermanM1)
> > M1.model <- dm ~ dy2 + dR + dR1 + dp + ecm.res + season
> > 
> > ## compute squared recursive residuals
> > w2 <- recresid(M1.model, data = GermanM1)^2
> > ## compute CUSUM of squares process
> > sr <- ts(cumsum(c(0, w2))/sum(w2), end = end(GermanM1$dm), freq = 12) 
> > ## the border (r-k)/(T-k)
> > border <- ts(seq(0, 1, length = length(sr)),
> >             start = start(sr), freq = 12)
> > 
> > ## nice plot
> > plot(sr, xaxs = "i", yaxs = "i", main = "CUSUM of Squares") 
> > lines(border, col = grey(0.5))
> > lines(0.4 + border, col = grey(0.5))
> > lines(- 0.4 + border, col = grey(0.5))
> > 
> > Instead of 0.4 you would have to use the appropriate critical values
> > from Durbin (1969) if my reading of the paper is correct. 
> > 
> > hth,
> > Z
> > 
> > > Does anyone know if this represents some commonly performed type of
> > > normalisation than exists in another function??
> > >
> > > I will hunt out the 1969 paper for the critical values but prior to 
> > > doing this I am a bit confused as to how they will
> > > implemented/interpreted... the CUSUM squared plot does/should run
> > > diagonally up from left to right and there are two straight lines that
> > > one would put around this from the critical values.  Hence, a 
> > > different interpretation/implementation of confidence levels than in
> > > other contexts.  I realise this is not just a R thing but a problem
> > > with my theoretical background.
> > >
> > >
> > > Thanks for detailed reply! 
> > >
> > > Rick.
> > >
> > >
> > > >
> > > > But depending on the model and hypothesis you want to test, another
> > > > technique than CUSUM of squares might be more appropriate and also
> > > > available in strucchange. 
> > >
> > > >
> > > > hth,
> > > > Z
> > > >
> > > > > Any help or pointers about where to look would be more than
> > > > > appreciated!  Hopefully I have just missed obvious something in 
> > > > > the package...
> > > > >
> > > > > Many thanks,
> > > > >
> > > > > Rick R.
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide! 
> > > > > http://www.R-project.org/posting-guide.html
> > > > >
> > > >
> > >
> > 
> 
>



From r.ramyar at gmail.com  Thu Jul 28 21:45:00 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 20:45:00 +0100
Subject: [R] R Reference Card (especially useful for Newbies)
In-Reply-To: <5f4f827905072810163f02dada@mail.gmail.com>
References: <200507271722.j6RHMqCK027849@faraday.gene.com>
	<5f4f827905072810163f02dada@mail.gmail.com>
Message-ID: <5f4f8279050728124532da4858@mail.gmail.com>

Sorry guys, resending this - none of my posts have gone through
because HTML emails where not being delivered... sending this
plaintext now!

On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
> This is a very useful resource.  I also wandered around the rest of the site
> when I found this.  Rpad itself looks like a fanstastic tool.  
> 
> On 27/07/05, Berton Gunter <gunter.berton at gene.com> wrote: 
> > 
> > Newbies (and others!) may find useful the R Reference Card made available
> by
> > Tom Short and Rpad at
> http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
> > the "Contributed" link on CRAN (where some other reference cards are also
> > linked). It categorizes and organizes a bunch of R's basic, most used 
> > functions so that they can be easily found. For example, paste() is under
> > the "Strings" heading and expand.grid() is under "Data Creation." For
> > newbies struggling to find the right R function as well as veterans who 
> > can't quite remember the function name, it's very handy.
> > 
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> > 
> > "The business of the statistician is to catalyze the scientific learning 
> > process."  - George E. P. Box
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> 
>



From r.ramyar at gmail.com  Thu Jul 28 21:45:22 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Thu, 28 Jul 2005 20:45:22 +0100
Subject: [R] Forcing coefficents in lm(), recursive residuals, etc.
In-Reply-To: <5f4f8279050728112365e9aeb3@mail.gmail.com>
References: <5f4f82790507281003589a4b7c@mail.gmail.com>
	<5f4f8279050728112365e9aeb3@mail.gmail.com>
Message-ID: <5f4f82790507281245360af0e3@mail.gmail.com>

Sorry guys, resending this - none of my posts have gone through
because HTML emails where not being delivered... sending this
plaintext now!

On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
> Resending cos I think this didn't get through for some reason... apologies
> if it arrives twice!
> 
> 
> ---------- Forwarded message ----------
> From: Rick Ram < r.ramyar at gmail.com>
> Date: 28-Jul-2005 18:03
> Subject: Forcing coefficents in lm(), recursive residuals, etc.
> To: R-help <r-help at stat.math.ethz.ch >
> 
> Hello all,
>  
> Does anyone know how to constrain/force specific coefficients when running
> lm()?  
>  
> I need to run recresid() {strucchange package} on the residuals of
> forecast.lm, but forecast.lm's coefficients must be determined by
> parameter.estimation.lm  
>  
> I could estimate forecast.lm without lm() and use some other kind of
> optimisation, but recresid() requires an object with class lm.  recresid()
> allows you to specify a formula, rather than an lm object, but it looks like
> coefficients are estimated this way too and can't be forced.  
>  
> Here is a bit of code to compensate for my poor explanation:.  
>  
> # Estimate the coefficients of model
> parameter.estimation.lm = lm(formula = y ~ x1 + x2, data =
> estimation.dataset)
> # How do I force the coefficients in forecast.lm to the coeff estimation
> from parameter.estimation.lm??
> 
> forecast.lm = lm(formula = y ~ x1 + x2, data = forecast.dataset)
> # Because I need recursive residuals from the application of the
> coefficients from parameter.estimation.lm to a different dataset
> recresid(forecast.lm)
>  
> Thanks in advance guys,
>  
> R.



From gregory_gentlemen at yahoo.ca  Thu Jul 28 21:58:18 2005
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Thu, 28 Jul 2005 15:58:18 -0400 (EDT)
Subject: [R] using integrate with optimize nested in the integration
Message-ID: <20050728195818.903.qmail@web31205.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/a8958aac/attachment.pl

From reid_huntsinger at merck.com  Thu Jul 28 22:20:34 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 28 Jul 2005 16:20:34 -0400
Subject: [R] using integrate with optimize nested in the integration
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9556@uswpmx00.merck.com>

In your example, f is a function, but
optimize(f,c(0,15),maximum=TRUE)$maximum is just a number (the point at
which f reaches its maximum value). I'm not sure what you want, but if you
had say

f <- function(x,y) x^3 + yx + 1

and defined

g <- function(y) optimize(f,c(0,5),maximum=TRUE,y)$maximum

then g(t) is the x-value at which the function f(x,t) (over x in (0,5))
reaches its maximum for this fixed t. That could then be integrated. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gregory Gentlemen
Sent: Thursday, July 28, 2005 3:58 PM
To: r-help at stat.math.ethz.ch
Subject: [R] using integrate with optimize nested in the integration


Hi guys
im having a problem getting R to numerically integrate for some function, 
say f(bhat)*optimize(G(bhat)), over bhat. Where id like to integrate this
over some finite range, so that here as we integrate over bhat optimize
would return a different optimum.
 
For instance consider this simple example for which I cannot get R to return
the desired result:
 
f <- function(bhat) exp(bhat)
g <- function(bhat) optimize(f,c(0,15),maximum=TRUE)$maximum*bhat
integrate(g,lower=0,upper=5)
which returns:
187.499393759 with absolute error < 2.1e-12

However this is an approximation of : 15*(5^2/2 - 0)=187.5, not what I
intended on getting. Its not identifying that f is a function of bhat ...
any advice or ways I can get integrate to not treat this as a constant?

Any help is appreciated.
 
Gregoy Gentlemen

__________________________________________________



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Thu Jul 28 22:27:25 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 28 Jul 2005 15:27:25 -0500
Subject: [R] using integrate with optimize nested in the integration
In-Reply-To: <20050728195818.903.qmail@web31205.mail.mud.yahoo.com>
References: <20050728195818.903.qmail@web31205.mail.mud.yahoo.com>
Message-ID: <42E93FAD.3040307@pdf.com>



Gregory Gentlemen wrote:
> Hi guys
> im having a problem getting R to numerically integrate for some function, 
> say f(bhat)*optimize(G(bhat)), over bhat. Where id like to integrate this over some finite range, so that here as we integrate over bhat optimize would return a different optimum.
>  
> For instance consider this simple example for which I cannot get R to return the desired result:
>  
> f <- function(bhat) exp(bhat)
> g <- function(bhat) optimize(f,c(0,15),maximum=TRUE)$maximum*bhat
> integrate(g,lower=0,upper=5)
> which returns:
> 187.499393759 with absolute error < 2.1e-12
> 
> However this is an approximation of : 15*(5^2/2 - 0)=187.5, not what I intended on getting. Its not identifying that f is a function of bhat ... any advice or ways I can get integrate to not treat this as a constant?
> 
> Any help is appreciated.
>  
> Gregoy Gentlemen
> 

Hi, Gregory,

Your example is not very useful here since the optimize function is 
constant.

I think you want something more like:

f <- function(bhat, x) {
   exp(bhat * x)
}
g <- function(bhat) {
   o <- vector("numeric", length(bhat))
   for(i in seq(along = bhat))
     o[i] <- optimize(f, c(0,15), maximum = TRUE, x = bhat[i])$maximum
   bhat * o
}
integrate(g, lower = 0, upper = 5)

Because of the way "integrate" works, "g(bhat)" will always return a vector.

HTH,

--sundar



From gregory_gentlemen at yahoo.ca  Thu Jul 28 22:36:50 2005
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Thu, 28 Jul 2005 16:36:50 -0400 (EDT)
Subject: [R] using integrate with optimize nested in the integration
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9556@uswpmx00.merck.com>
Message-ID: <20050728203650.68194.qmail@web31212.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/c5724f53/attachment.pl

From sundar.dorai-raj at pdf.com  Thu Jul 28 22:48:53 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 28 Jul 2005 15:48:53 -0500
Subject: [R] using integrate with optimize nested in the integration
In-Reply-To: <20050728203650.68194.qmail@web31212.mail.mud.yahoo.com>
References: <20050728203650.68194.qmail@web31212.mail.mud.yahoo.com>
Message-ID: <42E944B5.10600@pdf.com>

Again, not a good example, since f is linear in n so the max will always 
be at 15.

Try this:

f <- function(x, n) -(x - 2.5 * n)^2 # max is at 2.5*n
g <- function(n) {
    o <- vector("numeric", length(n))
    for(i in seq(along = n))
      o[i] <- optimize(f, c(0, 15), maximum = TRUE, n = n[i])$maximum
    n * o
}
integrate(g, lower = 0, upper = 5)

# int_0^5 (2.5 * n^2) dn = 2.5/3 * 5^3 = 104.1667

--sundar


Gregory Gentlemen wrote:
> Thanks for the prompt reply.
> Your right, that was a weak example.
> Consider this one though:
>  
> f <- function(n,x) (x-2.5)^2*n
> g <- function(y) optimize(f,c(0,15), maximum=TRUE,x=y)$maximum*y
>  
> then if you try:
> integrate(g,lower=0,upper=5)
> it produces:
> Error in optimize(f, c(0, 15), maximum = TRUE, x = y) : 
>         invalid function value in 'optimize'
> 
> Any ideas? 
> My problem is a similar more complex function in which optimizaiton depends on the value of the integrator.
> 
> "Huntsinger, Reid" <reid_huntsinger at merck.com> wrote:
> In your example, f is a function, but
> optimize(f,c(0,15),maximum=TRUE)$maximum is just a number (the point at
> which f reaches its maximum value). I'm not sure what you want, but if you
> had say
> 
> f <- function(x,y) x^3 + yx + 1
> 
> and defined
> 
> g <- function(y) optimize(f,c(0,5),maximum=TRUE,y)$maximum
> 
> then g(t) is the x-value at which the function f(x,t) (over x in (0,5))
> reaches its maximum for this fixed t. That could then be integrated. 
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gregory Gentlemen
> Sent: Thursday, July 28, 2005 3:58 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] using integrate with optimize nested in the integration
> 
> 
> Hi guys
> im having a problem getting R to numerically integrate for some function, 
> say f(bhat)*optimize(G(bhat)), over bhat. Where id like to integrate this
> over some finite range, so that here as we integrate over bhat optimize
> would return a different optimum.
> 
> For instance consider this simple example for which I cannot get R to return
> the desired result:
> 
> f <- function(bhat) exp(bhat)
> g <- function(bhat) optimize(f,c(0,15),maximum=TRUE)$maximum*bhat
> integrate(g,lower=0,upper=5)
> which returns:
> 187.499393759 with absolute error < 2.1e-12
> 
> However this is an approximation of : 15*(5^2/2 - 0)=187.5, not what I
> intended on getting. Its not identifying that f is a function of bhat ...
> any advice or ways I can get integrate to not treat this as a constant?
> 
> Any help is appreciated.
> 
> Gregoy Gentlemen
> 
> __________________________________________________
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> 
> ------------------------------------------------------------------------------
> 
> ------------------------------------------------------------------------------
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From siddique at ucla.edu  Thu Jul 28 23:00:29 2005
From: siddique at ucla.edu (Juned Siddique)
Date: Thu, 28 Jul 2005 14:00:29 -0700
Subject: [R] Displaying p-values in latex
Message-ID: <002a01c593b7$629e58d0$769b300a@hsrcenter.local>

Hi. I want to create a table in latex with regression coefficients and their
corresponding p-values. My code is below. How do I format the p-values so
that values less than 0.0001 are formated as <.0001 rather than just rounded
to 0.0000? Thank you.

model<-lm(y~x1+x2)

output<-summary(model)
output<-as.matrix(coefficients(output))
output<-format.df(ouput,cdec=c(2,2,2,4))

latex(output,longtable=TRUE, file="C:/model.tex")



From reid_huntsinger at merck.com  Thu Jul 28 23:10:43 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 28 Jul 2005 17:10:43 -0400
Subject: [R] using integrate with optimize nested in the integration
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A955A@uswpmx00.merck.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050728/d6a5615b/attachment.pl

From hari at stat.colostate.edu  Fri Jul 29 00:40:12 2005
From: hari at stat.colostate.edu (Hari Iyer)
Date: Thu, 28 Jul 2005 16:40:12 -0600
Subject: [R] Identify function with matplot
Message-ID: <42E95ECC.8000200@stat.colostate.edu>

Hello
Is there a way to use the   "identify" function with "matplot"  ?
Thanks.
Hari



From simonb at cres10.anu.edu.au  Fri Jul 29 01:39:56 2005
From: simonb at cres10.anu.edu.au (Simon Blomberg)
Date: Fri, 29 Jul 2005 09:39:56 +1000
Subject: [R] Anova's in R
In-Reply-To: <6.1.1.1.2.20050728220728.01a8b6e8@mail.student.unimelb.edu .au>
References: <6.1.1.1.2.20050728220728.01a8b6e8@mail.student.unimelb.edu.au>
Message-ID: <6.2.1.2.0.20050729093426.01dcd9e8@mail.ozemail.com.au>

If I understand you correctly, this looks like a split-plot design.

The anova is:

aov(response~burning*temperature+Error(site), data=dataset)

Alternatively in lme:

lme(response~burning*temperature, random=~1|site, data=dataset)

At 03:09 PM 29/07/2005, you wrote:
>Hello.
>
>I am looking for some help using anova's in RGui.
>
>My experiment ie data, has a fixed burning treatment (Factor A) 2 levels,
>unburnt/burnt.
>Nested within each level of Factor A are 2 random sites (Factor B).
>All sites are crossed with a fixed temperature treatment (Factor C) 2
>levels, 0 degreesC/2 degreesC, with many replicates of these temperature
>treatments randomly located at each site.
>
>I am trying the following
>aov(dependent
>variable~burning*temperature*site+Error(replicate),data=dataset) and
>variations on that, however can't get it quite right.... the F ratios are
>not correct. I imagine this is a fairly common experimental design in
>ecology and would ask that anyone who has some advice please reply to this
>email?
>
>Thank-you,
>Frith
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From MSchwartz at mn.rr.com  Fri Jul 29 02:37:56 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 28 Jul 2005 19:37:56 -0500
Subject: [R] Displaying p-values in latex
In-Reply-To: <002a01c593b7$629e58d0$769b300a@hsrcenter.local>
References: <002a01c593b7$629e58d0$769b300a@hsrcenter.local>
Message-ID: <1122597476.4101.22.camel@localhost.localdomain>

On Thu, 2005-07-28 at 14:00 -0700, Juned Siddique wrote:
> Hi. I want to create a table in latex with regression coefficients and their
> corresponding p-values. My code is below. How do I format the p-values so
> that values less than 0.0001 are formated as <.0001 rather than just rounded
> to 0.0000? Thank you.
> 
> model<-lm(y~x1+x2)
> 
> output<-summary(model)
> output<-as.matrix(coefficients(output))
> output<-format.df(ouput,cdec=c(2,2,2,4))
> 
> latex(output,longtable=TRUE, file="C:/model.tex")

I have not used Frank's latex() function, so there may be a setting that
helps with this, but something along the lines of:

# Using one example from ?lm:

ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2,10,20, labels=c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)

output <- (summary(lm.D9))
output <- as.matrix(coefficients(output))
output <- format.df(output, rdec = c(2, 2, 2, 4))

# Here is the line to re-format the p values
# Note that column alignment on the decimal may be problematic here 
# depending upon the specifications for column justifications. 
# If the column is right justified, it should be easier, since we 
# are forcing four decimal places.

output[, 4] <- ifelse(as.numeric(output[, 4]) < 0.0001, 
                      "$<$0.0001", sprintf("%6.4f", output[, 4]))


> output
            Estimate Std. Error t value Pr(>|t|)   
(Intercept) "5.03"   "0.22"     "22.85" "$<$0.0001"
groupTrt    "-0.37"  "0.3114"   "-1.19" "0.2490"   
attr(,"col.just")
[1] "r" "r" "r" "r"


Note the use of "$" to indicate math mode for the symbols. Presumably
Frank has a similar approach when the object passed to latex() is a
model, since the heading for the p value column should end up being
something like:

   Pr($>$$|$t$|$)

and the minus signs in the second line should similarly be surrounded:

   $-$0.37

I have cc:d Frank here for his clarifications.

HTH,

Marc Schwartz



From mcclatchie.sam at saugov.sa.gov.au  Fri Jul 29 04:13:15 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Fri, 29 Jul 2005 11:43:15 +0930
Subject: [R] lattice/ grid.layout/ multiple graphs per page
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73BAA@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.0.0
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------

>-----Original Message-----
>From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
>Sent: Friday, 29 July 2005 3:44 AM
>To: McClatchie, Sam (PIRSA-SARDI)
>Cc: R-Help-Request (E-mail)
>Subject: Re: [R] lattice/ grid.layout/ multiple graphs per page

>Depends on the details of your 'working trellis code'. Are you using
>print() explicitly with 'newpage=FALSE'? See ?print.trellis for
>details.
>
>Deepayan

-------------------------
Sorry Deepayan, I should have included the full code originally.
Here it is, with your suggestion for using the newpage=FALSE option to
print(). I still have not got it right. 

"egg.and.larvae.vs.intermittency.conditioned.plots" <-
  function()
{
  library(lattice)
  library(grid)

  n <- layout(matrix(c(1,2,3,4), 2,2))
                                        #grid.newpage()
                                        #pushViewport(viewport(layout =
grid.layout(2, 2)))
 
#pushViewport(viewport(layout.pos.col=1, layout.pos.row=1))
            
                                        #trellis.device(postscript,
#file="../figures/egg.and.larvae.vs.intermittency.conditioned.plots.2001.egg
s.ps",horizontal=FALSE,# color=TRUE)

  trellis.par.set(par.xlab.text = list(cex = 1.5))
  trellis.par.set(par.ylab.text = list(cex = 1.5))
  trellis.par.set(axis.text = list(cex = 1.2))
  
  d.2001 <-
mn.ts.e.zoo.vol.vol.filt.anchovy.egg.larv.mn.fluor.wire.intermittency.2001
  d.2002 <-
mn.ts.e.zoo.vol.vol.filt.anchovy.egg.larv.mn.fluor.wire.intermittency.2002
                                        #browser()
  
  attach(d.2001)
                                        #intermittency.blocks <-
cut(percent.unstable, breaks=c(0,10,20,30,40,50))
  intermittency.blocks <- cut(percent.unstable,
breaks=c(0,5,10,15,20,25,30,35,40,45,50))

###trellis plots, 2001 data
  int <- matrix(c(0,50,45,100,90,200), ncol=2, byrow=TRUE)  
  egg.counts <- shingle(d.2001$eggs2.Pilch.Eggs, intervals = int)
  out1 <- bwplot(eggs2.Pilch.Eggs ~  intermittency.blocks |  egg.counts,
                 data = d.2001,
                 xlab = "intermittency  (% of water column unstable)",
                 ylab = "eggs m^2",
                 main = "2001",
                 ylim = c(0,200),
                 layout = c(1,3),
                 auto.key = T)
  print(out1, newpage=F)
  par(par.old) 
                                        #  graphics.off()
  
                                        #  x11()
                                        # trellis.device(postscript,
#file="../figures/egg.and.larvae.vs.intermittency.conditioned.plots.2001.lar
vae.ps",horizontal=FALS#E, color=TRUE)

 
#pushViewport(viewport(layout.pos.col=1, layout.pos.row=2))
  trellis.par.set(par.xlab.text = list(cex = 1.5))
  trellis.par.set(par.ylab.text = list(cex = 1.5))
  trellis.par.set(axis.text = list(cex = 1.2))
  
  int <- matrix(c(0,50,45,100,90,200), ncol=2, byrow=TRUE)  
  larval.counts <- shingle(d.2001$eggs2.Pilch.Larv, intervals = int)
  out2 <- bwplot(eggs2.Pilch.Larv ~  intermittency.blocks |  larval.counts,
                 data = d.2001,
                 xlab = "intermittency (% of water column unstable)",
                 ylab = expression(paste("larvae ",m^{-2})),
                 main = "2001",
                 ylim = c(0,200),
                 layout = c(1,3),
                 auto.key = T)
  detach("d.2001")
  print(out2, newpage=F)
                                        #graphics.off()
  
###trellis plots, 2002 data
  attach(d.2002)
                                        #  x11()
                                        #trellis.device(postscript,
#file="../figures/egg.and.larvae.vs.intermittency.conditioned.plots.2002.egg
s.ps",horizontal=FALSE#, color=TRUE)

 
#pushViewport(viewport(layout.pos.col=2, layout.pos.row=1))
  trellis.par.set(par.xlab.text = list(cex = 1.5))
  trellis.par.set(par.ylab.text = list(cex = 1.5))
  trellis.par.set(axis.text = list(cex = 1.2))
  
  int <- matrix(c(0,50,45,100,90,200), ncol=2, byrow=TRUE)  
  egg.counts <- shingle(d.2002$eggs2.Pilch.Eggs, intervals = int)
  out3 <- bwplot(eggs2.Pilch.Eggs ~  intermittency.blocks |  egg.counts,
                 data = d.2002,
                 xlab = "intermittency  (% of water column unstable)",
                 ylab = "eggs m^2",
                 main = "2002",
                 ylim = c(0,200),
                 layout = c(1,3),
                 auto.key = T)
  
  par(par.old)
                                        #graphics.off()
  print(out3, newpage=F)
                                        # x11()
                                        # trellis.device(postscript,
#file="../figures/egg.and.larvae.vs.intermittency.conditioned.plots.2002.lar
vae.ps",horizontal=FALS#E, color=TRUE)

 
#pushViewport(viewport(layout.pos.col=2, layout.pos.row=2))
  trellis.par.set(par.xlab.text = list(cex = 1.5))
  trellis.par.set(par.ylab.text = list(cex = 1.5))
  trellis.par.set(axis.text = list(cex = 1.2))
  
  int <- matrix(c(0,50,45,100,90,200), ncol=2, byrow=TRUE)   
  larval.counts <- shingle(d.2002$eggs2.Pilch.Larv, intervals = int)
  out4 <- bwplot(eggs2.Pilch.Larv ~  intermittency.blocks |  larval.counts,
                 data = d.2002,
                 xlab = "intermittency (% of water column unstable)",
                 ylab = expression(paste("larvae ", m^{-2})),
                 main = "2002",
                 ylim = c(0,200),
                 layout = c(1,3),
                 auto.key = T)
  print(out4, newpage=F)
  par(par.old)
                                        #  graphics.off()
  detach("d.2002")
}

>-----Original Message-----
>From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com]
>Sent: Friday, 29 July 2005 3:44 AM
>To: McClatchie, Sam (PIRSA-SARDI)
>Cc: R-Help-Request (E-mail)
>Subject: Re: [R] lattice/ grid.layout/ multiple graphs per page
>
>
>On 7/27/05, McClatchie, Sam (PIRSA-SARDI)
><mcclatchie.sam at saugov.sa.gov.au> wrote:
>> Background:
>> OS: Linux Mandrake 10.1
>> release: R 2.0.0
>> editor: GNU Emacs 21.3.2
>> front-end: ESS 5.2.3
>> ---------------------------------
>> Colleagues
>> 
>> I have a set of lattice plots, and want to plot 4 of them on 
>the page.
>> I am having trouble with the layout.
>> 
>> grid.newpage()
>> pushViewport(viewport(layout = grid.layout(2,2)))
>> pushviewport(viewport(layout.pos.col = 1, layout.pos.row = 1))
>> working trellis graph code here
>> pushviewport(viewport(layout.pos.col = 1, layout.pos.row = 2))
>> working trellis graph code here
>> pushviewport(viewport(layout.pos.col = 2, layout.pos.row = 1))
>> working trellis graph code here
>> pushviewport(viewport(layout.pos.col = 2, layout.pos.row = 2))
>> 
>> I'm obviously doing something wrong since my graphs still 
>print one per
>> page?
>
>Depends on the details of your 'working trellis code'. Are you using
>print() explicitly with 'newpage=FALSE'? See ?print.trellis for
>details.
>
>Deepayan
>
>> Would you be able to provide advice?
>> 
>> Thanks
>> 
>> Sam
>> ----
>> Sam McClatchie,
>> Biological oceanography
>> South Australian Aquatic Sciences Centre
>> PO Box 120, Henley Beach 5022
>> Adelaide, South Australia
>> email <mcclatchie.sam at saugov.sa.gov.au>
>> Telephone: (61-8) 8207 5448
>> FAX: (61-8) 8207 5481
>> Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
>> 
>>                    /\
>>       ...>><xX(??>
>>                 //// \\\\
>>                    <??)Xx><<
>>               /////  \\\\\\
>>                         ><(((??>
>>   >><(((??>   ...>><xX(??>O<??)Xx><<
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>>
>



From pantd at unlv.nevada.edu  Fri Jul 29 04:20:13 2005
From: pantd at unlv.nevada.edu (pantd@unlv.nevada.edu)
Date: Thu, 28 Jul 2005 19:20:13 -0700
Subject: [R] gamma distribution
In-Reply-To: <17128.41738.29156.99223@stat.math.ethz.ch>
References: <1122433827.42e6fb232560f@webmail.scsv.nevada.edu>
	<42E73542.6090505@statistik.uni-dortmund.de>
	<1122504171.42e80debf07e9@webmail.scsv.nevada.edu>
	<42E88919.9020603@statistik.uni-dortmund.de>
	<1122536682.42e88cea1592f@webmail.scsv.nevada.edu>
	<17128.41738.29156.99223@stat.math.ethz.ch>
Message-ID: <1122603613.42e9925dd4046@webmail.scsv.nevada.edu>

Hi Christopher and Uwe. thanks for your time and guidance.
I deeply appreciate it.


-dev


Quoting Christoph Buser <buser at stat.math.ethz.ch>:

> Hi
>
> As Uwe mentioned be careful about the difference the
> significance level alpha and the power of a test.
>
> To do power calculations you should specify and alternative
> hypothesis H_A, e.g. if you have two populations you want to
> compare and we assume that they are normal distributed (equal
> unknown variance for simplicity). We are interested if there is
> a difference in the mean and want to use the t.test.
> Our Null hypothesis H_0: there is no difference in the means
>
> To do a power calculation for our test, we first have to specify
> and alternative H_A: the mean difference is 1 (unit)
> Now for a fix number of observations we can calculate the power
> of our test, which is in that case the probability that (if the
> true unknown difference is 1, meaning that H_A is true) our test
> is significant, meaning if I repeat the test many times (always
> taking samples with mean difference of 1), the number of
> significant test divided by the total number of tests is an
> estimate for the power.
>
>
> In you case the situation is a little bit more complicated. You
> need to specify an alternative hypothesis.
> In one of your first examples you draw samples from two gamma
> distributions with different shape parameter and the same
> scale. But by varying the shape parameter the two distributions
> not only differ in their mean but also in their form.
>
> I got an email from Prof. Ripley in which he explained in
> details and very precise some examples of tests and what they
> are testing. It was in addition to the first posts about t tests
> and wilcoxon test.
> I attached the email below and recommend to read it carefully. It
> might be helpful for you, too.
>
> Regards,
>
> Christoph Buser
>
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------
>
> ________________________________________________________________________
>
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To: Christoph Buser <buser at stat.math.ethz.ch>
> cc: "Liaw, Andy" <andy_liaw at merck.com>
> Subject: Re: [R] Alternatives to t-tests (was Code Verification)
> Date: Thu, 21 Jul 2005 10:33:28 +0100 (BST)
>
> I believe there is a rather more to this than Christoph's account.  The
> Wilcoxon test is not testing the same null hypothesis as the t-test, and
> that may very well matter in practice and it does in the example given.
>
> The (default in R) Welch t-test tests a difference in means between two
> samples, not necessarily of the same variance or shape.  A difference in
> means is simple to understand, and is unambiguously defined at least if
> the distributions have means, even for real-life long-tailed
> distributions.  Inference from the t-test is quite accurate even a long
> way from normality and from equality of the shapes of the two
> distributions, except in very small sample sizes.  (I point my beginning
> students at the simulation study in `The Statistical Sleuth' by Ramsey and
> Schafer, stressing that the unequal-variance t-test ought to be the
> default choice as it is in R.  So I get them to redo the simulations.)
>
> The Wilcoxon test tests a shift in location between two samples from
> distributions of the same shape differing only by location.  Having the
> same shape is part of the null hypothesis, and so is an assumption that
> needs to be verified if you want to conclude there is a difference in
> location (e.g. in means).  Even if you assume symmetric distributions (so
> the location is unambiguously defined) the level of the test depends on
> the shapes, tending to reject equality of location in the presence of
> difference of shape.  So you really are testing equality of distribution,
> both location and shape, with power concentrated on location-shift
> alternatives.
>
> Given samples from a gamma(shape=2) and gamma(shape=20) distributions, we
> know what the t-test is testing (equality of means).  What is the Wilcoxon
> test testing?  Something hard to describe and less interesting, I believe.
>
> BTW, I don't see the value of the gamma simulation as this
> simultaneously changes mean and shape between the samples.  How about
> checking holding the mean the same:
>
> n <- 1000
> z1 <- z2 <- numeric(n)
> for (i in 1:n) {
>    x <- rgamma(40, 2.5, 0.1)
>    y <- rgamma(40, 10, 0.1*10/2.5)
>    z1[i] <- t.test(x, y)$p.value
>    z2[i] <- wilcox.test(x, y)$p.value
> }
> ## Level
> 1 - sum(z1>0.05)/1000  ## 0.049
> 1 - sum(z2>0.05)/1000  ## 0.15
>
> ? -- the Wilcoxon test is shown to be a poor test of equality of means.
> Christoph's simulation shows that it is able to use difference in shape as
> well as location in the test of these two distributions, whereas the
> t-test is designed only to use the difference in means.  Why compare the
> power of two tests testing different null hypotheses?
>
> I would say a very good reason to use a t-test is if you are actually
> interested in the hypothesis it tests ....
>
>
>
>
>
> pantd at unlv.nevada.edu writes:
>  > thanks for your response. btw i am calculating the power of the wilcoxon
> test. i
>  > divide the total no. of rejections by the no. of simulations. so for 1000
>  > simulations, at 0.05 level of significance if the no. of rejections are 50
> then
>  > the power will be 50/1000 = 0.05. thats y im importing in excel the p
> values.
>  >
>  > is my approach correct??
>  >
>  > thanks n regards
>  > -dev
>  >
>  >
>



From f.harrell at vanderbilt.edu  Fri Jul 29 05:38:40 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Jul 2005 23:38:40 -0400
Subject: [R] Displaying p-values in latex
In-Reply-To: <1122597476.4101.22.camel@localhost.localdomain>
References: <002a01c593b7$629e58d0$769b300a@hsrcenter.local>
	<1122597476.4101.22.camel@localhost.localdomain>
Message-ID: <42E9A4C0.3020402@vanderbilt.edu>

Marc Schwartz wrote:
> On Thu, 2005-07-28 at 14:00 -0700, Juned Siddique wrote:
> 
>>Hi. I want to create a table in latex with regression coefficients and their
>>corresponding p-values. My code is below. How do I format the p-values so
>>that values less than 0.0001 are formated as <.0001 rather than just rounded
>>to 0.0000? Thank you.
>>
>>model<-lm(y~x1+x2)
>>
>>output<-summary(model)
>>output<-as.matrix(coefficients(output))
>>output<-format.df(ouput,cdec=c(2,2,2,4))
>>
>>latex(output,longtable=TRUE, file="C:/model.tex")
> 
> 
> I have not used Frank's latex() function, so there may be a setting that
> helps with this, but something along the lines of:
> 
> # Using one example from ?lm:
> 
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> group <- gl(2,10,20, labels=c("Ctl","Trt"))
> weight <- c(ctl, trt)
> lm.D9 <- lm(weight ~ group)
> 
> output <- (summary(lm.D9))
> output <- as.matrix(coefficients(output))
> output <- format.df(output, rdec = c(2, 2, 2, 4))
> 
> # Here is the line to re-format the p values
> # Note that column alignment on the decimal may be problematic here 
> # depending upon the specifications for column justifications. 
> # If the column is right justified, it should be easier, since we 
> # are forcing four decimal places.
> 
> output[, 4] <- ifelse(as.numeric(output[, 4]) < 0.0001, 
>                       "$<$0.0001", sprintf("%6.4f", output[, 4]))
> 
> 
> 
>>output
> 
>             Estimate Std. Error t value Pr(>|t|)   
> (Intercept) "5.03"   "0.22"     "22.85" "$<$0.0001"
> groupTrt    "-0.37"  "0.3114"   "-1.19" "0.2490"   
> attr(,"col.just")
> [1] "r" "r" "r" "r"
> 
> 
> Note the use of "$" to indicate math mode for the symbols. Presumably
> Frank has a similar approach when the object passed to latex() is a
> model, since the heading for the p value column should end up being
> something like:
> 
>    Pr($>$$|$t$|$)
> 
> and the minus signs in the second line should similarly be surrounded:
> 
>    $-$0.37
> 
> I have cc:d Frank here for his clarifications.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 

I've only implemented a fully automatic latex function for anova output, 
which handles P-value printing as requested.  Look at anova.Design for 
the code. -Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Fri Jul 29 08:47:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Jul 2005 07:47:39 +0100 (BST)
Subject: [R] Running Internet Explorer from Withing R
In-Reply-To: <x27jfaq4xs.fsf@turmalin.kubism.ku.dk>
References: <200507281608.j6SG8O8v031251@hypatia.math.ethz.ch>
	<x27jfaq4xs.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0507290745370.6812@gannet.stats>

On Thu, 28 Jul 2005, Peter Dalgaard wrote:

> "Walter R. Paczkowski" <dataanalytics at earthlink.net> writes:
>
>> Good morning,
>>
>> Is it possible to open an html file using IE but from within R? I
>> wrote a small function to generate tables in html but I'd like to
>> write another function to call IE and open the html file.
>
> browseURL() (if it exists on Windows)

It does.

And more generally for any file type, shell.exec (only on Windows) opens 
a file with the associated application. (browseURL is built on top of 
shell.exec.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Allan at STATS.uct.ac.za  Fri Jul 29 10:20:22 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 29 Jul 2005 10:20:22 +0200
Subject: [R] R: non parametric regression/kernels
Message-ID: <42E9E6C6.E8218FEE@STATS.uct.ac.za>

hi all

i have a another stats question.

i would like to solve the following question:

y(i)=a+b*x(i)+e(i)

i.e. estimate a and b (they should be fixed) but i dont want to specify
the standard density to the straight line.

this can be done using kernel regression. the fitted line is however
fitted locally. does anyone have a reference that will help me with my
problem.

i am still new to kernels/kernel regression and would like to get into
the subject.

/
allan

From vito_ricci at yahoo.com  Fri Jul 29 10:52:49 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 29 Jul 2005 10:52:49 +0200 (CEST)
Subject: [R] R: non parametric regression/kernels
Message-ID: <20050729085249.55462.qmail@web41213.mail.yahoo.com>

Hi Clark,

see: 
? loess

http://finzi.psych.upenn.edu/R/library/stats/html/loess.html

? scatter.smooth 

http://finzi.psych.upenn.edu/R/library/stats/html/scatter.smooth.html

Regards,

Vito

Clark Allan <Allan <at> STATS.uct.ac.za> wrote:

hi all

i have a another stats question.

i would like to solve the following question:

y(i)=a+b*x(i)+e(i)

i.e. estimate a and b (they should be fixed) but i
dont want to specify
the standard density to the straight line.

this can be done using kernel regression. the fitted
line is however
fitted locally. does anyone have a reference that will
help me with my
problem.

i am still new to kernels/kernel regression and would
like to get into
the subject.

/
allan



Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write"
H. G. Wells

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palesesanto_spirito/



From lami at faunalia.it  Fri Jul 29 11:07:21 2005
From: lami at faunalia.it (Leonardo Lami)
Date: Fri, 29 Jul 2005 11:07:21 +0200
Subject: [R] predictive classification groups functions
Message-ID: <200507291107.22006.lami@faunalia.it>

Hi all,
I have a little question about linear discriminant analysis in R.
I have a dataframe with datas groupped by 4 cluster.
I'd like to know if there is a command that give 4 predictive classification 
groups functions (one for every cluster group).

Thanks of all
Leonardo

-- 
Leonardo Lami
lami at faunalia.it            www.faunalia.it
Via Colombo 3 - 51010 Massa e Cozzile (PT), Italy   Tel: (+39)349-1310164
GPG key @: hkp://wwwkeys.pgp.net http://www.pgp.net/wwwkeys.html
https://www.biglumber.com



From ligges at statistik.uni-dortmund.de  Fri Jul 29 11:31:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Jul 2005 11:31:39 +0200
Subject: [R] Identify function with matplot
In-Reply-To: <42E95ECC.8000200@stat.colostate.edu>
References: <42E95ECC.8000200@stat.colostate.edu>
Message-ID: <42E9F77B.8010300@statistik.uni-dortmund.de>

Hari Iyer wrote:

> Hello
> Is there a way to use the   "identify" function with "matplot"  ?

Not directly, but you can write a wrapper for the particular case as in:

sines <- outer(1:20, 1:4, function(x, y) sin(x / 20 * pi * y))
matplot(sines, pch = 1:4, type = "o", col = rainbow(ncol(sines)))

identifyMat <- function(x, ...){
   label <- paste(row(x), ", ", col(x), sep="")
   identify(row(x), x, labels=label, ...)
}
identifyMat(sines)

Uwe Ligges


> Thanks.
> Hari
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Fri Jul 29 12:01:30 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 29 Jul 2005 11:01:30 +0100
Subject: [R] NA handling with lm
In-Reply-To: <42E92B2E.25031.1D2BCBD@localhost>
References: <42E92B2E.25031.1D2BCBD@localhost>
Message-ID: <1122631290.5999.0.camel@ipc143004.lif.icnet.uk>

Also have a look at this thread from last week
https://stat.ethz.ch/pipermail/r-help/2005-July/074550.html

Regards, Adai



On Thu, 2005-07-28 at 18:59 +0200, Petr Pikal wrote:
> Hallo
> 
> On 28 Jul 2005 at 18:44, Andreas Cordes wrote:
> 
> > Hi,
> > I have a problem that is hopefully easily solvable, but I dont find
> > the clue in the documentation. I am examining a linear model. One of
> > the variables has NA values. Even though na.action=na.omit, i get NA
> > as results for this variable. Can I use lm in such a case to get
> > estimates? Or do I have to do some form of imputation before doing so?
> > Here is the call and the results, hope you can help. Best regards,
> > Andreas
> > ----------------------------------------------------------------------
> > ----------------------- lm(formula = ESSIK ~ ALTER + as.factor(S2) +
> > as.factor(S15A) +
> >     as.factor(S8) + as.factor(LAND) + as.factor(S18B) +
> >     as.factor(BERUF) + as.factor(KIRCHE) + as.factor(H_EINKOM) +
> >     as.factor(PARTNERS), na.action = na.omit)
> > 
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -17.0675  -2.0151   0.4267   2.7644   9.7333
> > 
> > Coefficients: (2 not defined because of singularities)
> 
> Problem is not in NA handling but that some of your coeficients 
> can be represented as linear combination of other coeficients. You 
> have to omit them.
> 
> HTH
> Petr
> 
> 
> >                       Estimate Std. Error t value Pr(>|t|)   
> > (Intercept)          23.755915   1.844110  12.882  < 2e-16 ***
> > [...]
> > as.factor(BERUF)7    -1.236836   0.701323  -1.764 0.077942 . 
> > as.factor(KIRCHE)1   -0.811751   0.237699  -3.415 0.000649 ***
> > as.factor(H_EINKOM)2        NA         NA      NA       NA   
> > as.factor(H_EINKOM)3        NA         NA      NA       NA   
> > as.factor(PARTNERS)1  2.057070   0.342546   6.005 2.23e-09 ***
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Fri Jul 29 12:11:59 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 29 Jul 2005 11:11:59 +0100
Subject: [R] matrix form
In-Reply-To: <4715.128.240.6.80.1122569123.squirrel@128.240.6.80>
References: <4715.128.240.6.80.1122569123.squirrel@128.240.6.80>
Message-ID: <1122631919.5999.8.camel@ipc143004.lif.icnet.uk>

If you mean "how to create a matrix like below", then you can code it as

   arm   <- 1:10
   study <- rep( 1:4, c(3,3,2,2) )
   Num   <- c(2, 9, ...)

   df <- cbind( arm, study, Num )

but this is really painful. If you have this information stored in
Excel, save it as tab delimited or comma separated format and read in
using read.delim or read.csv respectively. You can also use scan.
See help("read.delim"), help("read.csv") or help("scan").
   
If you mean "how to extract information from such a matrix", then see
help(subset) or read the manuals.

Regards, Adai



On Thu, 2005-07-28 at 17:45 +0100, Hathaikan Chootrakool wrote:
> I am a new user, i was wondering how to define a collection of data in
> matrix form,
> this is a part of my data,there are 26 studies, 3 Treatments
> 
>    Arm No  Study no.  Treatment  Num(r) Total(n)
> 1    1        1         1          2    43
> 2    2        1         2          9    42
> 3    3        1         3          13   41
> 4    4        2         1          12   68
> 5    5        2         2          13   73
> 6    6        2         3          13   72
> 7    7        3         1           4   20
> 8    8        3         3           4   16
> 9    9        4         1          20   116
> 10  10        4         3          30   111
> 
> I would like to use matrix [study No,Treatment] how can i define code for
> using matrix?
> 
> has anyone can help me?,thank you very much.
> 
> Hathaikan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From r.ramyar at gmail.com  Fri Jul 29 12:35:58 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Fri, 29 Jul 2005 11:35:58 +0100
Subject: [R] Forcing coefficents in lm(), recursive residuals, etc.
In-Reply-To: <5f4f82790507281245360af0e3@mail.gmail.com>
References: <5f4f82790507281003589a4b7c@mail.gmail.com>
	<5f4f8279050728112365e9aeb3@mail.gmail.com>
	<5f4f82790507281245360af0e3@mail.gmail.com>
Message-ID: <5f4f827905072903352c19b9ab@mail.gmail.com>

Hi all,
Just to clarify, I know that the predict() function would the normal
avenue for applying a model but the problem is that I need to
calculate recursive residuals, and the recresid() function needs an
object with class "lm".
Best,
R.

On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
> Sorry guys, resending this - none of my posts have gone through
> because HTML emails where not being delivered... sending this
> plaintext now!
> 
> On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
> > Resending cos I think this didn't get through for some reason... apologies
> > if it arrives twice!
> >
> >
> > ---------- Forwarded message ----------
> > From: Rick Ram < r.ramyar at gmail.com>
> > Date: 28-Jul-2005 18:03
> > Subject: Forcing coefficents in lm(), recursive residuals, etc.
> > To: R-help <r-help at stat.math.ethz.ch >
> >
> > Hello all,
> >
> > Does anyone know how to constrain/force specific coefficients when running
> > lm()?
> >
> > I need to run recresid() {strucchange package} on the residuals of
> > forecast.lm, but forecast.lm's coefficients must be determined by
> > parameter.estimation.lm
> >
> > I could estimate forecast.lm without lm() and use some other kind of
> > optimisation, but recresid() requires an object with class lm.  recresid()
> > allows you to specify a formula, rather than an lm object, but it looks like
> > coefficients are estimated this way too and can't be forced.
> >
> > Here is a bit of code to compensate for my poor explanation:.
> >
> > # Estimate the coefficients of model
> > parameter.estimation.lm = lm(formula = y ~ x1 + x2, data =
> > estimation.dataset)
> > # How do I force the coefficients in forecast.lm to the coeff estimation
> > from parameter.estimation.lm??
> >
> > forecast.lm = lm(formula = y ~ x1 + x2, data = forecast.dataset)
> > # Because I need recursive residuals from the application of the
> > coefficients from parameter.estimation.lm to a different dataset
> > recresid(forecast.lm)
> >
> > Thanks in advance guys,
> >
> > R.
>



From jsorkin at grecc.umaryland.edu  Fri Jul 29 13:17:07 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 29 Jul 2005 07:17:07 -0400
Subject: [R] Binary outcome with non-absorbing outcome state
Message-ID: <s2e9d805.062@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050729/75e57ae6/attachment.pl

From Torsten.Schindler at chello.at  Fri Jul 29 13:21:05 2005
From: Torsten.Schindler at chello.at (Torsten Schindler)
Date: Fri, 29 Jul 2005 13:21:05 +0200
Subject: [R] PLS component selection for GPLS question
Message-ID: <A4D2013F-7CAD-402A-8439-DAF213DF9E35@chello.at>

How to select the number of PLS components for GPLS for data sets  
with few samples?

Concrete problem:
My data set: 9 samples of class A and 37 of class B with 254  
descriptors.

In the paper: "Classification Using Generalized Partial Least  
Squares", Beiying Ding, Robert Gentleman, Bioconductor
Project Working Papers, year 2004, paper 5

Section 2.6 Assessing Prediction:
Cite: "The optimal number of PLS components is selected by choosing  
that value of K which minimizes LOOCV
error rate for the training set."

and in section 3.1.3 Colon data, subsection: Random splitting
Cite: "Due to the instability of LOOCV error rates for data with few  
samples and many covariates, comparison of various
classifiers based solely on LOOCV classification errors may not be  
reliable."

the authors use random splitting to determine the number of PLS  
components in GPLS, but I'm still not sure how to
choose the right number of PLS components for my data set.

I used the function errorest() from package ipred to estimate the  
error rates und gpls() with Firth procedure switched on.
The attached PDF Graphik illustrates the problem for my data set.

S_n is the model sensitivity and S_p the model specifity.
With 4 component I get the best crossvalidation error rate 17% and  
with 5 components the best bootstrap error rate 9%, but
the sensitivity of the model is only 11% !
If one choose 13 components, one gets 100% sensitivity and 100%  
specifity and CV error is 34% and the boostrap error is 40%
and the risk that the model is overtrained is higher.

How much components should I choose now to get the best GPLS model?

-------------- next part --------------
A non-text attachment was scrubbed...
Name: GPLS_component_selection.pdf
Type: application/pdf
Size: 11328 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050729/c2f5fb54/GPLS_component_selection.pdf
-------------- next part --------------





From Allan at STATS.uct.ac.za  Fri Jul 29 13:30:54 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 29 Jul 2005 13:30:54 +0200
Subject: [R] R: graphics devices
Message-ID: <42EA136E.570529EE@STATS.uct.ac.za>

a simple question

how does one produce plots on two different graphics devices?

/
allan

From sean.oriordain at gmail.com  Fri Jul 29 13:34:32 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Fri, 29 Jul 2005 11:34:32 +0000
Subject: [R] R: graphics devices
In-Reply-To: <42EA136E.570529EE@STATS.uct.ac.za>
References: <42EA136E.570529EE@STATS.uct.ac.za>
Message-ID: <8ed68eed0507290434949cb65@mail.gmail.com>

Alan,
I'm not sure what you mean...

perhaps

plot(y~x) # to the screen
pdf("myplot.pdf")
plot(y~x) # write the plot to the file
dev.off() # close the file
dev.off() # close the graphics window

s/

On 29/07/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> a simple question
> 
> how does one produce plots on two different graphics devices?
> 
> /
> allan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Allan at STATS.uct.ac.za  Fri Jul 29 13:43:38 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 29 Jul 2005 13:43:38 +0200
Subject: [R] R: graphics devices
References: <42EA136E.570529EE@STATS.uct.ac.za>
	<8ed68eed0507290434949cb65@mail.gmail.com>
Message-ID: <42EA166A.E819D89D@STATS.uct.ac.za>

one could use the par command to plot several plots on 1 graphics
device.

i would like to do the following:

say plot(y~x) on one screen and then

plot(q~w) on another screen so that one can see both of them together
but not on the same graphics device.

Sean O'Riordain wrote:
> 
> Alan,
> I'm not sure what you mean...
> 
> perhaps
> 
> plot(y~x) # to the screen
> pdf("myplot.pdf")
> plot(y~x) # write the plot to the file
> dev.off() # close the file
> dev.off() # close the graphics window
> 
> s/
> 
> On 29/07/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> > a simple question
> >
> > how does one produce plots on two different graphics devices?
> >
> > /
> > allan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >

From Matthias.Templ at statistik.gv.at  Fri Jul 29 13:38:45 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 29 Jul 2005 13:38:45 +0200
Subject: [R] R: graphics devices
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAB87@xchg1.statistik.local>

Eventually one way:

With X11() you can open additional graphic devices.

x <- rnorm(100)
y <- x + runif(100)

plot(x)
X11()
plot(x,y)

Best,
Matthias

> 
> a simple question
> 
> how does one produce plots on two different graphics devices?
> 
> /
> allan
>



From ccleland at optonline.net  Fri Jul 29 13:58:54 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 29 Jul 2005 07:58:54 -0400
Subject: [R] R: graphics devices
In-Reply-To: <42EA166A.E819D89D@STATS.uct.ac.za>
References: <42EA136E.570529EE@STATS.uct.ac.za>
	<8ed68eed0507290434949cb65@mail.gmail.com>
	<42EA166A.E819D89D@STATS.uct.ac.za>
Message-ID: <42EA19FE.5020103@optonline.net>

   Is this on windows?  If so, how about the following:

 > windows()
 > plot(y~x)
 > windows()
 > plot(q~x)

   For me, this creates two different plot _windows_ and you could tile 
them to see each side-by-side.  But I'm not sure how that would be 
preferable to having the plots in the same window.  What exactly do you 
mean by different screens?

Clark Allan wrote:
> one could use the par command to plot several plots on 1 graphics
> device.
> 
> i would like to do the following:
> 
> say plot(y~x) on one screen and then
> 
> plot(q~w) on another screen so that one can see both of them together
> but not on the same graphics device.
> 
> Sean O'Riordain wrote:
> 
>>Alan,
>>I'm not sure what you mean...
>>
>>perhaps
>>
>>plot(y~x) # to the screen
>>pdf("myplot.pdf")
>>plot(y~x) # write the plot to the file
>>dev.off() # close the file
>>dev.off() # close the graphics window
>>
>>s/
>>
>>On 29/07/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
>>
>>>a simple question
>>>
>>>how does one produce plots on two different graphics devices?
>>>
>>>/
>>>allan
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>
>>>------------------------------------------------------------------------
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From manuel.schneider at eawag.ch  Fri Jul 29 14:01:47 2005
From: manuel.schneider at eawag.ch (Manuel Schneider)
Date: Fri, 29 Jul 2005 14:01:47 +0200
Subject: [R] Console not found
Message-ID: <42EA1AAB.5090802@eawag.ch>

I played around with memory limits in R 2.1.0 under XP in order to be 
able to work with large matrixes (3600x4100). Among several things I 
tried was to alter console settings and saving them.
Since then, I can't restart Rgui. It says several times 'Console not 
found' with pieces of the text that usually appears in the console and 
then crashes. Rterm.exe works fine.
I've now unistalled R 2.1.0 and installed R 2.1.1 with no effect, still 
console is not found.
Any clues on this?

Best regards

Manuel

??????
Manuel Schneider
Eawag
Environmental chemistry
Ueberlandstr. 133
8600 D??bendorf
Phone +41 44 823 51 18



From sean.oriordain at gmail.com  Fri Jul 29 14:03:26 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Fri, 29 Jul 2005 12:03:26 +0000
Subject: [R] R: graphics devices
In-Reply-To: <42EA19FE.5020103@optonline.net>
References: <42EA136E.570529EE@STATS.uct.ac.za>
	<8ed68eed0507290434949cb65@mail.gmail.com>
	<42EA166A.E819D89D@STATS.uct.ac.za> <42EA19FE.5020103@optonline.net>
Message-ID: <8ed68eed050729050374802bea@mail.gmail.com>

actually X11() also works under windows!

X11()
plot(y~x)
X11()
plot(q~x)


On 29/07/05, Chuck Cleland <ccleland at optonline.net> wrote:
>    Is this on windows?  If so, how about the following:
> 
>  > windows()
>  > plot(y~x)
>  > windows()
>  > plot(q~x)
> 
>    For me, this creates two different plot _windows_ and you could tile
> them to see each side-by-side.  But I'm not sure how that would be
> preferable to having the plots in the same window.  What exactly do you
> mean by different screens?
> 
> Clark Allan wrote:
> > one could use the par command to plot several plots on 1 graphics
> > device.
> >
> > i would like to do the following:
> >
> > say plot(y~x) on one screen and then
> >
> > plot(q~w) on another screen so that one can see both of them together
> > but not on the same graphics device.
> >
> > Sean O'Riordain wrote:
> >
> >>Alan,
> >>I'm not sure what you mean...
> >>
> >>perhaps
> >>
> >>plot(y~x) # to the screen
> >>pdf("myplot.pdf")
> >>plot(y~x) # write the plot to the file
> >>dev.off() # close the file
> >>dev.off() # close the graphics window
> >>
> >>s/
> >>
> >>On 29/07/05, Clark Allan <Allan at stats.uct.ac.za> wrote:
> >>
> >>>a simple question
> >>>
> >>>how does one produce plots on two different graphics devices?
> >>>
> >>>/
> >>>allan
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>
> >>>
> >>>------------------------------------------------------------------------
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>



From spyridoula.tsonaka at med.kuleuven.be  Fri Jul 29 14:15:54 2005
From: spyridoula.tsonaka at med.kuleuven.be (Spyridoula Tsonaka)
Date: Fri, 29 Jul 2005 14:15:54 +0200
Subject: [R] R: graphics devices
References: <42EA136E.570529EE@STATS.uct.ac.za>
Message-ID: <01a301c59437$44ad5710$1540210a@www.domain>

Hi Allan,

in case of many plots in the same window you can try this:

options(graphics.record=TRUE)
plot(x)
plot(x,y)

with PgUp and PgDn you check both in the same window.

I hope this helps.

Roula

=================
Spyridoula Tsonaka
Doctoral Student
Biostatistical Centre
Catholic University of Leuven
Kapucijnenvoer 35
B-3000 Leuven
Belgium
Tel: +32/16/336899
Fax: +32/16/337015


----- Original Message ----- 
From: "Clark Allan" <Allan at STATS.uct.ac.za>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, July 29, 2005 1:30 PM
Subject: [R] R: graphics devices


>a simple question
>
> how does one produce plots on two different graphics devices?
>
> /
> allan


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jul 29 14:28:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Jul 2005 13:28:16 +0100 (BST)
Subject: [R] Console not found
In-Reply-To: <42EA1AAB.5090802@eawag.ch>
References: <42EA1AAB.5090802@eawag.ch>
Message-ID: <Pine.LNX.4.61.0507291322560.9557@gannet.stats>

Sounds like you have corrupted your Rconsole file.

Fire up rterm and try ?Rconsole and/or read the rw-FAQ to find the file 
that it is use.  It is not removed by uninstalling, and you do need to 
remove it.

(The console settings have nothing whatsoever to do with memory settings.)

Your matrices are not particularly large, BTW, provided you have 1GB or 
more of RAM.  If you have much less, adding some RAM is the most effective 
way of using R.


On Fri, 29 Jul 2005, Manuel Schneider wrote:

> I played around with memory limits in R 2.1.0 under XP in order to be
> able to work with large matrixes (3600x4100). Among several things I
> tried was to alter console settings and saving them.
> Since then, I can't restart Rgui. It says several times 'Console not
> found' with pieces of the text that usually appears in the console and
> then crashes. Rterm.exe works fine.
> I've now unistalled R 2.1.0 and installed R 2.1.1 with no effect, still
> console is not found.
> Any clues on this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Fri Jul 29 14:51:33 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 29 Jul 2005 20:51:33 +0800
Subject: [R] R: graphics devices
Message-ID: <0IKE00GB939XLU@mail.fudan.edu.cn>

That's interesting,how can I see the help document of "graphics.record" option? I use ?options ,but can not find anything about it.


>Hi Allan,
>
>in case of many plots in the same window you can try this:
>
>options(graphics.record=TRUE)
>plot(x)
>plot(x,y)
>
>with PgUp and PgDn you check both in the same window.
>
>I hope this helps.
>
>Roula
>
>=================
>Spyridoula Tsonaka
>Doctoral Student
>Biostatistical Centre
>Catholic University of Leuven
>Kapucijnenvoer 35
>B-3000 Leuven
>Belgium
>Tel: +32/16/336899
>Fax: +32/16/337015
>
>
>----- Original Message ----- 
>From: "Clark Allan" <Allan at STATS.uct.ac.za>
>To: <r-help at stat.math.ethz.ch>
>Sent: Friday, July 29, 2005 1:30 PM
>Subject: [R] R: graphics devices
>
>
>>a simple question
>>
>> how does one produce plots on two different graphics devices?
>>
>> /
>> allan


 

2005-07-29

------
Deparment of Sociology
Fudan University

Blog:http://sociology.yculblog.com



From f.harrell at vanderbilt.edu  Fri Jul 29 15:25:35 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 29 Jul 2005 09:25:35 -0400
Subject: [R] Binary outcome with non-absorbing outcome state
In-Reply-To: <s2e9d805.062@grecc.umaryland.edu>
References: <s2e9d805.062@grecc.umaryland.edu>
Message-ID: <42EA2E4F.0@vanderbilt.edu>

John Sorkin wrote:
> I am trying to model data in which subjects are followed through time to
> determine if they fall, or do not fall. Some of the subjects fall once,
> some fall several times. Follow-up time varies from subject to subject.
> I know how to model time to the first fall (e.g. Cox Proportional
> Hazards, Kaplan-Meir analyses, etc.) but I am not sure how I can model
> the data if I include the data for those subjects who fall more than
> once. I would appreciate suggestions about a models that I could use,
> how I would quantify the follow-up time, how I account for the imbalance
> in the data (some subjects would contribute one outcome measure, others
> multiple measures), etc.
>  
> Many thanks,
> John   

A great reference for this is

@Book{the00mod,
   author =               {Therneau, Terry and Grambsch, Patricia},
   title =                {Modeling Survival Data: Extending the Cox Model},
   publisher =    {Springer-Verlag},
   year =                 2000,
   address =              {New York}
}

Frank

>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From kjetil at acelerate.com  Fri Jul 29 15:24:49 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 29 Jul 2005 09:24:49 -0400
Subject: [R] Binary outcome with non-absorbing outcome state
In-Reply-To: <s2e9d805.062@grecc.umaryland.edu>
References: <s2e9d805.062@grecc.umaryland.edu>
Message-ID: <42EA2E21.2000309@acelerate.com>

John Sorkin wrote:

>I am trying to model data in which subjects are followed through time to
>determine if they fall, or do not fall. Some of the subjects fall once,
>some fall several times. Follow-up time varies from subject to subject.
>I know how to model time to the first fall (e.g. Cox Proportional
>Hazards, Kaplan-Meir analyses, etc.) but I am not sure how I can model
>the data if I include the data for those subjects who fall more than
>once. I would appreciate suggestions about a models that I could use,
>how I would quantify the follow-up time, how I account for the imbalance
>in the data (some subjects would contribute one outcome measure, others
>multiple measures), etc.
> 
>Many thanks,
>John   
> 
>John Sorkin M.D., Ph.D.
>Chief, Biostatistics and Informatics
>Baltimore VA Medical Center GRECC and
>University of Maryland School of Medicine Claude Pepper OAIC
> 
>University of Maryland School of Medicine
>Division of Gerontology
>Baltimore VA Medical Center
>10 North Greene Street
>GRECC (BT/18/GR)
>Baltimore, MD 21201-1524
> 
>410-605-7119 
>-- NOTE NEW EMAIL ADDRESS:
>jsorkin at grecc.umaryland.edu
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>
help.search("recurrent")
leads to CRAN pakage   survrec.
You couls also have a look at CRAN package eha and at 
Lindsey's package event

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ute.visser at ufz.de  Fri Jul 29 15:43:15 2005
From: ute.visser at ufz.de (Ute Visser)
Date: Fri, 29 Jul 2005 15:43:15 +0200
Subject: [R] Errbar()-function, cap and logarithmic scaling
Message-ID: <000d01c59443$78d96540$6233418d@oesa.leipzig.ufz.de>

Hello!

If I use the errbar-function and have a logarithmic scale on the x-axis,
then the little horizontal bars at the end of the errbars (cap)
disappear. What can I do?

Thanks for helping!
Ute



From frankfunder at earthlink.net  Fri Jul 29 15:43:42 2005
From: frankfunder at earthlink.net (Frank Funderburk)
Date: Fri, 29 Jul 2005 09:43:42 -0400 (GMT-04:00)
Subject: [R] Binary outcome with non-absorbing outcome state
Message-ID: <881789.1122644622975.JavaMail.root@wamui-karabash.atl.sa.earthlink.net>

Singer & Willett (2003) also cover this ground.

Singer, JD & Willett, JB (2003).  Applied longitudinal data analysis:  Modeling change and event occurrence. New Yok:  Oxford University Press.

-----Original Message-----
From: Frank E Harrell Jr <f.harrell at vanderbilt.edu>
Sent: Jul 29, 2005 9:25 AM
To: John Sorkin <jsorkin at grecc.umaryland.edu>
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Binary outcome with non-absorbing outcome state

John Sorkin wrote:
> I am trying to model data in which subjects are followed through time to
> determine if they fall, or do not fall. Some of the subjects fall once,
> some fall several times. Follow-up time varies from subject to subject.
> I know how to model time to the first fall (e.g. Cox Proportional
> Hazards, Kaplan-Meir analyses, etc.) but I am not sure how I can model
> the data if I include the data for those subjects who fall more than
> once. I would appreciate suggestions about a models that I could use,
> how I would quantify the follow-up time, how I account for the imbalance
> in the data (some subjects would contribute one outcome measure, others
> multiple measures), etc.
>  
> Many thanks,
> John   

A great reference for this is

@Book{the00mod,
   author =               {Therneau, Terry and Grambsch, Patricia},
   title =                {Modeling Survival Data: Extending the Cox Model},
   publisher =    {Springer-Verlag},
   year =                 2000,
   address =              {New York}
}

Frank

>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> --- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Frank Funderburk
 
Converting Data to 
... Information for Action
.........................Through Understanding
 
frankfunder at earthlink.net

voice mail:  888-431-7594



From murdoch at stats.uwo.ca  Fri Jul 29 15:47:45 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 29 Jul 2005 09:47:45 -0400
Subject: [R] Console not found
In-Reply-To: <42EA1AAB.5090802@eawag.ch>
References: <42EA1AAB.5090802@eawag.ch>
Message-ID: <42EA3381.4030200@stats.uwo.ca>

On 7/29/2005 8:01 AM, Manuel Schneider wrote:
> I played around with memory limits in R 2.1.0 under XP in order to be 
> able to work with large matrixes (3600x4100). Among several things I 
> tried was to alter console settings and saving them.
> Since then, I can't restart Rgui. It says several times 'Console not 
> found' with pieces of the text that usually appears in the console and 
> then crashes. Rterm.exe works fine.
> I've now unistalled R 2.1.0 and installed R 2.1.1 with no effect, still 
> console is not found.
> Any clues on this?

You have probably got something bad in your startup files.  See 
appendices B.1 and B.2 of the R-intro manual for all the details.  In 
summary, R looks in Renviron.site, Rprofile.site, .Rprofile, .RData and 
Rconsole for startup information.  (Where it looks is complicated; see 
the manual.) There are command line options to tell it to skip these; in 
particular, --vanilla tells it to skip all of them.

I'd guess your problem is with Rconsole, because that's where the 
console settings are normally saved.  Rename it to something else and 
your problems should go away.

Duncan Murdoch



From macq at llnl.gov  Fri Jul 29 16:19:55 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 29 Jul 2005 07:19:55 -0700
Subject: [R] R: graphics devices
In-Reply-To: <42EA136E.570529EE@STATS.uct.ac.za>
References: <42EA136E.570529EE@STATS.uct.ac.za>
Message-ID: <p06210202bf0fe9a5eaa7@[128.115.153.6]>

I'd suggest starting with the documentation. See

   ?Devices

and then look at some of the other functions referenced in the "See 
Also" section of ?Devices.

Basically, you produce plots on two different graphics devices by 
this sequence:
    1) open a graphics device
    2) produce a plot
    3) open another graphics device
    4) produce a plot
and depending on which graphic device(s) you are using you may have 
follow with dev.off() to actually get any useful result.

And of course there is always the documentation you can download from 
the R website. Specifically, the one titled
   "An Introduction to R"
in which I found a section titled
    "Multiple graphics devices"
which starts with
  "In advanced use of R it is often useful to have several graphics 
devices in use at the same time."
(this was from the R 2.0.1 version of the manual, to be precise)

-Don

At 1:30 PM +0200 7/29/05, Clark Allan wrote:
>a simple question
>
>how does one produce plots on two different graphics devices?
>
>/
>allan
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ripley at stats.ox.ac.uk  Fri Jul 29 17:09:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Jul 2005 16:09:13 +0100 (BST)
Subject: [R] R: graphics devices
In-Reply-To: <0IKE00GB939XLU@mail.fudan.edu.cn>
References: <0IKE00GB939XLU@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0507291604410.11317@gannet.stats>

This is for Windows only, and documented in

?windows
README.rw2011 (or whatever)

It's more usual to switch recording on when you need it, either in the 
windows() call or from a menu.

?options only tells you about the standard options, not those used on 
specific platforms or by packages (and it does say that, in `Options used 
in base R').

On Fri, 29 Jul 2005, ronggui wrote:

> That's interesting,how can I see the help document of "graphics.record" 
> option? I use ?options ,but can not find anything about it.
>
>
>> Hi Allan,
>>
>> in case of many plots in the same window you can try this:
>>
>> options(graphics.record=TRUE)
>> plot(x)
>> plot(x,y)
>>
>> with PgUp and PgDn you check both in the same window.
>>
>> I hope this helps.
>>
>> Roula
>>
>> =================
>> Spyridoula Tsonaka
>> Doctoral Student
>> Biostatistical Centre
>> Catholic University of Leuven
>> Kapucijnenvoer 35
>> B-3000 Leuven
>> Belgium
>> Tel: +32/16/336899
>> Fax: +32/16/337015
>>
>>
>> ----- Original Message -----
>> From: "Clark Allan" <Allan at STATS.uct.ac.za>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Friday, July 29, 2005 1:30 PM
>> Subject: [R] R: graphics devices
>>
>>
>>> a simple question
>>>
>>> how does one produce plots on two different graphics devices?
>>>
>>> /
>>> allan
>
>
>
>
> 2005-07-29
>
> ------
> Deparment of Sociology
> Fudan University
>
> Blog:http://sociology.yculblog.com
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dieter.menne at menne-biomed.de  Fri Jul 29 17:26:16 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 29 Jul 2005 15:26:16 +0000 (UTC)
Subject: [R] Binary outcome with non-absorbing outcome state
References: <s2e9d805.062@grecc.umaryland.edu>
Message-ID: <loom.20050729T172538-475@post.gmane.org>

John Sorkin <jsorkin <at> grecc.umaryland.edu> writes:

> 
> I am trying to model data in which subjects are followed through time to
> determine if they fall, or do not fall. Some of the subjects fall once,
> some fall several times. Follow-up time varies from subject to subject.

Chapter 4.3 in 

http://www.mayo.edu/hsr/people/therneau/survival.ps

might also help.

Dieter



From HDoran at air.org  Fri Jul 29 17:44:35 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 29 Jul 2005 11:44:35 -0400
Subject: [R] Error Downloading Matrix Package
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050729/4eac6866/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Jul 29 17:56:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Jul 2005 17:56:55 +0200
Subject: [R] Error Downloading Matrix Package
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>
Message-ID: <42EA51C7.20300@statistik.uni-dortmund.de>

Doran, Harold wrote:
> I'm trying to update my Matrix package given the update last night. But
> the following error is generated. I've tried restarting R and deleting
> my old Matrix package. Can anyone suggest how this might be resolved? 

Do you have write permission on the library into which you are trying to 
install package Matrix? In particular, have you really deleted the 
package from the right library?
Do you have the package loaded (R should tell it, hence I do not believe 
this is the problem)? If this is a library used by multiple users, does 
any of those users have the package loaded?

Uwe Ligges


> 
>>install.packages('Matrix')
> 
> trying URL
> 'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.1/Mat
> rix_0.98-1.zip'
> Content type 'application/zip' length 891288 bytes
> opened URL
> downloaded 870Kb
> 
> package 'Matrix' successfully unpacked and MD5 sums checked
> Error: cannot remove prior installation of package 'Matrix'
> 
> 
> 
>>traceback()
> 
> 4: stop(sprintf(gettext("cannot remove prior installation of package
> '%s'"), 
>        curPkg), domain = NA, call. = FALSE)
> 3: unpackPkg(foundpkgs[okp, 2], foundpkgs[okp, 1], lib, installWithVers)
> 2: .install.winbinary(pkgs = pkgs, lib = lib, contriburl = contriburl, 
>        method = method, available = available, destdir = destdir, 
>        installWithVers = installWithVers, dependencies = dependencies)
> 1: install.packages("Matrix")
> 
> 
> This is on a Windows XP machine for R 2.11
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jul 29 17:57:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Jul 2005 16:57:57 +0100 (BST)
Subject: [R] Error Downloading Matrix Package
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>
Message-ID: <Pine.LNX.4.61.0507291654280.12027@gannet.stats>

Possible answers:

1) You do not have permission to remove the package from its previous 
location.

2) Windows mistakenly thinks that some file in the package is still open.

Try deleting the directory from Windows Explorer.  If 2) you may have to 
log out or reboot Windows before you can do so.

BTW, my Windows setup updated successfully this morning, so this is indeed 
a local problem.


On Fri, 29 Jul 2005, Doran, Harold wrote:

> I'm trying to update my Matrix package given the update last night. But
> the following error is generated. I've tried restarting R and deleting
> my old Matrix package. Can anyone suggest how this might be resolved?
>
>> install.packages('Matrix')
> trying URL
> 'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.1/Mat
> rix_0.98-1.zip'
> Content type 'application/zip' length 891288 bytes
> opened URL
> downloaded 870Kb
>
> package 'Matrix' successfully unpacked and MD5 sums checked
> Error: cannot remove prior installation of package 'Matrix'
>
>
>> traceback()
> 4: stop(sprintf(gettext("cannot remove prior installation of package
> '%s'"),
>       curPkg), domain = NA, call. = FALSE)
> 3: unpackPkg(foundpkgs[okp, 2], foundpkgs[okp, 1], lib, installWithVers)
> 2: .install.winbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
>       method = method, available = available, destdir = destdir,
>       installWithVers = installWithVers, dependencies = dependencies)
> 1: install.packages("Matrix")
>
>
> This is on a Windows XP machine for R 2.11
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HDoran at air.org  Fri Jul 29 18:01:21 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 29 Jul 2005 12:01:21 -0400
Subject: [R] Error Downloading Matrix Package
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7409C120CC@dc1ex2.air.org>

After restarting Windows Matrix was properly updated. Not quite sure
where the error was, but it is certainly local. 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, July 29, 2005 11:58 AM
To: Doran, Harold
Cc: r-help at stat.math.ethz.ch; bates at stat.wisc.edu
Subject: Re: [R] Error Downloading Matrix Package

Possible answers:

1) You do not have permission to remove the package from its previous
location.

2) Windows mistakenly thinks that some file in the package is still
open.

Try deleting the directory from Windows Explorer.  If 2) you may have to
log out or reboot Windows before you can do so.

BTW, my Windows setup updated successfully this morning, so this is
indeed a local problem.


On Fri, 29 Jul 2005, Doran, Harold wrote:

> I'm trying to update my Matrix package given the update last night.
But
> the following error is generated. I've tried restarting R and deleting
> my old Matrix package. Can anyone suggest how this might be resolved?
>
>> install.packages('Matrix')
> trying URL
>
'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.1/Mat
> rix_0.98-1.zip'
> Content type 'application/zip' length 891288 bytes
> opened URL
> downloaded 870Kb
>
> package 'Matrix' successfully unpacked and MD5 sums checked
> Error: cannot remove prior installation of package 'Matrix'
>
>
>> traceback()
> 4: stop(sprintf(gettext("cannot remove prior installation of package
> '%s'"),
>       curPkg), domain = NA, call. = FALSE)
> 3: unpackPkg(foundpkgs[okp, 2], foundpkgs[okp, 1], lib,
installWithVers)
> 2: .install.winbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
>       method = method, available = available, destdir = destdir,
>       installWithVers = installWithVers, dependencies = dependencies)
> 1: install.packages("Matrix")
>
>
> This is on a Windows XP machine for R 2.11
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Fri Jul 29 18:03:27 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Jul 2005 09:03:27 -0700
Subject: [R] replace matrix values with names from a dataframe
In-Reply-To: <1122558749.42e8e31dd4eb5@webmail.fas.harvard.edu>
References: <mailman.9.1122544801.6372.r-help@stat.math.ethz.ch>
	<1122558749.42e8e31dd4eb5@webmail.fas.harvard.edu>
Message-ID: <42EA534F.2020109@pdf.com>

	  Does the following help:

 > set.seed(1)
 > Lvls <- factor(letters[1:4])
 > A <- array(sample(4, 6, replace=TRUE), dim=c(2,3))
 > A[] <- levels(Lvls)[A]
 > A
      [,1] [,2] [,3]
[1,] "b"  "c"  "a"
[2,] "b"  "d"  "d"
 >

	  If not, PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html" and submit another 
question (if the process of working the posting guide does not itself 
provide enlightenment).

	  spencer graves

jhainm at fas.harvard.edu wrote:

> Hi,
> 
> I am looking for a way to replace matrix values with names from a dataframe.
> 
> Let me do this by example: I have a dataframe:
> 
> 
>>data
> 
>   city.name
> 1    munich
> 2     paris
> 3     tokio
> 4    london
> 5    boston
> 
> each city name corresponds to only one index number (there is only one
> observation for each city). After doing some matching I end up with a matrix
> that looks something like this:
> 
> 
>>X
> 
>        [,1] [,2]
>   [1,]    2    4
>   [2,]    5    1
>   [3,]    5    3
>   [4,]   12  217
>   [5,]   16   13
> 
> Here the numbers in the matrix are the index numbers from my original dataset,
> each row is a matched pair (so e.g. the first row tells me that obs. number 2
> (i.e. Paris) was matched to obs number 4 (i.e. London)).
> 
> Now I am looking for a quick way to transform the index numbers back to city
> names, so that at the end I have a matrix that looks something like this:
> 
> 
>>X.transformed
> 
>          [,1]     [,2]
>   [1,]  paris   london
>   [2,] boston   munich
>   [3,] boston    tokio
>   [4,]     12      217
>   [5,]     16       13
> 
> etc. So instead of the index number, the matrix should contain the names that
> corresponds to it. In my real data, I have many many names and replacing each
> value by hand would take too long. Any help is highly appreciated.
> 
> Thank you.
> 
> Regards,
> Jens
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Jul 29 18:06:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Jul 2005 09:06:08 -0700
Subject: [R] Error Downloading Matrix Package
In-Reply-To: <Pine.LNX.4.61.0507291654280.12027@gannet.stats>
References: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>
	<Pine.LNX.4.61.0507291654280.12027@gannet.stats>
Message-ID: <42EA53F0.8030601@pdf.com>

	  I've gotten that error message when trying to update a package I had 
attached.  oops.  (like do-it-yourself lobotomy.)  Then I had to 
"install.packages", because much of it got deleted and it wouldn't work 
any more.
	
	  spencer graves

Prof Brian Ripley wrote:

> Possible answers:
> 
> 1) You do not have permission to remove the package from its previous 
> location.
> 
> 2) Windows mistakenly thinks that some file in the package is still open.
> 
> Try deleting the directory from Windows Explorer.  If 2) you may have to 
> log out or reboot Windows before you can do so.
> 
> BTW, my Windows setup updated successfully this morning, so this is indeed 
> a local problem.
> 
> 
> On Fri, 29 Jul 2005, Doran, Harold wrote:
> 
> 
>>I'm trying to update my Matrix package given the update last night. But
>>the following error is generated. I've tried restarting R and deleting
>>my old Matrix package. Can anyone suggest how this might be resolved?
>>
>>
>>>install.packages('Matrix')
>>
>>trying URL
>>'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.1/Mat
>>rix_0.98-1.zip'
>>Content type 'application/zip' length 891288 bytes
>>opened URL
>>downloaded 870Kb
>>
>>package 'Matrix' successfully unpacked and MD5 sums checked
>>Error: cannot remove prior installation of package 'Matrix'
>>
>>
>>
>>>traceback()
>>
>>4: stop(sprintf(gettext("cannot remove prior installation of package
>>'%s'"),
>>      curPkg), domain = NA, call. = FALSE)
>>3: unpackPkg(foundpkgs[okp, 2], foundpkgs[okp, 1], lib, installWithVers)
>>2: .install.winbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
>>      method = method, available = available, destdir = destdir,
>>      installWithVers = installWithVers, dependencies = dependencies)
>>1: install.packages("Matrix")
>>
>>
>>This is on a Windows XP machine for R 2.11
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tlumley at u.washington.edu  Fri Jul 29 18:20:35 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 29 Jul 2005 09:20:35 -0700 (PDT)
Subject: [R] Binary outcome with non-absorbing outcome state
In-Reply-To: <s2e9d805.062@grecc.umaryland.edu>
References: <s2e9d805.062@grecc.umaryland.edu>
Message-ID: <Pine.A41.4.61b.0507290915250.178890@homer06.u.washington.edu>

On Fri, 29 Jul 2005, John Sorkin wrote:

> I am trying to model data in which subjects are followed through time to
> determine if they fall, or do not fall. Some of the subjects fall once,
> some fall several times. Follow-up time varies from subject to subject.
> I know how to model time to the first fall (e.g. Cox Proportional
> Hazards, Kaplan-Meir analyses, etc.) but I am not sure how I can model
> the data if I include the data for those subjects who fall more than
> once.

Various people have already given references that deal with marginal Cox 
models. I'd second Frank Harrell's recommendation of Therneau & Grambsch.
  Computationally this is very straightforward: each person has multiple 
records corresponding to the times between events, and in a Cox model you 
add
    +cluster(id)
to the model formula to get the right standard errors, where id is unique 
identifier for individuals.

The difficult part is deciding which person-time to compare: eg should 
someone who has recently had a second event at time 500 be compared to 
other people who have recently had a second event, other people who have 
recently had any sort of event, other people at time 500, etc.

Another possibility is frailty models, the analogue of generalized linear 
mixed models.  As with GLMMs, even fitting these is tricky and statistical 
theory isn't that well-developed. The survival package does have an 
implementation, though.


 	-thomas



From tlumley at u.washington.edu  Fri Jul 29 18:23:04 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 29 Jul 2005 09:23:04 -0700 (PDT)
Subject: [R] Binary outcome with non-absorbing outcome state
In-Reply-To: <loom.20050729T172538-475@post.gmane.org>
References: <s2e9d805.062@grecc.umaryland.edu>
	<loom.20050729T172538-475@post.gmane.org>
Message-ID: <Pine.A41.4.61b.0507290922210.178890@homer06.u.washington.edu>

On Fri, 29 Jul 2005, Dieter Menne wrote:

> John Sorkin <jsorkin <at> grecc.umaryland.edu> writes:
>
>>
>> I am trying to model data in which subjects are followed through time to
>> determine if they fall, or do not fall. Some of the subjects fall once,
>> some fall several times. Follow-up time varies from subject to subject.
>
> Chapter 4.3 in
>
> http://www.mayo.edu/hsr/people/therneau/survival.ps
>
> might also help.
>

That document is included in the survival package, but AFAICS it 
doesn't talk about multiple events, just about time-varying covariates.

 	-thomas



From ripley at stats.ox.ac.uk  Fri Jul 29 18:34:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Jul 2005 17:34:32 +0100 (BST)
Subject: [R] Error Downloading Matrix Package
In-Reply-To: <42EA53F0.8030601@pdf.com>
References: <88EAF3512A55DF46B06B1954AEF73F7409C120C2@dc1ex2.air.org>
	<Pine.LNX.4.61.0507291654280.12027@gannet.stats>
	<42EA53F0.8030601@pdf.com>
Message-ID: <Pine.LNX.4.61.0507291730370.12432@gannet.stats>

You should have got a different error message, namely

 	package some_pkg is in use and will not be installed

and that is what I got when I tried it.


On Fri, 29 Jul 2005, Spencer Graves wrote:

> 	  I've gotten that error message when trying to update a package I 
> had attached.  oops.  (like do-it-yourself lobotomy.)  Then I had to 
> "install.packages", because much of it got deleted and it wouldn't work any 
> more.
> 		  spencer graves
>
> Prof Brian Ripley wrote:
>
>> Possible answers:
>> 
>> 1) You do not have permission to remove the package from its previous 
>> location.
>> 
>> 2) Windows mistakenly thinks that some file in the package is still open.
>> 
>> Try deleting the directory from Windows Explorer.  If 2) you may have to 
>> log out or reboot Windows before you can do so.
>> 
>> BTW, my Windows setup updated successfully this morning, so this is indeed 
>> a local problem.
>> 
>> 
>> On Fri, 29 Jul 2005, Doran, Harold wrote:
>> 
>> 
>>> I'm trying to update my Matrix package given the update last night. But
>>> the following error is generated. I've tried restarting R and deleting
>>> my old Matrix package. Can anyone suggest how this might be resolved?
>>> 
>>> 
>>>> install.packages('Matrix')
>>> 
>>> trying URL
>>> 'http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.1/Mat
>>> rix_0.98-1.zip'
>>> Content type 'application/zip' length 891288 bytes
>>> opened URL
>>> downloaded 870Kb
>>> 
>>> package 'Matrix' successfully unpacked and MD5 sums checked
>>> Error: cannot remove prior installation of package 'Matrix'
>>> 
>>> 
>>> 
>>>> traceback()
>>> 
>>> 4: stop(sprintf(gettext("cannot remove prior installation of package
>>> '%s'"),
>>>      curPkg), domain = NA, call. = FALSE)
>>> 3: unpackPkg(foundpkgs[okp, 2], foundpkgs[okp, 1], lib, installWithVers)
>>> 2: .install.winbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
>>>      method = method, available = available, destdir = destdir,
>>>      installWithVers = installWithVers, dependencies = dependencies)
>>> 1: install.packages("Matrix")
>>> 
>>> 
>>> This is on a Windows XP machine for R 2.11
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>> 
>> 
>> 
>
> -- 
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From caobg at email.uc.edu  Fri Jul 29 20:06:46 2005
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 29 Jul 2005 14:06:46 -0400
Subject: [R] about usage of weights in nnet
Message-ID: <200507291806.CPR93144@mirapoint.uc.edu>

Hi There,
    I got some results from using nnet on a two-class problem, and I'd like to hear your comments to understand well about the algorithm. In the training set, the ratio of class 1 to class 2 is about 23:77. I did a 5-fold cross validation. The networks were trained twice, one with 'weights=1', one with 'weights=ifelse(species=="class1", 77/33, 1)'(pointed out by Prof. Brian Ripley).All other settings are same. The average Matthew Correlation Coeffience for the one with weights=1 is 0.80, significantly larger than that of the other, 0.74. So, it seems weighting the unbalanced samples does not help performance on evaluations, which is against my initial thoughts. My question would be, does that mean the training data is not unbalanced enough? then how unbalanced is enough? Or it was totally just a signal event? Or it was just some suboptimal results? Any references regarding this issue in particular? Thanks!

Best regards,                 
	Baoqiang Cao



From gunter.berton at gene.com  Fri Jul 29 20:13:59 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 29 Jul 2005 11:13:59 -0700
Subject: [R] R: graphics devices
In-Reply-To: <Pine.LNX.4.61.0507291604410.11317@gannet.stats>
Message-ID: <200507291814.j6TIE0Js023926@faraday.gene.com>

I put the options() call into my Rprofile.site code (there are other ways to
do this, too) so that it's automatic at startup. ?Startup documents other
ways you can do this. I've never run into problems having recording always
"on", but maybe there are some I'm unaware of.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Friday, July 29, 2005 8:09 AM
> To: ronggui
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R: graphics devices
> 
> This is for Windows only, and documented in
> 
> ?windows
> README.rw2011 (or whatever)
> 
> It's more usual to switch recording on when you need it, 
> either in the 
> windows() call or from a menu.
> 
> ?options only tells you about the standard options, not those used on 
> specific platforms or by packages (and it does say that, in 
> `Options used 
> in base R').
> 
> On Fri, 29 Jul 2005, ronggui wrote:
> 
> > That's interesting,how can I see the help document of 
> "graphics.record" 
> > option? I use ?options ,but can not find anything about it.
> >
> >
> >> Hi Allan,
> >>
> >> in case of many plots in the same window you can try this:
> >>
> >> options(graphics.record=TRUE)
> >> plot(x)
> >> plot(x,y)
> >>
> >> with PgUp and PgDn you check both in the same window.
> >>
> >> I hope this helps.
> >>
> >> Roula
> >>
> >> =================
> >> Spyridoula Tsonaka
> >> Doctoral Student
> >> Biostatistical Centre
> >> Catholic University of Leuven
> >> Kapucijnenvoer 35
> >> B-3000 Leuven
> >> Belgium
> >> Tel: +32/16/336899
> >> Fax: +32/16/337015
> >>
> >>
> >> ----- Original Message -----
> >> From: "Clark Allan" <Allan at STATS.uct.ac.za>
> >> To: <r-help at stat.math.ethz.ch>
> >> Sent: Friday, July 29, 2005 1:30 PM
> >> Subject: [R] R: graphics devices
> >>
> >>
> >>> a simple question
> >>>
> >>> how does one produce plots on two different graphics devices?
> >>>
> >>> /
> >>> allan
> >
> >
> >
> >
> > 2005-07-29
> >
> > ------
> > Deparment of Sociology
> > Fudan University
> >
> > Blog:http://sociology.yculblog.com
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From thchung at tgen.org  Fri Jul 29 21:13:16 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Fri, 29 Jul 2005 12:13:16 -0700
Subject: [R] Way to make R idle for some time and try something again later
Message-ID: <BF0FCDDC.6C9B%thchung@tgen.org>

Hi, All;

I have a question. In R, what is the best way to make R idle for a while and
try something again later? For example, suppose there is an R job which
accesses a file that may be shared with other active jobs. So when the file
is being accessed by other job, your job will not be able to access the file
and your job will crash because of that. To avoid this, you want your job to
try to access the file repeatedly with some time interval, say every 10
seconds or something like that. Which is the best way to do this in R?

Thanks in advance,

Tae-Hoon Chung

Post-Doctoral Researcher
Translational Genomics Research Institute (TGen)
445 N. 5th Street (Suite 530)
Phoenix, AZ 85004
1-602-343-8724 (Direct)
1-480-323-9820 (Mobile)
1-602-343-8840 (Fax)



From murdoch at stats.uwo.ca  Fri Jul 29 21:33:20 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 29 Jul 2005 15:33:20 -0400
Subject: [R] Way to make R idle for some time and try something again
 later
In-Reply-To: <BF0FCDDC.6C9B%thchung@tgen.org>
References: <BF0FCDDC.6C9B%thchung@tgen.org>
Message-ID: <42EA8480.8080407@stats.uwo.ca>

On 7/29/2005 3:13 PM, Tae-Hoon Chung wrote:
> Hi, All;
> 
> I have a question. In R, what is the best way to make R idle for a while and
> try something again later? For example, suppose there is an R job which
> accesses a file that may be shared with other active jobs. So when the file
> is being accessed by other job, your job will not be able to access the file
> and your job will crash because of that. To avoid this, you want your job to
> try to access the file repeatedly with some time interval, say every 10
> seconds or something like that. Which is the best way to do this in R?

Sys.sleep(10) should give you a 10 second pause with very little impact 
on the system.

Duncan Murdoch



From ramasamy at cancer.org.uk  Fri Jul 29 21:35:05 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 29 Jul 2005 20:35:05 +0100
Subject: [R] Way to make R idle for some time and try something
	again	later
In-Reply-To: <BF0FCDDC.6C9B%thchung@tgen.org>
References: <BF0FCDDC.6C9B%thchung@tgen.org>
Message-ID: <1122665705.5939.2.camel@dhcppc3>

Which operating system are you using ?

See help(Sys.sleep), which might be what you want but there may be other
ways in determining if a file is being accessed by another program.

Regards, Adai



On Fri, 2005-07-29 at 12:13 -0700, Tae-Hoon Chung wrote:
> Hi, All;
> 
> I have a question. In R, what is the best way to make R idle for a while and
> try something again later? For example, suppose there is an R job which
> accesses a file that may be shared with other active jobs. So when the file
> is being accessed by other job, your job will not be able to access the file
> and your job will crash because of that. To avoid this, you want your job to
> try to access the file repeatedly with some time interval, say every 10
> seconds or something like that. Which is the best way to do this in R?
> 
> Thanks in advance,
> 
> Tae-Hoon Chung
> 
> Post-Doctoral Researcher
> Translational Genomics Research Institute (TGen)
> 445 N. 5th Street (Suite 530)
> Phoenix, AZ 85004
> 1-602-343-8724 (Direct)
> 1-480-323-9820 (Mobile)
> 1-602-343-8840 (Fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Fri Jul 29 21:35:05 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 29 Jul 2005 20:35:05 +0100
Subject: [R] Way to make R idle for some time and try something
	again	later
In-Reply-To: <BF0FCDDC.6C9B%thchung@tgen.org>
References: <BF0FCDDC.6C9B%thchung@tgen.org>
Message-ID: <1122665705.5939.2.camel@dhcppc3>

Which operating system are you using ?

See help(Sys.sleep), which might be what you want but there may be other
ways in determining if a file is being accessed by another program.

Regards, Adai



On Fri, 2005-07-29 at 12:13 -0700, Tae-Hoon Chung wrote:
> Hi, All;
> 
> I have a question. In R, what is the best way to make R idle for a while and
> try something again later? For example, suppose there is an R job which
> accesses a file that may be shared with other active jobs. So when the file
> is being accessed by other job, your job will not be able to access the file
> and your job will crash because of that. To avoid this, you want your job to
> try to access the file repeatedly with some time interval, say every 10
> seconds or something like that. Which is the best way to do this in R?
> 
> Thanks in advance,
> 
> Tae-Hoon Chung
> 
> Post-Doctoral Researcher
> Translational Genomics Research Institute (TGen)
> 445 N. 5th Street (Suite 530)
> Phoenix, AZ 85004
> 1-602-343-8724 (Direct)
> 1-480-323-9820 (Mobile)
> 1-602-343-8840 (Fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Fri Jul 29 21:44:51 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 29 Jul 2005 12:44:51 -0700
Subject: [R] Way to make R idle for some time and try something again
	later
In-Reply-To: <BF0FCDDC.6C9B%thchung@tgen.org>
Message-ID: <200507291944.j6TJipkV018975@hertz.gene.com>

Dear Tae-Hoon:

1. RSiteSearch('wait') will tell you that Sys.sleep() is what you want.
Although this list is terrific, R's built-in help/search tools should always
be tried first (they're faster when you hit the right search term).

2. The other part of the puzzle is ?try

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tae-Hoon Chung
> Sent: Friday, July 29, 2005 12:13 PM
> To: RHelp
> Subject: [R] Way to make R idle for some time and try 
> something again later
> 
> Hi, All;
> 
> I have a question. In R, what is the best way to make R idle 
> for a while and
> try something again later? For example, suppose there is an R 
> job which
> accesses a file that may be shared with other active jobs. So 
> when the file
> is being accessed by other job, your job will not be able to 
> access the file
> and your job will crash because of that. To avoid this, you 
> want your job to
> try to access the file repeatedly with some time interval, 
> say every 10
> seconds or something like that. Which is the best way to do this in R?
> 
> Thanks in advance,
> 
> Tae-Hoon Chung
> 
> Post-Doctoral Researcher
> Translational Genomics Research Institute (TGen)
> 445 N. 5th Street (Suite 530)
> Phoenix, AZ 85004
> 1-602-343-8724 (Direct)
> 1-480-323-9820 (Mobile)
> 1-602-343-8840 (Fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From macq at llnl.gov  Sat Jul 30 00:52:48 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 29 Jul 2005 15:52:48 -0700
Subject: [R] Way to make R idle for some time and try something again
 later
In-Reply-To: <BF0FCDDC.6C9B%thchung@tgen.org>
References: <BF0FCDDC.6C9B%thchung@tgen.org>
Message-ID: <p06210209bf105ab36a33@[128.115.153.6]>

I done something very similar -- have R watch a file, and whenever 
new data is added to the file, read the new data from the file. In my 
case, new data was arriving once per minute, so I needed to have R 
wait about a minute before looking for new data.

On my unix-based system, I found that if I usd
       Sys.sleep( N )
then cpu usage immediately went up drastically. If the the system is 
otherwise fairly idle, cpu usage goes up to nearly 100%. A cpu 
monitor shows that R is using the cpu cycles.

If I use instead
      system('sleep N')
cpu usage does not go up.

(where N is the number of seconds to sleep)

>  version
          _                       
platform powerpc-apple-darwin7.9.0
arch     powerpc                 
os       darwin7.9.0             
system   powerpc, darwin7.9.0    
status                           
major    2                       
minor    1.1                     
year     2005                    
month    06                      
day      20                      
language R                       


At 12:13 PM -0700 7/29/05, Tae-Hoon Chung wrote:
>Hi, All;
>
>I have a question. In R, what is the best way to make R idle for a while and
>try something again later? For example, suppose there is an R job which
>accesses a file that may be shared with other active jobs. So when the file
>is being accessed by other job, your job will not be able to access the file
>and your job will crash because of that. To avoid this, you want your job to
>try to access the file repeatedly with some time interval, say every 10
>seconds or something like that. Which is the best way to do this in R?
>
>Thanks in advance,
>
>Tae-Hoon Chung
>
>Post-Doctoral Researcher
>Translational Genomics Research Institute (TGen)
>445 N. 5th Street (Suite 530)
>Phoenix, AZ 85004
>1-602-343-8724 (Direct)
>1-480-323-9820 (Mobile)
>1-602-343-8840 (Fax)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From murdoch at stats.uwo.ca  Sat Jul 30 01:33:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 29 Jul 2005 19:33:50 -0400
Subject: [R] Way to make R idle for some time and try something again
 later
In-Reply-To: <p06210209bf105ab36a33@[128.115.153.6]>
References: <BF0FCDDC.6C9B%thchung@tgen.org>
	<p06210209bf105ab36a33@[128.115.153.6]>
Message-ID: <42EABCDE.7030709@stats.uwo.ca>

Don MacQueen wrote:
> I done something very similar -- have R watch a file, and whenever 
> new data is added to the file, read the new data from the file. In my 
> case, new data was arriving once per minute, so I needed to have R 
> wait about a minute before looking for new data.
> 
> On my unix-based system, I found that if I usd
>        Sys.sleep( N )
> then cpu usage immediately went up drastically. If the the system is 
> otherwise fairly idle, cpu usage goes up to nearly 100%. A cpu 
> monitor shows that R is using the cpu cycles.

This is system-specific.  In Windows, CPU usage measures in at 0% while 
in a Sys.sleep loop.  (It's not really zero, because R checks for events 
to update the display, but it's very low).

Duncan Murdoch

> 
> If I use instead
>       system('sleep N')
> cpu usage does not go up.
> 
> (where N is the number of seconds to sleep)
> 
> 
>> version
> 
>           _                       
> platform powerpc-apple-darwin7.9.0
> arch     powerpc                 
> os       darwin7.9.0             
> system   powerpc, darwin7.9.0    
> status                           
> major    2                       
> minor    1.1                     
> year     2005                    
> month    06                      
> day      20                      
> language R                       
> 
> 
> At 12:13 PM -0700 7/29/05, Tae-Hoon Chung wrote:
> 
>>Hi, All;
>>
>>I have a question. In R, what is the best way to make R idle for a while and
>>try something again later? For example, suppose there is an R job which
>>accesses a file that may be shared with other active jobs. So when the file
>>is being accessed by other job, your job will not be able to access the file
>>and your job will crash because of that. To avoid this, you want your job to
>>try to access the file repeatedly with some time interval, say every 10
>>seconds or something like that. Which is the best way to do this in R?
>>
>>Thanks in advance,
>>
>>Tae-Hoon Chung
>>
>>Post-Doctoral Researcher
>>Translational Genomics Research Institute (TGen)
>>445 N. 5th Street (Suite 530)
>>Phoenix, AZ 85004
>>1-602-343-8724 (Direct)
>>1-480-323-9820 (Mobile)
>>1-602-343-8840 (Fax)
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Sat Jul 30 04:53:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Jul 2005 22:53:57 -0400
Subject: [R] Don't understand plotmath behaviour (bug?)
In-Reply-To: <164116838.20050725141338@eimb.ru>
References: <1443722582.20050725121124@eimb.ru>
	<Pine.A41.4.61b.0507251436580.217516@homer04.u.washington.edu>
	<164116838.20050725141338@eimb.ru>
Message-ID: <971536df0507291953f5f8907@mail.gmail.com>

Numbers, not in characters strings do not come out bold:

plot(1:5, type = "n")
text(x=3,y=3, quote(bold(paste("a"==a ~~ "0.5" == 0.5))))


On 7/25/05, Wladimir Eremeev <wl at eimb.ru> wrote:
> Dear Thomas,
> 
> TL> In case 1 you have the string "0.5", in case 2 you have the number 0.5.
> TL>    text(x=2,y=2, quote(bold("0.5"==0.5)))
> TL> shows what is happening.
> 
> I know about the different types in the cases.
> That is, 'bold' affects only on text strings.
> Am I right?
> 
> ---
> Best regards,
> Wladimir                mailto:wl at eimb.ru
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sat Jul 30 07:44:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Jul 2005 06:44:13 +0100 (BST)
Subject: [R] Way to make R idle for some time and try something again
 later
In-Reply-To: <p06210209bf105ab36a33@[128.115.153.6]>
References: <BF0FCDDC.6C9B%thchung@tgen.org>
	<p06210209bf105ab36a33@[128.115.153.6]>
Message-ID: <Pine.LNX.4.61.0507300618320.19789@gannet.stats>

This depends on what else is going on.  My guess is that you are running 
the Aqua GUI, and it is servicing the GUI which is taking the time, not R 
itself.

On all of Linux, Solaris and Windows (RGui or Rterm) Sys.sleep() does use 
very close to zero resources at the beginning of a session, but things may 
be different if e.g. tcltk widgets are in use.

On Fri, 29 Jul 2005, Don MacQueen wrote:

> I done something very similar -- have R watch a file, and whenever
> new data is added to the file, read the new data from the file. In my
> case, new data was arriving once per minute, so I needed to have R
> wait about a minute before looking for new data.
>
> On my unix-based system, I found that if I usd

I don't think your system IS `unix-based' (Unix is a trademark, and MacOS 
X is based on a rather different kernel).  It is quite possible that it is 
behaving differently from the POSIX description of Unix system calls on 
which R is based for Unix-alikes.

>       Sys.sleep( N )
> then cpu usage immediately went up drastically. If the the system is
> otherwise fairly idle, cpu usage goes up to nearly 100%. A cpu
> monitor shows that R is using the cpu cycles.
>
> If I use instead
>      system('sleep N')
> cpu usage does not go up.

Does that freeze the GUI?  It certainly freezes tcltk widgets on Unix.

> (where N is the number of seconds to sleep)
>
>>  version
>          _
> platform powerpc-apple-darwin7.9.0
> arch     powerpc
> os       darwin7.9.0
> system   powerpc, darwin7.9.0
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
>
>
> At 12:13 PM -0700 7/29/05, Tae-Hoon Chung wrote:
>> Hi, All;
>>
>> I have a question. In R, what is the best way to make R idle for a while and
>> try something again later? For example, suppose there is an R job which
>> accesses a file that may be shared with other active jobs. So when the file
>> is being accessed by other job, your job will not be able to access the file
>> and your job will crash because of that. To avoid this, you want your job to
>> try to access the file repeatedly with some time interval, say every 10
>> seconds or something like that. Which is the best way to do this in R?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Jul 30 09:16:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jul 2005 09:16:26 +0200
Subject: [R] Don't understand plotmath behaviour (bug?)
In-Reply-To: <971536df0507291953f5f8907@mail.gmail.com>
References: <1443722582.20050725121124@eimb.ru>
	<Pine.A41.4.61b.0507251436580.217516@homer04.u.washington.edu>
	<164116838.20050725141338@eimb.ru>
	<971536df0507291953f5f8907@mail.gmail.com>
Message-ID: <x27jf8931x.fsf@turmalin.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> Numbers, not in characters strings do not come out bold:
> 
> plot(1:5, type = "n")
> text(x=3,y=3, quote(bold(paste("a"==a ~~ "0.5" == 0.5))))

(what's the paste() for?)

...and it is a design choice of course. Notice that operators are not
boldfaced either, e.g., in text(x=3,y=3, quote(bold("x"==2*a + b ~~ "x
= 2a+b"))) you get the = and the + in non-bold in the first part
(granted, a bit hard to see for =). As I recall it, LaTeX does
similarly unless you jump through a couple of hoops. It is
intentional, but if you try to do it otherwise you'd run into trouble
with the absense of a bold symbol font.

> On 7/25/05, Wladimir Eremeev <wl at eimb.ru> wrote:
> > Dear Thomas,
> > 
> > TL> In case 1 you have the string "0.5", in case 2 you have the number 0.5.
> > TL>    text(x=2,y=2, quote(bold("0.5"==0.5)))
> > TL> shows what is happening.
> > 
> > I know about the different types in the cases.
> > That is, 'bold' affects only on text strings.
> > Am I right?
> > 
> > ---
> > Best regards,
> > Wladimir                mailto:wl at eimb.ru
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From renaud.lancelot at cirad.fr  Sat Jul 30 10:11:23 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sat, 30 Jul 2005 10:11:23 +0200
Subject: [R] Binary outcome with non-absorbing outcome state
In-Reply-To: <881789.1122644622975.JavaMail.root@wamui-karabash.atl.sa.earthlink.net>
References: <881789.1122644622975.JavaMail.root@wamui-karabash.atl.sa.earthlink.net>
Message-ID: <42EB362B.6070804@cirad.fr>

Frank Funderburk a ??crit :
> Singer & Willett (2003) also cover this ground.
> 
> Singer, JD & Willett, JB (2003).  Applied longitudinal data analysis:
> Modeling change and event occurrence. New Yok:  Oxford University
> Press.
> 
> -----Original Message----- From: Frank E Harrell Jr
> <f.harrell at vanderbilt.edu> Sent: Jul 29, 2005 9:25 AM To: John Sorkin
> <jsorkin at grecc.umaryland.edu> Cc: R-help at stat.math.ethz.ch Subject:
> Re: [R] Binary outcome with non-absorbing outcome state
> 
> John Sorkin wrote:
> 
>> I am trying to model data in which subjects are followed through
>> time to determine if they fall, or do not fall. Some of the
>> subjects fall once, some fall several times. Follow-up time varies
>> from subject to subject. I know how to model time to the first fall
>> (e.g. Cox Proportional Hazards, Kaplan-Meir analyses, etc.) but I
>> am not sure how I can model the data if I include the data for
>> those subjects who fall more than once. I would appreciate
>> suggestions about a models that I could use, how I would quantify
>> the follow-up time, how I account for the imbalance in the data
>> (some subjects would contribute one outcome measure, others 
>> multiple measures), etc.
>> 
>> Many thanks, John
> 
> 
> A great reference for this is
> 
> @Book{the00mod, author =               {Therneau, Terry and Grambsch,
> Patricia}, title =                {Modeling Survival Data: Extending
> the Cox Model}, publisher =    {Springer-Verlag}, year =
> 2000, address =              {New York} }
> 
> Frank
> 
> 
>> 
>> John Sorkin M.D., Ph.D. Chief, Biostatistics and Informatics 
>> Baltimore VA Medical Center GRECC and University of Maryland School
>> of Medicine Claude Pepper OAIC
>> 
>> University of Maryland School of Medicine Division of Gerontology 
>> Baltimore VA Medical Center 10 North Greene Street GRECC (BT/18/GR)
>>  Baltimore, MD 21201-1524
>> 
>> 410-605-7119 ---- NOTE NEW EMAIL ADDRESS: 
>> jsorkin at grecc.umaryland.edu

Another possibility is to discretize the time, group the observations by
covariate pattern and use a beta-binomial model (accounting for possible
overdispersion caused by the within-subject repeated events) with a
cloglog link. When time interval is short (e.g., 1 day), this is
equivalent to a Cox prpoprtional hazards model. See:

Prentice, R.L., Gloeckler, L.A., 1978. Regression analysis of grouped
survival data with application to breast cancer data. Biometrics, 34: 57-67.

and

Prentice, R.L., 1986. Binary regression using an extended beta-binomial
distribution, with discussion of correlation induced by covariate
measurement errors. J.A.S.A. 81, 321-327.

and subsequent papers.

Function betabin in package aod (among others) allows to fit such models.

Best,

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From wongg62 at pacbell.net  Sat Jul 30 10:21:05 2005
From: wongg62 at pacbell.net (Gary Wong)
Date: Sat, 30 Jul 2005 01:21:05 -0700 (PDT)
Subject: [R] How to hiding code for a package
Message-ID: <20050730082105.77961.qmail@web80306.mail.yahoo.com>

Hey everyone,

I have made a package and wish to release it but
before then I have a problem. I have a few functions
in this package written in R that I wish to hide such
that after installation, someone can use say the
function >foo(parameters = "") but cannot do >foo.
Typing foo should not show the source code or at least
not all of it. Is there a way to do this ? I have
searched the mailing list and used google, and have
found something like "[R] Hiding internal package
functions for the doc. pkg-internal.Rd" but this seems
different since it seems that the keyword internal
just hides the function from showing in the index and
hides documentation, not the function itself. Can
someone help? Thanks

Gary



From ripley at stats.ox.ac.uk  Sat Jul 30 11:35:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Jul 2005 10:35:12 +0100 (BST)
Subject: [R] How to hiding code for a package
In-Reply-To: <20050730082105.77961.qmail@web80306.mail.yahoo.com>
References: <20050730082105.77961.qmail@web80306.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0507301029170.28675@gannet.stats>

What you ask is impossible.  For a function to be callable it has to be 
locatable and hence can be printed.

One possibility is to have a namespace, and something like

foo <- function(...) foo_internal(...)

where foo is exported but foo_internal is not.  Then foo_internal is 
hidden from casual inspection, but it can be listed by cognescenti.

Why do you want to do this?  Anyhone can read the source code of your 
package, and any function which can be called can be deparsed, possibly 
after jumping through a few hoops.

On Sat, 30 Jul 2005, Gary Wong wrote:

> Hey everyone,
>
> I have made a package and wish to release it but
> before then I have a problem. I have a few functions
> in this package written in R that I wish to hide such
> that after installation, someone can use say the
> function >foo(parameters = "") but cannot do >foo.
> Typing foo should not show the source code or at least
> not all of it. Is there a way to do this ? I have
> searched the mailing list and used google, and have
> found something like "[R] Hiding internal package
> functions for the doc. pkg-internal.Rd" but this seems
> different since it seems that the keyword internal
> just hides the function from showing in the index and
> hides documentation, not the function itself. Can
> someone help? Thanks

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Sat Jul 30 14:12:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Jul 2005 08:12:32 -0400
Subject: [R] Don't understand plotmath behaviour (bug?)
In-Reply-To: <x27jf8931x.fsf@turmalin.kubism.ku.dk>
References: <1443722582.20050725121124@eimb.ru>
	<Pine.A41.4.61b.0507251436580.217516@homer04.u.washington.edu>
	<164116838.20050725141338@eimb.ru>
	<971536df0507291953f5f8907@mail.gmail.com>
	<x27jf8931x.fsf@turmalin.kubism.ku.dk>
Message-ID: <971536df050730051222a6296d@mail.gmail.com>

On 30 Jul 2005 09:16:26 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Gabor Grothendieck <ggrothendieck at gmail.com> writes:
> 
> > Numbers, not in characters strings do not come out bold:
> >
> > plot(1:5, type = "n")
> > text(x=3,y=3, quote(bold(paste("a"==a ~~ "0.5" == 0.5))))
> 
> (what's the paste() for?)

I had a different example at the beginning but changed it and forgot
to remove the paste.

> 
> ...and it is a design choice of course. Notice that operators are not
> boldfaced either, e.g., in text(x=3,y=3, quote(bold("x"==2*a + b ~~ "x
> = 2a+b"))) you get the = and the + in non-bold in the first part
> (granted, a bit hard to see for =). As I recall it, LaTeX does
> similarly unless you jump through a couple of hoops. It is
> intentional, but if you try to do it otherwise you'd run into trouble
> with the absense of a bold symbol font.
> 
> > On 7/25/05, Wladimir Eremeev <wl at eimb.ru> wrote:
> > > Dear Thomas,
> > >
> > > TL> In case 1 you have the string "0.5", in case 2 you have the number 0.5.
> > > TL>    text(x=2,y=2, quote(bold("0.5"==0.5)))
> > > TL> shows what is happening.
> > >
> > > I know about the different types in the cases.
> > > That is, 'bold' affects only on text strings.
> > > Am I right?
> > >
> > > ---
> > > Best regards,
> > > Wladimir                mailto:wl at eimb.ru
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> --
>   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>



From ggrothendieck at gmail.com  Sat Jul 30 14:21:45 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Jul 2005 08:21:45 -0400
Subject: [R] How to hiding code for a package
In-Reply-To: <20050730082105.77961.qmail@web80306.mail.yahoo.com>
References: <20050730082105.77961.qmail@web80306.mail.yahoo.com>
Message-ID: <971536df050730052124dd188b@mail.gmail.com>

You could set the source attribute like this:

R> f <- function(x) x+1

R> # displays the word hidden instead of showing the source
R> attr(f, "source") <- "hidden"
R> f
hidden
R> f(10)  # still works as a function properly
[1] 11


Of course, someone knowledgable could change the source 
attribute back but this would prevent casual inspection.

On 7/30/05, Gary Wong <wongg62 at pacbell.net> wrote:
> Hey everyone,
> 
> I have made a package and wish to release it but
> before then I have a problem. I have a few functions
> in this package written in R that I wish to hide such
> that after installation, someone can use say the
> function >foo(parameters = "") but cannot do >foo.
> Typing foo should not show the source code or at least
> not all of it. Is there a way to do this ? I have
> searched the mailing list and used google, and have
> found something like "[R] Hiding internal package
> functions for the doc. pkg-internal.Rd" but this seems
> different since it seems that the keyword internal
> just hides the function from showing in the index and
> hides documentation, not the function itself. Can
> someone help? Thanks
> 
> Gary
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dhiren22 at hotmail.com  Fri Jul 29 16:18:51 2005
From: dhiren22 at hotmail.com (Dhiren DSouza)
Date: Fri, 29 Jul 2005 10:18:51 -0400
Subject: [R] Wild card characters
Message-ID: <BAY102-F285684A56922D6DCE48D7AD3CE0@phx.gbl>

I have a string '982323.1' and would like to replace everything after the 
'.' with a '41'.  So the string should look like '982323.41'.  The code I 
use to do this is

sub('\.$','41',982323.1)

This works fine as long as there is only 1 digit after the decimal.  If I 
have '982323.10', then the result of the code is '982323.141' instead of 
'982323.41'

How do I fix the code to replace all characters after the decimal by 41?

Thank you

-D



From ligges at statistik.uni-dortmund.de  Sat Jul 30 19:09:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 30 Jul 2005 19:09:15 +0200
Subject: [R] Wild card characters
In-Reply-To: <BAY102-F285684A56922D6DCE48D7AD3CE0@phx.gbl>
References: <BAY102-F285684A56922D6DCE48D7AD3CE0@phx.gbl>
Message-ID: <42EBB43B.4080204@statistik.uni-dortmund.de>

Dhiren DSouza wrote:

> I have a string '982323.1' and would like to replace everything after the 
> '.' with a '41'.  So the string should look like '982323.41'.  The code I 
> use to do this is
> 
> sub('\.$','41',982323.1)

You have to escape "\", see ?regexp:

sub('\\.[[:digit:]]*$','\\.41', "982323.1")

Uwe Ligges



> This works fine as long as there is only 1 digit after the decimal.  If I 
> have '982323.10', then the result of the code is '982323.141' instead of 
> '982323.41'
> 
> How do I fix the code to replace all characters after the decimal by 41?
> 
> Thank you
> 
> -D
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Mike.Lawrence at Dal.Ca  Sat Jul 30 21:04:37 2005
From: Mike.Lawrence at Dal.Ca (Mike Lawrence)
Date: Sat, 30 Jul 2005 16:04:37 -0300
Subject: [R] xyplot auto.key issue
Message-ID: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>

Hi all,

I'm having a problem with the auto.key function in xyplot. I hate to bother the
list like this and I'm positive I must be missing something very simple, yet
I've spent the last day searching for a solution to no avail.

Essentially, I want a key that contains entries in which the plot points are
superimposed on a line of the same color as the points, like this: o--o--o
Now, given the presence of the default "divide" command, I assume this is
simple; indeed, I get the impression that this representation is supposed to be
produced automatically. Yet I can't seem to get it to work!

Now, I've incorporated various other tweaks to my xyplot function, so I'm
wondering if these tweaks are somehow hindering my efforts. The function is
pasted below; I am making a 3x3 plot, each panel contains 5 lines and it is
these lines that I want represented in the key. See the comments for
descriptions of the modifications.

Any help would be greatly appreciated.

Cheers,

Mike


xyplot(
	#basic settings
	bias ~ sample_size | measure,
	data = bias,
	groups = exp_tau,
	type = "b",
	pch = c(1,2,3,4,5),
	#make strips transparent
	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
	# tweak scales
	scales=list(
		x=list(
			at = c(20, 40, 60),
			tck = c(1,0),
			alternating = F
		),
		y=list(
			at = c(-50, -25, 0, 25, 50),
			tck = c(1,0),
			alternating = F
		)
	),
	# tell key to match symbols to those used in the plot
	par.settings = list(
		superpose.symbol = list(
			cex = .8,
			pch = c(1,2,3,4,5)
		),
		lines = T,
		type = "b"
	),
	# key settings
	auto.key = list (
		lines = T,
		size = 7,
		border = T,
		cex.title = 1.2,
		title = "Expected Tau",
		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
		space = "right",
	)
)



-- 

Mike Lawrence, BA(Hons)
Research Assistant to Dr. Gail Eskes
Dalhousie University & QEII Health Sciences Centre (Psychiatry)

Mike.Lawrence at Dal.Ca

"The road to Wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein



From Mike.Lawrence at Dal.Ca  Sat Jul 30 21:17:29 2005
From: Mike.Lawrence at Dal.Ca (Mike Lawrence)
Date: Sat, 30 Jul 2005 16:17:29 -0300
Subject: [R] xyplot auto.key issue
In-Reply-To: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>
References: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>
Message-ID: <20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>

Quick correction:

The lines "lines = T," & "type = "b"" in the "par.settings" section should not
be there. They are remnants of my previous (failed) attempts at solving the
problem. Below is the correct code:

xyplot(
	#basic settings
	bias ~ sample_size | measure,
	data = bias,
	groups = exp_tau,
	type = "b",
	pch = c(1,2,3,4,5),
	#make strips transparent
	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
	# tweak scales
	scales=list(
		x=list(
			at = c(20, 40, 60),
			tck = c(1,0),
			alternating = F
		),
		y=list(
			at = c(-50, -25, 0, 25, 50),
			tck = c(1,0),
			alternating = F
		)
	),
	# tell key to match symbols to those used in the plot
	par.settings = list(
		superpose.symbol = list(
			cex = .8,
			pch = c(1,2,3,4,5)
		)
	),
	# key settings
	auto.key = list (
		lines = T,
		size = 7,
		border = T,
		cex.title = 1.2,
		title = "Expected Tau",
		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
		space = "right",
	)
)



Quoting Mike Lawrence <Mike.Lawrence at dal.ca>:

> Hi all,
>
> I'm having a problem with the auto.key function in xyplot. I hate to 
> bother the
> list like this and I'm positive I must be missing something very simple, yet
> I've spent the last day searching for a solution to no avail.
>
> Essentially, I want a key that contains entries in which the plot points are
> superimposed on a line of the same color as the points, like this: o--o--o
> Now, given the presence of the default "divide" command, I assume this is
> simple; indeed, I get the impression that this representation is 
> supposed to be
> produced automatically. Yet I can't seem to get it to work!
>
> Now, I've incorporated various other tweaks to my xyplot function, so I'm
> wondering if these tweaks are somehow hindering my efforts. The function is
> pasted below; I am making a 3x3 plot, each panel contains 5 lines and it is
> these lines that I want represented in the key. See the comments for
> descriptions of the modifications.
>
> Any help would be greatly appreciated.
>
> Cheers,
>
> Mike
>
>
> xyplot(
> 	#basic settings
> 	bias ~ sample_size | measure,
> 	data = bias,
> 	groups = exp_tau,
> 	type = "b",
> 	pch = c(1,2,3,4,5),
> 	#make strips transparent
> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> 	# tweak scales
> 	scales=list(
> 		x=list(
> 			at = c(20, 40, 60),
> 			tck = c(1,0),
> 			alternating = F
> 		),
> 		y=list(
> 			at = c(-50, -25, 0, 25, 50),
> 			tck = c(1,0),
> 			alternating = F
> 		)
> 	),
> 	# tell key to match symbols to those used in the plot
> 	par.settings = list(
> 		superpose.symbol = list(
> 			cex = .8,
> 			pch = c(1,2,3,4,5)
> 		),
> 		lines = T,
> 		type = "b"
> 	),
> 	# key settings
> 	auto.key = list (
> 		lines = T,
> 		size = 7,
> 		border = T,
> 		cex.title = 1.2,
> 		title = "Expected Tau",
> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> 		space = "right",
> 	)
> )
>
>
>
> --
>
> Mike Lawrence, BA(Hons)
> Research Assistant to Dr. Gail Eskes
> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>
> Mike.Lawrence at Dal.Ca
>
> "The road to Wisdom? Well, it's plain and simple to express:
> Err and err and err again, but less and less and less."
> - Piet Hein
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



-- 

Mike Lawrence, BA(Hons)
Research Assistant to Dr. Gail Eskes
Dalhousie University & QEII Health Sciences Centre (Psychiatry)

Mike.Lawrence at Dal.Ca

"The road to Wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein



From mblanche at uclink.berkeley.edu  Sat Jul 30 21:57:31 2005
From: mblanche at uclink.berkeley.edu (Marco Blanchette)
Date: Sat, 30 Jul 2005 12:57:31 -0700
Subject: [R]  Premature termination of script
Message-ID: <BF1129BB.3A30%mblanche@uclink.berkeley.edu>

I am fairly new to R and I am writing a script that would take a file, as an
input, and generates a bunch of graphs out of it. My first task is to be
sure that the file is of the right type by looking if there is a valid
barcode in it as in (the barcode is beetween a double and single
underscore):

library(tkWidgets)

##############
#Loading a file
#testing if valid if not kill the execution
##############
data.file <- fileBrowser(textToShow="Select a splice junction array
extraction file")
if ( t <- regexpr("__\\d*_",data.file, perl=T) == -1){
    cat ("The selected file does not seem to be of the right type\n")
    #Should be able to exit the script here
}
cat("File look good keep going\n")

How do I prevent R from interpreting the rest of the script if the user
don't input the right file?




Marco Blanchette, Ph.D.

mblanche at uclink.berkeley.edu

Donald C. Rio's lab
Department of Molecular and Cell Biology
16 Barker Hall
University of California
Berkeley, CA 94720-3204

Tel: (510) 642-1084
Cell: (510) 847-0996
Fax: (510) 642-6062



From ggrothendieck at gmail.com  Sat Jul 30 22:31:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Jul 2005 16:31:37 -0400
Subject: [R] Wild card characters
In-Reply-To: <42EBB43B.4080204@statistik.uni-dortmund.de>
References: <BAY102-F285684A56922D6DCE48D7AD3CE0@phx.gbl>
	<42EBB43B.4080204@statistik.uni-dortmund.de>
Message-ID: <971536df05073013312afae0f6@mail.gmail.com>

Another way to do it is:

sub("[.].*", ".41", x)

This says to replace the first dot and everything after by .41.  When . appears
in a character class, i.e. [.], then you don't need backslashes.  Also
you don't need backslashes in the second argument.  


On 7/30/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Dhiren DSouza wrote:
> 
> > I have a string '982323.1' and would like to replace everything after the
> > '.' with a '41'.  So the string should look like '982323.41'.  The code I
> > use to do this is
> >
> > sub('\.$','41',982323.1)
> 
> You have to escape "\", see ?regexp:
> 
> sub('\\.[[:digit:]]*$','\\.41', "982323.1")
> 
> Uwe Ligges
> 
> 
> 
> > This works fine as long as there is only 1 digit after the decimal.  If I
> > have '982323.10', then the result of the code is '982323.141' instead of
> > '982323.41'
> >
> > How do I fix the code to replace all characters after the decimal by 41?
> >
> > Thank you
> >
> > -D
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dhiren22 at hotmail.com  Sat Jul 30 22:42:30 2005
From: dhiren22 at hotmail.com (Dhiren DSouza)
Date: Sat, 30 Jul 2005 16:42:30 -0400
Subject: [R] Wild card characters
In-Reply-To: <971536df05073013312afae0f6@mail.gmail.com>
Message-ID: <BAY102-F2797FAFA75D653199C05F0D3C10@phx.gbl>

Thank you both :)

-Dhiren

From: Gabor Grothendieck <ggrothendieck at gmail.com>
Reply-To: Gabor Grothendieck <ggrothendieck at gmail.com>
To: Uwe Ligges <ligges at statistik.uni-dortmund.de>
CC: Dhiren DSouza <dhiren22 at hotmail.com>, r-help at stat.math.ethz.ch
Subject: Re: [R] Wild card characters
Date: Sat, 30 Jul 2005 16:31:37 -0400

Another way to do it is:

sub("[.].*", ".41", x)

This says to replace the first dot and everything after by .41.  When . 
appears
in a character class, i.e. [.], then you don't need backslashes.  Also
you don't need backslashes in the second argument.


On 7/30/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
 > Dhiren DSouza wrote:
 >
 > > I have a string '982323.1' and would like to replace everything after 
the
 > > '.' with a '41'.  So the string should look like '982323.41'.  The code 
I
 > > use to do this is
 > >
 > > sub('\.$','41',982323.1)
 >
 > You have to escape "\", see ?regexp:
 >
 > sub('\\.[[:digit:]]*$','\\.41', "982323.1")
 >
 > Uwe Ligges
 >
 >
 >
 > > This works fine as long as there is only 1 digit after the decimal.  If 
I
 > > have '982323.10', then the result of the code is '982323.141' instead 
of
 > > '982323.41'
 > >
 > > How do I fix the code to replace all characters after the decimal by 
41?
 > >
 > > Thank you
 > >
 > > -D
 > >
 > > ______________________________________________
 > > R-help at stat.math.ethz.ch mailing list
 > > https://stat.ethz.ch/mailman/listinfo/r-help
 > > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >



From ggrothendieck at gmail.com  Sat Jul 30 22:48:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 30 Jul 2005 16:48:37 -0400
Subject: [R] Premature termination of script
In-Reply-To: <BF1129BB.3A30%mblanche@uclink.berkeley.edu>
References: <BF1129BB.3A30%mblanche@uclink.berkeley.edu>
Message-ID: <971536df050730134835d83cf8@mail.gmail.com>

On 7/30/05, Marco Blanchette <mblanche at uclink.berkeley.edu> wrote:
> I am fairly new to R and I am writing a script that would take a file, as an
> input, and generates a bunch of graphs out of it. My first task is to be
> sure that the file is of the right type by looking if there is a valid
> barcode in it as in (the barcode is beetween a double and single
> underscore):
> 
> library(tkWidgets)
> 
> ##############
> #Loading a file
> #testing if valid if not kill the execution
> ##############
> data.file <- fileBrowser(textToShow="Select a splice junction array
> extraction file")
> if ( t <- regexpr("__\\d*_",data.file, perl=T) == -1){
>    cat ("The selected file does not seem to be of the right type\n")
>    #Should be able to exit the script here
> }
> cat("File look good keep going\n")

How about just using an if statement:

if (...) { 
   cat("bad")
} else {
   cat("ok")
   # ... more processing
}

> 
> How do I prevent R from interpreting the rest of the script if the user
> don't input the right file?
> 
> 
> 
> 
> Marco Blanchette, Ph.D.
> 
> mblanche at uclink.berkeley.edu
> 
> Donald C. Rio's lab
> Department of Molecular and Cell Biology
> 16 Barker Hall
> University of California
> Berkeley, CA 94720-3204
> 
> Tel: (510) 642-1084
> Cell: (510) 847-0996
> Fax: (510) 642-6062
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pburns at pburns.seanet.com  Sat Jul 30 22:59:39 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 30 Jul 2005 21:59:39 +0100
Subject: [R] Premature termination of script
In-Reply-To: <BF1129BB.3A30%mblanche@uclink.berkeley.edu>
References: <BF1129BB.3A30%mblanche@uclink.berkeley.edu>
Message-ID: <42EBEA3B.5040304@pburns.seanet.com>

You can use 'stop' instead of  'cat' -- in which case you
don't need the newline at the end of the string.

But I suspect you would be better off writing a function
rather than a script.  S Poetry is one of many sources on
writing functions.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Marco Blanchette wrote:

>I am fairly new to R and I am writing a script that would take a file, as an
>input, and generates a bunch of graphs out of it. My first task is to be
>sure that the file is of the right type by looking if there is a valid
>barcode in it as in (the barcode is beetween a double and single
>underscore):
>
>library(tkWidgets)
>
>##############
>#Loading a file
>#testing if valid if not kill the execution
>##############
>data.file <- fileBrowser(textToShow="Select a splice junction array
>extraction file")
>if ( t <- regexpr("__\\d*_",data.file, perl=T) == -1){
>    cat ("The selected file does not seem to be of the right type\n")
>    #Should be able to exit the script here
>}
>cat("File look good keep going\n")
>
>How do I prevent R from interpreting the rest of the script if the user
>don't input the right file?
>
>
>
>
>Marco Blanchette, Ph.D.
>
>mblanche at uclink.berkeley.edu
>
>Donald C. Rio's lab
>Department of Molecular and Cell Biology
>16 Barker Hall
>University of California
>Berkeley, CA 94720-3204
>
>Tel: (510) 642-1084
>Cell: (510) 847-0996
>Fax: (510) 642-6062
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From nxt7 at psu.edu  Sun Jul 31 01:25:28 2005
From: nxt7 at psu.edu (NATALIA F TCHETCHERINA)
Date: Sat, 30 Jul 2005 19:25:28 -0400 (EDT)
Subject: [R] partial SS for anova
Message-ID: <200507302325.TAA04781@webmail5.cac.psu.edu>

Hello,
I use  lme4 package.
library(lme4)
fit=lmer(y ~ time+dye+trt+trt:time + (1|rep), data=dataset, na.action='na.omit')
anova(fit)

The anova gives sequential F-tests and sequential SS.

My question is: how I can get partial F-tests and partial SS?
 For lm (not lmer)  
anova(lm(y~x+z))
we can use 
anova(fit, ssType=3)
but it is not work for lmer.

Natalia.



From spencer.graves at pdf.com  Sun Jul 31 02:53:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Jul 2005 17:53:46 -0700
Subject: [R] Minimum-chi-square estimation
In-Reply-To: <42E8A683.7050605@bth.se>
References: <42E8A683.7050605@bth.se>
Message-ID: <42EC211A.1090104@pdf.com>

	  I just got 11 hits with RSiteSearch("minimum chi-square");  none of 
them seemed relevant.

	  If you still have time and interest for this, why don't you tell us 
about the problem you are trying to solve and why you think minimum 
chi-square is appropriate?  You should be able to write a simple 
function to compute your favorite chi-square and then feed that to 
"optim" to get what you want.  After you try something and get stuck 
please submit another question.  But first, PLEASE do read the posting 
guide! "http://www.R-project.org/posting-guide.html", including the part 
about including a small, self-contained example that someone can copy 
from your email into R and try a few things before replying.  It can 
help you find more things yourself AND increase the chances of getting 
useful reponses to posts.

	  spencer graves

-- 
Dragos Ilie wrote:
> Is there any R package that does point estimation by using the
> minimum-chi-square method?
> 
> Regards,
> Dragos
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul 31 03:27:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Jul 2005 18:27:31 -0700
Subject: [R] catching errors in a loop
In-Reply-To: <6.2.0.14.2.20050728152314.01ee3750@pop.uio.no>
References: <6.2.0.14.2.20050728152314.01ee3750@pop.uio.no>
Message-ID: <42EC2903.9040906@pdf.com>

	  Unfortunately, I don't know how to get any more information from nls. 
  My approach to this kind of problem is to write my own function to 
compute the sum of squares and then use "optim(..., hessian=TRUE)". 
This is less likely to choke, because "optim" will continue with a 
singular hessian, while "nls" may not.  As I recall, when "optim" 
stopped without giving me an answer, the function I asked "optim" to 
either stopped itself or returned NAs to "optim".  Often a little work 
with something like Taylor's theorem produced a decent asymptotic 
approximation for numbers close to the singularities, and I was able to 
get through those cases.  Sometimes, the hessian would be singular, but 
"optim" still gave an answer.  In that case, I knew that the optimum was 
not unique.

	  Alternatively, you could continue with 'try' in the loop you 
described and then explore later the cases where "nls" bombed.

	  If you still have time for this and want to try more, I encourage you 
to try something else like the above and send us another question if you 
get stuck again.  Also I encourage you to read the posting guide! 
"http://www.R-project.org/posting-guide.html", especially the part about 
submitting a simple, self-contained example showing what you tried and 
why it didn't do what you wanted.  If someone can copy a few lines from 
your email into R and try something different, it will typically 
increase the chances that you will get a useful reply.

	  spencer graves	

Anders Bjrgester wrote:

> Hello
> 
> I can't figure out how to handle errors in R. I have a loop, e.g.
> 
> for (i in 2:n) {
> .
> fit <- nls(model), start=list
> if any type of error occur write i to a text file
> .
> }
> 
> I putted try around the nls-expression and this let me run through the 
> loop without R stopping (which I want because each loop takes some time so 
> I do not want it to stop), but I also want to capture the variable when an 
> error occur.
> 
> Appreciate any help
> 
> /Anders
> - - - - - - - - - - - - - - - - - - -
> I tried to use:
> **options(error=write(variable.names(matrix[i]), 
> file="..\\error.txt",append = TRUE)), hoping this made R write to the text 
> file every time an error occurred (but this made R write all is in the 
> loop to the text file).
> **tryCatch(<- nls(model), start=list), finally =write() also writes to a 
> text file but not necessary when there is an error.
> **if (Parameter>x errorM=9 else errorM =0 works but I want to capture any 
> type of error.
> - - - - - - - - - - - - - - - - -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul 31 04:46:21 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Jul 2005 19:46:21 -0700
Subject: [R] Forcing coefficents in lm(), recursive residuals, etc.
In-Reply-To: <5f4f827905072903352c19b9ab@mail.gmail.com>
References: <5f4f82790507281003589a4b7c@mail.gmail.com>	<5f4f8279050728112365e9aeb3@mail.gmail.com>	<5f4f82790507281245360af0e3@mail.gmail.com>
	<5f4f827905072903352c19b9ab@mail.gmail.com>
Message-ID: <42EC3B7D.4030001@pdf.com>

	  I know nothing about recresid, but does the following help:

 > x <- 1:4
 > set.seed(1)
 > DF <- data.frame(x=x, y=x+rnorm(4))
 > fit <- lm(y~offset(x), DF)
 > recresid(fit)
[1] 1.2799320 0.7232336 3.4826581
 >
	  spencer graves

Rick Ram wrote:

> Hi all,
> Just to clarify, I know that the predict() function would the normal
> avenue for applying a model but the problem is that I need to
> calculate recursive residuals, and the recresid() function needs an
> object with class "lm".
> Best,
> R.
> 
> On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
> 
>>Sorry guys, resending this - none of my posts have gone through
>>because HTML emails where not being delivered... sending this
>>plaintext now!
>>
>>On 28/07/05, Rick Ram <r.ramyar at gmail.com> wrote:
>>
>>>Resending cos I think this didn't get through for some reason... apologies
>>>if it arrives twice!
>>>
>>>
>>>---------- Forwarded message ----------
>>>From: Rick Ram < r.ramyar at gmail.com>
>>>Date: 28-Jul-2005 18:03
>>>Subject: Forcing coefficents in lm(), recursive residuals, etc.
>>>To: R-help <r-help at stat.math.ethz.ch >
>>>
>>>Hello all,
>>>
>>>Does anyone know how to constrain/force specific coefficients when running
>>>lm()?
>>>
>>>I need to run recresid() {strucchange package} on the residuals of
>>>forecast.lm, but forecast.lm's coefficients must be determined by
>>>parameter.estimation.lm
>>>
>>>I could estimate forecast.lm without lm() and use some other kind of
>>>optimisation, but recresid() requires an object with class lm.  recresid()
>>>allows you to specify a formula, rather than an lm object, but it looks like
>>>coefficients are estimated this way too and can't be forced.
>>>
>>>Here is a bit of code to compensate for my poor explanation:.
>>>
>>># Estimate the coefficients of model
>>>parameter.estimation.lm = lm(formula = y ~ x1 + x2, data =
>>>estimation.dataset)
>>># How do I force the coefficients in forecast.lm to the coeff estimation
>>>from parameter.estimation.lm??
>>>
>>>forecast.lm = lm(formula = y ~ x1 + x2, data = forecast.dataset)
>>># Because I need recursive residuals from the application of the
>>>coefficients from parameter.estimation.lm to a different dataset
>>>recresid(forecast.lm)
>>>
>>>Thanks in advance guys,
>>>
>>>R.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jul 31 04:55:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Jul 2005 19:55:23 -0700
Subject: [R] Errbar()-function, cap and logarithmic scaling
In-Reply-To: <000d01c59443$78d96540$6233418d@oesa.leipzig.ufz.de>
References: <000d01c59443$78d96540$6233418d@oesa.leipzig.ufz.de>
Message-ID: <42EC3D9B.2000207@pdf.com>

	  I just got 23 hits from 'RSiteSearch("errbar")'.  The second was a 
help file for "errbar" in "sfsmisc", noting there was a similar function 
in "Hmisc".

	  PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  In particular, please 
provide a self-contained, reproducible example, telling us which package 
you are using and which operating system.  If someone can copy a few 
lines of code from your email and paste into R and see what you got, you 
will have a greater likelihood of getting a useful reply.

	  spencer graves
-- 

Ute Visser wrote:

> Hello!
> 
> If I use the errbar-function and have a logarithmic scale on the x-axis,
> then the little horizontal bars at the end of the errbars (cap)
> disappear. What can I do?
> 
> Thanks for helping!
> Ute
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From deepayan.sarkar at gmail.com  Sun Jul 31 07:46:43 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 31 Jul 2005 00:46:43 -0500
Subject: [R] xyplot auto.key issue
In-Reply-To: <20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>
References: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>
	<20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>
Message-ID: <eb555e66050730224650ab391c@mail.gmail.com>

On 7/30/05, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
> Quick correction:
> 
> The lines "lines = T," & "type = "b"" in the "par.settings" section should
> not
> be there. They are remnants of my previous (failed) attempts at solving the
> problem. Below is the correct code:
> 
> xyplot(
> 	#basic settings
> 	bias ~ sample_size | measure,
> 	data = bias,
> 	groups = exp_tau,
> 	type = "b",
> 	pch = c(1,2,3,4,5),
> 	#make strips transparent
> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> 	# tweak scales
> 	scales=list(
> 		x=list(
> 			at = c(20, 40, 60),
> 			tck = c(1,0),
> 			alternating = F
> 		),
> 		y=list(
> 			at = c(-50, -25, 0, 25, 50),
> 			tck = c(1,0),
> 			alternating = F
> 		)
> 	),
> 	# tell key to match symbols to those used in the plot
> 	par.settings = list(
> 		superpose.symbol = list(
> 			cex = .8,
> 			pch = c(1,2,3,4,5)
> 		)
> 	),
> 	# key settings
> 	auto.key = list (
> 		lines = T,
> 		size = 7,

You seem to be missing a 'type="b"' somewhere here. The type="b"
argument to xyplot is actually handled by the panel function. The key
has type="l" by default (see under 'key' in ?xyplot) and has to be
changed explicitly.

> 		border = T,
> 		cex.title = 1.2,
> 		title = "Expected Tau",
> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> 		space = "right",
> 	)
> )
> 
> 
> 
> Quoting Mike Lawrence <Mike.Lawrence at dal.ca>:
> 
> > Hi all,
> >
> > I'm having a problem with the auto.key function in xyplot. I hate to 
> > bother the
> > list like this and I'm positive I must be missing something very simple,
> yet
> > I've spent the last day searching for a solution to no avail.
> >
> > Essentially, I want a key that contains entries in which the plot points
> are
> > superimposed on a line of the same color as the points, like this:
> o--o--o
> > Now, given the presence of the default "divide" command, I assume this is
> > simple; indeed, I get the impression that this representation is 
> > supposed to be
> > produced automatically. Yet I can't seem to get it to work!
> >
> > Now, I've incorporated various other tweaks to my xyplot function, so I'm
> > wondering if these tweaks are somehow hindering my efforts. The function
> is
> > pasted below; I am making a 3x3 plot, each panel contains 5 lines and it
> is
> > these lines that I want represented in the key. See the comments for
> > descriptions of the modifications.
> >
> > Any help would be greatly appreciated.
> >
> > Cheers,
> >
> > Mike
> >
> >
> > xyplot(
> > 	#basic settings
> > 	bias ~ sample_size | measure,
> > 	data = bias,
> > 	groups = exp_tau,
> > 	type = "b",
> > 	pch = c(1,2,3,4,5),
> > 	#make strips transparent
> > 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> > 	# tweak scales
> > 	scales=list(
> > 		x=list(
> > 			at = c(20, 40, 60),
> > 			tck = c(1,0),
> > 			alternating = F
> > 		),
> > 		y=list(
> > 			at = c(-50, -25, 0, 25, 50),
> > 			tck = c(1,0),
> > 			alternating = F
> > 		)
> > 	),
> > 	# tell key to match symbols to those used in the plot
> > 	par.settings = list(
> > 		superpose.symbol = list(
> > 			cex = .8,
> > 			pch = c(1,2,3,4,5)
> > 		),
> > 		lines = T,
> > 		type = "b"
> > 	),
> > 	# key settings
> > 	auto.key = list (
> > 		lines = T,
> > 		size = 7,
> > 		border = T,
> > 		cex.title = 1.2,
> > 		title = "Expected Tau",
> > 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> > 		space = "right",
> > 	)
> > )
> >
> >
> >
> > --
> >
> > Mike Lawrence, BA(Hons)
> > Research Assistant to Dr. Gail Eskes
> > Dalhousie University & QEII Health Sciences Centre (Psychiatry)
> >
> > Mike.Lawrence at Dal.Ca
> >
> > "The road to Wisdom? Well, it's plain and simple to express:
> > Err and err and err again, but less and less and less."
> > - Piet Hein
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> 
> -- 
> 
> Mike Lawrence, BA(Hons)
> Research Assistant to Dr. Gail Eskes
> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
> 
> Mike.Lawrence at Dal.Ca
> 
> "The road to Wisdom? Well, it's plain and simple to express:
> Err and err and err again, but less and less and less."
> - Piet Hein
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Mike.Lawrence at Dal.Ca  Sun Jul 31 08:28:54 2005
From: Mike.Lawrence at Dal.Ca (Mike Lawrence)
Date: Sun, 31 Jul 2005 03:28:54 -0300
Subject: [R] xyplot auto.key issue
In-Reply-To: <eb555e66050730224650ab391c@mail.gmail.com>
References: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>
	<20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>
	<eb555e66050730224650ab391c@mail.gmail.com>
Message-ID: <20050731032854.ay1z0k8bu7co8os4@my1.dal.ca>

Hi Deepayan,

Thanks for the reply, but when I enter the " type = "b" " code into the 
auto.key
(see below) command I get the following message:

Error in valid.pch(x$pch) : zero-length 'pch'

Any suggestions?

xyplot(
	#basic settings
	bias ~ sample_size | measure,
	data = bias,
	groups = exp_tau,
	type = "b",
	pch = c(1,2,3,4,5),
	xlab = "Sample Size",
	ylab = "Bias (ms)",
	#make strips transparent
	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
	# tweak scales
	scales=list(
		x=list(
			at = c(20, 40, 60),
			tck = c(1,0),
			alternating = F
		),
		y=list(
			at = c(-50, -25, 0, 25, 50),
			tck = c(1,0),
			alternating = F
		)
	),
	# tell key to match symbols to those used in the plot
	par.settings = list(
		superpose.symbol = list(
			cex = .8,
			pch = c(1,2,3,4,5)
		)
	),
	# key settings
	auto.key = list (
		type = "b",
		lines = T,
		border = T,
		cex.title = 1.2,
		title = "Expected Tau",
		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
		space = "right"
	)
)


Quoting Deepayan Sarkar <deepayan.sarkar at gmail.com>:

> On 7/30/05, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
>> Quick correction:
>>
>> The lines "lines = T," & "type = "b"" in the "par.settings" section should
>> not
>> be there. They are remnants of my previous (failed) attempts at solving the
>> problem. Below is the correct code:
>>
>> xyplot(
>> 	#basic settings
>> 	bias ~ sample_size | measure,
>> 	data = bias,
>> 	groups = exp_tau,
>> 	type = "b",
>> 	pch = c(1,2,3,4,5),
>> 	#make strips transparent
>> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>> 	# tweak scales
>> 	scales=list(
>> 		x=list(
>> 			at = c(20, 40, 60),
>> 			tck = c(1,0),
>> 			alternating = F
>> 		),
>> 		y=list(
>> 			at = c(-50, -25, 0, 25, 50),
>> 			tck = c(1,0),
>> 			alternating = F
>> 		)
>> 	),
>> 	# tell key to match symbols to those used in the plot
>> 	par.settings = list(
>> 		superpose.symbol = list(
>> 			cex = .8,
>> 			pch = c(1,2,3,4,5)
>> 		)
>> 	),
>> 	# key settings
>> 	auto.key = list (
>> 		lines = T,
>> 		size = 7,
>
> You seem to be missing a 'type="b"' somewhere here. The type="b"
> argument to xyplot is actually handled by the panel function. The key
> has type="l" by default (see under 'key' in ?xyplot) and has to be
> changed explicitly.
>
>> 		border = T,
>> 		cex.title = 1.2,
>> 		title = "Expected Tau",
>> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>> 		space = "right",
>> 	)
>> )
>>
>>
>>
>> Quoting Mike Lawrence <Mike.Lawrence at dal.ca>:
>>
>> > Hi all,
>> >
>> > I'm having a problem with the auto.key function in xyplot. I hate to
>> > bother the
>> > list like this and I'm positive I must be missing something very simple,
>> yet
>> > I've spent the last day searching for a solution to no avail.
>> >
>> > Essentially, I want a key that contains entries in which the plot points
>> are
>> > superimposed on a line of the same color as the points, like this:
>> o--o--o
>> > Now, given the presence of the default "divide" command, I assume this is
>> > simple; indeed, I get the impression that this representation is
>> > supposed to be
>> > produced automatically. Yet I can't seem to get it to work!
>> >
>> > Now, I've incorporated various other tweaks to my xyplot function, so I'm
>> > wondering if these tweaks are somehow hindering my efforts. The function
>> is
>> > pasted below; I am making a 3x3 plot, each panel contains 5 lines and it
>> is
>> > these lines that I want represented in the key. See the comments for
>> > descriptions of the modifications.
>> >
>> > Any help would be greatly appreciated.
>> >
>> > Cheers,
>> >
>> > Mike
>> >
>> >
>> > xyplot(
>> > 	#basic settings
>> > 	bias ~ sample_size | measure,
>> > 	data = bias,
>> > 	groups = exp_tau,
>> > 	type = "b",
>> > 	pch = c(1,2,3,4,5),
>> > 	#make strips transparent
>> > 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>> > 	# tweak scales
>> > 	scales=list(
>> > 		x=list(
>> > 			at = c(20, 40, 60),
>> > 			tck = c(1,0),
>> > 			alternating = F
>> > 		),
>> > 		y=list(
>> > 			at = c(-50, -25, 0, 25, 50),
>> > 			tck = c(1,0),
>> > 			alternating = F
>> > 		)
>> > 	),
>> > 	# tell key to match symbols to those used in the plot
>> > 	par.settings = list(
>> > 		superpose.symbol = list(
>> > 			cex = .8,
>> > 			pch = c(1,2,3,4,5)
>> > 		),
>> > 		lines = T,
>> > 		type = "b"
>> > 	),
>> > 	# key settings
>> > 	auto.key = list (
>> > 		lines = T,
>> > 		size = 7,
>> > 		border = T,
>> > 		cex.title = 1.2,
>> > 		title = "Expected Tau",
>> > 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>> > 		space = "right",
>> > 	)
>> > )
>> >
>> >
>> >
>> > --
>> >
>> > Mike Lawrence, BA(Hons)
>> > Research Assistant to Dr. Gail Eskes
>> > Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>> >
>> > Mike.Lawrence at Dal.Ca
>> >
>> > "The road to Wisdom? Well, it's plain and simple to express:
>> > Err and err and err again, but less and less and less."
>> > - Piet Hein
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> >
>>
>>
>>
>> --
>>
>> Mike Lawrence, BA(Hons)
>> Research Assistant to Dr. Gail Eskes
>> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>>
>> Mike.Lawrence at Dal.Ca
>>
>> "The road to Wisdom? Well, it's plain and simple to express:
>> Err and err and err again, but less and less and less."
>> - Piet Hein
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>



-- 

Mike Lawrence, BA(Hons)
Research Assistant to Dr. Gail Eskes
Dalhousie University & QEII Health Sciences Centre (Psychiatry)

Mike.Lawrence at Dal.Ca

"The road to Wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein



From hb at maths.lth.se  Sun Jul 31 10:53:48 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sun, 31 Jul 2005 10:53:48 +0200
Subject: [R] catching errors in a loop
In-Reply-To: <42EC2903.9040906@pdf.com>
References: <6.2.0.14.2.20050728152314.01ee3750@pop.uio.no>
	<42EC2903.9040906@pdf.com>
Message-ID: <42EC919C.4010807@maths.lth.se>

I think you forgot to catch "error":s in your code, i.e. tryCatch({...}) 
will be the same as if you did {...}.  You want to use a construct like

for (ii in 1:10) {
   tryCatch({
     # Put the code that may generate an error here.
   }, error = function(ex) {
     # here 'ex' is an instance of 'simpleError'
     print(as.character(ex))
     print(ex$message)
   })
}

See ?tryCatch.  There are some additional examples at
http://www.maths.lth.se/help/R/ExceptionHandlingInR/.

A note: if you're not sure how it works, don't catch warnings.  If you 
catch warnings, the function that generates the warning will be 
interrupted, i.e. the warning will in effect be treated an an error 
(that normally interrupts the code).

/Henrik B

Spencer Graves wrote:
> 	  Unfortunately, I don't know how to get any more information from nls. 
>   My approach to this kind of problem is to write my own function to 
> compute the sum of squares and then use "optim(..., hessian=TRUE)". 
> This is less likely to choke, because "optim" will continue with a 
> singular hessian, while "nls" may not.  As I recall, when "optim" 
> stopped without giving me an answer, the function I asked "optim" to 
> either stopped itself or returned NAs to "optim".  Often a little work 
> with something like Taylor's theorem produced a decent asymptotic 
> approximation for numbers close to the singularities, and I was able to 
> get through those cases.  Sometimes, the hessian would be singular, but 
> "optim" still gave an answer.  In that case, I knew that the optimum was 
> not unique.
> 
> 	  Alternatively, you could continue with 'try' in the loop you 
> described and then explore later the cases where "nls" bombed.
> 
> 	  If you still have time for this and want to try more, I encourage you 
> to try something else like the above and send us another question if you 
> get stuck again.  Also I encourage you to read the posting guide! 
> "http://www.R-project.org/posting-guide.html", especially the part about 
> submitting a simple, self-contained example showing what you tried and 
> why it didn't do what you wanted.  If someone can copy a few lines from 
> your email into R and try something different, it will typically 
> increase the chances that you will get a useful reply.
> 
> 	  spencer graves	
> 
> Anders Bjrgester wrote:
> 
> 
>>Hello
>>
>>I can't figure out how to handle errors in R. I have a loop, e.g.
>>
>>for (i in 2:n) {
>>.
>>fit <- nls(model), start=list
>>if any type of error occur write i to a text file
>>.
>>}
>>
>>I putted try around the nls-expression and this let me run through the 
>>loop without R stopping (which I want because each loop takes some time so 
>>I do not want it to stop), but I also want to capture the variable when an 
>>error occur.
>>
>>Appreciate any help
>>
>>/Anders
>>- - - - - - - - - - - - - - - - - - -
>>I tried to use:
>>**options(error=write(variable.names(matrix[i]), 
>>file="..\\error.txt",append = TRUE)), hoping this made R write to the text 
>>file every time an error occurred (but this made R write all is in the 
>>loop to the text file).
>>**tryCatch(<- nls(model), start=list), finally =write() also writes to a 
>>text file but not necessary when there is an error.
>>**if (Parameter>x errorM=9 else errorM =0 works but I want to capture any 
>>type of error.
>>- - - - - - - - - - - - - - - - -
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From sundar.dorai-raj at pdf.com  Sun Jul 31 12:19:08 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 31 Jul 2005 05:19:08 -0500
Subject: [R] xyplot auto.key issue
In-Reply-To: <20050731032854.ay1z0k8bu7co8os4@my1.dal.ca>
References: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>	<20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>	<eb555e66050730224650ab391c@mail.gmail.com>
	<20050731032854.ay1z0k8bu7co8os4@my1.dal.ca>
Message-ID: <42ECA59C.7040205@pdf.com>

Hi, Mike,

Mike Lawrence wrote:
> Hi Deepayan,
> 
> Thanks for the reply, but when I enter the " type = "b" " code into the 
> auto.key
> (see below) command I get the following message:
> 
> Error in valid.pch(x$pch) : zero-length 'pch'
> 
> Any suggestions?
> 

Why not just ignore auto.key and use key? Personally, I use auto.key 
only when I want the defaults. If I want something more customized, then 
I use key. As in,

library(lattice)
set.seed(1)
z <- expand.grid(x=1:10, g = LETTERS[1:5])
z$y <- rnorm(nrow(z))
trellis.par.set(theme = col.whitebg())
par.line <- trellis.par.get("superpose.line")
par.symb <- trellis.par.get("superpose.symbol")
n <- seq(nlevels(z$g))
my.key <- list(space = "right",
                border = TRUE,
                cex.title = 1.2,
                title = "My Key",
                size = 7,
                lines = list(pch = par.symb$pch[n],
                   lty = par.line$lty[n],
                   col = par.line$col[n],
                   type = "b"),
                text = list(levels(z$g)))
xyplot(y ~ x, z, groups = g,
        pch = par.symb$pch[n], type = "b",
        key = my.key)

> xyplot(
> 	#basic settings
> 	bias ~ sample_size | measure,
> 	data = bias,
> 	groups = exp_tau,
> 	type = "b",
> 	pch = c(1,2,3,4,5),
> 	xlab = "Sample Size",
> 	ylab = "Bias (ms)",
> 	#make strips transparent
> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> 	# tweak scales
> 	scales=list(
> 		x=list(
> 			at = c(20, 40, 60),
> 			tck = c(1,0),
> 			alternating = F
> 		),
> 		y=list(
> 			at = c(-50, -25, 0, 25, 50),
> 			tck = c(1,0),
> 			alternating = F
> 		)
> 	),
> 	# tell key to match symbols to those used in the plot
> 	par.settings = list(
> 		superpose.symbol = list(
> 			cex = .8,
> 			pch = c(1,2,3,4,5)
> 		)
> 	),
> 	# key settings
> 	auto.key = list (
> 		type = "b",
> 		lines = T,
> 		border = T,
> 		cex.title = 1.2,
> 		title = "Expected Tau",
> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> 		space = "right"
> 	)
> )
> 
> 
> Quoting Deepayan Sarkar <deepayan.sarkar at gmail.com>:
> 
> 
>>On 7/30/05, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
>>
>>>Quick correction:
>>>
>>>The lines "lines = T," & "type = "b"" in the "par.settings" section should
>>>not
>>>be there. They are remnants of my previous (failed) attempts at solving the
>>>problem. Below is the correct code:
>>>
>>>xyplot(
>>>	#basic settings
>>>	bias ~ sample_size | measure,
>>>	data = bias,
>>>	groups = exp_tau,
>>>	type = "b",
>>>	pch = c(1,2,3,4,5),
>>>	#make strips transparent
>>>	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>>>	# tweak scales
>>>	scales=list(
>>>		x=list(
>>>			at = c(20, 40, 60),
>>>			tck = c(1,0),
>>>			alternating = F
>>>		),
>>>		y=list(
>>>			at = c(-50, -25, 0, 25, 50),
>>>			tck = c(1,0),
>>>			alternating = F
>>>		)
>>>	),
>>>	# tell key to match symbols to those used in the plot
>>>	par.settings = list(
>>>		superpose.symbol = list(
>>>			cex = .8,
>>>			pch = c(1,2,3,4,5)
>>>		)
>>>	),
>>>	# key settings
>>>	auto.key = list (
>>>		lines = T,
>>>		size = 7,
>>
>>You seem to be missing a 'type="b"' somewhere here. The type="b"
>>argument to xyplot is actually handled by the panel function. The key
>>has type="l" by default (see under 'key' in ?xyplot) and has to be
>>changed explicitly.
>>
>>
>>>		border = T,
>>>		cex.title = 1.2,
>>>		title = "Expected Tau",
>>>		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>>>		space = "right",
>>>	)
>>>)
>>>
>>>
>>>
>>>Quoting Mike Lawrence <Mike.Lawrence at dal.ca>:
>>>
>>>
>>>>Hi all,
>>>>
>>>>I'm having a problem with the auto.key function in xyplot. I hate to
>>>>bother the
>>>>list like this and I'm positive I must be missing something very simple,
>>>
>>>yet
>>>
>>>>I've spent the last day searching for a solution to no avail.
>>>>
>>>>Essentially, I want a key that contains entries in which the plot points
>>>
>>>are
>>>
>>>>superimposed on a line of the same color as the points, like this:
>>>
>>>o--o--o
>>>
>>>>Now, given the presence of the default "divide" command, I assume this is
>>>>simple; indeed, I get the impression that this representation is
>>>>supposed to be
>>>>produced automatically. Yet I can't seem to get it to work!
>>>>
>>>>Now, I've incorporated various other tweaks to my xyplot function, so I'm
>>>>wondering if these tweaks are somehow hindering my efforts. The function
>>>
>>>is
>>>
>>>>pasted below; I am making a 3x3 plot, each panel contains 5 lines and it
>>>
>>>is
>>>
>>>>these lines that I want represented in the key. See the comments for
>>>>descriptions of the modifications.
>>>>
>>>>Any help would be greatly appreciated.
>>>>
>>>>Cheers,
>>>>
>>>>Mike
>>>>
>>>>
>>>>xyplot(
>>>>	#basic settings
>>>>	bias ~ sample_size | measure,
>>>>	data = bias,
>>>>	groups = exp_tau,
>>>>	type = "b",
>>>>	pch = c(1,2,3,4,5),
>>>>	#make strips transparent
>>>>	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>>>>	# tweak scales
>>>>	scales=list(
>>>>		x=list(
>>>>			at = c(20, 40, 60),
>>>>			tck = c(1,0),
>>>>			alternating = F
>>>>		),
>>>>		y=list(
>>>>			at = c(-50, -25, 0, 25, 50),
>>>>			tck = c(1,0),
>>>>			alternating = F
>>>>		)
>>>>	),
>>>>	# tell key to match symbols to those used in the plot
>>>>	par.settings = list(
>>>>		superpose.symbol = list(
>>>>			cex = .8,
>>>>			pch = c(1,2,3,4,5)
>>>>		),
>>>>		lines = T,
>>>>		type = "b"
>>>>	),
>>>>	# key settings
>>>>	auto.key = list (
>>>>		lines = T,
>>>>		size = 7,
>>>>		border = T,
>>>>		cex.title = 1.2,
>>>>		title = "Expected Tau",
>>>>		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>>>>		space = "right",
>>>>	)
>>>>)
>>>>
>>>>
>>>>
>>>>--
>>>>
>>>>Mike Lawrence, BA(Hons)
>>>>Research Assistant to Dr. Gail Eskes
>>>>Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>>>>
>>>>Mike.Lawrence at Dal.Ca
>>>>
>>>>"The road to Wisdom? Well, it's plain and simple to express:
>>>>Err and err and err again, but less and less and less."
>>>>- Piet Hein
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>--
>>>
>>>Mike Lawrence, BA(Hons)
>>>Research Assistant to Dr. Gail Eskes
>>>Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>>>
>>>Mike.Lawrence at Dal.Ca
>>>
>>>"The road to Wisdom? Well, it's plain and simple to express:
>>>Err and err and err again, but less and less and less."
>>>- Piet Hein
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>
> 
> 
>



From Soren.Hojsgaard at agrsci.dk  Sun Jul 31 13:46:31 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Sun, 31 Jul 2005 13:46:31 +0200
Subject: [R] Drawing a graph with vertices and edges using tcl/tk
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>

Dear all; I would like to draw a graph with vertices and edges, and I guess tcl/tk would be appropriate. I don't know tcl/tk but have googled for a 10-page (or so) introduction to 'getting started with tcl/tk in R' but without any luck.
- Does anyone know of the existence of such a document or any other web-based material on the subject? 
- Does anyone have an (informative) piece of code which does something similar to what I want?
Thanks in advance
S??ren



From p.dalgaard at biostat.ku.dk  Sun Jul 31 14:45:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Jul 2005 14:45:46 +0200
Subject: [R] Drawing a graph with vertices and edges using tcl/tk
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
Message-ID: <x21x5fcfet.fsf@turmalin.kubism.ku.dk>

S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> Dear all; I would like to draw a graph with vertices and edges, and I guess tcl/tk would be appropriate. I don't know tcl/tk but have googled for a 10-page (or so) introduction to 'getting started with tcl/tk in R' but without any luck.
> - Does anyone know of the existence of such a document or any other web-based material on the subject? 
> - Does anyone have an (informative) piece of code which does something similar to what I want?
> Thanks in advance
> S??ren

Er, one might have expected that you'd be aware of the fact that Jens
Henrik's dynamicGraph package is implemented with tcltk.

If you want something less entangled in S4 classes, here's a bit
of code that goes back to the early gR days. Still seems to work:

graphdiddle <- function(X,Y,Labels,from,to)
{
    if (length(X)!=length(Y) || length(from)!=length(to))
        stop("invalid data")

    top <- tktoplevel()
    tktitle(top) <- "Graph diddler"
    canvas <- tkcanvas(top, relief="raised", width=400, height=400)
    tkpack(canvas)


    moveNode <- function(i)
    {
        force(i)
        function(x,y){
            x <- as.numeric(x)
            y <- as.numeric(y)
            for ( e in nodeEdges[[i]] ){
                tkcoords(canvas,e$edgeItem,x,y,X[e$to],Y[e$to])
            }
            tkmove(canvas, nodeItem[i], x-X[i],y-Y[i])
            X[i] <<- x
            Y[i] <<- y
        }
    }

    nodeEdges <- vector("list",length(x))
    nodeItem <-  vector("character",length(x))
    for ( i in seq(along=from) )
    {
        f <- from[i]
        t <- to[i]
        # add line to canvas
        e <- tkcreate(canvas, "line", X[f],Y[f],X[t],Y[t],
                                   width=2)
        nodeEdges[[f]] <- c(nodeEdges[[f]],list(list(to=t,
    edgeItem=e)))
        nodeEdges[[t]] <- c(nodeEdges[[t]],list(list(to=f,
    edgeItem=e)))
    }
    for ( i in seq(along=x) )
    {
        # add the nodes
        p <- tkcreate(canvas,"oval",X[i]-6,Y[i]-6,X[i]+6,Y[i]+6,
                                   fill="red")
        l <- tkcreate(canvas,"text", X[i]+6, Y[i], text=Labels[i],
                                 anchor="nw", font="10x20")
        tag <- paste("node",i,sep="")
        tkaddtag(canvas, tag, "withtag", p)
        tkaddtag(canvas, tag, "withtag", l)
        nodeItem[i] <- tag
        # animate them
        tkitembind(canvas, p, "<B1-Motion>", moveNode(i))
    }
}
# test code
library(tcltk)
x <- c(100,200,300,200)
y <- c(100,200,300,300)
lbl <- c("sex", "drug", "wok", "wool")
from <- c(1,2,3)
to <- c(2,3,4)
graphdiddle(x,y,lbl,from,to)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Sun Jul 31 15:49:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 31 Jul 2005 09:49:21 -0400
Subject: [R] Drawing a graph with vertices and edges using tcl/tk
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
Message-ID: <971536df05073106495c97b8f1@mail.gmail.com>

Another approach is the Bell Labs graphviz system.  See

   http://www.graphviz.com 

For an example of using it from R, see the dot.proto function
in the 'proto' package.  dot.proto generates dot language output 
to draw a graphviz graph of the objects in any R program written 
using 'proto'.

   library(proto)
   example(dot.proto)  
   # see ?dot.proto for instructions on displaying graph

The above approach should work on any OS supported by R
(since graphviz works on all of them); however, on UNIX one
could also use the Rgraphviz package that specifically
interfaces R with graphviz:

  http://www.bioconductor.org/repository/release1.4/package/html/Rgraphviz.html


On 7/31/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Dear all; I would like to draw a graph with vertices and edges, and I guess tcl/tk would be appropriate. I don't know tcl/tk but have googled for a 10-page (or so) introduction to 'getting started with tcl/tk in R' but without any luck.
> - Does anyone know of the existence of such a document or any other web-based material on the subject?
> - Does anyone have an (informative) piece of code which does something similar to what I want?
> Thanks in advance
> S??ren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rgentlem at fhcrc.org  Sun Jul 31 16:09:51 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Sun, 31 Jul 2005 07:09:51 -0700
Subject: [R] Drawing a graph with vertices and edges using tcl/tk
In-Reply-To: <971536df05073106495c97b8f1@mail.gmail.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
	<971536df05073106495c97b8f1@mail.gmail.com>
Message-ID: <42ECDBAF.2060509@fhcrc.org>

Rgraphviz is a fairly complete interface that works on all systems now, 
including Windows.
http://www.bioconductor.org/packages/bioc/devel/src/contrib/html/Rgraphviz.html

but not the absolute latest Graphviz (2.4) as it has gone through pretty 
extensive code rearrangements. We should have a version for Graphviz 2.4 
in the next few weeks.

Robert

Gabor Grothendieck wrote:
> Another approach is the Bell Labs graphviz system.  See
> 
>    http://www.graphviz.com 
> 
> For an example of using it from R, see the dot.proto function
> in the 'proto' package.  dot.proto generates dot language output 
> to draw a graphviz graph of the objects in any R program written 
> using 'proto'.
> 
>    library(proto)
>    example(dot.proto)  
>    # see ?dot.proto for instructions on displaying graph
> 
> The above approach should work on any OS supported by R
> (since graphviz works on all of them); however, on UNIX one
> could also use the Rgraphviz package that specifically
> interfaces R with graphviz:
> 
>   http://www.bioconductor.org/repository/release1.4/package/html/Rgraphviz.html
> 
> 
> On 7/31/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> 
>>Dear all; I would like to draw a graph with vertices and edges, and I guess tcl/tk would be appropriate. I don't know tcl/tk but have googled for a 10-page (or so) introduction to 'getting started with tcl/tk in R' but without any luck.
>>- Does anyone know of the existence of such a document or any other web-based material on the subject?
>>- Does anyone have an (informative) piece of code which does something similar to what I want?
>>Thanks in advance
>>S??ren
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From deepayan.sarkar at gmail.com  Sun Jul 31 16:47:29 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 31 Jul 2005 09:47:29 -0500
Subject: [R] xyplot auto.key issue
In-Reply-To: <20050731032854.ay1z0k8bu7co8os4@my1.dal.ca>
References: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>
	<20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>
	<eb555e66050730224650ab391c@mail.gmail.com>
	<20050731032854.ay1z0k8bu7co8os4@my1.dal.ca>
Message-ID: <eb555e6605073107472cb3dee5@mail.gmail.com>

On 7/31/05, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
> Hi Deepayan,
> 
> Thanks for the reply, but when I enter the " type = "b" " code into the 
> auto.key
> (see below) command I get the following message:
> 
> Error in valid.pch(x$pch) : zero-length 'pch'

Yes, this was a bug. I believe it's fixed in the latest release
(0.12-1) and if not, it will be fixed in the soon to be released
0.12-2. In either case, you need to specify

pch=c(1, 2, 3, 4, 5)

in auto.key, since you don't want the default pch anyway.

> Any suggestions?
> 
> xyplot(
> 	#basic settings
> 	bias ~ sample_size | measure,
> 	data = bias,
> 	groups = exp_tau,
> 	type = "b",
> 	pch = c(1,2,3,4,5),
> 	xlab = "Sample Size",
> 	ylab = "Bias (ms)",
> 	#make strips transparent
> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> 	# tweak scales
> 	scales=list(
> 		x=list(
> 			at = c(20, 40, 60),
> 			tck = c(1,0),
> 			alternating = F
> 		),
> 		y=list(
> 			at = c(-50, -25, 0, 25, 50),
> 			tck = c(1,0),
> 			alternating = F
> 		)
> 	),
> 	# tell key to match symbols to those used in the plot
> 	par.settings = list(
> 		superpose.symbol = list(
> 			cex = .8,
> 			pch = c(1,2,3,4,5)
> 		)
> 	),
> 	# key settings
> 	auto.key = list (
> 		type = "b",
> 		lines = T,
> 		border = T,
> 		cex.title = 1.2,
> 		title = "Expected Tau",
> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> 		space = "right"
> 	)
> )
> 
> 
> Quoting Deepayan Sarkar <deepayan.sarkar at gmail.com>:
> 
> > On 7/30/05, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
> >> Quick correction:
> >>
> >> The lines "lines = T," & "type = "b"" in the "par.settings" section
> should
> >> not
> >> be there. They are remnants of my previous (failed) attempts at solving
> the
> >> problem. Below is the correct code:
> >>
> >> xyplot(
> >> 	#basic settings
> >> 	bias ~ sample_size | measure,
> >> 	data = bias,
> >> 	groups = exp_tau,
> >> 	type = "b",
> >> 	pch = c(1,2,3,4,5),
> >> 	#make strips transparent
> >> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> >> 	# tweak scales
> >> 	scales=list(
> >> 		x=list(
> >> 			at = c(20, 40, 60),
> >> 			tck = c(1,0),
> >> 			alternating = F
> >> 		),
> >> 		y=list(
> >> 			at = c(-50, -25, 0, 25, 50),
> >> 			tck = c(1,0),
> >> 			alternating = F
> >> 		)
> >> 	),
> >> 	# tell key to match symbols to those used in the plot
> >> 	par.settings = list(
> >> 		superpose.symbol = list(
> >> 			cex = .8,
> >> 			pch = c(1,2,3,4,5)
> >> 		)
> >> 	),
> >> 	# key settings
> >> 	auto.key = list (
> >> 		lines = T,
> >> 		size = 7,
> >
> > You seem to be missing a 'type="b"' somewhere here. The type="b"
> > argument to xyplot is actually handled by the panel function. The key
> > has type="l" by default (see under 'key' in ?xyplot) and has to be
> > changed explicitly.
> >
> >> 		border = T,
> >> 		cex.title = 1.2,
> >> 		title = "Expected Tau",
> >> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> >> 		space = "right",
> >> 	)
> >> )
> >>
> >>
> >>
> >> Quoting Mike Lawrence <Mike.Lawrence at dal.ca>:
> >>
> >> > Hi all,
> >> >
> >> > I'm having a problem with the auto.key function in xyplot. I hate to
> >> > bother the
> >> > list like this and I'm positive I must be missing something very
> simple,
> >> yet
> >> > I've spent the last day searching for a solution to no avail.
> >> >
> >> > Essentially, I want a key that contains entries in which the plot
> points
> >> are
> >> > superimposed on a line of the same color as the points, like this:
> >> o--o--o
> >> > Now, given the presence of the default "divide" command, I assume this
> is
> >> > simple; indeed, I get the impression that this representation is
> >> > supposed to be
> >> > produced automatically. Yet I can't seem to get it to work!
> >> >
> >> > Now, I've incorporated various other tweaks to my xyplot function, so
> I'm
> >> > wondering if these tweaks are somehow hindering my efforts. The
> function
> >> is
> >> > pasted below; I am making a 3x3 plot, each panel contains 5 lines and
> it
> >> is
> >> > these lines that I want represented in the key. See the comments for
> >> > descriptions of the modifications.
> >> >
> >> > Any help would be greatly appreciated.
> >> >
> >> > Cheers,
> >> >
> >> > Mike
> >> >
> >> >
> >> > xyplot(
> >> > 	#basic settings
> >> > 	bias ~ sample_size | measure,
> >> > 	data = bias,
> >> > 	groups = exp_tau,
> >> > 	type = "b",
> >> > 	pch = c(1,2,3,4,5),
> >> > 	#make strips transparent
> >> > 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
> >> > 	# tweak scales
> >> > 	scales=list(
> >> > 		x=list(
> >> > 			at = c(20, 40, 60),
> >> > 			tck = c(1,0),
> >> > 			alternating = F
> >> > 		),
> >> > 		y=list(
> >> > 			at = c(-50, -25, 0, 25, 50),
> >> > 			tck = c(1,0),
> >> > 			alternating = F
> >> > 		)
> >> > 	),
> >> > 	# tell key to match symbols to those used in the plot
> >> > 	par.settings = list(
> >> > 		superpose.symbol = list(
> >> > 			cex = .8,
> >> > 			pch = c(1,2,3,4,5)
> >> > 		),
> >> > 		lines = T,
> >> > 		type = "b"
> >> > 	),
> >> > 	# key settings
> >> > 	auto.key = list (
> >> > 		lines = T,
> >> > 		size = 7,
> >> > 		border = T,
> >> > 		cex.title = 1.2,
> >> > 		title = "Expected Tau",
> >> > 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
> >> > 		space = "right",
> >> > 	)
> >> > )
> >> >
> >> >
> >> >
> >> > --
> >> >
> >> > Mike Lawrence, BA(Hons)
> >> > Research Assistant to Dr. Gail Eskes
> >> > Dalhousie University & QEII Health Sciences Centre (Psychiatry)
> >> >
> >> > Mike.Lawrence at Dal.Ca
> >> >
> >> > "The road to Wisdom? Well, it's plain and simple to express:
> >> > Err and err and err again, but less and less and less."
> >> > - Piet Hein
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >> >
> >>
> >>
> >>
> >> --
> >>
> >> Mike Lawrence, BA(Hons)
> >> Research Assistant to Dr. Gail Eskes
> >> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
> >>
> >> Mike.Lawrence at Dal.Ca
> >>
> >> "The road to Wisdom? Well, it's plain and simple to express:
> >> Err and err and err again, but less and less and less."
> >> - Piet Hein
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> 
> 
> 
> -- 
> 
> Mike Lawrence, BA(Hons)
> Research Assistant to Dr. Gail Eskes
> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
> 
> Mike.Lawrence at Dal.Ca
> 
> "The road to Wisdom? Well, it's plain and simple to express:
> Err and err and err again, but less and less and less."
> - Piet Hein
> 
>



From Mike.Lawrence at Dal.Ca  Sun Jul 31 17:22:05 2005
From: Mike.Lawrence at Dal.Ca (Mike Lawrence)
Date: Sun, 31 Jul 2005 12:22:05 -0300
Subject: [R] xyplot auto.key issue
In-Reply-To: <42ECA59C.7040205@pdf.com>
References: <20050730160437.pcxaeujuwldcowoc@my1.dal.ca>
	<20050730161729.kayxch9ckcz4o0cs@my1.dal.ca>
	<eb555e66050730224650ab391c@mail.gmail.com>
	<20050731032854.ay1z0k8bu7co8os4@my1.dal.ca> <42ECA59C.7040205@pdf.com>
Message-ID: <20050731122205.qnck3l5zt96so0sg@my1.dal.ca>

Hi again,

Deepayan, I tried adding a "pch = c(1,2,3,4,5)" line in the auto key but the
"zero-length 'pch'" error still occurs.

Sundar, I tried your code using key instead of auto.key, and after tweaking it
to fit my design (i.e. from "x~y, groups = a" to "x~y | a, groups = b") it
works perfectly! Thanks!

Here's the code that finally worked (that is, the modifications to Sundar's
code, which might be clearer for others to follow than my own):

library(lattice)
set.seed(1)

z <- expand.grid(x=1:10, b = LETTERS[1:5], a = LETTERS[10:18])
z$y <- rnorm(nrow(z))

trellis.par.set(theme = col.whitebg())
par.line <- trellis.par.get("superpose.line")
par.symb <- trellis.par.get("superpose.symbol")
n <- seq(nlevels(z$b))

my.key <- list(
	space = "right",
	border = TRUE,
	cex.title = 1.2,
	title = "My Key",
	size = 7,
	lines = list(
		pch = par.symb$pch[n],
		lty = par.line$lty[n],
		col = par.line$col[n],
		type = "b"
	),
	text = list(
		levels(z$b)
	)
)

xyplot(
	y ~ x | a,
	data = z,
	groups = b,
	pch = par.symb$pch[n],
	type = "b",
	key = my.key
)










Quoting Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>:

> Hi, Mike,
>
> Mike Lawrence wrote:
>> Hi Deepayan,
>>
>> Thanks for the reply, but when I enter the " type = "b" " code into 
>> the auto.key
>> (see below) command I get the following message:
>>
>> Error in valid.pch(x$pch) : zero-length 'pch'
>>
>> Any suggestions?
>>
>
> Why not just ignore auto.key and use key? Personally, I use auto.key 
> only when I want the defaults. If I want something more customized, 
> then I use key. As in,
>
> library(lattice)
> set.seed(1)
> z <- expand.grid(x=1:10, g = LETTERS[1:5])
> z$y <- rnorm(nrow(z))
> trellis.par.set(theme = col.whitebg())
> par.line <- trellis.par.get("superpose.line")
> par.symb <- trellis.par.get("superpose.symbol")
> n <- seq(nlevels(z$g))
> my.key <- list(space = "right",
>                border = TRUE,
>                cex.title = 1.2,
>                title = "My Key",
>                size = 7,
>                lines = list(pch = par.symb$pch[n],
>                   lty = par.line$lty[n],
>                   col = par.line$col[n],
>                   type = "b"),
>                text = list(levels(z$g)))
> xyplot(y ~ x, z, groups = g,
>        pch = par.symb$pch[n], type = "b",
>        key = my.key)
>
>> xyplot(
>> 	#basic settings
>> 	bias ~ sample_size | measure,
>> 	data = bias,
>> 	groups = exp_tau,
>> 	type = "b",
>> 	pch = c(1,2,3,4,5),
>> 	xlab = "Sample Size",
>> 	ylab = "Bias (ms)",
>> 	#make strips transparent
>> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>> 	# tweak scales
>> 	scales=list(
>> 		x=list(
>> 			at = c(20, 40, 60),
>> 			tck = c(1,0),
>> 			alternating = F
>> 		),
>> 		y=list(
>> 			at = c(-50, -25, 0, 25, 50),
>> 			tck = c(1,0),
>> 			alternating = F
>> 		)
>> 	),
>> 	# tell key to match symbols to those used in the plot
>> 	par.settings = list(
>> 		superpose.symbol = list(
>> 			cex = .8,
>> 			pch = c(1,2,3,4,5)
>> 		)
>> 	),
>> 	# key settings
>> 	auto.key = list (
>> 		type = "b",
>> 		lines = T,
>> 		border = T,
>> 		cex.title = 1.2,
>> 		title = "Expected Tau",
>> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>> 		space = "right"
>> 	)
>> )
>>
>>
>> Quoting Deepayan Sarkar <deepayan.sarkar at gmail.com>:
>>
>>
>>> On 7/30/05, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
>>>
>>>> Quick correction:
>>>>
>>>> The lines "lines = T," & "type = "b"" in the "par.settings" section should
>>>> not
>>>> be there. They are remnants of my previous (failed) attempts at 
>>>> solving the
>>>> problem. Below is the correct code:
>>>>
>>>> xyplot(
>>>> 	#basic settings
>>>> 	bias ~ sample_size | measure,
>>>> 	data = bias,
>>>> 	groups = exp_tau,
>>>> 	type = "b",
>>>> 	pch = c(1,2,3,4,5),
>>>> 	#make strips transparent
>>>> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>>>> 	# tweak scales
>>>> 	scales=list(
>>>> 		x=list(
>>>> 			at = c(20, 40, 60),
>>>> 			tck = c(1,0),
>>>> 			alternating = F
>>>> 		),
>>>> 		y=list(
>>>> 			at = c(-50, -25, 0, 25, 50),
>>>> 			tck = c(1,0),
>>>> 			alternating = F
>>>> 		)
>>>> 	),
>>>> 	# tell key to match symbols to those used in the plot
>>>> 	par.settings = list(
>>>> 		superpose.symbol = list(
>>>> 			cex = .8,
>>>> 			pch = c(1,2,3,4,5)
>>>> 		)
>>>> 	),
>>>> 	# key settings
>>>> 	auto.key = list (
>>>> 		lines = T,
>>>> 		size = 7,
>>>
>>> You seem to be missing a 'type="b"' somewhere here. The type="b"
>>> argument to xyplot is actually handled by the panel function. The key
>>> has type="l" by default (see under 'key' in ?xyplot) and has to be
>>> changed explicitly.
>>>
>>>
>>>> 		border = T,
>>>> 		cex.title = 1.2,
>>>> 		title = "Expected Tau",
>>>> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>>>> 		space = "right",
>>>> 	)
>>>> )
>>>>
>>>>
>>>>
>>>> Quoting Mike Lawrence <Mike.Lawrence at dal.ca>:
>>>>
>>>>
>>>>> Hi all,
>>>>>
>>>>> I'm having a problem with the auto.key function in xyplot. I hate to
>>>>> bother the
>>>>> list like this and I'm positive I must be missing something very simple,
>>>>
>>>> yet
>>>>
>>>>> I've spent the last day searching for a solution to no avail.
>>>>>
>>>>> Essentially, I want a key that contains entries in which the plot points
>>>>
>>>> are
>>>>
>>>>> superimposed on a line of the same color as the points, like this:
>>>>
>>>> o--o--o
>>>>
>>>>> Now, given the presence of the default "divide" command, I assume this is
>>>>> simple; indeed, I get the impression that this representation is
>>>>> supposed to be
>>>>> produced automatically. Yet I can't seem to get it to work!
>>>>>
>>>>> Now, I've incorporated various other tweaks to my xyplot function, so I'm
>>>>> wondering if these tweaks are somehow hindering my efforts. The function
>>>>
>>>> is
>>>>
>>>>> pasted below; I am making a 3x3 plot, each panel contains 5 lines and it
>>>>
>>>> is
>>>>
>>>>> these lines that I want represented in the key. See the comments for
>>>>> descriptions of the modifications.
>>>>>
>>>>> Any help would be greatly appreciated.
>>>>>
>>>>> Cheers,
>>>>>
>>>>> Mike
>>>>>
>>>>>
>>>>> xyplot(
>>>>> 	#basic settings
>>>>> 	bias ~ sample_size | measure,
>>>>> 	data = bias,
>>>>> 	groups = exp_tau,
>>>>> 	type = "b",
>>>>> 	pch = c(1,2,3,4,5),
>>>>> 	#make strips transparent
>>>>> 	strip = function(bg, ...) strip.default(bg = 'transparent', ...),
>>>>> 	# tweak scales
>>>>> 	scales=list(
>>>>> 		x=list(
>>>>> 			at = c(20, 40, 60),
>>>>> 			tck = c(1,0),
>>>>> 			alternating = F
>>>>> 		),
>>>>> 		y=list(
>>>>> 			at = c(-50, -25, 0, 25, 50),
>>>>> 			tck = c(1,0),
>>>>> 			alternating = F
>>>>> 		)
>>>>> 	),
>>>>> 	# tell key to match symbols to those used in the plot
>>>>> 	par.settings = list(
>>>>> 		superpose.symbol = list(
>>>>> 			cex = .8,
>>>>> 			pch = c(1,2,3,4,5)
>>>>> 		),
>>>>> 		lines = T,
>>>>> 		type = "b"
>>>>> 	),
>>>>> 	# key settings
>>>>> 	auto.key = list (
>>>>> 		lines = T,
>>>>> 		size = 7,
>>>>> 		border = T,
>>>>> 		cex.title = 1.2,
>>>>> 		title = "Expected Tau",
>>>>> 		text = c("30 ms", "80 ms", "130 ms", "180 ms", "230 ms"),
>>>>> 		space = "right",
>>>>> 	)
>>>>> )
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>>
>>>>> Mike Lawrence, BA(Hons)
>>>>> Research Assistant to Dr. Gail Eskes
>>>>> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>>>>>
>>>>> Mike.Lawrence at Dal.Ca
>>>>>
>>>>> "The road to Wisdom? Well, it's plain and simple to express:
>>>>> Err and err and err again, but less and less and less."
>>>>> - Piet Hein
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!
>>>>
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>
>>>> --
>>>>
>>>> Mike Lawrence, BA(Hons)
>>>> Research Assistant to Dr. Gail Eskes
>>>> Dalhousie University & QEII Health Sciences Centre (Psychiatry)
>>>>
>>>> Mike.Lawrence at Dal.Ca
>>>>
>>>> "The road to Wisdom? Well, it's plain and simple to express:
>>>> Err and err and err again, but less and less and less."
>>>> - Piet Hein
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>
>>
>>
>



-- 

Mike Lawrence, BA(Hons)
Research Assistant to Dr. Gail Eskes
Dalhousie University & QEII Health Sciences Centre (Psychiatry)

Mike.Lawrence at Dal.Ca

"The road to Wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein



From dkoschuetzki at gmx.de  Sun Jul 31 17:51:38 2005
From: dkoschuetzki at gmx.de (Dirk Koschuetzki)
Date: Sun, 31 Jul 2005 17:51:38 +0200
Subject: [R] Drawing a graph with vertices and edges using tcl/tk
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A6D@DJFPOST01.djf.agrsci.dk>
Message-ID: <op.susboclf61mj6a@moon>

Hello,

On Sun, 31 Jul 2005 13:46:31 +0200, S??ren H??jsgaard  
<Soren.Hojsgaard at agrsci.dk> wrote:

> Dear all; I would like to draw a graph with vertices and edges

Another option besides graphviz and Rgraphviz is the sna package from  
cran. Look into the gplot function, perhaps it is what you need.

Cheers,
Dirk



From pwallem at bio.puc.cl  Sun Jul 31 17:51:56 2005
From: pwallem at bio.puc.cl (Petra Wallem)
Date: Sun, 31 Jul 2005 12:51:56 -0300
Subject: [R] shlib compilation problem
Message-ID: <1122825109.7066.6.camel@linux.site>

Hello everybody

I am having the following problem intalling a packages on R 2.1.0 on OS.
Suse 9.1
Using the command "R CMD" I get following error message
.../SHLIB/: line 1: make: command not found ERROR: compliation failed
for package 'vegan'

Any help on why the make command is not found? 
Thanks a lot
Petra 
-- 
Petra Wallem
Centro de Estudios Avanzados en Ecologa & Biodiversidad (CASEB)
Departamento de Ecologa
Facultad de Ciencias Biolgicas
Pontificia Universidad Catlica de Chile
Av. Libertador Bernardo O'Higgins # 340
Casilla 114-D



From ligges at statistik.uni-dortmund.de  Sun Jul 31 19:00:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 31 Jul 2005 19:00:45 +0200
Subject: [R] shlib compilation problem
In-Reply-To: <1122825109.7066.6.camel@linux.site>
References: <1122825109.7066.6.camel@linux.site>
Message-ID: <42ED03BD.9050209@statistik.uni-dortmund.de>

Petra Wallem wrote:

> Hello everybody
> 
> I am having the following problem intalling a packages on R 2.1.0 on OS.
> Suse 9.1
> Using the command "R CMD" I get following error message

R CMD what?


> .../SHLIB/: line 1: make: command not found ERROR: compliation failed
> for package 'vegan'
> 
> Any help on why the make command is not found? 

This is not related to R but to the setup of your OS.

Is "make" installed?
Is the path known in the corresponing environment variable?

Uwe Ligges


> Thanks a lot
> Petra



From kubovy at virginia.edu  Sun Jul 31 19:29:35 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 31 Jul 2005 13:29:35 -0400
Subject: [R] OS X: workspace clear & workspace browser
Message-ID: <6CBB4A1C-68BB-48C7-A5B1-C69C469D8CB0@virginia.edu>

R Version 2.1.1  (2005-06-20); R Cocoa GUI 1.12 (1622);  Mac OS X  
10.4.2 (8C46)
 > rm(list = ls(all = TRUE))
 > ls()
character(0)

But the workspace browser lists many objects.

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From kubovy at virginia.edu  Sun Jul 31 19:48:54 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 31 Jul 2005 13:48:54 -0400
Subject: [R] Updating to nlme 3.1-62 failing from source (OS X)
Message-ID: <3155B34E-506F-4C64-AC0C-88A79368A539@virginia.edu>

R Version 2.1.1  (2005-06-20); R Cocoa GUI 1.12 (1622);  Mac OS X  
10.4.2 (8C46)

For nlme, the R package installer for CRAN (binaries) gives 3.1-60 as  
Repository Version as well as Installed Version.
For CRAN (sources) it gives 3.1-62 as Repository Version and 3.1-60  
as Installed Version.

When I try to update nlme, I get

* Installing *source* package 'nlme' ...
** libs
gcc-3.3 -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/ 
include  -I/usr/local/include   -fno-common  -g -O2 -c corStruct.c -o  
corStruct.o
/var/tmp//ccOTBSDa.s:6088:** Removing '/Library/Frameworks/ 
R.framework/Versions/2.1.1/Resources/library/nlme'
** Restoring previous '/Library/Frameworks/R.framework/Versions/2.1.1/ 
Resources/library/nlme'

The downloaded packages are in
     /private/tmp/RtmpL1j8Sa/downloaded_packages
Unknown pseudo-op: .p2align
/var/tmp//ccOTBSDa.s:6088:Rest of line ignored. 1st junk character  
valued 50 (2).
/var/tmp//ccOTBSDa.s:6414:Unknown pseudo-op: .p2align
/var/tmp//ccOTBSDa.s:6414:Rest of line ignored. 1st junk character  
valued 50 (2).
/var/tmp//ccOTBSDa.s:6608:Unknown pseudo-op: .p2align
/var/tmp//ccOTBSDa.s:6608:Rest of line ignored. 1st junk character  
valued 50 (2).
/var/tmp//ccOTBSDa.s:7111:Unknown pseudo-op: .subsections_via_symbols
make: *** [corStruct.o] Error 1
ERROR: compilation failed for package 'nlme'


_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



